---

title: Preventing migration of a virtual machine from affecting disaster recovery of replica
abstract: To prevent a user from initiating potentially dangerous virtual machine migrations, a storage migration engine is configured to be aware of replication properties for a source datastore and a destination datastore. The replication properties are obtained from a storage array configured to provide array-based replication. A recovery manager discovers the replication properties of the datastores stored in the storage array, and assigns custom tags to the datastores indicating the discovered replication properties. When storage migration of a virtual machine is requested, the storage migration engine performs or prevents the storage migration based on the assigned custom tags.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09575856&OS=09575856&RS=09575856
owner: VMware, Inc.
number: 09575856
owner_city: Palo Alto
owner_country: US
publication_date: 20140829
---
This application is related to U.S. patent application Ser. No. 14 473 744 filed Aug. 29 2014 also entitled Preventing Migration of a Virtual Machine from Affecting Disaster Recovery of Replica which is assigned to the assignee of this application and the entire contents of which are incorporated by reference herein.

Storage replication is a data protection strategy in which data objects e.g. files physical volumes logical volumes file systems etc. are replicated to provide some measure of redundancy. Storage replication may be used for many purposes such as ensuring data availability upon storage failures site disasters or planned maintenance. Storage replication may be managed directly between storage systems such as storage arrays in an approach referred to as storage based replication or array based replication ABR in contrast to host based replication.

In virtualized computer systems in which disk images of virtual machines are stored in storage arrays disk images of virtual machines are migrated between storage arrays as a way to balance the loads across the storage arrays. For example the Storage VMotion product that is available from VMware Inc. of Palo Alto Calif. allows disk images of virtual machines to be migrated between storage arrays without interrupting the virtual machine whose disk image is being migrated or any applications running inside it. However in certain situations migrations of virtual machines can interfere with storage replication and affect disaster recovery at the replication site.

To facilitate understanding identical reference numerals have been used where possible to designate identical elements that are common to the figures. It is contemplated that elements disclosed in one embodiment may be beneficially utilized on other embodiments without specific recitation.

Protected computing resources include one or more host computers i.e. host s that execute one or more hypervisors which include virtual machines VMs P that are protected. Recovery computing resources include one or more host computers i.e. host s that execute one or more hypervisors which include recovery VMs R that will become available after a failover of the corresponding protected VMs. Each of hypervisor and can be a bare metal hypervisor such as vSphere ESXi commercially available from VMware Inc. of Palo Alto Calif. Alternatively one or more of hypervisor s can execute on top of an operating system OS which is executing on a host. Hypervisors and provide a software interface layer that abstracts computing resource hardware into virtualized hardware enabling sharing of the computing resource hardware among virtual machines. Hypervisor acts as an interface between VMs P and protected computing resources and hypervisor acts as an interface between VMs R and recovery computing resources . Hypervisors and may run on top of an operating system or directly on respective computing resources.

VMs P and R share hardware resources of protected computing resources and recovery computing resources respectively. Each VM typically includes a guest operating system OS and virtualized system hardware not shown implemented in software to emulate corresponding components of an actual computer system. VMs P are part of protected group s of VMs and hence the computing resources shared by VMs P are referred to as protected computing resources. VMs R represent VMs which will become available after a failover or disaster recovery and hence the computing resources shared by VMs R are referred to as recovery computing resources. 

Each of host s is coupled to one or more storage systems and each of host s is coupled to one or more storage systems . Storage systems can include one or more mass storage devices associated networks and the like. Storage system stores protected datastores P and storage system stores recovery datastores R corresponding to protected datastores P. A datastore is a logical container analogous to a file system which hides specifics of each storage device and provides a uniform model for storing files that a hypervisor uses to run virtual machines. A datastore can store one or more virtual disks which store files and data for guest operating systems and applications running in the virtual machines. A datastore can also store VM configuration file s file s that contain VM snapshot s and the like used by a hypervisor to configure and run VMs. Datastores P store files for protected VMs P and datastores R store files for recovery VMs R. Datastores P R are abstracted from the underlying mass storage of storage systems . For example a given datastore can be stored on one or more logical storage units and respectively sometimes referred to as logical volumes or logical unit numbers LUNs which are effectively logical block storage devices exposed by the storage system. Alternatively a given logical storage unit of a storage system can store multiple datastores and a datastore can span across multiple logical storage units.

Computing system includes a virtualization management module that may communicate to the plurality of hosts . In one embodiment virtualization management module is a computer program that resides and executes in a central server which may reside in computing system or alternatively running as a VM in one of hosts . One example of a virtualization management module is the vCenter Server product made available from VMware Inc. of Palo Alto Calif. Virtualization management module is configured to carry out administrative tasks for the computing system including managing hosts managing VMs running within each host provisioning VMs migrating VMs from one host to another host load balancing between hosts creating resource pools comprised of computing resources of hosts and VMs P modifying resource pools to allocate and de allocate VMs and physical resources and modifying configurations of resource pools. It is noted that virtualization management module may be configured similarly.

In one embodiment virtualization management modules is configured to perform storage migration which migrates one or more virtual disks of a virtual machine or the entire virtual machine on a source datastore to a different datastore i.e. destination datastore . Such storage migration may be performed whether virtual machines are offline or are running which is sometimes referred to as a live migration without service disruption. One example storage migration technology may be found in Storage vMotion technology made commercially available by VMware Inc. of Palo Alto Calif.

Virtualization management module is configured to perform manual storage migration in response to user input or in other embodiments automatic storage migration for purposes of balancing the I O load or managing free space across datastores. In one embodiment virtualization management module may automate I O and space load balancing in datastore clusters. Virtualization management module may monitor the I O and space usage of all the datastore in a cluster and automatically or with an explicit user confirmation initiate a storage migration operation for certain virtual disks or entire virtual machines when needed. This enables users to manage their virtual datacenter easily and efficiently without any manual intervention. Virtualization management module may be configured to evacuate a datastore when the datastore needs to be placed in a maintenance mode i.e. taken out of use to be serviced. In such cases virtualization management module triggers a live storage migration for all virtual machines residing on that datastore. In some embodiments virtualization management module may suggest an initial placement of a virtual disk or a virtual machine during a virtual machine provisioning workflow.

In one or more embodiments storage system s include storage based replication manager s and storage system s include storage based replication manager s . Storage based replication managers can control replication of datastores and associated VMs between sites P and R. In another embodiment hypervisor s can include replication manager s and hypervisor s can include replication manager s . Replication managers can control replication of VMs between sites P and R. Some hypervisors can replicate individual VMs to existing datastores. Other hypervisors can replicate the VMs by replicating the datastores on which the VMs reside. Storage based replication managers can operate together with host based replication managers in place of replication managers or can be omitted in favor of only replication managers .

Recovery manager is configured to coordinate with a corresponding recovery manager to perform disaster recovery operations on protected computing resources e.g. VMs P datastores P of protected site P using corresponding recovery computing resources e.g. VMs R datastores R of recovery site R. In some embodiments recovery managers may be executing as a virtual machine or on a physical server not shown . Recovery managers enable a user to plan the availability of virtual workloads in times of a disaster for example by enabling the user to recover their virtual workloads quickly when the production datacenter i.e. protected site P is unavailable. In one embodiment recovery managers are configured to orchestrate a planned failover of virtual workloads along with the associated storage across sites P and R a test failover of virtual workloads along with the associated storage on the recovery site R or unplanned or disaster failover of virtual workloads along with the associated storage across sites P and R. An example recovery manager that provides disaster recovery is vSphere vCenter Site Recovery Manager commercially available from VMware Inc. of Palo Alto Calif.

In order to protect virtual workloads a user may configure recovery managers with a set of replicated datastores that are critical for their business operations. Recovery manager may discover these replicated datastores from storage system and make the datastores available for disaster recovery protection. Datastores may be replicated independently i.e. as standalone datastores or as part of a consistency group. A consistency group is defined as a set of datastores for which the write order is preserved during a replication.

Storage based replication managers may perform replication in a synchronous or asynchronous manner. In synchronous replication any data written to the protected site is also written to the recovery site and I O acknowledgement is returned to the writer only after receiving acknowledgment from both sites. This approach however may suffer from performance and latency issues and often requires the recovery site to be physically proximate to the protected site to alleviate such latency issues. In asynchronous replication subsequent I O operations at the protected site are not held up by replicated I O at the recovery site. Rather asynchronous replication to the recovery site may occur based on a schedule or a constraint known as a Recovery Point Objective RPO that typically specifies an upper limit on the potential data loss upon a failure or disaster. An RPO can be specified in terms of time write operations amount of data changed and the like. For example if an RPO for a certain set of data objects is specified as twenty four hours then a storage replication method designed to support this RPO would need to replicate such a set of data objects at least every twenty four hours. This particular method replicates data object contents in such a way that the RPO for each data object is met. This approach improves performance at the risk of data loss if the protected site fails before data have been replicated to the recovery site.

In operation while protected VMs P are operating recovery VMs R are not operating and datastores P are being replicated to datastores R. In case of disaster recovery initially none of VMs P and R are operating. Recovery managers can begin a disaster recovery workflow that processes datastores R in order to bring online VMs R effectively failing over VMs P to VMs R. After the disaster recovery workflow is complete VMs R are operating in place of VMs P. The same process may works in reverse for fail back of recovery VMs R to protected VMs P.

In every pair of replicated storage devices one datastore is the replication source and the other is the replication target. Data written to the source datastore is replicated to the target datastore on a schedule controlled by replication manager of storage system .

In one embodiment when a VM P is protected by recovery manager there are at least two replication mechanisms involved. First storage based replication is performed by storage system at protected site P to replicate data e.g. virtual machine files to the peer storage system at recovery site R. Second metadata replication is performed by recovery manager to replicate additional metadata associated with a VM such as the identity of the replica device and the path at which the VM replica can be found to the peer recovery manager at recovery site R.

However these replication mechanisms are typically asynchronous with different replication schedules i.e. RPO as described above. Thus if a VM is migrated from one datastore to another within protected site P there may be a window of time in which the replica i.e. the corresponding datastore R at recovery site R is out of sync with the production data. In some situations this may lead to the recovery managers inability to recover a virtual machine from the replica at recovery site R should a disaster happen during this out of sync window. This could effectively lead to the user completely losing the affected virtual machine. This problem is illustrated in below. In addition there could a mismatch between the storage replication in storage systems and the metadata replication in recovery managers as described later in conjunction with .

In at an initial state t t datastore P stores VM data including virtual machine files virtual disk files virtual state files etc. for a VM1 identified as VMDK1 .

In at t t a storage migration operation is initiated to migrate virtual machine files of the VM from source datastore P to destination datastore P within site P. Once the VM data has been copied to destination datastore P the VM data is deleted from source datastore P as depicted in dashed outline.

If source datastore P and destination datastore P do not belong to a same consistency group from a replication perspective datastores P P are likely to be replicated with different replication schedules. For example as shown in datastore P has an RPO of 15 minutes while datastore P has an RPO of 20 minutes. It is understood that even if the datastores had the same RPO setting the asynchronous nature of storage replication may still result in replication mismatches during windows of time albeit brief in which one datastore has been updated before the other. As a result of these mismatches in some cases the deletion of VM data from the source datastore could be replicated to the recovery site before the data migration to the destination datastore is replicated to the recovery site as depicted in .

In at time t t the deletion of the VM data from source datastore P is replicated to recovery site R at datastore R before the data migration of VMDK1 to datastore R is replicated to recovery site R at time t t as shown in . As shown in the VM data have already been deleted from datastore R and the migrated VM data have not been fully replicated to datastore R. Consequently this is a window of time e.g. between t tand t t during which neither the replica i.e. datastore R of source datastore P nor the replica i.e. datastore R of destination datastore P will have the full copy of the VM data. If a failover were initiated for the given VM during this window of time recovery managers would not have the requisite VM data to recover the VM at recovery site R and irrecoverable data loss may occur.

For example datastore P is replicated at time t 7 min. thereby fulfilling its 15 minute RPO by replicating at least once up to every 15 minutes. As such the window of time during which neither replica has the full copy of VM data begins at t 7 min. to the time when the other datastore P replicates which could last until time t 20 min. i.e. the latest time possible to fulfill its 20 minute RPO.

Accordingly embodiments of the present disclosure provide a mechanism to prevent virtual machine storage migrations either automatically or manually initiated when a potential mismatch between a source datastore and a destination datastore exists. To prevent such potentially dangerous VM migrations storage migration functionality of virtualization management module is modified to become aware of the replication properties for the source datastore and a destination datastore as provided by the underlying storage systems. Other approaches to this problem have attempted to configure the components that control storage migration to talk directly to an underlying storage array to discover the replication properties of the datastore involved. However in practice this can be difficult to implement as no common application programming interface API is available that could be used to talk to various storage arrays provided by different vendors. Further by being agnostic of the underlying properties of datastores embodiments of the present disclosure provide storage migration functionality that can support non array backed datastores such as datastores backed by local storage.

Computer system includes a recovery manager configured to discover replication properties of all datastores in a storage system . In one embodiment recovery manager includes a tag manager configured to tag replicated datastores with special tags indicating storage based replication properties of the datastores. In some embodiments recovery manager may tag a datastore with a status tag indicating whether the datastore is replicated or not with a consistency group tag indicating which consistency group the datastore belongs to and a protection group tag indicating in which protection group the datastore is protected in recovery manager . Other types of special tags may be utilized.

Computer system further includes a virtualization management module having a storage migration engine and an inventory service . Storage migration engine is configured to analyze tags of a datastore to evaluate a potential impact of a requested storage migration of a VM on the disaster recovery protection of that VM e.g. by recovery manager . Inventory service is configured to maintain an inventory of objects corresponding to physical and virtualized computing resources of system including hosts VMs P datastores logical storage units and storage systems . The inventory maintained by inventory service includes locations of each physical and virtualized computing resource such as which datastore is stored in which logical storage unit and other properties associated with each physical and virtualized computing resource of system . Inventory service is configured to handle queries for inventory objects and their associated objects. Inventory service is configured to add remove and or modify tags assigned to inventory objects such as datastores which can be used to categorize replication properties. In one embodiment tag manager of recovery manager acts as a proxy for adding and removing tags via inventory service of virtualization management module . The tags are searchable metadata and as such inventory service is configured to provide inventory objects and their replication properties based on queries for certain tags for example from storage migration engine .

Method begins at step where recovery manager discovers one or more logical storage units of storage system and determines one or more replication properties associated with each logical storage unit . In one embodiment recovery manager communicates with storage system via a vendor supported common interface referred to as a storage replication adapter SRA . Recovery manager may query storage system e.g. via SRA for a device configuration which includes identification of the plurality of logical storage units e.g. LUNs and their associated replication properties. Recovery manager may query storage system periodically for example once every 24 hours although any periodicity may be used. Recovery manager may also query storage system in response to user input that forces a rescan of storage system .

In one or more embodiments recovery manager receives a replication topology of logical storage units indicating which of if any logical storage units are configured for storage based replication and other metadata associated with replication. If storage system supports consistency groups storage system may also report which consistency groups if any each of logical storage units belongs to. As used herein a consistency group refers to a set of datastores for which the write order is preserved during replication.

At step recovery manager maps logical storage units of storage system to datastores stored therein and associates the corresponding replication properties with datastores . Recovery manager may query inventory service to determine which datastores are stored in which logical storage unit s or portions of logical storage units . In one embodiment a datastore is deemed replicated if all of its underlying logical storage unit s are configured for replication. This all or nothing principle may be applied to the other replication properties of datastores. For example a datastore is deemed to be a member of a given consistency group if all of its underlying logical storage unit s are members of that consistency group.

At step recovery manager assigns one or more tags to datastores indicating the associated replication properties if any. In one embodiment tag manager of recovery manager directs inventory service to add one or more custom tags to inventory objects corresponding to the mapped datastores indicating the associated replication properties. In an alternative embodiment recovery manager writes the one or more tags directly to datastore such that the tags are accessible to other components within system that can access datastore .

In one embodiment recovery manager tags all replicated datastores with a replication status tag e.g. Status Replicated . Recovery manager tags all datastores that are part of a consistency group with a consistency group tag which contains a unique group identifier . In this way all datastores that belong to the same consistency group will have the same tag assigned to them. Similarly recovery manager tags all datastores that are part of a protection group with a protection group tag which contains a protection group identifier . The tags assigned during step may be categorized in inventory service as being related to replication so as to distinguish from other types of tags that might be assigned to the datastores and facilitate easy retrieval later on. The tag category of the assigned tags may be a distinct field of metadata or in other embodiments may be specified using a predetermined prefix in the tag name e.g. SRM that can be text searched.

In the example shown in recovery manager discovers a first logical storage unit with array based replication enabled and belonging to Consistency Group 1 and to Protection Group A and a second logical storage unit with storage based replication enabled and belonging to Consistency Group 2 and to a Protection Group B. Recovery manager then determines that datastore is stored within unit and associates the replication properties of unit e.g. replication enabled Consistency Group 1 Protection Group A with datastore itself. A similar mapping is performed to associate replication properties of unit with datastore . Then tag manager of recovery manager tags datastore with a replication status tag indicating datastore is configured to storage based replication i.e. Status Replicated with a consistency group tag specifying the identifier associated with Consistency Group 1 e.g. CG GUID 0001 and a protection group tag specifying the identifier associated with Protection Group A e.g. PG GUID 000A . Tag manager assigns tags to second datastore in a similar manner as shown in .

Recovery manager keeps tags of datastore up to date by monitoring the replication topology reported by storage system e.g. as in step . For example when the protection group properties of a datastore change recovery manager will react accordingly and add remove protection group tags as necessary.

Each time that storage migration engine needs to consider a potential VM migration storage migration engine analyzes the tags assigned to the datastore s that are subject to the potential migration to evaluate the potential impact of the migration on the disaster recovery protection of VMs stored on those datastore s .

Method begins at step where storage migration engine receives a request to perform storage migration of a virtual machine from a source datastore to a destination datastore. For example storage migration engine might field a request to migrate virtual machine files from one datastore to another datastore at a protected site. The storage migration may be manually requested by a user or may be automatically requested during redistribution and or storage load balancing e.g. Storage Distributed Resource Scheduler technology made available by VMware Inc. of Palo Alto Calif. .

At step storage migration engine retrieves any tags associated with the source and destination datastores. In one embodiment storage migration engine queries inventory service for any tags that are assigned to the source datastore and the destination datastore and that are categorized as replication related.

At step storage migration engine determines whether the source datastore and the destination datastore are both non replicated. In one embodiment storage migration engine checks replication status tag of source datastore and replication status tag of destination datastore. If both non replicated storage migration engine proceeds to step and performs the requested storage migration of the virtual machine from the source datastore to the destination datastore. In other words migrating between non replicated datastores is immediately allowed.

Responsive to determining to that the source datastore and the destination datastore are not both non replicated storage migration engine proceeds to step . It is noted that this operation covers cases where the source and destination datastores are both replicated and cases where one datastore is replicated and the other is not. In an alternative embodiment if only one datastore is replicated i.e. the other datastore is not replicated storage migration engine may proceed directly to step instead.

At step storage migration engine determines whether the source datastore and the destination datastore are members in a same consistency group. In one embodiment storage migration engine compares the identifier found in consistency group tag associated with the source datastore with the identified value found in consistency group tag associated with the destination datastore. If the same consistency group storage migration engine proceeds to step and performs the requested storage migration. In other words migrating virtual machine storage between replicated datastores in a same consistency group is immediately allowed.

At step storage migration engine determines an impact rating of the request storage migration based on the retrieved tags and further based on an impact matrix. The impact rating may vary between a low impact rating a mild impact rating a severe impact rating although other rating values may be used. The impact matrix represents the variety of scenarios in which the source datastore has different replication properties than the destination datastore which might impact disaster recovery of the datastore at the recovery site.

In one embodiment storage migration engine may determine a mild impact rating for a storage migration from a non replicated datastore to a replicated datastore based on a determination that both source and target datastores are not in any protection groups. In this case a replication overhead fault may be generated. In another embodiment storage migration engine may determine a mild impact rating for a storage migration of a VM from a non replicated datastore to a replicate datastore based on a determination that only the target datastore is in a protection group. In this case a protection overhead fault may be generated. In one embodiment storage migration engine may determine a heightened mild impact rating for a storage migration of a VM from a replicated source datastore to a non replicated target datastore based on a determination that both source and target datastores are not in any protection group and raise a fault indicating a loss of replication may occur. In another embodiment storage migration engine may determine a severe impact rating for a storage migration of a VM from a replicated source datastore to a non replicated target datastore based on a determination that the source datastore was in a protection group and the target datastore is not and raise a fault indicating a loss of protection may occur. These impact ratings are summarized in the example impact matrix shown in Table 1 below.

In situations where a storage migration of a VM from a replicated source datastore and a replicated target datastore is requested where the source and target datastores are in different consistency groups storage migration engine may determine impact ratings as follows. In one embodiment storage migration engine determines a heightened mild impact rating for such a storage migration based on a determination that the source and target datastores are in the same protection group s and raises a replication within group fault. In some embodiments storage migration engine determines a severe impact rating for such a storage migration based on a determination that the source and target datastores have different protection group s and raises a replication outside group fault. In some embodiments storage migration engine determines a heightened severe impact rating for such a storage migration based on a determination that only the source datastore was in a protection group and the target is not and raises a replication loss of protection fault. In some embodiments storage migration engine determines a mild impact rating for such a storage migration based on a determination that only the target datastore is in a protection group and the source is not and raises a replication protection overhead fault. In some embodiments storage migration engine determines a mild impact rating for such a storage migration based on a determination that both the source and target datastores are not in any protection groups and raises a replication no protection fault. The above described impact ratings are summarized in the example impact matrix shown in Table 2 below.

In one or more embodiments storage migration across datastores is allowed to proceed if the source datastore and the destination datastore are in the same consistency group or if both datastores are not being replicated by the underlying storage system. In one embodiment for datastores arranged in clusters for load balancing purposes storage migration engine may perform automatic VM migration between datastores from the same consistency group. For other migrations storage migration engine may generate manual recommendations ranging from low impact to high impact as described above. The user e.g. administrator may still execute these migrations if deemed by the user to be necessary however it is noted that the disaster recovery protection may be at risk and the user has to override the warning to perform the requested storage migration.

At step storage migration engine notifies a user e.g. system administrator of the determined impact rating. In one embodiment storage migration engine presents the impact rating as an alert or warning in a user interface used by the user to access virtualization management module . In some embodiments storage migration engine may transmit an alert e.g. via e mail SMS message instant message to the user.

At step storage migration engine determines whether the user has overridden the rejection of the storage migration. In one embodiment the alert presented or transmitted in step may include a user interface element configured to receive authorization from the user to override the rejection of the storage migration e.g. Override Are you sure . Responsive to determining that the user has overridden the rejection of the storage migration storage migration engine proceeds to step and performs the requested storage migration of the virtual machine s from the source datastore to the destination datastore. Otherwise at step storage migration engine disallows its storage migration request.

As mentioned above another problem that can arise is a mismatch between storage replication performed by the underlying storage arrays e.g. storage systems which replicates virtual machine files and metadata replication in recovery managers which replicates metadata associated with a virtual machine for example the identity of the replica storage device and the path at which the virtual machine replica can be found.

In at an initial state t t datastore P stores VM data for a VM1 including virtual machine files virtual disk files virtual state files etc. Recovery manager tracks VM metadata A associated with VM1 for example metadata indicating that the location of VM data for VM1 is at datastore P. As datastore P is replicated using storage based replication to recovery site R datastore R stores a copy of VM data . Recovery manager maintains a copy of VM metadata A indicating the location of VM data for VM1 is also stored at the replica datastore R.

As shown in at t t a storage migration operation is initiated to migrate VM data from source datastore P to a destination datastore P. Once VM data has been copied to destination datastore P VM data is deleted from source datastore P as depicted in dashed outline. In this example both source datastore P and destination datastore P are a part of the same consistency group from the replication perspective. As such the order of writes across both datastores P and P will be preserved in both replica datastores R and R. Thus the migration of VM data is replicated atomically to datastore R i.e. the replica of source datastore P and datastore R i.e. the replica of destination datastore P at some subsequent time t t as shown in . When datastores are configured in the same consistency group there would not be the problematic situation when neither replica would contain VM data as in the example depicted in described above.

The VM metadata is updated to indicate the new location of VM1 at destination datastore P depicted as metadata B. In order for recovery manager to be able to successfully recover VM1 at this point recovery manager would also need to replicate the updated VM metadata B indicating in part the new location of VM1 at the replica of destination datastore P i.e. datastore R once the storage migration completes at protected site P. This metadata replication typically happens independently and asynchronously of the storage replication of VM data in storage system . As shown in at t t metadata replication is performed to update VM metadata from A to B.

However if VM metadata B is replicated slower than VM data there may be some time during which recovery manager at recovery site R would think that VM1 should still be recovered from datastore R i.e. the replica of source datastore P even though datastore R i.e. the replica of destination datastore P already contains the latest VM data . For instance this out of sync period occurs between time tand time tin the example depicted in . Conversely if VM metadata B is replicated faster than VM data there will be some time during which recovery manager at recovery site R would think that VM1 should be recovered from datastore R i.e. the replica of destination datastore P even though datastore R does not yet contain the migrated VM data . In both cases recovery manager would not be able to recover VM1 during this replication out of sync window.

Accordingly embodiments of the present disclosure provide a technique for detecting during a failover operation if there is a potential mismatch between the known location of the VM replica and the actual data in the replica presented by the storage array. To achieve this the recovery manager at the recovery site is modified to recover VMs not only in the same datastore as the corresponding protected datastore but also to recover VMs in a different datastore as the corresponding protected datastore. A VM migration engine is modified to store additional metadata in the VM files which are used by the recovery manager at the recovery site to find the appropriate instance of the VM in the replica. Prior techniques for VM failover remember the location of VM data looks for the VM data in the recovered datastore and fails if the VM is not found. Embodiments described herein search for all instances of VM data found within the recovered datastores and selects the latest instance of VM data that corresponds to the recovered VM. As such the described technique can handle scenarios where a user manually initiates a storage migration to a datastore in a same protection group or recovery plan as well as scenarios where the user configures for load balancing to automatically move VM data and VMs around the datastores in a same consistency group.

In one or more embodiments datastores and are formatted to store virtual machine data associated with VMs running on host . In the embodiment shown in storage system stores VM data associated with VM P executing in host . VM data includes one or more virtual disk files e.g. VMDK files that store the contents of VM P s virtual hard disk drive. VM data further includes one or more VM related configuration files that stores settings for VM P. VM data may include other data such as log files of the VM s activity a paging file e.g. .vmem files which backs up VM s memory on the host file system i.e. in cases of memory over commitment which are not shown for clarity of illustration. VM data may be organized within a datastore in a designated directory for VM P or in a packaged file. It should be noted that VM data for other VMs executing on host are also contained within datastores of storage system and are omitted from for clarity.

In one embodiment VM configuration file specifies a plurality of settings and configurations associated with an associated VM and in one implementation are embodied by .vmx files. VM configuration file may include virtual hardware settings such as disk sizes amounts of RAM parallel and serial port information NIC information advanced power and resource settings information about a guest operating system power management options. Each virtual machine is assigned an identifier sometimes referred to as a universally unique identifier UUID which can be stored in VM configuration file . In one implementation the UUID may be a 128 bit integer based on a physical computer s identifier and the path to the virtual machine s configuration file. According to one or more embodiments VM configuration file stores information related to the storage migration of the associated VM if pertinent.

Computer system includes a virtualization management module having a storage migration engine at protected site P. Storage migration engine at protected site P is configured to perform a storage migration operation depicted as arrow that moves VM data from a source datastore to a destination datastore . Storage migration engine is further configured to mark instances of VM data during storage migration . These marks are stored directly on the datastore together with the VM data such that the marks are replicated by storage system consistently with the VM data. Computer system includes a recovery manager at recovery site R configured to analyze these marks to choose a best candidate instance of VM data to recover each VM.

In one embodiment storage migration engine is configured to generate a mark indicating whether a given instance of VM data is a source of a migration a mark indicating whether a given instance of VM data is a destination i.e. target of a migration and a mark indicating whether a given instance of VM data has been migrated successfully the latter of which is stored only with a target instance of VM data. In the embodiment shown the instance of VM data stored in datastore includes a migration source field indicating that instance of VM data is a source of a migration arrow . The instance of VM data on datastore includes a migration target field indicating that instance of VM data is a target of the migration and a migration completed field . It should be recognized that the described marks may be stored within the datastore using a variety of implementations for example encoded in a single data field or in any of multiple fields. In an alternative embodiment marks indicating whether a given instance of VM data is a source or a destination may share a single field having a logical true or false value respectively.

Method begins at step where storage migration engine at protected site P receives a request to migrate VM data from a source datastore to a destination datastore . The storage migration operation may be initiated manually by a user or automatically by a load balancing component of virtualization management module . At step storage migration engine prepares to copy VM data to destination datastore . In some embodiments storage migration engine creates files and directories at destination datastore including VM configuration files .

At step storage migration engine marks the instance of VM data at source datastore indicating the instance as a source of a storage migration and marks the newly created instance of VM data at destination datastore indicating the instance as a destination of a storage migration. In one embodiment storage migration engine writes a first value e.g. logical true to a migration source field within VM configuration file indicating that the instance of VM data is associated with a source of a migration. Storage migration engine writes a value e.g. logical true to a migration target field within VM configuration file within datastore indicating the instance of VM data is associated with a destination of a migration. In some embodiments storage migration engine may write a value e.g. logical false to a migration completed field within VM configuration file within datastore indicating that that instance of VM data at datastore has not yet been migrated successfully.

At step storage migration engine copies VM data including VMDK s and other VM related files from source datastore to datastore . At step responsive to completing copying of VM data from the source datastore to the destination datastore storage migration engine marks the instance of VM data in destination datastore with an indication that the given VM data has been migrated successfully. For example storage migration engine changes the value e.g. to a logical true of migration completed field within VM configuration file to indicate that that instance of VM data at datastore has now been successfully and completely migrated. In some embodiments storage migration engine marks the source instance as migration completed as well i.e. changes the value of a migration completed field not shown of the instance of VM data at the source datastore .

At step storage migration engine updates VM metadata with the new location of VM data i.e. at datastore and deletes the old instance of VM data at source datastore . In some embodiments storage migration engine may update the inventory service of virtualization management module to specify the new storage location of files associated with VM P within datastore .

At step of method recovery manager at recovery site R receives a request to fail over VM P to the replica i.e. VM R at recovery site R. The failover operation may be initiated manually by a user as a planned or test failover or automatically as part of a planned or unplanned failover. The failover operation may specify one or more virtual machines to be recovered including particular virtual machine s virtual machines on particular datastores and virtual machines in a particular protection group.

At step recovery manager at the recovery site scans all recovered datastores to locate and identify all instances of VM data stored on the datastores. In one embodiment recovery manager searches datastores at recovery site R for all VM configuration files e.g. all .vmx files corresponding to instances of VM data . In some embodiments recovery manager interacts with datastores via a datastore browser functionality provided by virtualization management module which enables recovery manager to delete files from a datastore search files of a datastore or move files between datastores. In other embodiments recovery manager interacts directly with datastores via a storage interface of storage system .

Depending on the timing of the failover operation relative to any storage migrations between datastores that may have occurred at protected site P and to replication of such changes to recovery site R a mismatch of VM data may exist in datastores at recovery site R. For example a replica VM at recovery site R may have more than one instance of VM data i.e. one instance of VM data on datastore and another instance on datastore in cases of storage replication of an in progress storage migration at protected site P. In cases where a failover operation initiates after a storage migration at protected site P but before storage replication an instance of VM data for replica VM R might be stored on one datastore even though an early replication of metadata at recovery site R might indicate the VM s data should be on datastore . As such recovery manager uses metadata stored directly within datastores to recover VM R.

At step recovery manager retrieves one or more marks from the identified instances of VM data in datastores . In one embodiment recovery manager retrieves from each identified instance an indication of whether the given instance of VM data is a source of a storage migration an indication of whether the given instance of VM data is a target of a storage migration and an indication of whether the given instance is a target instance that has been migrated successfully. In the example of recovery manager retrieves the values of migration source field from VM configuration file of the first instance of VM data discovered in datastore and the values of migration target field and migration completed field from VM configuration file of the second instance of VM data discovered in datastore .

At step recovery manager groups the identified instances of VM data according to their associated virtual machine. In one embodiment recovery manager retrieves an identifier e.g. UUID from each instance of VM data e.g. contained in VM configuration file specifying the virtual machine associated with the instance. Recovery manager groups each instance of VM data according to the VM identifier e.g. UUID contained in the instance. The VM identifier may be used to filter for those VM data to be used for recovery. For each VM to be recovered and having a given UUID recovery manager performs the operations as follows.

At step recovery manager determines whether the VM to be recovered has more than two instances of VM data having the associated identifier i.e. UUID . If so at step recovery manager raises an error and does not recover the VM as there is no clear candidate VM data instance. Similarly in some embodiments recovery manager may raise an error if no instances of VM data are found having the identifier associated with the VM to be recovered. Recovery manager may proceed to a next VM to be recovered and perform the operations described below.

At step recovery manager determines whether only a single instance of VM data exists for a particular VM to be recovered based on the associated identifier. If so at step recovery manager performs the requested failover operation using that single instance of VM data. This operation covers the normal scenario for failover in which no problems involving storage migration and replication have arisen. This operation also enables for failover even in cases where the instance of VM data exists at a different location than previously known by recovery manager i.e. a scenario of replication mismatch between storage replication and metadata replication described earlier. As such recovery manager performs the requested failover using that single instance of VM data which may mean disregarding location metadata that has been replicated to recovery manager for that VM and might indicate a different location than ultimately used e.g. VM metadata A in .

At step recovery manager deletes in progress destination instance s of VM data i.e. instances of VM data from incomplete migrations. In one implementation recovery manager deletes any instances of VM data having migration target field indicating the instance is a storage migration destination and migration completed field e.g. false indicating the migration has not been completed successfully. In alternative embodiments rather than delete the file s as referred to herein recovery manager only marks the incomplete destination instances of VM data for deletion and omits the instance from further consideration for recovery. In another embodiment recovery manager merely ignores such incomplete destination instances of VM data and leaves the instances as is without deleting.

At step recovery manager determines whether a completed destination instance of VM data exists. If so at step recovery manager performs the requested failover operation to a VM R at recovery site R using the completed destination instance of VM data . This operation enables failover in the scenario in which a storage migration was completed at protected site P however only the destination datastore had been replicated to recovery site R. As a result both datastore and datastore contain instances of VM data. This case may arise when the involved datastores and are in the same protection group but not in the same consistency group and replication mismatches between source and destination datastore occur.

In one embodiment at step recovery manager deletes any completed source instance of VM data from the underlying datastores if exists. For example responsive to determining the instance of VM data on datastore has a migration target field indicating the instance is a target migration destination and migration completed field e.g. true indicating the migration has been completed recovery manager deletes the corresponding source instance of VM data stored on datastore having a migration source field indicating the instance is a source instance of a successfully completed migration . In some embodiments recovery manager deletes the source instance of VM data stored on datastore having a migration source field as well as a migration completed field not shown indicating the instance is a source of a successfully completed migration i.e. true true .

At step responsive to determining that a complete destination instance of VM data does not exist recovery manager checks whether there is now exactly one instance of VM data remaining i.e. an in progress source instance of VM data. If so at step recovery manager performs the requested failover operation using the in progress source instance of VM data. That is recovery manager performs failover on a VM R using the instance of VM data having a migration source field indicating i.e. true that the instance is a storage migration source and in some embodiments a migration completed field indicating i.e. false that the storage migration has not yet been completed successfully. This operation enables an unplanned failover to recovery site R when replication had occurred in the middle of a VM storage migration. Responsive to determining that there is not exactly one instance of VM remaining i.e. none or two remaining instances recovery manager proceeds to step raising an error and does not recover the VM.

The various embodiments described herein may employ various computer implemented operations involving data stored in computer systems. For example these operations may require physical manipulation of physical quantities usually though not necessarily these quantities may take the form of electrical or magnetic signals where they or representations of them are capable of being stored transferred combined compared or otherwise manipulated. Further such manipulations are often referred to in terms such as producing identifying determining or comparing. Any operations described herein that form part of one or more embodiments of the invention may be useful machine operations. In addition one or more embodiments of the invention also relate to a device or an apparatus for performing these operations. The apparatus may be specially constructed for specific required purposes or it may be a general purpose computer selectively activated or configured by a computer program stored in the computer. In particular various general purpose machines may be used with computer programs written in accordance with the teachings herein or it may be more convenient to construct a more specialized apparatus to perform the required operations.

The various embodiments described herein may be practiced with other computer system configurations including hand held devices microprocessor systems microprocessor based or programmable consumer electronics minicomputers mainframe computers and the like.

One or more embodiments of the present invention may be implemented as one or more computer programs or as one or more computer program modules embodied in one or more computer readable media. The term computer readable medium refers to any data storage device that can store data which can thereafter be input to a computer system computer readable media may be based on any existing or subsequently developed technology for embodying computer programs in a manner that enables them to be read by a computer. Examples of a computer readable medium include a hard drive network attached storage NAS read only memory random access memory e.g. a flash memory device a CD Compact Discs CD ROM a CD R or a CD RW a DVD Digital Versatile Disc a magnetic tape and other optical and non optical data storage devices. The computer readable medium can also be distributed over a network coupled computer system so that the computer readable code is stored and executed in a distributed fashion.

Although one or more embodiments of the present invention have been described in some detail for clarity of understanding it will be apparent that certain changes and modifications may be made within the scope of the claims. Accordingly the described embodiments are to be considered as illustrative and not restrictive and the scope of the claims is not to be limited to details given herein but may be modified within the scope and equivalents of the claims. In the claims elements and or steps do not imply any particular order of operation unless explicitly stated in the claims.

Plural instances may be provided for components operations or structures described herein as a single instance. Finally boundaries between various components operations and data stores are somewhat arbitrary and particular operations are illustrated in the context of specific illustrative configurations. Other allocations of functionality are envisioned and may fall within the scope of the invention s . In general structures and functionality presented as separate components in exemplary configurations may be implemented as a combined structure or component. Similarly structures and functionality presented as a single component may be implemented as separate components. These and other variations modifications additions and improvements may fall within the scope of the appended claims.

