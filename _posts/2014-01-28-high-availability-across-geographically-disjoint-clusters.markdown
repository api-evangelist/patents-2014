---

title: High availability across geographically disjoint clusters
abstract: Exemplary methods, apparatuses, and systems include a first virtual infrastructure management (VIM) server monitoring a first host device to determine if the first host device receives one or more messages within an interval of time from a first storage device indicating a failure of one or more logical disks within the first storage device. The first VIM server manages a first virtual datacenter including the first host device and the first storage device. A second VIM server manages a second virtual datacenter including a second host device and a second storage device. The logical disk is replicated on the second storage device. The first VIM server determines, that a plurality of virtual machines running on the first host device is dependent upon the logical disk(s). The first VIM server performs, in response to the dependency upon the logical disk, a failover of the virtual machines to the second host device.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09183101&OS=09183101&RS=09183101
owner: VMware, Inc.
number: 09183101
owner_city: Palo Alto
owner_country: US
publication_date: 20140128
---
The various embodiments described herein relate to fault recovery in virtual datacenters. In particular embodiments relate to the failover of a virtual machine between geographically disjoint clusters of devices in response to the loss of a logical storage device.

A primary consideration for the architecture of a virtual datacenter is how to best maximize the availability of the services provided by the virtual machines. Availability solutions are designed to improve the resiliency of local systems or entire sites and fall broadly into the categories of downtime avoidance and fault recovery. Fault recovery solutions include high availability and disaster recovery. High availability HA is an automated failover solution typically within a single datacenter that responds to unplanned outages and restarts virtual machines as appropriate. For example if a virtual machine fails on one host device HA may respond by restarting the virtual machine on another host device. Disaster recovery is a manual process for recovering all or a portion of a datacenter at a recovery site from replicated data. For example a disaster recovery tool alerts an administrator of a possible site failure. The administrator may then provide input to the disaster recovery tool to initiate recovery of all or a portion of the inventory of virtual machines within the protected datacenter.

Recently HA has been applied to clusters of devices that span datacenter sites. These stretched clusters offer the ability to balance workloads between two datacenters enabling migration of services between geographically close sites without sustaining an outage. Stretched clusters add benefits to site level availability and downtime avoidance but introduce considerable complexity at the network and storage layers as well as demanding rigorous operational management and change control. A cluster depends upon a single logical storage subsystem and single virtualization management server. As a result the stretched cluster does not provide fault tolerance for the virtualization management server. A stretched cluster expands upon the functionality of a cluster by enabling devices within multiple locations to be a part of a single cluster. For example disk writes are committed synchronously at both locations to ensure that data is consistent regardless of the location from which it is being read. The stretched cluster replication model however does not support asynchronous replication and requires significant bandwidth and very low latency between the sites involved in the cluster. As a result stretched cluster sites are kept within a limited geographic range e.g. within 100 kilometers or 5 microseconds round trip time latency. Additionally should a major portion of the virtual environment fail current implementations of HA are not designed for complex disaster recovery scenarios in which virtual machines start in a particular sequence. For example critical virtual machines may need to start prior to other systems that are dependent on those virtual machines. Current implementations of HA are unable to control this start order handle alternate workflows or handle different scenarios for failure. Current implementations of HA also do not provide geographically distant multisite recovery.

While disaster recovery tools enable complex recovery scenarios while providing site and virtualization management server fault tolerance current implementations of HA restrict the ability to use disaster recovery tools because HA is dependent upon a single virtualization management server and disaster recovery tools are dependent upon multiple virtualization management servers.

Exemplary methods apparatuses and systems include a first virtual infrastructure management VIM server monitoring a first host device to determine if the first host device receives one or more messages within an interval of time from a first storage device indicating a failure of one or more logical disks within the first storage device. The first VIM server manages a first virtual datacenter site including the first host device and the first storage device. A second VIM server manages a second virtual datacenter site including a second host device and a second storage device. The logical disk is replicated on the second storage device. The first VIM server determines that a plurality of virtual machine VMs running on the first host device is dependent upon the logical disk s . The first VIM server automatically performs in response to the determination that the plurality of VMs is dependent upon the logical disk a failover of the VMs to the second host device using the replicated logical disk.

In one embodiment the failover includes determining that the first virtual machine belongs to a first protection group of one or more other virtual machines that are also dependent upon the logical disk. As a result the virtual machines belonging to the first protection group are failed over to the second virtual datacenter site.

Other features and advantages will be apparent from the accompanying drawings and from the detailed description.

Embodiments described herein include a first virtual infrastructure management VIM server monitoring a first host device to determine if the first host device receives a message from a storage device indicating a failure of a logical disk within the storage device. The first VIM server automatically determines in response to detecting the message that a plurality of virtual machine VMs running on the first host device is dependent upon the logical disk and performs a failover of the plurality of VMs from a first datacenter managed by the first VIM sever to a second datacenter managed by a second VIM server. The recovered VMs are started on a second host device using the second datacenter s replicated copy of the logical disk. As a result the functionality of high availability that was previously limited to a single datacenter is extended to geographically disjoint datacenters managed by separated VIM servers. In addition to providing site recovery failure tolerance using either synchronous or asynchronous replication and without special hardware embodiments described herein provide automated recovery for both an entire site as well as a single device including the virtualization management server.

VMs are complete computation environments containing virtual equivalents of the hardware and system software components of a physical system and are typically implemented by an extensive virtualization infrastructure which includes a variety of software and hardware components. In one embodiment one or more of the VMs implement virtual desktops. A virtual desktop is a virtual computing system that operates as a desktop or workstation computer with which an end user can interact using desktop remoting protocol and the software and or hardware of client device coupled to the corresponding host device over a network. In one embodiment on or more of the VMs implement a virtualized compute networking storage or security service e.g. a firewall webserver database server etc. .

Virtualization software layer runs on hardware of host device e.g. a physical computer and manages one or more VMs. Virtualization software layer manages physical resources e.g. hardware as well as maintains virtual to physical hardware mappings. For example virtualization software may manage VM access to a processor memory or network interface within hardware as well as a virtual disk for each VM within storage A or storage B .

VIM server A includes datacenter manager A . In one embodiment datacenter manager A provides a management console for manual and automated control of hosts VMs and storage A . Similarly datacenter manager B provides a management console for manual and automated control of hosts VMs and storage B . For example datacenter manager A and datacenter manager B provision configure and maintain VMs as virtual desktops or network services manage pools of computer resources e.g. within storage A storage B to run the VMs and provide remote administrative access define protection groups recovery plans a preferred recovery type and other failover policies. As used herein a protection group refers to a set of one or more VMs that use a replicated data store or data store group. For example when a replicated data store is failed over that operation affects the VMs in the corresponding protection group. A recovery plan specifies how the virtual machines in a protection group are recovered. For example a recovery plan may control the steps of the recovery process including the order in which VMs are powered off or powered on the network addresses that recovered VMs use etc.

VIM server A further includes recovery manager A and VIM server B further includes recovery manager B . Recovery managers each monitor their respective host devices for indications of an error with or failure of a logical storage device. Additionally recovery managers manage replication of data between storage A and storage B and implement failover of VMs as described with reference to .

Each of storage A and storage B includes one or more storage devices including non volatile memory. In one embodiment storage A and storage B are storage arrays and include storage controllers to serve read and write requests and management commands queries from host devices and VIM servers . Each of storage A and storage B is partitioned into logical units volumes and or disks which are used interchangeably herein that are stored on one or more of the storage devices. For example storage A is partitioned into six logical units including three protected logical units identified by logical unit numbers LUNs LUN A LUN A and LUN A and three recovery logical units identified by LUN B LUN B and LUN B. Similarly storage B is partitioned into six logical units including three protected logical units identified by LUN B LUN B and LUN B and three recovery logical units identified by LUN A LUN A and LUN A.

Components of datacenter A are coupled to components of datacenter B . While various components are illustrated as being directly coupled for the ease of explanation e.g. to illustrate the direction of replicated data a network including one or more switches or routers may lie between datacenter A and datacenter B and facilitate the coupling of the various components.

For example VIM server A configures storage A by designating one or more protected logical units LUN A LUN A and LUN A to be replicated to datacenter B . VIM server B e.g. in coordination with VIM server A configures storage B by designating on ore more recovery logical units LUN A LUN A and LUN A to receive and store the replicated data. Additionally VIM server A configures protected logical units as active e.g. to be available for read and write commands from VMs in host device s . VIM server B configures recovery logical units as read only for VMs in host device s to prevent corruption of the back up data.

Similarly VIM server B may configure storage B by designating one or more protected logical units LUN B LUN B and LUN B to be replicated to datacenter A and to be available for read and write commands from VMs . VIM server A e.g. in coordination with VIM server B configures storage A by designating on or more recovery logical units LUN B LUN B and LUN B to receive and store the replicated data and to prevent any write attempts by VMs . As a result datacenters A and B are configured for bidirectional replication of data and each provide for recovery of the other. For the ease of explanation however the remainder of method will be described with reference to protection of data from datacenter A to datacenter B and the corresponding failover. The replication of data from datacenter B to datacenter A and corresponding failover may be similarly performed.

In one embodiment storage A and B replicate data synchronously. For example when VM writes data to protected LUN A storage A sends a copy of the data to recovery LUN A in storage B . Upon receiving confirmation from storage B of the write to recovery LUN A storage A confirms the completion of the write with VM . Given the delay in waiting for storage B to confirm the write a synchronous embodiment is implemented with datacenters within a limited geographic range e.g. within 100 kilometers or 5 microseconds round trip time latency.

Alternatively storage A and B replicate asynchronously. Similar to synchronous replication when VM writes data to protected LUN A storage A sends a copy of the data to recovery LUN A in storage B individually or batched with other writes . Storage A confirms the completion of the write with VM upon completion of the write to protected LUN A . Storage A however does not wait to receive confirmation from storage B of the write to recovery LUN A before confirming the completion of the write with VM and therefore does not have the latency geographic range requirement of synchronous replication. With asynchronous replication the copy of data stored within recovery LUN A may represent a back up of data stored within protected LUN A within a threshold time delay.

In one embodiment VIM server A configures each VM that is dependent upon a protected logical unit such that VIM server B can add each VM to the inventory at the recovery site datacenter B . For example each VM is assigned to a resource pool folder and network that exist within datacenter B . In one embodiment VIM server B adds placeholder VMs for each protected VM . Placeholder VMs are treated like any other VMs of datacenter B inventory although they are not powered on. When a placeholder VM is created its folder network and compute resource assignments are derived from inventory mappings established at the protected site datacenter A .

At block VIM server A monitors host device to determine if host device receives a message from storage A indicating a failure of a logical disk unit within storage device A . In one embodiment VIM server A polls host device at an interval for logical disk failures or errors. For example recovery manager A utilizes an application programming interface API within virtualization software to request any received permanent device loss PDL events or other logical unit failures. As used herein a PDL refers to when storage controller is not able to communicate with the logical device unit and thus cannot serve any read or write requests for an indefinite time period. Alternatively the monitoring includes host device forwarding the message to or otherwise alerting VIM server of the logical unit failure.

If host device has not received a message from storage A indicating a failure of or error with a logical unit method returns to block and storage A continues replicating data and VIM server A continues polling host device for logical unit failures. If host device received a message from storage A indicating a failure of a logical disk at block VIM server A optionally groups failure messages. For example VIM server A may use an adaptive polling frequency to poll for additional PDL messages and or buffer logical disk failure messages received within a time period. An exemplary method of grouping failure messages is described herein with reference to . In one embodiment VIM server A utilizes the adaptive polling and sliding window buffering described in U.S. patent application Ser. No. 13 856 167 which is hereby incorporated by reference in its entirety.

At block VIM server A automatically determines one or more VMs dependent upon the failed logical unit in response to the determination that host device received the message. In an embodiment in which the messages are buffered VIM server A maps the messages to one or more virtual machines dependent upon the failed logical unit s . An exemplary method of mapping one or more failure messages to VMs dependent upon the failed logical unit is described herein with reference to .

In one embodiment VIM server A configures multiple VMs dependent upon a single logical unit or group of logical units to be a part of a protection group. As described herein when the logical unit s that support a protection group failover the failover operation affects all VMs that use the logical unit s in the protection group. As a result VIM server A maps the failure of a logical unit to all VMs within the preconfigured protection group.

At block VIM server A optionally determines a recovery type. For example recovery manager A may be configured by a manual or default setting to prioritize the failover of a subset of the plurality of VMs to facilitate an expedient recovery of a critical service. The remainder of the plurality VMs may be failed over separately subsequently or not at all. For example a subset of VMs within datacenter A may be assigned to a critical service and failed over first to minimize any interruption to that critical service. Alternatively the recovery type setting may indicate that a minimum number of VMs to be failed over and VIM server A waits until the minimum number is reached prior to initiating the fail over.

At block VIM server A optionally selects one or more recovery plans. For example VIM server A may store a mapping between one or more of the protection groups error types recovery types and recovery plans. Based upon the failed logical unit and stored mapping VIM server A selects a recovery plan. For example the failure of a logical disk may be addressed by more than one recovery plan. The recovery type or other recovery policy may be used to indicate which recovery plan to choose. The recovery plan specifies ordered steps to be taken in a failover of VMs e.g. belonging to the protection group mapped to the failed logical unit . For example protected VMs may be powered down within datacenter A in a particular order and started as VMs within datacenter B in a particular order. In one embodiment the recovery plan further includes a prioritized order in which multiple protection groups are failed over.

At block VIM server A and VIM server B automatically perform the failover of VM s in response to the determination that the VM s are dependent upon the failed logical unit. For example the protected VMs are stopped within datacenter A and storage A replicates any remaining data that can be completed from operational protected logical units e.g. within the same protection group as the failed protected logical unit . VIM server A then stops the replication of the logical units being failed over. VIM server B restarts stopped protected VMs as recovered VMs within datacenter B using recovery logical units . VIM server B configures recovery logical units as active enabling recovered VMs to read from and write to recovery logical units . Similarly VIM server A configures corresponding logical units as inactive read only.

In an embodiment in which a recovery plan is selected the failover is performed according to the order specified in the selected recovery plan.

At block VIM server A polls or otherwise monitors host device to determine if host device receives a message from storage A indicating a failure of a logical disk unit within storage device A . At block VIM server A determines if a new failure has occurred. If no new failure has occurred at block VIM server A optionally causes the polling to sleep for an interval of time. After the sleep interval or if no sleep interval VIM server A resumes polling for failures at block .

If a failure has occurred at block VIM server A starts a timer for a long interval as a maximum amount of time to poll for additional disk failures to group with the first failure. At block VIM server A adds the first disk failure to a buffer. At block VIM server A causes the polling to sleep for a short interval of time. For example the short interval of time is shorter than the long timer.

After the short sleep interval at block VIM server A polls for an additional failure. At block VIM server A determines if an additional failure has occurred. If an additional failure has occurred at block VIM server A adds the additional failure to the buffer. At block VIM server A determines if the long timer has expired. If the long timer has not expired method returns to block . If the long timer has expired or if no additional disk failures occur after the short interval of sleep at block VIM server A processes the buffered failures e.g. as described with reference to . After processing the failures method returns to block either directly or optionally after sleeping for an interval of time at block .

At block VIM server A determines if the data store has already been marked for failover. If the data store has not yet been marked at block VIM server A marks the data store for failover. Once the data store is marked or if the data store was already marked at block VIM server A determines if any additional failures remain in the group e.g. in the buffer described above . If there are additional failures to process method returns to block to process the next disk failure.

If there are no additional failures to process at block VIM server A determines corresponding VMs e.g. a protection group for a first next marked data store. At block VIM server A determines if the protection group has already been marked for failover. If not at block VIM server A marks the protection group for failover. If or once the protection group is marked for failover at block VIM server A determines if additional data stores remain to be processed in the group of marked data stores. If there are additional data stores method returns to block to process the next data store.

If there are no additional data stores at block VIM server A selects a set of one or more recovery plans for the marked protection groups and performs the failover according to the recovery type described above.

Data processing system includes memory which is coupled to microprocessor s . Memory may be used for storing data metadata and programs for execution by the microprocessor s . Memory may include one or more of volatile and non volatile memories such as Random Access Memory RAM Read Only Memory ROM a solid state disk SSD Flash Phase Change Memory PCM or other types of data storage. Memory may be internal or distributed memory.

Data processing system includes network and port interfaces such as a port connector for a dock or a connector for a USB interface FireWire Thunderbolt Ethernet Fibre Channel etc. to connect the system with another device external component or a network. Exemplary network and port interfaces also include wireless transceivers such as an IEEE 802.11 transceiver an infrared transceiver a Bluetooth transceiver a wireless cellular telephony transceiver e.g. 2G 3G 4G etc. or another wireless protocol to connect data processing system with another device external component or a network and receive stored instructions data tokens etc.

Data processing system also includes display controller and display device and one or more input or output I O devices and interfaces . Display controller and display device provides a visual user interface for the user. I O devices allow a user to provide input to receive output from and otherwise transfer data to and from the system. I O devices may include a mouse keypad or a keyboard a touch panel or a multi touch input panel camera optical scanner audio input output e.g. microphone and or a speaker other known I O devices or a combination of such I O devices.

It will be appreciated that one or more buses may be used to interconnect the various components shown in .

Data processing system is an exemplary representation of one or more of VIM server A host device s storage A VIM server B host device s and storage B described above. Data processing system may be a personal computer tablet style device a personal digital assistant PDA a cellular telephone with PDA like functionality a Wi Fi based telephone a handheld computer which includes a cellular telephone a media player an entertainment system or devices which combine aspects or functions of these devices such as a media player combined with a PDA and a cellular telephone in one device. In other embodiments data processing system may be a network computer server or an embedded processing device within another device or consumer electronic product. As used herein the terms computer device system processing system processing device and apparatus comprising a processing device may be used interchangeably with data processing system and include the above listed exemplary embodiments.

It will be appreciated that additional components not shown may also be part of data processing system and in certain embodiments fewer components than that shown in may also be used in data processing system . It will be apparent from this description that aspects of the inventions may be embodied at least in part in software. That is the computer implemented method may be carried out in a computer system or other data processing system in response to its processor or processing system executing sequences of instructions contained in a memory such as memory or other non transitory machine readable storage medium. The software may further be transmitted or received over a network not shown via network interface device . In various embodiments hardwired circuitry may be used in combination with the software instructions to implement the present embodiments. Thus the techniques are not limited to any specific combination of hardware circuitry and software or to any particular source for the instructions executed by data processing system .

An article of manufacture may be used to store program code providing at least some of the functionality of the embodiments described above. Additionally an article of manufacture may be used to store program code created using at least some of the functionality of the embodiments described above. An article of manufacture that stores program code may be embodied as but is not limited to one or more memories e.g. one or more flash memories random access memories static dynamic or other optical disks CD ROMs DVD ROMs EPROMs EEPROMs magnetic or optical cards or other type of non transitory machine readable media suitable for storing electronic instructions. Additionally embodiments of the invention may be implemented in but not limited to hardware or firmware utilizing an FPGA ASIC a processor a computer or a computer system including a network. Modules and components of hardware or software implementations can be divided or combined without significantly altering embodiments of the invention.

In the foregoing specification the invention has been described with reference to specific exemplary embodiments thereof. Various embodiments and aspects of the invention s are described with reference to details discussed herein and the accompanying drawings illustrate the various embodiments. The description above and drawings are illustrative of the invention and are not to be construed as limiting the invention. References in the specification to one embodiment an embodiment an exemplary embodiment etc. indicate that the embodiment described may include a particular feature structure or characteristic but not every embodiment may necessarily include the particular feature structure or characteristic. Moreover such phrases are not necessarily referring to the same embodiment. Furthermore when a particular feature structure or characteristic is described in connection with an embodiment such feature structure or characteristic may be implemented in connection with other embodiments whether or not explicitly described. Blocks with dashed borders e.g. large dashes small dashes dot dash dots are used herein to illustrate optional operations that add additional features to embodiments of the invention. However such notation should not be taken to mean that these are the only options or optional operations and or that blocks with solid borders are not optional in certain embodiments of the invention. Numerous specific details are described to provide a thorough understanding of various embodiments of the present invention. However in certain instances well known or conventional details are not described in order to provide a concise discussion of embodiments of the present inventions.

It will be evident that various modifications may be made thereto without departing from the broader spirit and scope of the invention as set forth in the following claims. For example the methods described herein may be performed with fewer or more features blocks or the features blocks may be performed in differing orders. Additionally the methods described herein may be repeated or performed in parallel with one another or in parallel with different instances of the same or similar methods.

