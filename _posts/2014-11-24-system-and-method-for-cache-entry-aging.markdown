---

title: System and method for cache entry aging
abstract: A system comprises a host device and a cache controller. The host device includes a command buffer and a host application that posts a cache command that includes a cache key and a key aging alias in the command buffer. The cache controller includes logic circuitry configured to load the cache command from the command buffer of the first host device into the buffer memory, identify a match, if any, for the cache key in the command queue, perform the cache command, and return cache completion status information to the first host application, wherein the cache completion status information includes a value of the key aging alias in cache metadata when a match for the cache key is found and includes a value of the key aging alias provided by the first host application when a match for the cache key is not found.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09384147&OS=09384147&RS=09384147
owner: Saratoga Speed, Inc.
number: 09384147
owner_city: San Jose
owner_country: US
publication_date: 20141124
---
This Application is a Continuation Application of U.S. application Ser. No. 14 458 606 filed Aug. 13 2014 the contents of which are hereby incorporated by reference in its entirety and the benefit of priority is claimed herein.

The present disclosure relates generally to networked storage systems and more particularly in some embodiments to subsystems for facilitating data storage and access in flash based networked storage systems.

The large amounts of information generated daily challenge data handling facilities as never before. In the context of today s information generation data is being generated at rates perhaps thousands or tens of thousands of times greater than was the data generation rate in the 1990s. Historically large volumes of data sparked explosive growth in data communications. Responses to growing amounts of data generation centered on improving the movement of data based in increased transmission data rates to enhance throughput in communication channels. For instance transmission pipelines grew from a few tens of megabits per second Mb s transmission rates to several tens of gigabits per second Gb s rates during the 1990s.

In the same period typical storage devices such as hard disk drives HDDs when amassed in sufficient numbers might accommodate large volumes of data but the rates at which data may be stored and retrieved have not scaled at the same rate as the volume of data stored on the devices has increased. Data access rates for HDDs are at similar orders of magnitude today as they were in the 1990s.

Fundamental storage subsystems have not integrated technology to enable scaling of effective data storage at the same rate that data generation is growing. Hence the challenge to systems handling large volumes of data is not likely to be alleviated by the combination of contemporary HDD technology with high speed data transmission channels. In order to handle and manage big data information processing facilities will be pressured to utilize larger volumes of storage with higher performance rates for capturing and accessing data.

The following description is presented to enable any person skilled in the art to create and use a computer system that provides high speed access to data storage devices particularly Flash storage devices. Various modifications to the embodiments will be readily apparent to those skilled in the art and the generic principles defined herein may be applied to other embodiments and applications without departing from the spirit and scope of the invention. Moreover in the following description numerous details are set forth for the purpose of explanation. However one of ordinary skill in the art will realize that the invention might be practiced without the use of these specific details. In other instances well known data structures and processes are shown in block diagram form in order not to obscure the description of the invention with unnecessary detail. Identical reference numerals may be used to represent different views of the same item in different drawings. Flow diagrams in drawings referenced below are used to represent processes. A computer system is configured to perform some of these processes. The flow diagrams that represent computer implemented processes include modules that represent the configuration of a computer system according to computer program code to perform the acts described with reference to these modules. Thus the present invention is not intended to be limited to the embodiments shown but is to be accorded the widest scope consistent with the principles and features disclosed herein.

In accordance with some embodiments the network fabric of the first and second packet routing networks is compliant with the PCI Express Base Specification hereinafter PCIe released by the PCISIG PCI Special Interest Group . See PCI Express Technology Comprehensive Guide to Generations 1.x 2.x and 3.0 by M. Jackson and R. Budruk 2102 Mindshare Inc. PCIe specifies point to point bidirectional serial communication paths between endpoints over switches and connection lines. Information is transmitted in packets between endpoints over the routing networks . A PCIe network includes serial connection lines commonly referred to as links that are capable of sending and receiving information at the same time. More specifically information transmitted through either one or the other of the routing networks is encapsulated in packets that include routing information that indicates a source endpoint and a destination endpoint. According to the PCIe specification and in accordance with some embodiments a link can include one or more serial transmit and serial receive connection pairs. Each individual pair is referred to as a lane. A link can be made up of multiple lanes. Each lane uses differential signaling sending both positive and negative versions of the same signal. Advantages of differential signaling include improved noise immunity and reduced signal voltage. Each endpoint device coupled to one or both of the routing networks includes core logic that implements one or more functions. A device that is a component of a typical PCIe compliant network can have multiple functions up to eight in some embodiments each implementing its own configuration space.

The first management processor is used to configure the first packet routing network circuit to provide point to point communication between components operably coupled to it. The second management processor is used to configure the second packet routing network circuit to provide point to point communication between components operably coupled to it. In some embodiments the first and second management processors configure point to point routing within the first and second packet routing networks. In other words for a given pair of resource circuits a fixed route among switches in the internal network circuits or is configured to transmit packets between the pair.

The PCIe specification specifies use of a root complex to configure a PCIe compliant network. A root complex includes interface circuitry e.g. processor interface DRAM interface that couples a management processor and the rest of a PCIe network. Management processor includes first and second root complexes that act as interfaces between processor and network circuits and . Management processor includes second and third root complexes that act as interfaces between processor and network circuits and . The term root is used to indicate that the root complex is disposed at a root of an inverted tree topology that is characteristic of a hierarchical PCIe compliant network.

Referring again to the I O interface circuits to provide high speed connections between the external network e.g. InfiniBand Fibre Channel and or Ethernet and the first switch network circuitry . The I O circuitry provides protocol conversion including packet format conversion during high speed data communication between the external network and the first switch network circuitry . In some embodiments the external network I O interface circuits to are implemented as network interface cards commonly referred to as NICs which include circuits that are configured to transform packets to suitable formats as they pass between the external network and the routing networks .

The storage I O interface circuits to manage the distribution of data across the Flash storage circuits to . In some embodiments the storage I O interface circuits are configured to implement a file system used to control how data is stored in and retrieved from storage devices. In some embodiments the storage I O interface circuits to are implemented as RAID controllers configured to organize data across multiple storage devices such as Flash storage devices to . The term RAID refers to data storage schemes that combine multiple disk drive components into a logical unit for the purposes of data redundancy and performance improvement. Persons skilled in the art will appreciate that Flash storage sometimes referred to as solid state drive SSD is a data storage device using integrated circuit assemblies as memory to store data persistently. Each of the storage access switch networks to provides point to point connections to respectively using a serial protocol that moves data to and from the Flash storage devices to . In some embodiments the storage access switch networks to use a protocol that includes the SAS Serial Attached SCSI protocol. In general according to the SAS protocol there are three types of SAS devices initiators e.g. RAID controllers target storage devices e.g. Flash circuits and expanders. An initiator device attaches to one or more target storage devices to create a SAS domain. In some embodiments the storage I O interface circuits implemented as RAID controllers act as SAS initiators. In accordance with some embodiments the Flash storage circuits to act as SAS targets. Using expanders e.g. low cost high speed switches the number of targets attached to an initiator can be increased to create a larger SAS domain.

Communication paths couple storage I O interface circuit to exchange data with storage access switch networks and . Communication paths couple storage I O interface circuit to exchange data with storage access switch circuits and . Communication paths couple storage I O interface circuit to exchange data with storage access network circuits and . Communication paths couple storage I O interface circuit to exchange data with storage access switch networks and . Thus all Flash circuits to are accessible via the first internal network circuit via the storage I O interface circuits coupled to it and all Flash circuits to are accessible via the second internal network circuit via the storage I O interface circuits coupled to it.

In some embodiments the first and second packet processing circuits are implemented as field programmable gate array FPGAs . FPGA circuitry often can impart services with less latency delay and therefore faster than a typical general purpose management processor for example since the programmable logic can be programmed in advance to dedicate specific hardware circuitry to provide the services. Programmable hardware logic such as FPGA circuitry often can perform operations faster than for example a general purpose processor which often uses software interrupts to transition between different operations. Alternatively in accordance with some embodiments one or more of the packet processing circuits can include a special purpose processor an application specific integrated circuit ASIC or an array of processors configured to run software to perform a given service.

The first and second packet processing circuits of also are directly coupled to each other so that the same data can be cached at both. In some embodiments a communication path coupling the first and second programmable logic circuits includes a circuit connection compliant with a high speed network communication protocol. In some embodiments the communication path complies with the Ethernet protocol.

The first programmable logic circuit is operably coupled to first cache storage circuitry . The second programmable logic circuit is operably coupled to second cache storage circuitry . In some embodiments the first and second cache circuits include DRAM circuits. More particularly in some embodiments the first and second cache circuits include Flash backed DRAM circuits in which Flash circuits are coupled to stored data persistently in the event of failure of a corresponding DRAM circuit.

Cached data and cache metadata can be stored in Flash backed up DRAM included in cache storage circuitry and . is a block diagram illustrating a dual inline memory module DIMM containing DRAM and flash memory which in accordance with some embodiments can be plugged into a standard DDR3 DIMM socket. In some embodiments during operation the module behaves similar to a standard DDR3 DRAM DIMM however upon the occurrence of the events specified below data is copied between the DRAM and flash. The entire contents of on board DRAM are written to flash upon the occurrence of any of the following 

The packet processing circuits and of can be configured to include a cache controller to provide cache management services. The circuitry of a cache controller may include a state machine to perform the functions described. This offloads cache control functions to hardware from software to increase cache speed. A cache controller may also be capable of accommodating other hardware accelerators for off loading CPU software functions to hardware. Some examples of these functions include encryption decryption duplication de duplication compression de compression processing replication and snapshot.

The software driver e.g. of a management processor and cache controller allow for 1 1 redundancy of data storage between cache memory and system flash memory. Data redundancy may also be applied to the cached data by having two cache controllers operate together to mirror the cache data. If one cache controller fails the other cache controller can operate independently and cached data is preserved without any data loss.

Each cache controller may include an interface to DRAM that includes the storage space of the cache modules. In the example shown in the cache controller includes two interfaces labeled DDR3 CH and DDR3 CH . Two dual in line memory modules RDIMMs can be connected to the cache controller for data and cache table storage. In certain variations one RDIMM can store 8 gigabytes 8 GB . Other sizes are possible such as 16 GB or 32 GB for example.

Referring again to at block the first cache command and the second cache command are loaded into a buffer memory of the first cache controller and the first cache command and the second cache command are loaded into a buffer memory of the second cache controller. In some examples the writing of the command buffers initiates the performance or action of a cache controller. At block the order of execution of the first and second cache commands is synchronized in the first and second cache controllers. The synchronizing of the order of execution of the first and second cache commands can include communicating an indication between the first and second cache controllers that the loading of the first and second cache commands is completed. The synchronizing can include communicating the size of the loaded cache commands between the first and second cache controllers. This information may be used by the two cache controllers to coordinate the setting of a pointer for the command buffer memories.

At block both the first and second cache commands are performed using both the first cache controller and the second cache controller. The contents of the data storage of the first cache memory and the second cache memory are substantially identical after the first and second cache commands are performed by both of the first and second cache controllers. This results in redundancy of the cache memory contents. If one cache memory fails or cache data is otherwise corrupted a duplicate exists. The non failing cache can operate independently and cached data is preserved without loss of data.

In the example shown in each cache controller includes a cache memory interface labelled DDR3 I F for DDR3 DRAM DIMM Interface . As shown in the example of a cache controller may have multiple interfaces to cache memories or cache modules. The cache controller also includes an inter cache controller communication link . The inter cache controller communication link may be a serial link that provides bidirectional communication with the other cache controller. The cache controller includes two peripheral interfaces. A first peripheral interface is used to communicate with the first host device and a second peripheral interface is used to communicate with the second host device . In some variations the peripheral interfaces are PCIe interfaces. A cache controller and a routing network of can reside on a switch card and hosts running on a management processor communicate with the cache controller via the routing network. The first peripheral interface PCIe can be connected to a first host device via a PCIe interface local to the cache controller and the second peripheral device PCIe can be connected to the second host device via a PCIe interface remote from the cache controller .

The cache controller can include logic circuitry . In some variations the logic circuitry includes hardware circuits that implement a state machine. The logic circuitry may be reconfigurable or non reconfigurable. The logic circuitry controls operation of the cache controller . For instance the logic circuitry may load a cache command from the cache command memory of the first host device and load a cache command from a cache command memory of the second cache controller. The logic circuitry may trigger the pulling and execution of a cache command when an indication is received that a cache command is loaded into at least one of the cache command memories

Each host e.g. left L host and right R host posts cache commands to its own command ring buffer within memory of the host device. As a host posts cache commands to the command ring buffer it updates a tail pointer of the ring to a register within the cache controller memory space. Each cache controller can include a first in first out memory buffer FIFO to store cache commands loaded or pulled from the first and second host devices. The cache controller pulls cache commands from the command ring buffers into its on board FIFOs. This can be done as memory reads MEMRD to the host memory with the length of the MEMRD command set to the PCIe maximum transmission unit MTU or the number of commands on the ring preferably whichever is less. After each command pull the cache controller waits for the other cache controller to perform the same command pull. The logic circuitry can be configured to reorder cache commands received from the hosts and stored in the FIFO of the cache controller to match an execution order of commands of the FIFO of the other cache controller. This ordering ensures that the same sequence of cache command is performed by the cache controllers for commands from both of the left and right hosts.

The cache controller may include a first content addressable memory CAM internal to the cache controller . The cache controller may include a second CAM including associative memory configured to describe the contents of the cache memory. The first CAM is used to store one or more queues containing cache commands. These queues can be viewed as Virtual Queues VQs that include a sequence of cache commands for a specified cache entry index for the cache memory. After re ordering in a FIFO cache commands are placed within the VQs based on the command cache key. A cache key is used to locate an entry in the cache and is included in field of a command word to indicate the cache key used in the cache operation. If the command is a Cache Query command the Cache Key field includes the number of entries returned by the Cache Query command and an index of the last returned valid cache entry. A cache key may be assigned its own queue and the queues may be serviced in a round robin fashion. After the cache keys are assigned to queues the cache command keys of the received commands are matched to the keys assigned to the queues. If a match is found for the key of the received command the key is placed in the matching queue. If no match is found for a key of a received command a new queue is created if space allows. When all commands of a queue are completed the queue is deleted. The first CAM can be relatively small e.g. large enough to hold 256 command entries or 64 queues . The second CAM can be relatively large e.g. 4 million entries and can be used to store metadata for the cache memory contents.

After a cache command operation is fully completed and data is written to either DRAM or host memory the cache controller may write a status word to the host memory indicating the status of the completed command an error status can be reported at this stage to indicate a failure during command execution . Once each host receives the status word from both cache controllers it can free the data buffer memory within its memory space. Although only one cache controller returns cache read data to the host for cache read command both cache controllers should return the same completion status indication to the host. It is an error condition when only one cache controller returns completion status or if the returned completion statuses are not the same.

Each host posts cache commands to the companion cache controller. At the Left Host updates a tail pointer TL PR update of the command ring buffer to a register within the right cache controller memory space R FPGA RO FIFO and Right Host updates a tail pointer of the command ring buffer to a register within the left cache controller memory space L FPGA RO FIFO . At the size of the cache commands is communicated between the left and right cache controllers and matched by the cache controllers.

At each cache controller sends a command pull request to each host and cache commands are pulled from each host device buffer memory by each cache controller. At an indication the completion of loading of the first and second cache commands is communicated between the left and right cache controllers. At the cache controller transfers the cache commands into virtual queues. The cache commands are shown queued by the left cache controller first and right cache controller second.

The cache commands include cache read CR and cache write CW commands. shows read data buffers for the host devices respectively. For each cache read operation 4 kilobytes 4 KB of data is read from the DRAM and returned to the host memory. The block 4 KB is only one option for the block read size and other block sizes are possible. In some examples the cache controller includes direct memory access engine DMA . The block of read data is returned to the host memory from the DRAM by the DMA engine. A CR command includes one or more bits to indicate which cache controller will return the read data. If there are two cache controllers designated as left and right this may include one L R bit to designate left or right. This prevents both cache controllers from writing the same data to the same read buffer in host memory. Preventing the redundant write preserves the bandwidth at the PCIe and DRAM interfaces.

Returning to write data buffers are shown for host devices respectively. Cache write data can be pre fetched from the write data buffer . In certain variations the pre fetching is performed by the DMA . Pre fetching ensures no waiting at the DRAM interface for a PCIe MEMRD operation. The write data is written to the DRAM once the DRAM address is obtained from second CAM .

A cache command can include a dirty bit labeled DRTY in . A cache write command sets the state of the dirty bit in the cache metadata to the state of the DRTY bit in the command. The DRTY bit can be used to properly implement cache access synchronization by the multiple hosts and redundant cache controllers. This concept can be illustrated by the following scenario. Assume the Left Host has just completed a read from the flash memory as a result of a cache miss. Also assume the Right Host has received a write request corresponding to at least a portion of the same data as the Right Host from an application program and the Right Host determines to write the data to cache memory. The Right Host writes the data to cache memory with DRTY bit active e.g. set to one in the cache command. The Left Host may try to retain a copy of the data from flash memory by writing the data to cache. It will attempt a cache write with the DRTY bit set inactive e.g. set to 0 . If allowed to go through the cache write operation from the Left Host would cause the data from the Right Host to be lost. Consequently the cache controller will prevent the cache write by the Left Host. The cache controller may return a completion status that indicates the entry was found but not written.

The cache command can include a Cache Write Mask filed labeled CW MSK in that can comprise 8 bits and is used for disabling writes to blocks of 512 bytes when data is written to cache memory during execution of the Cache Write command.

The cache command can include a sequence field labeled SEQ that can comprise 17 bits and is used as a pointer to the original command. Software can use the field as an index into the cache command ring buffer. The Eviction Candidate bit field labeled EC indicates a cache entry has been tagged for eviction. The Eviction Candidate bit can only be set to an active state or 1 by a cache read modify command described below . If a cache write command is executed by a cache controller the Eviction Candidate bit is set to an inactive state or 0. The Cache Key or Cache Query Index and Entry Count field can comprise 96 bits. As used by all commands except cache query command the Cache Key filed indicates the Cache Key used in the cache operation. For the Cache Query command this field indicates the starting entry index and maximum number of valid cache entries to be returned by the cache controller to the host.

The Host Buffer Address field can comprise 64 bits and indicates the physical address of the Host Data buffer in the host memory. It can be used as the address of the data for the cache block transfers to and from the host memory. The Host Status Address can comprise 32 bits and is used as an offset for the status information once a cache command is completed. The Host Status Address can be added to the Host Buff Address to form a 64 bit physical address within the host memory. The Cache Key Aging Alias can comprise 32 bits and is used in an algorithm to determine cache eviction.

Steps taken by a cache controller are described below for cache commands including a cache read a cache read modify cache write cache modify cache evict and cache query. For a Cache Read CR command if the there is a hit on the data in cache a match in a cache tag the cache controller will perform a search of the corresponding cache metadata in CAM. The address of the data is returned by the CAM response and the data is then read from cache memory DRAM . The data may be read as a block of data by a DMA bock transfer to the host memory. The cache controller returns the completion status of the operation to the host memory. If there is a cache miss the cache controller returns the completion status with an indication of the miss to the host memory.

The Cache Read Modify CRM command is similar to the Cache Read command except that it will set the Eviction Candidate bit in the cache metadata. The host originating the command does not have to be the owner of the read cache entry to issue this command i.e. either host can issue this command . This command is the only command which can set the EC bit to an active state e.g. set the EC bit to a 1 .

For a cache write CW command the cache controller performs a pre fetch of a block of write data e.g. 4 KB from the host memory. The cache controller also looks for a hit on the key in second CAM . The current state of the DRTY bit within the cache metadata is compared with the state of DRTY bit within the cache command. If the DRTY bit is set to 1 in the cache metadata and set to 0 in the cache command the cache controller will return a Failed status. Otherwise the DRTY bit in the cache metadata is set to the same state as the DRTY bit in the cache command and the pre fetched block of data is written to the DRAM at the address obtained by CAM search command.

In the case of a cache miss the cache controller inserts the cache key in the cache metadata and sets the DRTY bit and the ownership bit of the cache line in the cache metadata. The DRAM address of the data is obtained as part of the CAM response and the cache controller writes the pre fetched block of data to the DRAM at the address given by CAM. The cache controller then sends the command completion status to the host memory to indicate buffer can be freed. The command completion status can be sent using the cache controller DMA.

For a Cache Modify CM command the cache controller looks for a hit on the data in cache. If there is a cache hit the cache controller performs a search of the cache metadata in CAM. If the Eviction Candidate bit is set then the DRTY bit of the metadata for the cache entry is cleared otherwise the cache controller returns a Failed status. In response to the cache hit the cache controller returns a cache completion status of hit to the host memory. In the event of a cache miss the cache controller sends a cache completion status of miss to the host memory. The command completion status can be sent using the cache controller DMA.

For a Cache Evict CE command the cache controller looks for a hit on the data in cache. If there is a cache hit the cache controller performs a search of the cache metadata in CAM. If the host issuing the CE command is the owner of the cache entry and the Eviction Candidate bit is set for the entry the cache controller deletes the cache entry and the cache key. This is done regardless of the state of the DRTY bit in the metadata. A programming option can be provided to disable the checking of the ownership bit before deleting the cache entry. In response to the cache hit the cache controller returns a cache completion status of hit and success or fail to the host memory. In the event of a cache miss the cache controller sends a cache completion status of miss to the host memory.

The Cache Query CQ command instructs the CAM to return N valid cache entries where N is a positive integer. The cache entries start from the specified cache entry index in the command. If there are no more valid entries in the cache before N is reached CAM will stop returning entries once the last entry is reached. The Cache Key field in the CQ command is used to specify the number of valid cache entries to be returned N as well as the starting index of the last valid entry. The Host Buffer Address field of the command is used to specify where in host memory to place the returned cache entries. The L R bit of the command indicates whether the Left of Right cache controller is to scan the cache and return the valid entries. The cache controller returns a completion status that indicates how many cache entries were returned and whether all of the entries have been scanned. The completion status also indicates the index of the last valid entry returned.

The cache commands described have referred to returning a completion status word. shows an example of a format for a completion status word. The Completion Status Field labeled Cmpl Status in indicates the completion status of the cache command. The completion status field may include one or more the following a Miss bit to indicate that the cache command resulted in a cache miss an Error bit to indicate that execution of the command resulted in an error and a Fail bit the cache command was not successful. A command failure has different meaning depending on the cache command. For instance a Cache Evict command may fail due to the EC bit being 0. The completion status field may include a separate Insert Fail bit to indicate the failure of a Cache Write command to insert a cache entry due to the cache being full in the case of a cache miss. The completion status field may also include one or more cyclic redundancy code CRC bits to indicate that the data read as result of a CR or CRM command had a CRC error. The cache controller may include CRC generating circuitry and the CRC error may indicate that the CRC generated while reading the DRAM did not match the CRC stored in the cache metadata. The Completion Status Field may include a Q End bit to indicate when the cache entries returned for a Cache Query command are the last valid entries in the cache. This bit is only applicable to the Cache Query command

The completion status word may include additional fields. In some examples the completion status word includes a Furthest Cmpl Ptr field that contains a pointer to the furthest completed cache command. The completion status word may include a CMD field that contains the original cache command for which the returned completion status corresponds and may include a DRTY field of one bit to indicate the content of the dirty bit within the cache metadata. The DRTY field is only valid if the cache operation resulted in a hit. This bit is undefined for a Cache Query command.

The completion status word may include an OWNER field. The OWNER field may comprise one bit to indicate the owner of the cache entry e.g. a cache line . A 0 in the field may indicate the left host as the owner and a 1 in the field may indicate the right host. The owner is the host whose Cache Write command resulted in an insert of the cache entry e.g. as a result of a Cache Write miss . The OWNER field is used to for evicting a cache entry and for clearing the DRTY field in the metadata. For instance a cache entry can only be evicted using the Cache Eviction command when the host issuing the Cache Eviction command is the owner of the cache entry. A DRTY bit in the metadata for a cache entry can only be set by a Cache Write command and the cache owner is determined at the time of the cache write. The DRTY bit can only be cleared by a Cache Modify Command sent from the owner of the cache entry when the Eviction Candidate bit is set. The OWNER field is undefined for the Cache Query command.

The completion status word may also include one or more of a L R field and an EC field. The L R field can comprise one bit to indicate whether the Left or Right cache controller is returning data for the Cache Read Cache Read Modify and Cache Query commands. For instance the Left cache controller will return data to the requesting host if the field is set to 0 and the Right cache controller will return data to the requesting host if the field is set to 1. The EC field may include a bit that indicates the value of the Eviction Candidate bit within the cache metadata when the cache command was executed. This bit is undefined for the CQ command.

The completion status word may include a sequence number SEQ field. This field contains the original sequence number in the corresponding cache command. The contents of the SEQ field can originally be set by software as a cache command index in the command ring. This field can be used by software to quickly find the corresponding entry in the command ring buffer.

The completion status word may also include a Cache Key field and a Key Aging field. Like the command word the Cache Key field contains the cache key within the cache metadata. If the cache command is a Cache Query command this field indicates the number of entries returned in response to the Cache Query command and may also indicate the index of the last returned valid entry. The Key Aging field is a value stored in the cache metadata to track cache entry aging.

Cache entry aging determines what data to displace from a cache module. Data to be displaced or evicted from a cache module can be determined according to a set of cache eviction rules or a cache eviction algorithm. The cache aging algorithm can be performed using driver software residing in a host device of . Information required by the driver software can be stored in hardware within the cache metadata.

According to some examples the cache aging algorithm implements an empty list and a least recently used LRU linked list using memory of the host device. The empty list includes pointers to free memory space where the data structures of the LRU list can be stored. The number of entries in the empty list may depend on the amount of cache memory allocated to the host device. If there is more than one host device e.g. two host devices as in the example shown in each host device is allocated a portion of the cache memory. The LRU list includes cache keys for the entries stored in the cache memory. The LRU list orders the cache entries according to the amount of activity or access involving the cache entries. Activity can be tracked according to a number of hits or the hit rate of an entry or according to latency between accesses to an entry.

At cache completion status information is returned from the cache controller to the host application. If a match for the cache key was found in the command queue a cache hit the cache completion status includes the value of the key aging alias in cache metadata. If a match for the cache key was not found in the command queue a cache miss the cache completion status includes the value of the key aging alias provided by the host application.

At the cache key is at the end of a least recently used LRU linked list when the cache completion status information includes the value of the key aging alias provided by the host application. The cache key is moved to the end of the LRU when the cache completion status information includes the value of the key aging alias in cache metadata.

As explained above the system can include two host devices and two cache controllers. Host Applications of both host devices can post cache commands. Both of the cache controllers pull commands from the command buffers of both the host devices. For instance a first cache command can be posted by a first host application e.g. of the first host device and a second cache command can be posted by a second host application e.g. of a second host device . The logic circuitry of the second cache controller loads or pulls the first cache command into its buffer memory performs the first cache command and returns cache completion status information to the first host application for the first cache command.

Cache commands are performed using both the first cache controller and the second cache controller. This provides redundancy in cache management of the system. Referring to the cache controllers include an inter cache controller communication link that provides bidirectional communication. The logic circuitry of a cache controller reorders cache commands in the buffer memory according to the cache key and to match the order of cache commands in the buffer memory of the other cache controller.

As explained previously once a host application receives the cache completion status information from both of the first and second cache controllers it can free the data buffer memory within its memory space. If the first cache command is a cache read CR command each of the first cache controller and the second cache controller perform the cache read and return cache completion status information to the first host application but only the first cache controller returns cache read data to the first host application. Both of the host applications implement and maintain a copy of the empty list and LRU linked list in memory of its host device.

According to some examples the driver software of a host device implements an eviction list as part of the cache aging algorithm. The eviction list includes the candidate cache entries for deletion from the cache memory. A specified number of entries at the head of the LRU list are moved to the eviction list when the number of entries in the empty list is less than or equal to a specified threshold number of entries. The candidate cache entries are sent to the cache controller with the cache evict CE command described previously and the cache controller evicts the cache entry and the cache key from the cache memory.

The host application of a host device determines when a cache entry will be evicted. The host application is only allowed to evict those entries it owns e.g. as indicated by an ownership bit . For instance a first host application may post a first cache command and a second host application of a different host device may post a second cache command that results in a cache entry being written into cache memory. The logic circuitry of the first cache controller loads the second cache command into its buffer memory and updates the cache metadata to indicate ownership of the cache entry by the second host application. The cache entry is evicted by the logic circuitry in response to a request to do so by the second host application when the cache metadata still indicates ownership of the cache entry by the second host application.

As part of the eviction of a cache entry the host application generates a cache command e.g. a CRM command that includes a request to read a cache entry from cache memory according to the LRU linked list of the host application e.g. the entry is at or near the head of the LRU linked list . The cache command indicates that the entry is a candidate for eviction e.g. by setting an Eviction Candidate bit in the command . When the read data is returned to the host application the host application writes the cache entry to flash memory that is separate from the cache memory .

The cache metadata includes an indication that the read cache entry has been modified from what is stored in flash memory e.g. a dirty bit . When the host application writes the cache entry to flash memory as part of an eviction of the cache entry the host application may generate a request for the cache controllers to clear the indication or the dirty bit. The cache controllers return cache completion status information to the requesting host application when the command is performed. The cache controllers may return a failed completion status to the requesting host application when the requesting host application is not the owner of the cache entry. The cache controllers may also return failed completion status to the requesting host application when the requesting host device is attempting to evict a cache entry but the indication of the cache entry as an eviction candidate has been cleared e.g. an EC bit is cleared because the other host application has modified the cache entry.

Cache entry aging and eviction has been described using a least recently used algorithm as an example. Other algorithms can be used to implement cache aging such as by tracking most recently used entries or by using an adaptive algorithm that changes the specified threshold number of entries used to initiate cache eviction.

Referring to it is determined whether it is time to perform cache capacity maintenance at block . Maintenance may be a continuous background operation a periodic background operation or on a need basis type of operation. Maintenance frequency can be a system setting user setting or dynamic setting based on current operating conditions of the system of . If maintenance is initiated yes branch of block then at block the cache controller determines whether the current data storage capacity of the cache modules or depending on which set of cache modules is associated with the given cache controller of the packet processing circuit and is at or above a pre set maximum capacity level. The pre set maximum capacity level is a certain value that is pre set by the system or user and represents the portion of the total data storage capacity of the cache modules that can be occupied while having a safe amount of available space in case for example an unexpectedly large write request is received. Examples of pre set maximum capacity level include but are not limited to 70 80 or some other value. In some embodiments the pre set maximum capacity level may be adjusted over time as more system usage information becomes available. Instead of expressing the pre set maximum capacity level as a percentage of the total data storage capacity for example it is understood that it can be expressed as a minimum available or reserved free space.

If the current cache capacity is below the pre set maximum capacity level no branch of block then the flow diagram returns to block . Otherwise the current cache capacity is too close to the pre set maximum capacity level and some of the stored data needs to be moved to the flash modules and evicted or erased from the cache modules or depending on which set of cache modules is associated with the given cache controller yes branch of block .

Next at block the cache controller determines what data to displace from the associated cache modules according to a set of cache eviction rules such as according to a Least Recently Used algorithm for example that evicts data that is the least used. Data that is pinned stays within the cache module based on a user specified directive.

Once the cache line s to empty are identified at block the cache controller sends data stored in those cache line s to the flash modules to for storage. Such data is erased emptied or evicted from those particular cache line s at block . The flow diagram then returns to block . Thus the cache capacity of cache modules or depending on which set of cache modules is associated with the given cache controller is maintained at or below the pre set maximum capacity level. It is understood that blocks and may occur simultaneously of each other.

Next at block a look up of the key in a cache table is performed to determine whether the requested data exists in the cache modules . If a matching cache key is found a cache hit or the yes branch of block the cache controllers access the data corresponding to the matching cache tag from the cache module and sends the retrieved data to the software driver and the data is sent to the originating I O circuit at block . The retrieved data is the requested data in the read request. The tracking metrics e.g. according to a least recently used algorithm is updated at block . If the retrieved data was previously written to the cache module in a previous write request and such data was not evicted from the cache module due to cache management operations see then such data is present in the cache module for later access such as the present read request. When data is present in the cache module there is no need to retrieve the data from the flash modules to . Data retrieval from a DRAM cache is significantly faster than from flash based memory upwards of a thousand times faster using cache than flash.

If no matching cache key is found a cache miss or the no branch of block the requested data is not present in the cache modules and is retrieved from the flash modules. At block the cache controllers initiate retrieval of the requested data from the appropriate flash modules. Both cache controllers load the cache read command posted by the software driver of host. Both cache controllers perform the command but only one cache controller may return the cache read data to the host.

Next at block a system setting or user specified setting is checked to see whether the requested data retrieved from one or more of the flash modules to should be copied to the cache modules. If the system is set to not copy the data to cache modules no branch of block then the flow diagram proceeds to block . Otherwise the retrieved data is copied to the cache modules yes branch of block and block . The retrieved data is also sent to the I O circuit one of to that made the read request at block . The associated tracking metrics e.g. for a least recently used algorithm are also updated at block .

At block the software driver of a host receives a write request originating from one of the I O circuits to and the request is then passed onto the cache controllers included in the packet processing circuits assuming normal operational state of the system . The data request includes the data to be written as well as a particular memory address location of the flash modules at which the data is to be written.

At block the software driver determines whether the data associated with the write request is exceptional. While the default rule is to store all data associated with write requests to the cache modules packet processing circuits and then at some later point in time copy data from the cache modules to the flash modules to one or more exceptions to the default rule may be implemented. One or more exception criteria may be a system setting or user specified setting. For example the exception may comprise there being no exception to the default rule. As another example data exceeding a certain size e.g. data that if written to the cache modules may exceed the cache capacity or likely to exceed the pre set maximum capacity level may warrant storing directly in the flash modules without first storing in the cache modules. As still another example the write request or the data associated with the write request itself may specify that the data will be rarely accessed e.g. is archival data or has a certain characteristic that warrants being stored directly in the flash modules to without first being stored in the cache modules .

If the data associated with the write request is determined to be exceptional yes branch of block then such data is sent to the flash modules for writing to at block . Otherwise the data associated with the write request is not exceptional no branch of block and operations are performed to write to the cache modules . At block the cache table is checked for a key of the cache data containing the same flash memory address location as provided in the write request.

If a matching cache key is found yes branch of block this means that an older version of the data associated with the write request or some data in general is currently stored in the cache line s now intended for the data associated with the write request. The cache controllers facilitate overwriting the existing data at these cache line s with the data associated with the write request at block . Then the flow diagram proceeds to block . If no matching cache key is found no branch of block then the cache controllers facilitate writing the data associated with the write request to empty available cache line s in the cache modules at block .

Next at block the data associated with the write request is additionally copied to empty available cache line s in the cache modules associated with the other FPGA packet processing circuit. A cache command associated with the write request is posted by the software driver of the host and loaded by both cache controllers. In this way the write request from the I O circuit is formed into two identical requests one going to the packet processing circuit and the other to the packet processing circuit . Then the cache controller in each of the packet processing circuits can store the data associated with the write request also referred to as write data in its respective cache modules. At block the associated tracking metrics are updated to reflect the addition of the data associated with the write request into certain cache line s of the cache modules.

Because flash modules to comprise the primary or permanent data storage medium for the storage system the data associated with the write request although already written to the cache modules see blocks and is eventually written to the flash modules to . Nevertheless cache management of the system is configured to intelligently perform data writes to the flash modules taking into account the characteristics of the flash modules. In order to prolong the usability of flash modules which are limited to a certain number of writes before degrading the cache management accumulates certain type of data corresponding to a plurality of write requests and then performs a single write of the accumulated data to flash modules rather than performing a write to flash modules for each write request. This means that if for example there are 25 write requests instead of writing to flash modules 25 times or once for each of the 25 write requests the data corresponding to these 25 write requests may be written at the same time and once e.g. a single write operation to the flash modules.

Accordingly the cache management acts as a middleman between the I O circuits to and flash modules to for every read and write requests from the I O circuits. For all read and write requests the presence of data associated with the read or write request in the cache modules is checked before the flash modules are involved. Based on the presence or absence of such data in the cache modules the cache management performs optimization operations to complete the data requests significantly faster than is possible with flash modules alone. The cache management also prolongs the useful lifespan of flash modules by minimizing the number of writes to flash modules without sacrificing completeness of data being stored in the flash modules. Data associated with write requests are written to cache memory prior to be written to flash modules unless the data fits an exception. Data associated with read requests that are retrieved from the flash modules may or may not be written to cache modules corresponding to both packet processing circuits depending upon a system or user setting . The cache management actively maintains the used storage capacity level of the cache modules at or below a pre set capacity level e.g. 70 80 etc. by evicting data stored in the cache modules that fit one or more eviction rules as needed. An example of an eviction rule comprises evicting data that has the least amount of access read or write and moving it to the flash modules.

Although the present invention has been described in connection with some embodiments it is not intended to be limited to the specific form set forth herein. One skilled in the art would recognize that various features of the described embodiments may be combined in accordance with the invention. Moreover it will be appreciated that various modifications and alterations may be made by those skilled in the art without departing from the scope of the invention.

The Abstract is provided to allow the reader to quickly ascertain the nature of the technical disclosure. It is submitted with the understanding that it will not be used to interpret or limit the scope or meaning of the claims. In addition in the foregoing Detailed Description it can be seen that various features are grouped together in a single embodiment for the purpose of streamlining the disclosure. This method of disclosure is not to be interpreted as reflecting an intention that the claimed embodiments require more features than are expressly recited in each claim. Rather as the following claims reflect inventive subject matter lies in less than all features of a single disclosed embodiment. Thus the following claims are hereby incorporated into the Detailed Description with each claim standing on its own as a separate embodiment.

