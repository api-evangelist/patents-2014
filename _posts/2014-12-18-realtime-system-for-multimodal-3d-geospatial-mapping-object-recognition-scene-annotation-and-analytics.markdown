---

title: Real-time system for multi-modal 3D geospatial mapping, object recognition, scene annotation and analytics
abstract: A multi-sensor, multi-modal data collection, analysis, recognition, and visualization platform can be embodied in a navigation capable vehicle. The platform provides an automated tool that can integrate multi-modal sensor data including two-dimensional image data, three-dimensional image data, and motion, location, or orientation data, and create a visual representation of the integrated sensor data, in a live operational environment. An illustrative platform architecture incorporates modular domain-specific business analytics “plug ins” to provide real-time annotation of the visual representation with domain-specific markups.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09488492&OS=09488492&RS=09488492
owner: SRI INTERNATIONAL
number: 09488492
owner_city: Menlo Park
owner_country: US
publication_date: 20141218
---
This application claims the benefit of and priority to U.S. Provisional Application Ser. No. 61 954 635 filed Mar. 18 2014 and U.S. Provisional Application Ser. No. 62 074 674 filed Nov. 4 2014 each of which is incorporated herein by this reference in its entirety.

This invention was made in part with government support under contract number N00014 12 C 0070 awarded by the Office of Naval Research Arlington. The Government has certain rights in this invention.

This disclosure relates to the technical fields of computer vision mobile robot navigation and geospatial mapping and analysis. In computer vision mathematical techniques are used to detect the presence of and recognize various elements of the visual scenes that are depicted in digital images. Localized portions of an image on which specific types of computations are done to produce visual features may be used to analyze and classify the image. Low level and mid level features such as interest points and edges edge distributions color distributions shapes and shape distributions may be computed from an image and used to detect for example people objects and landmarks that are depicted in the image. Machine learning algorithms are often used for image recognition.

In robot navigation technology cameras and other sensors are used to determine the robot s location and orientation with respect to its surrounding real world environment i.e. the robot s frame of reference . Computer vision techniques and mathematical computations are performed to interpret digital images of the environment within the robot s frame of reference generate a mathematical representation of the environment and generate a mapping of objects in the real world to the mathematical representation of the environment e.g. a map . The robot uses the map to navigate about its environment. In order to navigate the robot performs mathematical computations to develop a navigational path to a goal location.

Geospatial technology relates to the acquisition analysis and presentation of geographical and or geospatial data such as Global Positioning System GPS data and geographic information system GIS data.

While the concepts of the present disclosure are susceptible to various modifications and alternative forms specific embodiments thereof are shown by way of example in the drawings and are described in detail below. It should be understood that there is no intent to limit the concepts of the present disclosure to the particular forms disclosed. On the contrary the intent is to cover all modifications equivalents and alternatives consistent with the present disclosure and the appended claims.

Mobile robots vehicles and other mobile computing devices can be equipped with a number of different types of live sensors such as cameras LIDAR and GPS which can collect a range of different types of information about the real world physical environment surrounding the device in real time. The technology disclosed herein can in real time integrate the diverse sensor data produced by these and or other multiple different types of live sensors including three dimensional data in order to provide real time analysis and mapping of geospatial areas using an automated tool. The disclosed technology can analyze and annotate the integrated sensor data using domain specific business logic and or more generic analytics as needed. For example the disclosed architecture enables the use of plug in data analytics modules designed for specific applications such as surveillance situation awareness facility and equipment monitoring asset monitoring disaster relief search and rescue and or other applications. Integrating live multi sensor multi modal data including three dimensional 3D data in a real time system as disclosed herein can among other things enable low latency map updates and analytics to be delivered to requesting users and or services in real time or interactively. Additionally the integration of data produced by multi modal sensors such as cameras LIDAR and GPS can provide improved map accuracy and enable a broad range of analysis and information products.

Referring now to an embodiment of a computing system implementing an automated tool for the collection analysis recognition and visualization of multi sensor multi modal data including 3D data is shown. In the computing system is shown in the context of an environment that may be created during the operation of the computing system e.g. a physical and or virtual execution or runtime environment . The computing system and each of the platforms subsystems modules and other components of the computing system is embodied as a number of machine readable instructions data structures and or other components or devices which may be implemented as computer hardware firmware software or a combination thereof.

The illustrative computing system includes a multi sensor data collection analysis recognition and visualization platform platform . The illustrative platform may be embodied as a mobile computing device including a navigation capable device such as a mobile robot an unmanned or unpiloted ground and or aerial vehicle e.g. a drone a driverless vehicle etc. a motorized vehicle or even in some cases as a personal mobile electronic device e.g. smart glasses smart appliance etc. . In other embodiments the platform is embodied as another type of computing device such as a component of a security or surveillance system that is mounted to a wall or fixture or as a combination of multiple devices.

In operation the platform receives sensor data streams from a number of sensors . The sensors are of N different sensor types where N is a positive integer . For example the sensor may be embodied as a two dimensional 2D image sensor e.g. a 2D still or video camera the sensor may be embodied as a 3D image sensor e.g. LIDAR and the sensor may be embodied as an inertial sensor such as an inertial measurement unit IMU including e.g. an accelerometer and a gyroscope or another type of sensor capable of producing motion location and or orientation data. In the illustrative embodiments the sensor is a low end IMU such as a Micro Electro Mechanical Systems MEMS MEI that is readily commercially available and thus suitable for use in connection with a wide range of consumer grade applications. In other embodiments the sensors may include a higher end IMU and GPS based integrated navigation system.

The illustrative platform is embodied as a software pipeline including a set of interconnected modules including a sensor data capture and synchronization module a multi modal navigation and geo spatial mapping subsystem a multi modal geo spatial data integration module a multi modal live analytics subsystem a domain specific data correlation module a real time multi modal compression subsystem and a live on platform visualization module . In other embodiments any of the modules subsystems may be embodied as hardware firmware software or a combination thereof.

The modules subsystems interface with the live sensors align and analyze the sensor data correlate the sensor data with domain specific business data e.g. domain specific data maps stored at least temporarily in a domain specific data map cache compress and transmit the resulting information e.g. compressed multi modal data which may include annotations such as geo tags and or markups as described below and provide visualization and interactive user interfaces on the platform and or other computing devices as described in more detail below. Unlike other approaches embodiments of the platform can operate in real time and can combine multi sensor multi modal navigation mapping analytics compression and visualization functionality all in a single architecture.

Referring further to the illustrative sensor data capture and synchronization module operates as front end software that communicates with the sensors synchronizes the sensor data streams and timestamps the sensor data such that the data from all of the sensors can be utilized concurrently e.g. simultaneously . The sensor data capture and synchronization module outputs synchronized sensor data for use by the multi modal navigation and geospatial mapping subsystem . The illustrative multi modal navigation and geospatial mapping subsystem uses the synchronized sensor data to make temporal spatial and geospatial associations across the multi modal sensor data in order to estimate both a navigation path of the platform and also to build up map data e.g. a platform internal representation of the real world environment surrounding the platform with respect to a frame of reference of the platform . The multi modal navigation and geospatial mapping subsystem outputs multi modal navigation and mapping data for use by the multi modal geospatial data integration module . The multi modal geospatial data integration module executes data integration and data fusion algorithms on the multi modal navigation path and mapping data to produce integrated multi modal geospatial data e.g. geo spatially organized 3D maps. The integrated data is used by the multi modal live analytics subsystem and the real time multi modal compression subsystem . The multi modal live analytics subsystem applies different object and or scene detection and recognition algorithms to the geo spatially integrated data and outputs annotations of the integrated data e.g. live analytics geo tags . Whereas the analytics performed by the multi modal live analytics subsystem may utilize generic algorithms in some embodiments e.g. standard vehicle or face detection algorithms the domain specific data correlation module applies domain specific business logic for a selected e.g. specialized field of interest and outputs domain specific markups for the integrated data for use by the compression subsystem . The annotations produced by the live analytics subsystem and or the domain specific data correlation module e.g. the geo tags and the domain specific markups may take the form of for example graphical overlays such as color coded highlighting graphical symbols markings such as lines circles and arrows etc. In some embodiments these annotations may be interactive such that if activated by a user e.g. by a tap on a touchscreen display showing a visualization of the compressed multi modal data they may display additional information provide an expanded user input area into which a user may input notes or other information or user activation of the interactive annotation may launch or otherwise provide access to another application or service such as a messaging service.

The architecture of the platform allows for each or either of the multi modal live analytics subsystem and the domain specific data correlation module to be implemented as plug in modules in that the live analytics subsystem and or the data correlation module can be selected based on the needs or the design of a particular implementation of the platform . The real time multi modal compression subsystem utilizes compression algorithms that develop a 3D representation of the integrated data that is suitable for compression and live streaming and outputs the compressed multi modal data e.g. annotated 3D 2D data streams for use by the live on platform visualization module and or a multi platform data aggregation subsystem which is illustratively located off of the platform . The live on platform visualization module prepares a domain specific visualization of the collected and integrated map data and analytics results e.g. geo tags markups which can be accessed directly on the platform in the field or at another computing device e.g. a computing device of the multi platform data aggregation subsystem . The multi platform data aggregation subsystem receives the compressed multi modal data from the platform and in some embodiments other platforms as well. The data aggregation subsystem fuses the annotated multi modal data received from all platforms into a centralized aggregated map database which may be stored in a data storage device on the platform or on one or more other devices e.g. data storage device and or shown in . The data aggregation subsystem applies business logic to the aggregated multi modal data to interpret the aggregated data alone or in combination with other business specific data that is accessible to the data aggregation subsystem . The crowd sourcing app may be embodied as for example a business specific mobile application for modern smartphones or other personal electronic devices. Data made available by the crowd sourcing app can be used to augment the business analytics output and 3D map information e.g. integrated map data geo tags markups in real time. For instance the crowd sourcing app may supply domain specific geo tags that have been obtained or derived from the public crowd sourcing of information over the Internet.

As shown in described below the platform may communicate with one or more other computing devices such as user computing devices e.g. desktop laptop or tablet computers and enterprise or command center computing devices e.g. servers networks of servers etc. . For example the platform may transmit data collected from the sensors and processed by the platform to the multi platform data aggregation subsystem which may be embodied in a data aggregation computing device and the crowd sourcing application may be installed on a user computing device .

Referring now in more detail to the sensor data capture and synchronization module of the illustrative sensor data capture and synchronization module reads the data output by the sensors continuously while the platform is in operation. The data capture and synchronization module tightly integrates and synchronizes the data before publishing the synchronized sensor data for use by the navigation and mapping subsystem and the data integration module . Illustratively the data capture and synchronization module represents each type of sensor e.g. LIDAR inertial measurement unit IMU camera in software by an abstract class that can be instantiated using different drivers and which can be configured differently for different types of sensor devices. The data capture and synchronization module reads the data asynchronously or synchronously from each sensor device timestamps the data and publishes the data for use by the downstream modules e.g. modules etc. . The processes performed by the data capture and synchronization module helps ensure low latency of the pipeline such that the delivery of geospatial analytics and mapping data occurs in real time or at least in interactive time.

Referring now to components of an embodiment of the multi modal navigation and geospatial mapping subsystem of are shown in more detail. Each of the components shown in may be embodied as software hardware firmware or a combination thereof. In the components of the navigation and geospatial mapping subsystem are shown in the context of an environment that may be created during the operation of the computing system e.g. a physical and or virtual execution or runtime environment . The illustrative navigation and mapping subsystem includes a temporal association module a spatial association module a geospatial association module and a multi modal localization and mapping module which includes a 6DOF pose estimation module described in more detail below with reference to . The components of the navigation and mapping subsystem operate to maintain accurate position and orientation information for each of the sensors and make temporal spatial and geospatial associations across the data obtained from the various different sensors . The operations performed by the components of the navigation and mapping subsystem allow the map representation to be created over time. The navigation and mapping subsystem is capable of exploiting various combinations of multi modal observations derived from the data to generate a navigation path and map for the platform . For example the navigation and mapping subsystem can utilize a combination of data obtained from an IMU and video data to estimate an initial navigation path for the platform and then use the relative 6 degrees of freedom 6DOF poses that are estimated by the 6DOF pose estimation module to generate a 3D map based on data obtained from e.g. a scanning LIDAR sensor. The 3D LIDAR features can then be further exploited to improve the navigation path and the map previously generated based on e.g. the IMU and video data.

Components of the navigation and mapping subsystem compute navigation estimation for the platform . For reliable integration of LIDAR and other multi modal sensor data accurate geospatial position and orientation estimates of the sensor system e.g. the combination of sensors are needed. Whereas other approaches rely on expensive high end IMU and GPS based integrated navigation systems to provide the requisite position and orientation estimates the disclosed navigation and mapping subsystem can operate equally as well using e.g. a MEMs IMU based system. As described below with reference to the navigation and mapping subsystem integrates other sensor measurements such as visual features obtained from 2D and or 3D imaging devices with the position and orientation information to enable robust 6DOF motion estimation.

Referring now in more detail to the multi modal geo spatial data integration module of components of the illustrative multi modal geo spatial data integration module integrate and fuse the multi modal navigation path and mapping data in a 3D geospatial map. The accurate position estimation of the different sensors with respect to real world coordinates and to the other sensors performed by the navigation and mapping subsystem combined with the accurate synchronization of the sensor data provided by the data capture and synchronization module allows the data integration module to integrate and fuse the data in a 3D geospatial map. For example a 3D map generated from LIDAR can be used to identify hyper spectral or RGB video features associated with the 3D surface generated from the map. Illustratively the data integration module fuses the sensor data by combining the output of the different data sources e.g. sensors into a single channel or layer of the integrated map. For example the output of different cameras with overlapping fields of view may be combined into a single RGB overlay or the LIDAR points may be given color attributes from the overlay of the RGB or Hyper Spectral imagery. The output of the data integration module is geospatially aligned and integrated multi modal data which can be accessed for immediate compression and visualization by the compression subsystem and visualization module and or analyzed by the analytics subsystem and data correlation module .

Referring now to components of an embodiment of the multi modal live analytics subsystem of are shown in more detail. Each of the components shown in may be embodied as software hardware firmware or a combination thereof. In the components of the multi modal live analytics subsystem are shown in the context of an environment that may be created during the operation of the computing system e.g. a physical and or virtual execution or runtime environment . The illustrative multi modal live analytics subsystem includes a change detection module an object scene recognition and classification subsystem and an anomaly detection module . The illustrative object scene recognition and classification subsystem is described in more detail below with reference to . The components of the live analytics subsystem compute live analytics on the 3D multi modal data to extract higher level information constructs that can be exploited by other modules and or services. The platform provides a plug and play framework to plug in analytics modules according to the needs of a particular application. The recognition and classification component of the live analytics subsystem applies feature and attribute extraction methods to 2D and or 3D multi modal data . Detection and classification tools of the recognition and classification component allow for identifying different objects of interest e.g. roads buildings poles wires trees foliage people animals fire hydrants transformers sign posts billboards bridges vehicles etc. .

A change detection module can compare the current multi modal map to a reference map or a previously generated map in order to identify areas of the map that have changed since an earlier round of data collection. An anomaly detection module can identify regions that do not fit the norm e.g. obstructed roadways or damaged buildings or tree limbs hanging over wires or poles down . The change detection module allows the platform to identify real changes e.g. changes that are not due to occlusion while reducing false alarms. In the illustrative system the change detection module exploits both 3D and 2D e.g. video hyperspectral information for identifying changes in the data over time. In performing 3D change detection the change detection module distinguishes differences in the data that are due to occlusions from differences in the data that are due to actual change. The system can be configured to actively or passively ensure that sufficient looks are maintained to avoid occlusions. With the occlusions omitted or ignored the basic 3D and image changes can then be fed into a higher level change evaluation module. In some embodiments multi modal training data is used to learn features of robust change and for suppression of false alarms. Such training is performed through e.g. supervised training based on ground truthing pre collected data or by online observation of user in the loop nominations.

In some embodiments the general purpose 3D analytics provided by the live analytics subsystem are by the domain specific data correlation module further expanded within the plug and play framework of the platform to bring in specific business logic for one or more specialized fields of interest. The plug and play aspect of the platform provides a facility for accessing the business data and exploitation tools that is separate from and maintained outside the main sensor data streaming pipeline but which can utilize the sensor data. For example a road survey service can introduce specific logic about materials used in a road construction process in order to improve the detection of faults. Another example is a first responder team that might introduce a wide area flood plain analysis module and building location and road network map to determine real time evacuation routes. A facility monitoring analysis module may detect fallen poles or foliage and tree branches touching wires etc. A power company may use specific knowledge about the network of wire connections to determine the effect of a broken wire detected by the platform .

The system provides a general purpose geospatial caching framework to bring in the reference data e.g. prior data business data that includes information relevant to the current geospatial context. The reference data is illustratively indexed spatially using e.g. an octree data structure allowing for fast queries of data within a given area of interest. The data is automatically loaded from disk upon query if it is not in memory and pushed to disk when no longer relevant. This type of general purpose data caching framework allows large scale storage and utilization of data and can accelerate the integration of new data types and new business logic.

Referring now to components of an embodiment of the real time multi modal compression subsystem of are shown in more detail. Each of the components shown in may be embodied as software hardware firmware or a combination thereof. In the components of the multi modal compression subsystem are shown in the context of an environment that may be created during the operation of the computing system e.g. a physical and or virtual execution or runtime environment . The illustrative real time multi modal compression subsystem includes data stores for reference data updates geo tags and domain specific markups as well as a multi level compression module . The reference data includes updated geospatial data generated by the data integration module . The geo tags are obtained as a result of analytics performed by the live analytics subsystem . The domain specific markups are obtained as a result of the correlations performed by the domain specific data correlation module . In other words the illustrative data stores storing reference data geo tags and markups contain only the relevant data updates and annotations prepared based on the most recent set of sensor data rather than the entire set of integrated multi modal geospatial data . Thus the multi level compression module operates using only the updated changed set of reference data and annotations e.g. geo tags markups as inputs to its compression algorithms.

The compression subsystem enables efficient compression and robust transmission of the data between the air ground mobile computing platform and other devices such as a command center computing system. The resulting data compression helps ensure real time or at least interactive time delivery of data and analytics results to requesting users and or services. As shown in some embodiments of the compression subsystem utilize an octree forest based representation of map data which can transmit the changes e.g. additions to the 3D collection of data geo tags and markups to a remote receiver such as the data aggregation subsystem as streaming elements. Other sources such as video HSI can exploit the 2D data layers to improve compression. The illustrative compression subsystem also provides for the handling of dropped data packets and corruption from faulty transmission in order to ensure resilient communication.

In some embodiments the compression module utilizes a sparsely occupied yet densely mapped 3D voxel grid supported by an octree forest data structure. A summary of this technology is provided below. Further details are provided in the aforementioned U.S. Provisional Application Ser. No. 62 074 674 filed Nov. 4 2014 which is entitled Streaming Point Cloud Visualization. Voxel discretization size can be specified to be within the error threshold of the used depth sensing device thus providing semi lossless representation or a preferred size. Points from the sensor are globally integrated into a world coordinate frame. Points then fall within the spatial region of prospective voxels causing those voxels to become occupied. A mirrored voxel grid is maintained between the visualization client e.g. a data aggregation platform and the server e.g. a robot and differential changes to this grid are compressed and transmitted to the client thus maintaining synchronized occupancy representations.

The world model can be understood as a collection of occupied cells within an infinitely extendable 3D regular grid of a specified resolution voxel size thus discretizing 3D space into a lattice of aligned addressable volumes. Grid cell occupancy is Boolean in nature either being occupied or not. If at least one integrated point falls within a cell volume that cell is considered occupied and any subsequent points falling within the same volume are considered redundant and ignored. An occupied cell is represented as a voxel and stored within the data representation. In computer based modeling or graphic simulation a voxel may refer to each element of an array of elements of volume that constitutes a notional three dimensional space e.g. each element of an array of discrete elements into which a representation of a three dimensional object is divided.

In some embodiments the data representation used to hold the world voxel occupancy information is an octree forest F that is defined to be a 3D array of forest cells C each containing an octree O arranged in a regular grid. In these embodiments all octrees are of equal size and equal depth where all leaves reside at the maximum depth of the tree. Existing octree leaves represent occupied voxels thus the octree is used as memory efficient representation of a sparsely filled dense 3D grid.

In the example of compression of a data representation for efficient transmission is achieved by the multi level compression module through multiple levels shown by the schematics . The first level of compression e.g. schematic is achieved by exploiting temporal and spatial redundancy within the accumulating integrated point cloud. Points are first quantized into configurably representative voxel occupancy this has the effect of removing redundant points from consideration. The second level of compression e.g. schematic is achieved during the encoding of the new voxels produced by change detection. The new voxels are added as leaves to a new octree e.g. schematic whose spatial bounds match the forest octree they inhabit. The adding of leaves to a new octree is considered a change. Since the change tree bounds match the forest tree bounds both of the trees will match structurally as the exact branch nodes that exist in the forest octree e.g. to support the new leaves voxels will also exist in the change tree e.g. to support those same leaves. Since the change is a true octree it can be encoded into a byte stream e.g. schematic by an octree structure serialization method. In this method a breadth first traversal of all branch nodes is performed and at each branch node visited an 8 bit child occupancy mask is generated where each bit corresponds to the occupancy of the corresponding octant. The 8 bit child occupancy masks bytes are stored in order of generation and the resulting byte array represents the complete structure of the octree. The disclosed 3D analysis can also allow for transmission of video or hyper spectral data. For example 3D classifications can help identify salient features in the data to allow more bits to be allocated to these regions during compression. includes point cloud data representations which illustrate an occupancy based pruning example. Image shows point cloud reference data with 1.89 M M million points with 8 cm cm centimeter resolution sampling given 75 k k thousand points. Image illustrates a swatch of 316 k point cloud that with temporal pruning has 12 k points. Image illustrates a swath of 309 k point cloud that with pruning has 9.5 k points. Image illustrates each swath colored by red green and blue respectively showing overlap and additions within the octree structure up to 8 cm. Another approach to data compression for streaming point cloud data for visualization which utilizes an XOR representation to octree changes is described in J. Kammerl IEEE International Conference on Robotics and Automation ICRA 2012.

Referring now in more detail to the live on platform visualization module of the illustrative live on platform visualization module enables real time visualization of the 3D map data route and analytics directly on a display device on the platform . The real time visualization of the data collection on the platform allows a platform operator or other user to quickly verify the quality and coverage of the sensor data and then re collect data if necessary. The illustrative visualization module thereby enables users to turn on and off layers of the real time data and to manually annotate the map with custom geo tagged information if needed in addition or as an alternative to the automated tagging provided by the analytics subsystem and the correlation module . The visualization module also allows a user to leave geo tags in the form of messages to other users. For instance rather than simply highlighting a portion of the visualization with a tag a user can pin a note to the area of interest on the visualization alternatively or in addition to other forms of geo tags.

Referring now to components of an embodiment of the multi platform data aggregation subsystem of are shown in more detail. Each of the components shown in may be embodied as software hardware firmware or a combination thereof. In the components of the multi platform data aggregation subsystem are shown in the context of an environment that may be created during the operation of the computing system e.g. a physical and or virtual execution or runtime environment . The illustrative multi platform data aggregation subsystem includes a live multi platform situational awareness module a multi platform data fusion module a domain specific data maps data store and an aggregated multi platform data store . The live multi platform situational awareness module allows users of the data aggregation subsystem to interact with the visualizations produced by the computing system in real time in some embodiments . In some implementations the data aggregation subsystem is embodied as a central command center that can receive multimodal data streams from multiple platforms and fuse the multi platform multi modal data streams into an aggregated map database e.g. by the multi platform data fusion module . The aggregated data can be shared across each of the platforms e.g. as updates. Similarly the aggregated data can be fused with additional business specific data and business logic can be applied to interpret and understand the integrated data by e.g. the domain specific data correlation module . Examples of such business logic include the planning of domain specific routing information or generating an appropriate response to a detected fault where the real time information helps modify subsequent business actions that are taken.

Increasingly crowd sourcing is being recognized as a valuable and powerful tool for gathering data. Crowd sourcing is supported in the real time system through a mobile crowd sourcing application that can directly send data points to the aggregated map database in the data aggregation subsystem . For example a by a public network a mobile user can mark a location on a map displayed on a display screen of his or her mobile device and with the crowd sourcing app select from a list in the mobile crowd sourcing app issues or observations that he or she has made while viewing the data. Such a list can be developed based on the business logic of information relevant to the specific business process e.g. a user can mark a location with a pot hole on a road utility app or take a picture of a damaged home after a tornado for a disaster response app. The user selections may include the GPS coordinates of the marked location and a picture of the observation or other supporting information in some embodiments.

The computational resources of the illustrative data aggregation subsystem enable large scale 3D data visualization with layers of real time data and higher level metadata extracted either automatically or by a human user e.g. a data analyst . The data aggregation subsystem is configured with an application interface that permits extensive exploration of historical data including change detection between different collections and long duration temporal analysis.

The data aggregation subsystem can provide visualization of the multi modal data which may be annotated data with geo tags and or markups aggregated from multiple platforms to users at locations that are remote from the platform e.g. at a ground facility or mobile device used to control the operation of the platform . Alternatively or in addition aggregated multi modal data may be presented on the platform via the live on platform visualization module . In addition to the automated analytics provided by the live analytics subsystem and the correlation module users of the data aggregation subsystem can manually nominate information analytics to for example detect domain specific anomalies or changes or to detect specific objects of interest. The resulting information can in turn can be provided to the analytics modules e.g. analytics subsystem correlation module used online or offline for adapting algorithms for better performance. The user specified nominations can also be used as exemplars for changing the visualization presentation to accommodate user specific preferences.

Embodiments of the platform provide a point cloud streaming pipeline that enables an interactive visualization system that is capable of providing a live and low latency updated 3D representation of the immediate area surrounding the platform as well as areas the platform has already visited in one geometrically consistent model map. In some cases the platform provides remote platform operators with an improved ability to navigate and explore environments including hostile environments which is superior to that which can be achieved by the limited camera systems currently in use.

Some embodiments of the data aggregation subsystem utilize e.g. graphics processing unit GPU hardware accelerated OpenGL for rendering the visualization. The medium of presentation for the environment representative 3D model is a point cloud visualization where each point is derived from the centroids of all voxels in the model and possibly associated metadata . The specific technique for optimized OpenGL streaming buffer updates can be selected based on requirements of a particular implementation of the platform . In some implementations it may be desirable to limit OpenGL central processing unit CPU to GPU implicit synchronizations for better performance.

In some embodiments the visualization output includes a number of different colors since drawing individual points as single white pixels would be ineffective for visually differentiating different environmental structures from one another. The visualization technology utilized by the visualization module and or the data aggregation subsystem outputs data according to a color scheme and or provides the user with a toolbox of point coloring schemes and interactively adjustable parameters such as max colored range from virtual camera which may be selected e.g. toggled as needed for various specific spatial awareness tasks. The different available color coding modes include for example color gradient by height color gradient by distance to selected point color linear by distance to selected point color by distance to virtual camera color linear by distance to camera color by normal orientation color by detected planes and color by projected camera imagery. Whereas changing the actual color properties of points by adjusting a GPU resident vertex data would be very slow the disclosed visualization technology utilizes vertex and fragment shaders written in GLSL to allow toggling between color modes on the fly without ever touching the GPU resident data. In some cases all color generation is performed via a combination of vertex and shaders that run on the GPU. This method allows rapid toggling between colorization modes by just switching the active shaders. This method is extremely fast and is further optimized by utilizing the GLSL subroutine system.

Navigation within the point cloud is also provided by the disclosed visualization technology. In some embodiments to facilitate relative sensor platform to point cloud spatial understanding a virtual proxy model of the platform such as a Talon robot is rendered within the point cloud. The position and orientation of the virtual proxy model match those of a real world platform with respect to the point cloud. This method also allows the operator to switch between 3 base navigation modes an orbit camera mode which allows the operator to select any point within the entire point cloud and interactively yaw pitch and zoom the virtual camera about that point an FPS flight mode which allows the operator to fly the virtual camera as if it were an aircraft that can hover interactively adjusting speed and a follow camera mode which allows the operator to yaw pitch and zoom the virtual camera about the platform proxy utilizing a simulated elastic tether based on a critically damped spring model between the platform proxy and the virtual camera as the platform moves throughout the point cloud where the virtual camera is smoothly pulled along with it. The importance of point colorization and navigation is to aid the operator with comprehending the spatial distribution of the rendered points. Since the points are drawn on a 2D surface depth cues are provided to aid in the operator s understanding of the points relative placement within 3D space. This is accomplished through colorization by altering point color based on spatial parameters such as distance from ground distance from camera distance from platform planarity of a surface via normal colorization etc. This is also accomplished by parallax via intuitive movement of the virtual camera in respect to the point cloud. Examples of colorization modes and visualization features are shown in FIG. 17 of the aforementioned U.S. Provisional Application Ser. No. 61 954 635 filed Mar. 18 2014 entitled Real Time System for Multi Modal 3D Geo Spatial Mapping Object Recognition Scene Annotation And Analytics. 

Referring now to components of an embodiment of the 6DOF pose estimation module of are shown in more detail. Each of the components shown in may be embodied as software hardware firmware or a combination thereof. In the components of the 6DOF pose estimation module are shown in the context of an environment that may be created during the operation of the computing system e.g. a physical and or virtual execution or runtime environment . In the illustrative 6DOF pose estimation module an IMU centric framework for doing accurate 6DOF pose estimation even with lower grade IMUs such as MEMS IMUs includes a motion location orientation sensor e.g. IMU mechanization processing module a visual feature detection and matching module a relative pose estimation module a feature track measurement module and an error state extended Kalman filter . The components of the 6DOF pose estimation module utilize both motion orientation location data and visual features extracted from 2D and or 3D imagery to generate 6DOF pose estimates. Motion location orientation data stream s are input to the IMU mechanization processing module . The IMU mechanization processing module generates a motion model . The visual features detection and matching module receives 2D and or 3D image sensor data streams and generates feature and match data . The relative pose estimation module utilizes the motion model and the feature and match data to generate feature tracks . The feature tracks are used by the feature track measurement module to generate local relative track measurements which are used by the error state extended Kalman filter to generate 6DOF pose estimates . The IMU mechanization processing module utilizes the 6DOF pose estimates to generate corrected navigation data e.g. an estimated navigation path for the platform which can be used by other modules of the platform as described herein.

In more detail the illustrative Kalman filter module utilizes an error state e.g. indirect form of the Kalman filter in order to avoid the need for dynamic modeling of the complex kinematics associated with each specific sensor . In the pose estimation module the motion model is derived from integrating the gyroscope and accelerometer data from the IMU as performed in the IMU mechanization processing module to form the building block for state prediction rather than explicitly modeling the platform dynamics and using the IMU on the measurement side. Since the IMU is able to follow very accurately the high frequency motion of the platform the indirect Kalman filter operates on the inertial system error propagation equations which evolve smoothly and are much more adequately represented as linear. An advantage of this approach is that the navigation solution can be transferred to another mobile platform carrying equivalent sensors without any changes to the sensor fusion algorithms.

As each new frame is received from data stream s visual features are extracted by the feature detection and matching module by use of e.g. a Harris corner feature detector. These features are matched by the feature detection and matching module to the previously received frame using e.g. normalized correlation of image patches around each such feature. Match success or failure is determined based on e.g. mutual agreement criterion. In some embodiments the 6DOF pose estimation module does not use any thresholding of correlation scores rather it decides that a match is successful if both features in the current and previous frame pick each other as their best highest scoring matches. A feature track table that accommodates several hundred frames is used to record the feature locations and maintain the track information and track lengths. After the initial matching step there are usually a great number of outliers due to false matches. In the pose estimation module all features that are tracked for at least three frames e.g. feature and match data are input to a robust two point relative pose estimation algorithm that enforces geometric constraints across three frames. Following this process inlier feature tracks e.g. feature tracks are determined based on e.g. the comparison of trifocal Sampson error for each feature against a predetermined threshold. Those tracks that fail this test are terminated and reset as fresh features that were newly detected at the current frame as these features may become inliers in the future.

At every video frame of the data stream s the entire feature track history with inlier tracks from the current frame extending to the past frames in the sequence is made available to the Kalman filter in the error state extended Kalman filter module . A separate measurement equation is created for each feature that is tracked for more than three frames. After this step the measurement equations for all the tracks are stacked to form the final set of measurement model equations which are a function of both the previous state and the current predicted state so they are relative local in nature as opposed to more typical global measurements which are a lot more straightforward to treat. In order to properly handle such relative measurements in the extended Kalman filter in some embodiments a stochastic cloning framework is employed. One approach to handling relative measurements between the previous and current time instants is to express them in terms of motion estimates between the two states and take into account the cross correlation arising from such a formulation. Stochastic cloning provides the framework to process such relative measurements by augmenting the state vector with two copies of the state estimate one evolving and one stationary clone. The evolving clone is propagated by the process model similar to a conventional Kalman filter framework whereas the stationary clone is kept static and does not evolve. The relative measurement between the previous and current time instant is then expressed as a function of these two states and a Kalman filter update modified to incorporate the joint covariance of the two clone states is performed.

Similar to the video based features LIDAR based features can be integrated and tracked to provide measurements to the Kalman filter in the error state extended Kalman filter module . However in the case of scanning LIDAR systems the initial Kalman pose estimates e.g. 6DOF pose estimates are used to locally integrate the LIDAR data based point to enable feature extractions. Salient features such a 3D spin images can be utilized for tracking these features.

In some embodiments an addition to the local Kalman filtering process simultaneous localization and mapping SLAM methods can be used to improve precision while developing a navigation map. Embodiments of the disclosed system utilize visual features such as HOG and SIFT descriptors for establishing longer range correspondences. Similarly with LIDAR data ICP iterative closest point methods can be used to establish longer range matches across swaths of locally integrated data. The longer range methods can also exploit the GPS data DEM digital elevation model data and other sensor sources in the joint inference process.

Referring now to components of an embodiment of the object scene recognition and classification subsystem of are shown in more detail. shows a functional architecture of a scalable object recognition system for detecting and identifying a large class of objects even in cases where some objects can only be identified using the context of the full scene. Each of the components shown in may be embodied as software hardware firmware or a combination thereof. In the components of the object scene recognition and classification subsystem are shown in the context of an environment that may be created during the operation of the computing system e.g. a physical and or virtual execution or runtime environment . As described below the illustrative object scene recognition and classification subsystem includes automated tools for feature and attribute extraction object detection and scene classification. The recognition and classification subsystem includes both online and offline functionality. Those modules that are typically implemented for online operational use e.g. in an interactive real time operational environment of the platform are denoted by the dashed box . The modules that are typically implemented for offline use e.g. in the background or as part of the process of developing maintaining the functionality of the online system are denoted by the dashed box .

The illustrative online functionality includes a large scale object and region extraction module a feature computation module a context free object identification module and a contextual object identification module . The illustrative offline functionality includes a visual words computation module an object classifier training module and a contextual relationship determination module . One or more of the components of the offline functionality may communicate with one or more of the components of the online functionality from time to time e.g. to provide model updates to the components of the online functionality .

The illustrated architecture provides a layered object recognition framework for the recognition of a set of object and scene classes where a class may refer to a descriptive label such as a type or category of objects . The layered framework progresses from recognition of simple to complex classes from large scale structure recognition to finer scale recognition and from independent object level recognition to recognition with contextual inferencing. Object extraction and identification are intertwined in an iterative computation in which progressive disambiguation of class labels is used to refine extraction which in turn improves the accuracy of the object identification.

The illustrative architecture of the recognition and classification subsystem realizes the object recognition algorithms as a scaleable system for large area classification with an extensible set of classes. In some embodiments efficient data structures and algorithms for large scale handling of spatial data image data multi dimensional features and learning classifiers are implemented using off the shelf systems. As a result the disclosed architecture supports easy interfacing with users of the technology and enables the recognition and classification subsystem to transition to a real time 3D reasoning system if desired.

In some applications urban infra structural objects from dumpsters to alleys are important for tactical operational planning. The disclosed technology e.g. offline technology can be configured to identify a comprehensive collection of urban objects. In some embodiments object recognition technology is used to automatically extract and identify numerous urban and semi urban classes of tactical relevance with 3D LIDAR and 2D EO electro optical sensor data of city scale environments. This 3D urban object recognition technology can be deployed in among other applications real time 3D query and reasoning systems.

The illustrative recognition and classification subsystem is embodied as a layered end to end recognition system that understands a scene by extracting and identifying progressively complex object classes. Easy to detect distinctive large scale objects e.g. ground foliage roofs walls and buildings are recognized first as a coarse layer. Recognition of these large scale objects exposes compact regions of space that are supported by the recognized objects which are e.g. the ground a roof or a wall. The regions of space defined by the recognized larger scale objects are extracted as proto objects and may typically contain one or more as yet unidentified classes of objects. Proto object extraction provides a focus of attention mechanism for efficiently handling complexity in a large cluttered scene. Objects in these proto object regions are progressively identified applying multiple hypotheses to ambiguous objects. Some context free objects e.g. a crane or a satellite dish with discriminative invariant features are identified first without contextual reasoning where context free may indicate that the object can be identified accurately without reference to the particular context in which it occurs . Context sensitive objects e.g. doors parking area are identified next utilizing 3D spatial relationships where context sensitive may indicate that an object s identification accuracy can be improved by the use of context information such as surrounding or neighboring objects or landscape . Finally compound objects e.g. gas stations which include buildings gas pumps parking areas etc. are identified through e.g. bag of object characteristics loose configuration of simpler objects co located .

Using the layered architecture of the recognition and classification subsystem early recognition of large scale and easily recognizable objects occurs pre attentively and quickly and sets the stage for recognizing more complex objects within a context. Using 3D and 2D data in a combined unary and contextual recognition approach avoids inattentive blindness where human visual systems miss conspicuous events. In contrast with one filter per class and one shot classification approaches some embodiments of the disclosed approach can avoid errors by entertaining unique or multiple hypotheses as the data may suggest.

In the illustrative architecture object recognition progresses in a continuous manner from large scale structures to fine scale recognition and from independent context free recognition to recognition with contextual inferencing. This architecture utilizes a multi hypotheses non committal strategy in which early coarse scale ambiguous recognition is successively refined with finer scale data and context that leads to progressively more unambiguous object recognition.

The illustrative large scale object and region extraction module utilizes a 2D extraction module and a 3D extraction module to perform large scale object extraction and initial proto object extraction. In some embodiments the large scale object and region extraction module handles LIDAR scans by sub dividing the scans to cover in the range of about 1 5 square kilometers of urban locales. The extraction module handles the challenge of rapid area delimitation by first extracting and identifying easy objects recognizable as large scale coarse entities such as ground foliage roofs walls and buildings. This is accomplished through use of low level 3D features local planes 3D scatter matrices and tensor voting and efficient data structures for the representation of occupied and free 3D space. Extraction and identification of the ground plane buildings and similar large scale structures achieves at least two goals. First it provides context for finer scale object recognition and second it enables intelligent search to efficiently handle non uniform space occupancy.

The extraction module uses contextual frames of reference such as ground and buildings to extract isolated regions of space referred to herein as proto objects which may contain multiple objects that are as yet unidentified. Finer scale objects within the proto object regions are extracted and identified in subsequent modules of the object recognition and classification subsystem . The extracted proto objects provide focus of attention for efficiently handling the complexity of a large scale 3D scene.

The goal of the large scale and initial proto object extraction performed by the extraction module is to rapidly analyze large areas of the scene using low level cues such as planarity and perceptual grouping to generate 3D regions that contain objects of interest with high probability. To do this the components of the extraction module execute a series of algorithms such as those described below.

1. Ground Extraction. The ground e.g. earth floor roadway etc. is extracted using local vertical and planarity constraints with 3D data sampled at a coarse resolution. 3D points with z component local up direction close to the local minimum ground height are retained as putative ground points. Local surface normals are computed at voxels with these ground points. Ground regions are grown from these voxels by recursively connecting neighboring voxels with consistent local plane estimates. This ensures a sharp delineation of the 3D objects that do not belong to the ground. A parametric model is finally fit to obtain a terrain model. Ground points are removed for the subsequent steps.

2. Off Ground Local Surface Classification. At each voxel the illustrative extraction module adaptively performs an eigenvalue decomposition of the local 3D scatter matrix at multiple scales thus ensuring a stable fit while preserving local 3D discontinuities. Larger voxels are used to extract big structures such as buildings while smaller voxels are used for other objects. Voxels are labeled with the rank of the scatter matrix as locally planar rank 1 curvilinear rank 2 or 3D texture rank 3 .

3. Proto Object Extraction. Rank 1 and rank 2 tensors are grouped over larger 3D regions using perceptual organization. Coherent 3D surface rank 2 or wire like regions rank 1 are grouped into larger regions based on proximity and consistent surface normals or tangent directions respectively. In some implementations proto objects are 3D regions that have a high likelihood of belonging to a part or a whole object that is to be identified in subsequent layers. The relationship between these regions is encapsulated in an adjacency graph which will be used during contextual object identification performed by the contextual object identification module described below. The Off Ground Local Surface Classification and the Proto Object Extraction are repeated at progressively finer resolutions as needed to extract smaller proto objects.

4. Building Extraction and Vegetation Clutter Removal. Buildings are extracted within 3D regions using region descriptors such as i histogram of angles between the normals of a region and the local ground normal ii histogram of angles between normals of the region and the casting direction iii histogram of projections of 3D points in a region along the ground direction iv first and second order moments of 3D points within a region v area of the region. These descriptors are used to train a classifier to label regions as clutter wall large vertical surface roof large horizontal or tilted surface and proto object. Priors such as heights of roofs size of building walls are also incorporated into the classifier. Neighboring regions containing a roof and walls are further labeled as building candidates. The 3D orientation of each building and extracted proto object are estimated from the region descriptors and projected into the corresponding 2D images to extract 2D object regions.

The coarse to fine processing described above enables an efficient computation of distinctive coherent surfaces and rapid rejection of vegetation clutter which lacks the smoothness of artificial objects. It also enables the inference of relationships between the proto objects. In an example shows results of building and ground extraction and foliage removal that have been achieved with 50 aerial data. In the example the image depicts the original point cloud the image depicts a 2D planar surface grouping the image depicts identification of planar structures vs. foliage and the image depicts identification of buildings vs. ground . Other examples of large scale and proto object identification are provided in FIGS. 7 and 9 of the aforementioned U.S. Provisional Application Ser. No. 61 954 635.

Referring again to the feature computation module performs feature computation for object recognition using the extracted objects and regions output by the extraction module . Proto objects extracted by the extraction module the first layer are recognized using modules the feature computation module the context free object identification module the contextual object identification module and the complex object identification module in subsequent layers. The feature computation module utilizes extracted objects and regions produced by the extraction module to generate a representation of objects in terms of data driven parts. To do this the feature computation module uses invariant semi local 3D and 2D features produced by e.g. 3D feature computation module and 2D feature computation module . The invariant 2D and 3D features enable the following 1 objects can be recognized under occlusions and missing data without the need for full geometric relationships between parts 2 invariant features handle pose variations and viewing distances 3 multi scale features work with a wide range of resolutions present in aerial and ground collections 4 clustered feature prototypes called 3D shapemes 2D visemes and 3D 2D textons succinctly capture intra class and inter class variations for efficient object recognition.

Some examples of features for object recognition include invariant 3D shape histograms such as spin images and shape contexts 2D shape and appearance features such as histograms of gradients steerable pyramids and Gabors and texture features such as textons. In some cases joint exploitation of 3D and 2D data is performed to fix the scale and perspective of 2D images for view invariant 2D feature computation. Occlusion reasoning in 3D can be used to infer missing object parts.

The feature computation module utilizes semi local scale and pose invariant 3D and 2D features for object recognition. The 3D 2D shape and appearance shapemes visemes and 3D 2D texture textons features provide robustness of recognition as well as efficiency of computation. These features are resilient to missing data pose and view variations partial occlusions and the presence of unknown classes as clutter and can handle intra and inter class appearance and shape variability. Generalizability with features enables recognition so that precise models for each class are not required. The semi local discriminative features used by the feature computation module for data driven part and sub part based description of objects are designed to capture 3D and 2D shape appearance and texture properties of a wide range of object classes.

Embodiments of the feature computation module can 1 work directly with point cloud data without the need for polygonal models of objects of interest thereby enabling easy extension of the set of targeted features without expensive modeling and learning 2 apply invariant surface and feature representations such as spin images or the vertical support histogram which are resilient to surrounding clutter missing and incomplete data due to occlusions and non uniform densities in point cloud capture 3 achieve precision alignment of 2D and 3D shapes for verification of features for ensuring high accuracy and low false alarms. Semi local multi scale invariant 3D and 2D features help address data and model issues such as robustness to partial occlusions pose invariance handling variable resolution and precision resilience to clutter part based recognition and intra and inter class variability. Additionally the invariant 3D and 2D features can represent significant and salient components of an object without the need for precise segmentation into semantic parts. Instead the presence of features in flexible geometric configurations is sufficient for robust recognition.

Referring again to the illustrative context free object identification module performs context free object identification with feature sharing using the 2D and 3D features extracted by the feature computation module . The context free object identification module uses features for joint learning of multiple classes using feature sharing and context in the proto object identification layer. This enables the system to used limited exemplars and models for multi class object recognition and for recognition in context. Classification with feature sharing can be performed using e.g. an indexable database of joint features for a large database of objects such as vehicles. The context free object identification module utilizes multiple techniques to handle a large class of objects including for example 1 automated learning of shared features amongst classes using a logarithmic number of features as a function of the number of classes and 2 a visual vocabulary of feature prototypes to create clusters of similar classes for linear in number of classes complexity for learning.

Referring again to the illustrative contextual object identification module performs principled contextual inferencing. The principled approach to object classification is used with inter class and spatial constraints expressed as graph models with belief propagation. This approach enables learning context both from examples as well as priors derived from generic world knowledge and human specified rules. depicts an example of the use of context information and belief propagation to classify smaller scale objects. Proto object classification of objects depicted in images may identify an upright object as either a fire hydrant or a post box mailbox . The proto object classification is used to initialize a graphical model with which belief propagation is performed resulting in a final object identification of fire hydrant . The classification of fire hydrant may be based on for example 3D neighboring objects that have already been classified. For example if the system has already identified a building and a street these object identifications can be used to influence the classification of the unknown object. further illustrates the use of context information and belief propagation to classify objects based on the relationships of the objects to one other. In the relative distances between the object classes e.g. C C C C are used to compute a potential between each of the connected nodes. Connected nodes exchange messages m which are combined with local evidence to compute a belief at each node. The computed belief leads to object classification.

In order to recognize some object or scene classes such as an alley or a factory the recognition and classification subsystem uses context for object extraction and identification. The interaction between object identification and extraction and bag of objects identification can be used to improve accuracy in object recognition. Graphical models can be used for the representation of context and belief propagation can be used as a principled method for inferencing with context. Contextual inferencing can be extended to include classes that can be loosely represented as a bag of objects. These classes can represent complex objects such as a mobile home park a factory or a railroad yard. The complex objects may be a conglomeration of object types arranged in flexible geometric relationships.

The illustrative recognition and classification subsystem performs context free object identification and contextual object identification iteratively. The context free object identification module generates object class hypotheses which are used by the contextual object identification module to produce context based extracted objects which are then fed back to the context free object identification module .

Referring still to the illustrative complex object identification module performs bag of objects identification techniques to identify complex objects. The context free and contextual identification layers performed by e.g. the context free object identification module and the contextual object identification module identify and extract objects typically in connected compact regions of space. The next processing layer performed by the complex object identification module addresses the recognition of complex extended object classes such as a golf course processing plant or mobile home park. These complex objects consist of a loose geometric configuration of other small objects and are called bag of objects. For example individual units of a mobile home park can be at various distances from each other. Embodiments of the complex object identification module can employ the following types of features to identify and extract these complex objects i textons for characterizing texture and appearance in 3D and 2D date acquired mainly from the air. For example using textons large geographical areas can be characterized as being a grassy surface forest crop field or an urban environments ii bag of objects descriptors. Bag of objects is akin to the popular bag of features descriptor used in computer vision and information retrieval for retrieving images video and documents using histograms of visual words and textual words. Geometrical relationships between visual words are ignored with the idea that visual words are rich enough to capture class specific characteristics. The bag of objects descriptor is a histogram of object classes which occur within an area. The descriptor does not capture the relationship between the objects but can be used for recognizing semantic objects that have little or no constraints between the components such as a factory or feeding lot. In essence the bag of objects descriptor captures the gist of regions within a scene i.e. it specifies the individual content but not the relationship between components iii contextual bag of objects descriptor which consists of concatenated bag of objects histograms of recognized small objects within regions centered at a 3D location. The contextual bag of objects provides more discrimination compared to the simpler bag of objects feature yet is flexible to configuration changes between the individual objects. For example objects such as a gas station have a constrained spatial distribution between the objects pumps occur at a certain distance from the building . The foregoing features designed to capture the characteristics of complex objects are employed in a classification framework that utilizes training data from annotated scenes as well as from pre defined rules is used. The rules can be in the form of classes present as well as sketches of rough arrangements of classes.

Training and context learning are typically performed offline by the visual words computation module the object classifier training module and the contextual relationship determination module . The components of the offline functionality develop multi class classifiers learning classifiers with limited exemplars e.g. classifiers that automatically and optimally share features across classes and can work with limited training examples. This approach can reduce error in classification by not committing to a labeling on a class by class basis but rather keeping hypotheses alive until more features from later layers lead to more specificity. The classifiers are constructed automatically and require limited training data. The classifiers naturally achieve generalization and robustness to intra class variations. New classes can be incrementally added.

Referring now to an exemplary plot illustrates the heterogeneity of object classes and the disclosed layered approach for exploiting that heterogeneity. The classes range along the y axis from smaller scale objects such as fire hydrants and trash cans to larger scale objects such as cranes and buildings to complex objects such as streets golf courses and trailer parks. The objects are ranked along the context x axis. Categories such as buildings trees and ground may be capable of being recognized with very little context while complex e.g. compound objects such as factories and golf courses may only be recognized by detecting the presence and co location of a large number of smaller object classes using e.g. a bag of objects .

As noted above principled contextual inferencing can be used to exploit scene context using the identity of objects within local spatial regions. For example independently distinguishing small objects such as fire hydrants from other small objects such as post boxes may be difficult. However context information relating them to larger more salient objects may disambiguate the exact class identity of small objects e.g. fire hydrants occur close to buildings. Contextual information is encoded as a graphical model where each node encapsulates the belief that the object belongs to a class. Belief propagation is used to refine and detect object classes.

Referring now to an example of a process implemented as computer readable instructions executed by the computing system to perform multi sensor multi modal 3D geospatial mapping object recognition scene annotation and analytics is shown. The process may be embodied as computerized programs routines logic and or instructions executed by the platform of the computing system for example by one or more of the modules and other components of the platform shown in described above. At block the system receives sensor data from the multiple multi modal sensors e.g. sensor data from sensors . In block the system temporally aligns or synchronizes the various multi modal data streams received in block . In block the system spatially aligns the synchronized sensor data using e.g. spatial and geo spatial association techniques. In block the system generates a navigation path and geospatial map for the platform . To do this the system may perform multi sensor 6DOF pose estimation in block using e.g. an error state extended Kalman filter as described above with reference to . In block the system performs geospatial data integration of the multi sensor multi modal data e.g. sensor data .

The data integration performed in block may result in an integrated map representation of the sensor data which can be visualized or analyzed by e.g. plug in style analytics and or business logic. In block the system determines whether a live analytics module e.g. a live analytics subsystem is available. If no live analytics are available the system proceeds to block to perform compression of the integrated sensor data for visualization and or transmission to another device. If live analytics are available the system proceeds to block . In block the system performs live analytics on the integrated multi modal sensor data and annotates the integrated multi modal sensor data. To do this the system performs object scene recognition and classification change detection and or anomaly detection algorithms to identify useful annotations and adds the annotations to the map representation of the integrated multi modal sensor data.

In block the system determines whether any domain specific business logic e.g. a domain specific data correlation module is available. If no business logic is available the system proceeds to block to perform compression of the integrated sensor data for visualization and or transmission to another device. If business logic is available the system proceeds to block . In block the system executes the domain specific business logic on the integrated multi modal sensor data and annotates the integrated multi modal sensor data with markups produced as a result of the application of the domain specific business logic. To do this the system may perform domain specific object scene recognition and classification change detection and or anomaly detection algorithms to identify useful domain specific markups and adds the domain specific markups to the map representation of the integrated multi modal sensor data.

In block the system creates a compressed version of the annotated and marked up as needed map representation of the integrated multi modal sensor data. To do this the system may employ one or more of the approaches described above with reference to . Following the data compression performed in block the system may proceed to block and or block . In block the system presents a live visualization of the annotated and marked up as needed integrated multi modal sensor data locally e.g. on the platform . In block the system aggregates the locally produced annotated sensor data with similar data produced by other platforms and presents a visualization of the aggregated data on a display device. This data aggregation and visualization of the aggregated data may be performed on the local platform or on another computing device such as a ground based command center or mobile device.

Referring now to an example of a process implemented as computer readable instructions executed by the computing system to perform object recognition and classification is shown. The process may be embodied as computerized programs routines logic and or instructions executed by the computing system for example by one or more of the modules and other components of the computing system shown in described above. The illustrative process includes steps or subprocesses contained in box that can execute concurrently e.g. by multiple processors as described below with reference to . The process also includes steps or subprocesses contained in box which can execute iteratively.

At block the system performs large scale and or proto object recognition on the visual imagery captured by e.g. 2D and or 3D image sensors coupled to or integrated with the computing system . To do this the system may utilize any of the techniques described above with reference to the extraction module of . In block the system performs context free object identification using the large scale object and or proto object information generated in block . To do this the system may utilize any of the techniques described above with reference to the context free object identification module of . In block the system performs contextual object identification e.g. contextual inferencing utilizing the object identification information generated in block . The system may iterate between blocks to further classify smaller scale objects detected within the large scale or proto object areas detected in block . In block the system utilizes the smaller scale object identification information produced by block and or block to perform complex object recognition. To do this the system may utilize a bag of objects algorithm as described above with reference to the complex object identification module of . The resulting output of the process can be used in the generation of a map representation for e.g. the platform or for further analysis including change detection and or anomaly detection as described above .

Many various applications of the multi sensor multi modal data collection analysis recognition and visualization technologies disclosed herein are possible including applications addressed to automated collection and analysis of geospatial data for facility monitoring asset protection and monitoring or surveillance of natural resources e.g. water supplies agricultural properties forests etc. or man made structures including public utility networks e.g. electric gas oil water cable dams levees roads bridges tunnels etc.

In one implementation the data representation compression and visualization modules were assessed qualitatively and quantitatively using data collected by a multi sensor rig mounted on a Segway RMP model 200 . Evaluation was performed using two different sensor configurations that reflect common operational configurations for ground robots. Both configurations used LIDAR sensors paired with stereo cameras and a low cost MEMS IMU. The cameras and IMU form a navigation solution that can provide reliable 6 DOF pose in GPS enabled or GPS denied environments. The first sensor configuration paired a single laser scanning LIDAR the Hokuyo UTM LTX mounted coronally on a rotary stage that repeats a constant velocity 180 degree pan behavior thus collecting a 360 degree point cloud with full hemispheric field of view. The Hokuyo sensor receives range measurements in the form of line scans with 1081 samples 0.25 degree angular resolution at 40 Hz. The second sensor configuration used the Velodyne HDL 32e a multi laser LIDAR that has been extensively used for autonomous vehicles in the last decade. The sensor was mounted rigidly with the same stereo camera set and IMU as in the Hokuyo configuration. The Velodyne HDL 32e spins at 5 Hz and collects range and intensity values from 32 lasers thus sampling a 360 degree field of view. For both configurations the raw LIDAR range values were integrated into a global coordinate frame using real time pose estimates obtained from an extended Kalman filter that integrates IMU measurements with visual odometry. To aid in drift correction incoming 3D point clouds were aligned to previously accumulated patches via real time ICP iterative closest point thus feeding back drift corrections to the 6DOF vision based localization. To evaluate the system data was collected from several outdoor environments with a mix of trees buildings vehicles etc. The data sequences are 5 to 20 minutes long and contain up to 0.5 billion points. The data was processed in packets of 3000 points at a time and the compression statistics and performance times were aggregated across all sequences for the same sensor. The streaming framework was implemented in C and uses an octree class derived from the base class in Point Cloud Library. Tests were performed on a laptop with an i7 processor. The compression rate was evaluated at a high lossless and low lossy resolution. The Hokuyo sensor was tested with 1 cm and 4 cm voxel size and the Velodyne sensor was tested with 4 cm and 8 cm voxel size. The normals are computed at the 4 cm voxel octree depth and using a search radius of 30 cm with a maximum of 20 nearest neighbors. The normals were encoded with 11 bits per voxel. Results are shown in FIGS. 18 19 and 20 of the aforementioned U.S. Provisional Application Ser. No. 61 954 635.

In another configuration a ground vehicle is equipped with multiple sensors including LIDAR EO IR infrared and multi spectral sensors. These sensors are used to construct multi modal 3D maps of the environment. A navigation module on the vehicle precisely locates and orients the vehicle in geo spatial coordinates using data from GPS IMU odometer compass and camera based sensors. The 3D pose estimated by the navigation module is used to stitch together the LIDAR data to generate 3D map data. 3D swath data can be further matched to improve the 3D alignment. Final estimates of these poses allow highly accurate map layer with all modalities of data co located in geo spatial coordinates.

A multi modal change detection module e.g. change detection module enables comparing the captured multi modal 3D to previous collects by aligning the two data sets automatically. Anomalous changes are automatically detected and flagged for future examination by an analyst. Current environment conditions are used to infer if the change is due to environmental conditions e.g. different temperatures or natural illumination at the two collects or if it is truly a real change based on the business rules of the application.

In another configuration an aerial vehicle is equipped with multiple sensors including LIDAR EO and IR sensors. These sensors are used to construct multi modal 3D maps of the environment. A navigation module on the aerial vehicle precisely locates and orients the vehicle in geo spatial coordinates using data from GPS IMU air data compass and camera based sensors. The 3D pose estimated by the navigation module is used to stitch together the LIDAR data with the EO and IR data and geo spatial reference terrain data of the earth. A multi modal change detection module enables comparing the captured multi modal 3D to previous collects by aligning the two data sets automatically. Anomalous changes are automatically detected and flagged for future examination by an analyst.

For some applications multi sensor data collected from the air and ground platforms can be combined to build an integrated multi modal 3D map of the environment. The combination is typically be done by aligning the two data sets. The alignment may be done at the pixel or voxel level by matching 2D and 3D point clouds collected from the air and ground. Alternatively the alignment can be done at the semantic level where high level features and objects such as roads buildings poles etc. are detected in each of the ground and aerial collects and then their detected locations are used to match the two data sets. Analytics and change detection can be performed on either the combined or separate aerial and ground collects and then fused together.

Referring now to an exemplary implementation of the computing system utilizes a commercial off the shelf COTS multi processing computing platform. As described above scalable methods for processing large amounts of data for performing 3D modeling and 3D object classification using LIDAR and EO data acquired by both aerial and ground sensors can be used. In particular terabytes of aerial LIDAR data can be processed using an off the shelf dual quad core desktop computer for performing LIDAR based 3D object classification ground extraction vegetation clutter removal and 3D building segmentation automatic 2D 3D registration for texturing 3D models for geospatial areas in the range of thousands of square kilometers. The scalability of the object recognition is provided by partitioning the input LIDAR EO data into tiles which can be individually processed across multiple CPUs as shown in . This data level parallelization ensures a high degree of scalability of the processing with minimal scripting language required e.g. bash under Cygwin in the case of the Windows operating system .

The illustrative system architecture of the implementation includes a multiprocessor task distribution manager which handles the distribution of the sensor data from the various sensors e.g. aerial LIDAR ground LIDAR 2D imagery to processing modules data ingestion and octree representation creation and large scale proto object extraction distributes volumes of interest across multiple CPUs e.g. processor processor N where N is a positive integer and handles large scale contextual identification . Each of the processors 1 N can handle the object recognition and classification tasks e.g. proto object extraction invariant feature computation proto object unary and contextual identification . The multiprocessor task distribution manager also oversees the output of identification results and map representations to the visualization interface and accesses and updates to from the data stores including a feature database classifier parameters and or learned context data.

The task distribution manager has the role of communicating with individual algorithmic components and to assign tasks to each of the processors using e.g. the Message Passing Interface MPI protocol available both on Windows and Unix platforms. The system can use an octree data structure to ensure an efficient memory usage over large terrain areas and to access data at different resolution levels. After ground removal buildings and proto objects are extracted at a coarse scale and each volume of interest VOI is sent to be further processed on multiple processors e.g. processors 1 to N . Within each processor proto objects are extracted at finer resolution and invariant features in 3D and 2D are computed for classification. The feature computation and the classification are the most computationally intensive modules and offer a high degree of data level parallelization. In the last stage the object classification results are collected from all the nodes to find large semantic structures using the bag of objects approach.

Referring now to a simplified block diagram of an embodiment of the real time multi modal 3D geospatial mapping object recognition scene annotation analytics and visualization computing system is shown. While the illustrative embodiment is shown as involving multiple components and devices it should be understood that the computing system may constitute a single computing device alone or in combination with other devices. The embodiment includes a sensor computing device which embodies features and functionality of the multi sensor data collection analysis recognition and visualization computing system e.g. the sensors and the platform shown in a data aggregation computing device which embodies features and functionality of the multi platform data aggregation subsystem shown in and a user computing device which embodies features and functionality of the crowd sourcing app shown in . The embodiment includes multiple additional sensor computing devices each of which may be embodied in a similar manner as the sensor computing device . Each or any of the computing devices may be in communication with one another via one or more networks or other types of communication links .

The computing system or portions thereof may be distributed across multiple computing devices that are connected to the network s as shown. In other embodiments however the computing system may be located entirely on for example the sensor computing device or one of the devices . In some embodiments portions of the system may be incorporated into other systems or computer applications e.g. as a plugin . Such applications or systems may include for example operating system software or GIS applications. As used herein application or computer application may refer to among other things any type of computer program or group of computer programs whether implemented in software hardware or a combination thereof and includes self contained vertical and or shrink wrapped software applications distributed and cloud based applications and or others. Portions of a computer application may be embodied as firmware as one or more components of an operating system a runtime library an application programming interface API as a self contained software application or as a component of another software application for example.

The illustrative sensor computing device includes at least one processor e.g. a microprocessor microcontroller digital signal processor etc. memory and an input output I O subsystem . The sensor computing device may be embodied as any type of computing device capable of performing the functions described herein such as a mobile robot a navigation capable electronic device an unmanned or unpiloted aerial or ground vehicle a personal computer e.g. desktop laptop tablet smart phone body mounted device wearable device etc. a smart appliance a server an enterprise computer system a network of computers a combination of computers and other electronic devices or other electronic devices. Although not specifically shown it should be understood that the I O subsystem typically includes among other things an I O controller a memory controller and one or more I O ports. The processor and the I O subsystem are communicatively coupled to the memory . The memory may be embodied as any type of suitable computer memory device e.g. volatile memory such as various forms of random access memory .

The I O subsystem is communicatively coupled to a number of hardware and or software components including the components of the system shown in a data storage device a number of sensors a user interface UI subsystem and a communication subsystem . The sensors may include one or more cameras e.g. 2D and or 3D still and or video cameras as well as other types of sensors such as IMUs accelerometers gyroscopes GPS receivers LIDAR systems and or others. As used herein a camera may refer to any type of 2D or 3D image sensor or other device that is capable of acquiring and recording two dimensional 2D or three dimensional 3D video images of portions of the real world environment and may include cameras with one or more fixed camera parameters and or cameras having one or more variable parameters fixed location cameras such as stand off cameras that are installed in walls or ceilings and or mobile cameras such as cameras that are integrated with consumer electronic devices such as laptop computers smart phones tablet computers wearable electronic devices and or others.

The user interface subsystem includes one or more user input devices e.g. a touchscreen keyboard virtual keypad microphone etc. and one or more output devices e.g. speakers display devices LEDs etc. . The user interface subsystem may include devices such as a touchscreen display a touch sensitive keypad a kinetic sensor and or other gesture detecting device an eye tracking sensor and or other devices that are capable of detecting human interactions with a computing device.

The devices are illustrated in as being in communication with the sensor computing device and or other devices by the network communication links . It should be understood that any or all of the devices may be integrated with the sensor computing device or embodied as a separate component.

The I O subsystem is communicatively coupled to components of the multi sensor data collection analysis recognition and visualization platform described above one or more data storage devices e.g. machine readable storage media a user interface UI subsystem and a communication subsystem . The storage media may include one or more hard drives or other suitable data storage devices e.g. flash memory memory cards memory sticks and or others . In some embodiments portions of the computing system e.g. the platform and or other data and components reside at least temporarily in the storage media . Portions of the computing system e.g. portions of the platform may be copied to the memory during operation of the computing device for faster processing or other reasons.

The communication subsystem communicatively couples the sensor computing device to one or more other devices systems or communication networks e.g. a local area network wide area network personal cloud enterprise cloud public cloud and or the Internet for example. Accordingly the communication subsystem may include one or more wired or wireless network interface software firmware or hardware for example as may be needed pursuant to the specifications and or design of the particular embodiment of the system .

The user computing device and the data aggregation computing device each may be embodied as any suitable type of computing device or personal electronic device capable of performing the functions described herein such as any of the aforementioned types of devices or other electronic devices. The illustrative user computing device and data aggregation computing device each include components having the same or similar names to components of the sensor computing device described above and accordingly those components of the computing devices may be embodied similarly. Further each of the devices may include components similar to those described above and the computing system may include other components sub components and devices not illustrated in . In general the components of the computing system are communicatively coupled as shown in by signal paths which may be embodied as any type of wired or wireless signal paths capable of facilitating communication between the respective devices and components.

Illustrative examples of the technologies disclosed herein are provided below. An embodiment of the technologies may include any one or more and any combination of the examples described below.

In an example 1 a navigation capable vehicle includes one or more processors and in communication with the one or more processors a two dimensional image sensor a three dimensional image sensor one or more sensors to determine motion location and orientation of the navigation capable vehicle and one or more non transitory machine accessible storage media including instructions to cause the navigation capable vehicle to temporally and spatially align sensor data received from the two dimensional sensor the three dimensional sensor and the one or more motion location and orientation sensors generate a map representation of a real world environment in a frame of reference of the navigation capable vehicle based on the temporally and spatially aligned sensor data recognize a plurality of visual features in the map representation using one or more computer vision algorithms and annotate one or more of the visual features in accordance with domain specific business logic.

An example 2 includes the subject matter of example 1 including instructions to cause the navigation capable vehicle to present a visualization of the annotated visual features on the navigation capable vehicle. An example 3 includes the subject matter of example 1 or example 2 including instructions to cause the navigation capable vehicle to estimate a navigation path for the navigation capable vehicle. An example 4 includes the subject matter of any of examples 1 3 including instructions to cause the navigation capable vehicle to estimate a navigation path for the navigation capable vehicle. An example 5 includes the subject matter of any of examples 1 4 including instructions to cause the navigation capable vehicle to algorithmically detect changes in the visual features of the map representation over time. An example 6 includes the subject matter of example 5 including instructions to cause the navigation capable vehicle to transmit data indicative of the detected changes to a ground based computing device in real time. An example 7 includes the subject matter of example 5 or example 6 wherein the domain specific business logic is to cause the navigation capable vehicle to detect a change in a visual feature of a physical component of a ground based utility service. An example 8 includes the subject matter of any of examples 5 7 wherein the domain specific business logic is to cause the navigation capable vehicle to detect a change in a visual feature of a natural resource. An example 9 includes the subject matter of any of examples 5 8 wherein the domain specific business logic is to cause the navigation capable vehicle to detect a change in a visual feature of a man made physical structure. An example 10 includes the subject matter of any of examples 1 9 wherein the navigation capable vehicle includes an unmanned aerial vehicle.

In an example 11 a multi sensor data collection analysis recognition and visualization platform includes instructions embodied in one or more non transitory computer readable storage media and executable by one or more processors to cause a navigation capable vehicle to receive sensor data from a plurality of sensors including a two dimensional image sensor a three dimensional image sensor and one or more sensors to determine motion location and orientation of the navigation capable vehicle temporally and spatially align the sensor data received from the two dimensional sensor the three dimensional sensor and the one or more motion location and orientation sensors generate a map representation of the real world surroundings of the navigation capable vehicle based on the temporally and spatially aligned sensor data recognize a plurality of visual features in the map representation by executing one or more computer vision algorithms annotate one or more of the visual features in accordance with domain specific business logic and present a visualization of the annotated visual features on the navigation capable vehicle.

An example 12 includes the subject matter of example 11 including instructions to cause the navigation capable vehicle to present the visualization of the annotated visual features on the navigation capable vehicle in real time. An example 13 includes the subject matter of example 11 or example 12 including instructions to cause the navigation capable vehicle to transmit the visualization of the annotated visual features to a ground based computing device in real time. An example 14 includes the subject matter of any of examples 11 13 wherein the domain specific business logic includes a change detection algorithm to detect one or more domain specific changes in the visual features over time and the platform includes instructions to annotate the visual features to identify the detected domain specific changes on the visualization. An example 15 includes the subject matter of any of examples 11 14 wherein the domain specific business logic includes an anomaly detection algorithm to detect one or more domain specific anomalies in the visual features over time and the platform includes instructions to annotate the visual features to identify the detected domain specific anomalies on the visualization.

In an example 16 a system for multi sensor data collection analysis recognition and visualization by a navigation capable vehicle the system including one or more computing devices configured to temporally and spatially align data received from a two dimensional sensor a three dimensional sensor and one or more motion location and orientation sensors generate a map representation of the real world surroundings of the navigation capable vehicle based on the temporally and spatially aligned sensor data recognize a plurality of visual features in the map representation by executing one or more computer vision algorithms estimate a navigation path for the navigation capable vehicle annotate one or more of the visual features in accordance with domain specific business logic and present a visualization of the annotated visual features on the navigation capable vehicle.

An example 17 includes the subject matter of example 16 configured to interactively modify the visualization of the annotated visual features in response to user input. An example 18 includes the subject matter of example 16 or example 17 configured to tag one or more of the annotated visual features in the visualization in response to user input. An example 19 includes the subject matter of any of examples 16 18 configured to execute domain specific anomaly detection logic on the visual features and annotate the visualization based on the execution of the domain specific anomaly detection logic. An example 20 includes the subject matter of any of examples 16 19 configured to execute domain specific change detection logic on the visual features and annotate the visualization based on the execution of the domain specific change detection logic.

In an example 21 a mobile computing device includes one or more processors and in communication with the one or more processors one or more image sensors configured to obtain two dimensional image data and three dimensional image data one or more non transitory machine accessible storage media including instructions to cause the mobile computing device to temporally and spatially align the two dimensional image data and three dimensional image data generate a map representation of a geo spatial area of the real world surroundings of the mobile computing device based on the temporally and spatially aligned two dimensional and three dimensional image data and recognize a plurality of visual features in the map representation using one or more computer vision algorithms to recognize larger scale objects recognize smaller scale objects by iteratively performing context free object identification and contextual object identification and recognize a complex object including a plurality of the smaller scale objects using a classifier.

An example 22 includes the subject matter of example 21 including instructions to cause the mobile computing device to detect the larger scale objects by determining a contextual frame of reference and use the contextual frame of reference to identify the larger scale objects. An example 23 includes the subject matter of example 21 or example 22 including instructions to cause the mobile computing device to recognize the larger scale objects by executing an invariant three dimensional feature detection algorithm directly on point cloud data obtained from at least one of the image sensors. An example 24 includes the subject matter of example 23 including instructions to cause the mobile computing device to recognize the larger scale objects by executing an invariant two dimensional feature detection algorithm. An example 25 includes the subject matter of any of examples 21 24 including instructions to cause the mobile computing device to recognize the larger scale objects by executing an invariant two dimensional feature detection algorithm. An example 26 includes the subject matter of any of examples 21 25 including instructions to cause the mobile computing device to recognize the smaller scale objects by executing a context free feature sharing algorithm. An example 27 includes the subject matter of example 26 including instructions to cause the mobile computing device to recognize the smaller scale objects by obtaining context information and classifying the smaller scale objects based on the context information. An example 28 includes the subject matter of example 26 or example 27 including instructions to cause the mobile computing device to recognize the complex objects by executing a contextual bag of objects algorithm.

In an example 29 an object scene recognition system includes instructions embodied in one or more non transitory computer readable storage media to and executable by one or more processors to cause a mobile computing device to obtain two dimensional image data and three dimensional image data from one or more image sensors temporally and spatially align the two dimensional image data and three dimensional image data recognize a plurality of visual features in the image data using one or more computer vision algorithms to recognize larger scale objects recognize smaller scale objects by iteratively performing context free object identification and contextual object identification and recognize a complex object including a plurality of the recognized smaller scale objects using a classifier.

An example 30 includes the subject matter of example 29 including instructions to cause the mobile computing device to detect the larger scale objects by determining a contextual frame of reference and use the contextual frame of reference to identify the larger scale objects. An example 31 includes the subject matter of example 29 or example 30 including instructions to cause the mobile computing device to recognize the larger scale objects by executing an invariant three dimensional feature detection algorithm directly on point cloud data obtained from at least one of the image sensors. An example 32 includes the subject matter of example 31 including instructions to cause the mobile computing device to recognize the larger scale objects by executing an invariant two dimensional feature detection algorithm. An example 33 includes the subject matter of any of examples 29 32 including instructions to cause the mobile computing device to recognize the larger scale objects by executing an invariant two dimensional feature detection algorithm. An example 34 includes the subject matter of example 30 including instructions to cause the mobile computing device to recognize the smaller scale objects by executing a context free feature sharing algorithm. An example 35 includes the subject matter of example 34 including instructions to cause the mobile computing device to recognize the smaller scale objects by obtaining context information and classifying the smaller scale objects based on the context information. An example 36 includes the subject matter of example 34 or example 35 including instructions to cause the mobile computing device to recognize the complex objects by executing a contextual bag of objects algorithm.

In an example 37 an object scene recognition method includes with one or more mobile computing devices obtaining two dimensional image data and three dimensional image data from one or more image sensors temporally and spatially aligning the two dimensional image data and the three dimensional image data and recognizing a plurality of visual features in the image data by recognizing larger scale objects recognizing smaller scale objects by iteratively performing context free object identification and contextual object identification and recognizing a complex object including a plurality of the smaller scale objects using a classifier.

An example 38 includes the subject matter of example 37 including determining a contextual frame of reference and using the contextual frame of reference to identify the larger scale objects. An example 39 includes the subject matter of example 37 or example 38 including executing an invariant three dimensional feature detection algorithm directly on point cloud data obtained from at least one of the image sensors. An example 40 includes the subject matter of example 38 including iteratively executing a context free feature sharing algorithm to recognize the smaller scale objects obtaining context information and classifying the smaller scale objects based on the context information.

In the foregoing description numerous specific details examples and scenarios are set forth in order to provide a more thorough understanding of the present disclosure. It will be appreciated however that embodiments of the disclosure may be practiced without such specific details. Further such examples and scenarios are provided for illustration and are not intended to limit the disclosure in any way. Those of ordinary skill in the art with the included descriptions should be able to implement appropriate functionality without undue experimentation.

References in the specification to an embodiment etc. indicate that the embodiment described may include a particular feature structure or characteristic but every embodiment may not necessarily include the particular feature structure or characteristic. Such phrases are not necessarily referring to the same embodiment. Further when a particular feature structure or characteristic is described in connection with an embodiment it is believed to be within the knowledge of one skilled in the art to affect such feature structure or characteristic in connection with other embodiments whether or not explicitly indicated.

Embodiments in accordance with the disclosure may be implemented in hardware firmware software or any combination thereof. Embodiments may also be implemented as instructions stored using one or more machine readable media which may be read and executed by one or more processors. A machine readable medium may include any mechanism for storing or transmitting information in a form readable by a machine e.g. a computing device or a virtual machine running on one or more computing devices . For example a machine readable medium may include any suitable form of volatile or non volatile memory.

Modules data structures blocks and the like are referred to as such for ease of discussion and are not intended to imply that any specific implementation details are required. For example any of the described modules and or data structures may be combined or divided into sub modules sub processes or other units of computer code or data as may be required by a particular design or implementation. In the drawings specific arrangements or orderings of schematic elements may be shown for ease of description. However the specific ordering or arrangement of such elements is not meant to imply that a particular order or sequence of processing or separation of processes is required in all embodiments. In general schematic elements used to represent instruction blocks or modules may be implemented using any suitable form of machine readable instruction and each such instruction may be implemented using any suitable programming language library application programming interface API and or other software development tools or frameworks. Similarly schematic elements used to represent data or information may be implemented using any suitable electronic arrangement or data structure. Further some connections relationships or associations between elements may be simplified or not shown in the drawings so as not to obscure the disclosure. This disclosure is to be considered as exemplary and not restrictive in character and all changes and modifications that come within the spirit of the disclosure are desired to be protected.

