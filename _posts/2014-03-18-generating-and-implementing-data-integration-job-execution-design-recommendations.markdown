---

title: Generating and implementing data integration job execution design recommendations
abstract: The method of managing performance of data integration are described. A performance analyzer may receive data about a data integration job execution. The performance analyzer may determine whether there is a performance issue of the data integration job execution. The performance analyzer analyzes the data about the data integration job execution when there is a performance issue. The performance analyzer generates a job execution design recommendation based on the analysis of the data and a set of predefined recommendation rules. The performance analyzer then displays the data about the data integration job execution and when there is a generated job execution design recommendation, displaying the job execution design recommendation.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09501377&OS=09501377&RS=09501377
owner: International Business Machines Corporation
number: 09501377
owner_city: Armonk
owner_country: US
publication_date: 20140318
---
The present disclosure relates to data integration and more specifically to performance monitoring and analysis for data integration.

Data integration may be described as extracting data from a source transforming the data and loading the data to a target. That is data integration is Extract Transform Load ETL processing. Data integration processing engines may be scalable and capable of processing large volumes of data in complex data integration projects. It is common for multiple users e.g. customers and projects to share a single data integration processing engine that is responsible for handling all of the data integration processing for those multiple users. This high volume and highly concurrent processing may be resource intensive and users try to balance the availability of system resources with the need to process large volumes of data efficiently and concurrently.

Workload management capabilities may be available at Operating System OS or lower levels. Workload management operates at a level that is removed from the data integration environment.

According to embodiments of the present disclosure a method a performance analyzer and a computer program product performing the method of managing performance of data integration are described. A performance analyzer may receive data about a data integration job execution. The performance analyzer may determine whether there is a performance issue of the data integration job execution. The performance analyzer analyzes the data about the data integration job execution when there is a performance issue. The performance analyzer generates a job execution design recommendation based on the analysis of the data and a set of predefined recommendation rules. The performance analyzer then displays the data about the data integration job execution and when there is a generated job execution design recommendation displaying the job execution design recommendation.

The above summary is not intended to describe each illustrated embodiment or every implementation of the present disclosure.

The descriptions of the various embodiments of the present disclosure have been presented for purposes of illustration but are not intended to be exhaustive or limited to the embodiments disclosed. Many modifications and variations will be apparent to those of ordinary skill in the art without departing from the scope and spirit of the described embodiments. The terminology used herein was chosen to best explain the principles of the embodiments the practical application or technical improvement over technologies found in the marketplace or to enable others of ordinary skill in the art to understand the embodiments disclosed herein.

Aspects of the present disclosure relate to data integration and more specifically to performance monitoring and analysis for data integration. Aspects include executing a data integration job and collecting performance data and or resource utilization data with a performance analyzer. The collected data may be presented to a user in real time or as a replay with the performance analyzer for the user to determine and correct issues with the job flow of the data integration job. The performance analyzer may determine when an issue is present and recommended solutions to a user as to correct or lessen the issue. Definable rules when analyzing the data may help determine actions to correct for problems. While the present disclosure is not necessarily limited to such applications various aspects of the disclosure may be appreciated through a discussion of various examples using this context.

In large data integration use cases due to complex business requirements it is common to have several hundreds of stages in a single data flow. To leverage system resources effectively a parallel engine that executes such complex data flow may choose to implement a pipeline mechanism that is to have many processes concurrently executing and one s output is directed to another s input using various techniques such as shared memory TCP IP socket or named pipe. With this technique as soon as a record is produced by producer and written to the pipeline it is read and processed by consumer. After this step finishes this consumer writes its own output to next pipeline which is further processed by its downstream consumers. All processes are simultaneously executing and the intermediate results are not landed on disk so such parallel engine architecture can efficiently leverage available system resources.

Further to speed up data processing for large data set the parallel engine may implement a data partitioned mechanism that is an entire input dataset is partitioned into multiple smaller segments based on a specific partitioning algorithm and each segment is sent to a separate instance of a processing stage. With this technique if a processing stage needs to process 1 billion records if using one instance all 1 billion records would flow though that stage. If there are 2 instances of the same stage and data is evenly distributed across those two instances then each instance would process 500 million records. As long as the system still has available resources partitioning would significantly reduce the total processing time of the entire data flow.

A parallel engine that implements both pipeline and data partitioned mechanisms can deliver good performance and scalability for extract transform load ETL data flows. Today numerous large enterprise customers rely on such systems to build their backbone for information integration and data warehousing applications.

From time to time due to data flow design network configuration storage system performance issue or parallel engine defect customers may run into performance problems for parallel data flow sometime those data flows can grind to halt. For example some common performance problems are 

When debugging such a problem one has to collect information needed across various files including job design job execution log input data files schemas configuration files performance data files etc. A typical debugging process starts with analyzing the job execution log. A log file could normally contain a lot of messages with no clear correlation between one another.

For simple to moderate data flows it might be manageable to collect information from various files and manually analyze such information to find the root cause. For complex data flow that has several hundreds of stages with many partitioning methods employed it can be very daunting to find out where the weakest spot or bottleneck is and what the right solution to solve the performance problem is. Very often being able to pinpoint the exact bottleneck can significantly speed up the problem resolution.

Embodiments herein provide for a dynamic graphic view on job performance data regardless of whether the job is executed in a parallel execution environment or a distributed execution environment. In various embodiments capabilities to analyze the performance data and present performance improvement recommendations to the end users are disclosed.

The computer system may contain one or more general purpose programmable central processing units CPUs A and B herein generically referred to as the processor . In an embodiment the computer system may contain multiple processors however in another embodiment the computer system may alternatively be a single CPU system. Each processor executes instructions stored in the memory and may include one or more levels of on board cache.

In an embodiment the memory may include a random access semiconductor memory storage device or storage medium either volatile or non volatile for storing or encoding data and programs. In another embodiment the memory represents the entire virtual memory of the computer system and may also include the virtual memory of other computer systems coupled to the computer system or connected via a network . The memory is conceptually a single monolithic entity but in other embodiments the memory is a more complex arrangement such as a hierarchy of caches and other memory devices. For example memory may exist in multiple levels of caches and these caches may be further divided by function so that one cache holds instructions while another holds non instruction data which is used by the processor or processors. Memory may be further distributed and associated with different CPUs or sets of CPUs as is known in any of various so called non uniform memory access NUMA computer architectures.

The memory may store all or a portion of the following a performance analyzer . This program and data structures are illustrated as being included within the memory in the computer system however in other embodiments some or all of them may be on different computer systems and may be accessed remotely e.g. via a network . The computer system may use virtual addressing mechanisms that allow the programs of the computer system to behave as if they only have access to a large single storage entity instead of access to multiple smaller storage entities. Thus while the performance analyzer is illustrated as being included within the memory it may not necessarily all be completely contained in the same storage device at the same time.

In an embodiment performance analyzer may include instructions or statements that execute on the processor or instructions or statements that are interpreted by instructions or statements that execute on the processor to carry out the functions as further described below. In another embodiment performance analyzer may be implemented in hardware via semiconductor devices chips logical gates circuits circuit cards and or other physical hardware devices in lieu of or in addition to a processor based system. In an embodiment the performance analyzer may include data in addition to instructions or statements.

The computer system may include a bus interface unit to handle communications among the processor the memory a display system and the I O bus interface unit . The I O bus interface unit may be coupled with the I O bus for transferring data to and from the various I O units. The I O bus interface unit communicates with multiple I O interface units and which are also known as I O processors IOPs or I O adapters IOAs through the I O bus . The display system may include a display controller a display memory or both. The display controller may provide video audio or both types of data to a display device . The display memory may be a dedicated memory for buffering video data. The display system may be coupled with a display device such as a standalone display screen computer monitor television or a tablet or handheld device display. In an embodiment the display device may include one or more speakers for rendering audio. Alternatively one or more speakers for rendering audio may be coupled with an I O interface unit. In alternate embodiments one or more of the functions provided by the display system may be on board an integrated circuit that also includes the processor . In addition one or more of the functions provided by the bus interface unit may be on board an integrated circuit that also includes the processor .

The I O interface units support communication with a variety of storage and I O devices. For example the terminal interface unit supports the attachment of one or more user I O devices which may include user output devices such as a video display device speaker and or television set and user input devices such as a keyboard mouse keypad touchpad trackball buttons light pen or other pointing device . A user may manipulate the user input devices using a user interface in order to provide input data and commands to the user I O device and the computer system and may receive output data via the user output devices. For example a user interface may be presented via the user I O device such as displayed on a display device played via a speaker or printed via a printer.

The storage interface supports the attachment of one or more disk drives or direct access storage devices which are typically rotating magnetic disk drive storage devices although they could alternatively be other storage devices including arrays of disk drives configured to appear as a single large storage device to a host computer or solid state drives such as flash memory . In another embodiment the storage device may be implemented via any type of secondary storage device. The contents of the memory or any portion thereof may be stored to and retrieved from the storage device as needed. The I O device interface provides an interface to any of various other I O devices or devices of other types such as printers or fax machines. The network interface provides one or more communication paths from the computer system to other digital devices and computer systems these communication paths may include e.g. one or more networks .

Although the computer system shown in illustrates a particular bus structure providing a direct communication path among the processors the memory the bus interface the display system and the I O bus interface unit in alternative embodiments the computer system may include different buses or communication paths which may be arranged in any of various forms such as point to point links in hierarchical star or web configurations multiple hierarchical buses parallel and redundant paths or any other appropriate type of configuration. Furthermore while the I O bus interface unit and the I O bus are shown as single respective units the computer system may in fact contain multiple I O bus interface units and or multiple I O buses . While multiple I O interface units are shown which separate the I O bus from various communications paths running to the various I O devices in other embodiments some or all of the I O devices are connected directly to one or more system I O buses.

In various embodiments the computer system is a multi user mainframe computer system a single user system or a server computer or similar device that has little or no direct user interface but receives requests from other computer systems clients . In other embodiments the computer system may be implemented as a desktop computer portable computer laptop or notebook computer tablet computer pocket computer telephone smart phone or any other suitable type of electronic device.

The services tier may include the performance analyzer which may collect data from a data flow of a data integration job execution in the engines tier . The performance analyzer may collect a job execution plan and design metadata performance monitoring data and system resource utilization data from the engines tier where the data integration is occurring. The services tier may also receive requests from the user in the web clients on which data it needs to receive from the engines tier and commands to send to the engines tier . The services tier may be on a different server than the engines tier the services tier and the engines tier may communicate over a network.

The engine tier may be where the data integration job is being performed. A service provider agent may be in communication with the performance analyzer of the services tier . The service provider may receive requests for data from the performance analyzer retrieve the data that is requested by the performance analyzer and send the data to the performances analyzer over the network when the service tier and engines tier are on different servers. Within the engines tier the service provider may be in communication with a performance monitor and a resource tracker through respective sockets.

The performance monitor may gather real time performance data of the data flow of one or more parallel processes of the data integration job. The gathering of performance data may be done at a tunable interval which may be based on every N records or every N seconds for example. The parallel processes may be multiple jobs being performed in multiple environments. The multiple environments may be different retargetable engines such as a parallel engine or a distributed engine. The performance monitor may also store the real time performance data from the parallel processes in a performance data database for later use by serializing the data.

The resource tracker may receive dynamic real time system resource utilization data. The resource tracker may also store system resource utilization data in a system resource utilization database by serializing the data. The resource tracker may be in communication with the retargetable engines . The resource tracker may be independent of the engine with which it communicates so the resource tracker may be used to monitor resource utilization for both parallel and distributed environments. Log files may also be produced by the parallel processes and may be requested by the performance analyzer and used by the performance analyzer to determine areas of the data flow where data integration issues occur.

In various embodiments the performance analyzer may be requesting gathering and analyzing data in real time while in an on line mode. In various embodiments the performance analyzer may be used in an offline mode where the performance analyzer receives past data of a data integration job execution by loading performance data from the performance data database system resource data form the system resource utilization database and log files for example. The performance analyzer should be able to replay the job execution in a simulated mode so that users can understand how the job transitions its state and correlates state transitions to the job logs. Also in offline mode the performance analyzer may build an execution plan representation and create snapshots to show job execution progress at various time intervals. The offline mode may be useful for post execution remote analysis. For example the user may send all the files to technical support. A support engineer may import all the files to the performance analyzer and conduct post execution analysis to identify any performance patterns or issues.

In the on line real time analysis done the by the performance analyzer a request may be sent from the web client to the performance monitor to receive an execution plan and metadata information of the job execution. The performance analyzer uses this information to build the data flow representation. The performance analyzer then regularly polls performance data and resource utilization data to show job execution progress. The performance analyzer may be used to analyze the performance for job executions on different types of retargetable engines e.g. parallel engine or distributed engine .

In various embodiments the data collector may receive data from the engines tier of . The data may include performance data and resource utilization data in the online mode. In an offline mode the data collector may receive performance data from the performance data persistence job execution log files and system resource utilization data from the system resource utilization persistence . The data collector may send the data collected to the data analyzer . The data analyzer may determine which data to send to the performance analysis module and the data visualization module .

The performance analysis module may analyze the data coming into the performance analyzer with one or more rules from the rules engine . The rules engine may have one or more rules that may define certain actions to take when data shows a certain criteria. The rules may be flexible in that an operator may define the rules. Based on the analysis of the rules and the data the performance analysis module may produce the report . The report may include recommendations that a user may take to correct performance issues. The recommendations may be accessible to a user in the performance analyzer graphical user interface of in the job design recommendations view .

In various embodiments the data analyzer may send data to the data visualization module . The data visualization module may compile the data into visual displays such as a statistic display . The statistic display may display through the performance analyzer GUI graphical execution plan the process tree and resource usage . These visual displays may assist the user in determining corrective action for performance issues and where the performance issues are occurring.

In operation the performance analyzer may determine from the data of the data integration job whether there is a performance issue or not. If there is no performance issue then the performance analyzer may visually display the data of the data integration in operation and the method may end. If there is a performance issue then the performance analyzer may analyze the data it receives from the data integration in operation . The data analyzer of the performance analyzer may determine what is causing the issue based on rules. The data analyzer may generate recommendations for the job design of the data integration job in operation which is also based on the rules the issue from the analysis and data. The method may continue with operation . In operation the performance analyzer may visually display data and any recommendations the performance analyzer determined. The method may continue to operation . In operation the performance analyzer may determine whether the data integration job is still performing. If it is not then the method may end. If the data integration job is still running then the method may return to operation to gather more data from the data integration job and analyze it.

Returning to operation if the performance analyzer is in an offline mode then the method may continue with operation . In operation the performance analyzer may receive persistent data of a previously executed job. The persistent data may include the performance data and resource utilization data. In operation the performance analyzer may determine from the persistent data whether there is a performance issue with the past job execution being analyzed. If there is a performance issue then the method may continue to operation . In operation the performance analyzer may analyze the data. In operation the performance analyzer may generate job design recommendations based on the persistent data and rules. The method may continue with operation . In operation the performance analyzer may visually display data and any recommendations the performance analyzer determined.

Returning to operation if there is no performance issue with the past job execution then the method may continue with operation . In operation the performance analyzer may visually display data for a user to view. The method may continue to operation . In operation the performance analyzer may determine whether the data integration job is still performing. If it is not then the method may end. If the data integration job is still running then the method may return to operation to gather more data from the data integration job and analyze it.

One of the views in the top panel of the GUI may be the Execution Plan . The actual job execution plan may be different from the job flow in the design environment. The Execution Plan may illustrate the designed job flow which may be different than the actual job flow. The Processes view may display the actual job flow. There are several factors that may cause the actual job flow to differ from the design job flow. One factor may be partitioning and sort insertion. To satisfy parallel job design semantics or to avoid hanging process for certain job design patterns e.g. fork join pattern the parallel framework may insert partitioners sort or buffer operators to the user specified data flow. Another factor that may cause the actual job flow to differ from the design job flow may be operator combining. Two or more combinable operators may be combined into a single process to improve efficiency and reduce resource usage. Another factor may be composite operators. One single stage operator may be expanded into multiple sub level operators to implement processing logic. Parallelism is another factor. A parallel stage may have multiple player instances to implement data partitioned parallelism.

Monitoring top level design job flow may not provide enough information to understand what has actually happened during the job execution because of the differences between the design job flow and the actual executed job flow. Being able to monitor low level runtime execution plan to track the performance of each operator of the job while the job is executing may be useful to the user. By selecting the Execution Plan the designed job flow execution may be presented. In embodiments the Execution Plan may be displayed as a direct acyclic graph where the data flow order is from left to right e.g. the data source is presented on the left side and the target is presented on the right side.

A Process Logs tab may also be in the bottom panel . The Process Logs tab may display the messages produced by the selected operator. Furthermore the bottom panel may display a number of job execution statistics. The job execution statistics may include the number of input records consumed number of output records produced input throughput output throughput CPU utilization memory utilization I O utilization disk space utilization and scratch space utilization for example. The partitions of each operator may be changed with a partition view tab . By selecting the partition of an operator if any the performance information discussed above may be displayed for the selected partition of the selected operator.

The Process view may also include several other menu items in menu . Some of the other items may be but not limited to Switch to Execution Plan Dump Stack Dump Core Terminate Process and Export Process. The Switch to Execution Plan option may allow a user to switch to the operator in the execution plan that correlates to a selected process entry. The Terminate Process option may terminate an executing process. The Export Process option may persist the job process structure into plain text HTML or XML files for further analysis.

The Dump Stack option when selected may send a signal to an executing process to dump its execution stack. If the executing process is a player process leaf node then the Dump Stack option may cause the player process to dump its stack. If the executing process selected is a section leader process then selecting the Dump Stack option may trigger the section leader plus any children process entries to dump their stacks. If the executing process is the conductor process root node the Dump Stack option triggers all of the executing processes associated with the job to dump their stacks.

The Dump Core option when selected may send a signal to an executing process to dump its process image. If the executing process is a player process leaf node then the Dump Stack option may cause the player process to dump its process image. If the executing process selected is a section leader process then selecting the Dump Stack option may trigger the section leader plus any children process entries to dump their process images. If the executing process is the conductor process root node the Dump Stack option triggers all of the executing processes associated with the job to dump their process images.

By selecting a process entry in the process tree structure the information related to the process entry may be displayed in the bottom panel . The information may include environmental variables input schemas output schemas and process logs. Respective Environments tab Input Schemas tab Output Schemas tab and Process Logs tab may allow the user to select the information of the process entry to view. Other information not limited to the information that is displayed for operators in could also be displayed for the process entries.

Also illustrated in and is an option to select the Recommendations view . As illustrated in the performance analyzer may generate a report that contains the recommendations on how to optimize job design for better performance. The performance analyzer may analyze performance results and may provide a set of selective changes for each issue identified. One recommendation may be about flow pattern validation. The performance analyzer may analyze the performance results and correlates those results with the data flow design to help identify potential deadlocks incorrect fork join patterns unnecessary split and merge unneeded stages stages that may be simplified or replaced optimal break down sub flows and use of shared local containers for example.

Other sample rules that may be used in making recommendation decisions include parallel configuration buffer tuning partitioning and sort insertion operator combining and operator selection. Parallel configuration recommendations include the number of logical and physical partitions node pools and node constraints. For a job with a large number of stages one node logical node configuration could lead to a large number of processes at runtime. If multiple logical nodes are used performance could degrade due to resource contention. An optimal design may be to reduce the number of logical partitions and increase the number of physical partition if possible. For a small job that executes across multiple physical partitions it may be optimal to increase the logical partition and keep all the logical partitions on the same server to minimize data transport across the network.

Buffer tuning such as buffer insertion and parameter settings may be a recommendation. Buffer tuning may help avoid deadlock situations caused by fork join patterns or remove bottlenecks. Recommendations would let the user know where why and how to turn on buffering.

Partitioning and sort insertions may be yet another recommendation which may include partitioning method consolidation and sort keys decomposition. An example is to partition and sort data upfront at the start of the data flow and keep the partitioned data and sort order up to the point where needed. This may be better than using inconsistent keys through the entire data flow as data has to be repartitioned and sorted wherever keys are changed. Another example is to refine partitioning keys. If data is skewed across partitions for a particular stage once can consider modifying the keys used to partition the data by adding more keys or picking up different keys that can lead to even data distribution across all of the partitions.

Recommendations may also be made for operator combining. The recommendation may determine whether or not to enable or disable operator combining based on but not limited to the number of processes executing on the system CPU utilization I O throughput and disk space usage. For example if multiple sort operators are combined to execute in the same process then only one sort may execute at a time. All other sorts are blocked for receiving more input. It may be optimal to disable sorts if the input data volume is large so that disk I O will be better utilized. If input data volume is small combining sorts would probably be acceptable as sorting is most likely done in memory. Another example may be connectivity operators. If a database operator is combined with upstream processing operators and the database legacy slows down the entire process then the combination for the database operator may be disabled so upstream processing operators may be impacted less by the database performance.

Another recommendation may be the selection of an operator. It may be important to understand performance characteristics of an operator in terms of CPU memory I O. or disk space utilization. This may help select an operator that can fulfill a specific application need. For example lookup join or merge can all combine data from different streams into one stream but each operator may have its own performance characteristics. Lookup may work better for small sharable tables. Lookup may also be updatable and range lookup capabilities which join and merger do not have. Join may work well for large size reference table and merge may work well for multiple tables. Another example may be choosing modify filter transform. Using modify or filter may achieve data conversions and constraints which are available in transform but with less overhead. On the other hand transform has looping capabilities and more advanced transformation logic. In other examples users may need to choose between using multiple lookup tables within one lookup versus one lookup table with multiple lookups and using sequential files over parallel datasets.

To make these recommendations the performance analyzer may support pluggable rules the performance analyzer may follow when making recommendations. Any stage written in application programming interfaces API executing on parallel framework may provide a pre defined set of rules to inform Performance Analyzer how to make recommendations based on some specific performance data. For example the rules used to define a bottleneck may be pluggable. On may use the relative throughput or an absolute throughput to define a bottleneck. An absolute throughput is a hard limit such as the number of records per second. By default there may be a job wide hard limit any operator whose throughput is below the hard limit may be considered a bottleneck. An operator with specific processing need may override this hard limit by calling a rule API. A relative throughput is throughput in percentage compared to other operators. An example of a rule may be that if the throughput of an operator is 50 less than its producing operator then this operator may be considered a bottleneck.

Furthermore in and is a Resource Usage view . The Resource Usage view may display a number of charts or graphs for but not limited to system CPU usage I O usage and memory usage. It may display information collected and analyzed by the performance analyzer . The Resource Usage view may be used by the user to evaluate the actual job flow.

Referring back to embodiments may be a system a method and or a computer program product. The computer program product may include a computer readable storage medium or media having computer readable program instructions thereon for causing a processor to carry out aspects of the present invention.

The computer readable storage medium can be a tangible device that can retain and store instructions for use by an instruction execution device. The computer readable storage medium may be for example but is not limited to an electronic storage device a magnetic storage device an optical storage device an electromagnetic storage device a semiconductor storage device or any suitable combination of the foregoing. A non exhaustive list of more specific examples of the computer readable storage medium includes the following a portable computer diskette a hard disk a random access memory RAM a read only memory ROM an erasable programmable read only memory EPROM or Flash memory a static random access memory SRAM a portable compact disc read only memory CD ROM a digital versatile disk DVD a memory stick a floppy disk a mechanically encoded device such as punch cards or raised structures in a groove having instructions recorded thereon and any suitable combination of the foregoing. A computer readable storage medium as used herein is not to be construed as being transitory signals per se such as radio waves or other freely propagating electromagnetic waves electromagnetic waves propagating through a waveguide or other transmission media e.g. light pulses passing through a fiber optic cable or electrical signals transmitted through a wire.

Computer readable program instructions described herein can be downloaded to respective computing processing devices from a computer readable storage medium or to an external computer or external storage device via a network for example the Internet a local area network a wide area network and or a wireless network. The network may comprise copper transmission cables optical transmission fibers wireless transmission routers firewalls switches gateway computers and or edge servers. A network adapter card or network interface in each computing processing device receives computer readable program instructions from the network and forwards the computer readable program instructions for storage in a computer readable storage medium within the respective computing processing device.

Computer readable program instructions for carrying out operations of the present invention may be assembler instructions instruction set architecture ISA instructions machine instructions machine dependent instructions microcode firmware instructions state setting data or either source code or object code written in any combination of one or more programming languages including an object oriented programming language such as Java Smalltalk C or the like and conventional procedural programming languages such as the C programming language or similar programming languages. The computer readable program instructions may execute entirely on the user s computer partly on the user s computer as a stand alone software package partly on the user s computer and partly on a remote computer or entirely on the remote computer or server. In the latter scenario the remote computer may be connected to the user s computer through any type of network including a local area network LAN or a wide area network WAN or the connection may be made to an external computer for example through the Internet using an Internet Service Provider . In some embodiments electronic circuitry including for example programmable logic circuitry field programmable gate arrays FPGA or programmable logic arrays PLA may execute the computer readable program instructions by utilizing state information of the computer readable program instructions to personalize the electronic circuitry in order to perform aspects of the present invention.

Aspects of the present invention are described herein with reference to flowchart illustrations and or block diagrams of methods apparatus systems and computer program products according to embodiments of the invention. It will be understood that each block of the flowchart illustrations and or block diagrams and combinations of blocks in the flowchart illustrations and or block diagrams can be implemented by computer readable program instructions.

These computer readable program instructions may be provided to a processor of a general purpose computer special purpose computer or other programmable data processing apparatus to produce a machine such that the instructions which execute via the processor of the computer or other programmable data processing apparatus create means for implementing the functions acts specified in the flowchart and or block diagram block or blocks. These computer readable program instructions may also be stored in a computer readable storage medium that can direct a computer a programmable data processing apparatus and or other devices to function in a particular manner such that the computer readable storage medium having instructions stored therein comprises an article of manufacture including instructions which implement aspects of the function act specified in the flowchart and or block diagram block or blocks.

The computer readable program instructions may also be loaded onto a computer other programmable data processing apparatus or other device to cause a series of operational steps to be performed on the computer other programmable apparatus or other device to produce a computer implemented process such that the instructions which execute on the computer other programmable apparatus or other device implement the functions acts specified in the flowchart and or block diagram block or blocks.

The flowchart and block diagrams in the Figures illustrate the architecture functionality and operation of possible implementations of systems methods and computer program products according to various embodiments of the present invention. In this regard each block in the flowchart or block diagrams may represent a module segment or portion of instructions which comprises one or more executable instructions for implementing the specified logical function s . In some alternative implementations the functions noted in the block may occur out of the order noted in the figures. For example two blocks shown in succession may in fact be executed substantially concurrently or the blocks may sometimes be executed in the reverse order depending upon the functionality involved. It will also be noted that each block of the block diagrams and or flowchart illustration and combinations of blocks in the block diagrams and or flowchart illustration can be implemented by special purpose hardware based systems that perform the specified functions or acts or carry out combinations of special purpose hardware and computer instructions.

The descriptions of the various embodiments of the present disclosure have been presented for purposes of illustration but are not intended to be exhaustive or limited to the embodiments disclosed. Many modifications and variations will be apparent to those of ordinary skill in the art without departing from the scope and spirit of the described embodiments. The terminology used herein was chosen to best explain the principles of the embodiments the practical application or technical improvement over technologies found in the marketplace or to enable others of ordinary skill in the art to understand the embodiments disclosed herein.

