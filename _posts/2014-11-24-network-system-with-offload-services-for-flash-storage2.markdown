---

title: Network system with offload services for flash storage
abstract: A system is provided comprising: a packet routing network; Flash storage circuitry; a management processor coupled as an endpoint to the network; an input/output (I/O) circuit coupled as an endpoint to the network; a packet processing circuit coupled as an endpoint to the network; and a RAID management circuit coupled as an endpoint to the network and configured to send and receive packets to and from the Flash storage circuitry; wherein the management processor is configured to determine routing of packets among the I/O circuit, packet processing circuit and RAID management circuit.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09608936&OS=09608936&RS=09608936
owner: Sanmina Corporation
number: 09608936
owner_city: San Jose
owner_country: US
publication_date: 20141124
---
This Application is a Continuation Application of U.S. application Ser. No. 14 324 028 filed Jul. 3 2014 the contents of which are hereby incorporated by reference in its entirety and the benefit of priority is claimed herein.

The large amounts of information generated daily challenge data handling facilities as never before. In the context of today s information generation data is being generated at rates perhaps thousands or tens of thousands of times greater than was the data generation rate in the 1990s. Historically large volumes of data sparked explosive growth in data communications. Responses to growing amounts of data generation centered on improving the movement of data based in increased transmission data rates to enhance throughput in communication channels. For instance transmission pipelines grew from a few tens of megabits per second Mb s transmission rates to several tens of gigabits per second Gb s rates during the 1990s.

In the same period typical storage devices such as hard disk drives HDDs when amassed in sufficient numbers might accommodate large volumes of data but the rates at which data could be stored and retrieved have not scaled at the same rate as the volume of data stored on the devices has increased. Data access rates for HDDs are at similar orders of magnitude today as they were in the 90s.

Fundamental storage subsystems have not integrated technology to enable scaling of effective data storage at the same rate that data generation is growing. Hence the challenge to systems handling large volumes of data is not likely to be alleviated by the combination of contemporary HDD technology with high speed data transmission channels. In order to handle and manage big data information processing facilities will be pressured to utilize larger volumes of storage with higher performance rates for capturing and accessing data.

The following description is presented to enable any person skilled in the art to create and use a computer system that provides high speed access to data storage devices particularly Flash storage devices. Various modifications to the embodiments will be readily apparent to those skilled in the art and the generic principles defined herein may be applied to other embodiments and applications without departing from the spirit and scope of the invention. Moreover in the following description numerous details are set forth for the purpose of explanation. However one of ordinary skill in the art will realize that the invention might be practiced without the use of these specific details. In other instances well known data structures and processes are shown in block diagram form in order not to obscure the description of the invention with unnecessary detail. Identical reference numerals may be used to represent different views of the same item in different drawings. Flow diagrams in drawings referenced below are used to represent processes. A computer system is configured to perform some of these processes. The flow diagrams that represent computer implemented processes include modules that represent the configuration of a computer system according to computer program code to perform the acts described with reference to these modules. Thus the present invention is not intended to be limited to the embodiments shown but is to be accorded the widest scope consistent with the principles and features disclosed herein.

In accordance with some embodiments the network fabric of the first and second packet routing networks is compliant with the PCI Express Base Specification hereinafter PCIe released by the PCISIG PCI Special Interest Group . See PCI Express Technology Comprehensive Guide to Generations 1. 2. and 3.0 by M. Jackson and R. Budruk 2102 Mindshare Inc. PCIe specifies point to point bidirectional serial communication paths between endpoints over switches and connection lines. Information is transmitted in packets between endpoints over the routing networks . A PCIe network includes serial connection lines commonly referred to as links that are capable of sending and receiving information at the same time. More specifically information transmitted through either one or the other of the routing networks is encapsulated in packets that include routing information that indicates a source endpoint and a destination endpoint. According to the PCIe specification in accordance with some embodiments a link can include one or more serial transmit and serial receive connection pairs. Each individual pair is referred to as a lane . A link can be made up of multiple lanes. Each lane uses differential signaling sending both positive and negative versions of the same signal. Advantages of differential signaling include improved noise immunity and reduced signal voltage. Each endpoint device coupled to one or both of the routing networks includes core logic that implements one or more functions. A device that is a component of a typical PCIe compliant network can have multiple functions up to eight in some embodiments each implementing its own configuration space.

Referring again to the first management processor is used to configure the first packet routing network circuit to provide point to point communication between components operably coupled to it. The second management processor is used to configure the second packet routing network circuit to provide point to point communication between components operably coupled to it. In some embodiments the first and second management processors configure point to point routing within the first and second packet routing networks. In other words for a given pair of resource circuits a fixed route among switches in the internal network circuits or is configured to transmit packets between the pair.

The PCIe specification specifies use of a root complex to configure a PCIe compliant network. A root complex includes interface circuitry e.g. processor interface DRAM interface that couples a management processor and the rest of a PCIe network. Management processor includes first and second root complexes that act as interfaces between processor and network circuits and . Management processor includes second and third root complexes that act as interfaces between processor and network circuits and . The term root is used to indicate that the root complex is disposed at a root of an inverted tree topology that is characteristic of a hierarchical PCIe compliant network.

Referring again to The I O interface circuits to provide high speed connections between the external network e.g. InfiniBand Fibre Channel and or Ethernet and the first switch network circuitry . The I O circuitry provides protocol conversion including packet format conversion during high speed data communication between the external network and the first switch network circuitry . In some embodiments the external network I O interface circuits to are implemented as network interface cards commonly referred to as NICs which include circuits that are configured to transform packets to suitable formats as they pass between the external network and the routing networks .

The storage I O interface circuits to manage the distribution of data across the Flash storage circuits to . In some embodiments the storage I O interface circuits are configured to implement a file system used to control how data is stored in and retrieved from storage devices. In some embodiments the storage I O interface circuits to are implemented as RAID controllers configured to organize data across multiple storage devices such as Flash storage devices to . The term RAID refers to data storage schemes that combine multiple disk drive components into a logical unit for the purposes of data redundancy and performance improvement. Persons skilled in the art will appreciate that Flash storage sometimes referred to as solid state drive SSD is a data storage device using integrated circuit assemblies as memory to store data persistently. Each of the storage access switch networks to provides point to point connections to respectively using a serial protocol that moves data to and from the Flash storage devices to . In some embodiments the storage access switch networks to use a protocol that includes the SAS Serial Attached SCSI protocol. In general according to the SAS protocol there are three types of SAS devices initiators e.g. RAID controllers target storage devices e.g. Flash circuits and expanders. An initiator device attaches to one or more target storage devices to create a SAS domain. In some embodiments the storage I O interface circuits implemented as RAID controllers act as SAS initiators. In accordance with some embodiments the Flash storage circuits to act as SAS targets. Using expanders e.g. low cost high speed switches the number of targets attached to an initiator can be increased to create a larger SAS domain.

Communication paths couple storage I O interface circuit to exchange data with storage access switch networks and . Communication paths couple storage I O interface circuit to exchange data with storage access switch circuits and . Communication paths couple storage I O interface circuit to exchange data with storage access network circuits and . Communication paths couple storage I O interface circuit to exchange data with storage access switch networks and . Thus all Flash circuits to are accessible via the first internal network circuit via the storage I O interface circuits coupled to it and all Flash circuits to are accessible via the second internal network circuit via the storage I O interface circuits coupled to it.

In some embodiments the first and second packet processing circuits are implemented as field programmable gate array FPGAs . The first programmable logic circuit is operably coupled to first cache circuit . The second programmable logic circuit is operably coupled to second cache circuit . In some embodiments the first and second cache circuits include DRAM circuits. More particularly in some embodiments the first and second cache circuits include Flash backed DRAM circuits in which Flash circuits are coupled to stored data persistently in the event of failure of a corresponding DRAM circuit. The first and second packet processing circuits also are directly coupled to each other so that the same data can be cached at both. In some embodiments a communication path coupling the first and second programmable logic circuits includes a circuit connection compliant with a high speed network communication protocol. In some embodiments the communication path complies with the Ethernet protocol.

FPGA circuitry often can impart services with less latency delay and therefore faster than a typical general purpose management processor for example since the programmable logic can be programmed in advance to dedicate specific hardware circuitry to provide the services. Programmable hardware logic such as FPGA circuitry often can perform operations faster than a general purpose processor for example which often use software interrupts often to transition between different operations. Alternatively in accordance with some embodiments one or more of the packet processing circuits can include a special purpose processor an application specific integrated circuit ASIC or an array of processors configured to run software to perform a given service.

Each of the cache interface cache control interface PCI interface and ring s or crossbar s comprises circuitry including logic in some instances specifically configured to perform functionalities and operations as described in more detail below. The circuitry may comprise for example a plurality of logic components also referred to as logic blocks interconnects memory elements e.g. flip flops or memory blocks CPUs and other circuit elements. All or just a portion of the packet processing circuits may be reconfigurable circuitry hardware. For example the cache control may be reconfigurable circuitry hardware while the other components of the packet processing circuits comprise non reconfigurable circuitry hardware. The packet processing circuits may comprise an application specific integrated circuit ASIC or an integrated circuit IC component in general.

Although not shown the packet processing circuits may include additional memory CPU and or processors to facilitate functionalities and operations of any of the cache interface cache control inter FPGA interface PCI interface and or ring s or crossbar s .

Some embodiments of the network storage system are configured to provide a cache management service in order to handle data requests from the I O circuits at a faster rate than can be performed using HDDs flash based memory or software. The packet processing circuits cache modules and tags and matching logic modules facilitate faster handling of data communications using hardware circuitry. In some embodiments the respective cache controls only one shown included in the respective packet processing circuits controls data to and from the cache modules via the cache interface and ring s or crossbar s also included in the packet processing circuits . The respective cache controls included in the packet processing circuits also communicate with respective tags and matching logic modules only one shown to control the cache modules . Similarly respective the cache controls included in the respective packet processing circuits controls data to and from the cache modules via the cache interface and ring s or crossbar s . The packet processing circuits in particular cache interface and cache control cache modules and tags and matching logic modules are collectively referred to as the cache management subsystem.

From the perspective of the I O circuits to it appears that all the data it needs to access and or all the data it needs to store in the system are provided from the cache modules and or . Thus the cache modules appear to be an infinite cache. This is the case even though the flash modules to are the primary storage elements for the system and the capacity of the cache modules is insufficient to hold all the data stored in the system .

The cache management subsystem is implemented within the system to take advantage of features such as the access times for the cache modules which include DRAMs being approximately a thousand times or so faster than for the flash circuits to . And unlike flash which is limited to a certain number of writes before it degrades DRAMs do not degrade. In some embodiments the cache control may comprise reconfigurable circuitry hardware. In other embodiments the cache control may comprise non reconfigurable circuitry hardware.

The tags and matching logic modules comprise hardware circuitry configured to hold at least a cache table or similar data structure information that correlates memory locations of the flash circuits to to memory locations of the cache modules respectively. In some embodiments the tags and matching logic modules comprise a type of hardware circuitry e.g. TCAM capable of very rapid searching or lookup of data stored within it.

Each row of the cache lines may indicate among other things one or more of whether data is stored in that cache line data area what portion of that cache line data area is empty or occupied information about the stored data and other information relating to the stored data and or use of the cache line data area. Data stored in a given cache line is associated with a unique cache tag also referred to as a tag that serves as an identifier or name for the data and or the particular cache line. Each cache tag comprises one or more pieces of information including but not limited to the flash memory address corresponding to the data associated with the given cache tag. For example the flash memory address may be included in an I O request originating from one of the I O circuits to . As another example if the particular data was obtained from the flash modules and then stored on the cache modules the particular flash memory location s from which the data was taken is reflected in the corresponding cache tag. The cache tags are searched or looked up to determine whether data associated with a given tag resides in the cache modules .

Lastly the data stored in each of a given cache line also has associated with it one or more tracking metrics such as but not limited to age e.g. when the data was written to the cache modules measured in number of CPU cycles number of read requests for that data number of write requests for that data user specified information e.g. data will be rarely accessed data is to be stored in archival area of flash circuits to often used data etc. system known information e.g. reconfigurable application engine knows that the output data is generates will be rarely accessed data and other data use information that can be tracked for statistical and or cache management purposes. Tracking metrics may also be referred to as data counters.

A backup copy of the cache table is maintained at all times within the system . For instance if the tags and matching logic modules comprise TCAMs then in the case of power failure the cache table will be lost. To ensure against such a scenario a duplicate copy of the cache table can be maintained within the packet processing circuits .

Referring to module determines whether it is time to perform cache capacity maintenance. Maintenance may be a continuous background operation a periodic background operation or on a need basis type of operation. Maintenance frequency can be a system setting user setting or dynamic setting based on current operating conditions of the system . If maintenance is initiated yes branch of block then the cache control determines whether the current data storage capacity of the cache modules or depending on which set of cache modules is associated with the given cache control is at or above a pre set maximum capacity level block . The pre set maximum capacity level is a certain value that is pre set by the system or user and represents the portion of the total data storage capacity of the cache modules that can be occupied while having a safe amount of available space in case for example an unexpectedly large write request is received. Examples of pre set maximum capacity level include but are not limited to 70 80 or some other value. In some embodiments the pre set maximum capacity level may be adjusted over time as more system usage information becomes available. Instead of expressing the pre set maximum capacity level as a percentage of the total data storage capacity for example it is understood that it can be expressed as a minimum available or reserved free space.

If the current cache capacity is below the pre set maximum capacity level no branch of block then the flow diagram returns to block . Otherwise the current cache capacity is too close to the pre set maximum capacity level and some of the stored data needs to be moved to the flash modules and evicted or erased from the cache modules or depending on which set of cache modules is associated with the given cache control yes branch of block .

Next at a block the cache control determines what data to displace from the cache modules or depending on which set of cache modules is associated with the given cache control according to a set of cache eviction rules. In some embodiments the cache eviction rules may comprise an algorithm implemented in software. The cache eviction rules may be set by the system or a user. The cache eviction rules may comprise one or more rules and if it comprises more than one rule rules may have a priority order relative to each other a certain rule may override another rule two rules in combination may override a third rule or the like. Example cache eviction rules comprise without limitation 

The cache control checks the cache table included in its corresponding tags and matching logic module and in particular compares the information provided in the tracking metrics field of the cache table for all cache lines containing data against each other according to the cache eviction rules. In one embodiment the cache eviction rule may comprise evicting data stored in the cache line s that is the least written. In another embodiment the cache eviction rule may comprise evicting data stored in the cache line s that is the least written except for data that is pinned to stay within the cache based on a user specified directive.

Once the cache line s to empty are identified the cache control sends data stored in those cache line s to the flash modules to for storage block . Such data is erased emptied or evicted from those particular cache line s at a block . The flow diagram then returns to block . Thus the cache capacity of cache modules or depending on which set of cache modules is associated with the given cache control is maintained at or below the pre set maximum capacity level. It is understood that blocks and may occur simultaneously of each other.

Next at a block the tags and matching logic module or corresponding to the particular cache control handling the read request performs a look up of its cache table to determine whether the requested data exists in the cache modules . The cache tags are searched to see which one if any contains the same flash memory address location as the particular memory address location provided in the data request. In one embodiment all of the cache tags in the cache table may be searched fully associative . In another embodiment a subset of the cache tags may be searched set associative . In an alternative embodiment a particular one of the cache tags may be searched direct mapped . The tags and matching logic module is configured to perform the look up function several orders of magnitude faster than may be possible if the cache table resides in the packet processing circuits for example. This may be the case even if there are a large number of rows e.g. cache lines in the cache table such as thousands of rows.

If a matching cache tag is found yes branch of block the cache control accesses the data corresponding to the matching cache tag from the cache module and sends the retrieved data to the originating I O circuit block . The retrieved data is the requested data in the read request. The tracking metrics for at least that data is updated in the block . For example the counter for the number of reads of that data may be incremented by one. If the retrieved data was previously written to the cache module in a previous write request and such data was not evicted from the cache module due to cache management operations see then such data is present in the cache module for later access such as the present read request. Then there is no need to retrieve the data from the flash modules to . Data retrieval from a DRAM cache is significantly faster than from flash based memory upwards of a thousand times faster using cache than flash.

If no matching cache tag is found no branch of block the requested data is not present in the cache modules and is retrieved from the flash modules. At a block the cache control initiates retrieval of the requested data from the appropriate flash modules. Next at a block a system setting or user specified setting is checked to see whether the requested data retrieved from one or more of the flash modules to should be copied to the cache modules. If the system is set not to copy to cache modules no branch of block then the flow diagram proceeds to block . Otherwise the retrieved data is copied to the cache modules yes branch of block and block .

The retrieved data is also sent by the cache control to the I O circuit one of to that made the read request block . The cache table is correspondingly updated at a block . Because data is written to particular cache line s of the cache modules that did not exist before the cache tags and cache lines fields for those cache line s are populated accordingly. The associated tracking metrics are also populated at least for example the age field.

Although blocks and are shown prior to block in it is contemplated that block and blocks may be performed simultaneously to each other or in reverse order from that shown in .

At a block the cache control determines whether the data associated with the write request is exceptional. While the default rule is to store all data associated with write requests to the cache modules packet processing circuits and then from the cache modules copy to the flash modules to at some later point in time one or more exceptions to the default rule may be implemented. One or more exception criteria may be a system setting or user specified setting. For example the exception may comprise there being no exception to the default rule. As another example data exceeding a certain size e.g. data that if written to the cache modules may exceed the cache capacity or likely to exceed the pre set maximum capacity level may warrant storing directly in the flash modules without first storing in the cache modules. As still another example the write request or the data associated with the write request itself may specify that the data will be rarely accessed e.g. is archival data or has a certain characteristic that warrants being stored directly in the flash modules to without first being stored in the cache modules .

If the data associated with the write request is determined to be exceptional yes branch of block then the cache control sends such data to be written to the flash modules to block . Otherwise the data associated with the write request is not exceptional no branch of block and operations are performed to write to the cache modules . At a block the tags and matching logic module checks the cache table for a cache tag containing the same flash memory address location as provided in the write request. If a matching cache tag is found yes branch of block this means that an older version of the data associated with the write request or some data in general is currently stored in the cache line s now intended for the data associated with the write request. The cache control facilitates overwriting the existing data at these cache line s with the data associated with the write request block . Then the flow diagram proceeds to block .

If no matching cache tag is found no branch of block then the cache control facilitates writing the data associated with the write request to empty available cache line s in the cache modules block .

Next at a block the data associated with the write request is additionally copied to empty available cache line s in the cache modules associated with the other FPGA packet processing circuit. This mirroring of data between the cache modules occurs via the interface and the crossover path connecting the packet processing circuit to packet processing circuit . In some embodiments block is optional when the crossover path is omitted from the storage system . In other embodiments the mirroring of data associated with the write request in both cache modules and is initiated before the write request is received at a given packet processing circuit. The write request from the I O circuit is split into two identical requests one going to the packet processing circuit and the other to the packet processing circuit . Then the cache control in each of the packet processing circuits can store the data associated with the write request also referred to as write data in its respective cache modules. At a block the cache table included in the tags and matching logic module is updated to reflect the addition of the data associated with the write request into certain cache line s of the cache modules.

Because flash modules to comprise the primary or permanent data storage medium for the storage system the data associated with the write request although already written to the cache modules see blocks and is eventually written to the flash modules to . Nevertheless the cache management subsystem is configured to intelligently perform data writes to the flash modules taking into account the characteristics of the flash modules. In order to prolong the usability of flash modules which are limited to a certain number of writes before degrading the cache management subsystem accumulates certain type of data corresponding to a plurality of write requests and then performs a single write of the accumulated data to flash modules rather than performing a write to flash modules for each write request. This means that if for example there are 25 write requests instead of writing to flash modules 25 times once for each of the 25 write requests the data corresponding to these 25 write requests may be written at the same time and once e.g. a single write operation to the flash modules.

After the data associated with the write request is written to a cache module and cache table updated accordingly the cache control determines whether the data associated with the write request and data associated with a previous write request are associated with consecutive block s of the flash modules to block . Both the data associated with the write request and data associated with a previous write request are handled by the same cache control . If both data are associated with consecutive block s of the flash modules yes branch of block then the cache control waits to write data associated with the write request and the data associated with previous write request to flash modules block . The cache control accumulates data to be written to the flash modules. If the two data are associated with non consecutive block s of flash modules no branch of block then the cache control sends data associated with the previous write request to be written in flash modules block .

Accordingly the cache management subsystem is configured to act as a middleman between the I O circuits to and flash modules to for every read and write requests from the I O circuits. For all read and write requests the presence of data associated with the read or write request in the cache modules is checked before the flash modules are involved. Based on the presence or absence of such data in the cache modules the cache management subsystem performs optimization operations to complete the data requests significantly faster than is possible with flash modules alone. The cache management subsystem also prolongs the useful lifespan of flash modules by minimizing the number of writes to flash modules without sacrificing completeness of data being stored in the flash modules. Data associated with write requests are written to cache modules prior to be written to flash modules unless the data fits an exception. Data associated with read requests that are retrieved from the flash modules may or may not be written to cache modules depends upon system or user setting . Data associated with write requests similarly may or may not be written to cache modules corresponding to both packet processing circuits depends upon system or user setting . The cache management subsystem actively maintains the used storage capacity level of the cache modules at or below a pre set capacity level e.g. 70 80 etc. by evicting data stored in the cache modules that fit one or more eviction rules as needed. An example of an eviction rule comprises evicting data that has the least amount of write activity and moving it to the flash modules .

In accordance with some embodiments the services imparted by the packet processing circuits may alter packet payload content and may include one or more of encryption decryption duplication de duplication compression de compression processing replication and or snapshot for example.

An encryption service can be used for example to encode packet information in such a way that only authorized parties can read it. In a typical encryption scheme for example information referred to as plaintext is encrypted using an encryption algorithm turning it into an unreadable ciphertext. A decryption service provides the reverse of an encryption service. Moreover different styles of encryption and decryption may be provided and each different style may constitute a different service.

A de duplication service also can be used for example to reduce physical space occupied by a data block within a packet. Raw data sometimes contains entire repeated blocks. A common example is an email database in which emails to several individuals contain identical attachments. Some de duplication services keep a lookup table with en entry for each data block seen so far and when it detects duplicate blocks it replaces the duplicate data with a pointer to the data of the first block seen. A duplication service provides the reverse of a de duplication service.

A compression service can be used for example to reduce the physical storage space occupied by a data block within a packet. For example some compression processes recognize patterns within the data and replace raw data with more compact patterns. For example in run length encoding a string of twenty a characters could be replaced by the string which occupies only three characters. A de compression service provides the reverse of a compression service.

In some embodiments services imparted by the packet processing circuits do not alter packet payload content and may include may include cache storage or general parsing services for example. For example parsing services may involve setting up a parsing table paring packets using the parsing table and extracting information fields from packets and acting upon the extracted information. Conversely services may include the reverse of parsing in which packet generation tables are set up and packets are generated from input data fields combined with instructions in the packet generation tables. Services may include counting services in which a programmable logic resource circuit is informed of events in the system such as packets read written or bad packets or packet latency times and using these events it updates internal counters and later responds to queries by delivering the counters. Moreover different styles of compression and de compression may be provided and each different style may constitute a different service.

A replication service can be used to broadcast a packet to multiple storage sites for high availability for example. A packet may be received that specifies a write of data to a particular LUN for example. A replication service can recognize that the data should be written to multiple LUNs rather than only to the specified LUN. The replication service can create multiple different packets each containing header information to designate a write of the data to a different LUN located at a different storage site not shown . The replication service can cause the multiple different packets to be broadcast to geographically dispersed storage sites so as to provide backup storage of the data and or so as to store the data at a site that is physically closer to where a user likely to use it is located.

A snapshot service can be used to capture additional writes to a LUN that occur while a LUN is being backed up for example. The data in a LUN may be backed up periodically to a different storage location for example. During the backup operation new data may be written to the LUN. A snapshot service creates a snapshot i.e. a copy of the LUN at the moment that the backup operation begins and during the course of the backup operation new write data is written to the snapshot rather than to the LUN that is being backed up. Upon completion of the backup blocks e.g. Logical Block Addresses within the snapshot that are written to during the snapshot are copied to the backup storage and also to the LUN that has been backed up. Thus backup can proceed without loss of write data received during the backup.

In accordance with some embodiments the management processors manage routing of packets over the routing networks to and from the I O circuits to packet processing circuits and the RAID I O circuits to . The receive and transmit ring buffers are used in some embodiments to synchronize the flow of data packets during direct memory access DMA transmission of information to and from each of the I O circuits to packet processing circuits and the RAID I O circuits to and the local memory or which are accessed by the management processor .

Persons skilled in the art will understand that a typical ring buffer includes a FIFO data structure that defines a circular queue. In general a ring buffer is used to serialize data from one process to another process. The serialization allows some elasticity in time between the processes. A ring buffer typically has two indices to storage elements in the queue a head and a tail indicated for the illustrative ring buffers of . In operation data is inserted at a ring buffer element indicated by the head index to be the head element and data is removed from a ring buffer element indicated by the tail index to be the tail element. Initially the head and tail have the same buffer location value typically zero indicating that the ring buffer is empty. As the buffer fills up with data the tail pulls away from the head. The ring buffer will be full when the tail gets so far ahead of the head that it wraps around and catches up to the head. As the head catches up to the tail the buffer empties. The ring buffer will be empty once again when the head completely overtakes the tail.

In operation when the first endpoint operates as a producer and the second endpoint operates as a consumer the first endpoint provides one or more data packets to the first output queue which outputs the one or more data packets to the head of the transport ring buffer and transport ring buffer provides the one or more data packets at its tail to the second input queue which inputs the packet to the second endpoint . Conversely when the first endpoint operates as a consumer and the second endpoint operates as a producer the second endpoint provides one or more data packets to the second output queue which outputs the one or more data packets to the head of the receive ring buffer and receive ring buffer provides the one or more data packets to the first input queue which inputs the packet to the first endpoint.

Still referring to assume that a packet containing a read request is transmitted from I O circuit over the routing network to the management processor . Assume that in response to the read request the management processor determines to send the packet over the network to the RAID controller to request the read. The RAID controller manages reading the requested data from one or more Flash circuits to . In response to obtaining the read data the RAID controller sends one or more packets contacting the read data to the management processor. Next the management processor determines to send the one or more data packets to the packet processing circuit which is configured to impart a service. The imparted service can include encryption decryption duplication de duplication compression de compression replication snapshot for example. After imparting the service to the one or more read data packets the processing circuit sends the one or more read data packets over the routing network to the management processor . Next the management processor determines to transmit the one or more read data packets over the network to the I O circuit .

For example as illustrated in the routing examples of packets may be routed to a packet processing circuit or not depending upon a determination of whether a service is to be provided. Module creates a return record in local memory or to indicate a next endpoint destination for the packet or a responsive packet. For example referring to return information for a read request packet received by the management processor from the I O circuit that is transmitted by the management processor to the RAID I O may include an indication of a next hop address indicating the packet processing circuit . Alternatively referring to for example return information for a write request packet received by the management processor from the I O circuit that is transmitted by the management processor to the packet processing circuit may include a next hop address indicating the RAID I O . Module places the modified packet onto a corresponding transmit ring buffer e.g. as represented by transport ring buffer in .

Although the present invention has been described in connection with some embodiments it is not intended to be limited to the specific form set forth herein. One skilled in the art would recognize that various features of the described embodiments may be combined in accordance with the invention. Moreover it will be appreciated that various modifications and alterations may be made by those skilled in the art without departing from the scope of the invention.

