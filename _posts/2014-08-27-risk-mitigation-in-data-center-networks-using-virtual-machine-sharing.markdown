---

title: Risk mitigation in data center networks using virtual machine sharing
abstract: A method employing resource orchestration algorithms may find a fewest number of working data centers (DCs) to guarantee K-connect survivability and a fewest number of virtual machines (VMs) among the DCs using an overlay network representing a physical optical network. The overlay network may exclude certain topological features of the physical optical network. An intra-request VM sharing method may share VMs among DCs allocated for an aggregation request. An intra-request VM sharing method may share VMs among DCs represented in the overlay network and among multiple aggregation requests.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09503367&OS=09503367&RS=09503367
owner: Fujitsu Limited
number: 09503367
owner_city: Kawasaki
owner_country: JP
publication_date: 20140827
---
This application is related to U.S. application Ser. No. 14 102 313 filed Dec. 10 2013 which is hereby incorporated by reference.

The present disclosure relates generally to data center networks and more particularly to risk mitigation in data center networks using virtual machine sharing.

As more applications and workloads are moving to online network computing resources also generally referred to as the cloud geographically distributed data centers DCs are being deployed across wide area networks including optical networks. Such data centers may provide various instances of virtual machines VMs that may individually instantiate a computing environment such as a server operating system for example. Cloud applications may rely on distributed DCs for improved user experience. However some cloud service providers may not own optical network infrastructure and may count on network providers to optically interconnect distributed DCs. Some network providers may be unwilling and or unable to expose their full network topology information to cloud service providers.

Many cloud applications in distributed DCs are arranged in an aggregation communication pattern whereby an aggregation DC collects data processed at distributed DCs and outputs final results to users. Cloud applications can make physically dispersed VMs operate logically as one DC by collecting results from dispersed VMs at an aggregation DC. Other applications such as cloud search and data backup for example can allocate VMs close to data stored in distributed DCs and provide results at an aggregation DC for access by users. In certain instances complicated communication patterns can be constituted by scheduling a sequence of data aggregations.

Due to the reliance on distributed DCs and aggregation DCs survivability in the face of various risks such as network outages DC failure s and or equipment failure among other examples is becoming an important issue for cloud applications. Accordingly there is a need in the art for an overlay framework that enables cloud service providers to control cloud network connections and optimize resource orchestration yet enables network operators to offer network services while retaining detailed network topology information.

In one aspect a disclosed method includes identifying a smallest M number of data centers DCs in an overlay network for K connect survivability associated with an aggregation request for an aggregation DC and a smallest V number of virtual machines VMs among the M DCs. When K connect survivability is satisfied for the M number of DCs the method may include determining a number of VMs at each of the M DCs including determining a number of working VMs and a number of protecting VMs respectively. The method may also include generating a VM share table according to the aggregation request. A minimum number of shared protecting VMs may be calculated for a set of risks associated with each of the M DCs. The method may further include updating the number of working VMs and the number of protecting VMs based on the minimum number of shared protecting VMs for each of the M DCs. The value K may represent a minimum number of DCs that remain accessible to the aggregation DC. The overlay network may represent a physical network.

In particular embodiments the method operation of determining that K connect survivability is satisfied may include determining the number of VMs at each of the M DCs for the aggregation request calculating a current value of an integrated factor I for each of m DCs that are unselected sorting the m DCs according to I and adding a DC from the m DCs having a lowest value for I to the M DCs.

Additional disclosed aspects for identifying a smallest M number of data centers DCs for K connect survivability include an article of manufacture comprising a non transitory computer readable medium and computer executable instructions stored on the computer readable medium. A further aspect includes a management system comprising a memory a processor coupled to the memory and computer executable instructions stored on the memory.

The object and advantages of the embodiments will be realized and achieved at least by the elements features and combinations particularly pointed out in the claims. It is to be understood that both the foregoing general description and the following detailed description are exemplary and explanatory and are not restrictive of the invention as claimed.

In the following description details are set forth by way of example to facilitate discussion of the disclosed subject matter. It should be apparent to a person of ordinary skill in the field however that the disclosed embodiments are exemplary and not exhaustive of all possible embodiments.

Throughout this disclosure a hyphenated form of a reference numeral refers to a specific instance of an element and the un hyphenated form of the reference numeral refers to the element generically or collectively. Thus as an example not shown in the drawings widget 12 1 refers to an instance of a widget class which may be referred to collectively as widgets 12 and any one of which may be referred to generically as a widget 12 . In the figures and the description like numerals are intended to represent like elements.

In U.S. application Ser. No. 14 102 313 a K connect survivability concept is disclosed that may guarantee resource availability under a wide range of risks for cloud applications. Two resource orchestration schemes are disclosed that may implement the K connect survivability concept in an overlay framework for optical network virtualization a risk based scheme and a delay based scheme. The resource orchestration schemes may identify a fewest number of data centers for guaranteeing K connect survivability where K represents a minimum number of DCs that remain accessible from an aggregation DC.

As described in further detail herein the previous K connect survivability concept is expanded to focus on VM allocation in the overlay framework with the objective of minimizing a total number of VMs allocated to the overlay network. Thus in addition to K connect survivability a given number of VMs connecting to an aggregation DC for any failure at any time is also satisfied by the methods described herein using VM sharing among DCs in the overlay framework. The following parameters in Table 1 which are integers greater than zero are used herein with respect to K connect survivability using VM sharing.

A K connect survivability KCS may be defined by a scenario where at least K number of DCs out of M original working DCs are reachable from an aggregation DC DC for an arbitrary risk such as but not limited to network outages DC failure s and or other types of equipment failure also referred to herein collectively as risk events . For the purposes of the present disclosure it may be assumed that DCdoes not fail. A risk event may result in multiple failures that may occur at DC sites e.g. due to power outages natural disasters and or system maintenance or in networks due to fiber cuts . For cloud applications requesting a fixed number of VMs V K connect survivability using VM sharing may satisfy an aggregation request specifying V while minimizing a total number of VMs allocated to DCs in the overlay framework. It is noted that the number of VMs allocated to the overlay framework may be determinative for a bandwidth of network connections with the overlay framework since each VM may be associated with a certain amount of bandwidth consumption.

As will be described herein an overlay framework is presented that interconnects distributed data centers by virtualized optical networks. Survivable resource orchestration algorithms based on the network information provided by the virtualized optical networks such as shared risk groups SRG and delay are disclosed. The disclosed resource orchestration algorithms may find a fewest number of working DCs to ensure K connect survivability and a fewest overall number of VMs to satisfy the requested value for V. Resource orchestration algorithms disclosed in U.S. application Ser. No. 14 102 313 may provision the fewest number of working DCs based on SRG information provided for overlay networks where physical network topology may be unavailable and routing for connections may not be possible. In addition to satisfying K connect survivability the fewest number of VMs may be allocated within the overlay network to satisfy the requested value for V.

Turning now to the drawings illustrates an example embodiment of overlay framework which may be based on optical network virtualization. In overlay framework is shown including overlay network software defined network SDN application programming interfaces APIs and physical network . As shown overlay network may comprise connections between DCs where a bandwidth of connections may be adjustable using optical network virtualization. In an underlying optical network represented by physical network may be an optical transport network OTN and or a flexible optical data plane e.g. flexible transceivers configured to adjust the bandwidth of connections.

In overlay network is shown comprising virtualized DCs and connections . In certain embodiments DCs may correspond to physical DCs for example DC  may represent DC A DC  may represent DC F DC  may represent DC E and DC  may represent DC C while DC B and DC D may not be explicitly included in overlay network . In other embodiments DCs may include computing resources from one or more physical DCs and may represent virtualized DCs for example DC  may represent at least portions of DC A and DC B etc. It will be understood that other arrangements and configurations of mapping DCs in physical network to DCs in overlay network may be practiced in different embodiments. Furthermore connections may represent virtualized connections having a given capacity for transporting data. As shown connections and may represent low capacity connections connections and may represent mid capacity connections while connections and may represent high capacity connections. Although connections are shown in overlay network connecting two DCs connections may be physically implemented using various network topologies and may actually represent physical connections that include different nodes and or network segments. However to a cloud service provider using overlay network as an operational network platform the actual physical topology may remain hidden and or may change over time.

Cloud service providers may have a centralized controller not shown in the drawings that manages VMs at DCs interconnected by overlay network . The centralized controller also referred to herein simply as the controller may obtain network information such as delay and SRG of connections and may request the bandwidth of connections through network application programming interfaces APIs with the help of network control and management tools such as software defined networks SDN . As shown in overlay framework SDN APIs may represent software tools for enabling a user e.g. a cloud provider of overlay network to query network information. It is noted that overlay framework may enable network providers of physical network to keep detailed physical network topology information hidden while allowing cloud service providers to easily set up cloud services to perform resource orchestration and to flexibly increase or reduce the bandwidth of connections. The cloud providers may use SDN APIs to query certain specific attributes for DCs and or connections in overlay network without having knowledge of the specific network topology of physical network and or without direct interaction with hidden components in physical network such as intermediate network devices along connection paths that are not included in overlay network .

Referring now to example embodiments of aggregation requests and corresponding protection schemes are illustrated in diagram form. The controller may receive cloud aggregation requests and may perform resource orchestration. In one example embodiment. As shown in aggregation request may illustrate how aggregation DC handles a basic aggregation request for example via the controller by aggregating data from DC DC and DC . More complicated aggregation requests may be generated using a combination of basic aggregation requests and or sets of basic aggregation requests.

Guaranteeing K connect survivability may save network cost by jointly considering information from physical networks and DCs. In separate blind protection shows an example where sindicates risk i and network connections may be blindly protected by providing path disjoint connections dotted lines from aggregation DC . In K connect survivability for K 2 i.e. 2 connect survivability may be guaranteed by protecting against risk events at DCs separately from the network connections which may result in 6 connections in separate protection . In joint protection illustrates by using shared risk group SRG information from underlying optical networks how 2 connect survivability may be guaranteed by finding network connections and DCs that can be jointly protected. For example risks Sand Smay be joined in one SRG risk Sand Smay be joined in a second SRG and risks Sand Smay be joined in a third SRG. In joint protection significant savings in network resources may be achieved by having 3 connections representing a savings of 3 protection connections as compared to .

Using SDN APIs see a subset of DCs with minimum delay may be identifiable when multiple subsets of DCs that satisfy K connect survivability exist. A delay of an aggregation request may be given by a total delay of connections between the subset of DCs and the aggregation DC DC . It is noted that DCmay be allocated to a DC that is relatively near to users or relatively near to a particular subset of DCs depending on specific applications.

Referring now to an example embodiment of overlay network for K connect survivability using VM sharing is illustrated in diagram form. Overlay network may include 6 DCs given by DC DC DC DC DC and DC . Overlay network is shown to provide an overlay framework for aggregation requests described below with respect to .

Referring now to example embodiments of aggregation requests and corresponding joint protection schemes are illustrated in diagram form. As shown in aggregation request illustrates an aggregation request for DC DC while aggregation request in illustrates an aggregation request for DC DC . In aggregation request DC is associated with shared risks Sand S DC is associated with shared risks Sand S and DC is associated with shared risks Sand S. In aggregation request DC is associated with shared risks Sand S and DC is associated with shared risks Sand S. Certain parameters associated with aggregation requests are listed in Table 2.

An aggregation request may satisfy K connect survivability and may also specify a given number of VMs V for risk events. When a risk event occurs an aggregation request with K connect survivability may allocate additional VMs at the surviving K DCs out of M number of working DCs in order to maintain V number of VMs. For determining K connect survivability without considering VM sharing it may be assumed that each DC is allocated the same number of VMs for a given aggregation request. Thus the total VMs for an aggregation request with K connect survivability may be given by V M K. However when VMs are shared among DCs associated with the aggregation request referred to as intra request VM sharing the number of allocated VMs may vary among the DCs. Furthermore when VMs are shared between different aggregation requests among DCs in the overlay network referred to as inter request VM sharing the number of allocated VMs may also vary among DCs.

Based on the following problem description may be applied to the methods for K connect survivability using VM sharing.

GIVEN An overlay network has N number of DC sites and a set of L shared risk groups SRGs for risks S s s . . . s . . . s. In the overlay network each connection Ebetween DCand DChas network information including delay d and a vector of associated SRGs A . . . . . . where 1 indicates that sis associated with E otherwise 0. Similarly each DCis associated with a set of SRGs A. Also an aggregation request is received that requires K DCs to be connected to an aggregation DCand requires that a total number of V VMs remain connected to DC even during a risk event. FIND At least M number of working DCs for each aggregation request such that 1 The total number of VMs required at an overlay network for all aggregation requests are minimized 2 K number of DCs remain connected to DCeven during a risk event which guarantees K connect survivability KCS and 3 V number of VMs remain connected to DCeven during a risk event.

As will now be described in further detail algorithms are disclosed for solving the KCS problem in optically interconnected distributed DC networks using intra request VM sharing and inter request VM sharing.

In algorithm VM1 a variable X is defined as K X M where X represents a number of DCs for allocating V working VMs. Then KCS may be solved to select X DCs using the delay based or risk based methods disclosed in U.S. application Ser. No. 14 102 313. Then among the X DCs a number of working VMs given by V X are allocated to each of the X DCs. It is noted that instead of V X another arbitrary number of working VMs may be allocated among the X DCs for example depending on the particular cloud applications being supported. The remaining VMs associated with the aggregation request given by V M K V may represent protecting VMs.

An example embodiment of results of algorithm VM1 corresponding to Request R in where each of DC DC DC may execute 15 VMs is shown in Table 3.

In algorithm VM2 a VM share table is generated for each aggregation request r in the set of aggregation requests R. The VM share table includes value pairs s v where Vis a number of protecting VMs required at DCwhen a risk sin the set of risks S occurs for aggregation request r. A minimum number of shared protecting VMs at DCis calculated as

Because the total number of working and protecting VMs is given by V M K a lower value for M will result in a lower number of VMs. The risk based KCS scheme disclosed in U.S. application Ser. No. 14 102 313 may then be applied to determine the least M number of DCs to satisfy KCS. When KCS is not satisfied according to the risk based scheme then algorithm VM2 may end without a result. When KCS is satisfied according to the risk based scheme then algorithm VM2 may subsequently include the operations in Table 6 according to the aggregation request.

In inter request VM sharing proper selection of DCs may result in fewer or no additional protecting VMs for a given aggregation request when enough shareable VMs are present in the overlay framework. Thus DCs with a lower total risk frequency and fewer additional protecting VMs may be preferentially selected for the aggregation request since such DCs may be more likely to share VMs with already allocated aggregation requests. In algorithm VM2 an integrated factor Iat DCmay be calculated as I totalRiskFrequency additionalProtectVMif additionalProtectVM 0 else as I totalRiskFrequency. The totalRiskFrequencyis the total frequency of risks that are associated with DCand the connection DC DC where a frequency of a risk is the number of DCs and respective connections to DCassociated with the risk. The value of additionalProtectVMis a number of VMs allocated at DCfor the aggregation request minus sharedVMwhere

Referring now to selected elements of an embodiment of method for implementing K connect survivability using VM sharing as described herein is shown in flow chart format. In various embodiments method may be implemented using KCS identification see . It is noted that certain operations depicted in method may be rearranged or omitted as desired.

Method may begin by determining whether KCS is satisfied operation for an aggregation request. When the result of operation is NO method may end operation . When the result of operation is YES a number of VMs at each of M DCs may be determined operation including determining a number of working VMs and a number of protecting VMs at each of the M DCs. A VM share table may be generated operation according to the aggregation request including calculating a minimum number of shared protecting VMs for a set of risks associated with each of the M DCs. The number of working VMs and the number of protecting VMs may be updated operation based on the minimum number of shared protecting VMs for each DC on the overlay network.

Referring now to selected elements of an embodiment of method for implementing K connect survivability using VM sharing as described herein is shown in flow chart format. Method may represent at least a portion of operation in method see . In various embodiments method may be implemented using KCS identification see . It is noted that certain operations depicted in method may be rearranged or omitted as desired.

Method may begin by determining operation a number of VMs at each of M DCs for the aggregation request. A current value of an integrated factor I may be calculated operation for each of the m DCs currently unselected. It is noted that when method is repeated m may initially have a value of zero 0 . The integrated factor I may be calculated as Idescribed above with respect to algorithm VM3. The m DCs may be sorted operation according to I. A DC from the m DCs having a lowest value for I may be added operation to the M DCs. Then a decision may be made whether KCS is satisfied or whether m 0 operation . When the result of operation is NO method may loop back to operation . When the result of operation is YES method may proceed to operation in method see .

Referring now to a block diagram of selected elements of an embodiment of management system is illustrated. In management system is represented as a computer system including physical and logical components for implementing K connect survivability using VM sharing as described herein and may accordingly include processor memory and network interface . Processor may represent one or more individual processing units and may execute program instructions interpret data and or process data stored by memory and or management system .

In memory may be communicatively coupled to processor and may comprise a system device or apparatus suitable to retain program instructions and or data for a period of time e.g. computer readable media . Memory may include various types components and devices such as random access memory RAM electrically erasable programmable read only memory EEPROM a PCMCIA card flash memory solid state disks hard disk drives magnetic tape libraries optical disk drives magneto optical disk drives compact disk drives compact disk arrays disk array controllers and or any suitable selection or array of volatile or non volatile memory. Non volatile memory refers to a memory that retains data after power is turned off. It is noted that memory may include different numbers of physical storage devices in various embodiments.

As shown in memory may include K connect survivability KCS identification which may represent respective sets of computer readable instructions that when executed by a processor such as processor may execute various algorithms for identifying DCs and or SRGs to satisfy K connect survivability including but not limited to Risk Based Algorithm A1 Delay Based Algorithm A2 see U.S. application Ser. No. 14 102 313 as well as Algorithm VM1 Intra Request VM Sharing Algorithm VM2 Inter Request VM Risk Based Sharing and or Algorithm VM3 Inter Request VM Integrated Sharing. Information storage may store various data and parameters such as data and parameters associated with KCS identification .

While the subject of this specification has been described in connection with one or more exemplary embodiments it is not intended to limit any claims to the particular forms set forth. On the contrary any claims directed to the present disclosure are intended to cover such alternatives modifications and equivalents as may be included within their spirit and scope.

