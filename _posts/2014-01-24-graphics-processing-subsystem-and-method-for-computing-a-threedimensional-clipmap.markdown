---

title: Graphics processing subsystem and method for computing a three-dimensional clipmap
abstract: A graphics processing subsystem and method for computing a 3D clipmap. One embodiment of the subsystem includes: (1) a renderer operable to render a primitive surface representable by a 3D clipmap, (2) a geometry shader (GS) configured to select respective major-plane viewports for a plurality of clipmap levels, the major-plane viewports being sized to represent full spatial extents of the 3D clipmap relative to a render target (RT) for the plurality of clipmap levels, (3) a rasterizer configured to employ the respective major-plane viewports and the RT to rasterize a projection of the primitive surface onto a major plane corresponding to the respective major-plane viewports into pixels representing fragments of the primitive surface for each of the plurality of clipmap levels, and (4) a plurality of pixel shader (PS) instances configured to transform the fragments into respective voxels in the plurality of clipmap levels, thereby voxelizing the primitive surface.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09390543&OS=09390543&RS=09390543
owner: Nvidia Corporation
number: 09390543
owner_city: Santa Clara
owner_country: US
publication_date: 20140124
---
This application claims the benefit of U.S. Provisional Application Ser. No. 61 892 288 filed by Bolotov et al. on Oct. 17 2013 entitled A Method for Optimizing Regions for Voxelization Updates and U.S. Provisional Application Ser. No. 61 892 316 filed by Bolotov et al. on Oct. 17 2013 entitled Using Clipmaps to Represent Volumetric Data for GI and AO Algorithms commonly assigned with this application and fully incorporated herein by reference.

This application is directed in general to a graphics processing subsystem and more specifically to computing a three dimensional 3D clipmap representation of a rendered primitive surface in a scene.

Many computer graphic images are created by mathematically modeling the interaction of light with a 3D scene from a given viewpoint. This process called rendering generates a two dimensional 2D image of the scene from the given viewpoint and is analogous to taking a photograph of a real world scene.

As the demand for computer graphics and in particular for real time computer graphics has increased computer systems with graphics processing subsystems adapted to accelerate the rendering process have become widespread. In these computer systems the rendering process is divided between a computer s general purpose central processing unit CPU and the graphics processing subsystem architecturally centered about a graphics processing unit GPU . Typically the CPU performs high level operations such as determining the position motion and collision of objects in a given scene. From these high level operations the CPU generates a set of rendering commands and data defining the desired rendered image or images. For example rendering commands and data can define scene geometry lighting shading texturing motion and or camera parameters for a scene. The graphics processing subsystem creates one or more rendered images from the set of rendering commands and data.

Scene geometry is typically represented by geometric primitives such as points lines polygons for example triangles and quadrilaterals and curved surfaces defined by one or more two or three dimensional vertices. Each vertex may have additional scalar or vector attributes used to determine qualities such as the color transparency lighting shading and animation of the vertex and its associated geometric primitives.

Many graphics processing subsystems are highly programmable through an application programming interface API enabling complicated lighting and shading algorithms among other things to be implemented. To exploit this programmability applications can include one or more graphics processing subsystem programs which are executed by the graphics processing subsystem in parallel with a main program executed by the CPU. Although not confined merely to implementing shading and lighting algorithms these graphics processing subsystem programs are often referred to as shading programs programmable shaders or simply shaders. 

A variety of shading programs are directed at modeling illumination in a scene. The physical plausibility of rendered illumination often depends on the application more specifically whether or not the rendering is done in real time. Physically plausible illumination at real time frame rates is often achieved using approximations. For example ambient occlusion is a popular approximation because of its high speed and simple implementation. Another example is directional occlusion. Many algorithms can only approximate direct illumination which is light coming directly from a light source.

Global illumination or GI is a concept that accounts for both direct illumination and indirect illumination which is light that reflects off other surfaces in rendering the scene. In doing so a significantly more realistic image is achievable. However real time global illumination remains problematic for large and dynamic scenes. Efforts to mitigate the latency introduced by these comprehensive illumination algorithms are ongoing. For example some algorithms partially pre compute illumination. Another example is instant radiosity which models indirect lighting as a set of point lights the contributions of which are accumulated over multiple rendering passes. Yet another approach is to limit indirect lighting to a single bounce under the assumption that one bounce indirect illumination is sufficiently realistic. Still real time frame rates are typically only achievable through approximations.

Ambient occlusion or AO is an example of a shading algorithm commonly used to add a global illumination look to rendered images. AO is not a natural lighting or shading phenomenon. In an ideal system each light source would be modeled to determine precisely the surfaces it illuminates and the intensity at which it illuminates them taking into account reflections refractions scattering dispersion and occlusions. In computer graphics this analysis is accomplished by ray tracing or ray casting. The paths of individual rays of light are traced throughout the scene colliding and reflecting off various surfaces.

In non real time applications each surface in the scene can be tested for intersection with each ray of light producing a high degree of visual realism. This presents a practical problem for real time graphics processing rendered scenes are often very complex incorporating many light sources and many surfaces such that modeling each light source becomes computationally overwhelming and introduces large amounts of latency into the rendering process. AO algorithms address the problem by modeling light sources with respect to an occluded surface in a scene as white hemispherical lights of a specified radius centered on the surface and oriented with a normal vector at the occluded surface. Surfaces inside the hemisphere cast shadows on other surfaces. AO algorithms approximate the degree of occlusion caused by the surfaces resulting in concave areas such as creases or holes appearing darker than exposed areas. AO gives a sense of shape and depth in an otherwise flat looking scene.

The most realistic AO techniques are global the illumination at each point is a function of other geometry in the scene. Screen space AO SSAO can render only local effects and therefore fails to recognize the more subtle illuminations that lend realism. For this reason SSAO will not be further described herein.

Several methods are available to compute global AO but its sheer computational intensity makes it an unjustifiable luxury for most real time graphics processing systems. To appreciate the magnitude of the effort AO entails consider a given point on a surface in the scene and a corresponding hemispherical normal oriented light source surrounding it. The illumination of the point is approximated by integrating the light reaching the point over the hemispherical area. The fraction of light reaching the point is a function of the degree to which other surfaces obstruct each ray of light extending from the surface of the sphere to the point.

One aspect provides a graphics processing subsystem. In one embodiment the subsystem includes 1 a renderer operable to render a primitive surface representable by a 3D clipmap 2 a geometry shader GS configured to select respective major plane viewports for a plurality of clipmap levels the major plane viewports being sized to represent full spatial extents of the 3D clipmap relative to a render target RT 3 a rasterizer configured to employ the respective major plane viewports and the RT to rasterize a projection of the primitive surface onto a major plane corresponding to the respective major plane viewports into pixels representing fragments of the primitive surface for each of the plurality of clipmap levels and 4 a plurality of pixel shader PS instances configured to transform the fragments into respective voxels in the plurality of clipmap levels thereby voxelizing the primitive surface.

In another embodiment the subsystem includes 1 a memory configured to store 1a a 3D clipmap representing a primitive surface in a scene and having a plurality of clipmap levels 1b an RT for the plurality of clipmap levels and 1c respective major plane viewports for the plurality of clipmap levels wherein each of the respective major plane viewports represent full extents of the 3D clipmap relative to the RT 2 a renderer operable to render an update to the primitive surface 3 a GS configured to 3a select a major plane that maximizes area of the primitive surface s projection thereon 3b generate a projection of the primitive surface onto the major plane and 3c replicate the projection for each of the plurality of clipmap levels thereby generating respective instances of the primitive surface 4 a rasterizer configured to employ the RT and respective major plane viewports that correspond to the major plane to rasterize the respective instances into pixels representing fragments of the primitive surface for each of the plurality of clipmap levels and 5 a plurality of PS instances configured to transform the fragments into respective voxels in the plurality of clipmap levels and cause the respective voxels to be written to the memory thereby carrying out the update on the 3D clipmap.

Another aspect provides a method of computing a 3D clipmap representation of a rendered primitive surface in a scene. In one embodiment the method includes 1 defining respective major plane viewports and an RT for a plurality of clipmap levels wherein the respective major plane viewports represent full extents of the 3D clipmap representation relative to the plurality of clipmap levels and the RT maintain a constant clip level resolution 2 selecting a major plane and corresponding respective major plane viewports wherein the major plane maximizes a projected area of the rendered primitive surface thereon and 3 employing the corresponding respective major plane viewports the RT and the projected area in voxelizing the rendered primitive surface into the plurality of clipmap levels.

A mipmap is a collection of correlated images of increasingly reduced resolution. Mip is actually an acronym representing the Latin phrase multum in parvo meaning much in little. Mipmaps are often described as resolution pyramids starting with level zero the largest and finest level. Each lower level represents the image using half as many texels in each dimension. Consequently for a two dimensional 2D image each lower level consumes one quarter of the memory required for the level above. For a 3D scene each lower level consumes one eighth the memory required for the level above. Rendering processes can gain access to the various levels of detail LODs to use the texels contained therein to render an image. Mipmaps are intended to increase rendering speed and reduce aliasing.

A clipmap is a representation of a partial mipmap in which the finest levels are clipped to a specified maximum size. Rather than the pyramidal shape seen in mipmaps the clipmap more resembles an obelisk. Clipped levels are referred to as clip levels and unclipped levels are referred to as mip levels. A clipmap has at least one mip level and at least one clip level. The mip levels of a clipmap represent the same spatial region of a scene with increasing resolution beginning with the coarsest level. Each of the clip levels has the same resolution that is equal to the resolution of the finest mip level. While the clip levels maintain a constant resolution across the corresponding LODs the respective spatial regions represented shrink as the LODs become finer. This representation reduces the amount of memory required to represent parts of the scene with high spatial resolution and cover a large region of the scene at the same time.

Clipmaps resulted from the realization that the majority of a mipmap is not used to render a single frame of a scene. In fact the viewpoint and display resolution determine the part of the mipmap that is used to render a frame. The clipmap is intended to be the minimal subset of the mipmap needed to render each frame. Thus clipmaps should be updated as frames change over time. For this reason practical clipmaps are updatable.

The clipmap data structure can be expanded to represent volumetric data for a 3D scene. Volumetric data is packaged in a volumetric pixel or voxel. Clipmaps were originally implemented as 2D mipmaps with the finest levels clipped such that they have the same number of texels but cover different sized spatial regions. A 3D clipmap has advantages over alternative representations such as a sparse voxel octree in that it can be updated more quickly and more quickly sampled than an octree.

Clipmaps can be used in many graphics rendering processes including ambient occlusion AO and global illumination GI . To evaluate a viewpoint in a particular scene the scene is voxelized to form a clipmap that is centered on or in some embodiments slightly offset from the viewpoint. Generally when computing an effect on the viewpoint geometry that is further from the viewpoint has less impact on the computation than nearer geometry. When processing the viewpoint samples are taken from the various LODs of the clipmap. Nearer samples are taken from the finer LODs and distant samples are taken from the coarser LODs.

Updating the clipmap quickly is essential to using clipmaps in processes such as AO or GI for large scenes or animated objects. It is realized herein that rendering the scene multiple times per frame to update the various clipmap levels is inefficient because it creates additional processing load for the graphics processing subsystem and possibly the CPU. It is realized herein the scene geometry can be rendered just once per frame and then replicated or instanced in a GS. It is further realized herein the GS can instance the geometry by using an array of viewports to represent the various LODs of the clipmap. A viewport is a rectangular viewing window for projecting a 3D scene onto a 2D display. It is realized herein the geometry can be rasterized by the GPU for each LOD in the clipmap by using a viewport that is sized such that it represents the full clipmap extents relative to an RT. The RT is sized to maintain a constant resolution across all clipmap levels and therefore assumes its smallest spatial dimensions for the finest LOD. The viewport is sized relative to that RT typically making the viewport larger than the RT which is contrary to typical implementations of viewports and RTs. The viewport spatial dimensions are scaled such that the RT contains the appropriate portion of the volume for a given LOD.

For example at the coarsest LOD the RT and viewport assume the same spatial dimensions. At the next finer LOD the RT dimensions are halved in each dimension but the RT maintains the clip level resolution. The viewport dimensions are doubled in each dimension for the next finer LOD making the area within the viewport four times larger than the area within the RT which is the appropriate ratio for that LOD. As the LODs become increasingly finer the viewport dimensions continue to expand to achieve the appropriate ratio of viewport area to RT area for the LOD.

It is also realized herein the voxelization processing load can be optimized by optimizing the selection of the viewport for each LOD of the clipmap. A primitive surface can be voxelized by projecting it onto a viewport rasterizing and then transforming the 2D positions into 3D positions based on the depth of the 2D position. The transforming is typically carried out by instances e.g. threads of a PS. As those skilled in the pertinent art understand a PS is a program designed to be instantiated multiple times in a GPU to process pixels of an image in parallel. It is realized herein for a given LOD of a clipmap a viewport can be created for each of the three major planes of the clipmap those major planes being defined by the X Y and Z axes. Of those major plane viewports it is realized herein one can maximize parallelization of voxelization processing. It is realized herein that by selecting the major plane that increases e.g. maximizes the area of a projected primitive surface onto the major plane the serial processing load for each PS is reduced e.g. minimized . It is further realized herein the trade off is an increase in the number of PS instances required to carry out the pixel computations. However modern GPUs and graphics processing subsystems provide increasingly larger numbers of parallel lanes within which these PS processes can be executed.

Before describing various embodiments of the graphics processing subsystem and method of computing a 3D clipmap introduced herein a computing system within which the graphics processing subsystem or method maybe embodied or carried out will be described.

As shown the system data bus connects the CPU the input devices the system memory and the graphics processing subsystem . In alternate embodiments the system memory may connect directly to the CPU . The CPU receives user input from the input devices executes programming instructions stored in the system memory operates on data stored in the system memory and configures the graphics processing subsystem to perform specific tasks in the graphics pipeline. The system memory typically includes dynamic random access memory DRAM employed to store programming instructions and data for processing by the CPU and the graphics processing subsystem . The graphics processing subsystem receives instructions transmitted by the CPU and processes the instructions in order to render and display graphics images on the display devices .

As also shown the system memory includes an application program an application programming interface API and a GPU driver . The application program generates calls to the API in order to produce a desired set of results typically in the form of a sequence of graphics images. The application program also transmits zero or more high level shading programs to the API for processing within the GPU driver . The high level shading programs are typically source code text of high level programming instructions that are designed to operate on one or more shading engines within the graphics processing subsystem . The API functionality is typically implemented within the GPU driver . The GPU driver is configured to translate the high level shading programs into machine code shading programs that are typically optimized for a specific type of shading engine e.g. vertex geometry or fragment .

The graphics processing subsystem includes a GPU an on chip GPU memory an on chip GPU data bus a GPU local memory and a GPU data bus . The GPU is configured to communicate with the on chip GPU memory via the on chip GPU data bus and with the GPU local memory via the GPU data bus . The GPU may receive instructions transmitted by the CPU process the instructions in order to render graphics data and images and store these images in the GPU local memory . Subsequently the GPU may display certain graphics images stored in the GPU local memory on the display devices .

The GPU includes one or more streaming multiprocessors . Each of the streaming multiprocessors is capable of executing a relatively large number of threads concurrently. Advantageously each of the streaming multiprocessors can be programmed to execute processing tasks relating to a wide variety of applications including but not limited to linear and nonlinear data transforms filtering of video and or audio data modeling operations e.g. applying of physics to determine position velocity and other attributes of objects and so on. Furthermore each of the streaming multiprocessors may be configured as a shading engine that includes one or more programmable shaders each executing a machine code shading program i.e. a thread to perform image rendering operations. The GPU may be provided with any amount of on chip GPU memory and GPU local memory including none and may employ on chip GPU memory GPU local memory and system memory in any combination for memory operations.

The on chip GPU memory is configured to include GPU programming code and on chip buffers . The GPU programming may be transmitted from the GPU driver to the on chip GPU memory via the system data bus . The GPU programming may include a machine code vertex shading program a machine code geometry shading program a machine code fragment shading program or any number of variations of each. The on chip buffers are typically employed to store shading data that requires fast access in order to reduce the latency of the shading engines in the graphics pipeline. Since the on chip GPU memory takes up valuable die area it is relatively expensive.

The GPU local memory typically includes less expensive off chip dynamic random access memory DRAM and is also employed to store data and programming employed by the GPU . As shown the GPU local memory includes a frame buffer . The frame buffer stores data for at least one two dimensional surface that may be employed to drive the display devices . Furthermore the frame buffer may include more than one two dimensional surface so that the GPU can render to one two dimensional surface while a second two dimensional surface is employed to drive the display devices .

The display devices are one or more output devices capable of emitting a visual image corresponding to an input data signal. For example a display device may be built using a cathode ray tube CRT monitor a liquid crystal display or any other suitable display system. The input data signals to the display devices are typically generated by scanning out the contents of one or more frames of image data that is stored in the frame buffer .

Having described a computing system within which the graphics processing subsystem and method of computing a 3D clipmap may be embodied or carried out various embodiments of the graphics processing subsystem and method will be described.

Mip level is the coarsest LOD in 3D clipmap and includes a single voxel that represents the full spatial extent spanned by 3D clipmap volume . Consequently mip level requires the least memory to store. Mip level doubles the resolution in each dimension with respect to mip level making the resolution eight voxels. Mip level again doubles the resolution in each dimension with respect to mip level making the resolution 64 voxels. As resolution increases from mip level to mip level to mip level the memory necessary to store those LODs also increases. The spatial regions or volumes spanned by mip level mip level and mip level are the same.

Clip level maintains the 64 voxel resolution of mip level increases the detail represented and decreases the spatial region represented by the LOD. In a 3D mipmap representation the resolution would again double in each dimension. However 3D clipmap clips voxels outside the maximum size for each dimension thereby leaving a portion of the full spatial extent volume unrepresented in clip level . Clip level is centered about a viewpoint which is located where the octants of clip level meet.

Clip level is the finest LOD in 3D clipmap and also maintains the 64 voxel resolution of mip level and clip level . Clip level increases the detail represented relative to clip level and decreases the spatial region represented by the LOD. Clip level leaves a larger portion of the full spatial extent volume unrepresented than clip level and is centered about the same viewpoint.

As stated above it is realized herein that scene geometry can be rendered just once per frame and thereafter instanced in a GS. To make the GS simpler an array of viewports is used that represent different LODs of the clipmap. If each level is a cube with C texels in every dimension an array of L viewports is created in which each viewport represents the full clipmap extent relative to one of the clipmap levels. A square RT with C pixels in both dimensions is also created for the clipmap.

To illustrate this shows three levels of an example clipmap having L levels Level Level and Level in which each level has an associated viewport and RT for C 128 and L 3. The viewport in Level and the RT assume the same spatial dimensions. While Levels and of the example clipmap of are cubic non cubic levels can be accommodated in a straightforward manner through the use of additional RTs.

Primitive surfaces are triangular surfaces in a 3D scene. Each time the scene is drawn or updated renderer renders primitive surfaces . In certain embodiments the application that generates primitive surfaces identifies particular surfaces or regions containing particular surfaces that require an update thus renderer would render only the necessary primitive surfaces. Additionally renderer need only render primitive surfaces a single time as opposed to rendering primitive surfaces multiple times for each of the LODs in 3D clipmap . Renderer is generally a rendering pipeline or collection of rendering modules for carrying out the various rendering processes. Primitive surfaces are rendered by renderer as geometric meshes that intersect one or more LOD in 3D clipmap . The meshes are rendered in an object reference frame referred to as object space.

VS transforms the coordinates of the vertices in the geometric meshes generated by renderer from object space into world space a common reference frame for all of primitive surfaces . Primitive surfaces are represented in world space as a collection of triangles. GS instances the collection of triangles for each of the N LODs in 3D clipmap . Each instance of the triangles is rasterized separately by rasterizer for each LOD. Rasterizer employs viewports and RT to rasterize the triangles.

Viewports include major plane viewports for each LOD in clipmap . GS selects which of the major planes should be used by rasterizer as the viewport. The selection is made by determining onto which major plane a projection of a triangle would maximize the projected area. The major plane viewports for each of LODs through N corresponding to that major plane are passed to rasterizer .

Rasterizer employs both the viewport corresponding to a particular LOD and the RT to rasterize the projected triangle. Viewports are sized relative to the RT . For a given LOD its corresponding major plane viewports are sized such that they represent the full extents of 3D clipmap relative to RT .

Rasterizer generates 2D images of primitive surfaces for each of LODs through N. The 2D images are composed of pixels representing fragments of primitive surfaces . The primitives are scaled according to the viewport dimensions but only fragments that belong to the render target are produced which essentially clips the rasterized image for all LODs except the coarsest one. Graphics processing subsystem includes a PS instance for each pixel in each of LODs through LOD N. PS instances are configured to transform the pixels for LOD into voxels. PS instances through N employ the rasterized triangles and the depth data from the projection of primitive surfaces onto the selected major plane to compute voxel locations for each clipmap level. PS instances through N then gain access to the memory in which 3D clipmap is stored and update the appropriate LODs through N.

The rendered primitive surface is typically rendered in an object reference frame or object space. Primitive surfaces are typically triangles although they are not limited to triangles. The rendered primitive surface in certain embodiments is transformed from object space into a reference frame that can be used for computing the 3D clipmap such as world space or a clipmap space. This transformation is typically carried out by a VS. The VS transforms coordinates from one reference frame to another.

Continuing the embodiment of in a selection step a major plane is selected that maximizes the projected area of the rendered primitive onto the major plane. By selecting the major plane a corresponding major plane viewport is effectively selected. The selection can be made in a variety of ways. For example in certain embodiments the normal of the rendered primitive surface can be projected onto the major axes and the projection compared. The major axis having the largest magnitude projected thereon is perpendicular to the major plane that would maximize the projected area. For example in an embodiment where the primitive surface has a unit normal vector if that primitive surface is nearly parallel to a major plane the projection of the unit normal vector onto a perpendicular axis would have a magnitude of nearly one. Likewise that same unit normal vector projected onto the remaining two axes would have a magnitude of nearly zero as the unit normal vector is effectively orthogonal to both those axes. The projection of a primitive surface onto a major plane that is parallel maximizes the projected area. As the primitive surface becomes less parallel to a major plane the maximum projected area is found by minimizing the angle subtended by the primitive surface and the major plane which can be quantified by comparing the projected normal vectors or simply normals. 

In the embodiment of the selected major plane viewport and RT are employed in a voxelization step . The rendered primitive is projected onto the selected major plane via the selected major plane viewport and rasterized using the selected major plane viewport and the corresponding RT for each LOD in the clipmap. Rasterization generates a 2D representation of the rendered primitive surface. Each pixel in the 2D representation represents a fragment of the rendered primitive surface. Each fragment has a depth that is ultimately represented by the voxels in the 3D clipmap. The voxels are generated by employing the depth data for the projected rendered primitive to transform the 2D positions of each fragment into one or more 3D locations in the corresponding clipmap level. The voxels are packed with data and stored in the 3D clipmap data structure. The voxelization is repeated for each LOD in the clipmap. The method then ends in an end step .

Those skilled in the art to which this application relates will appreciate that other and further additions deletions substitutions and modifications may be made to the described embodiments.

