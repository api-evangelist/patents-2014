---

title: Image segmentation using color and depth information
abstract: Image segmentation utilizing 3D image data. A plurality of pixels of an image frame may be segmented based at least on a function of pixel color and a pixel depth over the spatial positions within the image frame. A graph-cut technique may be utilized to optimize a data cost and smoothness cost in which at least the data cost function includes a component that is a dependent on a depth associated with a given pixel in the frame. In further embodiments, both the data cost and smoothness functions are dependent on a color and a depth associated with each pixel. Components of at least the data cost function may be weighted for each pixel to arrive at most likely segments. Segmentation may be further predicated on a pre-segmentation label assigned based at least on a 3D spatial position clusters.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09547907&OS=09547907&RS=09547907
owner: Intel Corporation
number: 09547907
owner_city: Santa Clara
owner_country: US
publication_date: 20141215
---
A digital camera is a component often included in commercial electronic media device platforms. Digital cameras are now available in wearable form factors e.g. video capture earpieces video capture headsets video capture eyeglasses etc. as well as embedded within smartphones tablet computers and notebook computers etc. Three dimensional 3D cameras are becoming more common and can now be found on many mobile devices platforms. These devices provide enhanced entertainment and utility experiences to an end user. For example photography may be enhanced by depth information output from the 3D camera.

Often a digital camera user wishes to segment an image frame into visually distinct objects. The definition of an object can vary from a single instance to a whole class of objects. Once selected special effects may be applied to one or more objects objects from multiple photos may be mixed into one objects may be removed from photos etc. Such object based image processing may be on line or real time with image capture or may be performed during post processing.

Segmentation algorithms typically allow a user to select parts of an image or specific object of interest. In conventional tools this is accomplished through color or texture based image segmentation. is a schematic showing conventional foreground background image data segmentation based on color or texture information. A 2D image frame captured by digital camera includes a representation of real world object e.g. subject person in the foreground a real world object e.g. tree and a real world object e.g. sky . A foreground background segmentation method is performed for example when a user selects a displayed region of image frame corresponding to object . Foreground background segmentation process outputs a visual indication of a foreground segment and a background segment . However because real world objects are composed of multiple colors and textures a foreground background segmentation method may define a segment border that erroneously includes some portion of object in addition to object . This may happen because color or texture information alone is insufficient to segment a single object having multiple colors textures etc. that should be combined. Also a real world scene is composed of multiple objects and not just a single foreground and background. Multiple user interaction steps may then be required to arrive at an acceptable segment for example achieving the segment border .

Thus there is a need for a multi layer segmentation algorithm that can separate a scene into multiple objects each with a unique label or segment ID based on color and depth information obtained using any 3D camera or 3D scanner. However depth information included in image data is often noisy sparse and lower resolution compared to the color image. Also two objects may be at indistinguishable depths. For example a person standing on a road objects placed on a table etc. Thus depth alone may also be insufficient to suitably segment a scene for end user applications.

Hence there is a need for a multi layer segmentation algorithm that employs both the color and depth information jointly. Automated image segmentation techniques and system s to perform such techniques that are capable of fully integrating the richer data set generated by a 3D camera are therefore advantageous.

One or more embodiments are described with reference to the enclosed figures. While specific configurations and arrangements are depicted and discussed in detail it should be understood that this is done for illustrative purposes only. Persons skilled in the relevant art will recognize that other configurations and arrangements are possible without departing from the spirit and scope of the description. It will be apparent to those skilled in the relevant art that techniques and or arrangements described herein may be employed in a variety of other systems and applications beyond what is described in detail herein.

Reference is made in the following detailed description to the accompanying drawings which form a part hereof and illustrate exemplary embodiments. Further it is to be understood that other embodiments may be utilized and structural and or logical changes may be made without departing from the scope of claimed subject matter. Therefore the following detailed description is not to be taken in a limiting sense and the scope of claimed subject matter is defined solely by the appended claims and their equivalents.

In the following description numerous details are set forth however it will be apparent to one skilled in the art that embodiments may be practiced without these specific details. Well known methods and devices are shown in block diagram form rather than in detail to avoid obscuring more significant aspects. References throughout this specification to an embodiment or one embodiment mean that a particular feature structure function or characteristic described in connection with the embodiment is included in at least one embodiment. Thus the appearances of the phrase in an embodiment or in one embodiment in various places throughout this specification are not necessarily referring to the same embodiment. Furthermore the particular features structures functions or characteristics described in the context of an embodiment may be combined in any suitable manner in one or more embodiments. For example a first embodiment may be combined with a second embodiment anywhere the particular features structures functions or characteristics associated with the two embodiments are not mutually exclusive.

As used in the description of the exemplary embodiments and in the appended claims the singular forms a an and the are intended to include the plural forms as well unless the context clearly indicates otherwise. It will also be understood that the term and or as used herein refers to and encompasses any and all possible combinations of one or more of the associated listed items.

As used throughout the description and in the claims a list of items joined by the term at least one of or one or more of can mean any combination of the listed terms. For example the phrase at least one of A B or C can mean A B C A and B A and C B and C or A B and C.

The terms coupled and connected along with their derivatives may be used herein to describe functional or structural relationships between components. It should be understood that these terms are not intended as synonyms for each other. Rather in particular embodiments connected may be used to indicate that two or more elements are in direct physical optical or electrical contact with each other. Coupled may be used to indicated that two or more elements are in either direct or indirect with other intervening elements between them physical optical or electrical contact with each other and or that the two or more elements co operate or interact with each other e.g. as in a cause an effect relationship .

Some portions of the detailed descriptions provide herein are presented in terms of algorithms and symbolic representations of operations on data bits within a computer memory. Unless specifically stated otherwise as apparent from the following discussion it is appreciated that throughout the description discussions utilizing terms such as calculating computing determining estimating storing collecting displaying receiving consolidating generating updating or the like refer to the action and processes of a computer system or similar electronic computing device that manipulates and transforms data represented as physical electronic quantities within the computer system s circuitry including registers and memories into other data similarly represented as physical quantities within the computer system memories or registers or other such information storage transmission or display devices.

While the following description sets forth embodiments that may be manifested in architectures such system on a chip SoC architectures for example implementation of the techniques and or arrangements described herein are not restricted to particular architectures and or computing systems and may be implemented by any architecture and or computing system for similar purposes. Various architectures employing for example multiple integrated circuit IC chips and or packages and or various computing devices and or consumer electronic CE devices such as set top boxes smartphones etc. may implement the techniques and or arrangements described herein. Further while the following description may set forth numerous specific details such as logic implementations types and interrelationships of system components logic partitioning integration choices etc. claimed subject matter may be practiced without such specific details. Furthermore some material such as for example control structures and full software instruction sequences may not be shown in detail in order not to obscure the material disclosed herein.

Certain portions of the material disclosed herein may be implemented in hardware for example as logic circuitry in an image processor. Certain other portions may be implemented in hardware firmware software or any combination thereof. At least some of the material disclosed herein may also be implemented as instructions stored on a machine readable medium which may be read and executed by one or more processors graphics processors and or central processors . A machine readable medium may include any medium and or mechanism for storing or transmitting information in a form readable by a machine e.g. a computing device . For example a machine readable medium may include read only memory ROM random access memory RAM magnetic disk storage media optical storage media flash memory devices electrical optical acoustical or other similarly non transitory tangible media.

One or more system apparatus method and computer readable media is described below for multilayer image segmentation utilizing 3D image data. A plurality of pixels of an image frame may be segmented based at least on a function of pixel color and a pixel depth over the spatial positions within the image frame. The functions employed in some embodiments herein are grounded in the logic that pixels of a same depth and or connected in 3D space are more likely to belong to the same object even if they have different color texture.

In some embodiments a graph cut technique is utilized to optimize a data cost and smoothness cost in which at least the data cost function includes a component that is a dependent on a depth associated with a given pixel in the frame. In some such embodiments a joint RGB D automated segmentation algorithm employs a Markov Random Field MRF formulation. In further embodiments both the data cost and smoothness functions are dependent on a color e.g. RGB and a depth D associated with each pixel. In some embodiments adaptive weights are utilized in the MRF formulation which scale with the confidence in the depth information. In some embodiments the adaptive weights may limit impact of noisy and or sparse depth data. In some embodiments segmentation may be further predicated on a pre segmentation label assigned based at least on 3D position clusters.

The techniques described below in detail for exemplary embodiments can be used many different platforms and or 3D image data such as but not limited to multi camera array camera systems and active light depth sensors. is a schematic depicting multilayer image data segmentation based on color and depth information collected by an array camera in accordance with exemplary embodiments. For purposes herein multi layer segmentation separates a scene into multiple objects with each object receiving a unique label or segment ID rather than merely a foreground background designation.

In some embodiments array camera is a component of a mobile computing device having a plurality of camera modules and with a predetermined baseline relationship Although in the exemplary embodiments three camera modules are illustrated any number of camera modules and or image sensors may be included in an array camera as embodiments herein are not limited in this respect. Each of the plurality of camera modules output an image captured from a different camera viewpoint. In exemplary embodiments the image s captured from each viewpoint is captured at substantially the same instant of time such that they contain image data for a given scene. For example at least a portion of scene including foreground object subject and background is captured in three image frames one of which may be designated as a reference and combined into an image frame having depth information. For example where camera module has a higher resolution e.g. 8 megapixel or more than camera modules e.g. 720 p HD etc. camera module may provide a default reference image. Camera modules and may be considered supplemental to the reference and are each associated with predetermined baseline vector length and direction from camera module . In an exemplary embodiment where camera modules and are on a mobile platform the baseline vector between the reference camera module and each supplemental camera module may have a length of tens of millimeters to tens of centimeters depending on the form factor. In other embodiments where camera modules are separate infrastructure fixtures baseline lengths may be on the order of meters. In one exemplary mobile device embodiment camera modules and are along one baseline with camera modules and spaced by known distances on opposite sides of reference camera module .

In accordance with some embodiments multilayer segmentation method is performed to determine multiple unique labels for the different objects at least in part on color and depth information. For example real world object is labeled image segment real world object is labeled image segment and real world background e.g. sky is labeled image segment . Compared to segment segment is more accurately distinguished from surrounding segments . In addition the segment is separated from the segment unlike conventional foreground background segmentation. An automated association made between segment and output image frame may then be in better agreement with human perception.

In one exemplary embodiment image data received at operation includes pixel values e.g. intensity for each of a plurality of color channels. The color channels may be in any color space. In some embodiments the input image data received at operation includes color information in the RGB color space denoted herein as image data I. illustrates a frame of input image data I in accordance with some embodiments. Image data Imay also be in other color spaces perhaps having been preprocessed upstream of method for example converted to color information to the YUV HSL HSV space from another color space such as the RGB color space the YPP luma blue difference chroma Y and red difference chroma P color space the YCC luma Y blue difference chroma C and red difference chroma C color space or the like.

In some embodiments depth information received at operation is in the form of a depth map correlated with a plurality of pixels each having an image coordinate x y associated with image I. In other embodiments the depth information received at operation is in the form of a disparity map correlated with a plurality of pixels each having an image coordinate x y associated with image I. illustrates a disparity map associated with the input image frame depicted in in accordance with some embodiments. As shown in real world objects e.g. nearby region of ground e.g. basketball player have considerable contrast from other features in the scene.

For some disparity embodiments image data received at operation may further include camera parameters such as camera focal length Cand camera baseline C from which disparity values corresponding to pixels in a reference Iimage may be estimated at operation from a plurality of images Igenerated by an array camera e.g. array camera in . For such embodiments a disparity value associated with a pixel indicates the correspondence of the pixel in one image e.g. collected by camera module to a pixel in another image e.g. collected by camera module . The disparity estimation may be made by any means as embodiments herein are not limited in this respect. In some embodiments the image data has been preprocessed upstream of method for example to rectify multiple images to a reference prior to a disparity map calculation.

In some embodiments the color information Iand depth information at input operation are of equal height and width albeit potentially of different resolution. For example the depth disparity data may be sparse with a predetermined value DISP UNKNOWN applied to any pixel of Iwhere disparity is not known because of a lower resolution sensor output occlusion etc.

Referring still to method continues at operation with filtering the input image Ito generate a filtered image I. Any known filtering technique to quantize the color channels and or reduce input image noise may be performed at operation as embodiments herein are not limited in this respect. Filtering operation may improve performance of subsequent segmentation operations for example reducing the likelihood that objects are over segmented. In some exemplary embodiments the input image Iis processed through a Gaussian blur filter at operation . In some such embodiments processing of the input image Iat operation further entails a pyramid based mean shift filter. illustrates a filtered image in accordance with some embodiments where a Gaussian blur filter and a pyramid based mean shift filter is applied to input image . A smoothening in region denoted in from that in A is attributable to an exemplary embodiment of color filtering operation .

In some embodiments segmentation of an input image is based at least in part on one or more heuristic that enables object segmentation to better match human perception. Returning to one such heuristic may be introduced at operation by excluding a portion of the disparity values received at operation based on predetermined criteria. Input disparity map D may be filtered to remove noise and or disparity values having low confidence. Any number of statistical techniques and or filters may be applied to the population of disparity values associated with the plurality of pixels in the image frame to arrive at a subset of disparity values that are to be included in subsequent segmentation operations. Operation therefore is a first control point for modulating the strength of the depth component of the segmentation process. In some embodiments a disparity mean m and disparity standard deviation sis computed for the input disparity map D. A disparity value failing to satisfy a predetermined threshold e.g. maximum minimum mor s may then be set to a predetermined value e.g. DISP UNKNOWN to affect exclusion from the subsequent segmentation process. In some embodiments all disparity values that are above m sare removed at operation . Although operation is described in the context of depth information received as disparity data similar heuristic processing may be applied to filter a depth map. For example depth maps may be thresholded beyond a certain range based on the use case. All values in the Z and the corresponding pixels in X Y image positions that fall outside the desired depth range may be set to a predetermined value for exclusion.

For some embodiments e.g. where a 3D coordinate map is not received as an input at operation method continues to operation where a world 3D coordinate map of the image data is generated at operation . For some disparity based embodiments the world 3D coordinate map is computed at operation as 

In some exemplary embodiments the X Y Z coordinate values generated at operation or received in suitable form at operation are combined into a new representation useful for identifying 3D spatial position data clusters. The 3D spatial position representation generated at operation may further implement another heuristic that enables object segmentation to better match human perception. In some embodiments the world X Y Z coordinate values are combined into a 3D spatial position image I 

Returning to pre segmentation of the input image data is performed at operation . Pre segmentation generates an initial image segmentation labeling Iof the input scene based on a plurality of depth bins binsdetermined from clusters in the 3D spatial positional image I. The initial binning of the pixels determined at operation may then be employed in a subsequent segmentation stage of method .

At operation a set of candidate or potential 3D spatial position clustering bins are then determined by splitting or merging the initial 3D spatial position bins based on a depth sensitive threshold. The potential bins are to be utilized for clustering the objects in the scene at operation . Pseudo code for determining the potential bins potbins of bin width potbinsw according to some embodiments is provided below where cbin hbin denote the center and number of pixels in the ibin and binw is the width of each bin 

Method continues with operation where the pre segmentation image Iis generated with labels based on the potential candidate bins. 3D spatial position clusters that satisfy a predetermined minimum cluster size are separated into first connected components. A pre segmentation label such as an integer value ID is then assigned to one or more pixel in each first connected component that satisfies a predetermined minimum connected component size . Pseudo code for the pre segmentation labeling in accordance with some exemplary embodiments is provided below 

In some embodiments color clustering is utilized to resolve any pixels that do not receive a pre segmentation label at operation based on 3D spatial position clusters. At operation color clustering resolves those pixels that failed to satisfy the minimum 3D spatial position cluster size . In some embodiments a mask Iof pixels that do not have a pre segmentation label is generated e.g. if I x y 0 then I x y 1 . An indexed image Iis then generated by clustering the color R G B from the filtered image Ifor those masked pixels into a number of color bins nbins. Any color clustering technique may be utilized as embodiments are not limited in this respect. The color clusters are then separated from Iinto connected components and a new label integer ID K is assigned to each pixel within each connected component in I for example 1 bins DISP UNKNOWN in current component

Method is then complete with the pre segmentation image Ibeing an output returned to method . illustrates in gray scale a pre segmentation image generated from the Iimage and further based on the filtered input image in accordance with some embodiments. The output of operation method may include pixels with unknown segmentation ID and the scene may be under segmented or over segmented. For instance in ground region has multiple labels even though human perception would be that these labels belong to the same object.

Returning to method continues with segmentation operation . In embodiments image data is segmented at operation based at least on pixel depth pixel color and pixel position within the image frame. In exemplary embodiments the segmentation operation is further based on pre segmentation image Ioutput from operation with operation refining the pre segmentation and optimizing the problem setup to obtain a final labeling.

In some embodiments segmentation at operation is modeled as a labeling problem where the image pixels are modeled as Markov Random Field MRF which may be solved using alpha expansion also known as graph cut . The objective is then to minimize the following energy formulation using graph cut alpha expansion 6 where the first term in the summation is the Data cost and the second is the Smoothness cost . N is a neighborhood of a pixel p that includes a pixel q as further defined below. With P being the set of pixels in the filtered input image and L being the set of labels corresponding to the labels in bins i.e. L L L . . . L the function is a labeling mapping function that assigns a label L to each pixel p P.

The data cost intuitively measures the cost of assigning a given label to a given pixel. In other words the data cost term determines the cost of assigning a label to pixel p. In embodiments the data cost of assigning a label to p is a function of pixel depth pixel color pixel position and pre segmentation. In some embodiments the data cost is premised on the logic that pixels of a same depth are more likely to belong to the same object than are those at different depths. Similarly pixels of the same color are more likely to belong to the same object than are those of different color. Pixels closer together in the image plane XY are more likely to belong to the same object than are those farther apart and pixels having a same pre segmentation label are more likely to belong to a same object than are those having different pre segmentation labels. bins 7 In Eq. 7 the first term is the pixel depth component and is based on the 3D spatial position image I. The second term is the pixel color component and is based on a histogram of colors generated from pixels of Iwith label l and a number of bins hbins. Letting H b represent the bhistogram bin corresponding to color channel i. The third term is based on pixel position. A distance map is calculated with p being the normalized distance of pixel p from the nearest pixel of label l. The fourth term is based on the pre segmentation image I.

Each component of the data cost function D p is weighted by a corresponding per pixel scalar weight w w w w. Each weight is tunable and or adaptive in the sense that the weight may be varied to suit the implementation. For example where a particular 3D sensor generates lower confidence depth data wmay be reduced. In some embodiments the weights are set for a pixel p as 8 where x is one of d depth c color p position and s pre segmentation . W is the scalar weight and is a scalar offset that is configurable tunable for example by a user through the application layer of the software stack. Through manipulation of the offset objects may be segmented at operation more or less on depth for example to ensure object may be separated from the ground or not.

The data cost kernel function may be any known to be suitable for a graph cut for example absolute difference or a squared difference etc. In some exemplary embodiments the data cost kernel function is 1 9 where is scalar and in some embodiments the standard deviation of all values of x.

The smoothness component of Eq. 6 determines the cost of assigning to p and to q where p q are neighboring pixels as defined by N. The smoothness cost represents the costs for two neighboring pixels to have same or different labels. In some embodiments the neighborhood of pixel p is defined as a four nearest pixel grid i.e. q is four nearest neighbors of p . In other embodiments the neighborhood is defined to be 8 nearest pixels. Both of these exemplary neighborhood sizes are suitable for embodiments where the smoothness cost is a function of at least the pixel depth and the pixel color. In some embodiments the smoothness cost is computed as 10 where are scalars and in some embodiments are the standard deviation of all pixel values in the filtered image Iand 3D spatial image I respectively. The weights w wmay be calculated similarly to the data cost weights e.g. following Eq. 8 but with independent different weight and offset values. The above cost function including components for both color and depth intuitively maintains smoothness a same segment label over two neighboring pixels that have similar color and or depth.

Operation then completes with solving the MRF optimization setup as described above to obtain the final segmentation or labeling of the input image. Any MRF optimization techniques known may be utilized at operation as embodiments herein are not limited in this respect. illustrates multilayered segmentation of the filtered input image based on color and depth information in accordance with some embodiments. is in grayscale with each segmented object e.g. ground region and basketball player at a different gray value. Indications e.g. integer label IDs of the final segmentation may then be output as segmentation data at operation . The segmentation data may be further stored to an electronic memory in association with the input image received at operation .

Image capture device includes CM and . In the exemplary embodiment CM further includes a camera sensor and CM includes a camera sensor . Sensor may be a HD FHD QXGA WQXGA QSXGA or UHD format digital image device for example. In one embodiment sensor has at least 8 megapixel resolution. Sensor may be a HD FHD QXGA WQXGA QSXGA or UHD format digital image device for example. In one embodiment sensor has a lower pixel resolution than sensor for example 1 5 mega pixel. Although not illustrated in in further embodiments image capture device further includes a third CM including a third camera sensor substantially the same as sensor and three images output by the three sensors are utilized by the image capture device for example to provide image depth data for multilayered segmentation.

Camera sensors may provide a color resolution of 8 bits or more per pixel is operable to capture continuous video frames progressively. Sensor may have a pixel frequency of 170 MHz or more. Camera sensors may include an RGB Bayer color filter an analog amplifier an A D converter other components to convert incident light into a digital signal corresponding to raw image data. Sensors may be controlled to operate a rolling shutter or electronic focal plane shutter process where pixels are read out progressively in a line sequential fashion for a frame. In exemplary video embodiments sensors output multiple consecutively exposed frames. CM may output raw data associated with the consecutively exposed frames in conformance with any known streaming protocol such as a MIPI. Raw image video data is input to ISP . ISP is to receive and analyze frames of raw video data during the horizontal and or vertical blanking periods associated with CM . During raw image data processing ISP may perform one or more of color space conversion noise reduction pixel linearization and shading compensation for example.

Pre processed video data output by ISP may be buffered and queued as input image data ready for image segmentation. In exemplary embodiments processor implements one or more of the pre segmentation module and multilayer segmentation module . Processor may for example include one or more programmable logic circuits to perform one or more stages of the multilayer segmentation method described above. Subsystem drivers within a kernel space of an operating system OS instantiated by processor may communicate various image segmentation parameters such as camera baseline parameters reference camera designation etc.

In embodiments pre segmentation module includes logic to perform the pre segmentation operations and algorithms described elsewhere herein. In some embodiments pre segmentation module includes logic to perform one or more of the operations of pre segmentation method . In some embodiments pre segmentation module logic is implemented with programmable circuitry that has been configured through software instruction s . In some embodiments pre segmentation module includes logic to determine a plurality of pre segmentation labels for a plurality of pixels of an input image based on a distribution e.g. mass function or histogram of three dimensional 3D spatial positions e.g. I .

In embodiments multilayer segmentation module includes logic to perform the multilayer segmentation operations and algorithms described elsewhere herein. In some embodiments segmentation module includes logic to perform one or more of the operations of multilayer segmentation method . In some embodiments segmentation module logic is implemented with programmable circuitry that has been configured through software instruction s . In some embodiments multilayer segmentation module includes logic to segment the image data based at least on a pixel depth a pixel color and a pixel spatial position within the image frame. In some embodiments multilayer segmentation module includes logic to segment the image data based at least on a pixel depth a pixel color and a pixel spatial position within the image frame and the pre segmentation labels output from pre segmentation module .

Both software and hardware implementations may be well suited to implementing multilayered segmentation method . For hardware implementations pre segmentation module and or multilayer segmentation module may be implemented by fixed function logic for example provided by DSP . For software implementations any known programmable processor including a core of processor an execution unit of a graphics processor or other similar vector processor may be utilized to implement the logic of pre segmentation module and or multilayer segmentation module . Processor may be solely responsible for generating object segmentation data from input image data received from ISP . In one exemplary embodiment pre segmentation module and or multilayer segmentation module are invoked through the user space of a software stack instantiated by processor . In some embodiments processor executes a multilayered segmentation algorithm instantiated in a kernel space of the software stack. In some embodiments processor is programmed with instructions stored on a computer readable media to cause the processor to perform one or more multilayer segmentation method.

As further illustrated in image segmentation data may be output to storage display transmission pipeline . In one exemplary storage pipeline embodiment image segmentation data is written to electronic memory e.g. DDR etc. to supplement stored input image data. Memory may be separate or a part of a main memory accessible to processor . Alternatively or in addition storage display transmission pipeline is to transmit image segmentation data and or input image data off image capture device .

In one exemplary embodiment illustrated by processor further includes 3A module that is to implement one or more camera control algorithm CCA . Exemplary CCA algorithms include automatic white balancing AWB automatic focus AF and automatic exposure control AEC often referred to together as 3A control. AEC and AF involve the control of CM while AWB involves the control of ISP . Exemplary CM control parameters include aperture size shutter speed neutral density ND filter control flash power analog gain AG and digital gain DG . Exemplary ISP control parameters include white balancing gains lens shading correction LSC gains and noise suppression. In some embodiments 3A module generates camera control parameters based on image segmentation data output from multilayer segmentation module . For example 3A module may execute at least one of an automatic focus AF algorithm automatic exposure AE algorithm or automatic white balance AWB algorithm based on an image segment output from multilayer segmentation module .

An embodiment of data processing system can include or be incorporated within a server based gaming platform a game console including a game and media console a mobile gaming console a handheld game console or an online game console. In some embodiments data processing system is a mobile phone smart phone tablet computing device or mobile Internet device. Data processing system can also include couple with or be integrated within a wearable device such as a smart watch wearable device smart eyewear device augmented reality device or virtual reality device. In some embodiments data processing system is a television or set top box device having one or more processors and a graphical interface generated by one or more graphics processors .

In some embodiments the one or more processors each include one or more processor cores to process instructions which when executed perform operations for system and user software. In some embodiments each of the one or more processor cores is configured to process a specific instruction set . In some embodiments instruction set may facilitate Complex Instruction Set Computing CISC Reduced Instruction Set Computing RISC or computing via a Very Long Instruction Word VLIW . Multiple processor cores may each process a different instruction set which may include instructions to facilitate the emulation of other instruction sets. Processor core may also include other processing devices such a Digital Signal Processor DSP .

In some embodiments the processor includes cache memory . Depending on the architecture the processor can have a single internal cache or multiple levels of internal cache. In some embodiments the cache memory is shared among various components of the processor . In some embodiments the processor also uses an external cache e.g. a Level 3 L3 cache or Last Level Cache LLC not shown which may be shared among processor cores using known cache coherency techniques. A register file is additionally included in processor which may include different types of registers for storing different types of data e.g. integer registers floating point registers status registers and an instruction pointer register . Some registers may be general purpose registers while other registers may be specific to the design of the processor .

In some embodiments processor is coupled to a processor bus to transmit data signals between processor and other components in system . System has a hub system architecture including a memory controller hub and an input output I O controller hub . Memory controller hub facilitates communication between a memory device and other components of system while I O Controller Hub ICH provides connections to I O devices via a local I O bus.

Memory device can be a dynamic random access memory DRAM device a static random access memory SRAM device flash memory device or some other memory device having suitable performance to serve as process memory. Memory can store data and instructions for use when processor executes a process. Memory controller hub also couples with an optional external graphics processor which may communicate with the one or more graphics processors in processors to perform graphics and media operations.

In some embodiments ICH enables peripherals to connect to memory and processor via a high speed I O bus. The I O peripherals include an audio controller a firmware interface a wireless transceiver e.g. Wi Fi Bluetooth a data storage device e.g. hard disk drive flash memory etc. and a legacy I O controller for coupling legacy e.g. Personal System2 PS 2 devices to the system. One or more Universal Serial Bus USB controllers connect input devices such as keyboard and mouse combinations. A network controller may also couple to ICH . In some embodiments a high performance network controller not shown couples to processor bus .

System includes a device platform that may implement all or a subset of the various image segmentation methods described above in the context of . In various exemplary embodiments video processor executes image segmentation methods for example as described elsewhere herein. Video processor includes logic circuitry implementing pre segmentation module and or multilayer segmentation module to segment an image based on both color and depth for example as described elsewhere herein. In some embodiments one or more computer readable media may store instructions which when executed by CPU and or video processor cause the processor s to execute one or more of the pre segmentation and segmentation operations described elsewhere herein. One or more image data frames exposed by CM and or CM may then be stored in memory in associated with segmentation data.

In embodiments device platform is coupled to a human interface device HID . Platform may collect raw image data with CM and which is processed and output to HID . A navigation controller including one or more navigation features may be used to interact with for example device platform and or HID . In embodiments HID may include any monitor or display coupled to platform via radio and or network . HID may include for example a computer display screen touch screen display video monitor television like device and or a television.

In embodiments device platform may include any combination of CM chipset processors memory storage applications and or radio . Chipset may provide intercommunication among processors memory video processor applications or radio .

One or more of processors may be implemented as one or more Complex Instruction Set Computer CISC or Reduced Instruction Set Computer RISC processors x86 instruction set compatible processors multi core or any other microprocessor or central processing unit CPU .

Memory may be implemented as a volatile memory device such as but not limited to a Random Access Memory RAM Dynamic Random Access Memory DRAM or Static RAM SRAM . Memory may also be implemented as a non volatile storage device such as but not limited to flash memory battery backed up SDRAM synchronous DRAM magnetic memory phase change memory and the like.

Radio may include one or more radios capable of transmitting and receiving signals using various suitable wireless communications techniques. Such techniques may involve communications across one or more wireless networks. Example wireless networks include but are not limited to wireless local area networks WLANs wireless personal area networks WPANs wireless metropolitan area network WMANs cellular networks and satellite networks. In communicating across such networks radio may operate in accordance with one or more applicable standards in any version.

In embodiments system may be implemented as a wireless system a wired system or a combination of both. When implemented as a wireless system system may include components and interfaces suitable for communicating over a wireless shared media such as one or more antennas transmitters receivers transceivers amplifiers filters control logic and so forth. An example of wireless shared media may include portions of a wireless spectrum such as the RF spectrum and so forth. When implemented as a wired system system may include components and interfaces suitable for communicating over wired communications media such as input output I O adapters physical connectors to connect the I O adapter with a corresponding wired communications medium a network interface card NIC disc controller video controller audio controller and the like. Examples of wired communications media may include a wire cable metal leads printed circuit board PCB backplane switch fabric semiconductor material twisted pair wire co axial cable fiber optics and so forth.

The thresholded pixel value matching and associated object processes comporting with exemplary embodiments described herein may be implemented in various hardware architectures cell designs or IP cores. 

As described above system may be embodied in varying physical styles or form factors. further illustrates embodiments of a mobile handset device in which platform and or system may be embodied. In embodiments for example device may be implemented as a mobile computing handset device having wireless capabilities. As shown in mobile handset device may include a housing with a front and back . Device includes a display an input output I O device and an integrated antenna . Device also may include navigation features . Display may include any suitable display unit for displaying information appropriate for a mobile computing device. I O device may include any suitable I O device for entering information into a mobile computing device. Examples for I O device may include an alphanumeric keyboard a numeric keypad a touch pad input keys buttons switches microphones speakers voice recognition device and software and so forth. Information also may be entered into device by way of microphone not shown or may be digitized by a voice recognition device. Embodiments are not limited in this context. Integrated into at least the back is cameras and e.g. each including a lens an aperture and an imaging sensor both of which may be components of one or more CM through which image data is exposed and output to a multilayer image segmentation module for example as described elsewhere herein.

As exemplified above embodiments described herein may be implemented using hardware elements software elements or a combination of both. Examples of hardware elements or modules include processors microprocessors circuitry circuit elements e.g. transistors resistors capacitors inductors and so forth integrated circuits application specific integrated circuits ASIC programmable logic devices PLD digital signal processors DSP field programmable gate array FPGA logic gates registers semiconductor device chips microchips chip sets and so forth. Examples of software elements or modules include applications computer programs application programs system programs machine programs operating system software middleware firmware routines subroutines functions methods procedures software interfaces application programming interfaces API instruction sets computing code computer code code segments computer code segments data words values symbols or any combination thereof. Determining whether an embodiment is implemented using hardware elements and or software elements may vary in accordance with any number of factors considered for the choice of design such as but not limited to desired computational rate power levels heat tolerances processing cycle budget input data rates output data rates memory resources data bus speeds and other design or performance constraints.

One or more aspects of at least one embodiment may be implemented by representative instructions stored on a machine readable storage medium. Such instructions may reside completely or at least partially within a main memory and or within a processor during execution thereof by the machine the main memory and the processor portions storing the instructions then also constituting a machine readable storage media. Programmable logic circuitry may have registers state machines etc. configured by the processor implementing the computer readable media. Such logic circuitry as programmed may then be understood as physically transformed into a system falling within the scope of the embodiments described herein. Instructions representing various logic within the processor which when read by a machine may also cause the machine to fabricate logic adhering to the architectures described herein and or to perform the techniques described herein. Such representations known as cell designs or IP cores may be stored on a tangible machine readable medium and supplied to various customers or manufacturing facilities to load into the fabrication machines that actually make the logic or processor.

While certain features set forth herein have been described with reference to embodiments this description is not intended to be construed in a limiting sense. Hence various modifications of the implementations described herein as well as other implementations which are apparent to persons skilled in the art to which the present disclosure pertains are deemed to be within the spirit and scope of the present disclosure.

In one or more first embodiments an apparatus comprises an input to receive image data including at least color information and depth information for each of a plurality of pixels of an image frame an image segmentation module coupled to the input the segmentation module including logic to segment the image data based at least on a pixel depth a pixel color and a pixel spatial position within the image frame and an electronic memory to store one or more label indicative of the segmenting in association with the image data.

In furtherance of the first embodiments the segmentation module further comprises logic to determine for each pixel a scalar weight for each of the pixel depth pixel color and pixel position within the image frame and segment the image data based on a function of the pixel depth a function of the pixel color and a function of the pixel spatial position within the image frame each function weighted by a corresponding one of the scalar weights.

In furtherance of first embodiments immediately above the segmentation module further comprises logic to determine a data cost as a function of at least the weighted pixel depth the weighted pixel color and the weighted pixel spatial position within the image frame logic to determine a smoothness cost as a function of at least the pixel depth and the pixel color relative to one or more neighboring pixels logic to perform a graph cut that optimizes the data cost and smoothness cost.

In furtherance of the first embodiments the apparatus further comprises a pre segmentation module including logic to determine a plurality of pre segmentation labels for the plurality of pixels based on a distribution of three dimensional 3D spatial positions. The segmentation module further comprises logic to segment the image data based further on the pre segmentation labels.

In furtherance of the first embodiments immediately above the segmentation module further comprises logic to determine for each pixel a scalar weight for each of the pixel depth pixel color pixel position within the image frame and the pre segmentation label logic to determine a data cost as a function of at least the weighted pixel depth the weighted pixel color the weighted pixel spatial position within the image frame and the weighted pre segmentation label logic to determine a smoothness cost as a function of at least the pixel depth and the pixel color relative to one or more neighboring pixels and logic to perform a graph cut that optimizes the data cost and smoothness cost.

In furtherance of the first embodiments immediately above the pre segmentation module further comprises logic to cluster the 3D spatial position from the plurality of pixels to satisfy a minimum cluster size logic to cluster the color from the plurality of pixels failing to satisfy the minimum cluster size and logic to assign one or more pre segmentation label to each 3D spatial position cluster and each color cluster.

In furtherance of the first embodiments immediately above the pre segmentation module further comprises logic to separate the 3D spatial position clusters that satisfy the minimum cluster size into first connected components logic to assign a pre segmentation label to one or more pixel in each first connected component to satisfy a minimum connected component size logic to cluster the color from the plurality of pixels failing to satisfy the minimum connect component size logic to separate the color clusters into second connected components and logic to assign a pre segmentation label to one or more pixel in each first connected component.

In furtherance of the first embodiments immediately above the pre segmentation module further comprises logic to cluster the 3D spatial position with logic further to determine from the plurality of pixels a probability of an occurrence within each of a plurality of initial 3D spatial position bins logic further to generate candidate 3D spatial position bins by splitting or merging the initial 3D spatial position bins based on a depth sensitive threshold and logic further to assign to every pixel having a 3D spatial position value spanned by a candidate 3D spatial position bin a pre segmentation label unique to each candidate 3D spatial position bin.

In furtherance of the first embodiments the apparatus further comprises a camera to generate the image data and a 3A module coupled to the electronic memory to execute at least one of an automatic focus AF algorithm automatic exposure AE algorithm or automatic white balance AWB algorithm based on the segmentation label.

In one or more second embodiments a computer implemented image processing method comprises receiving image data including at least color information and depth information for each of a plurality of pixels of an image frame segmenting the image frame based at least on a pixel color a pixel depth and a pixel spatial position within the image frame and storing one or more label indicative of the segmenting in association with the image data.

In furtherance of the second embodiments the method further comprises determining for each pixel a scalar weight for each of the pixel depth pixel color and pixel position within the image frame and segmenting the image data based on a function of the pixel depth a function of the pixel color and a function of the pixel spatial position within the image frame each function weighted by a corresponding one of the scalar weights.

In furtherance of the second embodiments immediately above segmenting the image further comprises determining a data cost as a function of at least the weighted pixel depth the weighted pixel color and the weighted pixel spatial position within the image frame segmenting the image further comprises determining a smoothness cost as a function of at least the pixel depth and the pixel color relative to one or more neighboring pixels and segmenting the image further comprises performing a graph cut that optimizes the data cost and smoothness cost.

In furtherance of the second embodiments the method further comprises determining a plurality of pre segmentation labels for the plurality of pixels based on a distribution of three dimensional 3D spatial positions and the segmenting is based further on the pre segmentation labels.

In furtherance of the second embodiments immediately above the segmenting further comprises determining for each pixel a scalar weight for each of the pixel depth pixel color pixel position within the image frame and the pre segmentation label. The segmenting further comprises determining a data cost as a function of at least the weighted pixel depth the weighted pixel color the weighted pixel spatial position within the image frame and the weighted pre segmentation label. The segmenting further comprises determining a smoothness cost as a function of at least the pixel depth and the pixel color relative to one or more neighboring pixels the segmenting further comprises performing a graph cut that optimizes the data cost and smoothness cost.

In furtherance of the second embodiments determining the plurality of pre segmentation labels further comprises clustering the 3D spatial position from the plurality of pixels to satisfy a minimum cluster size clustering the color from the plurality of pixels failing to satisfy the minimum cluster size and assigning one or more pre segmentation label to each 3D spatial position cluster and each color cluster.

In furtherance of the second embodiments immediately above determining the plurality of pre segmentation labels further comprises separating the 3D spatial position clusters that satisfy the minimum cluster size into first connected components assigning a pre segmentation label to one or more pixel in each first connected component to satisfy a minimum connected component size clustering the color from the plurality of pixels failing to satisfy the minimum connect component size separating the color clusters into second connected components and assigning a pre segmentation label to one or more pixel in each first connected component.

In furtherance of the second embodiments immediately above determining the plurality of pre segmentation labels further comprises determining from the plurality of pixels a probability of an occurrence within each of a plurality of initial 3D spatial position bins generating candidate 3D spatial position bins by splitting or merging the initial 3D spatial position bins based on a depth sensitive threshold and assigning to every pixel having a 3D spatial position value spanned by candidate 3D spatial position bin a pre segmentation label unique to each candidate 3D spatial position bin.

In one or more third embodiments an apparatus comprises a means to perform any one of the second embodiments.

In furtherance of the one or more third embodiments the means further comprises an applications processor including a user space and a kernel space.

In one or more fourth embodiments one or more computer readable storage media has instructions stored thereon which when executed by a processor cause the processor to perform any one of the second embodiments.

In one or more fifth embodiments one or more computer readable storage media has instructions stored thereon which when executed by a processor cause the processor to perform a method comprising receiving image data including at least color information and depth information for each of a plurality of pixels of an image frame segmenting the image frame based at least on a pixel color a pixel depth and a pixel spatial position within the image frame and storing one or more label indicative of the segmenting in association with the image data.

In furtherance of the fifth embodiments the media further stores instructions thereon which when executed by a processor cause the processor to further perform a method comprising determining for each pixel a scalar weight for each of the pixel depth pixel color and pixel position within the image frame and segmenting the image data based on a function of the pixel depth a function of the pixel color and a function of the pixel spatial position within the image frame each function weighted by a corresponding one of the scalar weights.

In furtherance of the fifth embodiments immediately above the media further stores instructions thereon which when executed by a processor cause the processor to further perform the segmenting by determining a data cost as a function of at least the weighted pixel depth the weighted pixel color and the weighted pixel spatial position within the image frame by determining a smoothness cost as a function of at least the pixel depth and the pixel color relative to one or more neighboring pixels and by performing a graph cut that optimizes the data cost and smoothness cost.

In furtherance of the fifth embodiments the media further stores instructions thereon which when executed by a processor cause the processor to further perform a method comprising determining a plurality of pre segmentation labels for the plurality of pixels based on a distribution of three dimensional 3D spatial positions and the segmenting is based further on the pre segmentation labels.

In furtherance of the fifth embodiments the media further stores instructions thereon which when executed by a processor cause the processor to perform the segmentation by further performing a method comprising determining for each pixel a scalar weight for each of the pixel depth pixel color pixel position within the image frame and the pre segmentation label determining a data cost as a function of at least the weighted pixel depth the weighted pixel color the weighted pixel spatial position within the image frame and the pre segmentation label determining a smoothness cost as a function of at least the pixel depth and the pixel color relative to one or more neighboring pixels and performing a graph cut that optimizes the data cost and smoothness cost.

It will be recognized that the embodiments are not limited to the exemplary embodiments so described but can be practiced with modification and alteration without departing from the scope of the appended claims. For example the above embodiments may include specific combination of features. However the above embodiments are not limited in this regard and in embodiments the above embodiments may include undertaking only a subset of such features undertaking a different order of such features undertaking a different combination of such features and or undertaking additional features than those features explicitly listed. Scope should therefore be determined with reference to the appended claims along with the full scope of equivalents to which such claims are entitled.

