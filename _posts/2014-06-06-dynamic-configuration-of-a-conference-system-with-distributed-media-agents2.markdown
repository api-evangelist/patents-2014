---

title: Dynamic configuration of a conference system with distributed media agents
abstract: A conference controller receives access requests to access a conference session from respective callers. Responsive to the requests, the controller sends a conference identifier (ID) and respective agent discovery information to each of the callers. Each caller discovers an appropriate respective media agent based on the respective agent discovery information and sends a join request including the conference ID to that media agent. In turn, the media agents send requests for configuration information to the controller. Responsive to the requests from the media agents, the controller provides configuration information to the media agents that the media agents use to form a media connection with each other for the conference session through which the callers exchange media packets.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09614687&OS=09614687&RS=09614687
owner: Cisco Technology, Inc.
number: 09614687
owner_city: San Jose
owner_country: US
publication_date: 20140606
---
Today conference solutions are generally of two types. There are premise based conferencing systems and there are cloud based conferencing systems. Premise based systems have the benefits of keeping media associated with a conference session on premise allowing for enterprise Quality of Service management reduction of wide area network bandwidth costs low latency and so on. However premise based systems are more complicated to manage for certain topologies and require relatively high up front costs. Cloud based conference services seamlessly enable business to business and business to consumer conferencing but can incur relatively high wide area network costs latency penalties and possible quality degradation.

Presented herein are techniques to combine the best of both of a premise based conference system and a cloud based conference system. Resources in the combined conference system are dynamically configured. To this end a controller is provided to control geographically distributed media agents configured to perform media packet processing operations. The controller receives access requests to access a conference session from respective callers. Responsive to the requests the controller sends a conference identifier ID and respective agent discovery information to each of the callers. This information may be in the form of a Uniform Resource Locator URL . Each caller discovers an appropriate respective media agent based on the respective agent discovery information and sends a join request including the conference ID to that media agent. In turn the media agents send requests for configuration information to the controller. Responsive to the requests from the media agents the controller provides configuration information to the media agents that the media agents use to form a media connection with each other for the conference session through which the callers exchange media packets.

Referring to there is an illustration of a highly distributed cloud based conference system in which techniques presented herein may be implemented. This system can be referred to as a hybrid system that provides a mix of cloud based features and on premise based features which the short comings of either solution in isolation. Conference system includes a central controller located in a cloud and configured to perform overall control of the system . Conference system includes multiple geographically distributed groups or clusters of media agents A configured to operate under control of the controller . For example cluster includes media agents and cluster includes media agents cluster includes media agents and so on.

Multiple callers clients access and participate in conference sessions also referred to as conference calls through media agents under control of the controller . The example of shows two callers caller at and caller at engaged in a conference session through two associated media agents agent at and agent at in clusters and respectively. In the ensuing description media agents A are referred to collectively as media agents depending on the context. Similarly callers clients may be referred to collectively as callers and agent clusters may be referred collectively as clusters . 

The controller resides in a cloud or data center . To control system the controller performs control plane signal operations functions using such features functions afforded the Session Initiation Protocol SIP H323 access rosters and conference control operations including e.g. mute kick etc. . The controller also performs orchestration which means it is responsible for controlling the connections between media agents in different clusters to ensure that a particular conference is fully connected and provides the necessary audio and video processing functions. Clusters of media agents each reside in a respective cloud or data center shown at reference numerals which may include enterprise networks branch networks and offices carrier access networks public clouds and so on. It is desirable for users to connect to media agents which are topologically and physically near them and for users in the same location for the same conference to be connected to the same media agent. Media agents perform media e.g. audio and video packet processing operations in support of conference sessions in which callers participate such as but not limited to media packet mixing switching encoding decoding and transcoding. One or more load balancing LB servers also referred to simply as load balancer s co located and associated with each cluster perform local control and selection of media agents in that cluster. For example there are load balancers LBs and associated with cluster LBs and associated with cluster and LBs and associated with cluster . In addition the load balancer function can be integrated into the media agent so that it does not exist as a distinct physical component.

A conference session may involve the exchange of one or more of audio and video between any number of participants callers as well as the sharing of content by one participant with one or more other participants. Such shared content may include documents presentations audio content video content etc.

As will be described in detail below techniques presented herein dynamically configure highly distributed resources in conference system including controller media agents and load balancers to support conference sessions initiated by callers on an as needed basis. In support of these techniques controller media agents callers and load balancers implement and interact with each other using a variety of communication protocols to establish conference sessions and exchange media streams packets in the conference sessions. Such communication protocols include but are not limited to the Interactive Connectivity Establishment ICE protocol the Session Traversal Utilities for Network Address Translation Translator NAT STUN protocol modified extended to use STUN URLs in accordance with techniques presented herein the User Datagram Protocol UDP and the Real Time Transport Protocol RTP . The techniques described herein use the aforementioned protocols by way of example only other similar protocols may also be used instead of or in combination with the mentioned protocols as would be appreciated by one of ordinary skill in the relevant arts having access to the description presented herein.

Distributed media agents may number in the tens of thousands and be distributed geographically around the world. Similarly callers may be located anywhere in the world. Thus conference system is referred to as highly distributed. A challenge presented by such a distributed arrangement is to construct a best media topology in which callers are assigned to topologically nearest media agents in support of media exchange between callers in conference sessions. In one conventional technique the conference controller attempts to track relative locations of media agents and callers and uses control plane signaling to direct the callers to nearest agents however this does not scale upwardly to highly distributed systems because the controller cannot always determine the best nearest media agents due to outdated agent location databases and deficiencies in control plane signaling that can lead to ambiguities. Moreover media agents are prone to failure and often the conference controller may not become aware of such failure in a timely manner or at all given the network separation between the controller and the agent and inherent delays in control plane signaling caused by that separation. In addition it becomes challenging to centrally track and manage the available capacity for such a larger number of media agents. Finally network based techniques for discovery of localized media agents including anycast cannot be done utilizing a central controller.

Accordingly techniques presented herein address the challenge of assigning best media agents to callers in highly distributed conference system and rapidly recovering from failures. An example of a best media agent for a given caller is a media agent that is i available i.e. operationally capable of performing media agent operations ii topologically nearest to the caller compared to other media agents and iii has available compute networking and memory capacity to handle the conferences. The techniques delegate the process of discovering best media agents away from conference controller to media plane signaling and discovery combined with late binding configuration of media topologies i.e. arranging the best media agents for media exchange between the callers . At a high level the techniques establish or setup a conference session in two stages. A first stage call access and discovery uses call access signaling primarily between conference controller and callers that wish to access the conference session. During the call access and discovery stage controller provides information to callers to enable the callers to discover addresses of nearest available agents to support conference sessions which advantageously relieves the controller of this task. In a second stage the controller configures media agents discovered by callers in the first stage into a media topology. Callers then exchange media packets in the conference session over the so configured media topology.

High level transactions for establishing a conference session are now described. Each of multiple callers initially contact conference controller to access the conference session and in response the controller sends a URL for the conference session to each of the callers. The URL includes a conference identifier ID and information from which nearest media agents are discoverable. Alternatively instead of using a URL the conference ID and media agent discovery information can be provided to the clients directly. Using the information in the URL each of callers discovers a respective Internet Protocol IP address corresponding to a nearest media agent . Each caller sends a conference join request including the URL to a respective one of the nearest media agents perhaps through a load balancer via the discovered IP address. Each media agent receives the respective join request discovers an IP address for controller from the URL in the join request and then queries the controller to ask for further information about the conference. Controller associates the media agents that sent join requests having the same conference ID with each other and with the conference session and configures the associated media agents into the appropriate set of cascades over which the callers exchange media packets in the conference session. In embodiments in which the IP addresses discovered by callers corresponds to one or more load balancers each configured to control a respective cluster of local media agents callers send the respective join requests to the discovered load balancer s . Each load balancer selects an available agent from the local agent cluster forwards the join requests to the selected media agent and then the selected media agent forwards the join request to controller after discovering the controller as mentioned above.

In an alternative embodiment the load balancer functionality can be absorbed into the media agent. When the load balancer receives the join request it interrogates a shared database amongst the load balancers and determines which if any media agent is already servicing the conference. If there is already one assigned the load balancer redirects the client to that media agent. If not the load balancer redirects the client to a media agent in the cluster with available capacity.

Before describing the above mentioned high level transactions in detail the following definitions are provided for various components and protocols mentioned above.

Media Agent A media agent e.g. any of the media agents performs media processing functions under control of conference controller such as switching mixing transcoding presentation layout arranging and the like. A media agent is configured to form a pipeline which defines a set of internal media processing functions that are applied. These include buffering decoding mixing switching energy level computation and so on. The media agent can also be configured to form a cascade. A cascade is a connection between two media agents which carries media between them in order to extend a conference across multiple media agents. Conference controller instructs each media agent as to how to assemble the media pipeline and to which other media agents it should form cascades. Media agents may communicate with other local media agents in the same cluster over an inter media agent message bus.

Conference controller Conference controller provides overall control of initializing and configuring resources such as media agents to support a conference session. A conference session is also referred to herein as a venue. Conference controller exposes Internet web Application Programming Interfaces APIs to callers and media agents which permit remote applications to request creation and manipulation of venues. The venue is a related set of conference media streams which are logically connected together with a media pipeline and cascades i.e. media packets associated with each of the media streams are mixed together and routed through the pipeline by the media agents connected with the pipeline. Conference controller determines the composition of the media pipeline and cascades across media agents which will support the venue. For any particular conference or venue there is a single conference controller instance in charge though there may be replication of the data to other instances for purpose of high availability .

STUN Session Traversal Utilities for NAT is a standardized set of methods and a network protocol to enable an endpoint host to discover an associated public IP address of the host if the host is located behind a Network Address Translation Translator NAT . STUN permits NAT traversal for applications of real time media including voice video messaging and other interactive IP communications. STUN is intended as a tool used in other protocols such as Interactive Connectivity Establishment ICE . STUN is documented in RFCs 5389 and 7046.

ICE ICE is a technique used in computer networking involving NATs in Internet applications of Voice over IP VoIP peer to peer communications video instant messaging and other interactive media. ICE is published in RFC 5245.

STUN LB Callers that comply with the ICE standard perform STUN transactions called connectivity checks during and before transmission of Real Time Transport Protocol RTP media flows. These STUN transactions serve as identification of media streams. The STUN LB e.g. load balancer interacts with callers in the STUN transactions and in the RTP flows which follow and direct them to available media agents associated with the callers.

With reference to there is depicted a block diagram of an example generalized controller for any of conference controller media agent load balancer and caller . Conference controller media agent and load balancer may each comprise one or more computer servers controlled by an instance of generalized controller . Caller may be a client device such as but not limited to a Smartphone a tablet a laptop personal computer and the like controlled by an instance of generalized controller .

Generalized controller includes a processor that processes instructions to perform operations for a respective one of conference controller media agent load balancer and client and a memory to store a variety of data and software instructions for execution by the processor . Generalized controller also includes a network interface unit e.g. network interface card or multiple network interface cards that enables network communications so that the generalized controller can communicate with other devices as explained in further detail hereinafter. Memory may comprise read only memory ROM random access memory RAM magnetic disk storage media devices optical storage media devices flash memory devices electrical optical or other physical tangible e.g. non transitory memory storage devices. The processor is for example a microprocessor or microcontroller that executes instructions for implementing the processes described herein. Thus in general the memory may comprise one or more tangible non transitory computer readable storage media e.g. a memory device encoded with software e.g. control logic software comprising computer executable instructions and when the software is executed by the processor it is operable to perform the operations described herein. In addition memory includes a data store or database to store data used and generated by logic . Instances of memory residing in conference controller media agent caller and load balancer respectively includes conference controller logic media agent logic caller logic and load balancer logic to perform the operations for the respective device as described below.

Turning to there will now be described a series of example transaction diagrams that depict message transactions between and operations performed by the various components in system e.g. callers media agents conference controller load balancers and so on that are used to establish or setup a conference session. The examples of establish a conference session for caller and caller at reference numerals and depicted in . In the example of caller and caller connect with media agent and media agent in clusters and respectively. In other arrangements the media agents serving this conference session may reside in the same cluster. Also both caller and caller and other callers may all connect with the same media agent i.e. a single media agent handles all of the callers in a given conference session. In the ensuing description a media agent is also referred to simply as an agent. 

With reference to there is depicted an example transaction diagram for the first stage in establishing the conference session i.e. initially accessing and discovering agents for the conference session.

Transactions and described below through which caller contacts conference controller to initiate access to the conference session may rely on signaling protocols messages such as but not limited to SIP H.232 Representational State Transfer REST based APIs and the like.

At caller sends an access request in the form of an SDP offer to a locus . Locus represents a call agent or call manager that facilitates conference call setup and may offer REST based APIs to join a conference session. The SDP offer includes one or more caller identifiers IDs . Locus recognizes the SDP offer from caller as a conference call access request.

At locus sends a request to the controller to request creation of a new venue and to furthermore add the first media stream to this venue as defined by the SDP offer. The terms conference and venue as used herein are synonymous and interchangeable.

Controller receives the request from locus . Controller assigns a unique conference ID for the conference session that is about to be established for the first stream. Controller begins tracking various ones of streams from callers and agents that are will be associated with the conference session i.e. with the assigned conference ID as will be described more fully below .

Controller uses the caller IDs to retrieve pre provisioned information including e.g. domain names and or IP addresses through which candidate media agents associated with the caller IDs may be discovered. The pre provisioned information may be stored in an agent agent cluster identifier database see e.g. database in described below . Controller constructs a STUN URL for the conference session based on the retrieved information. The STUN URL includes the new conference ID and the retrieved domain names and or IP addresses. An example of a process by which controller constructs the STUN URL based on the agent agent cluster identifier database is described below in connection with . The STUN URL includes information designators e.g. the domain names that will be used by caller to discover a best one of media agents to which caller can connect in the conference session. Thus the STUN URL enables controller to delegate agent discovery to caller. In essence the STUN URL provides a layer of indirection that allows caller to discover the best media agent. The indirection is result of the fact that the STUN URL does not provide an IP address that points directly to the best media agent rather the STUN URL is used by the caller as a basis for discovery of such an IP address. Of course simplified versions are possible where the controller does provide an IP address of one or more media agents in the cluster that the client should connect to.

In the example of the STUN URL is stun wx2.com venues 2312 streams 1. The STUN URL includes a URL type designator e.g. stun a domain name e.g. wbx2.com a unique conference venue identifier e.g. 2312 and an associated media stream source identifier e.g. 1 for caller . Another example STUN URL is stun media.wbx2.com conf283711 in which the conference ID is 283711. Other forms of the STUN URL are possible.

At controller sends an SDP answer including the STUN URL to locus . The STUN URL may replace the IP address and port that would have otherwise been placed into the SDP answer. Alternative encodings of the STUN URL are possible. More generally the SDP answer includes the unique conference ID and media agent discovery information that includes any information the caller will need use to discover an appropriate e.g. nearest media agent with which to connect as described below. The URL format for this information is only one of many different formats that may be used.

Caller receives the SDP answer with the STUN URL. In response at caller discovers the best agent based on the STUN URL. STUN URL discovery techniques and example scenarios are described in detail below in connection with but are summarized here. Any discovery technique may be used by caller to resolve the STUN URL to an IP address of the nearest available agent s . For example Domain Name System DNS discovery based on the STUN URL may be used. Geo DNS and split horizon DNS resolve the URL to an IP address of an agent cluster in a domain that is geographically local to caller. For example DNS discovery based on a domain name may return to the caller an anycast IP address shared by multiple media agents in a local branch office so that caller may be connected to the nearest agent in the local branch office. Another form of discovery that may be used is a service advertisement framework SAF for example. Alternatively the STUN URL may encode an anycast address directly. Or it may be the DNS name that resolves to one or more media agents in a specific cluster that the client should connect to.

Often the discovered IP address may be that of a load balancer in an agent cluster if this is the case the load balancer will select an available agent for caller among the agents in the cluster that is local to that load balancer.

In the above described transactions the usage of STUN including the STUN URL can be considered part of the ICE protocol used to establish the conference call.

Next transactions mirror transactions except that transactions relate to caller. In transaction caller requests access to the same venue as caller and receives an SDP offer from controller that includes a STUN URL that identifies that venue i.e. includes the same conference ID as the STUN URL sent from controller to caller at . In the example of the STUN URL sent from controller to caller at identifies the same domain name identified in the STUN URL sent from the controller to caller at e.g. wx2.com but this is not necessarily the case.

At the conclusion of the transactions shown in caller and caller have used discovery techniques to resolve respective STUN URLs received from controller to IP addresses for respective first and second load balancers or for individual agents in cases where no load balancer exists .

The discovered IP addresses are used in the second stage of conference session setup which includes configuring discovered agents into a media topology connected with caller and caller to support media exchange between the callers as is now described in connection with .

With reference to there is depicted a series of transactions performed in the second stage to configure agent in relation to caller.

At caller sends a join request to join the conference session. In the example of the join request is sent in the form of a STUN request to load balancer based on the IP address discovered for that load balancer from the STUN URL during the transactions shown in . The STUN request includes attributes related to caller including an IP address of and identity credentials for caller. The STUN request also includes the STUN URL sent to caller at . The inclusion of the STUN URL in the STUN request is an extension of the standard STUN protocol. In an example caller discovered the IP address for and sends the STUN request to load balancer in cluster . Alternatively the STUN URL can be conveyed separately to the media agent outside of the STUN protocol using media plane data channels. The STUN request is sent as part of connectivity checks mandated by the ICE protocol RFC 5245 .

In and subsequent figures load balancer may be referred to as a STUN load balancer or STUN LB because the load balancer operates in accordance with the STUN protocol extended to include the STUN URL in accordance with the techniques described herein. In essence STUN load balancer acts like a Hypertext Transfer Protocol HTTP reverse proxy but for media traffic and uses STUN as signaling to convey session parameters related to establishing the conference session. In other embodiments load balancer may operate in accordance with other protocols that do not include STUN or that may be combined with STUN.

Load balancer receives the STUN request from caller. In response at load balancer selects an available agent for caller from the local cluster of agents that operates under the control of that load balancer. Load balancer makes the selection based on factors evaluated across all of the agents in the cluster such as agent availability up down status agent computational loading processing bandwidth and so on. Load balancer selection operations are described more fully below in connection with . In the example of load balancer selects agent .

At load balancer forwards the STUN request from caller to the selected agent e.g. agent . In alternative embodiments load balancer may redirect the client to connect to the selected agent. For example the load balancer functionality may be incorporate into a media agent in which case when the discovered media agent receives the join request e.g. STUN request the media agent determines the conference session from the conference ID in the request identifies a media agent that is best suited to handle the conference session and redirects the caller to connect to the best suited media agent. In this example to identify the media agent that is best suited the media agent that received the join request determines whether the conference session is already being handled by a media agent in the cluster. If it is then that agent already handling the conference session is the one best suited to handle the conference session. If it is not the media agent identifies an available agent based on capacity with the identified agent being the one best suited to handle the conference session.

The selected agent e.g. agent receives the STUN request. In response at the agent connects to the controller i.e. forms a connection with the controller. In one embodiment the agent is configured with a static domain name for the farm of controllers and the conference ID is included in the HTTP request towards this farm. Using common web service design techniques any server in the farm can process the request and it will utilize the conference ID information included in the request to fetch the state for the conference from a backend database. In an alternative embodiment the STUN URL can include additional information which Identifies by DNS name or IP address the specific controller instance handling this conference.

At agent sends an action instruction request including the STUN URL to controller discovered at . The action instruction request is a request for instructions from controller on what next action agent is to take with respect to the STUN request for the venue indicated in the STUN URL.

Controller receives the action request from agent. Controller recognizes the conference ID in the STUN URL and that caller caller and now agent are associated with that conference ID. In response to such an action request generally controller commands the agent to perform specific functions associated with the conference session such as switching media mixing transcoding layout arranging etc. and provides IP addresses of other agents to which the agent should connect to form a dynamic cascade of agents i.e. the controller configures the media agents into a media topology connected with the callers . As such controller configures the media topology as a cascade as callers connect to their respective agents one caller at a time.

Continuing with transactions in the example of in response to the action request from agent at controller sends a make pipeline instruction to agent to cause agent to setup a media pipeline over which media packets can flow to and from caller. Because a second peer agent for caller has not yet been identified to controller for this venue the make pipeline instruction to agent uses an RTP ingester to devnull command to direct agent to accept ingest media packets from caller but discard devnull the media packets. In this example it is assumed that media packet flow will be in accordance with RTP however other media transaction protocols may be used.

More generally transactions and represent communication or interaction between the media agent and the controller by which the discovered media agent requests configuration information and obtains the media configuration information from the controller that the media agent then uses to form or set up a media connection over which the caller can exchange media packets. In other words in transactions and responsive to the requests from the discovered media agent the controller provides the necessary media configuration information to the media agent.

At agent sends to load balancer a STUN response indicating the agent has completed STUN actions initiated responsive to the STUN request sent at .

At load balancer forwards the STUN response to caller. The STUN request from transaction is essentially a peer to peer connectivity check that verifies the address of load balancer agent . As a result the STUN response may return to caller a peer reflexive address of the load balancer agent.

With reference to there is depicted a series of transactions performed in the second stage to configure agent in relation to caller .

Transactions mirror transactions discussed above except that transactions relate to caller and result in selection of agent as the nearest available agent for caller. The STUN request forwarded from agent to controller at carries the same conference ID as the STUN request forwarded from caller to the controller at in . Controller associates agent and agent with the same conference session based on the conference IDs in the forwarded STUN requests. The unique conference ID in the STUN URL is an end to end unifying ID in system because the conference ID was initially sent from controller to each caller and from each caller to the STUN load balancer respective agent and controller in turn. This enables controller to delegate discovery of agents to the caller yet learn the discovered agents later in the conference setup.

In one example agent and agent are in the same cluster and access to the agents is provided by a common load balancer for that cluster. In another example agent and agent are in different clusters and access to each agent is provided through a different load balancer one for each of the different clusters . In another example a single agent may be used i.e. agent and agent are collapsed to one agent. Thus load balancer in may represent one common load balancer or alternatively two different load balancers.

At controller sends an instruction to agent directing agent to ingest media packets from caller and form a media cascade i.e. media connection with agent over which media packets may be exchanged between caller and caller.

At controller sends an instruction to agent directing agent to form finalize the media cascade initiated at with agent. As a result agent and agent form the media cascade over which caller connected with agent and caller connected with agent can exchange media packets. Agent and agent mix and transcode the media packets flowing between the callers as necessary. In this simple use case since there are only two callers the agents simply forward the media packets.

After transactions and are completed media packets can flow between caller and caller in the conference session as depicted in .

With reference to there is depicted a transaction diagram in which media packets are exchanged between caller and caller in the conference session using the media topology connection pipeline pathway established by transactions depicted in . Transactions represent media flow from caller to caller in the order caller load balancer agent agent load balancer and caller. Media packets may flow in the reverse direction as well.

With reference to there is depicted a transaction diagram for agent failure recovery in the conference session established in .

At caller detects the absence of media packets from agent for e.g. 1 second as a failure and in response initiates STUN transactions.

At load balancer aware that agent has failed selects another available agent not agent in the local cluster and forwards the STUN request to that agent e.g. to agent.

At agent proceeds as if accessing a new call. Thus agent and controller exchange media configuration messages similar to those for a new conference as discussed above at and . The configuration messages terminate with STUN responses and . These STUN transactions in the media plane not the control plane trigger the readjustment of the media plane topology to include agent as the new media agent for caller in place of failed agent. Traditional conferencing systems in this situation would require the caller to re establish the call including call signaling SDP offer answer exchanges media negotiation and discovery which are slow. In this invention the reconnection occurs only at the media plane layer using STUN or similar functionality which is faster.

With reference to there is depicted a transaction diagram of packet flow after the failure recovery implemented through transactions . Transactions represent media flow from caller to caller in the order caller load balancer agent load balancer and caller.

The STUN URL discovery techniques mentioned above are now described in detail with reference to example scenarios illustrated in .

With reference to there is depicted an example agent deployment model in which agents are deployed across various clouds. Model includes an enterprise branch that hosts only one agent . Additional agents are distributed in clusters across an enterprise campus partner clouds and a Cisco WebEx cloud . Clouds in may correspond to networks depicted in for example. Load balancers not shown in associated with each cluster in clouds provide access to local agents within that cloud. The clouds are associated with respective discovery priorities meaning that agents in enterprise branch have a higher discovery priority than the agents hosted in enterprise campus and so on down the line. Using load balancers if an agent cluster in any of clouds runs out of agent capacity that cluster will generate a STUN error when used with ICE this means the caller will connect to the highest priority cluster with available capacity. In an embodiment agents in enterprise campus or enterprise branch are used only by clients that are connected to an enterprise network. At each of the discovery priority levels a nearest agent can be discovered by a Geo DNS lookup. An anycast IP address may be preferred for enterprise branch .

With reference to there is a block diagram of an example standalone media agent deployed in either of enterprise branch or enterprise campus . Media agent corresponds to any of agents deployed in a standalone configuration that does not include multiple agents and does not include a load balancer. Standalone agent within enterprise cloud or has three IP interfaces including an anycast IP interface and an internal unicast IP interface used by callers and an external IP interface that is public facing. External IP interface operates on a known media port and is reachable from the public Internet. External IP interface is used when agent connects to other agents for cascaded media and to controller . Thus external IP interface is used for e.g. HTTP based communications with controller not shown in and RTP STUN with other agents. The IP address of interface should be pinholed in a firewall. A simplified deployment model is one in which external IP interface does not have an inbound pinhole enabled in that case the agent can only cascade with other public facing agent or it can utilize ICE techniques interagent.

Agents include full agents that implement a bi directional external IP interface . Agents may also include outbound agents that implement only an out bound external interface. In the outbound case agent uses the external IP interface for outbound HTTP transactions with controller and outbound RTP STUN transactions with other full agents. When controller orchestrates a media cascade between agents and one of the agents is outbound only that agent is connected to a full agent. The outbound agent will send RTP messages to the full agent but will begin such transactions with a STUN connectivity check to prime any firewalls then send the RTP messages. This outbound STUN RTP opens a communication pinhole for receiving reverse RTP. Alternatively the outbound agents may utilize full ICE in order to connect to each other even though both are behind firewalls. This enables a simplified configuration in the corporate firewall to allow outbound UDP and reverse from the known port and from the set of known agent IP addresses which avoids the need for a demilitarized zone DMZ box.

With reference to there is an illustration of a corporate configuration in which the above mentioned communication pinhole is opened. An outbound agent resides in an Intranet and a full agent resides in a Cisco cloud . Outbound agent sends messages to full agent in an outbound direction through a DMZ firewall a DMZ a firewall and the Internet . Alternatively in the same configuration two outbound agents can communicate with each other utilize full ICE between them in order to open communication pinholes in both firewalls.

With reference to there is depicted call setup transactions between controller and caller leading to discovery of an available agent in clouds that is nearest to the caller. The transactions include an SDP offer from caller to controller and an SDP answer returned from the controller . SDP answer includes a STUN URL list . STUN URL list lists in an order of priority from top to bottom STUN URLs for candidate agents or agent clusters a . . . . The STUN URLs can include an anycast address 1.2.3.4 for load balancers in enterprise branch and domain names . Caller discovers an available agent starting with the highest priority STUN URL anycast address and moving down the list.

With reference to there are depicted transactions for discovery of an agent using Geo DNS based on a DNS name. Typically but not necessarily a DNS name may be used for agent clusters hosted in partner clouds and the Cisco cloud e.g. WebEx . In the example of to initiate agent discovery caller sends the domain name media.ford.com to a Geo DNS server for Ford. Ford s DNS server uses Geo DNS techniques to return an IP address e.g. 10.1.2.3 of a closest agent cluster to caller . Typically the IP address returned to caller represents a virtual IP address of a load balancer of the closest agent cluster. Also the DNS lookup may produce an anycast IP address or a private address in the case of a corporate DNS. Alternatively if the load balancer function is integrated into the nodes in the cluster the DNS lookup may return a random node in the cluster. If caller is on enterprise campus corporate DNS names will be resolvable and produce a corporate internal virtual IP address or IP address of one of the nodes in the cluster. Caller sends the STUN request to load balancer identified by the returned discovered IP address see e.g. STUN request transaction in .

With reference to there is an illustration of branch discovery using an anycast address in a branch network including three branches and . Respective individual agents in each of branches share the same anycast address 10.1.2.3. When caller sends respective STUN requests to the anycast address 10.1.2.3 the STUN request will be routed to one of agents in the branch that is nearest to the caller. In a deployment of only one agent per branch with no load balancer the STUN request routes directly to the one agent. Anycast discovery offers certain advantages for example anycast discovery may be provide better discovery than GeoDNS for fine grained localized discovery. Anycast discovery is configuration free. Anycast works with UDP based services and is ideal for STUN. With anycast agents maybe highly compartmentalized enabling highly localized connectivity that would be difficult for GeoDNS to achieve.

With reference to there are depicted transactions between caller and agent used to lock down anycast IP addresses according to one embodiment. Because an RTP message sent to an anycast address may be routed to different agents . Transaction is a STUN transaction from caller to agent . This initiates a lock down of the destination IP address to that agent. Using ICE agent generates a reverse connectivity check transaction that is sent from the actual IP address of the agent. This will appear to caller as a new peer reflexive IP address and the caller will proceed to perform a check with it and use it as a higher priority.

With reference to there are depicted example transactions between caller and agent used to lock down anycast IP addresses according to another embodiment. Transactions lock down the IP address of agent using a STUN redirect that includes an ALTERNATE SERVER attribute that points to the actual IP of the agent itself.

In another alternative embodiment the locked down IP address can be provided through an out of band protocol such as a data channel protocol.

With reference to there is an illustration of resources used by controller to generate a STUN URL in response to an SDP offer from a caller e.g. caller . Drawing from examples described above caller sends an SDP offer at transaction to initiate access to a conference session. The SDP Offer includes an authorization token that carries e.g. caller ID s such as a device address a user name and so on that controller uses to identify and authorize the caller. Typically this token is carried in the HTTP request and not in the SDP per se.

Controller authenticates caller based on the caller ID s in the SDP offer based on authentication databases not shown in accessible to the controller.

Controller has access to a variety of databases including an Agent Agent Cluster Identifier database that stores a cross reference between caller IDs and agent agent cluster domain names Unicast IP addresses corresponding to clusters of agents e.g. load balancers as well as agents without load balancers . Generally database reflects the various domain names and or IP unicast addresses to which the various load balancers and agents are registered and cross references those domains addresses to caller IDs.

The entries in database may be pre provisioned. For example controller may provide an administrative portal through which an administrator may pre provision database e.g. enter the domain names and IP addresses for load balancers in agent clusters and agents without clusters and associate that information with caller IDs. Alternatively the entries of database may be generated automatically using automated discovery and configuration techniques.

Construction of database may rely on the following provisioning relationships. Every caller is associated with zero or one enterprises. This is zero for over the top callers and one for callers that are paid for by an associated enterprise. Every caller is associated with zero or one partners. This is zero for over the top callers or enterprise callers that e.g. Cisco sells to directly. It is one for partner provided enterprises. For example if Dimension Data is hosting agents and resells to Ford for a Ford caller Dimension Data is their partner. If e.g. Cisco has co location deals where agents are placed in partner data centers which are usable by any caller such data centers are considered Cisco data centers. An enterprise can optionally deploy agents at the campus level branch level or both. Through the administrative portal the administrator provisions a single IP address or domain name for branch and a single IP address or domain name for campus. For IP addresses these may be IP anycast addresses that the administrator configures to route to one of agents . For DNS names these may be GeoDNS capable and the administrator may set up GeoDNS resolution within their enterprise DNS. The administrator makes entries into database in accordance with such relationships.

Controller accesses agent cluster IDs e.g. domain names and IP addresses relevant to caller based on the authenticated ID of caller. For example controller may use the caller ID as an index to the relevant agent cluster IDs. In an embodiment there will be a single DNS name for all of e.g. Cisco s agent clusters and GeoDNS will be used to resolve the DNS name to a nearby cluster. If the caller is an enterprise user the partner campus and or branch DNS IP are obtained. These may have all been provisioned by the administrator as mentioned above.

Controller retrieves the accessed agent cluster IDs and generates the STUN URL s i.e. one URL or a URL list with the retrieved information. In the example of the retrieved URL list matches URL list depicted in based on the entries in database .

As mentioned above each agent cluster includes one or more load balancers to control agents local to that cluster. With reference to there is depicted a flowchart of an example method of load balancing across agents in a cluster. This method is performed by the load balancer for that cluster. Operations of method correspond with transactions and described above in connection with .

At the load balancer monitors determines an availability of each agent in the local cluster. The load balancer may determine an up down status i.e. operational failure status of each of agents. In addition the load balancer may determine a processor loading processing bandwidth for each agent. Other indicators factors of availability may be monitored by the load balancer.

At the load balancer determines selects one of agents to process the STUN request for the caller based on the determined agent availabilities e.g. the load balancer selects from among the agents that are determined to be operational. The select operation is made to be sticky meaning that in most but necessarily all circumstances the load balancer selects the same available agent for all STUN requests that include the same conference ID. The above mentioned stickiness represents a logical binding between the conference ID and the selected agent that will generally i.e. in most circumstances lead to that agent being selected for different callers accessing the same conference session i.e. using the same conference ID however the binding is weak enough to allow selection of different agents for the same conference ID in cases where that agent is not available due to for example insufficient processor bandwidth or failure of the agent as described below.

To this end the load balancer may perform a consistency hash over the conference ID included in the STUN request modulus a number of agents in the cluster so that the hash result is constrained to that number of agents. For example assuming 15 agents in a cluster the consistency hash will hash the conference ID to a whole number between 1 and 15 inclusive or more generally to 1 of 15 IDs for respective ones of the 15 agents. The consistency hash hashes the same conference ID to the same result to achieve stickiness so that the load balancer will select the same agent for multiple callers that have sent the same STUN URL conference ID to join access the same conference. On the other hand the consistency hash hashes different conference IDs to different hash results so the hash will select different agents across different conference IDs. Thus in this embodiment in general the load balancer selects a media agent that is determined to be operational and utilizing a consistent hash of the conference ID to the set of available media agents.

In another embodiment the load balancer randomly selects or uses a round robin technique to select an agent for a given conference ID and stores a mapping between that conference ID and the selected agent. Each time another STUN requests arrives with the same conference ID the load balancer selects the same agent based on the stored mapping.

In yet another embodiment the load balancer monitors determines the processor loading of each agent. If the load balancer detects that the processor loading of a given agent exceeds a high threshold the load balancer flags that agent as being unavailable. The load balancer selects a next agent instead of the flagged agent to handle subsequent STUN requests. Processor loading may be determined as a percentage of a total processing bandwidth that is currently utilized or alternatively a percentage of the total processing bandwidth that is currently not utilized and thus available .

In other embodiment if the load balancer detects that all of the available agents in a given cluster have exceeded the high threshold the load balancer may activate or spin up new virtual machines to be used as additional agents in order to add processing capacity to that cluster. Similarly if the load balancer detects that the processor loading of a given agent falls below a low threshold the load balancer may deactivate that agent to conserve resources.

In another embodiment the load balancer tracks the available CPU capacity of each of the nodes in the cluster. It furthermore maintains a database which can be a distributed database using DHT techniques for example and stores a mapping of venues to media agents. When a STUN request arrives the load balancer checks to see if the venue is already assigned to a media agent. If it is the load balancer redirects the client to that media agent. If it is a new venue the load balancer selects the most lightly loaded media agent stores the association of venue to that media agent and redirects the client to that media agent.

After the load balancer has selected an available agent there are two logical connections in play that are known to the load balancer including i a first logical connection between the caller and the load balancer represented as a first 5 tuple including a Source IP address caller Source Port caller destination IP address load balancer destination port load balancer and ii a second logical connection between the load balancer and the selected agent represented as a second 5 tuple including a Source IP address load balancer Source Port load balancer destination IP address agent destination port agent . The 5 tuple may also include a protocol descriptor.

At the load balancer associates the first and second logical connections to each other. For example the load balancer stores a mapping between the two logical connections such as a mapping between the first and second 5 tuples. That way when the load balancer receives a media packet from the caller over the first connection the load balancer knows to forward route the packet to the selected agent based on the association mapping between the first and second connections and vice versa.

At the load balancer supports media packet flow between the caller and the selected agent based on the stored connection mapping between the first and second connections. In an embodiment in which the media packets are RTP packets the packets do not contain the conference ID. Thus the load balancer relies on the connection mapping to perform the correct bidirectional routing forwarding of the media packets between the caller and the agent.

At if the load balancer detects that the selected agent is no longer available e.g. the agent has failed the load balancer rapidly selects a different available agent constructs and stores the appropriate connection mapping and then supports media flow between the caller and the new agent.

Alternatively the client may detect a loss in received media packets over a short period of time and construe the loss as an agent failure. This prompts the caller to resend the STUN request to the load balancer. In the meantime the load balancer may have also detected the failure. As a result the load balancer selects a different agent to handle the call and stores the appropriate connection mappings.

With reference to there is depicted an illustration of an example caller agent connection that results from performing method and in which a load balancer acts as an intermediary. In the example of load balancer has selected agent from among the 6 agents in an arbitrary cluster based on the conference ID and has formed first and second logical connections and over which media packets may flow bi directionally between the caller and the agent.

With reference to there is an illustration of global capacity handling that involves load balancing in deployment model with relatively high call numbers. Due to the presence of load balancers there is no need for a centralized awareness of overall resource availability i.e. controller does not need to be aware of or track the processor availability or utilization curves of individual agents distributed across clouds . The load balancer for each agent cluster in each of clouds monitors agent processor availability capacity and performs load balancing of the agent cluster in that cloud. As long as a given agent cluster has available processing bandwidth the local load balancer for that cluster sends a 200. When the local cluster has hit capacity the load balancer rejects STUN requests with a 500. The 200 and 500 represent response codes and as such these are messages sent from the load balancer back towards the caller client. When used in concert with ICE this will result in the client getting connected to the nearest cluster that has available capacity which is the desired result.

Techniques presented herein dynamically configure resources in a highly distribute cloud based conference system in connection with a conference session. Centralized controllers are located in the cloud. Media agents are geographically distributed in massive scale on the order of tens of thousands around the world so as to be located topologically near to callers clients i.e. users . A conference session access join process uses media path signaling e.g. STUN as part of ICE to connect a caller to a nearby media agent using any of a number of different discovery techniques including but not limited to anycast split horizon DNS and the like. Once the callers discover and connect to respective agents for the conference session the agents discover and contact the conference controller. In response the conference controller configures the agents into a media topology for the conference session. Failover is accomplished by having the client rapidly detect failure and repeat a STUN peer to peer transaction to connect to a new agent.

As a result callers at various locations can easily join and leave a conference session. The centralized controller can configure reconfigure the utilized media agents to begin or end communication with other media agents to facilitate conference session changes and or network changes. This results in optimal topologies as illustrated by the following examples. If a conference session is between callers in the same company foo.com and foo.com has a media agent deployed in its data centers the media will be directed from each caller to the agent in the company s data centers similar to a typical premise based conference session. If a conference is between callers in the same company but that company does not have a local media agent the media will be directed from each caller to the nearest cloud as in a WebEx conference session. If a conference is between callers in two different companies and both companies have an on premise media agent the callers in company A are all connected to the agent in company A. The callers in company B are all connected to the agent in company B and between them flows switched media with for example the audio and video of the top three active participants speakers . This type of topology is currently not available with conventional conference systems. An advantage is that it uses minimal WAN bandwidth.

Thus the techniques dynamically configure media topologies and effectively emulate topologies of pure cloud products pure premise products interexchange services remote dial ins and so on all within a singular system architecture. Advantages of such techniques include large scale distribution of media agents with a centralized controller an efficient and straight forward conference session joining process which uses media path discovery to connect a caller to a nearby agent a late binding control process by which the topology of media distribution is modified by the conference controller on demand as callers are connected or reconnected to nearby agents and rapid failover and recovery that uses the same topology configuration process as is used to initially establish a conference session to have a client failover to a new media agent if the previous agent fails or cannot be reached . Even further the techniques retain all of the benefits of centralized conference sessions in the cloud single conference URL single roster single SIP signaling ports singe conference control functions yet media is distributed locally for optimal usage of a wide area network WAN connection applicable to use cases with a singular architecture described above that are currently using disparate systems and create a conferencing service that easily scales upward while maintaining high quality.

Other techniques presented herein perform load balancing across a cluster of media agents in connection with a conference session. Load balancing of conference sessions is performed at the media layer using a STUN load balancer which directs STUN transactions and messages to an available back end media agent. The STUN messages include a conference ID which the load balancer can use as input to a consistent hash to route callers for the same conference session to the same media agent. The load balancer can monitor processor usage of the media agents to direct load balancing and or spin up down virtual machine instances. The load balancing techniques advantageously allow for media plane discovery in a centralized conference architecture with distributed media provide HTTP load balancing techniques at the media layer and are amenable to elastic expansion contraction of media agent server capacity allow for localized load balancing control while still retaining centralized conference state and allow routers switches to inspect STUN messages and obtain information therein as a result of the embedded conference ID in the STUN messages.

In summary in one form a method is provided comprising at a controller of a conference system including geographically distributed media agents configured to perform media packet processing operations receiving access requests to access a conference session from respective callers responsive to the requests sending a conference identifier ID and respective agent discovery information to each of the callers wherein each caller is configured to discover an appropriate respective media agent based on the respective agent discovery information and send a join request including the conference ID to that media agent and responsive to requests from the discovered media agents providing configuration information thereto that is used by the media agents to form a media connection with each other for the conference session through which the callers exchange media packets.

In summary in another form an apparatus is provided comprising a network interface unit configured to enable communications over a network with components of a conference system including callers and geographically distributed media agents configured to perform media packet processing operations and exchange content in conference sessions and a processor coupled to the network interface unit and configured to receive access requests to access a conference session from respective callers responsive to the requests send a conference identifier ID and respective agent discovery information to each of the callers wherein each caller is configured to discover an appropriate respective media agent based on the respective agent discovery information and send a join request including the conference ID to that media agent and responsive to requests from the discovered media agents provide configuration information thereto that is used by the media agents to form a media connection with each other for the conference session through which the callers exchange media packets.

In summary in yet another form a method is provided comprising at each of multiple callers initially accessing a conference session through a conference controller receiving from the controller a conference identifier ID and media agent discovery information for the conference session discovering an Internet Protocol IP address corresponding to an appropriate media agent among geographically distributed media agents based on the media agent discovery information and sending a join request including the conference ID to the discovered IP address at each of one or more media agents corresponding to the discovered IP addresses receiving the join request and communicating with the controller and at the controller associating the one or more media agents that have the same conference ID with the conference session and configuring the associated one or more media agents into a media pathway over which the callers exchange media packets in the conference session.

In summary in yet another form a method is provided comprising at a first media agent among geographically distributed media agents in a conference system including a conference controller configured to control the media agents receiving from a first caller a join request to join a conference session the join request including a conference identifier ID that was provided to the first caller by the controller receiving from the controller instructions to form a media connection with a second media agent used by a second caller in the conference session and forming the media connection with the second media agent over which media packets are exchanged between the first and second callers in the conference session.

The above description is intended by way of example only. Various modifications and structural changes may be made therein without departing from the scope of the concepts described herein and within the scope and range of equivalents of the claims.

