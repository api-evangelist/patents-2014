---

title: Scheduled network layer programming within a multi-topology computer network
abstract: In general, techniques are described for dynamically scheduling and establishing paths in a multi-layer, multi-topology network to provide dynamic network resource allocation and support packet flow steering along paths prescribed at any layer or combination of layers of the network. In one example, a multi-topology path computation element (PCE) accepts requests from client applications for dedicated paths. The PCE receives topology information from network devices and attempts to identify paths through a layer or combination of layers of the network that can be established at the requested time in view of the specifications requested for the dedicated paths and the anticipated bandwidth/capacity available in the network. The PCE schedules the identified paths through the one or more layers of the network to carry traffic for the requested paths. At the scheduled times, the PCE programs path forwarding information into network nodes to establish the scheduled paths.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09438508&OS=09438508&RS=09438508
owner: Juniper Networks, Inc.
number: 09438508
owner_city: Sunnyvale
owner_country: US
publication_date: 20140829
---
This application is a continuation of U.S. patent application Ser. No. 13 340 191 filed Dec. 29 2011 the entire content of which being incorporated herein by reference.

Large scale applications geographically distributed over large areas often process large distributed datasets that require massive data transfer over a wide area network. Service providers configure dedicated bandwidth channels over a network to provide capacity adequate to support the massive data transfer operations.

In general techniques are described for dynamically scheduling and establishing paths in a multi layer multi topology network to provide dynamic network resource allocation and support packet flow steering along paths prescribed at any layer or combination of layers of the network. The multi layer multi topology network includes an underlying base network layer having a topology of endpoint nodes connected by physical or logical links as well as one or more overlay networks that each has a topology of endpoint nodes connected by virtual links made up of paths connecting endpoints of the base network layer or a lower level overlay network.

In one example a bandwidth calendaring application BCA executing on a multi topology path computation element PCE accepts requests from client applications for one or more temporarily dedicated paths between specified endpoints. The PCE receives base network topology and overlay network topology information from network devices analyzes the various topologies to reconcile requests from multiple client applications and attempts to identify paths through a layer or combination of layers of the network that can be established at the requested time in view of the specifications requested for the temporarily dedicated paths and the anticipated bandwidth capacity available in the network.

The PCE schedules the identified paths through the one or more layers of the network to carry traffic for the requested paths. To then establish a requested path the PCE programs at the scheduled time path forwarding information into network nodes at any layer of the multi layer multi topology network that participates in forwarding traffic along the identified path. In this way the PCE may establish dedicated bandwidth channels in the form of reserved paths through the network as well as steer traffic onto the dedicated bandwidth channels to provide connectivity between distributed client applications for instance.

The techniques may provide one or more advantages. For example the BCA may have access by operation of the PCE to an enhanced view of the current state of the network at multiple different layers which may enable the BCA to identify paths that are not visible to a label edge router for example having a more limited view. The BCA may additionally by virtue of having access to this enhanced view steer traffic to underutilized portions of the network to increase the network capacity utilization. Still further using the BCA to identify establish and in some cases preempt temporarily dedicated paths for reconciling multiple possibly conflicting application requests may reduce first in time first in right access to network resources in favor of explicit centralized prioritization of application requests for dedicated paths.

In one example a method comprises receiving with a multi topology path computation element topology information for a base network layer of a multi topology network that comprises the base network layer having a plurality of network switches interconnected by base network layer three L3 links in a base network topology and also comprises an overlay network layer having a plurality of overlay switches interconnected by overlay network links in an overlay network topology wherein each of the overlay network links represents a path through the base network connecting two of the overlay switches. The method also comprises receiving with the multi topology path computation element topology information for the overlay network layer. The method further comprises receiving with the multi topology path computation element a path request that specifies two endpoints. The method also comprises computing by the multi topology path computation element a computed path to carry traffic between the two endpoints through one or more layers of the multi topology network using the topology information for the base network layer and the topology information for the overlay network layer wherein at least a portion of the computed path traverses the base network layer and wherein a first one of the network switches is an ingress network switch for the portion of the computed path. The method further comprises establishing by the multi topology path computation element a communication session with the first network switch. The method also comprises directing by the multi topology path computation element using the communication session the first network switch to establish the portion of the computed path through the base network layer.

In another example a multi topology path computation element comprises a multi topology traffic engineering database to store topology information for a base network layer of a multi topology network that comprises a plurality of network switches interconnected by base network layer three L3 links in a base network topology and to store topology information for an overlay network layer of the multi topology network that comprises a plurality of overlay switches interconnected by overlay network links in an overlay network topology wherein each of the overlay network links represents a path through the base network connecting two of the overlay switches. The multi topology path computation element also comprises a topology server interface to receive topology information for the base network layer. The multi topology path computation element further comprises an overlay controller interface to receive topology information for the overlay network layer. The multi topology path computation element also comprises a client interface to receive a path request that specifies two endpoints and a service path engine to compute a computed path to carry traffic between the two endpoints through one or more layers of the multi topology network using the topology information for the base network layer and the topology information for the overlay network layer wherein at least a portion of the computed path traverses the base network layer and wherein a first one of the network switches is an ingress network switch for the portion of the computed path. The multi topology path computation element further comprises a network switch interface to establish a communication session with the first network switch wherein the network switch interface directs the first network switch to establish the portion of the computed path through the base network layer.

In another embodiment a non transitory computer readable medium contains instructions. The instructions cause one or more programmable processors to receive with a multi topology path computation element topology information for a base network layer of a multi topology network that comprises the base network layer having a plurality of network switches interconnected by base network layer three L3 links in a base network topology and also comprises an overlay network layer having a plurality of overlay switches interconnected by overlay network links in an overlay network topology wherein each of the overlay network links represents a path through the base network connecting two of the overlay switches. The instructions also cause the processors to receive with the multi topology path computation element topology information for the overlay network layer. The instructions further cause the processors to receive with the multi topology path computation element a path request that specifies two endpoints. The instructions also cause the processors to compute by the multi topology path computation element a computed path to carry traffic between the two endpoints through one or more layers of the multi topology network using the topology information for the base network layer and the topology information for the overlay network layer wherein at least a portion of the computed path traverses the base network layer and wherein a first one of the network switches is an ingress network switch for the portion of the computed path. The instructions further cause the processors to establish by the multi topology path computation element a communication session with the first network switch. The instructions also cause the processors to direct by the multi topology path computation element using the communication session the first network switch to establish the portion of the computed path through the base network layer.

The details of one or more embodiments of the invention are set forth in the accompanying drawings and the description below. Other features objects and advantages of the invention will be apparent from the description and drawings and from the claims.

A base network layer of network or base network includes network switches A B collectively network switches connected to hosts B C and arranged in a physical topology. Network switches receive and forward packet data units PDUs for network flows according to forwarding information programmed into the switches by an administrator or external entity e.g. overlay controller or multi topology path computation element and or according to forwarding information learned by the switches whether by operation of one or more protocols e.g. interior gateway protocols IGPs or by recording information learned during PDU forwarding. Each of network switches may represent a router a layer three L3 switch a layer three L2 switch an L2 L3 switch or another network device that switches traffic according to forwarding information. Accordingly PDUs forwarded by network switches A may include for example L3 network packets e.g. Internet Protocol packets and or L2 packets e.g. Ethernet datagrams or Asynchronous Transfer Mode ATM cells . PDUs may be unicast multicast anycast and or broadcast.

An overlay network layer of network includes overlay switches A B collectively overlay switches arranged in a virtual topology over a physical topology defined by network switches . Individual links of the virtual topology of the overlay network or overlay links may be established paths through the base network and or physical links connecting overlay switches . The overlay network may represent a virtual private network VPN an OpenFlow network consisting of one or more OpenFlow switches or an application layer network with selection functionality built in to endpoint devices for example. Accordingly each of overlay switches may represent a router or routing instance e.g. a virtual routing and forwarding VRF instance a Virtual Private Local Area Network LAN Service VPLS instance a dedicated L2 L3 or L2 L3 switch or a virtual or soft switch e.g. an OpenFlow switch implemented by a router or by a dedicated switch for example. Overlay switch A for instance represents a dedicated overlay switch. Overlay switch B is implemented by network switch A and may represent for instance a soft switch. Network may include multiple overlay network layers of different or similar types e.g. multiple VPNs and or OpenFlow networks .

Topology server receives topology information from network switches for the base network of multi topology network . For example topology server may execute one or more IGPs or Exterior Gateway Protocols e.g. the Border Gateway Protocol BGP to listen to routing protocol advertisements sent by network switches . Topology server collects and stores the base network topology information then provides the base network topology information to multi topology path computation element PCE in base topology update messages . Topology information may include traffic engineering information for the network links such as the links administrative attributes and bandwidth at various priority levels available for use by label switched paths LSPs . In some examples network switches may send topology update messages to topology server that specify L2 link information for L2 links connecting the network switches. In some examples topology server is a component of PCE .

Overlay controller receives topology information for the overlay network of multi topology network in topology update messages sent by overlay switches in respective communication sessions . Topology update messages sent by overlay switches may include virtual and physical switch port information PDUs and associated metadata specifying respective ports and or interfaces on which PDUs are received. In some examples overlay controller is a routing protocol listener that executes one or more routing protocols to receive routing protocol advertisements sent by overlay switches . Such routing protocol advertisements may be associated with one or more VRFs for instance. Overlay controller collects and stores the overlay topology information then provides the overlay topology information to PCE in overlay topology update messages . In some examples overlay controller is a component of PCE .

Network switches may be configured to or otherwise directed to establish paths through the base network of multi topology network . Such paths may include for instance IP tunnels such as Generic Route Encapsulation GRE tunnels General Packet Radio Service GPRS Tunneling Protocol GTP tunnels LSPs or a simple route through the base network or a VPN identified by a static route with a route target for instance . Network switches provide path status information for paths established through the base network of multi topology network to PCE in communication sessions . Path status alternatively path state or LSP state information may include descriptors for existing operational paths as well as indications that an established path or path setup operation has failed. For example network switch A may attempt establish an LSP using a reservation protocol such as Resource reSerVation Protocol RSVP but fail due to insufficient network resources along a path specified by an Explicit Route Object ERO . As a result network switch A may provide an indication that the path setup operation failed to PCE in a communication session . PCE receives path status information and adds established paths through the base network of network as links in the overlay network topology.

PCE presents an interface by which clients A N collectively clients may request for a specified time a dedicated path between any combination of hosts . For example client A may request a 100 MB s path from host A to host B from 1 PM to 3 PM on a particular date. As another example client N may request a 50 MB s path from host A to host C from 2 PM to 3 PM on the same date. As a still further example client A may request a mesh or multipath of 50 MB s paths connecting each of hosts to one another from 4 PM to 6 PM on a particular date. The requested mesh is a multipoint to multipoint path consisting of multiple point to point paths. In addition to the bandwidth hosts and time path parameters exemplified above clients may request paths that conform to other quality of service QoS path request parameters such as latency and jitter and may further specify additional associated classifiers to identify a flow between the specified endpoints. Example flow classifiers or parameters are provided below.

PCE uses base network topology information for network received from topology server overlay network topology information for network received from overlay controller and path status information received from network switches to compute and schedule paths between hosts through network that satisfy the parameters for the paths requested by clients . PCE may receive multiple path requests from clients that overlap in time. PCE reconciles these requests by scheduling corresponding paths for the path requests that traverse different parts of network and increase capacity utilization for example or by denying some of the path requests.

At the scheduled time for a scheduled path PCE installs forwarding information to network nodes e.g. overlay switches and network switches to cause the nodes to forward traffic in a manner that satisfies the requested path parameters. In some examples PCE stores all path requests and then attempts to compute and establish paths at respective requested times. In some examples PCE receives path requests and schedules respective satisfactory paths in advance of the requested times. PCE in such examples stores the scheduled paths and uses resources allocated in advance for the scheduled paths as a constraint when attempting to compute and schedule later requested paths. For example where a scheduled path will consume all available bandwidth on a particular link at a particular time PCE may later compute a requested path at an overlapping time such that the later requested path does not include the completely subscribed link.

A requested path may traverse either or both domains of network . That is a requested path may traverse either or both of the base network and overlay network of multi topology network . For example because both host B and host C couple in the base network domain to one of network switches a requested path for traffic from host B to host C may traverse only the base network domain as a simple network route for instance from network switch A to network switch B. Host A however couples in the overlay network domain to overlay switch A. As a result any requested path for traffic between host A and host C for example first traverses the overlay network domain and then traverses the base network domain.

PCE installs forwarding information to overlay switches using overlay controller . Overlay controller presents a programming interface by which PCE may add delete and modify forwarding information in overlay switches . Forwarding information of overlay switches may include a flow table having one or more entries that specify field values for matching PDU properties and a set of forwarding actions to apply to matching PDUs. A set of one or more PDUs that match a particular flow entries represent a flow. Flows may be broadly classified using any parameter of a PDU such as source and destination MAC and IP addresses a Virtual Local Area Network VLAN tag transport layer information a Multiprotocol Label Switching MPLS or Generalized MPLS GMPLS label and an ingress port of a network device receiving the flow. For example a flow may be all PDUs transmitted in a Transmission Control Protocol TCP connection all PDUs sourced by a particular MAC address or IP address all PDUs having the same VLAN tag or all PDUs received at the same switch port.

PCE invokes the programming interface of overlay controller by sending overlay network path setup messages directing overlay controller to establish paths in the overlay network of network and or steer flows from hosts onto established paths. Overlay controller responds to overlay network path setup messages by installing to overlay switches using communication sessions forwarding information that implements the paths and or directs flows received from hosts onto established paths.

PCE installs forwarding information to network switches using communication sessions . Each of network switches may present a programming interface in the form of a management interface configuration interface and or a path computation client PCC . PCE may invoke the programming interface of network switches to configure a tunnel e.g. an LSP install static routes configure a VPLS instance configure an Integrated Routing and Bridging IRB interface and to otherwise configure network switches to forward packet flows in a specified manner. In some instances PCE directs one or more of networks switches to signal a traffic engineered LSP TE LSP through the base network of network to establish a path. In this way PCE may program a scheduled path through network by invoking a programming interface of only the head network device for the path.

At the end of a scheduled time for a requested path PCE may again invoke the programming interfaces of network switches and overlay switches to remove forwarding information implementing the requested paths. In this way PCE frees resources for future scheduled paths.

Because PCE has an enhanced view of the current state of the network at both the overlay network layer and base network PCE may identify paths that are not visible to any one of network switches or overlay switches having a more limited view. PCE may additionally by virtue of having access to this enhanced view steer traffic to underutilized portions of network to increase capacity utilization of network . In addition centralizing the path computation and establishment with PCE may allow network operators to reconcile multiple possibly conflicting application path requests and may reduce first in time first in right access to network resources in favor of explicit centralized prioritization of application requests for dedicated paths.

PCE includes a control unit and a network interface not shown to exchange packets with other network devices. Control unit may include one or more processors not shown in that execute software instructions such as those used to define a software or computer program stored to a computer readable storage medium again not shown in such as non transitory computer readable mediums including a storage device e.g. a disk drive or an optical drive or a memory such as Flash memory or random access memory RAM or any other type of volatile or non volatile memory that stores instructions to cause the one or more processors to perform the techniques described herein. Alternatively or additionally control unit may comprise dedicated hardware such as one or more integrated circuits one or more Application Specific Integrated Circuits ASICs one or more Application Specific Special Processors ASSPs one or more Field Programmable Gate Arrays FPGAs or any combination of one or more of the foregoing examples of dedicated hardware for performing the techniques described herein.

Control unit provides an operating environment for bandwidth calendaring application BCA . In one example BCA is a Java application executing on a virtual machine executed by PCE . However BCA may be implemented using any suitable programming language that produces instructions executable by a suitable platform. Furthermore while illustrated and described executing on a path computation element aspects of BCA may be delegated to other computing devices.

Bandwidth calendaring application accepts requests from client applications to schedule point to point and multipoint to multipoint paths multipaths between different endpoints. Reference herein to a path encompasses multipaths. Paths may be scheduled at different times and dates with BCA reconciling path requests from multiple client applications to schedule requested paths through a network based on requested path parameters and anticipated network resource availability.

Clients request paths through a network using client interface of BCA . In general a path request includes a requested date time a required bandwidth or other constraint and at least two endpoints. Client interface may be a command line interface CLI or graphical user interface GUI for instance. Client may also or alternatively provide an application programming interface API such as a web service. A user uses a client application to invoke client interface to input path request parameters and submit the request to BCA . Client interface receives path requests from clients and pushes the path requests to path request queue a data structure that stores path requests for computation distribution by path manager .

To compute and schedule paths through a network intelligently BCA receives topology information describing available resources at multiple layers of the network. Topology server interface illustrated as topology server IF communicates with a topology server to receive topology information for a base network layer of the network while overlay controller interface communicates with an overlay controller to receive topology information for an overlay network layer of the network. Topology server interface may include a routing protocol daemon that executes a routing protocol to receive routing protocol advertisements such as Open Shortest Path First OSPF or Intermediate System to Intermediate System IS IS link state advertisements LSAs or BGP UPDATE messages. Topology server interface may in some instances be a passive listener that neither forwards nor originates routing protocol advertisements.

In this example topology server interface receives topology information that includes traffic engineering TE information. Topology server interface may for example execute Intermediate System to Intermediate System with TE extensions IS IS TE or Open Shortest Path First with TE extensions OSPF TE to receive TE information for advertised links. Such TE information includes one or more of the link state administrative attributes and metrics such as bandwidth available for use at various LSP priority levels of links connecting routers of the domain. In some instances topology server interface executes Border Gateway Protocol to receive advertised TE information for inter AS and other out of network links. Additional details regarding executing BGP to receive TE info are found in U.S. patent application Ser. No. 13 110 987 filed May 19 2011 and entitled DYNAMICALLY GENERATING APPLICATION LAYER TRAFFIC OPTIMIZATION PROTOCOL MAPS which is incorporated herein by reference in its entirety.

Topology server interface may in some instances receive a digest of topology information collected by a topology server rather than executing a routing protocol to receive routing protocol advertisements directly. Topology server interface stores base network topology information with TE information in multi topology traffic engineering database illustrated as multi topology TED hereinafter MT TED which is stored by a computer readable storage medium of control unit for use in path computation. MT TED is described in further detail below.

Overlay controller interface illustrated as overlay controller IF receives topology information from an overlay controller that describes overlay network links connecting overlay switches. In general overlay network links are not advertised by network switches e.g. routers of the base network for the overlay network and so will not be described by topology information received by topology server interface . An overlay controller augments the base network topology with overlay network topology links by providing overlay network topology information to overlay controller interface which stores the overlay network topology information to MT TED . Overlay controller interface may receive topology information for multiple different overlay networks including VPNs and or OpenFlow networks. Different overlay networks may require different instances of overlay controller interface that communicate with network switches of the overlay network or with a topology server for example to receive overlay network topology information for respective overlay networks.

Multi topology traffic engineering database stores topology information for a base network layer and one or more overlay network layers of a network that constitutes a path computation domain for PCE . MT TED may organize topology information for respective network layers hierarchically with the base network topology information supporting the topology information for one or more overlay networks. Paths in a lower layer topology may appear as links in a higher layer topology. For example tunnels e.g. TE LSPs created in the base network layer can appears as links in an overlay network TE topology. BCA may then correlate overlay network links with paths established in the base network layer to efficiently compute paths that cross multiple overlay topologies. MT TED may include one or more link state databases LSDBs where link and node data is received in routing protocol advertisements received from a topology server and or discovered by link layer entities such as an overlay controller and then provided to BCA via overlay controller interface . In some instances an operator may configure traffic engineering or other topology information within MT TED via operator interface .

Topology server interface may also receive from a topology server or by execution of routing protocols to receive routing protocol advertisements that include reachability information endpoint information that describes endpoints reachable by specified nodes in any of the network topologies. Topology server interface may receive endpoint information for a base layer of the network as well as for one or more services e.g. VPNs provided by the network that may correspond to overlay networks of the network. Endpoint information may associate network address prefixes with a nodes of the multi topology network layers where network address prefixes may be e.g. IPv4 or IPv6. For example topology server interface may receive a BGP UPDATE message advertising a particular subnet as reachable from a particular node of the base network. As another example topology server interface may receive an Application Layer Traffic Optimization map that includes PIDs associating respective nodes of a multi topology network layer with network address prefixes reachable from the nodes. Endpoints that have network addresses that are members of the subnet are therefore reachable from the node and BCA may calculate paths for those endpoints to terminate i.e. begin or end at the node. Topology server interface stores endpoint information received for a layer to a corresponding one of endpoint databases A K illustrated as endpoint DB A K and collectively referred to as endpoint databases where K refers to a number of layers of the multi topology network that constitutes a path computation domain for PCE . Some of endpoint databases may therefore be associated with respective service instances e.g. respective VPNs that constitute overlay network layers of a multi topology network. BCA may therefore use endpoint databases to locate and validate endpoints specified in path requests received from clients.

Each of service path engines A K collectively SPEs compute requested paths through a layer of the multi topology network with which it is associated and for which it is responsible. Control unit may execute multiple SPEs concurrently e.g. as separate processes. Each of SPEs is associated with a corresponding one of generated path databases A K illustrated as generated path DB A K and collectively referred to as generated path databases . Path manager dequeues path requests from path request queue and assigns path requests to SPEs based on the layer of the multi topology network in which the endpoints reside as determined by path manager from endpoint databases . That is endpoints reachable by layers of a multi topology network that is a path computation domain for PCE are stored by at least one of endpoint databases and path manager determines the one or more endpoint databases that include endpoints specified for a dequeued path request.

Paths are unidirectional. If a client requests a bidirectional path path manager triggers two path requests for the requested path one for each direction. In some cases a path may cross multiple layers of the network e.g. at a gateway to the base layer that is implemented by one of the overlay network nodes or at a network node that participates in multiple overlay networks. In such cases multiple SPEs may cooperate to compute segments of the multi layer path that path manager stitches together at the gateway. Upon computing paths SPEs schedule the paths by storing the paths to respective generated path databases . A scheduled path stored in one of generated path databases includes path information used by path manager to establish the path in the network and may include scheduling information used by scheduler to trigger path manager to establish the path. As described in further detail below path scheduling may require locking generated path databases to perform path validation prior to committing the path.

When a servicing path request received from path manager an SPE may initially validate the request by determining from endpoint databases that the endpoints for the requested path whether expressed as logical interfaces or network addresses are known to PCE i.e. exist within the path computation domain of PCE . The SPE may additionally validate flow classifiers to ensure that the flow classifiers specified for a requested path exist. If initial validation fails for either both of these reasons the SPE rejects the requested path and path manager sends a path rejection message detailing the reasons to the requesting client via client interface .

To compute a requested path at a layer of a multi topology network a service path engine for the layer uses MT TED and the corresponding one of generated path databases for the layer to determine whether there exists a path in the layer that satisfies the TE specifications for the requested path for the duration of the requested time. SPEs may use the Djikstra constrained SPF CSPF and or the Bhandari Edge disjoint shortest pair for determining disjointed main and backup paths path computation algorithms for identifying satisfactory paths though the multi topology network. If a satisfactory computed path for the requested path exists the computing service path engine for the layer re validates the computed path and if validation is successful schedules the computed path by adding the computed path to the one of generated path databases for the layer. In addition the computing SPE adds the requested path start complete times to scheduler . A computed path added to one of generated path databases is referred to as a scheduled path until such time as path manager programs the scheduled path into the multi topology network whereupon the scheduled path becomes an active path. A scheduled or active path is a temporarily dedicated bandwidth channel for the scheduled time in which the path is or is to become operational to transport flows.

As noted above generated path databases store path information for scheduled and active paths. Path information may include an ERO that specifies a list of overlay or base network nodes for a TE LSP routes or tunnels to be configured in one or more overlay network or base network nodes forwarding information for overlay network nodes specifying respective sets of forwarding actions to apply to PDUs inbound to the overlay network nodes and or any other information usable by any of topology node interfaces to establish and steer flows onto scheduled paths in a multi topology network.

SPEs compute scheduled paths based upon a current state or snapshot of the multi topology network as represented by MT TED and generated path databases . Because multiple SPEs execute simultaneously in this example to compute and schedule paths through the multi topology network multiple SPEs may attempt to update generated path databases simultaneously which could in some cases result in network resource oversubscription and failure by PCE to satisfy requested paths. An SPE may therefore having computed a path execute a transaction that conforms to the ACID properties atomicity consistency isolation durability or another type of atomic transaction to both re validate and update generated path databases with a scheduled path. That is the SPE may first lock generated path databases to prevent other SPEs from modifying generated path databases . The SPE may then validate the computed path against the locked generated path databases as well as MT TED . If the computed path is valid the SPE updates generated path databases by adding the computed path as a scheduled path. The SPE then unlocks generated path databases . In this way all affected links are updated in the same transaction and subsequent path validations by other SPEs account for the updates. SPEs may use any suitable data structure locking mechanism such as monitors mutexes or semaphores to lock generated path databases .

If the SPE fails to validate a previously computed path the SPE attempts to re compute the path. Upon identifying a satisfactory path against the current snapshot of the multi topology network the SPE again attempts to validate the computed path and update generated path databases .

In some cases SPEs may be unable to identify a path through an overlay network with which to satisfy a path request. This failure may be due to any of a number of factors. For example sufficient network resources with which to satisfy the path request may be unavailable for the scheduled time due for instance to previously scheduled paths that include one or more links of the base network layer for any possible paths between the endpoints of the path request at an overlapping time. In this example path computation fails. In other words one or more paths between the endpoints of the path request exist but the paths are already sufficiently subscribed to prevent the additional reservation of adequate resources for the requested path. As another example SPEs may be unable to identify any paths through an overlay network between the endpoints of the path request because the computation failed due to a missing link in the overlay network. In other words the computed overlay network graph after removing unusable edges unable to satisfy path request constraints includes two disjoint subgraphs of the overlay network. However in this case a suitable path may be generated by creating a tunnel through the base layer between the subgraphs for the overlay network.

Where path computation fails because sufficient network resources do not exist at the requested time the computing SPE may consider policies set by an operator via operator interface that establish priorities among clients of PCE and or among path request parameters including bandwidth hosts time and QoS parameters as well as flow classifiers. A policy of policies may prioritize the requested path for which path computation failed over and against one or more scheduled paths of generated path databases . In such instances the computing SPE may preempt one or more of these scheduled paths by removing again in accordance with policies the paths from generated path databases and scheduler . In addition the computing SPE in such instances enqueues the removed paths as path requests to path request queue . Components of PCE may then again attempt to compute satisfactory paths for the path requests corresponding to paths removed from generated path databases . Where SPEs are unable to identify a satisfactory path for such a path request SPEs direct path manager to send a path rejection message to a requesting client that issued the path request via client interface . In effect PCE revokes a grant of scheduled multi topology network resources made to the requesting client.

Where path computation fails due to a missing link between disjoint subgraphs of an overlay network each providing reachability to respective endpoints for a requested path the computing SPE requests one of tunnel managers A K collectively tunnel managers to establish a tunnel in a lower layer of the multi topology network. For example one of SPEs for an overlay network may request a tunnel in a lower layer overlay network or in the base network layer. Each of tunnel managers is associated with one of the layers of the multi topology network and with one of generated path databases . In other words each of tunnel managers manages tunnels for one of the topologies.

Tunnel managers operate as intermediaries between generated path databases and SPEs . A higher layer SPE of SPEs may request a lower layer one of tunnel managers to establish a tunnel between two nodes of the lower layer to create a link in the higher layer. Because a tunnel traverses two layers of the multi topology network each of the two nodes may straddle the two layers by having an ingress and egress interface coupling the two layers. That is a first one of the two nodes may be an ingress network switch having an ingress interface to the base network layer while a second one of the two nodes may be an egress network switch having an egress interface from the base network layer. The tunnel manager in response may enqueue a path request specifying the two nodes in the lower layer of the multi topology network to path request queue . If a lower layer SPE is able to schedule a path for the path request this path becomes a link in the lower layer generated path database and the lower layer SPE notifies the requesting one of tunnel managers with link tunnel information for the link. The tunnel manager propagates this tunnel information to MT TED which triggers the higher layer SPE that a new link is available in the higher layer topology and prompts the higher layer SPE to reattempt computing a satisfactory path for the original requested path. Tunnel managers may also validate tunnel setup at their respective layer of a multi topology network.

Scheduler instigates path setup by tracking scheduled start times for scheduled paths in generated path databases and triggering path manager to establish the scheduled paths at their respective start times. Path manager establishes each scheduled path using one or more of topology node interfaces including overlay controller interface device management interface and network switch interface . Different instances of PCE may have different combinations of topology node interfaces .

Path manager may invoke the overlay controller interface to sending overlay network path setup messages e.g. overlay network path setup messages of directing an overlay controller to establish paths in an overlay network and or steer flows from hosts onto established paths in accordance with path information for scheduled paths in generated path databases . In this way BCA may program paths according to a permanent virtual circuit PVC or hop by hop model by programming forwarding state in network and or overlay switches to execute the paths being programmed.

Device management interface may represent a Simple Network Management Protocol SNMP interface a Device Management Interface DMI a CLI or any other network device configuration interface. Path manager may invoke device management interface to configure network switches e.g. routers with static routes TE LSPs or other tunnels in accordance with path information for scheduled paths in generated path databases . Network switch interface establishes communication sessions such as communication sessions of with network switches to receive and install path state information and to receive path setup event information. Network switch interface may be a PCE protocol PCEP interface a DMI or SNMP interface for example.

Path manager may invoke device management interface and or network switch interface to configure and direct network switches to establish paths in a base network layer or overlay network layer of a multi topology network. For example path manager may first configure a TE LSP within a network switch at a network edge then direct the network switch to signal a path for the TE LSP using RSVP with traffic engineering extensions RSVP TE or another signaling protocol. In this way BCA may program paths including TE LSPs into the network according to a soft PVC SPVC model. In this model the network presents a programming interface that BCA invokes to dynamically set up the SPVCs. In some examples BCA may use a combination of PVC and SPVC models to program paths into a multi topology network.

Upon receiving confirmation from topology node interfaces that a scheduled path setup is successful path manager transitions a status of the scheduled path in generated path databases to active. At the scheduled end time if any for an active path scheduler notifies path manager to tear down the active path using topology node interfaces . After tearing down the path path manager removes the path from generated paths .

A base network layer of multi topology network includes routers A D collectively routers connected in the illustrated topology by network links. Base network layer routers and interconnecting network links are illustrated in with a thin line weight in comparison to nodes and interconnecting communication links of the overlay network layer of multi topology network . Each of routers may represent an example of any of network switches A B of . Routers execute routing protocols to exchange routes that specify reachability to network subnets that each includes one or more of hosts A E collectively hosts . Each of hosts may represent an example of any of hosts of . For example router D provides reachability to the 3.0.0.0 8 network subnet which includes host B having network address 3.4.5.6 . As another example router B provides reachability to the 1.0.0.0 8 network subnet which includes hosts A C and D. Routers also exchange topology information by which the routers may determine paths through the base network layer to a router that provides reachability for the network subnets. Network subnets include prefixes that conform to a network addressing scheme of the base network layer. The network addressing scheme in the illustrated example is IPv4. In some examples the network addressing scheme is IPv6 or another network addressing scheme.

Each of routers may be geographically distributed over a wide area. The base network layer of multi topology network may include multiple autonomous systems that transport traffic between hosts to migrate data among distributed applications executing on hosts for example.

Path computation clients PCCs A D collectively PCCs of respective routers provide path status information for paths established through the base network of multi topology network to PCE in respective PCE protocol PCEP sessions . Path status information may include descriptors for existing operational paths as well as indications that an established path or path setup operation has failed. For example PCE may direct router A to establish an LSP over a computed path. Router A may attempt to signal the LSP using a reservation protocol such as RSVP TE but fail due to insufficient network resources along a path specified by an Explicit Route Object ERO . As a result router A may provide an indication that the path setup operation failed to PCE in a PCEP session .

PCE may be a stateful PCE that maintains synchronization not only between PCE and multi topology network base network layer topology and resource information as provided by PCCs but also between PCE and the set of computed paths and reserved resources in use in the network as provided by PCCs in the form of LSP state information. PCCs may send path setup failure and path failure event messages using LSP state report messages in extended PCEP sessions to provide LSP state information for LSPs configured in any of routers . Extensions to PCEP that include LSP state report messages are described more fully in J. Medved et al. U.S. patent application Ser. No. 13 324 861 PATH COMPUTATION ELEMENT COMMUNICATION PROTOCOL PCEP EXTENSIONS FOR STATEFUL LABEL SWITCHED PATH MANAGEMENT filed Dec. 13 2011 which is incorporated herein by reference in its entirety.

PCE receives path status information and adds established paths through the base network layer of multi topology network as links in an overlay network topology stored by PCE . The overlay network topology may be stored in an example of MT TED of . Tunnel in this example may be an instance of an established path computed by PCE and signaled by router A to reach router B. Tunnel may be a bi directional tunnel. Tunnel may thereafter be used to exchange L2 traffic between OpenFlow switch A and B. As a result tunnel is a link in the overlay topology network and is represented as such in the overlay network topology stored by PCE .

Extended PCEP sessions also allow PCE to actively update LSP parameters in PCCs that have delegated control to PCE over one or more LSPs headed by corresponding routers . The delegation and control techniques may for example allow PCE to trigger LSP re route by an LSP head end router such as any of routers in order to improve LSP placement. In addition LSP state injection using extended PCEP sessions may further enable to PCE to modify parameters of TE LSPs including bandwidth and state to synchronously coordinate demand placement thereby permitting ordered control of path reservations across network routers.

PCE may also configure new LSPs by configuring any of routers to include new LSP interfaces. For example PCE may use an example of device management interface of . to configure router A to include an LSP represented by tunnel . PCE may then use a PCEP session with PCC A to direct router A to signal the LSP toward router B. In this way PCE may program tunnels for the overlay network layer of multi topology network between any of routers .

The service provider or other administrator for network deploys Application Layer Traffic Optimization ALTO server to provide an application layer traffic optimization service over network . The application layer traffic optimization service may in some instances conform to the ALTO protocol. In general the ALTO service enables service and or content providers to influence the node selection process by applications to further service provider objectives which may include improving path computation by reducing transmission costs along network layer topology links to the provider load balancing service level discrimination accounting for bandwidth constraints decreasing round trip delay between hosts or between routers and other objectives. The ALTO service and ALTO protocol is described in further detail in J. Seedorf et al. RFC 5693 Application Layer Traffic Optimization ALTO Problem Statement Network Working Group the Internet Engineering Task Force draft October 2009 and R. Alimi et al. ALTO Protocol draft ietf alto protocol 06.txt ALTO Working Group the Internet Engineering Task Force draft October 2010 each of which is incorporated herein by reference in its entirety. Furthermore while generally described with respect to the ALTO service and ALTO servers as described in Seedorf et al. the techniques of this disclosure are applicable to any form of application layer traffic optimization.

ALTO server establishes respective peering sessions with routers A B and D that are edge routers of the base network layer of multi topology network . Each of peering sessions may comprise an Interior Border Gateway Protocol IBGP session or an exterior Border Gateway Protocol BGP session for instance. In this way ALTO Server receives in peering sessions topology information for the base network layer originated or forwarded by routing protocol speakers of multi topology network . The received topology information describes the topology of the routers base network layer of network and reachability of network address prefixes by any of routers . Peering sessions may comprise Transmission Control Protocol TCP sessions between ALTO server and routers A B and D. In some instances ALTO server may establish a single peering session with a route reflector not shown that reflects topology information to ALTO server that is received by the route reflector from routers .

Peering sessions may also or alternatively include interior gateway protocol IGP sessions between ALTO server and routers . ALTO server may operate as a passive IGP listener by peering with routers in peering sessions . That is ALTO server receives routing information from routers in peering sessions but does not originate or forward routing information for ALTO server does not route packets in its capacity as an ALTO server . Peering sessions may represent for example an OSPF or IS IS neighbor relationship or adjacency or may simply represent movement of current routing information from routers to ALTO server . In some instances peering sessions include traffic engineering extensions e.g. OSPF TE or IS IS TE and routers provide traffic engineering information to ALTO server .

ALTO server generates one or more network maps and cost maps for multi topology network using topology information received in peering sessions and provides these maps to ALTO clients such as PCE . A network map contains network location identifiers or PIDs that each represents one or more network devices in a network. In general a PID may represent a single device or device component a collection of devices such as a network subnet or some other grouping. A cost map contains cost entries for pairs of PIDs represented in the network map and an associated value that represents a cost to traverse a network path between the members of the PID pair. The value can be ordinal i.e. ranked or numerical e.g. actual . ALTO server provides the network maps and cost maps to PCE which uses the network maps and cost maps to compute paths through multi topology network .

In this example ALTO server generates at least two views of multi topology network in the form of network maps and corresponding cost maps in accordance with techniques of this disclosure a first view that constitutes an endpoint database for a base network layer e.g. an example of endpoint databases of and a second view for the base network layer that describes an L3 traffic engineering database at link level granularity where link level refers to the level of individual interfaces of routers . The second view in other words provides traffic engineering information for links connecting pairs of interfaces on respective routers . provides an example of the first view generated by ALTO server while provides an example of the second view.

Further details regarding generating network and cost maps for a network are found in Penno et al. U.S. patent application Ser. No. 12 861 645 entitled APPLICATION LAYER TRAFFIC OPTIMIZATION SERVICE SPANNING MULTIPLE NETWORKS filed Aug. 23 2010 the entire contents of which are incorporated herein by reference. Additional details regarding ALTO map updates are found in Raghunath et al. U.S. patent application Ser. No. 12 861 681 entitled APPLICATION LAYER TRAFFIC OPTIMIZATION SERVICE MAP UPDATES filed Aug. 23 2010 the entire contents of which are incorporated herein by reference.

ALTO server may comprise for example a high end server or other service device or a service card or programmable interface card PIC insertable into a network device such as a router or switch. ALTO server may operate as an element of a service plane of a router to provide ALTO services in accordance with the techniques of this disclosure. In some instances ALTO server is incorporated into PCE . ALTO server may represent an example embodiment of topology server of . Additional details regarding providing ALTO services as an element of a service plane of a router are found in Raghunath et al. incorporated above.

Multi topology network also includes overlay network layer of interconnected OpenFlow OF switches A F collectively OpenFlow switches controlled by OpenFlow controller . While the overlay network layer is an L2 network in this example the overlay network layer may be an L3 network in some instances. Each of OpenFlow switches performs packet lookups and forwarding according to one or more flow tables each having one or more flow entries. Each flow entry specifies one or more match fields and a set of instructions to apply to packets the match values of the match fields. A match field may match any of the PDU parameters described above with respect to e.g. source and destination MAC and IP addresses . The set of instructions associated with each flow entry describe PDU forwarding and PDU modifications for PDU flows. For example a set of instructions may direct one of OpenFlow switches to decrement a time to live TTL value for PDUs in matching flows and then output the PDUs to a particular outbound interface of the OpenFlow switch. Additional details regarding OpenFlow are found in OpenFlow Switch Specification version 1.1.0 OpenFlow Consortium February 2011 which is incorporated by reference herein. While not illustrated as such to simply the figure PCE may couple to ALTO server and OpenFlow controller to exchange data and control messages using communication links.

OpenFlow switches D F represent dedicated OpenFlow switches that may each be a standalone device in the form of a router L3 L2 or L2 L3 switch or another network device that switches traffic according to forwarding information. As dedicated OpenFlow switches OpenFlow switches D F do not in this example share a chassis or other hardware resources with a base network layer device e.g. any of routers . Routers A C implement corresponding OpenFlow switches A C to direct traffic on respective subsets of physical or virtual interfaces of the routers. For example router A may implement OpenFlow switch A to control a VPLS instance that switches L2 traffic among a set of interfaces that includes interfaces to OpenFlow switches B i.e. a virtual interface for tunnel E and F. In this way OpenFlow switches A C share hardware resources with corresponding routers A C.

The overlay network layer includes tunnel connecting OpenFlow switches A B. Tunnel is a service link that transports L2 communications between routers A B. Tunnel is illustrated in as a dashed lines to reflect that tunnel may not directly couple routers A B to one another but may be transported over one or more physical links and intermediate network devices that form tunnel . Tunnel may be implemented as a pseudowire operating over a TE LSP or GRE tunnel for example. Pseudowire service emulation is described in additional detail in Pseudo Wire Emulation Edge to Edge PWE3 Architecture Request for Comments 3985 Network Working Group Bryant and Pate ed. March 2005 which is incorporated by reference as if fully set forth herein.

Router B includes an integrated routing and bridging IRB interface that is a gateway between the overlay network layer and the base network layer of multi topology network . IRB interface connects the bridge domain that is the L2 overlay network layer of multi topology network to a routed domain that is the base network layer. IRB interface thus includes both a bridging instance that includes L2 learning tables as well as a routing instance mapped to the bridging instance. The bridging instance may include OpenFlow switch B operating over a VPLS or other L2 instance. IRB interface therefore acts as a L3 routing interface for a bridge domain in which OpenFlow switch B participates. In this way IRB interface provides simultaneous support for L2 bridging and L3 routing and can function as a gateway between the layers of multi topology network .

The bridge domain in this example includes subnet 1.0.0.0 8 for which router B advertises itself to other routers as providing reachability to. Elements of the overlay network e.g. hosts A C and D may identify routable L3 traffic by addressing the L3 traffic to a gateway L2 address e.g. a gateway MAC address known to IRB interface . The gateway L2 address may be a MAC address of router B a MAC address of an interface of router B that couples to an overlay network link or any other L2 address that IRB interface may use to classify PDUs arriving on an L2 interface of router B as L3 traffic.

OpenFlow controller establishes OpenFlow protocol sessions with each of OpenFlow switches to configure the flow tables therein and to receive copies of PDUs sent to OpenFlow controller by OpenFlow switches . OpenFlow switches also send OpenFlow controller identifiers for the respective physical and virtual if any ports on which PDUs are received. A port on which a PDU is received is also referred to as an in port. OpenFlow controller analyzes the received PDUs and associated in ports to determine an overlay network layer topology for multi topology network . In this example in other words OpenFlow controller performs L2 topology discovery. For example OpenFlow controller may receive a message in an OpenFlow protocol session from OpenFlow switch F that includes a copy of a PDU received by OpenFlow switch F at port P. The PDU specifies a destination MAC address D. OpenFlow controller may have previously configured OpenFlow switch D to output PDUs having destination MAC address D to port P of OpenFlow switch D. OpenFlow controller may use this information to determine that a L2 link is present in the overlay network layer between OpenFlow switch D and F. OpenFlow controller provides the discovered L2 topology to PCE which stores the L2 topology to a multi topology database which may be an example of MT TED of . OpenFlow controller may represent an example of overlay controller of . In some examples OpenFlow controller is incorporated within PCE .

PCE presents an interface by which clients may request for a specified time a dedicated path between any combination of hosts . PCE uses base network topology information for multi topology network received from ALTO server overlay network topology information for multi topology network received from OpenFlow controller and path status information received from PCCs to compute and schedule paths between hosts through multi topology network that satisfy the parameters for the paths requested by the clients. PCE may receive multiple path requests from clients that overlap in time. PCE reconciles these requests by scheduling corresponding paths for the path requests that traverse different parts of multi topology network and increase capacity utilization for example or by denying some of the path requests.

At the scheduled time for a scheduled path PCE installs forwarding information to multi topology network nodes e.g. OpenFlow switches and routers to cause the nodes to forward traffic in a manner that satisfies the requested path parameters. A requested path may traverse either or both domains of multi topology network . That is a requested path may traverse either or both of the base network layer and overlay network layer of multi topology network . Example path setup operations for different combinations of network layers traversal are described with respect to .

PCE installs forwarding information to OpenFlow switches using OpenFlow controller . OpenFlow controller presents a programming interface by which PCE may configure flow tables of OpenFlow switches using OpenFlow protocol sessions . PCE invokes the programming interface of OpenFlow controller by sending overlay network path setup messages not shown in directing OpenFlow controller to establish paths in the overlay network layer of multi topology network and or steer flows from hosts onto established paths. OpenFlow controller responds to overlay network path setup messages by installing forwarding information to OpenFlow switches that implements the paths and or directs flows received from hosts onto established paths.

PCE installs forwarding information to routers using PCEP sessions with PCCs and in some instances using network management interfaces to router routers . PCE may invoke the network management interfaces of routers to configure a tunnel e.g. an LSP install static routes configure a VPLS instance configure IRB interface and to otherwise configure routers to forward packet flows in a specified manner. PCE also communicates with PCCs to direct routers to signal LSPs through the base network layer of multi topology network to establish paths that may be used by the overlay network to transport L2 traffic along scheduled paths.

In this way the described techniques use network application programming interfaces APIs i.e. PCEP and OpenFlow to obtain topology information for multiple layers of multi topology network and also to program ephemeral forwarding information into the multiple layers. Obtaining topology information for multiple layers allows PCE to have access to a full multi topology and utilization of the network for path computation. As a result the techniques may improve network utilization by steering traffic to underutilized portions of multi topology network . In addition the techniques may avoid programming forwarding information into nodes of multi topology network using configuration methods which may require commits involving significant overhead.

PCE of includes interfaces to ALTO server OpenFlow controller and PCCs . ALTO client communicates with ALTO server using ALTO protocol to receive network and cost maps for multi topology network . In particular ALTO client receives an endpoint prefix network map for storage as base endpoint database illustrated as base endpoint DB . The endpoint prefix map and base endpoint database describes reachability to L3 prefixes from respective routers . ALTO client additional receives a network map and in some cases a cost map that describes an L3 traffic engineering database for the base network layer at link level granularity where link level refers to the level of individual interfaces of routers . ALTO client stores a representation of the network and cost map as base TE database B base TE DB B that is part of MT TED . provides an example representation of base TE DB B.

OpenFlow controller interface IF invokes an API or other interface exposing functionality of OpenFlow controller to receive overlay topology information and to configure OpenFlow switches . OpenFlow controller IF installs overlay topology information to overlay traffic engineering database A overlay TE DB A . Overlay TE DB A is an L2 topology and can change as OpenFlow controller IF receives new overlay topology information specifying the addition or modification of configurable L2 links in the overlay network such as newly added tunnels e.g. TE LSPs . Base tunnel manager may modify overlay TE DB A to include traffic engineering information for tunnels established by base tunnel manager illustrated as base tunnel mgr. . For example overlay TE DB may receive from OpenFlow controller IF overlay topology information specifying a new overlay network link that is a tunnel or generated path in base generated path DB B. Base tunnel manager may correlate the new overlay network link to the tunnel and associate the tunnel TE properties with the new overlay network link in overlay TE DB A. As a result SPEs may use overlay network link TE properties when computing requested paths through multi topology network .

PCEP interface IF implements PCE communication protocol PCEP extensions to receive and send extended PCEP messages to enable base tunnel manager . That is PCEP IF establishes extended PCEP sessions with PCCs operating on MPLS enabled routers in multi topology network . Via the extended PCEP sessions PCEP IF receives LSP state reports that include up to date LSP state for LSPs owned by the corresponding clients. When PCEP IF receives new LSP state information base tunnel manager may modify base generated path database B illustrated as base gen. path DB B to denote scheduled paths as active or to indicate path setup or path failure for example. LSP state reports may be included in PCRpt messages. LSP state received by PCEP IF and stored to base generated path database B for an LSP may include for example the LSP status e.g. up down symbolic name for inter PCEP session persistence LSP attributes such as setup priority and hold priority number of hops the reserved bandwidth a metric that has been optimized for the LSP e.g. an IGP metric a TE metric or hop counts and a path followed by the LSP. In this way PCEP IF may maintain strict synchronization between PCE and the set of computed paths and reserved resources in use in the base network layer of multi topology network . This may allow path manager to reroute paths where needed to improve network performance.

In addition PCEP IF may advertise PCE as having an LSP update capability. As a result LSP state reports received by PCEP IF may in some case include a delegation that provides access rights to PCE to modify parameters of the target LSP. In some instances the delegation may specify the particular parameters of the target LSP that are exposed for modification. Base tunnel manager invokes PCEP IF to send LSP update requests that specify the LSP parameter modifications for delegated LSPs. LSP update requests may be included in PCUpd messages and may be specified by an Explicit Route Object ERO . In this way path manager may establish paths through the base layer of multi topology network . PCEP IF also implements functionality for the operation of conventional PCEP such as path computation request reply messages.

Links connecting PIDs may be specified by an ALTO cost map that may be generated by ALTO server and denote a network link between the corresponding routers of connected pairs of PIDs . In some examples graph may not include links connecting PIDs . PCE uses graph as an endpoint database to identify ingress and egress routers of the base network layer in order to compute paths connecting hosts of multi topology network .

Each of links and link may be associated in the L2 overlay TE DB with a node identifier and a logical interface identifier. A combination of a node identifier and a logical interface identifier may be used by clients to specify an endpoint for end to end paths. The node identifier may refer to one of OpenFlow switches while a logical interface identifier refers to a virtual or physical L2 interface such as an LSP interface or GigE interface respectively.

Each of network interfaces may be associated with a network address for a router and can source and sink network traffic to from other network interfaces connected to the network interface. A network interface may be associated with multiple logical or physical interfaces. In some cases multiple PIDs may have identical router identifiers RTR IDs . This indicates that the multiple PIDs represent network interfaces located on the same router identified by the router identifier. For example PIDs A and B each include RTR ID A indicating PIDs A and B represent network interfaces A and B that are located on the router identified by RTR ID A.

Graph also includes unidirectional links connecting pairs of PIDs . Links may be specified by an ALTO cost map generated by ALTO server . Each link represents an L3 network link for the base network layer of multi topology network . Each link includes a traffic engineering cost e.g. 0 or Cx which may represent a value for bandwidth cost per megabyte latency or other traffic engineering metrics for the link. Links connecting multiple PIDs of the same router have cost 0. For example PIDs A and B located on the router identified by RTR ID A e.g. router A of have inter PID costs of 0 in both directions as indicated by the illustrated unidirectional links. In some instances ALTO server may provide multiple instances of ALTO cost maps to a path computation element where each instance includes costs for a different traffic engineering metric e.g. one ALTO cost map for bandwidth another ALTO cost map for price etc .

Router B includes a control unit and interface cards A N collectively IFCs coupled to control unit via internal links. Control unit may include one or more processors not shown in that execute software instructions such as those used to define a software or computer program stored to a computer readable storage medium again not shown in such as non transitory computer readable mediums including a storage device e.g. a disk drive or an optical drive or a memory such as Flash memory random access memory or RAM or any other type of volatile or non volatile memory that stores instructions to cause the one or more processors to perform the techniques described herein. Alternatively or additionally control unit may comprise dedicated hardware such as one or more integrated circuits one or more Application Specific Integrated Circuits ASICs one or more Application Specific Special Processors ASSPs one or more Field Programmable Gate Arrays FPGAs or any combination of one or more of the foregoing examples of dedicated hardware for performing the techniques described herein.

In this example control unit is divided into two logical or physical planes to include a first control or routing plane A control plane A and a second data or forwarding plane B data plane B . That is control unit implements two separate functionalities e.g. the routing control and forwarding data functionalities either logically e.g. as separate software instances executing on the same set of hardware components or physically e.g. as separate physical dedicated hardware components that either statically implement the functionality in hardware or dynamically execute software or a computer program to implement the functionality.

Control plane A of control unit executes the routing functionality of router B. In this respect control plane A represents hardware or a combination of hardware and software of control unit that implements routing protocols. In this example routing protocol daemon RPD is a process executed by control unit that executes routing protocols B illustrated as RPs B by which routing information stored in routing information base RIB and traffic engineering information stored in traffic engineering database TED may be determined. In addition RPD may establish peering sessions for one or more routing protocols B with another router route reflector or routing protocol listener e.g. ALTO server of and send L3 topology and or traffic engineering in RIB and or TED to the peers.

Routing protocols B may include for example IGPs such as OSPF TE or IS IS TE and or exterior gateway protocols such as BGP TE. RIB and TED may include information defining a topology of a network such as the base network layer of multi topology network of . Routing protocol daemon may resolve the topology defined by routing information in RIB to select or determine one or more routes through the network. Control plane A may then update data plane B with these routes where data plane B maintains these routes as forwarding information .

Forwarding or data plane B represents hardware or a combination of hardware and software of control unit that forwards network traffic in accordance with forwarding information . RIB may in some aspects comprise one or more routing instances implemented by router B with each instance including a separate routing table and other routing information. Control plane A in such aspects updates forwarding information with forwarding information for each of routing instances . In this respect routing instances each include separate forwarding information for use by data plane B in forwarding traffic in accordance with the corresponding routing instance. Further details of one example embodiment of a router can be found in U.S. patent application Ser. No. 12 182 619 filed Jul. 30 2008 and entitled STREAMLINED PACKET FORWARDING USING DYNAMIC FILTERS FOR ROUTING AND SECURITY IN A SHARED FORWARDING PLANE which is incorporated herein by reference.

Control plane A further includes management interface by which a network management system or in some instances an administrator using a command line or graphical user interface configures in VPLS module one or more VPLS instances for a network to interconnect combinations of L2 networks into a single Ethernet domain. For example an administrator may configure router B as a participant in a particular VPLS instance such as VPLS instance . VPLS module may perform auto discovery or other techniques to determine additional routers participating in a VPLS instance and additionally performing signaling techniques to establish a full mesh of pseudowires between router B and each of the additional routers. Furthermore while described as establishing and operating a VPLS VPLS module in various instances may establish and manage any type of L2VPN to provide an L2 emulation service that offers L2 interconnectivity.

Data plane B includes one or more forwarding units such as packet forwarding engines PFEs that provides high speed forwarding of network traffic received by interface cards via inbound links A N to outbound links A N. Integrated routing and bridging interface IRB interface of data plane B processes and forwards network traffic received on interfaces associated with the IRB interface which in this case includes interfaces associated with VPLS instance . An administrator may configure IRB interface via management interface to include VPLS instance an example of a bridging instance for a bridge domain and to map routing interface of IRB interface to one of routing instances of router B. Routing interface may represent a next hop or other reference of a logical interface IFL of IRB interface for example. In some embodiments aspects of data plane B are distributed to a number of distributed forwarding units such as packet forwarding engines each associated with a different one or more IFCs . In these embodiments IRB interface may be may be distributed to the distributed forwarding units to enable high speed integrated routing and bridging within the data plane.

Router B implements VPLS instance associated with IRB interface to operate as a virtual switch or virtual bridge to interconnect multiple L2 networks. VPLS instance maps a gateway L2 address e.g. a gateway MAC address to routing interface which maps to one of routing instances . In this respect the gateway L2 address maps to the routing instance. IRB interface classifies L2 PDUs received on an interface associated with VPLS instance and destined for a gateway L2 addresses of VPLS instance as L3 packets for routing using the one of routing instances mapped to routing interface . In other words when router B receives an L2 PDU on an interface associated with VPLS instance IRB interface determines the destination L2 address of the L2 PDU. When the destination L2 address matches the gateway L2 address mapped to routing interface IRB interface classifies the L2 PDU as an L3 packet and provides the L2 PDU to the mapped one of routing instances for L3 forwarding by data plane B. IRB interface may decapsulate the L2 PDU of the L2 header and footer. When a destination L2 address of an L2 PDU does not match the gateway L2 address VPLS instance may switch the L2 PDU according to a matching flow entry of flow table . As a result router B may operate as a gateway between an L2 overlay network layer and an L3 base network layer of multi topology network . In some instances IRB interface performs a prior logical operation to classify L2 PDU as either routing traffic or bridging traffic and then bridges the traffic or provides the traffic to a routing interface based on the result of classification.

Router A implements OpenFlow switch B to control switching of L2 PDUs among the set of virtual and or physical interfaces of router A that are associated with VPLS instance . Such interfaces may include attachment circuits for attaching L2 networks to VPLS instance . OpenFlow protocol interface IF of control plane A establishes an OpenFlow protocol session with an OpenFlow controller to provide L2 topology information and to receive forwarding information. OpenFlow protocol IF installs flow entries received in the OpenFlow protocol session to flow table to direct forwarding of PDUs received on interfaces associated with the VPLS instance . In some instances VPLS instance includes a L2 learning table and performs L2 learning with respect to interfaces of router B associated with VPLS instance .

A network management system or in some instances an administrator using a command line or graphical user interface may invoke management interface to configure label switched paths described in LSP database illustrated as LSP DB . LSP database includes LSP configuration data for example an LSP destination path e.g. a Reported Route Object and LSP attributes such as setup priority and hold priority number of hops the reserved bandwidth and or a metric that has been optimized for the LSP e.g. an IGP metric a TE metric or hop counts . LSP database may also include information designating zero or more attributes of each configured LSP as delegable parameters that may be set modified by a PCE using extended PCEP to modify the operation of the LSP when set up in the network. LSP attributes may be divided into three categories 1 non delegable parameters that RPD applies immediately using RSVP A and that are neither re signalled nor overridden by a PCE 2 delegable parameters that RPD applies when the LSP is re signaled due e.g. to LSP failure and 3 delegable parameters that may be overridden by a PCE and trigger re signaling by RPD . All delegable LSP parameters may include a configured default value that RPD applies when for example a PCEP session terminates the PCE otherwise becomes unavailable or the PCE returns a delegation.

RPD sets up LSP described in LSP database by executing a resource reservation protocol which in this instance is RSVP B that signals other routers in the network to reserve resources and provide MPLS forwarding information to RPD for use in forwarding MPLS packets. Various instances of router B may also or alternatively use RSVP TE or another Label Distribution Protocol LDP to signal LSPs. In addition RPD executes RPs B to receive traffic engineering information that affects the state of LSPs such as failed links and preempted resources that may result in a down state for LSPs. RPD may associate such LSP state information with corresponding LSPs in LSP database and may further directs path computation client B to send one or more LSP state reports to a PCE in response as described in further detail below.

Path computation client PCC B of control plane A mediates communication between RPD and a path computation element e.g. PCE of or . PCC B includes a PCE interface not shown that implements PCE communication protocol PCEP extensions to receive and send extended PCEP messages. The PCE interface also implements functionality for the operation of conventional PCEP such as path computation request reply messages.

Path computation client B establishes extended PCEP sessions with a PCE and sends via the extended PCEP sessions LSP state reports that include up to date LSP state for LSPs described in LSP state information. LSP state reports may be included in PCRpt messages. In this way PCC B maintains strict LSP state synchronization between router B and the PCE which the PCE may use when computing paths for an overlay network that make use of the LSPs.

In addition PCC B may advertise router B as allowing modification of delegable parameters. As a result LSP state reports sent by PCC B may in some case include a delegation that provides access rights to a PCE to modify parameters of the target LSP. In some instances the delegation may specify the particular parameters of the target LSP that are exposed for modification. PCC B may after delegating LSPs receive LSP update requests that specify LSP parameter modifications for one or more of the LSPs. LSP update requests may be included in PCUpd messages. PCC B in response notifies RPD of new parameters for target LSPs identified in LSP update requests. RPD may re signal the target LSPs in turn and as new LSPs are established switch traffic over to the new LSPs and send a notification to PCC B that the new LSPs have been successfully signaled. PCC B provides this updated LSP state in LSP status reports to a PCE with which router B has extended PCEP sessions. Router B thus extends existing RSVP TE functionality with an extended PCEP protocol that enables a PCE to set parameters for a TE LSP configured within the router. In this way router B may implement an SPVC like model to allow a bandwidth calendaring application executing on a PCE to signal computed paths through a multi topology network thereby dynamically setting up end to end paths as requested by clients.

PCE processes the path request according to associated path constraints if any provided by the requesting client the path. Upon successfully computing two opposing direction unidirectional paths through the overlay network layer between hosts A and B for path PCE schedules the unidirectional paths for setup at the requested time. At the scheduled time PCE directs OpenFlow controller to use OpenFlow protocol sessions to install flow entries in each of OpenFlow switches D E A and B to direct L2 PDUs from host A to host D along a unidirectional path through the overlay network layer toward host D and to direct L2 PDUs from host D to host A along a unidirectional path through the overlay network layer toward host A. In this way PCE establishes path through multi topology network using a PVC like model. An example mode of operation for PCE for establishing path is described in further detail with respect to .

PCE processes the path request according to associated path constraints if any provided by the requesting client the path. Initially PCE fails to compute two opposing direction unidirectional paths through the overlay network layer between hosts A and C for path . PCE therefore establishes tunnel by configuring routers A and C using in this example a management interface to configure the router A and C and extended PCEP sessions with respective PCCs A and C. PCE connects tunnel interfaces to OpenFlow switches A and C to create an overlay link between OpenFlow switches A and C. Tunnel having been established and installed into the overlay network layer topology PCE successfully computes the unidirectional paths for path and schedules the computed paths.

At the scheduled time PCE directs OpenFlow controller to use OpenFlow protocol sessions to install flow entries in each of OpenFlow switches D E A and C to direct L2 PDUs from host A to host C along a unidirectional path through the overlay network layer toward host C and to direct L2 PDUs from host C to host A along a unidirectional path through the overlay network layer toward host A. In this way PCE establishes path through multi topology network using a PVC like model. An example mode of operation for PCE for establishing path is described in further detail with respect to .

Client interface of bandwidth calendaring application executing on PCE receives a path request specifying two endpoints both located within an overlay network switching domain of multi topology network and also specifying path scheduling and path parameter information . While in this case the requested path is an end to end path in some instances the path request may specify multipoint to multipoint path. Endpoints may be specified as overlay network node identifier logical interface identifier pairs having additional associated classifiers to identity the one or more matching flows for the path. Endpoints may be specified using endpoint identifiers such as network addresses for hosts. Scheduling information includes a path start time indicating the date and time at which the path should be activated. Scheduling information also includes either an end time for the path or a traffic volume limit. Upon reaching a traffic volume limit PCE deactivates the path. Client interface enqueues the path request to path request queue .

Path manager dequeues the path request from path request and selects one of service path engines to process the path request . Because the path request in this example specifies endpoints in the overlay network path manager provides references to overlay TE DB A base tunnel manager and overlay generated path DB A to the selected service path engine .

The selected service path engine validates the path request by ensuring that specified endpoints exist and are reachable by in the case of hosts or located on in the case of node interface pairs one of OpenFlow switches . In some instances the selected service path engine may also ensure the validity of the specified classifiers. If the requested path is invalid NO branch of path manager sends a path rejection message via client interface to the client that requested the path . The path rejection message may detail the reasons for the rejection.

If the requested path is valid YES branch of the selected service path engine prepares constraints for path computation using the state of the multi topology network from overlay TE DB A and overlay generated path DB A based on path parameters specified in the path request. If the path start time is in the future the selected service path engine uses the maximum bandwidth available i.e. regardless of current utilization for each link of the overlay network as reflected in overlay TE DB for link utilization is indeterminate for future reservations. If however path activation is to be immediate the selected service path engine may use link utilization to determine whether path constraints may be met by a given overlay link. Further if the path start time is in the future the selected service path engine computes the scheduled bandwidth on each link for the time interval between the path start time and path end time using reserved bandwidth information for scheduled paths in overlay generated path DB A. If the scheduled bandwidth on a link exceeds a configurable threshold specified in policies the selected service path engine excludes the link from path computation in some examples the threshold is 80 utilization . Overlay generated path DB A may store bandwidth requirements for each link by time in the overlay network as a sum of bandwidth requirements from all scheduled paths that include the link. If a traffic volume rather than an end time is specified in the path request the selected service path engine may estimate the end time based on the average transfer rate on the path. For example the selected service path engine may compute the estimated end time as start time volume rate .

The selected service path engine attempts to compute a path for the path request according to the prepared constraints . The selected service path engine may lock overlay generated path DB A or otherwise execute the constraint preparation step as an atomic operation to prevent other service path engines from scheduling additional paths during constraint preparation which could lead to oversubscription of links if permitted.

If the selected service path engine successfully computes a path for the path request YES branch of the selected service path engine attempts to schedule the computed path in overlay generated path DB A. First the selected service path engine validates the computed path to ensure that path parameters may be satisfied despite other service path engines possibly scheduling additional paths to overlay generated path DB A . If the computed path has been invalidated in the interim by such a circumstance NO branch of the selected service path engine re prepares the constraints and again attempts to compute a path for the path request according to the prepared constraints .

If the computed path remains valid YES branch of the selected service path engine schedules the computed path to overlay generated path DB A for the associated start and end times . Validation and path scheduling may be executed atomically by the selected path engine . At the start time for the scheduled path scheduler triggers path manager to program the scheduled path into the overlay network layer of multi topology network by using OpenFlow controller interface to direct OpenFlow controller to install flow table entries that forward matching traffic along the scheduled path . As described in detail with respect to in some instances one of service path engines may validate the scheduled path prior as part of an activation process. After establishing and in some instances receiving confirmation from OpenFlow controller that the schedule path is operational path manager marks the scheduled path as active within overlay generated path DB A .

If the selected service path engine is unable to compute a path according to the determined constraints NO branch of the selected service path engine determines whether such failure due to the computed overlay network graph after removing unusable edges unable to satisfy path request constraints including two disjoint subgraphs of the overlay network. If this is not the case NO branch of path manager sends a path rejection message via client interface to the client that requested the path . The path rejection message may detail the reasons for the rejection where such reasons in this case include being unable to compute the requested path.

If the path could possibly be computed with a different overlay network topology however that connected the two disjoint subgraphs i.e. if there exists a base network layer path through the base network layer that connects the two disjoint subgraphs YES branch of then the selected service path engine requests overlay TE DB A to add an overlay link between a pair of OpenFlow switches with one member of the pair being drawn from each of the subgraphs. The selected service path engine may provide hints for the nodes to operate as endpoints for the overlay link. For example with respect to the selected service path engine requests overlay TE DB A to create an overlay link between OpenFlow switches A and C. Overlay TE DB A in turn requests a new tunnel through the base layer between the pair of OpenFlow switches which overlap as base layer network nodes from base tunnel manager .

Base tunnel manager maps the location of the overlay network nodes to the corresponding base layer network nodes. For example OpenFlow switches A and C are located on respective routers A and C. Base tunnel manager then generates a path request for a bidirectional tunnel connecting the corresponding base layer network nodes and enqueues the path request on path request queue . Path manager computes and establishes the requested path according to techniques of this disclosure . After the tunnel is established base tunnel manager connects logical interfaces for the tunnel which may include bidirectional TE LSPs to the OpenFlow switches to create the overlay network link . Base tunnel manager also notifies overlay TE DB A that the requested overlay network link is active which triggers recomputation .

PCE processes the path request according to associated path constraints if any provided by the requesting client the path. Upon determining that host A is reachable by the overlay network layer while host B is not PCE establishes sub path between host A and IRB interface using techniques of this disclosure. For example PCE may program OF switches D E and A to forward L3 traffic destined for host B toward the gateway L2 address for IRB interface . PCE may obtain the gateway L2 address using the Address Resolution Protocol ARP . In addition PCE may establish a bi directional tunnel for sub path from router B to router D using an extended PCEP session with PCC B. PCE may bind this tunnel to a forwarding equivalence class FEC for a subnet reachable from router D that includes host B e.g. 3.0.0.0 8 or for the host specific classifier e.g. 3.4.5.6 . In the other direction PCE may bind this tunnel to a FEC for a subnet reachable from router B that includes host A e.g. 1.0.0.0 8 or for the host specific classifier e.g. 1.2.3.4 . PCE may in this way connect sub paths and over respective layers of multi topology network using IRB interface to create a dedicated bidirectional path between hosts A and B in response to a path request from a client and groom flows directed to either of the hosts onto path .

PCE processes the path request according to associated path constraints if any provided by the requesting client the path. Upon computing a path over the base network layer between routers A and B PCE creates a bi directional tunnel between routers A and B for path using extended PCEP session with PCCs A and B and by configuring if needed tunnel interfaces in routers A and B using respective management interfaces of the routers. PCE may bind this tunnel to a forwarding equivalence class FEC for a subnet reachable from router B that includes host B e.g. 3.0.0.0 8 or for the host specific classifier e.g. 3.4.5.6 . In the other direction PCE may bind this tunnel to a FEC for a subnet reachable from router A that includes host E e.g. 4.0.0.0 8 or for the host specific classifier e.g. 4.5.6.7 . PCE may in this way establish a path over a base network layer of multi topology network to create a dedicated bidirectional path between hosts B and E in response to a path request from a client and groom flows directed to either of hosts B and E onto path .

If the scheduled path is invalid NO branch of the selected service path engine attempts to recomputed the path according to techniques described herein . If the recomputation is successful YES branch of path manager programs the scheduled path into multi topology network using any one or more of topology node interfaces . If the recomputation is unsuccessful NO branch of path manager determines by policies whether any currently active path in overlay generated path DB A is preempted by e.g. has a lower priority than the scheduled path. If no active paths are available to preempt NO branch of the scheduled path fails and path manager sends a path rejection message detailing the reasons to the requesting client via client interface .

If however a currently active path stored by overlay generated path DB A may be preempted for the scheduled path YES branch of path manager does so by putting the active path into a failed state and removing it from overlay generated path DB A . This may include tearing down the now failed path. Path manager then selects one of service path engines to recompute the scheduled path in a further attempt to establish the requested path .

The techniques described herein may be implemented in hardware software firmware or any combination thereof. Various features described as modules units or components may be implemented together in an integrated logic device or separately as discrete but interoperable logic devices or other hardware devices. In some cases various features of electronic circuitry may be implemented as one or more integrated circuit devices such as an integrated circuit chip or chipset.

If implemented in hardware this disclosure may be directed to an apparatus such a processor or an integrated circuit device such as an integrated circuit chip or chipset. Alternatively or additionally if implemented in software or firmware the techniques may be realized at least in part by a computer readable data storage medium comprising instructions that when executed cause a processor to perform one or more of the methods described above. For example the computer readable data storage medium may store such instructions for execution by a processor.

A computer readable medium may form part of a computer program product which may include packaging materials. A computer readable medium may comprise a computer data storage medium such as random access memory RAM read only memory ROM non volatile random access memory NVRAM electrically erasable programmable read only memory EEPROM Flash memory magnetic or optical data storage media and the like. In some examples an article of manufacture may comprise one or more computer readable storage media.

In some examples the computer readable storage media may comprise non transitory media. The term non transitory may indicate that the storage medium is not embodied in a carrier wave or a propagated signal. In certain examples a non transitory storage medium may store data that can over time change e.g. in RAM or cache .

The code or instructions may be software and or firmware executed by processing circuitry including one or more processors such as one or more digital signal processors DSPs general purpose microprocessors application specific integrated circuits ASICs field programmable gate arrays FPGAs or other equivalent integrated or discrete logic circuitry. Accordingly the term processor as used herein may refer to any of the foregoing structure or any other structure suitable for implementation of the techniques described herein. In addition in some aspects functionality described in this disclosure may be provided within software modules or hardware modules.

Various embodiments have been described. These and other embodiments are within the scope of the following examples.

