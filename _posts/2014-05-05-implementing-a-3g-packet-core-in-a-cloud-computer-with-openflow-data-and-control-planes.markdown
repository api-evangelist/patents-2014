---

title: Implementing a 3G packet core in a cloud computer with openflow data and control planes
abstract: A control plane device in a cloud computing system executes a plurality of virtual machines for implementing network function virtualization (NFV). The control plane device is operable to manage implementation of a general packet radio service (GPRS) tunnel protocol (GTP) in a packet core (PC) of a third generation (3G) network having a split architecture where a control plane of the PC of the 3G network is in the cloud computing system. The control plane communicates with a data plane of the PC through a control plane protocol. The data plane is implemented in a plurality of network devices of the 3G network. The control plane device and the plurality of virtual machines are operable to communicate with other control plane devices in the cloud computing system and with the plurality of network devices of the data plane.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09167501&OS=09167501&RS=09167501
owner: Telefonaktiebolaget L M Ericsson (publ)
number: 09167501
owner_city: Stockholm
owner_country: SE
publication_date: 20140505
---
The present patent application is a Continuation In Part of application Ser. No. 13 220 471 filed Aug. 29 2011.

The embodiments of the invention relate to a method and system for implementing a control plane of a third generation packet core in a cloud computer system. Specifically the embodiments of the invention relate to the use of the OpenFlow protocol to implement control of a data plane by the control plane being executed in a cloud computer system.

The general packet radios system GPRS is a system that is used for transmitting Internet Protocol packets between user devices such as cellular phones and the Internet. The GPRS system includes the GPRS core network which is an integrated part of the global system for mobile communication GSM . These systems are widely utilized by cellular phone network providers to enable cellular phone services over large areas.

The GPRS tunneling protocol GTP is an important communication protocol utilized within the GPRS core network. GTP enables end user devices e.g. cellular phones in a GSM network to move from place to place while continuing to connect to the Internet. The end user devices are connected to the Internet through a gateway GPRS support node GGSN . The GGSN tracks the end user device s data from the end user device s serving GPRS support node SGSN that is handling the session originating from the end user device.

Three forms of GTP are used by the GPRS core network. GTP U is used for transfer of user data in separated tunnels for each packet data protocol PDP context. GTP C is used within the GPRS core network for signaling between GGSN and SGSN. GTP is used for carrying charging data from Charging Data Function CDF of the GSM or UMTS network to the charging gateway function CGF which provides requisite end user device usage information to a billing system. GTP uses the same message structure as GTP C and GTP U.

A control plane device in a cloud computing system executes a plurality of virtual machines for implementing network function virtualization NFV . The control plane device is operable to manage implementation of a general packet radio service GPRS tunnel protocol GTP in a packet core PC of a third generation 3G network having a split architecture where a control plane of the PC of the 3G network is in the cloud computing system. The control plane communicates with a data plane of the PC through a control plane protocol. The data plane is implemented in a plurality of network devices of the 3G network. The control plane device and the plurality of virtual machines are operable to communicate with other control plane devices in the cloud computing system and with the plurality of network devices of the data plane.

The control plane device includes a storage medium to store a centralized control plane software including control plane modules for implementing the control plane of the PC and a processor communicatively coupled to the storage medium. The processor is operable to execute the plurality of virtual machines where at least one of the plurality of virtual machines is operable to execute the centralized control plane CCP software including at least one of the control plane modules. Each control plane module provides a set of control plane functions for managing the data plane. The CCP software is operable to receive a request to create a GTP tunnel in the PC of the 3G network between a serving GPRS support node SGSN and a gateway GPRS support node GGSN for a subscriber session. The CCP software is operable to configure a switch implementing a data plane of the SGSN via the control plane protocol to encapsulate and decapsulate packets of the subscriber session and to establish a first GTP tunnel endpoint. The CCP software is operable to configure at least one switch in a route of the GTP tunnel via the control plane protocol to forward packets of the subscriber session according to the GTP tunnel and the CCP software is operable to configure a switch implementing a data plane of the GGSN via the control plane protocol to encapsulate and decapsulate the packets of the subscriber session and to establish a second GTP tunnel endpoint.

The following description describes methods and apparatus for computation of . In the following description numerous specific details such as logic implementations opcodes means to specify operands resource partitioning sharing duplication implementations types and interrelationships of system components and logic partitioning integration choices are set forth in order to provide a more thorough understanding of the present invention. It will be appreciated however by one skilled in the art that the invention may be practiced without such specific details. In other instances control structures gate level circuits and full software instruction sequences have not been shown in detail in order not to obscure the invention. Those of ordinary skill in the art with the included descriptions will be able to implement appropriate functionality without undue experimentation.

References in the specification to one embodiment an embodiment an example embodiment etc. indicate that the embodiment described may include a particular feature structure or characteristic but every embodiment may not necessarily include the particular feature structure or characteristic. Moreover such phrases are not necessarily referring to the same embodiment. Further when a particular feature structure or characteristic is described in connection with an embodiment it is submitted that it is within the knowledge of one skilled in the art to affect such feature structure or characteristic in connection with other embodiments whether or not explicitly described.

Bracketed text and blocks with dashed borders e.g. large dashes small dashes dot dash and dots may be used herein to illustrate optional operations that add additional features to embodiments of the invention. However such notation should not be taken to mean that these are the only options or optional operations and or that blocks with solid borders are not optional in certain embodiments of the invention.

In the following description and claims the terms coupled and connected along with their derivatives may be used. It should be understood that these terms are not intended as synonyms for each other. Coupled is used to indicate that two or more elements which may or may not be in direct physical or electrical contact with each other co operate or interact with each other. Connected is used to indicate the establishment of communication between two or more elements that are coupled with each other.

An electronic device stores and transmits internally and or with other electronic devices over a network code which is composed of software instructions and which is sometimes referred to as computer program code or a computer program and or data using machine readable media also called computer readable media such as machine readable storage media e.g. magnetic disks optical disks read only memory ROM flash memory devices phase change memory and machine readable transmission media also called a carrier e.g. electrical optical radio acoustical or other form of propagated signals such as carrier waves infrared signals . Thus an electronic device e.g. a computer includes hardware and software such as a set of one or more processors coupled to one or more machine readable storage media to store code for execution on the set of processors and or to store data. For instance an electronic device may include non volatile memory containing the code since the non volatile memory can persist code data even when the electronic device is turned off when power is removed and while the electronic device is turned on that part of the code that is to be executed by the processor s of that electronic device is typically copied from the slower non volatile memory into volatile memory e.g. dynamic random access memory DRAM static random access memory SRAM of that electronic device. Typical electronic devices also include a set or one or more physical network interface s to establish network connections to transmit and or receive code and or data using propagating signals with other electronic devices. One or more parts of an embodiment of the invention may be implemented using different combinations of software firmware and or hardware.

The operations in the flow diagrams will be described with reference to the exemplary embodiments of the other figures. However it should be understood that the operations of the flow diagrams can be performed by embodiments of the invention other than those discussed with reference to the other figures and the embodiments of the invention discussed with reference to these other figures can perform operations different than those discussed with reference to the flow diagrams.

The embodiments of the present invention provide a method and system for avoiding the disadvantages of the prior art. The disadvantages of the prior art are that prior implementations of the 3G packet core use a pool of servers that are dedicated to a specific network entity such as a server pool that is dedicated to hosting a SGSN. When additional signaling demands require that extra capacity then a new SGSN instance is instantiated in the server pool. However when demand is high for the services of the home subscriber server HSS the server pool dedicated to the HSS servers will be heavily utilized but the server pool for the SGSN is underutilized. These underutilized server pools continue to require maintenance and incur operating expenses but are not providing optimum performance for the network operator.

In some situations managed services companies build and run mobile operator networks while the mobile operator itself handles marketing billing and customer relations. The signaling and data traffic for each mobile operator network is kept private and isolated from the traffic of their competitors even though their network and their competitors networks may be managed by the same managed services company. The managed services company must maintain a completely separate server pool and physical signaling network for each mobile operator it supports. As a result there is a large duplication of resources and an underutilization of server capacity. This increases operating expenses for the managed services companies and the mobile operator network due to the additional equipment power and cooling requirements.

The 3G packet core architecture as it is currently defined allows only one point of presence PoP between the mobile operator s fixed core Internet and the mobile aggregation network that is there is a single gateway GPRS support node GGSN . Mobile network operators cannot set up multiple peering points and PoPs between neighboring operators within the aggregation network. This would reduce the amount of traffic that flows into the mobile operator s core network thereby reducing the need for expensive and time consuming core network upgrades. In addition peering points are usually without cost to operators as long as the service level agreements SLAs are observed. However this flexibility of deployment is unavailable to mobile operators due to the need to anchor their PoP with the core Internet at a single mobile gateway.

The 3G PC architecture also contains little flexibility for specialized treatment of user flows. Though the architecture does provide support for establishing quality of service QoS other sorts of data management are not available. For example services involving middleboxes such as specialized deep packet inspection or interaction with local data caching and processing resources that might be utilized for transcoding or augmented reality applications is difficult to support with the current 3G PC architecture. Almost all such applications require the packet flows to exit through the GGSN thereby being de tunneled from GTP and to be processed within the wired network.

Implementing the control plane of an 3G PC in a cloud computing facility and the data plane of the 3G PC using a set of OpenFlow switches as well as managing communication between the control plane and the data plane using the OpenFlow protocol e.g. OpenFlow 1.1 creates a problem that the OpenFlow protocol does not support GTP or GTP tunnel endpoint identifier TEID routing which is necessary for implementing the data plane of the 3G PC.

The embodiments of the invention overcome these disadvantages of the prior art. The disadvantages of the prior art are avoided by splitting the control plane and the data plane for the 3G PC architecture and to implement the control plane by deploying the 3G PC control plane entities in a cloud computing facility while the data plane is implemented by a distributed collection of OpenFlow switches. The OpenFlow protocol is used to connect the two with enhancements to support GTP routing. While the 3G PC architecture already has a split between the control plane and the data plane in the sense that the SSGN and the GGSN are primarily data plane entities while the home location register HLR home subscriber server HSS and authentication center AUC are primarily control plane entities this split was made at the level of the mobility management protocol GTP.

The standard 3G PC architecture assumes a standard routed IP network for transport on top of which the mobile network entities and protocols are implemented. The enhanced 3G PC architecture described herein is instead at the level of IP routing and media access control MAC switching. Instead of using L2 routing and L3 internal gateway protocols to distribute IP routing and managing Ethernet and IP routing as a collection of distributed control entities L2 and L3 routing management is centralized in a cloud facility and the routing is controlled from the cloud facility using the OpenFlow protocol. As used herein the OpenFlow protocol refers to the OpenFlow network protocol and switching specification defined in the OpenFlow Switch Specification at www.openflowswitch.org a web site hosted by Stanford University. As used herein an OpenFlow switch refers to a network device implementing the OpenFlow protocol.

The standard 3G PC control plane entities HSS HLR AUC visitor location register VLR equipment identity register EIR sort message service interworking message center SMS IWMSC SMS gateway message center SMS GMSC and subscriber location function SLF and control plane aspects of the SGSN and GGSN are deployed in the cloud. The data plane consists of standard OpenFlow switches with enhancements as needed for routing GTP packets rather than IP routers and Ethernet switches. At a minimum the data plane parts of the SGSN and GGSN and the packet routing part of the NodeB in the E UTRAN require OpenFlow enhancements for GTP routing. Additional enhancements for GTP routing may be needed on other switches within the 3G PC architecture depending on how much fine grained control over the routing an operator requires. The enhancements for GTP routing include processes for establishing GTP tunnels modifying GTP tunnels and tearing down GTP tunnels within the 3G PC architecture.

The split between the control and data plane parts of the 3G PC can be used together with virtual private cloud VPC technology to implement multiple PoPs within a single 3G PC provide GTP flow specific routing for specialized applications and run multiple operator networks from a single cloud computing facility.

In one embodiment the cloud based 3G PC system can be implemented as a set of hardware devices. In another embodiment the system components are implemented in software for example microcode assembly language or higher level languages . These software implementations can be stored on a non transitory computer readable medium. A non transitory computer readable medium can include any medium that can store information. Examples of the non transitory computer readable medium include a read only memory ROM a floppy diskette a CD Rom a DVD a flash memory a hard drive an optical disc or similar medium.

A rule contains key fields from several headers in the protocol stack for example source and destination Ethernet MAC addresses source and destination IP addresses IP protocol type number incoming and outgoing TCP or UDP port numbers. To define a flow all the available matching fields may be used. But it is also possible to restrict the matching rule to a subset of the available fields by using wildcards for the unwanted fields.

The actions that are defined by the specification of OpenFlow 1.0 are Drop which drops the matching packets Forward which forwards the packet to one or all outgoing ports the incoming physical port itself the controller via the secure channel or the local networking stack if it exists . OpenFlow 1.0 protocol data units PDUs are defined with a set of structures specified using the C programming language. Some of the more commonly used messages are report switch configuration message modify state messages including a modify flow entry message and port modification message read state messages where while the system is running the datapath may be queried about its current state using this message and send packet message which is used when the controller wishes to send a packet out through the datapath.

OpenFlow 1.0 supports vendor extensions that allow certain protocol elements to be extended. Protocol messages and table actions can be extended but flow matching rules cannot. The use of these extensions in connection with the cloud based EPC architecture is discussed further herein below.

In one embodiment a group table can be supported in conjunction with the OpenFlow 1.1 protocol. Group tables enable a method for allowing a single flow match to trigger forwarding on multiple ports. Group table entries consist of four fields a group identifier which is a 32 bit unsigned integer identifying the group a group type that determines the group s semantics counters that maintain statistics on the group and an action bucket list which is an ordered list of action buckets where each bucket contains a set of actions to execute together with their parameters.

There are four different types of groups All which execute all actions in the bucket list this is used for broadcast or multicast forwarding Select which execute one bucket per packet based on an algorithm determined by the switch which is outside the OpenFlow protocol this is used to implement multipath forwarding Indirect which execute the single bucket on all packets this allows multiple flows or groups to point to a single collection of actions rather than having the actions defined in multiple forwarding table entries Fast Failover which execute the first live bucket where each bucket is associated with a port that controls its liveness this enables the switch to failover to another port without involving the controller.

OpenFlow 1.1 can be utilized to support virtual ports. A virtual port as used herein is an action block that performs some kind of processing action other than simply forwarding the packet out to a network connection like physical ports do. Examples of a few built in virtual ports include ALL which forwards the port out all ports except for the ingress port and any ports that are marked Do Not Forward CONTROLLER which encapsulates the packet and sends it to the controller TABLE which inserts the packet into the packet processing pipeline by submitting it to the first flow table this action is only valid in the action set of a packet out message and IN PORT which sends the packet out the ingress port. In other embodiments there can also be switched defined virtual ports.

The main function of the core network is to provide switching routing and transit for user traffic from the user equipment . The core network also contains databases and network management functions. It is the common packet core network for GSM GPRS wideband code division multiple access WCDMA high speed packet access HSPA and non 3GPP mobile networks. The core network is used for transmitting Internet Protocol IP packets. The core network interfaces with the Internet and other networks through the GGSN .

The core network is divided into circuit switched and packet switched domains. The circuit switched elements include a mobile services switching center MSC visitor location register VLR and Gateway MSC . Packet switched elements are SGSNs and GGSN . Other network devices like EIR HLR VLR and AUC are shared by both domains.

The architecture of the core network can change when new services and features are introduced. In other embodiments a number portability database NPDB can be used to enable a user to change networks while keeping an old phone number. A gateway location register GLR can be used to optimize the subscriber handling between network boundaries.

The primary functions of the core network with respect to mobile wireless networking are mobility management and QoS. These functions are not typically provided in a fixed broadband network but they are crucial for wireless networks. Mobility management is necessary to ensure packet network connectivity when a wireless terminal moves from one base station to another. QoS is necessary because unlike fixed networks the wireless link is severely constrained in how much bandwidth it can provide to the terminal so the bandwidth needs to be managed more tightly than in fixed networks in order to provide the user with acceptable quality of service.

The signaling for implementing the mobility management and QoS functions is provided by the GPRS Tunneling Protocol GTP . GTP has two components GTP C a control plane protocol that supports establishment of tunnels for mobility management and bearers for QoS management that matches wired backhaul and packet core QoS to radio link QoS and GTP U a data plane protocol used for implementing tunnels between network devices that act as routers. There are two versions of GTP C protocol i.e. GTP version 1 GTPv1 C and GTPv1 U and GTP version 2 C designed for LTE . GTPv1 is primarily utilized in conjunction with the 3G PC based system.

Network Services are considered to be end to end this means from a Terminal Equipment TE to another TE. An End to End Service may have a certain Quality of Service QoS which is provided for the user of a network service. It is the user that decides whether he is satisfied with the provided QoS or not. To realize a certain network QoS Service with clearly defined characteristics and functionality is to be set up from the source to the destination of a service.

In addition to the QoS parameters each bearer has an associated GTP tunnel. A GTP tunnel consists of the IP address of the tunnel endpoint nodes radio base station SGSN and GGSN a source and destination UDP port and a Tunnel Endpoint Identifier TEID . GTP tunnels are unidirectional so each bearer is associated with two TEIDs one for the uplink and one for the downlink tunnel. One set of GTP tunnels uplink and downlink extends between the radio base station and the SGSN and one set extends between the SGSN and the GGSN. The UDP destination port number for GTP U is while the destination port number for GTP C is . The source port number is dynamically allocated by the sending node. is a diagram of one embodiment of the header fields in the primary GTP U encapsulation header.

Data centers offer computing storage and network communication resources to outside customers. The offered services can consist of elastic on demand processing storage that for most practical purposes is limited only by the customer s ability to pay and network bandwidth into the Internet. This set of services provided by a data center is referred to herein as cloud computing.

Server virtualization technology allows a pool of servers to be managed as essentially one large compute resource. A layer of software called a hypervisor sits between the operating system and the hardware. The hypervisor schedules the execution of virtual machines VMs . A VM is an operating system image packaged with some applications. The hypervisor allows a VM to be suspended and moved between servers to load balance. Load balancing and monitoring of VM execution to catch crashes provides the same kind of fault tolerance and scalability services for enterprise applications that are achieved at much higher cost with specialized solutions. A cloud manager system oversees the execution of VMs the scheduling of execution to meet the demand of the VMs and the optimization of server utilization and minimization of power consumption. The cloud manager or cloud operating system is a software program that can schedule execution to allow an in service upgrade of hardware and software without impacting ongoing service provisioning to the VMs and their applications in the cloud computing system.

To support the arbitrary movement of VMs between machines the networking within the data center must also be virtualized. Cloud computing systems can virtualize the network by incorporating a virtual switch into the hypervisor. The virtual switch provides virtual network ports to the VMs executing under the control of the hypervisor. The virtual switch software also allows the network resources to be virtualized in a manner similar to how the server resources are virtualized by the hypervisor. The hypervisor and the virtual switch can thereby co operate to allow VMs to be moved between servers. When the hypervisor moves a VM it communicates with the virtual switch about the new location and the virtual switch ensures that the network routing tables for the VM s addresses L2 MAC address potentially also the IP address are updated so packets are routed to the new location.

A cloud computing system can be composed of any number of computing devices having any range of capabilities e.g. processing power or storage capacity . The cloud computing system can be a private or public system. The computing devices can be in communication with one another across any communication system or network. A cloud computing system can support a single cloud or service or any number of discrete clouds or services. Services applications and similar programs can be virtualized or executed as standard code. In one embodiment cloud computing systems can support web services applications. Web services applications consist of a load balancing front end that dispatches requests to a pool of Web servers. The requests originate from applications on remote machines on the Internet and therefore the security and privacy requirements are much looser than for applications in a private corporate network.

Cloud computer systems can also support secure multi tenancy in which the cloud computer system provider offers virtual private network VPN like connections between the clients distributed office networks outside the cloud and a VPN within the cloud computing system. This allows the clients applications within the cloud computing system to operate in a network environment that resembles a corporate WAN. For private data centers in which services are only offered to customers within the corporation owning the data center the security and privacy requirements for multi tenancy are relaxed. But for public data centers the cloud operator must ensure that the traffic from multiple tenants is isolated and there is no possibility for traffic from one client to reach another client network.

The cloud manager monitors the central processor unit CPU utilization of the 3G PC control plane entities and the control plane traffic between the 3G PC control plane entities within the cloud. It also monitors the control plane traffic between the end user devices UEs and NodeBs which do not have control plane entities in the cloud computing system and the 3G PC control plane entities . If the 3G PC control plane entities begin to exhibit signs of overloading such as the utilization of too much CPU time or the queuing up of too much traffic to be processed the overloaded control plane entity requests that the cloud manager start up a new VM to handle the load. Additionally the 3G PC control plane entities themselves can issue event notifications to the cloud manager if they detect internally that they are beginning to experience overloading.

The cloud manager also provides reliability and failover by restarting a VM for a particular control plane entity or function if any of the 3G PC control plane entities should crash. During this restart process the cloud manager can collect diagnostic data save any core files of the failed 3G PC control plane entity and inform the system administrators that a failure occurred. The control plane entities maintains the same protocol interface between themselves as in the 3GPP 3G PC architecture shown in .

The OpenFlow control plane shown here as a dotted line manages the routing and switching configuration in the network. The OpenFlow control plane connects the cloud computing system to the SGSN Ds i.e. the data plane of the SGSN the standard OpenFlow switches and the GGSN D i.e. the data plane of the GGSN . The physical implementation of the OpenFlow control plane can be as a completely separate physical network or it may be a virtual network running over the same physical network as the data plane implemented with a prioritized VLAN or with an MPLS label switched path or even with a generic routing encapsulation GRE or other IP tunnel. The OpenFlow control plane can in principle use the same physical control plane paths as the GTP C and other mobile network signaling. The SGSN Ds and the GGSN Ds act as OpenFlow GTP extended gateways encapsulating and decapsulating packets using the OpenFlow GTP switch extensions described further herein below.

The NodeBs have no control plane entities in the cloud because the radio access network RAN signaling required between the 3G PC and the NodeB includes radio parameters and not just IP routing parameters. Therefore there is no OpenFlow control plane connection between the OpenFlow controller in the cloud computing system and the NodeBs . The NodeBs can however act as OpenFlow GTP extended gateways by implementing a local control to data plane connection using OpenFlow. This allows the packet switching side of the NodeBs to utilize the same OpenFlow GTP switching extensions as the packet gateways.

The operation of the 3G PC cloud computer system works as follows. The UE NodeB SGSN and GGSN signal to the HLR HSS AUC SMS GMSC using the standard 3G PC protocols to establish modify and delete GTP tunnels. This signaling triggers procedure calls with the OpenFlow controller to modify the routing in the 3G PC as requested. The OpenFlow controller configures the standard OpenFlow switches the Openflow SGSN and GGSN with flow rules and actions to enable the routing requested by the control plane entities. Details of this configuration are described in further detail herein below.

If the cloud manager detects a threshold level of resource utilization or traffic load for any one of the plurality of control plane modules being monitored Block the cloud manager can takes steps to automatically respond to this scenario. The cloud manager can initialize a new control plane module or an instance of such a control plane module as a separate virtual machine Block . This new control plane module or instance can then share the load of existing control plane modules or instances of the same type thereby alleviating the load on these modules dynamically.

Similarly the cloud manager may detect the failure or the underutilization of one of the plurality of control plane modules Block . The cloud manager can then restart a failed control plane module or terminate an underutilized control plane module Block . Restarting the control plane module ensures a level of load sharing for a pool of control plane modules. Deactivating a control plane module frees up the resources and reduces the overhead created by the control plane module. The cloud manager can perform these functions across VPCs and mobile operators using the cloud computing system resources thereby maximizing the use of available resources and reducing the cost of operation while maintaining strict separation of data and traffic between mobile operators.

The long dash and dotted lines and arrows shows an example of a UE that is obtaining content from an external source. The content is originally not formulated for the UE s screen so the OpenFlow controller has installed flow rules and actions on the GGSN1 B SGSN and the OpenFlow switches to route the flow through a transcoding application in the cloud computing facility. The transcoding application reformats the content so that it will fit on the UE s screen. A MSC requests the specialized treatment at the time the UE sets up its session with the external content source via the IP Multimedia Subsystem IMS or another signaling protocol.

In one embodiment OpenFlow is modified to provide rules for GTP TEID Routing. is a diagram of one embodiment of the OpenFlow flow table modification for GTP TEID routing. An OpenFlow switch that supports TEID routing matches on the 2 byte 16 bit collection of header fields and the 4 byte 32 bit GTP TEID in addition to other OpenFlow header fields in at least one flow table e.g. the first flow table . The GTP TEID flag can be wildcarded i.e. matches are don t care . In one embodiment the 3G PC protocols do not assign any meaning to TEIDs other than as an endpoint identifier for tunnels like ports in standard UDP TCP transport protocols. In other embodiments the TEIDs can have a correlated meaning or semantics. The GTP header flags field can also be wildcarded this can be partially matched by combining the following bitmasks 0xFF00 Match the Message Type field 0xe0 Match the Version field 0x10 Match the PT field 0x04 Match the E field 0x02 Match the S field and 0x01 Match the PN field.

In one embodiment OpenFlow can be modified to support virtual ports for fast path GTP TEID encapsulation and decapsulation. An OpenFlow mobile gateway can be used to support GTP encapsulation and decapsulation with virtual ports. The GTP encapsulation and decapsulation virtual ports can be used for fast encapsulation and decapsulation of user data packets within GTP U tunnels and can be designed simply enough that they can be implemented in hardware or firmware. For this reason GTP virtual ports may have the following restrictions on traffic they will handle Protocol Type PT field 1 where GTP encapsulation ports only support GTP not GTP PT field 0 Extension Header flag E 0 where no extension headers are supported Sequence Number flag S 0 where no sequence numbers are supported N PDU flag PN 0 and Message type 255 where Only G PDU messages i.e. tunneled user data is supported in the fast path.

If a packet either needs encapsulation or arrives encapsulated with nonzero header flags header extensions and or the GTP U packet is not a G PDU packet i.e. it is a GTP U control packet the processing must proceed via the gateway s slow path software control plane. GTP C and GTP packets directed to the gateway s IP address are a result of mis configuration and are in error. They must be sent to the OpenFlow controller since these packets are handled by the SGSN and GGSN control plane entities in the cloud computing system or to the billing entity handling GTP and not the SGSN and GGSN data plane switches.

GTP virtual ports are configured from the OpenFlow controller using a configuration protocol. The details of the configuration protocol are switch dependent. The configuration protocol must support messages that perform the following functions allow the controller to query for and return an indication whether the switch supports GTP fast path virtual ports and what virtual port numbers are used for fast path and slow path GTP U processing and allow the controller to instantiate a GTP U fast path virtual port within a switch datapath for use in the OpenFlow table set output port action. The configuration command must be run in a transaction so that when the results of the action are reported back to the controller either a GTP U fast path virtual port for the requested datapath has been instantiated or an error has returned indicating why the request could not be honored. The command also allows the OpenFlow controller to bind a GTP U virtual port to a physical port. For decapsulation virtual ports the physical port is an input port. For encapsulation virtual ports the physical port is an output port.

The OpenFlow controller instantiates a virtual port for each physical port that may transmit or receive packets routed through a GTP tunnel prior to installing any rules in the switch for GTP TEID routing.

In one embodiment an OpenFlow GTP gateway maintains a hash table mapping GTP TEIDs into the tunnel header fields for their bearers. is a diagram of the structure of a flow table row. The TEID hash keys are calculated using a suitable hash algorithm with low collision frequency for example SHA 1. The gateway maintains one such flow table row for each GTP TEID bearer. The TEID field contains the GTP TEID for the tunnel. The VLAN tags and MPLS labels fields contain an ordered list of VLAN tags and or MPLS labels defining tunnels into which the packet needs to be routed. The VLAN priority bits and MPLS traffic class bits are included in the labels. Such tunnels may or may not be required. If they are not required then these fields are empty. The tunnel origin source IP address contains the address on the encapsulating gateway to which any control traffic involving the tunnel should be directed for example error indications . The tunnel end destination IP address field contains the IP address of the gateway to which the tunneled packet should be routed at which the packet will be decapsulated and removed from the GTP tunnel. The QoS DSCP field contains the DiffServe Code Point if any for the bearer in the case of a dedicated bearer. This field may be empty if the bearer is a default bearer with best effort QoS but will contain nonzero values if the bearer QoS is more than best effort.

In one embodiment slow path support for GTP is implemented with an OpenFlow gateway switch. An OpenFlow mobile gateway switch also contains support on the software control plane for slow path packet processing. This path is taken by G PDU message type 255 packets with nonzero header fields or extension headers and user data plane packets requiring encapsulation with such fields or addition of extension headers and by GTP U control packets. For this purpose the switch supports three local ports in the software control plane LOCAL GTP CONTROL the switch fast path forwards GTP encapsulated packets directed to the gateway IP address that contain GTP U control messages and the local switch software control plane initiates local control plane actions depending on the GTP U control message LOCAL GTP U DECAP the switch fast path forwards G PDU packets to this port that have nonzero header fields or extension headers i.e. E 0 S 0 or PN 0 . These packets require specialized handling. The local switch software slow path processes the packets and performs the specialized handling and LOCAL GTP U ENCAP the switch fast path forwards user data plane packets to this port that require encapsulation in a GTP tunnel with nonzero header fields or extension headers i.e. E 0 S 0 or PN 0 . These packets require specialized handling. The local switch software slow path encapsulates the packets and performs the specialized handling. In addition to forwarding the packet the switch fast path makes the OpenFlow metadata field available to the slow path software.

To support slow path encapsulation the software control plane on the switch maintains a hash table with keys calculated from the GTP U TEID. The TEID hash keys are calculated using a suitable hash algorithm with low collision frequency for example SHA 1. The flow table entries contain a record of how the packet header including the GTP encapsulation header should be configured. This includes the same header fields as for the hardware or firmware encapsulation table in values for the GTP header flags PT E S and PN the sequence number and or the N PDU number if any if the E flag is 1 then the flow table contains a list of the extension headers including their types which the slow path should insert into the GTP header.

In one embodiment the system implements a GTP fast path encapsulation virtual port. When requested by the SGSN and GGSN control plane software running in the cloud computing system the OpenFlow controller programs the gateway switch to install rules actions and TEID hash table entries for routing packets into GTP tunnels via a fast path GTP encapsulation virtual port. The rules match the packet filter for the input side of GTP tunnel s bearer. Typically this will be a 4 tuple of IP source address IP destination address UDP TCP SCTP source port and UDP TCP SCTP destination port. The IP source address and destination address are typically the addresses for user data plane traffic i.e. a UE or Internet service with which a UE is transacting and similarly with the port numbers. For a rule matching the GTP U tunnel input side the associated instructions and are the following 

The switch also writes an entry in the TEID hash table containing the tunnel header fields for the packet. GTP TEID is the GTP tunnel endpoint identifier. GTP Enacap VP is the GTP fast path encapsulation virtual port bound to the physical port out which the encapsulated packet will ultimately be routed.

When a packet header matches a rule associated with the virtual port the GTP TEID is written into the lower 32 bits of the metadata and the packet is directed to the virtual port. The virtual port calculates the hash of the TEID and looks up the tunnel header information in the tunnel header table. If no such tunnel information is present the packet is forwarded to the controller with an error indication. Otherwise the virtual port constructs a GTP tunnel header and encapsulates the packet. Any DSCP bits or VLAN priority bits are additionally set in the IP or MAC tunnel headers and any VLAN tags or MPLS labels are pushed onto the packet. The encapsulated packet is forwarded out the physical port to which the virtual port is bound.

In one embodiment the system implements a GTP fast path decapsulation virtual port. When requested by the SGSN and GGSN control plane software running in the cloud computing system the gateway switch installs rules and actions for routing GTP encapsulated packets out of GTP tunnels. The rules match the GTP header flags and the GTP TEID for the packet in the modified OpenFlow flow table shown in as follows the IP destination address is an IP address on which the gateway is expecting GTP traffic the IP protocol type is UDP 17 the UDP destination port is the GTP U destination port 2152 and the header fields and message type field is wildcarded with the flag 0XFFF0 and the upper two bytes of the field match the G PDU message type 255 while the lower two bytes match 0x30 i.e. the packet is a GTP packet not a GTP packet and the version number is 1. The virtual port simply removes the GTP tunnel header and forwards the enclosed user data plane packet out the bound physical port.

In one embodiment the system implements handling of GTP U control packets. The OpenFlow controller programs the gateway switch flow tables with 5 rules for each gateway switch IP address used for GTP traffic. These rules contain specified values for the following fields the IP destination address is an IP address on which the gateway is expecting GTP traffic the IP protocol type is UDP 17 the UDP destination port is the GTP U destination port 2152 the GTP header flags and message type field is wildcarded with 0xFFF0 the value of the header flags field is 0x30 i.e. the version number is 1 and the PT field is 1 and the value of the message type field is one of 1 Echo Request 2 Echo Response 26 Error Indication 31 Support for Extension Headers Notification or 254 End Marker .

This causes the packet to be forwarded to the gateway switch s local GTP U control port for processing by the local software control plane. GTP U control packets that are originated by the switch are generated on the software control plane and are routed by the control plane.

In one embodiment the system implements handling of G PDU packets with extension headers sequence numbers and N PDU numbers. G PDU packets with extension headers sequence numbers and N PDU numbers need to be forwarded to the local switch software control plane for processing. The OpenFlow controller programs 3 rules for this purpose. They have the following common header fields the IP destination address is an IP address on which the gateway is expecting GTP traffic and the IP protocol type is UDP 17 the UDP destination port is the GTP U destination port 2152 .

The header flags and message type fields for the three rules are wildcarded with the following bitmasks and match as follows bitmask 0xFFF4 and the upper two bytes match the G PDU message type 255 while the lower two bytes are 0x34 indicating that the version number is 1 the packet is a GTP packet and there is an extension header present bitmask 0xFFF2 and the upper two bytes match the G PDU message type 255 while the lower two bytes are 0x32 indicating that the version number is 1 the packet is a GTP packet and there is a sequence number present and bitmask 0xFF01 and the upper two bytes match the G PDU message type 255 while the lower two bytes are 0x31 indicating that the version number is 1 the packet is a GTP packet and a N PDU is present.

In one embodiment the system implements handling of user data plane packets requiring GTP U encapsulation with extension headers sequence numbers and N PDU numbers. User data plane packets that require extension headers sequence numbers or N PDU numbers during GTP encapsulation require special handling by the software slow path. For these packets the OpenFlow controller programs a rule matching the 4 tuple IP source address IP destination address UDP TCP SCTP source port and UDP TCP SCTP destination port. The instructions for matching packets are 

This sends the packet to the software slow path GTP encapsulation port and in addition makes the TEID available to the slow path.

The OpenFlow message programming the rule insertion also includes information on the values for the sequence number N PDU number or the type and contents of the extension header as well as the packet header fields designating the decapsulation gateway and bearer transport and the GTP TEID. This information is inserted by the switch s control plane software into the software encapsulation table keyed by the TEID.

In one embodiment the system implements handling of GTP C and GTP control packets. Any GTP C and GTP control packets that are directed to IP addresses on a gateway switch are in error. These packets need to be handled by the SGSN GGSN and GTP protocol entities in the cloud computing system not the SGSN and GGSN entities in the switches. To catch such packets the OpenFlow controller must program the switch with the following two rules the IP destination address is an IP address on which the gateway is expecting GTP traffic the IP protocol type is UDP 17 for one rule the UDP destination port is the GTP U destination port 2152 for the other the UDP destination port is the GTP C destination port 2123 the GTP header flags and message type fields are wildcarded.

These rules must be the lowest priority of all the GTP rules in the gateway switch s flow table. They will match any GTP packets that don t match other more specific rules. The instruction for these rules is the following 

In one embodiment the system implements non gateway GTP routing. A GTP extended Openflow switch can also accomplish GTP routing without performing the gateway functions of encapsulation and decapsulation. The GTP routing function can be performed by a gateway switch in addition to its gateway function or it can be performed by another switch that lacks a gateway function within the distributed EPC switching fabric.

A GTP extended Openflow switch contains at least one flow table that handles rules matching the GTP header fields as in . The Openflow controller programs the GTP header field rules in addition to the other fields to perform GTP routing and adds appropriate actions if the rule is matched. For example the following rule matches a GTP C control packet directed to a control plane entity MSC SGSN GGSN in the cloud computing system which is not in the control plane VLAN the VLAN tag is not set to the control plane VLAN the destination IP address field is set to the IP address of the targeted control plane entity the IP protocol type is UDP 17 the UDP destination port is the GTP C destination port 2123 the GTP header flags and message type is wildcarded with 0xF0 and the matched version and protocol type fields are 2 and 1 indicating that the packet is a GTPv1 control plane packet and not GTP .

The following actions push a control plane VLAN tag onto the packet and forward it to the cloud for processing by the relevant control plane entity. The packet is forwarded without any L3 processing i.e. not modifying the IP TTL 

The OpenFlow protocol can be modified to provide extensions for GTP that enable the management of the 3G PC. OpenFlow utilizes data structures referred to as flow match structures that enable the protocol to define criteria for matching rules to particular flows. The OpenFlow flow match structure of ofp match contains two fields type and length that allow the flow match structure to be extended. The type field can be set to the type of the extension and the length field can be set to the length of the extended ofp match structure. In one embodiment a new type based on a random number for GTP flow matching is defined 

The type can be been randomly generated so as not to interfere with other extended types. There is currently no organizational mechanism to register type identifiers in OpenFlow.

The gtp type n flags field contains the GTP message type in the upper 8 bits and the GTP header flags in the lower 8 bits. The gtp teid field contains the GTP TEID. The gtp wildcard field indicates whether the GTP type and flags and TEID should be matched. If the lower four bits are 1 the type and flags field should be ignored while if the upper four bits are 1 the TEID should be ignored. If the lower bits are 0 the type and fields flag should be matched subject to the flags in the gtp flag mask field while if the upper bits are 0 the TEID should be matched. The mask is combined with the message type and header field of the packet using logical AND the result becomes the value of the match. Only those parts of the field in which the mask has a 1 value are matched.

In addition to the flow table fields an object is required to encode the encapsulation of the virtual port TEID hash table entry. The ersmt gtptuninfo structure can be used to define this information 

The ermst mpls lbl struct provides a 24 bit data structure for encoding MPLS labels. The ersmt gtp tunifo structure contains fields describing a GTP tunnel. These are inserted into the encapsulation virtual port. The structure is variable length because it may contain a variable number of VLAN tags and or MPLS labels. The gtp tuninfo length field contains the length of the structure. The gtp tuninfo saddr gtp tuninfo daddr and gtp tuninfo dscp fields contain the source address of the tunnel the address of the interface on the switch performing the encapsulation the destination address of the tunnel the switch to which the tunneled packet will be routed and that will decapsulate the packet and the DiffServ Code Point if any assigned to the tunnel s bearer. The bearer DSCP will be nonzero if the bearer is a dedicated bearer and it is not a best effort bearer.

The gtp tuninfo vlan len and gtp tuninfo mpls len contain the length of the VLAN tags field and the MPLS labels field respectively. The gtp tuninfo vlan tags 0 and gtp tuninfo mpls labels 0 fields contain the actual VLAN tags and or the MPLS labels that need to be pushed onto the packet s tunnel header. These fields will be absent and the corresponding length fields will be zero if no VLAN or MPLS Label Switched Paths LSPs are used for the tunnel.

In one embodiment OpenFlow is modified to add extension messages for adding deleting or modifying a 3G PC bearer or GTP tunnel. The OpenFlow signaling for adding modifying or deleting a 3G PC bearer or GTP tunnel consists of one OpenFlow message the ofp flow mod message containing an ersmt gtp GTP flow definition. The standard OpenFlow ofp flow mod message can be used as long as the OpenFlow protocol parser can handle extended flows. If the flow modification requires a change to the encapsulation virtual port TEID hash table the OpenFlow controller must issue a GTP OpenFlow extension message containing the TEID hash table entry. The OpenFlow controller must issue both messages sequentially the ofp flow mod message first then the TEID hash table modification message then the OpenFlow controller must issue an OFPT BARRIER REQUEST message to force processing of both messages by the OpenFlow switch.

The OpenFlow message extension header structure ofp experimenter header contains an experimenter id field called experimenter. In one embodiment this field can be set to the Ericsson IEEE OUI 0x0lec or similar manufacturer or provider OUI. The rest of the structure contains the GTP extension messages. These messages can be identified by the following message codes 

The GTP OpenFlow extension contains a message for adding and for deleting a TEID hash table entry. Entries are modified by first deleting the entry for the TEID then adding a new entry for the same TEID. The GTP OpenFlow extension message for entering a new TEID entry in the encapsulation virtual port hash table is 

The teid table add type field is set to GTP ADD TEID TABLE ENTRY while the teid table add teid field contains the TEID and the teid table add entry contains the table entry to be added. The GTP OpenFlow extension message for deleting a TEID entry from the encapsulation virtual port hash table is 

The teid table del type field is set to GTP DEL TEID TABLE ENTRY while the teid table del teid field contains the TEID for the entry to be deleted.

In one embodiment the extensions to OpenFlow for GTP also encompass OpenFlow switch configuration. Prior to accepting any GTP routing update RPCs from 3G PC cloud control plane entities the OpenFlow controller must configure GTP encapsulation and or decapsulation virtual ports on the GTP extended OpenFlow gateway switches. The configuration is accomplished using a switch specific configuration protocol and is described above.

In addition to virtual port configuration on the GTP extended OpenFlow gateways QoS queue configuration may be required on any OpenFlow switch that will be forwarding better than best effort GTP bearer traffic. The OpenFlow protocol contains no messages for configuring queues this configuration is left up to the configuration protocol as is the case with virtual ports. Prior to installing any flow routes the OpenFlow controller must configure any queues to connect with physical and or virtual ports in switches that will route better than best effort GTP bearers. This configuration step must be done both for GTP extended OpenFlow switches and standard OpenFlow switches.

In one embodiment OpenFlow message flows for GTP operations are modified. As described above the 3G PC control plane entities including the 3G PC control plane parts of the SGSN and the SSSN reside in a cloud computing facility at a data center. The control plane of the SGSN and GGSN communicate via remote procedure calls RPCs or similar mechanism with the OpenFlow controller within the cloud when routing changes are triggered by the GTP signaling. The OpenFlow controller enacts the changes on the data plane to GTP extended OpenFlow enabled data plane gateways the control plane of the SGSN and GGSN and to OpenFlow switches that are extended for GTP routing referred to herein as GxOFS through OpenFlow signaling on the control plane network connecting the cloud to the gateways and switches.

In general no signaling is required to the GxOFS if no special routing treatment is required for GTP flows. Cases where such treatment might be required are for example where a where the operator s 3G PC has peering points with the Internet at more than one point and consequently has more than one gateway routing to the optimal gateway may require steering traffic within the 3G PC at intermediate switches and where the GTP flow must receive special treatment from an application somewhere within the operator s network for example within the cloud. An example of such special treatment is transcoding. The intermediate switches may require programming to route the user plane packets to the transcoding application. This list is not exhaustive many other applications of GTP routing on intermediate switches are possible.

GTP PDP context tunnels can be set up using the GTP C create PDP context request messages. This procedure is used in a variety of message sequences for example in an E UTRAN initial attach procedure.

The process for modifying a session is illustrated in . The process is initiated in response to a request to modify a GTP tunnel between a SGSN and a GGSN for a subscriber session Block . The subscriber session connects user equipment in the 3G PC to another endpoint of the subscriber session which can be another user equipment UE a server or similar endpoint. The GTP tunnel is an established route of the subscriber session across the core network of the 3G PC network to a peering point the Internet or similar endpoint. A modification process can be utilize to re route a subscriber session across the 3G PC network to the same endpoints or to different endpoints. Any combination of the endpoints of the GTP tunnel and the path of the GTP can be changed using this process. The controller modifies the configuration of the switch implementing the SGSN to encapsulate and decapsulate data packets for the subscriber session and to modify a first GTP tunnel endpoint Block . The controller also configures each switch in the current route and the new route of the GTP tunnel to forward packets in each direction according to the route of the GTP tunnel Block . The controller modifies the configuration of a data plane of the GGSN to encapsulate and decapsulate the packets of the subscriber session to establish a second endpoint of the GTP tunnel at the GGSN Block .

The process for deleting a session is illustrated in . The process is initiated in response to a request to delete a GTP tunnel between a SGSN and a GGSN for a subscriber session Block . The subscriber session connects user equipment in the 3G PC to another endpoint of the subscriber session which can be another user equipment UE a server or similar endpoint. The GTP tunnel is an established route of the subscriber session across the core network of the 3G PC network to a peering point the Internet or similar endpoint. The subscriber session and associated GTP tunnel are deleted when the associated application on the user equipment or the application on the other user equipment or the server application on the other end of the subscriber session terminate the connection. The subscriber session resources are then reclaimed by deleting the subscriber session and the GTP tunnel. The controller removes the configuration of the GTP tunnel at a switch implementing the SGSN that had been encapsulating and decapsulating data packets for the subscriber session and thereby removes a first GTP tunnel endpoint Block . The controller also removes configuration for the GTP tunnel from each switch in the route of the GTP tunnel that had been forwarding packets in each direction according to the route of the GTP tunnel Block . The controller removes configuration for the GTP tunnel from a data plane of the GGSN that had been encapsulating and decapsulating the packets of the subscriber session and thereby removes a second endpoint of the GTP tunnel at the GGSN Block .

In an example of the OpenFlow message flows for the create session request procedure are shown. In the illustrated example the SSGN control plane component sends a create session request to the GGSN control plane component in the cloud computing system which then sends the request to the controller through a GTP routing update RPC call. The RPC call requests that the OpenFlow controller establish a new GTP tunnel endpoint at the SGSN and GGSN in the data plane and to install routes for the new GTP bearer or tunnel on intermediate switches if necessary.

Before returning a result to the control plane GGSN from the GTP routing update RPC the OpenFlow controller issues a sequence of OpenFlow messages to the appropriate data plane gateway entity. In the example embodiment the sequence begins with an OFP BARRIER REQUEST to ensure that there are no pending messages that might influence processing of the following messages. Then an OFPT FLOW MOD message is issued including the ofp match structure with GTP extension as the match field and OFPFC ADD as the command field. The message specifies actions and instructions as described above to establish a flow route for the GTP tunnel that encapsulates and decapsulates the packets through the appropriate virtual port. In addition immediately following the OFPT FLOW MOD message the OpenFlow controller issues a GTP ADD TEID TABLE ENTRY message to the gateways containing the TEID hash table entries for the encapsulation virtual port. As described above the two OpenFlow messages are followed by an OFPT BARRIER REQUEST message to force the gateways to process the flow route and TEID hash table update before proceeding.

Prior to returning from the GTP routing update RPC the OpenFlow controller also issues GTP flow routing updates to any GTP extended OpenFlow Switches GxOFSs that need to be involved in customized GTP flow routing. The messages in these updates consist of an OFP BARRIER REQUEST followed by an OFPT FLOW MOD message containing the ofp match structure with GTP extension for the new GTP flow as the match field and OFPFC ADD as the command field and the actions and instructions described above for customized GTP flow routing. A final OFP BARRIER REQUEST forces the switch to process the change before responding. The flow routes on any GxOFSs are installed after installing the GTP tunnel endpoint route on the SGSN in the data plane and prior to installing the GTP tunnel endpoint route on the GGSN in the data plane as illustrated in . The OpenFlow controller does not respond to the control plane GGSN RPC until all flow routing updates have been accomplished.

Once the RPCs have returned the control plane GGSN and SGSN return create PDP context response messages. Characteristics of the GTP bearer are changed using an update PDP context request process. Such changes may for example include the QoS assigned to the IP packets. This procedure is used in a variety of 3G PC message sequences for example a UE triggered service request.

Before returning a result to the control plane GGSN from the GTP routing update RPC the OpenFlow controller issues a sequence of OpenFlow messages to the appropriate data plane gateway entity. The sequence begins with an OFP BARRIER REQUEST to ensure that there are no pending messages that might influence processing of the following messages. Then an OFPT FLOW MOD message is issued including the ofp match structure with GTP extension as the match field and OFPFC MODIFY or OFPFC MODIFY STRICT as the command field. If necessary the message specifies actions and instructions as described above to establish a new flow route for the GTP tunnel that encapsulates and decapsulates the packets through the appropriate virtual port. In addition if changes are required in the TEID hash table immediately following the OFPT FLOW MOD message the OpenFlow controller issues a TP DEL TEID TABLE ENTRY to delete the entry followed by a TP ADD TEID TABLE ENTRY message to install the new entry. As described above the two OpenFlow messages are followed by an OFPT BARRIER REQUEST message to force the gateways to process the flow route and TEID hash table update before proceeding.

Prior to returning from the GTP routing update RPC the OpenFlow controller also issues necessary GTP flow routing updates to any GTP extended OpenFlow Switches GxOFSs that need to be involved in customized GTP flow routing. The messages in these updates consist of an OFP BARRIER REQUEST followed by an OFPT FLOW MOD message containing the ofp match structure with GTP extension for the new GTP flow as the match field and OFPFC MODIFY or OFPFC MODIFY STRICT as the command field and if necessary the actions and instructions as described above for customized GTP flow routing. A final OFP BARRIER REQUEST forces the switch to process the change before responding. The flow routes on any GxOFSs are installed after installing the GTP tunnel endpoint route on the data plane SGSN and prior to installing the GTP tunnel endpoint route on the data plane GGSN as illustrated in . The OpenFlow controller does not respond to the control plane GGSN RPC until all flow routing updates have been accomplished. Once the RPCs have returned the control plane GGSN and SGSN return update PDP context messages.

GTP tunnels are deleted using the delete session request procedure. This procedure can be used in a variety of 3G PC message sequences for example a UE triggered detach request. is a diagram of one embodiment of the OpenFlow message sequence for the delete session request procedure. Similar to session creation and modification the 3G PC cloud control plane SGSN issues a delete PDP context request message to the control plane GGSN and the control plane GGSN issues a GTP routing update RPC to the OpenFlow controller including the tunnel deletion information. The OpenFlow controller then issues GTP extended OpenFlow messages to the data plane SGSN GxOFSes and the data plane GGSN.

The OpenFlow signaling is conducted prior to returning the GTP routing update RPC to the calling party. The sequence begins with an OFP BARRIER REQUEST to ensure that there are no pending messages that might influence processing of the following messages. Then an OFPT FLOW MOD message is issued including the ofp match structure with GTP extension as the match field and OFPFC DELETE or OFPFC DELETE STRICT as the command field. In addition immediately following the OFPT FLOW MOD message the OpenFlow controller issues a GTP DEL TEID TABLE ENTRY to delete the TEID hash table entry. As described above this OpenFlow message is followed by an OFPT BARRIER REQUEST message to force the gateways to process the flow route and TEID hash table update before proceeding.

Prior to returning from the GTP routing update RPC the OpenFlow controller also issues necessary GTP flow routing updates to any GTP extended OpenFlow Switches that need to be involved in customized GTP flow routing. The messages in these updates consist of an OFP BARRIER REQUEST followed by an OFPT FLOW MOD message containing the ofp match structure with GTP extension for the new GTP flow as the match field and OFPFC DELETE or OFPFC DELETE STRICT as the command field. A final OFP BARRIER REQUEST forces the switch to process the change before responding. The flow routes on any GxOFSs are installed after installing the GTP tunnel endpoint route on the data plane SGSN and prior to installing the GTP tunnel endpoint route on the data plane GGSN as illustrated in . The OpenFlow controller does not respond to the calling entity until all flow routing updates have been accomplished.

In other embodiments the split 3G PC architecture can be implemented in non cloud and non virtualized systems. The control plane entities of the 3G PC architecture can be stored and executed on a single server or distributed across any number of servers or similar computing devices. Similarly the control plane entities can be executed as standard software code and modules without virtualization or similar systems. These control plane entities can communicate with one another through local system or procedure calls remote procedure calls or similar mechanisms. In further embodiments a subset of the control plane entities can be virtualized or executed in a cloud computing system while another subset of the control plane entities can be executed in a server distributed server system or similar system. The control plane entities can communicate with the data plane through the use of the OpenFlow protocol as described herein above or through other control protocols as described herein below.

The cloud computing system described herein above is provided by way of example and not by way of limitation. One skilled in the art would understand that the principles and features described above in relation to the cloud computing system can also be implemented in other configurations such as single servers or distributed server systems. Similar principles and features to those described above can be implemented in single server systems distributed server systems and similar computing environments. These principles and features can also be implemented using a non virtualized environment including non virtualized control plane entities that are executed in any combination of cloud computing systems single servers distributed server systems and similar systems.

In other embodiments other control protocols can be utilized in place of OpenFlow as described herein. The use of OpenFlow is presented by way of example and not limitation. Other control protocols can also be utilized to manage the communication between the control plane and data plane and configuration of the data plane of the split 3G PC architecture. An example of such a protocol is FORCES an IETF standard protocol for splitting the control plane and forwarding plane in networks. The FORCES protocol specification is described in RFC . RFC describes the architecture of a FORCES forwarding element the equivalent of an OpenFlow switch. The FORCES protocol itself does not directly support programming routes into the forwarding element it is instead a framework for handling the interaction between the FORCES controller and a FORCES forwarding element. The forwarding element architecture describes how to design the protocol that actually allows a FORCES controller to program a FORCES forwarding element. One skilled in the art would understand that a FORCES based system could include features described herein above in relation to the OpenFlow embodiment such as the GTP OpenFlow extension to allow the controller to program the switches for GTP TEID routing.

FORCES and OpenFlow are provided by way of example and not limitation. One skilled in the art would understand that the principles and features described above in relation to the FORCES and OpenFlow protocols can also be implemented in other similar control protocols.

As used herein a network device ND is an electronic device that communicatively interconnects other electronic devices on the network e.g. other network devices end user devices . Some network devices are multiple services network devices that provide support for multiple networking functions e.g. routing bridging switching Layer 2 aggregation session border control Quality of Service and or subscriber management and or provide support for multiple application services e.g. data voice and video . Network devices can include the devices implementing the OpenFlow switches of the data plate the devices implementing the SGSN D GGSN D and similar devices in the network. The

Two of the exemplary ND implementations in are 1 a special purpose network device that uses custom application specific integrated circuits ASICs and a proprietary operating system OS and 2 a general purpose network device that uses common off the shelf COTS processors and a standard OS.

The special purpose network device includes networking hardware comprising compute resource s which typically include a set of one or more processors forwarding resource s which typically include one or more ASICs and or network processors and physical network interfaces NIs sometimes called physical ports as well as non transitory machine readable storage media having stored therein networking software . A physical NI is hardware in a ND through which a network connection e.g. wirelessly through a wireless network interface controller WNIC or through plugging in a cable to a physical port connected to a network interface controller NIC is made such as those shown by the connectivity between NDs A H. During operation the networking software may be executed by the networking hardware to instantiate a set of one or more networking software instance s . Each of the networking software instance s and that part of the networking hardware that executes that network software instance be it hardware dedicated to that networking software instance and or time slices of hardware temporally shared by that networking software instance with others of the networking software instance s form a separate virtual network element A R. Each of the virtual network element s VNEs A R includes a control communication and configuration module A R sometimes referred to as a local control module or control communication module and forwarding table s A R such that a given virtual network element e.g. A includes the control communication and configuration module e.g. A a set of one or more forwarding table s e.g. A and that portion of the networking hardware that executes the virtual network element e.g. A .

In the embodiments described herein above the network devices can implement any of the data plane functionality of the 3G PC as a set of data plane modules A. The data plane modules A can be configured via a ND control plane and interface with the forwarding plane of the ND. In some embodiments the data plane modules can implement the functionality of the SGSN data plane SGSN D or GGSN data plane GGSN D or similar components in the 3G PC.

The special purpose network device is often physically and or logically considered to include 1 the ND control plane sometimes referred to as a control plane comprising the compute resource s that execute the control communication and configuration module s A R and 2 a ND forwarding plane sometimes referred to as a forwarding plane a data plane or a media plane comprising the forwarding resource s that utilize the forwarding table s A R and the physical NIs . By way of example where the ND is a router or is implementing routing functionality the ND control plane the compute resource s executing the control communication and configuration module s A R is typically responsible for participating in controlling how data e.g. packets is to be routed e.g. the next hop for the data and the outgoing physical NI for that data and storing that routing information in the forwarding table s A R and the ND forwarding plane is responsible for receiving that data on the physical NIs and forwarding that data out the appropriate ones of the physical NIs based on the forwarding table s A R.

Returning to the general purpose network device includes hardware comprising a set of one or more processor s which are often COTS processors and network interface controller s NICs also known as network interface cards which include physical NIs as well as non transitory machine readable storage media having stored therein software . During operation the processor s execute the software to instantiate a hypervisor sometimes referred to as a virtual machine monitor VMM and one or more virtual machines A R that are run by the hypervisor which are collectively referred to as software instance s . A virtual machine is a software implementation of a physical machine that runs programs as if they were executing on a physical non virtualized machine and applications generally do not know they are running on a virtual machine as opposed to running on a bare metal host electronic device though some systems provide para virtualization which allows an operating system or application to be aware of the presence of virtualization for optimization purposes. Each of the virtual machines A R and that part of the hardware that executes that virtual machine be it hardware dedicated to that virtual machine and or time slices of hardware temporally shared by that virtual machine with others of the virtual machine s A R forms a separate virtual network element s A R.

The virtual network element s A R perform similar functionality to the virtual network element s A R. For instance the hypervisor may present a virtual operating platform that appears like networking hardware to virtual machine A and the virtual machine A may be used to implement functionality similar to the control communication and configuration module s A and forwarding table s A this virtualization of the hardware is sometimes referred to as network function virtualization NFV . Thus NFV may be used to consolidate many network equipment types onto industry standard high volume server hardware physical switches and physical storage which could be located in Data centers NDs and customer premise equipment CPE . However different embodiments of the invention may implement one or more of the virtual machine s A R differently. For example while embodiments of the invention are illustrated with each virtual machine A R corresponding to one VNE A R alternative embodiments may implement this correspondence at a finer level granularity e.g. line card virtual machines virtualize line cards control card virtual machine virtualize control cards etc. it should be understood that the techniques described herein with reference to a correspondence of virtual machines to VNEs also apply to embodiments where such a finer level of granularity is used.

In certain embodiments the hypervisor includes a virtual switch that provides similar forwarding services as a physical Ethernet switch. Specifically this virtual switch forwards traffic between virtual machines and the NIC s as well as optionally between the virtual machines A R in addition this virtual switch may enforce network isolation between the VNEs A R that by policy are not permitted to communicate with each other e.g. by honoring virtual local area networks VLANs .

In the embodiments described herein above the virtual machines A R can implement any of the data plane functionality of the 3G PC as the set of data plane modules A. The data plane modules A can control the forwarding plane of the local ND or any of the other ND in the network of the 3G PC. In some embodiments the data plane modules can implement the entire functionality or a portion of the functionality of the SGSN data plane SGSN D or GGSN data plane GGSN D or similar components in the 3G PC in a distributed scheme within the set of ND of the network of the 3G PC.

The third exemplary ND implementation in is a hybrid network device which includes both custom ASICs proprietary OS and COTS processors standard OS in a single ND or a single card within an ND. In certain embodiments of such a hybrid network device a platform VM i.e. a VM that that implements the functionality of the special purpose network device could provide for para virtualization to the networking hardware present in the hybrid network device .

Regardless of the above exemplary implementations of an ND when a single one of multiple VNEs implemented by an ND is being considered e.g. only one of the VNEs is part of a given virtual network or where only a single VNE is currently being implemented by an ND the shortened term network element NE is sometimes used to refer to that VNE. Also in all of the above exemplary implementations each of the VNEs e.g. VNE s A R VNEs A R and those in the hybrid network device receives data on the physical NIs e.g. and forwards that data out the appropriate ones of the physical NIs e.g. . For example a VNE implementing IP router functionality forwards IP packets on the basis of some of the IP header information in the IP packet where IP header information includes source IP address destination IP address source port destination port where source port and destination port refer herein to protocol ports as opposed to physical ports of a ND transport protocol e.g. user datagram protocol UDP RFC and Transmission Control Protocol TCP RFC and and differentiated services DSCP values RFC and .

The NDs of for example may form part of the Internet or a private network and other electronic devices not shown such as end user devices including workstations laptops netbooks tablets palm tops mobile phones smartphones multimedia phones Voice Over Internet Protocol VOIP phones terminals portable media players GPS units wearable devices gaming systems set top boxes Internet enabled household appliances may be coupled to the network directly or through other networks such as access networks to communicate over the network e.g. the Internet or virtual private networks VPNs overlaid on e.g. tunneled through the Internet with each other directly or through servers and or access content and or services. Such content and or services are typically provided by one or more servers not shown belonging to a service content provider or one or more end user devices not shown participating in a peer to peer P2P service and may include for example public webpages e.g. free content store fronts search services private webpages e.g. username password accessed webpages providing email services and or corporate networks over VPNs. For instance end user devices may be coupled e.g. through customer premise equipment coupled to an access network wired or wirelessly to edge NDs which are coupled e.g. through one or more core NDs to other edge NDs which are coupled to electronic devices acting as servers. However through compute and storage virtualization one or more of the electronic devices operating as the NDs in may also host one or more such servers e.g. in the case of the general purpose network device one or more of the virtual machines A R may operate as servers the same would be true for the hybrid network device in the case of the special purpose network device one or more such servers could also be run on a hypervisor executed by the compute resource s in which case the servers are said to be co located with the VNEs of that ND.

A virtual network is a logical abstraction of a physical network such as that in that provides network services e.g. L2 and or L3 services . A virtual network can be implemented as an overlay network sometimes referred to as a network virtualization overlay that provides network services e.g. layer 2 L2 data link layer and or layer 3 L3 network layer services over an underlay network e.g. an L3 network such as an Internet Protocol IP network that uses tunnels e.g. generic routing encapsulation GRE layer 2 tunneling protocol L2TP IPSec to create the overlay network .

A network virtualization edge NVE sits at the edge of the underlay network and participates in implementing the network virtualization the network facing side of the NVE uses the underlay network to tunnel frames to and from other NVEs the outward facing side of the NVE sends and receives data to and from systems outside the network. A virtual network instance VNI is a specific instance of a virtual network on a NVE e.g. a NE VNE on an ND a part of a NE VNE on a ND where that NE VNE is divided into multiple VNEs through emulation one or more VNIs can be instantiated on an NVE e.g. as different VNEs on an ND . A virtual access point VAP is a logical connection point on the NVE for connecting external systems to a virtual network a VAP can be physical or virtual ports identified through logical interface identifiers e.g. a VLAN ID .

Examples of network services include 1 an Ethernet LAN emulation service an Ethernet based multipoint service similar to an Internet Engineering Task Force IETF Multiprotocol Label Switching MPLS or Ethernet VPN EVPN service in which external systems are interconnected across the network by a LAN environment over the underlay network e.g. an NVE provides separate L2 VNIs virtual switching instances for different such virtual networks and L3 e.g. IP MPLS tunneling encapsulation across the underlay network and 2 a virtualized IP forwarding service similar to IETF IP VPN e.g. Border Gateway Protocol BGP MPLS IPVPN RFC from a service definition perspective in which external systems are interconnected across the network by an L3 environment over the underlay network e.g. an NVE provides separate L3 VNIs forwarding and routing instances for different such virtual networks and L3 e.g. IP MPLS tunneling encapsulation across the underlay network . Network services may also include quality of service capabilities e.g. traffic classification marking traffic conditioning and scheduling security capabilities e.g. filters to protect customer premises from network originated attacks to avoid malformed route announcements and management capabilities e.g. full detection and processing .

For example where the special purpose network device is used the control communication and configuration module s A R of the ND control plane typically include a reachability and forwarding information module to implement one or more routing protocols e.g. an exterior gateway protocol such as Border Gateway Protocol BGP RFC Interior Gateway Protocol s IGP e.g. Open Shortest Path First OSPF RFC and Intermediate System to Intermediate System IS IS RFC Routing Information Protocol RIP version 1 RFC version 2 RFC and next generation RFC Label Distribution Protocol LDP RFC Resource Reservation Protocol RSVP RFC as well as RSVP Traffic Engineering TE Extensions to RSVP for LSP Tunnels RFC Generalized Multi Protocol Label Switching GMPLS Signaling RSVP TE RFC RFC and that communicate with other NEs to exchange routes and then selects those routes based on one or more routing metrics. Thus the NEs A H e.g. the compute resource s executing the control communication and configuration module s A R perform their responsibility for participating in controlling how data e.g. packets is to be routed e.g. the next hop for the data and the outgoing physical NI for that data by distributively determining the reachability within the network and calculating their respective forwarding information. Routes and adjacencies are stored in one or more routing structures e.g. Routing Information Base RIB Label Information Base LIB one or more adjacency structures on the ND control plane . The ND control plane programs the ND forwarding plane with information e.g. adjacency and route information based on the routing structure s . For example the ND control plane programs the adjacency and route information into one or more forwarding table s A R e.g. Forwarding Information Base FIB Label Forwarding Information Base LFIB and one or more adjacency structures on the ND forwarding plane . For layer 2 forwarding the ND can store one or more bridging tables that are used to forward data based on the layer 2 information in that data. While the above example uses the special purpose network device the same distributed approach can be implemented on the general purpose network device and the hybrid network device .

For example where the special purpose network device is used in the data plane each of the control communication and configuration module s A R of the ND control plane typically include a control agent that provides the VNE side of the south bound interface . In this case the ND control plane the compute resource s executing the control communication and configuration module s A R performs its responsibility for participating in controlling how data e.g. packets is to be routed e.g. the next hop for the data and the outgoing physical NI for that data through the control agent communicating with the centralized control plane to receive the forwarding information and in some cases the reachability information from the centralized reachability and forwarding information module it should be understood that in some embodiments of the invention the control communication and configuration module s A R in addition to communicating with the centralized control plane may also play some role in determining reachability and or calculating forwarding information albeit less so than in the case of a distributed approach such embodiments are generally considered to fall under the centralized approach but may also be considered a hybrid approach .

While the above example uses the special purpose network device the same centralized approach can be implemented with the general purpose network device e.g. each of the VNE A R performs its responsibility for controlling how data e.g. packets is to be routed e.g. the next hop for the data and the outgoing physical NI for that data by communicating with the centralized control plane to receive the forwarding information and in some cases the reachability information from the centralized reachability and forwarding information module it should be understood that in some embodiments of the invention the VNEs A R in addition to communicating with the centralized control plane may also play some role in determining reachability and or calculating forwarding information albeit less so than in the case of a distributed approach and the hybrid network device . In fact the use of SDN techniques can enhance the NFV techniques typically used in the general purpose network device or hybrid network device implementations as NFV is able to support SDN by providing an infrastructure upon which the SDN software can be run and NFV and SDN both aim to make use of commodity server hardware and physical switches.

While shows the distributed approach separate from the centralized approach the effort of network control may be distributed differently or the two combined in certain embodiments of the invention. For example 1 embodiments may generally use the centralized approach SDN but have certain functions delegated to the NEs e.g. the distributed approach may be used to implement one or more of fault monitoring performance monitoring protection switching and primitives for neighbor and or topology discovery or 2 embodiments of the invention may perform neighbor discovery and topology discovery via both the centralized control plane and the distributed protocols and the results compared to raise exceptions where they do not agree. Such embodiments are generally considered to fall under the centralized approach but may also be considered a hybrid approach.

While illustrates the simple case where each of the NDs A H implements a single NE A H it should be understood that the network control approaches described with reference to also work for networks where one or more of the NDs A H implement multiple VNEs e.g. VNEs A R VNEs A R those in the hybrid network device . Alternatively or in addition the network controller may also emulate the implementation of multiple VNEs in a single ND. Specifically instead of or in addition to implementing multiple VNEs in a single ND the network controller may present the implementation of a VNE NE in a single ND as multiple VNEs in the virtual networks all in the same one of the virtual network s each in different ones of the virtual network s or some combination . For example the network controller may cause an ND to implement a single VNE a NE in the underlay network and then logically divide up the resources of that NE within the centralized control plane to present different VNEs in the virtual network s where these different VNEs in the overlay networks are sharing the resources of the single VNE NE implementation on the ND in the underlay network .

On the other hand respectively illustrate exemplary abstractions of NEs and VNEs that the network controller may present as part of different ones of the virtual networks . illustrates the simple case of where each of the NDs A H implements a single NE A H see but the centralized control plane has abstracted multiple of the NEs in different NDs the NEs A C and G H into to represent a single NE I in one of the virtual network s of according to some embodiments of the invention. shows that in this virtual network the NE I is coupled to NE D and F which are both still coupled to NE E.

While some embodiments of the invention implement the centralized control plane as a single entity e.g. a single instance of software running on a single electronic device alternative embodiments may spread the functionality across multiple entities for redundancy and or scalability purposes e.g. multiple instances of software running on different electronic devices .

Similar to the network device implementations the electronic device s running the centralized control plane and thus the network controller including the centralized reachability and forwarding information module may be implemented a variety of ways e.g. a special purpose device a general purpose e.g. COTS device or hybrid device . These electronic device s would similarly include compute resource s a set or one or more physical NICs and a non transitory machine readable storage medium having stored thereon the centralized control plane software. For instance illustrates a general purpose control plane device including hardware comprising a set of one or more processor s which are often COTS processors and network interface controller s NICs also known as network interface cards which include physical NIs as well as non transitory machine readable storage media having stored therein centralized control plane CCP software . The control plane device can be one of many such devices networked together to form a cloud computing system or similar system as described herein above and in particular for implementing the control plane of a 3G PC.

In embodiments that use compute virtualization the processor s typically execute software to instantiate a hypervisor sometimes referred to as a virtual machine monitor VMM and one or more virtual machines A R that are run by the hypervisor which are collectively referred to as software instance s . A virtual machine is a software implementation of a physical machine that runs programs as if they were executing on a physical non virtualized machine and applications generally are not aware they are running on a virtual machine as opposed to running on a bare metal host electronic device though some systems provide para virtualization which allows an operating system or application to be aware of the presence of virtualization for optimization purposes. Again in embodiments where compute virtualization is used during operation an instance of the CCP software illustrated as CCP instance A on top of an operating system A are typically executed within the virtual machine A. In embodiments where compute virtualization is not used the CCP instance A on top of operating system A is executed on the bare metal general purpose control plane device .

The operating system A provides basic processing input output I O and networking capabilities. In some embodiments the CCP instance A includes a network controller instance . The network controller instance includes a centralized reachability and forwarding information module instance which is a middleware layer providing the context of the network controller to the operating system A and communicating with the various NEs and an CCP application layer sometimes referred to as an application layer over the middleware layer providing the intelligence required for various network operations such as protocols network situational awareness and user interfaces . At a more abstract level this CCP application layer within the centralized control plane works with virtual network view s logical view s of the network and the middleware layer provides the conversion from the virtual networks to the physical view.

In one embodiment any number or combination of the control plane modules that implement the functions of the control plane of the 3G PC can be executed by a virtual machine A as part of a centralized control plane CCP instance A of the CCP software or in a similar virtualized configuration. The control plane modules can include the AUC HLR HSS SGSN C GGSN C or similar control plane modules of the 3G PC. The control plane modules of a particular 3G PC can be executed by the same virtual machine A or distributed over any number of virtual machines and over any number of control plane devices . This distribution can be dynamic to enable load balancing and similar resource management within a cloud computing system. In one embodiment the control plane modules can be part of the CCP application layer of a network controller instance .

The centralized control plane transmits relevant messages to the data plane based on CCP application layer calculations and middleware layer mapping for each flow. A flow may be defined as a set of packets whose headers match a given pattern of bits in this sense traditional IP forwarding is also flow based forwarding where the flows are defined by the destination IP address for example however in other implementations the given pattern of bits used for a flow definition may include more fields e.g. 10 or more in the packet headers. Different NDs NEs VNEs of the data plane may receive different messages and thus different forwarding information. The data plane processes these messages and programs the appropriate flow information and corresponding actions in the forwarding tables sometime referred to as flow tables of the appropriate NE VNEs and then the NEs VNEs map incoming packets to flows represented in the forwarding tables and forward packets based on the matches in the forwarding tables.

Standards such as OpenFlow define the protocols used for the messages as well as a model for processing the packets. The model for processing packets includes header parsing packet classification and making forwarding decisions. Header parsing describes how to interpret a packet based upon a well known set of protocols. Some protocol fields are used to build a match structure or key that will be used in packet classification e.g. a first key field could be a source media access control MAC address and a second key field could be a destination MAC address .

Packet classification involves executing a lookup in memory to classify the packet by determining which entry also referred to as a forwarding table entry or flow entry in the forwarding tables best matches the packet based upon the match structure or key of the forwarding table entries. It is possible that many flows represented in the forwarding table entries can correspond match to a packet in this case the system is typically configured to determine one forwarding table entry from the many according to a defined scheme e.g. selecting a first forwarding table entry that is matched . Forwarding table entries include both a specific set of match criteria a set of values or wildcards or an indication of what portions of a packet should be compared to a particular value values wildcards as defined by the matching capabilities for specific fields in the packet header or for some other packet content and a set of one or more actions for the data plane to take on receiving a matching packet. For example an action may be to push a header onto the packet for the packet using a particular port flood the packet or simply drop the packet. Thus a forwarding table entry for IPv4 IPv6 packets with a particular transmission control protocol TCP destination port could contain an action specifying that these packets should be dropped.

Making forwarding decisions and performing actions occurs based upon the forwarding table entry identified during packet classification by executing the set of actions identified in the matched forwarding table entry on the packet.

However when an unknown packet for example a missed packet or a match miss as used in OpenFlow parlance arrives at the data plane the packet or a subset of the packet header and content is typically forwarded to the centralized control plane . The centralized control plane will then program forwarding table entries into the data plane to accommodate packets belonging to the flow of the unknown packet. Once a specific forwarding table entry has been programmed into the data plane by the centralized control plane the next packet with matching credentials will match that forwarding table entry and take the set of actions associated with that matched entry.

A network interface NI may be physical or virtual and in the context of IP an interface address is an IP address assigned to a NI be it a physical NI or virtual NI. A virtual NI may be associated with a physical NI with another virtual interface or stand on its own e.g. a loopback interface a point to point protocol interface . A NI physical or virtual may be numbered a NI with an IP address or unnumbered a NI without an IP address . A loopback interface and its loopback address is a specific type of virtual NI and IP address of a NE VNE physical or virtual often used for management purposes where such an IP address is referred to as the nodal loopback address. The IP address es assigned to the NI s of a ND are referred to as IP addresses of that ND at a more granular level the IP address es assigned to NI s assigned to a NE VNE implemented on a ND can be referred to as IP addresses of that NE VNE.

Next hop selection by the routing system for a given destination may resolve to one path that is a routing protocol may generate one next hop on a shortest path but if the routing system determines there are multiple viable next hops that is the routing protocol generated forwarding solution offers more than one next hop on a shortest path multiple equal cost next hops some additional criteria is used for instance in a connectionless network Equal Cost Multi Path ECMP also known as Equal Cost Multi Pathing multipath forwarding and IP multipath RFC and may be used e.g. typical implementations use as the criteria particular header fields to ensure that the packets of a particular packet flow are always forwarded on the same next hop to preserve packet flow ordering . For purposes of multipath forwarding a packet flow is defined as a set of packets that share an ordering constraint. As an example the set of packets in a particular TCP transfer sequence need to arrive in order else the TCP logic will interpret the out of order delivery as congestion and slow the TCP transfer rate down.

While the invention has been described in terms of several embodiments those skilled in the art will recognize that the invention is not limited to the embodiments described can be practiced with modification and alteration within the spirit and scope of the appended claims. The description is thus to be regarded as illustrative instead of limiting.

