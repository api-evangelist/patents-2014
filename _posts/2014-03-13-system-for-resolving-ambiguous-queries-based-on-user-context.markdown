---

title: System for resolving ambiguous queries based on user context
abstract: A system for resolving a user's question regarding media at the user's location where the question includes ambiguity as to the context of the question. Context information is acquired from a device or devices at the user's location, and if multiple possible contexts are identified, the system may attempt to resolve the question for more than one of the possible contexts. The system for answering questions and the source of the media may be different and independent.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09286910&OS=09286910&RS=09286910
owner: AMAZON TECHNOLOGIES, INC.
number: 09286910
owner_city: Reno
owner_country: US
publication_date: 20140313
---
Reference tools that provide on demand information about media such as books and movies are becoming increasingly common. When someone is reading or watching something they can refer to the reference tool if they would like more information. The reference tool may be provided by the device displaying the media or may be provided by a separate device such as a smart phone or tablet computer. Some reference tools are embedded with the media while others require a network connection to obtain information.

On demand reference tools provide media viewers and e book readers with supplemental facts information and trivia about media being watched or read. These tools have generally fallen into one of two categories generalized tools that interpret user questions and attempt to provide specific answers but which have a limited capacity to interpret context from a user s perspective when resolving an answer and media specific tools supporting a single identified work or composition providing context specific information but with little or no capacity to interpret user questions.

Inline reference tools such as Amazon s X Ray and Sony s MovieIQ provide on demand reference information that further describe a specific scene or passage of media such as displaying the cast members in a current scene of a film and providing links to the cast member s biographies. However the media and reference information are linked to a particular source and to a particular work or composition such as to a streamed movie e.g. via a Kindle player or to a movie being played back from interactive media e.g. the BD Live service provided by Blu ray disc players and provide a user unfiltered information leaving it to the user to resolve the answer to any specific question.

 Second screen applications can decouple the reference tool from the media source using a microphone to sample audible content to determine context. Examples include Shazam s music identification service and Zeebox s social television platform. However while such services decouple the media and reference sources the reference information they provide is still linked to a single identified composition and it is still left to the user to resolve the answer to any specific question.

Generalized tools also decouple the reference tool from the media source. For example speech based intelligent assistants such as Evi Apple s Siri and Google Now provide generalized answers not linked to a particular work or composition. However other than device location these platforms possess limited information about the user s current perspective beyond any context provided by words in a user s question hindering their ability to correctly resolve ambiguity common in ordinary speech.

Efforts to improve language interpretation algorithms have incrementally increased the ability of software driven agents and assistants to interpret meaning but even when these systems have access to the same databases utilized by media specific tools their ability to resolve ambiguity in a question continues to hinder their utility. Something as simple as use of a pronoun out of context can stymie a computer or cause it to produce a nonsensical answers.

The user device such as a mobile telephone tablet computer etc. is associated with a user and is configured to receive natural language search queries from the user. These queries may be typed or be spoken utterances. System may use a common protocol for both natural language processing and the characterization of spoken utterances. Speech processing may include automatic speech recognition and natural language processing. Automatic speech recognition ASR comprises converting speech into an interpreted result such as text whereas natural language processing NLP comprises determining the meaning of that interpreted result.

The user may provide a signal e.g. push a button on device speak an attention word etc. to indicate that an utterance is a spoken question inquiry to be processed or the system may constantly listen and identify when an utterance is a question. If the system listens for user utterances NLP heuristics may optionally be used to identify that an utterance is a question based on the use of grammar such as the use of an interrogative sentence structure word or particle and or the use of an inflected verb form. As different languages signal that a sentence is a question differently the NLP heuristics may be language specific. As an alternative to pre programmed heuristics if an utterance is to be recognized as a question recognition may be performed using more basic features such as na ve Bayes over n grams or other generic features. Using such probabilistic modeling techniques may be more language independent than heuristics because only the training data would need to vary per language.

Once the system receives and identifies a user question the question is interpreted which includes parsing the question to identify component objects to identify relationships between the objects and to identify which objects are ambiguous without context. In essence the interpreter of system translates a question into a symbolic construct for processing.

The symbolic construct may be configured according to a number of different syntactical arrangements. For example the construct may use a syntax that interfaces with a knowledge representation system such as the knowledge representation systems described in U.S. Pat. No. 7 013 308 filed on Nov. 21 2001 in the name of Tunstall Pedoe U.S. Pat. No. 7 707 160 filed on Dec. 23 2005 in the name of Tunstall Pedoe U.S. Pat. No. 8 219 599 filed on Oct. 17 2011 in the name of Tunstall Pedoe and or U.S. Patent Application Publication 2011 0307435 A1 filed on May 12 2011 in the names of Overell et al. the contents of all of which are hereby expressly incorporated herein by reference in their entireties and collectively referred to below as the Incorporated Knowledge System or Incorporated Knowledge System documents.

As used herein the term question refers to any sequence of words in natural language the purpose of which is to solicit knowledge from the system as discussed in the Incorporated Knowledge System documents. A question need not necessarily conform to the classic grammatical definition of a question. For example it could be in imperative form such as Tell me what the capital of France is or the meaning could be implied.

The interpreter has identified that the statement is a question. The construction e.g. verb tense with that indicates the question is directed at the current moment in time and that how old is the fact to be resolved. However while that actor is identified as referring to a human being the contextual meaning phrase is identified as ambiguous requiring context based anaphora to resolve.

Possible contexts may be determined by collecting available information prior and or in response to interpreting the question and may come from multiple sources. For example the user device may determine what software applications the user is currently using on the user device itself what applications are in the foreground and or background and what media content is currently being provided to the user . The user device may also query other networked devices in a network neighborhood e.g. connected to a same network access point such as cable box digital video recorder DVR and television to determine what content is currently being provided to the user . The user device may also sample audible content such as the output of speakers to be compared by the system with stored acoustic fingerprints to determine content. Video or images may also be captured by a camera or cameras on user device . Context information may also be collected from other devices associated with the user and or user device such as devices whose output is detected by the sensor s of the user device or devices that are determined to be proximate to the user during a relevant time i.e. time of a question or time referred to in a question . As an example the system may also lookup other information about the user that may be relevant to determining context such as paperback books the user is known to have purchased sports teams the user has demonstrated an interest in etc. The user device may also determine the context of another media device if the other media device is accessing network based cloud services and or is sending status updates to a server such as if the user is reading an e book on a media device and the media device is sending updates to the server indicating that a page was flipped the furthest page reached etc. The server may be at a location that is physically remote from both the user device and the media device or may be co located with one or both. The book title or other identifier and a timestamp associated with the last update may be used to determine that the user is actively reading what they are reading and where they are in the book.

For example based on context information provided by cable box DVR and or resolved by the server based on audio sampling context related facts may be 

Other facts may be determined such as the current time index of the film at the time the query is received. Some facts may already exist as stored facts in the knowledge base or be determined by querying the cable box DVR such as 

Other general non content specific knowledge may already have been imported into the knowledge base from a movie database or may be uploaded after interpreting the question 

The system resolves ambiguity in the question by applying context specific facts to resolve the meaning of ambiguous words and general facts to resolve the question itself. For example the translated query can be answered after providing the anaphora resolution 

If the ambiguity and the question can be resolved based on more than one of the possible contexts answers may be prioritized and or culled based on the immediacy of the context e.g. if a question may be resolved for both a scene in a movie audible at the user s location and a scene in a book the user recently purchased the context information for the movie has greater immediacy and biased to favor the context of recently asked prior questions. If two answers resolve with equal priority bias both answers may be output. The semantics of the question can also be used to rule out context objects. For example if the user asked whose daughter is that actor then that actor could resolve to any of the actors on the screen but there is the additional constraint of gender which comes from the translated query using is the daughter of and the knowledge base s in may have a fact indicating that parent plus attribute class animal female is the left class of is the daughter of i.e. that only females can be daughters.

Contextual ambiguity may be resolved iteratively in parallel or in other orders. For example resolving ambiguity iteratively may start by attempting to resolve for the context with the highest immediacy and trying other possible contexts if the query will not resolve. In parallel contextual ambiguity may be resolved by processing for multiple contexts at the same time and then selecting the context that resolves with an NLP meaning weight that most closely matches the meaning associated with the rest of the question. If multiple contexts produce similarly weighted results other factors such as immediacy and user profile information e.g. comparing known interests of the user with aggregate user profiles to determine commonality of interest may be applied to generate a probability set from which a context that resolves is selected.

After the question is resolved the answer is output to the user s device such as outputting the answer to a display of the user device . A grammar framework may be used to convert the answer into conversational language prior to outputting to the user s device . A grammar framework may also be applied after the answer is received by the reference query response module discussed below. In addition to the answer additional related content and or links to such information may also be provided.

User device and server may be any computing devices or a collection of computing devices and are referred to herein as user device and server to facilitate understanding based upon a prevalent system architecture. Other arrangements are possible such as dividing or sharing the functionality of components of the user device and the server across multiple different device. Also a same server may serve different user devices having differing capabilities such that the server may provide different functionality to difference client devices depending upon the capabilities of the individual client devices.

Each of the device and the server may include one or more controllers processors comprising one or more central processing units CPUs for processing data and computer readable instructions and a memory for storing data and instructions. The memory may include volatile random access memory RAM non volatile read only memory ROM non volatile magnetoresistive MRAM and or other types of memory. The device and the server may also include a data storage component for storing data and processor executable instructions. The data storage component may include one or more non volatile storage types such as magnetic storage optical storage solid state storage etc. The device and server may also be connected to a removable or external non volatile memory and or storage such as a removable memory card memory key drive networked storage etc. through the input output device interfaces .

Executable instructions for operating the device the server and their various components may be executed by the controller s processor s using the memory as temporary working storage at runtime. The executable instructions may be stored in a non transitory manner in non volatile memory storage or an external device. Alternatively some or all of the executable instructions may be embedded in hardware or firmware in addition to or instead of software.

Referring to the system may include a variety of sensors such as those illustrated with device . Among the sensors are an audio capture component such as microphone s an image and or video capture component such as camera s an antenna and global positioning sensors . Several of each of these components may be included. The device may also include user interface components such as a display and a touch interface . Although shown as integrated within device some or parts of the various sensors and user interface components may be external to device and accessed through input output device interfaces .

The antenna and related components e.g. radio transmitter receiver transceiver modem etc. may be configured to operate with a wireless local area network WLAN such as WiFi Bluetooth and or wireless networks such as a Long Term Evolution LTE network WiMAX network 3G network etc.

The audio capture component may be for example a microphone or array of microphones a wireless headset e.g. in a wired headset e.g. in etc. If an array of microphones is included approximate distance to a sound s point of origin may be performed through acoustic localization based on time and amplitude differences between sounds captured by different microphones of the array.

The touch interface may be integrated with a surface of a display or may be separate e.g. a touch pad . The touch interface may be of any technology such as capacitive resistive optical infrared thermal temperature piezoelectric etc. Other pointing devices for interacting with a graphical user interface GUI may be included such as a touchpad a trackball or a mouse.

The global positioning module provides an interface for acquiring location information such as information from satellite geographic positioning system s . For example. the global positioning module may include a Global Positioning System GPS receiver and or a Global Navigation Satellite System GLONASS receiver. The global positioning module may also acquire location based information using other radio sources e.g. via antenna such as mapping services that triangulate off of known WiFi service set identifiers SSIDs or cellular towers within range of the device .

Examples of other sensors include an electronic thermometer to measure ambient temperature a proximity sensor to detect whether there is an object within a certain distance of the device and a physical button or switch that may be used to by the user to signal the system e.g. to signal that an utterance is a question .

Sensors may be communicatively coupled with other components of system via input output I O device interfaces and or via an address data bus . The address data bus conveys data among components of the device . Each component within the device may also be directly connected to other components in addition to or instead of being connected to other components across the bus .

The I O device interfaces may connect to a variety of components and networks. Among other things the I O device interfaces may include an interface for an external peripheral device connection such as universal serial bus USB FireWire Thunderbolt or other connection protocol. The input output device interfaces may also support a variety of networks via an Ethernet port and antenna .

The video output component of user device such as display may be used as a user interface via which the answer is output in and for displaying images and media e.g. movies books etc. The video output component may be a display of any suitable technology such as a liquid crystal display an organic light emitting diode display electronic paper an electrochromic display a pico projector etc. The video output component may be integrated into the device or may be separate.

The system may also include an audio output component such as a speaker a wired headset or a wireless headset. Other output devices include a haptic effect generator not illustrated . The haptic effect generator may be of any haptics technology including technologies to vibrate the entire device e.g. electromagnetic technologies such as vibratory motor or a coil with a central mass and or may comprise technologies allowing the haptic effect to be localized to a position of the touch interface such as electroactive polymers piezoelectrics electrostatics subsonic audio wave surface actuation etc.

As discussed above device include controller s processors memory and storage . In addition the device may include a reference query capture module and a reference query response module which may be part of a software application running in the foreground and or the background on the device

The reference query capture module captures questions input by the user using speech processing and or text entry as well as gathering context information from the sensors from other applications running on the device and from other independent devices in a network neighborhood. Context information may be periodically gathered and transmitted to the server prior to receiving a question.

A classifier system of the reference query capture module processes captured audio. The speech processing engine of classifier system may perform speech recognition and natural language processing NLP using speech models and heuristics stored in storage . The classifier system may comprise for example a Support Vector Machine SVM although other machine learning techniques might be used instead of or to augment SVM. The classifier system may utilize Hidden Markov Models HMMs Gaussian Mixture Models GMMs Mel Frequency Cepstrum Coefficients MFCCs perceptual linear predictive PLP techniques neural network feature vector techniques linear discriminant analysis semi tied covariance matrices etc. Heuristics used to identify whether an utterance is a question and to interpret an identified question for resolution may be performed as an NLP function of speech processing engine .

The NLP component of the speech processing engine may also be used to interpret typed queries. NLP may be used to analyze and tag format the text for subsequent processing and interpretation. NLP question recognition and question interpretation may be based on heuristics and models stored in storage . Such models and heuristics may be grammar based rule based or constructed in a different manner.

The automatic speech recognition ASR component of speech processing engine may also be configured to translate slang terminology abbreviated terms synonyms and other queries into textual expressions that can be understood and used by the query processing unit of the server .

How speech processing functionality is divided between the user device and the server depends upon how the components of system are configured and their computational capabilities. For example both speech recognition and natural language processing may be performed entirely on the user s device entirely on the server or some division of functionality in between such as performing speech recognition on the user s device but performing natural language processing on the server e.g. having speech processing engine of the server perform NLP .

An acoustic fingerprint encoder of the reference query capture module samples audio captured by the microphone s and encodes the audio for processing an acoustic fingerprint engine of a classifier system on the server to identify context. Encoding the audio samples reduces the volume of data shared between the user device and the server . However raw audio may also be sent by the reference capture module for processing on the server . Front end filtering used by the acoustic fingerprint encoder may be different from that used by classifier system such as using a low pass filter for filtering speech processed by speech processing engine and using a high pass filter for encoding acoustic fingerprints.

A services discovery engine of the reference query capture module queries nearby devices in the network neighborhood as well as determining what software applications the user is currently using on the user device itself what applications are in the foreground and what content is currently being provided to the user . Devices and applications may be queried for content information and metadata via various application and operating system level interfaces. The service discovery engine may use a zero configuration networking protocol to discover nearby devices that may have contextual information such as the service location protocol SLP .

Aspects of the services discovery engine may also exist on the server or the services discovery engine may exist entirely on the server . For example the server may check for pre registered devices associated with a network address e.g. Internet Protocol address at the same location as the user device with the server querying the other devices directly.

A search processing engine of reference query capture module transmits a question captured by classifier system to the query processing engine on the server . Depending upon the distribution of processing functionality the query processing engine may transmit a captured utterance recognized text of a captured utterance an utterance processed with NLP and or identified as a question an interpreted captured question or text of a keyed in question.

The reference query response module includes a presentation engine for outputting the answer to the user. Examples of the presentation engine may include a user interface component of a software application and a web browser.

Depending upon how functionality is divided aspects of the presentation engine may also be located on the server . For example if the answer is to be output as synthesized speech and the presentation engine on the user s device handles preparation of output the query processing engine of the server may send answer information to the presentation engine on the user s device and have the user s device synthesize the speech output. However this preparation of the output could also be prepared by aspects of the presentation engine located on the server sending the presentation engine on the user s device synthesized speech read to be output. Similar divisions of output preparation between the user s device and server also apply to text and graphical output.

Referring to the server is connected to a network via input output device interfaces . The input output device interfaces may also include an interface for an external peripheral device connection such as universal serial bus USB FireWire Thunderbolt or other connection protocol. The input output device interfaces may connect to one or more networks via an Ethernet port a wireless local area network WLAN such as WiFi radio Bluetooth and or wireless network radio such as a radio capable of communication with a wireless communication network such as a Long Term Evolution LTE network WiMAX network 3G network etc.

The server may include an address data bus for conveying data among components of the server . Each component within the server may also be directly connected to other components in addition to or instead of being connected to other components across the bus .

The server further includes a query processing module that determines content and answers received questions. The query processing module receives information from sensors on the device and resolves received questions. The query processing engine comprises an inference engine applying information stored in knowledge base s to resolve received questions. Query processing engine may utilize knowledge stored in knowledge base s as well as additional databases to determine facts related to a query. Contextual facts provided by the search processing engine and services discovery engine may be applied by the query processing engine to resolve a query such as cross referencing a location of the user device as determined by the global position interface with a programming schedule for television at the user s location. Fact relating to the user may likewise be acquired and processed.

The query processing module includes a classifier system comprising a speech processing engine and an acoustic fingerprint engine . As noted above the question forming the basis for a knowledge base search query may be a textual input or an audio speech input. The speech processing engine may identify and interpret user questions as discussed above with speech processing engine . Acoustic fingerprint engine compares captured audio from reference query capture module to identify audible media. If images or video are provided by the reference query capture module the classifier system may also perform image processing to determine context information. The classifier system of the server may use the same or similar techniques to those described with the classifier system of the user device and speech recognition techniques the acoustic fingerprints and image processing may utilize the same or similar pattern recognition techniques but with different models or may use different techniques altogether. Acoustic fingerprints speech processing models image processing models rules and heuristics may be stored in the knowledge base s in storage or in some other library or database.

Interpreting and resolving a question on either the user device or the server may include accessing the knowledge base and or query processing engine .

The knowledge base may include facts and their ontology structured using various classifiers relating to different types of compositions that may be basis for questions. This allows for efficient searching identification and return of facts to resolve questions into specific answers. Once the question has been processed the query processing engine sends the result to the device to be output by the presentation engine .

Components of the query processing module such as the speech processing engine the knowledge base and the query processing engine may be part of a knowledge representation system for example the Incorporated Knowledge System.

The natural language component of the speech processing engine may perform translation between natural language and internal representations as described in the Incorporated Knowledge System documents. For example the natural language component may translate or parse a query into one or more corresponding classes relations data document objects facts time negative facts categories etc. The natural language component may identify and interpret a captured utterance processing a question received from a user into a query format readable by the inference engine of the query processing engine . The natural language component of the speech processing engine may provide this same functionality or the functionality may be divided between speech processing engines .

The query processing engine may cause retrieval of knowledge from various knowledge bases and databases such as the knowledge base s described in the Incorporated Knowledge System documents. The query processing engine may reference information that is either located on the server or in a variety of different locations through multiple application programming interfaces APIs utilizing protocols supported by the respective source of information.

A user profile processing engine of the query processing module may process settings input by the user and other profile information associated with a user of device and or device itself and pass this information to the query processing engine . For example the query processing engine may receive the query from the natural language component of the speech processing engine and apply the user profile information to the query using the knowledge base s to obtain one or more results that satisfy the query. For example a user may specify whether the query processing engine is authorized to acquire and or utilize information about the user s media purchase history when determining possible contexts for question resolution.

The knowledge base s may be populated with patterns e.g. acoustic fingerprints image data and facts from one or more databases. This information may be generalized but may also be time chapter or scene indexed to facilitate resolution of contextual ambiguity. Third party databases may also be queried for facts and added to knowledge base s or queried on demand. The knowledge base s may be static dynamic or have multiple parts some of which are static and some of which are dynamic. The knowledge or information in the knowledge base s may be stored in a structured form such as a form described in the Incorporated Knowledge System documents. For example the knowledge in the knowledge base s may be structured using various classifiers such as objects recognition data relations classes data document objects facts time negative facts the golden rule categories of knowledge etc. such as described in the Incorporated Knowledge System documents.

In addition to using acoustic fingerprints to determine contextual information the classifier system may use techniques such as face recognition to identify a face in view of a camera or cameras on the user device . For example if the camera captures a face on a television face matching may be used to identify an actor to determine a face e.g. the actor s name . As noted above the acoustic fingerprints and image pattern data used to identify media people etc. may be stored in knowledge base in storage or in some other database. If the information used by the other database is not in the format used for facts by the query processing engine the classifier system may translate the fact data into a format for the query processing engine .

All or a part of the knowledge base s may be continuously or periodically updated to add new or additional facts and context identifying information as well as current programming schedules metadata and other time dependent contextual information. Time dependent contextual information may be maintained in a dynamic part of the knowledge base populated with and continuously updated with information from one or more separate databases.

If acoustic localization is performed and or proximity detection adaptive filtering may be applied to determine whether the origin of an utterance is the user or some other source. Likewise techniques such as speaker recognition may be applied to distinguish between questions originating with user and utterances captured from other sources.

Query processing engine may also keep a time indexed record of what context was determined when resolving prior user questions. The time index may be used in part by the query processing engine to determine the immediacy e.g. freshness of a context. In addition the presentation engine on the user device may provide an interface e.g. a virtual button for the user to signal that the query processing engine has the wrong context providing the query processing engine to try to resolve other contexts. The query processing engine may also provide the user multiple contexts to choose from if an initial attempt or attempts to resolve the question was unsuccessful.

The knowledge base s may be loaded with facts to respond to questions relating to a wide variety of different types of media. Other topics may also be included such as products e.g. answering user questions about products they are identified as having purchased .

The query processing engine may cross reference facts to resolve queries using rules and heuristics to generate resolvable sub queries. So for example if a video is being played and the services discovery engine reports the title and scene or captured audio or video is used by the classifiers system to determine title and scene questions such as where the scene was shot who is in the current scene what characters as opposed to actors are in the current scene then the semantic objects present in the question may be sufficient to tie the questions to the ontology facts and other information in the knowledge base s . However if a user asks a question such as what other movies has this actor been in resolving the query may require not just identifying the actor based on context but may require cross referencing the actor with a list of movies if the movie database does not link an actor to every movie in which they appeared. Similarly if there is a scene where music is playing and the user asks who is the drummer then in addition to recognizing the composition the composition could be cross referenced reframing a query using a mix of original and resolved semantic information to determine who is the drummer in this band what album the composition is from the meaning of the lyrics and other questions that may require multiple queries to resolve with each query resolving part of whatever ambiguity is identified.

The system may also process text such as closed captioning or subtitles of a program being viewed by a user or text from an e book the user recently read. For example the user device detects text from a program including a discussion about travel to a particular city and the user queries the user device with the question where is that the system may analyze the recent text for example subtitles or closed captioning to find semantic entities with a location and then answer the question.

Factual information may also be derived from the user s choice of words. For example if both a male actor and female actor appear in a current scene a user s use of the word actress may be used to identify gender. If the actors entries in the knowledge base s do not include an indication of gender the names of the persons identified in the current context may be compared with lists of names tied to gender to determine which actor is more likely female. In some languages other than English other grammatical aspects might be used for the same purpose such as a masculine or feminine form of a words being included in the question. Either query module or may also note a user s propensity to use certain words to resolve ambiguity. For example if a user has used gender specific words such as actor and actress then a question stating actor may be automatically assume to refer to the masculine. Such pattern recognition may be based on among other things machine learning technique.

The system monitors an audio input device s at the user device such as microphone to capture user utterances. After an utterance is received the speech processing engine processes it to determine that the utterance contains a question e.g. based on an interrogative sentence structure word or particle and or the use of an inflected verb form .

The question is then interpreted creating a semantic representation by converting the question into a symbolic construct for processing. This includes identifying the meaning of the question and identifying context related ambiguity.

The query processing engine acquires context related information such as information provided by the acoustic fingerprint encoder by the services discovery engine by the various sensors accessible to the user device such as the microphone camera and global positioning interface by other applications and or the operating system on running on the controller s processor s and by databases and knowledge base s containing information about media that the user may own or have accessed. User profile information from the user profile processing engine may restrict which resources are accessed when acquiring context related information.

The query processing engine then determines possible contexts based on the acquired information. Among other things this may include information from the classifier system identifying audio media or captured images based on sensor data or encoded acoustic fingerprints provided by user device . If multiple possible contexts are identified the query processing engine may prioritize them based on among other things heuristics assigning different context sources and or different levels of immediacy or other factors.

Examples of how immediacy may be determined include applying a set of heuristics to prioritize context based on a defined context hierarchy e.g. if a question may be resolved for both a scene in a movie audible at the user s location and a scene in a book the user recently purchased the context information for the movie may have greater immediacy based on physical proximity of the user to the media source e.g. using acoustic localization to approximate a distance between the user s device and an audible source such as speakers in based on how recently in time a user is known to have interacted with a media source e.g. if a user has an e book open on user device does the quantity of time that has transpired since the user last changed pages indicate that the user is actively reading and based on the context of other recently asked questions e.g. if possible contexts for recent questions have included a movie playing on the television and an e book on user device but resolved as being related to the e book and those same contexts are still possible candidates for a new question then the prioritization of contexts may be biased to favor the e book . Pattern recognition techniques may also be applied.

The query processing engine then selects the highest priority context and applies the selected context to the context related ambiguity. If the ambiguity does not resolve for the selected context No the query processing engine selects the context with the next highest priority and tries again to resolve the ambiguity.

In some cases for the same context there may be more than one solution to the context related ambiguity. For example if the user asks who is that actor on the screen and there are multiple actors each may resolve the ambiguity. In such cases the query processing engine may apply heuristics to cull the list of potential resolutions for the ambiguity. For example if the user profile indicates that an actor in the scene is a favorite of the user that is unlikely to be the actor the user is asking about. In comparison if the actor is appearing for the first time in the current scene it is more likely to be the actor the user is asking about. Examples of other factors include such indicia as how important the actor is based on a database ranking of actors whether the actor is known to have appeared in other media that the user is known to have watched whether the user has asked about the actor before whether the actor is a marquee actor in the movie or program etc. Probabilities are generated based on such indicia to determine a likely subject of the user s inquiry. If more than one candidate subject has a similarly high probability of being the subject that resolves the ambiguity the query processing engine may proceed to resolve the question for more than one candidate subject such that more than one answer may be output.

After a selected context does resolve the context related ambiguity Yes the query processing engine applies available facts to the processed query to resolve the original question.

Either the query processing engine or or the presentation engine formats the answer s based on the results of resolved question translating the result into answer s that are presented in conversational language. Among other things this may include applying a defined grammar framework based on the grammar and words appearing in the original question. The applied grammar framework may be chosen in part on the format of the expression resulting from interpreting the question . The presentation engine then outputs the answer s at the user device . Other related information may also be output.

The processes illustrated in may be performed in a different order than that illustrated. For example receipt of the utterance and acquiring of context related information may be performed in any order e.g. at the same time and or with ongoing periodic continual acquisition of information used to determine context which at some point includes the utterance containing the question . As another example determining possible context may be performed in advance of receiving the utterance . As another example as noted above rather than resolving the question based on context and facts in an iterative fashion based on a prioritization of possible context based on factors such as immediacy the query processing engine may attempt to resolve contextual ambiguity for multiple contexts at a same time and then select the context resolves with the NLP meaning weight that most closely matches the meaning associated with the rest of the question. If multiple contexts produce similarly weighted results other heuristics such as those used to prioritize possible contexts may be applied to narrow the solution set factoring in immediacy and user profile information e.g. comparing known interests of the user with aggregate user profiles to determine commonality of interest .

The above system is describe in the context of questions and or inquiries however other user inputs such as commands may also include ambiguities that the system may be configured to resolve so that the command may be properly executed. For example if a user says to a user device while watching a television program show me a list of other shows starring that actress the described system either on its own or in conjunction with one or more other systems may be used to resolve the ambiguity and execute the user s command. Also while the user s inquiry is discussed as being a spoken or written natural language inquiry the inquiry may be formatted in a different manner and need not be a natural language inquiry. For example the user might take a photograph of a scene or a character in a scene on the display and send that to the query processing engine as an inquiry.

Multiple user devices and servers may be employed in a single system . In such a multi device system each of the user devices and servers may include different components for performing different aspects of context identification and query processing. The multiple devices may include overlapping components. The components of user device as illustrated in is exemplary and may be a stand alone device or may be included in whole or in part as a component of a larger device or system.

The concepts disclosed herein may be applied within a number of different devices and computer systems including for example general purpose computing systems multimedia set top boxes televisions stereos radios server client computing systems mainframe computing systems telephone computing systems laptop computers cellular phones personal digital assistants PDAs tablet computers wearable computing devices watches glasses etc. other mobile devices etc.

As illustrated in multiple devices user devices to server cable box DVR may contain components of the system and the devices may be connected over a network . For example each of smart phone augmented reality AR glasses laptop computer tablet computer desktop computer and cable box DVR may include components of the reference query capture module and the reference query response module as well as components of the query processing module . The computational capabilities of individual the devices to and may range from significant e.g. rivaling that of the server to being a dumb terminal with comparatively rudimentary capabilities e.g. basic input capture output and networking capabilities .

Network may include a local or private network or may include a wide network such as the internet. Networked devices may capture and output audio through a number of audio input devices such as headsets and speakers . These audio capture and output devices may be connected to networked devices either through a wired or wireless connection. Networked devices may also include embedded audio input devices and output devices such as an internal microphone and speaker that are not illustrated in .

The user device may connect to and communicate with different media devices using different types of networks e.g. WLAN Bluetooth LTE etc. In addition if a media device is accessing network based cloud services provided by server s and or sending updates to the server s the user device may acquire context information about the media device without any direct communication with the media device. For example one the user device and the media device may communicate with the server s via a WLAN Internet access point whereas the other may communicate with the server s via a cellular data service e.g. LTE . Based on an identifier associated with the media device e.g. network address device name etc. or the user of the media device and subject to user privacy settings the user device may query the server for context information about the media device without direct device to device or device to network to device communication. Additional information may also be provided in a same manner such as the physical location of the media device as approximated by the servicing network e.g. the cellular network the location of the access point used by the media device e.g. cellular tower repeater WiFi router geographic coordinates provided by the media device itself etc.

The above examples are meant to be illustrative. They were chosen to explain the principles and application of the disclosed system and are not intended to be exhaustive or to limit the disclosure. Many modifications and variations of the disclosed system may be apparent to those of skill in the art. Persons having ordinary skill in the field of knowledge representation systems expert systems inference engines and natural language processing should recognize that components and process steps described herein may be interchangeable with other components or steps or combinations of components or steps and still achieve the benefits and advantages of the present disclosure. Moreover it should be apparent to one skilled in the art that the disclosure may be practiced without some or all of the specific details and steps disclosed herein.

Aspects of the disclosed system may be implemented as a computer method or as an article of manufacture such as a memory device or non transitory computer readable storage medium. The computer readable storage medium may be readable by a computer and may comprise instructions for causing a computer or other device to perform processes described in the present disclosure. The computer readable storage medium may be implemented by a volatile computer memory non volatile computer memory hard drive solid state memory flash drive removable disk and or other media. In addition one or more engines and components of reference query capture module reference query response module and query processing module may be implemented as firmware or as a state machine in hardware. For example at least the acoustic fingerprint encoder of the reference query capture module may be implemented as an application specific integrated circuit ASIC or a digital signal processor DSP or some combination thereof.

As used in this disclosure the term a or one may include one or more items unless specifically stated otherwise. Further the phrase based on is intended to mean based at least in part on unless specifically stated otherwise.

