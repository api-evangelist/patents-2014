---

title: Decoding of closed captions at a media server
abstract: Systems and methods of processing closed captions are disclosed. For example, a media server may receive a first video stream and first closed caption data associated with the first video stream. The media server may interpret at least one command included in the first closed caption data to generate interpreted closed caption data. The media server may transmit, to a destination device, a second video stream including second closed caption data that is generated based on the interpreted closed caption data.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09319626&OS=09319626&RS=09319626
owner: WOWZA MEDIA SYSTEMS, LLC.
number: 09319626
owner_city: Golden
owner_country: US
publication_date: 20140522
---
The present application is a continuation of and claims priority to U.S. patent application Ser. No. 13 857 572 filed Apr. 5 2013 and entitled DECODING OF CLOSED CAPTIONS AT A MEDIA SERVER the content of which is incorporated by reference in its entirety.

The popularity of the Internet coupled with the increasing capabilities of personal mobile electronic devices has provided consumers with the ability to enjoy multimedia content almost anytime and anywhere. For example live e.g. sports events and video on demand VOD content e.g. television shows and movies can be streamed via the Internet to personal electronic devices e.g. computers mobile phones and Internet enabled televisions .

Broadcast television channels may include one or more closed captioning tracks to make programs more accessible to the hearing impaired. Similarly movies on digital video discs DVDs may include subtitle tracks in multiple languages. However Internet accessible content may not be provided with closed captioning. Moreover the large number of available closed captioning formats makes it difficult to implement closed captioning systems that are compatible with the various electronic devices and platforms available to consumers even though government regulations may mandate closed captioning for Internet accessible content. For example in the United States the Federal Communications Commission FCC has issued a rule entitled Closed Captioning of Internet Protocol Delivered Video Programming Implementation of Twenty First Century Communications and Video Accessibility Act of 2010. Many providers for streaming video may be concerned with only a specific streaming technology application or platform. Thus a single platform closed captioning system offered by such providers in response to government regulations may not support a large number of users that use different closed captioning formats and may not scale as new closed captioning formats become popular.

Systems and methods of decoding and converting closed captions are disclosed. For example the described decoding and conversion techniques may be performed by a media server with respect to closed captions received in a stream e.g. a live stream . Closed captions in the stream may be represented in a consumer electronics association CEA 608 format. Upon detecting the closed captions in the stream the media server may decode the closed captions. In one example decoding the closed captions may include simulating or emulating a hardware closed caption decoder using a software decoder executing at the media server. Advantageously the software decoder may simulate multiple closed caption modes and on screen off screen display memories. The decoded closed captions may be converted into a platform independent format e.g. a timed text representation . The data in the platform independent format may then be converted into a desired output format that is compatible with a streaming protocol to be used to deliver the closed captions and video stream to a destination device.

Although one or more embodiments herein are described with reference to closed captioning e.g. text intended for the hearing impaired and therefore including transcriptions of sounds such as wind howls or knocking on door the embodiments may also be used with subtitles e.g. written translations of dialogue being spoken . Moreover embodiments described herein may also be used to process and deliver open captioning i.e. captions that appear hardcoded into a video stream and therefore cannot be selectively disabled like closed captioning .

The media server may include one or more processors and various components that are executable by the processor s . The media server may correspond to or include software application s that perform media serving or processing hardware systems e.g. servers that support or perform media serving and processing or any combination thereof. Thus various operations described with reference to the media server or components thereof may be implemented using hardware software e.g. instructions executable by the processor s or any combination thereof.

The media server may include one or more network interfaces . For example the network interface s may include input interface s and output interface s that are configured to receive data and to send data respectively. In a particular embodiment the network interface s may be wired and or wireless interfaces that enable the media server to communicate data via a network such as the Internet. For example the network interface s may include an Ethernet interface a wireless interface compatible with an Institute of Electrical and Electronics Engineers IEEE 802.11 e.g. Wi Fi protocol or other wired or wireless interfaces.

The network interface s may be configured to receive one or more streams such as an illustrative video stream that includes embedded closed caption CC data. The video stream may correspond to a live stream. The CC data may be a part of video content or may be separate from the video content e.g. the CC data may have a separate program identifier ID or may be part of a separate stream .

The network interface s may be configured to transmit one or more streams such as an illustrative video stream or an illustrative video stream . Each of the video streams may include embedded closed captioning. The network interface s may be configured to transmit one or more video streams e.g. the video stream to the one or more playback devices e.g. a smartphone a tablet computer a laptop computer a desktop computer a set top box a television a portable media player a game console etc. . In the embodiment of the playback devices include a desktop laptop computing device a television TV set top box a smartphone and a tablet computer . The network interface s may also be configured to transmit one or more video streams e.g. the video stream to the one or more other servers e.g. a media server a stream relay server a server of a content distribution network e.g. an edge server etc. . In the embodiment of the other servers include a media server stream relay server and a server of a content distribution network CDN . The video streams may be associated with the same encoding format and transmission protocol or may be associated with different encoding formats and transmission protocols as further described herein. In a particular embodiment generating the video streams and or includes performing video decoding encoding transcoding and or transmuxing operations at the media server e.g. to modify a video encoding format an audio encoding format a bitrate an aspect ratio packaging etc. relative to the incoming video stream . In a transmuxing operation encoded audio and video may be repackaged without modifying the encoded audio and video.

The media server may include various components configured to perform stream processing functions. For example the media server may include one or more video processing components such as encoders decoders and transcoders each of which may be implemented using hardware software or both. To illustrate one or more of the encoder s decoder s and transcoder s may be implemented using Java classes e.g. executable by a Java Virtual Machine JVM C instructions C instructions etc. The decoder s may decode data received by the media server . For example the decoder s may decode received streams e.g. live audio only video only or audio video streams . The encoder s may encode data that is to be transmitted by the media server . The transcoder s may be configured to perform bitrate conversion CODEC conversion frame size conversion etc. Depending on a format of a received stream a playback format supported by a requesting device and or transcoding parameters in use a transcoding operation performed by the transcoder s may trigger a decoding operation by the decoder s and or a re encoding operation by the encoder s . In a particular embodiment parameters used by the transcoder s are stored in one or more transcoding templates at the media server . The encoder s decoder s and transcoder s may thus enable the media server to process data in accordance with multiple coding technologies and protocols.

For example the media server may support video encoding types including but not limited to H.264 On2 VP6 Sorenson Spark Screen video Screen video 2 motion picture experts group MPEG 2 MPEG 2 and MPEG 4 Part 2. The media server may support audio encoding types including but not limited to advanced audio coding AAC AAC low complexity AAC LC AAC high efficiency HE AAC G.711 MPEG Audio Layer 3 MP3 Speex Nellymoser Asao and AC 3.

The media server may support communication e.g. adaptive streaming and non adaptive streaming protocols including but not limited to hypertext transfer protocol HTTP live streaming HLS HTTP dynamic streaming HDS smooth streaming and MPEG dynamic adaptive streaming over HTTP MPEG DASH also known as international organization for standardization ISO international electrotechnical commission IEC 23009 1 . The media server may also support real time messaging protocol RTMP and variants thereof real time streaming protocol RTSP real time transport protocol RTP and MPEG 2 transport stream MPEG TS . Additional audio formats video formats coder decoders CODECs and or protocols may also be supported.

The media server may include one or more data storage devices e.g. random access memory RAM disk based storage etc. . The data storage device s may store stream data e.g. frames of a live video stream files closed caption data images e.g. to be overlaid on top of a video stream and other data as further described herein.

The media server may include various components configured to perform closed caption processing functions. For example the media server may include a closed caption processing module . The closed caption processing module may include one or more closed caption decoders and one or more closed caption format converters each of which may be implemented using hardware software or both. For example one or more of the closed caption decoder s and the closed caption format converter s may be implemented using Java classes e.g. executable by a Java Virtual Machine JVM C instructions C instructions etc.

The closed caption decoder s may extract and decode closed captions embedded in received streams e.g. the video stream and files. The closed captions may be represented in a first format e.g. a first platform specific format . The closed caption format converter s may convert the decoded closed captions into a platform independent format e.g. a timed text representation . An example of a platform independent format for closed captions is further described with reference to . The data in the platform independent format may be converted into a desired output format that is compatible with a streaming protocol to be used to deliver the closed captions and video stream from the media server to a destination device. The closed caption decoder s and closed caption format converter s may thus enable the media server to process closed captions in accordance with multiple closed captioning standards and streaming protocols.

For example closed caption formats may include embedded formats and file formats. The embedded formats may include but are not limited to action message format AMF onTextData events consumer electronics association CEA 608 CEA 708 motion pictures expert group MPEG 4 part 17 3rd generation partnership project 3GPP timed text digital video broadcasting DVB subtitling sistema brasileiro de televisao digital SBTVD digital terrestrial multimedia broadcast DTMB and world system teletext WST . The file formats may include but are not limited to scenarist closed captioning SCC timed text markup language TTML distributed format exchange profile DFXP society of motion picture and television engineers SMPTE timed text SMPTE TT web video text tracks WebVTT SubRip SRT synchronized accessible media interchange SAMI European broadcasting union EBU STL and EBU timed text EBU TT . In a particular embodiment the video stream includes CEA 608 format closed captions that are interpreted by the decoder s and the video streams include AMF onTextData events generated by the converter s based on the output of the decoder s .

During operation the media server may receive the video stream including closed captions. The closed caption decoder s may extract and decode the closed captions from the video stream to generate interpreted closed caption data as further described with reference to . One or more of the closed caption format converter s may receive or access decoded closed captions generated by the closed caption decoder s . The closed caption format converter s may convert the decoded closed captions into the platform independent format. The closed caption data in the platform independent format may be stored in memory at the media server e.g. in the data storage device s . The stored platform independent closed caption data may be used to generate closed captions in one or more output protocols for transmission to one or more destination devices. For example when closed captions are requested by a particular destination device the converter s may generate closed captions in a format that is compatible with the requesting destination device.

Closed caption processing may be performed in response to a request from one of the playback devices for closed captions associated with the video stream which is being generated by the media server based on the video stream and being transmitted by the media server in accordance with a particular outbound streaming protocol. For example a user at a destination device may elect to turn on closed captions for a live video stream that the user is viewing. In a particular embodiment the request may also specify a desired language for the closed captions. The media server may support dynamically switching the closed caption language being provided to the destination device without interruption of the video stream when multiple languages are available in the received stream . In a particular embodiment when multiple closed caption languages are available closed captions in each of the languages are provided to a destination device. Alternately a single e.g. default language may be provided until a different language is requested.

In response to a request for closed captions and without interruption of the video stream the media server may load the closed caption decoder s to extract and decode the closed captions and load the closed caption format converter s to convert interpreted closed captions into the platform independent format and from the platform independent format into an output format compatible with the destination device. When additional destination devices associated with additional output formats request the closed captions additional converters may be loaded. In a particular embodiment closed captions of one or more closed caption feeds may be decoded converted and or transmitted. For example closed captions of a closed caption feed corresponding to the desired language may be transmitted to the destination device. In a particular embodiment a requested closed caption format may be specified by a playback device in a request to turn on closed captions. Thus receiving extracting decoding e.g. interpreting converting and transmitting of closed captions may be performed during a single live video streaming session.

The system of may thus enable dynamic receipt decoding conversion and transmission of closed captioning data. For example the media server may receive a live video stream with embedded closed captions in CEA 608 format and may provide AMF onTextData format closed captions to a destination device. Further closed caption decoding and conversion may be performed in memory using software components without use of dedicated hardware closed caption decoders at the media server . In addition use of a platform independent closed caption format may enable simultaneous conversion into different platform specific output formats.

In selected embodiments the media server may decode input closed captions prior to generating the platform independent timed text data that is used to generate output closed captions. For example the input closed captions may be represented in a first format e.g. CEA 608 . In a particular embodiment the video stream may include one or more closed caption feeds. For example a first closed caption feed may correspond to a first set of closed caption data e.g. in a first language and a second closed caption feed may correspond to a second set of closed caption data e.g. in a second language . Each closed caption feed may be provided to a corresponding closed caption decoder . For example closed caption data may be extracted from a video packet of the video stream . To illustrate the media server may extract the closed caption data from a supplemental enhancement information SEI network abstraction layer NAL unit of a video packet of the video stream .

The closed caption decoder s may extract and decode the closed captions from the one or more closed caption feeds to generate interpreted closed caption data. For example the closed caption decoder s may interpret one or more commands that are included in the extracted closed caption data. In a particular embodiment interpreting the one or more commands includes simulating or emulating using software executing at the media server execution of the one or more commands at a hardware closed caption decoder. Simulating or emulating execution of closed caption commands at the media server may enable the media server to generate closed caption data in multiple output formats. To illustrate the incoming closed caption data may include various commands that would result in a destination device e.g. one of the playback devices drawing characters on screen deleting previously drawn characters on screen etc. The incoming closed captioning data may also include commands that change decoding state variables but do not change what is shown on screen. As described above the media server may not be a destination device. Instead the media server may advantageously convert incoming closed caption data into multiple output formats. To accomplish such format conversion the media server may interpret the commands as if the commands were being executed by a destination device. Interpretation of closed caption commands is further described with reference to .

The first closed caption decoder may include a plurality of character memories e.g. a first character memory and a second character memory . For example each of the character memories and may represent a two dimensional e.g. 16 32 grid for closed captioning characters. Each of the character memories and may be designated as displayable D or non displayable ND . A closed caption decoder may include one or more displayable e.g. on screen character memories and zero or more non displayable e.g. off screen character memories. During interpretation of closed caption commands a displayable character memory may be the target of direct draw commands that when executed at a destination device e.g. by a hardware decoder would modify closed captions being displayed on a screen e.g. television screen . A non displayable character memory may be the target of off screen commands that when executed at a destination device e.g. by a hardware decoder would not modify the closed captions being displayed on screen. Examples of closed caption commands include but are not limited to a command to draw a character move a cursor erase a character clear a portion of a display screen change a text color change a font change a background color or any combination thereof. In CEA 608 the displayable memory and the non displayable memory may be swapped in response to a swap command. To simulate such swapping the decoder may dynamically swap the displayable and non displayable designations as shown at . Alternately the designations may be fixed and the contents of the character memories and may be swapped.

The decoder may also include a non caption processing module and state information . The non caption processing module may be used to process data that is included in the closed captioning feed but is not related to closed captions. For example in CEA 608 a closed caption feed may include non caption data e.g. stream metadata digital video recorder DVR metadata etc. that corresponds to an extended data service XDS mode. The non caption processing module may process the non caption data and provide access to the non caption data via an application programming interface API so that an external device can access the non caption data at the media server . Alternately or in addition the non caption data may be injected into an outgoing stream. For example a program description parental rating etc. may be captured and inserted into a stream as AMF data ID3 tags etc.

The state information may include data that is updated during processing of closed caption data. For example the state information may indicate a caption mode. In a particular embodiment the decoder may support caption modes including but not limited to a pop on mode a roll up mode a direct text mode alternately referred to as a paint on mode and the XDS mode.

In the pop on mode captions may be written to an off screen buffer e.g. a non displayable character memory and may then be swapped onto the screen all at once. Thus from the perspective of a viewer the pop on mode may be used to initiate display of multiple lines of closed captioning at the same time. In the roll up mode a particular number of closed captioning lines e.g. two three or four lines may be available. When a new line is added to the bottom the remaining lines may roll up towards the top line and the top line may be removed. In the direct text paint on mode individual characters may be inserted onto the screen at individual locations e.g. coordinates .

The state information may also indicate a current caption start time a current time and a time of a previously received caption. The times may be determined based on timecodes included in a video stream e.g. the received video stream of . For example if the current caption being processed is ABC the start time may correspond to the time that the A character was received. The current time may correspond to a current stream timecode and may be updated e.g. incremented as additional packets of the video stream are processed. The time of the previous caption may correspond to the timecode for which a caption was last generated by the decoder .

Although not shown in the second decoder may include similar components as the first decoder . In a particular embodiment one or more of the components described as being included within the decoders may be optional.

During operation the closed caption decoder s may receive closed caption feed s . For example the first closed caption decoder may receive the first closed caption feed and the second closed caption decoder may receive the second closed caption feed . The first closed caption decoder may extract closed caption data from the first closed caption feed and the second caption decoder may extract closed caption data from the second closed caption feed .

The first closed caption decoder may interpret commands included in the extracted closed caption data to generate interpreted closed caption data . A particular example of a method of interpreting closed caption commands is further described with reference to . Interpretation of the closed caption commands may include modifying the first character memory modifying the second character memory modifying the state information providing non caption data to the non caption processing module swapping the designation of displayable and non displayable memories etc.

When the first decoder detects an end of caption the first decoder may retrieve characters stored in the first character memory and or the second character memory to generate the interpreted closed caption data that is provided to a format converter e.g. the converters of for conversion to a platform independent format such as the platform independent format described with reference to . An end of caption may be detected in various ways. For example a closed caption command may explicitly designate an end of caption. As another example in the roll up mode a carriage return character may designate an end of caption. As yet another example a change from a first closed captioning mode to a second closed captioning mode may designate an end of caption. The particular event or combination of events that indicates an end of caption may depend on the incoming closed caption format.

In a particular embodiment an end of caption is inferred if a threshold amount of time e.g. idle time has elapsed since the last complete caption was received. For example with respect to the state information if the difference between the current time and the time of the previous caption exceeds a threshold amount of time the first decoder may infer an end of caption and flush the character memories and or to generate the interpreted closed caption data . In a particular embodiment the threshold amount of time is configurable by a user. For example the threshold amount of time may be 250 milliseconds. Use of the threshold idle time may prevent errors in situations where a caption is not finished for a long period of time e.g. because the received video stream is corrupted . Without the use of the threshold idle time captions may appear in an output stream later than expected e.g. a caption was scheduled to appear before a commercial break but appears after the commercial break .

The interpreted closed caption data may be converted into the platform independent format and may be used to generate one or more sets of closed captions in platform specific output formats. For example if a destination device supports AMF onTextData events the platform independent captions may be converted into AMF onTextData events and inserted into an output video stream at the appropriate timecodes. In a particular embodiment the incoming closed captions may be embedded in a live video stream. It should be noted that input formats other than CEA 608 may be supported. For example closed captions in CEA 708 DVB and other formats may also be decoded. In a particular embodiment to support processing of DVB closed captions the decoders and or the media server may include an optical character recognition OCR module that is configured to generate closed caption data by performing an OCR process on stream data e.g. video data . Further output formats other than AMF onTextData may be supported. For example output closed captions may be represented in WebVTT format smooth streaming format and other formats.

While the first decoder processes the first feed the second decoder may simultaneously or concurrently process the second feed . The closed caption decoders of may thus enable simultaneous real time or near real time decoding of multiple closed caption feeds received in a video stream. Further the decoders may provide interpreted closed caption data to format convert s for real time or near real time generation of closed captions in multiple desired output formats for streaming to destination devices via various streaming protocols e.g. HLS RTMP HDS etc. .

In the platform independent closed captioning data includes a first segment of timed text and a second segment of timed text. Each of the segments may include a segment number text a start time an end time a language identifier or any combination thereof. The segments may also include layout information such as a location e.g. coordinates . The segments may further include style information such as a font a font size a style a background color a foreground color or any combination thereof. In selected embodiments one or more of the data items illustrated in may be optional. For example layout information font information style information etc. may be optional. Moreover one or more of the data items may be indicated as applicable to only a subset of closed captioning data. For example in a caption ABC DEF ABC may be blue and bold and DEF may be red and italicized. In addition various formats and values shown in e.g. time being represented in H MM SS.sss format location being represented as an pair the font Proportional Sans Serif the color Black the color White the style Italics etc. are for illustration only. Alternate embodiments may use different formats and values e.g. named definitions or enumerated data types .

In a particular embodiment the segments may also include passed through data . The passed through data may represent closed captioning data or parameters that are left unmodified by a closed caption decoder e.g. the closed caption decoder s of and by a closed caption format converter e.g. the closed caption format converter s of when converting closed captioning data from an input format into the platform independent format. The passed through data may thus represent a tunneling mechanism through a media server e.g. the media server of for platform specific captioning data. A converter e.g. the closed caption format converter s of may use the passed through data when generating closed captioning data compatible with a specific output format or protocol. Alternately the converter may leave the passed through data unmodified so that a media player of a destination device receives and can act on the passed through data . In a particular embodiment the converter may ignore tunnel data.

The method may include receiving at a media server a first video stream including first closed caption data at . Alternately closed caption data may be received separately from video content. The first closed caption data may be represented in a first format e.g. CEA 608 . For example in the media server may receive the video stream where the video stream includes CEA 608 format closed captions.

The method may also include extracting the first closed caption data from the first video stream at and interpreting at least one command included in the extracted closed caption data to generate interpreted closed caption data at . For example in the decoder s may extract closed captions from SEI NAL units of the video stream and may interpret CEA 608 commands to generate interpreted closed caption data. Interpreting the CEA 608 commands may include tracking state information and modifying on screen and off screen character memories as further described with reference to . In a particular embodiment when closed caption data is not embedded within a video stream e.g. closed caption data is stored in a separate file or included in a separate stream extraction may not be performed.

The method may further include detecting an end of caption at . In a particular embodiment the end of caption may be detected based on a threshold amount of time e.g. 250 ms having elapsed since a previously received caption. Alternately the end of caption may be determined based on the interpreted command e.g. an end of caption command a command that switches closed caption modes a command that inserts a carriage return etc. . The method may include in response to detecting the end of caption converting the interpreted closed caption data into timed text data that is represented in a platform independent format at . For example in the converter s may convert the interpreted closed caption data generated by the decoder s into a platform independent format e.g. the format of .

The method may include generating second closed caption data by converting the timed text data from the platform independent format to a second format at and transmitting from the media server to a destination device a second video stream including the second closed caption data at . In a particular embodiment the second format may be AMF onTextData. For example in the converter s may generate closed captions that are embedded into the outgoing video streams and or . The method of may thus enable real time or near real time interpretation and format conversion of closed captions.

The method may include determining whether a stream or file includes additional caption data to process at . When there is additional caption data to process the method may advance to A at and may continue on . Turning to the method may include determining whether a closed caption command indicates a caption mode change at . When a caption mode change is detected the method may include determining whether the mode change indicates an end of caption at . When the mode change indicates an end of caption the method may include retrieving a caption from a display memory e.g. a character memory indicated as displayable at and resetting state information setting a new mode and setting a start of caption timecode at . When the mode change does not indicate an end of caption the method may advance to without retrieving a caption from memory. From the method may advance to B at and may continue on .

When it is determined at that the command does not indicate a caption mode change the method may include determining whether the command is a direct draw command at . For example a direct draw command may be a command that modifies display memory during roll up mode or direct text paint on mode. When the command is a direct draw command the method includes determining whether the command indicates an end of caption at . For example in roll up mode a command including a carriage return may indicate an end of caption. When the command indicates an end of caption the method may include retrieving a caption from display memory and clearing the display memory at and advancing to B at . Certain caption state information e.g. a caption start time and a time of previous caption may also be updated. When the command does not indicate an end of caption the method may include processing the command and updating a display memory at and advancing to B at .

When the command is not a direct draw command the method may include determining whether the command is an off screen command at . For example an off screen command may modify an off screen e.g. non displayable memory during pop on mode. When the command is an off screen command the method may include determining if the command is a swap command at . When the command is a swap command the method may include swapping the on screen and off screen memories and retrieving a caption from the on screen memory at and advancing to B at . For example swapping may be performed as described with reference to the swap of . When the command is not a swap command the method may include processing the command and updating the off screen memory at and advancing to B at .

When the command is not an off screen command the method may include determining whether the command is associated with a non drawing mode e.g. XDS mode at . When the command is associated with a non drawing mode the method may include sending the command to another processor e.g. an XDS processor such as the non caption module of at and advancing to B at . When the command is not associated with a non drawing mode the method may include advancing to B at .

Returning to when it is determined at that there is no caption data be processed or when the method advances to B the method may include determining whether an idle time has exceeded a threshold amount of time at . For example the threshold amount of time may be 250 ms and the idle time may be a difference between a current time and a time that a previous caption was received. When the idle time exceeds the threshold amount of time the method may include retrieving captions from display memory and or off screen memory at and advancing to . When the idle time does not exceed the threshold the method may include determining whether pending captions are stored e.g. cached at the decoder at . When pending captions are stored at the decoder the method may retrieve and send the pending captions e.g. to a format converter at . The method may return to and may repeat while additional packets of the video stream are received at the media server .

It should be noted that the order of steps illustrated in the flowcharts of are to be considered illustrative not limiting. In alternate embodiments the order of steps may be different. Further one or more steps may be optional and or replaced by other steps. In addition one or more steps may be consolidated. For example with respect to some decoders may perform non drawing mode operations first then off screen operations and then direct draw operations. Further some decoders may not perform a swap between on screen and off screen memories and may instead use other methods to move off screen data to on screen memory. For example instead of swapping a decoder may copy data from off screen memory to on screen memory.

Although one or more embodiments described with reference to illustrate processing of closed captions for live streams the media server of may also decode closed captions for non live streams such as video on demand streams. As used herein a live stream may differ from a video on demand VOD stream. A VOD stream originates from or corresponds to content that is available in its entirety at a stream source when a packet of the VOD stream is sent. For example a VOD stream may correspond to a movie or television show that is stored at a storage device e.g. a data storage device of the media server or a data storage device remote to the media server . Closed captions for the VOD stream may be stored as part of the VOD file or separately e.g. in a separate file . A live stream corresponds to content that is not available in its entirety when a packet of the live stream is sent. For example a live stream may be used to transmit audio and or video content corresponding to an event as the event is being captured e.g. in real time or near real time . Examples of such events may include but are not limited to in progress sporting events musical performances video conferences and webcam feeds. It should be noted that a live stream may be delayed with respect to the event being captured e.g. in accordance with government or industry regulations such as delay regulations enforced by the Federal Communications Commission FCC . Thus the closed caption decoders of may be used to interpret closed captions that are included in or provided separately from a VOD stream or a DVR stream.

In accordance with various embodiments of the present disclosure one or more methods functions and modules described herein may be implemented by software programs executable by a computer system. Further implementations can include distributed processing component object distributed processing and or parallel processing.

Particular embodiments can be implemented using a computer system executing a set of instructions that cause the computer system to perform any one or more of the methods or computer based functions disclosed herein. A computer system may include a laptop computer a desktop computer a server computer a mobile phone a tablet computer a set top box a media player one or more other computing devices or any combination thereof. The computer system may be connected e.g. using a network to other computer systems or peripheral devices. For example the computer system or components thereof can include or be included within any one or more of the media server of the desktop laptop computing device of the TV set top box of the smartphone of the tablet computer of the media server stream relay server of a server e.g. edge server of the CDN of or any combination thereof.

In a networked deployment the computer system may operate in the capacity of a server or as a client user computer in a server client user network environment or as a peer computer system in a peer to peer or distributed network environment. The term system can include any collection of systems or sub systems that individually or jointly execute a set or multiple sets of instructions to perform one or more computer functions.

In a particular embodiment the instructions can be embodied in a non transitory computer readable or a processor readable medium. The terms computer readable medium and processor readable medium include a single medium or multiple media such as a centralized or distributed database and or associated caches and servers that store one or more sets of instructions. The terms computer readable medium and processor readable medium also include any medium that is capable of storing a set of instructions for execution by a processor or that cause a computer system to perform any one or more of the methods or operations disclosed herein. For example a computer readable or processor readable medium or storage device may include random access memory RAM flash memory read only memory ROM programmable read only memory PROM erasable programmable read only memory EPROM electrically erasable programmable read only memory EEPROM registers a hard disk a removable disk a disc based memory e.g. compact disc read only memory CD ROM or any other form of storage medium or device.

In a particular embodiment a method includes receiving at a media server a first video stream and first closed caption data associated with the first video stream. The method also includes interpreting at least one command included in the first closed caption data to generate interpreted closed caption data. The method further includes transmitting from the media server to a destination device a second video stream including second closed caption data that is generated based on the interpreted closed caption data.

In another particular embodiment an apparatus includes a processor and a network interface configured to receive at a media server a first video stream and first closed caption data associated with the first video stream. The apparatus also includes a closed caption decoder executable by the processor to interpret at least one command included in the first closed caption data to generate interpreted closed caption data. The network interface is further configured to send from the media server to a destination device a second video stream including second closed caption data that is generated based on the interpreted closed caption data.

In another particular embodiment a computer readable storage device stores instructions that when executed by a computer cause the computer to receive at a media server a first video stream and first closed caption data associated with the first video stream wherein the first closed caption data is represented in a first format. The instructions are also executable by the computer to interpret at least one command included in the first closed caption data to generate interpreted closed caption data. The instructions are further executable by the computer to detect an end of caption in response to determining that a threshold amount of time has elapsed since a previously received caption. The instructions are executable by the computer to in response to detecting the end of caption convert the interpreted closed caption data to timed text data represented in a platform independent format. The instructions are also executable by the computer to generate second closed caption data by converting the timed text data from the platform independent format to a second format and to transmit from the media server to a destination device a second video stream including the second closed caption data.

The illustrations of the embodiments described herein are intended to provide a general understanding of the structure of the various embodiments. The illustrations are not intended to serve as a complete description of all of the elements and features of apparatus and systems that utilize the structures or methods described herein. Many other embodiments may be apparent to those of skill in the art upon reviewing the disclosure. Other embodiments may be utilized and derived from the disclosure such that structural and logical substitutions and changes may be made without departing from the scope of the disclosure. Accordingly the disclosure and the figures are to be regarded as illustrative rather than restrictive.

Although specific embodiments have been illustrated and described herein it should be appreciated that any subsequent arrangement designed to achieve the same or similar purpose may be substituted for the specific embodiments shown. This disclosure is intended to cover any and all subsequent adaptations or variations of various embodiments. Combinations of the above embodiments and other embodiments not specifically described herein will be apparent to those of skill in the art upon reviewing the description.

The Abstract of the Disclosure is submitted with the understanding that it will not be used to interpret or limit the scope or meaning of the claims. In addition in the foregoing Detailed Description various features may be grouped together or described in a single embodiment for the purpose of streamlining the disclosure. This disclosure is not to be interpreted as reflecting an intention that the claimed embodiments require more features than are expressly recited in each claim. Rather as the following claims reflect inventive subject matter may be directed to less than all of the features of any of the disclosed embodiments.

The above disclosed subject matter is to be considered illustrative and not restrictive and the appended claims are intended to cover all such modifications enhancements and other embodiments which fall within the scope of the present disclosure. Thus to the maximum extent allowed by law the scope of the present disclosure is to be determined by the broadest permissible interpretation of the following claims and their equivalents and shall not be restricted or limited by the foregoing detailed description.

