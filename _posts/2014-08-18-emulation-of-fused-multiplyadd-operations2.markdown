---

title: Emulation of fused multiply-add operations
abstract: At least one processor may emulate a fused multiply-add operation for a first operand, a second operand, and a third operand. The at least one processor may determine an intermediate value based at least in part on multiplying the first operand with the second operand, determine at least one of an upper intermediate value or a lower intermediate value, wherein determining the upper intermediate value comprises rounding, towards zero, the intermediate value by a specified number of bits, and wherein determining the lower intermediate value comprises subtracting the intermediate value by the upper intermediate value, determine an upper value and a lower value based at least in part on adding or subtracting the third operand to one of the upper intermediate value or the lower intermediate value, and determine an emulated fused multiply-add result by adding the upper value and the lower value.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09645792&OS=09645792&RS=09645792
owner: QUALCOMM Incorporated
number: 09645792
owner_city: San Diego
owner_country: US
publication_date: 20140818
---
Fused Multiply Add FMA is a required operation in IEEE standard 754 2008 for floating point arithmetic that can speed up and improve the accuracy of computations such as dot product matrix multiplication Newton Raphson and the like. Given operands a b and c the FMA operation operates to multiply operands a and b and to add the product of a and b to c. The sum of the product of a and b and c is rounded to nearest even RTE to produce the final result as follows result RTE a b c .

An unfused multiply add operation may approximate the results of an FMA operation. Given operands a b and c the unfused multiply add operation operates to multiply operands a and b rounds to nearest even the product of a and b and adds the rounded product of a and b to c to produce the final result as follows result RTE RTE a b c .

This disclosure presents techniques for emulating fused multiply add FMA operations via the use of assist instructions. According to the techniques of this disclosure FMA operations may be emulated via assist instructions such that existing hardware for performing unfused multiply add operations may be used to emulate fused multiply add operations without requiring other specialized hardware.

In one example of the disclosure a method for emulating a fused multiply add operation for a first operand a second operand and a third operand may include determining by at least one processor an intermediate value based at least in part on multiplying a first operand with a second operand. The method may further include determining by the at least one processor at least one of an upper intermediate value or a lower intermediate value wherein determining the upper intermediate value comprises rounding towards zero the intermediate value by a specified number of bits and wherein determining the lower intermediate value comprises subtracting the intermediate value by the upper intermediate value. The method may further include determining by the at least one processor an upper value and a lower value based at least in part on adding a third operand to one of the upper intermediate value or the lower intermediate value. The method may further include determining by the at least one processor an emulated fused multiply add result for the first operand the second operand and the third operand by adding the upper value and the lower value.

In another example an apparatus for emulating a fused multiply add operation for a first operand a second operand and a third operand may include a memory configured to store a first operand a second operand and a third operand. The apparatus may further include at least one processor configured to determine an intermediate value based at least in part on multiplying the first operand with the second operand determine at least one of an upper intermediate value or a lower intermediate value wherein determining the upper intermediate value comprises rounding towards zero the intermediate value by a specified number of bits and wherein determining the lower intermediate value comprises subtracting the intermediate value by the upper intermediate value determine an upper value and a lower value based at least in part on adding the third operand to one of the upper intermediate value or the lower intermediate value and determine an emulated fused multiply add result for the first operand the second operand and the third operand by adding the upper value and the lower value.

In another example an apparatus for emulating a fused multiply add operation for a first operand a second operand and a third operand may include means for determining an intermediate value based at least in part on multiplying a first operand with a second operand. The apparatus may further include means for determining by the at least one processor at least one of an upper intermediate value or a lower intermediate value wherein the means for determining the upper intermediate value comprises means for rounding towards zero the intermediate value by a specified number of bits and wherein the means for determining the lower intermediate value comprises means for subtracting the intermediate value by the upper intermediate value. The apparatus may further include means for determining by the at least one processor an upper value and a lower value based at least in part on adding a third operand to one of the upper intermediate value or the lower intermediate value. The apparatus may further include means for determining by the at least one processor an emulated fused multiply add result for the first operand the second operand and the third operand by adding the upper value and the lower value.

In another example of the disclosure a computer readable storage medium may store instructions that when executed cause one or more programmable processors to determine an intermediate value based at least in part on multiplying a first operand with a second operand determine at least one of an upper intermediate value or a lower intermediate value wherein determining the upper intermediate value comprises rounding towards zero the intermediate value by a specified number of bits and wherein determining the lower intermediate value comprises subtracting the intermediate value by the upper intermediate value determine an upper value and a lower value based at least in part on adding the third operand to one of the upper intermediate value or the lower intermediate value and determine an emulated fused multiply add result for the first operand the second operand and the third operand by adding the upper value and the lower value.

The details of one or more examples are set forth in the accompanying drawings and the description below. Other features objects and advantages will be apparent from the description and drawings and from the claims.

In general this disclosure describes techniques for emulating fused multiply add FMA operations via assist instructions such that FMA operations may be performed using existing multiplication and addition hardware without the need for a specialized adder to add the intermediate product of the first and second operands with the third operand. Because the intermediate product of the first and second operands may comprise more bits than either the first operand or the second operand alone instead of using a specialized adder hardware to sum the intermediate product and the third operand a processor may divide the intermediate product into an upper portion and a lower portion so that the third operand may be added to the upper portion or the lower portion via normal adder hardware and the upper portion and the lower portion may be combined to provide an emulated fused multiply add result.

Processors such as a central processing unit CPU a graphics processing unit GPU and the like may perform FMA operations that operate on floating point values. is a conceptual diagram illustrating an example floating point format. As shown in floating point number with value 82.3125 may be represented in 32 bit IEEE 754 2008 floating point format. Such a 32 bit floating format may be considered a single precision format while a 64 bit floating point format may be considered a double precision format and a 16 bit floating point format may be considered a half precision format. A single precision IEEE 754 2008 floating point format i.e. a 32 bit floating point number such as floating point number may include sign bit exponent and mantissa . Sign bit may be a single bit of the 32 bit number exponent may include 8 bits of the 32 bit number and mantissa may include 23 bits of the 32 bit number.

Sign bit may be 1 if the sign of floating point number is negative and may be 0 if the sign of floating point number is positive as one example or vice versa as another example. Exponent may have a bias of 127 such that an unbiased exponent may be calculated from the value of exponent by subtracting 127 from the value of exponent . Mantissa may have its integer bit hidden. For example the floating point value 82.3125 may be equal to 1.0100100101 2. In this example sign bit may be set to 1. Exponent may be 10000101 which is 133 i.e. 6 127 due to a bias of 127 in exponent and mantissa may be 01001001010000000000000because the integer bit of 1.0100100101may be hidden. In this way mantissa may represent a 24 bit value even though mantissa may take up 23 bits of floating point number .

A processor may also keep track of guard bit round bit and sticky bit during floating point computation. Guard bit round bit and sticky bit may be populated while shifting mantissa during normalization and rounding of mantissa . For example if the value of mantissa is 110011 a rightwards shift of 110011 may result in a value of 011001 and guard bit may be set to 1 to store the rightmost bit that was shifted. A subsequent rightwards shift of 011001 may result in a value of 001100 and the 1 in guard bit may be now stored in round bit and guard bit may be set to 1 to store the rightmost bit that was shifted. A subsequent rightwards shift of 001100 may result in a value of 000110 and the 1 stored in round bit may be shifted to being stored in sticky bit the 1 stored in the guard bit may be shifted to being stored in round bit and guard bit may be set 0 to store the rightmost bit that was shifted. A subsequent rightwards shift of 000110 may result in a value of 000011. In this case once sticky bit is set to 1 it may remain 1 regardless of the values of round bit . Therefore the 1 stored in the sticky bit remains in sticky bit the 1 stored in round bit may be dropped the 0 stored in guard bit may be shifted to being stored in round bit and guard bit may be set to 0 to store the rightmost bit that was shifted. As can be seen a 1 may be shifted first into the guard bit then into round bit and then sticky bit . Once a 1 is shifted into sticky bit the 1 may remain in sticky bit regardless of additional shifts while the bit stored in round bit may simply drop off upon another rightward shift. Alternatively sticky bit may be the result of performing an OR operation on all of the bits that have been shifted to the right of round bit .

Floating point number may be rounded in a number of ways for example using one of the rounding modes specified in IEEE754 2008. Rounding floating point number may include rounding mantissa to a specified number of bits. The processor may shift mantissa rightwards and may adjust the value of exponent to compensate for the shift. In round to nearest even rounding mode a number is rounded to the nearest value. If the number falls midway between two equally nearest values it is rounded to the nearest value with an even least significant bit in the round to nearest even rounding mode.

Rounding floating point number towards zero by a specified number of bits may truncate floating point number by dropping guard bit round bit and sticky bit . Furthermore if the floating point number is the intermediate result of an addition the addition of the 24 bit mantissas of two operands may result in a 25 bit mantissa in addition to guard bit round bit and sticky bit . In this case the intermediate result will have to be normalized. To accomplish this the processor may shift mantissa of floating point number rightwards by one bit and may also increment exponent of floating point number by one. Similarly if the floating point number is intermediate result of subtraction the subtraction of 24 bits of mantissa of two operands may result in 24 bits of mantissa with one or more of the operands most significant bits being zero. In this case the intermediate result may be normalized by being shifted left until the most significant bit of the mantissa becomes 1 and the exponent may be correspondingly decremented by the magnitude of the shift.

The result rounding of a floating point number such as floating point number to nearest even may be based at least in part upon the values of guard round and sticky bits respectively. Note that rounding is performed after normalization. In the case of round to nearest even RTE 1 is added to 24 bit mantissa if the guard bit is one and round bit and or sticky bit is one. Furthermore if the least significant bit of the 24 bits mantissa is 1 then 1 is added to LSB of the mantissa if guard bit is set and round and sticky bits are zero. In the case of round to odd RTO 1 is OR ed into the least significant bit of 24 bit mantissa if any one or more bits guard round sticky is 1.

As discussed above an FMA operation may operate on sets of three floating point operands. Table 1 shows the steps of an exemplary FMA operation. Given operands op0 0x76744000 op1 0x2721A200 and op2 0x2088E3EF the processor may determine FMA op1 op2 op3 RTE op1 op2 op3 

As shown in Table 1 above a processor may multiply op0 by op1. In response to multiplying op0 by op1 to result in value p the processor may normalize the resulting value p to result in value n by incrementing the exponent by one. Subsequently the processor may add op2 to n. To add two floating point values the processor may right shift the smaller one of the two floating point values to be added so that their exponents are the same. Because value n has an unbiased exponent of 61 a processor may shift op2 having an unbiased exponent of 62 rightwards by 123 to result in value t having an unbiased exponent of 61 that matches the unbiased exponent of value n. In response to shifting op2 the processor may sum values n and t normalize the sum and may round to nearest even the sum of n and t to the 32 bit result consisting of 24 bit mantissa resulting in a FMA result of 0x5E1A36D1.

As shown in a single precision IEEE 754 2008 floating point value such as floating point number may include a 23 bit mantissa . As discussed above because mantissa may have its integer bit hidden the 23 bit mantissa may actually represent a 24 bit mantissa value. Accounting for the guard round and sticky bits a 27 bit adder may be utilized to add two single precision i.e. 32 bit floating point values including adding two 24 bit mantissa values and two sets of guard round and sticky bits.

Because the processor multiplies a first operand and a second operand together to produce an intermediate value that is added with a third operand to perform the FMA operation multiplying the first operand by the second operand including multiplying two 24 bit mantissa values may produce an intermediate value with a 48 bit mantissa value. Accounting for the 48 bit mantissa value the processor may be required to include a 51 bit adder to add the 48 bit intermediate value with the 24 bit mantissa of the third operand.

Potential disadvantages of incorporating a 51 bit adder to a processor instead of a 27 bit adder may include more space and power requirements for the processor. Furthermore critical path issues may arise due to carrying a sum over such a large number of bits. One approach to approximate the results of an FMA operation without the need for an extra adder to handle the 51 bit addition may be an unfused multiply add operation. As discussed above an unfused multiply add operation may round the 48 bit intermediate value that is the product of the 24 bit mantissas of op0 and op1 back down to 24 bits and adding the 24 bit intermediate value with op2 followed by rounding the result of the sum.

In some examples processors may include hardware that may be able to perform unfused multiply add operations in place of FMA operations. Such hardware may include digital circuits hardware multipliers adders hardware logic and the like to perform unfused multiply add operations. In some examples the hardware may comprise a hardware pipeline. Because in an unfused multiply add operation the intermediate product of first and second operands is rounded down before the intermediate product is summed with the third operand the hardware for performing unfused multiply add operations may not include a specialized adder for adding large floating point values. For example in the example shown in Table 1 the processor may round the 48 bit mantissa resulting from multiplying op0 and op1 down to 24 bits before op3 is added to the product.

However unfused multiply add operations may sometimes provide inaccurate results that are different from the results of FMA operations. In the example shown in Table 1 if the processor instead computes the unfused multiply add result of op1 op2 and op3 the result may instead be 0x5E1A36D0 which is different from the result of the FMA operation of 0x5E1A36D1. For example because in unfused multiply add the mantissa of value n that is the result of normalized value p may be rounded down to the 24 bit mantissa value 0x9A36D0 the mantissa of the sum n t may be 0x9A36D0instead of 0x9A36D0800000. Thus while the guard round and sticky bits may be 1 0 0 as a result of normalizing the mantissa 0x9A36D0800000 this value is rounded to 0x9A36D0 immediately following the multiplication in unfused multiply add and the guard bit is lost. The guard round and sticky bits may be 001 during the add stage of the unfused multiply add. Accordingly the result of rounding to nearest even the mantissa 0x9A36D0with guard round and sticky bit values of 001 may be 0x9A36D0 which results in an unfused multiply add result of 0x5E1A36D0.

In accordance with aspects of the present disclosure a processor may use the same existing hardware used for computing unfused multiply add operations to emulate FMA operations with more accuracy than unfused multiply add operations. By utilizing existing adder hardware to emulate FMA operations a processor may not need to provide an additional 51 bit adder to perform FMA operations for single precision floating point numbers. At least one processor may be configured to determine an intermediate value based at least in part on multiplying a first operand with a second operand determine at least one of an upper intermediate value or a lower intermediate value wherein determining the upper intermediate value comprises rounding towards zero the intermediate value by a specified number of bits and wherein determining the lower intermediate value comprises subtracting the intermediate value by the upper intermediate value determine an upper value and a lower value based at least in part on adding subtracting the appropriate bits of the third operand to one of the upper intermediate value or the lower intermediate value and determine an emulated fused multiply add result for the first operand the second operand and the third operand by adding the upper value and the lower value. Whether the processor adds or subtracts the third operand may be based at least in part on whether the signs of the intermediate product and third operand are the same or different.

As discussed above for single precision floating point values the product of the two 24 bit mantissas of a first operand and a second operand may produce a 48 bit intermediate value and that a 51 bit adder may be required to perform an FMA operation by adding the 48 bit mantissa of the intermediate value and the 24 bit mantissa of the third operand. Instead to emulate an FMA operation without the additional 51 bit adder a processor may divide the mantissa of the intermediate value into an upper portion of bits and a lower portion of bits. For example for the 48 bit mantissa of the intermediate value a processor may divide the 48 bit mantissa of the intermediate value evenly into an upper portion of bits that includes the upper 24 bits 47 24 of the 48 bit mantissa and a lower portion of bits that includes the lower 24 bits 23 0 of the 48 bit mantissa.

The processor may be configured to determine at least one of an upper intermediate value and a lower intermediate value wherein the upper intermediate value s mantissa comprises the upper 24 bits of the 48 bit mantissa and wherein the lower intermediate value comprises the lower 24 bits of the 48 bit mantissa. The processor may determine an upper value and a lower value based at least in part on adding the third operand to one of the upper intermediate value and the lower intermediate value and may further determine an emulated fused multiply add result for the first operand the second operand and the third operand by adding the upper value and the lower value.

The processor may multiply operands and to produce intermediate value which may also be a floating point value. As discussed above because operands and may be single precision floating point values mantissas and may each be 24 bits a 23 bit mantissa plus a hidden integer bit and multiplying operands and may include multiplying mantissas and to result in a 48 bit mantissa in intermediate value . The processor may normalize the mantissa of intermediate value and may conceptually divide the normalized mantissa of intermediate value into upper portion and lower portion . Dividing the normalized mantissa of intermediate value may include dividing the normalized mantissa evenly into two equally sized halves so that upper portion includes the upper 24 bits of the mantissa and lower portion includes the lower 24 bits of the mantissa.

The processor may determine upper intermediate value by rounding to zero intermediate value to half the bits of the mantissa of intermediate value i.e. rounding to zero the 48 bit mantissa to 24 bits . The processor may also determine lower intermediate value by subtracting intermediate value by upper intermediate value . Thus upper intermediate value may include upper portion as its mantissa and lower intermediate value may include lower portion as its mantissa. Note that since intermediate value has been split into upper intermediate value and lower intermediate value the exponent value of lower intermediate value may be smaller by 24 compared to the exponent value of upper intermediate value . Furthermore in some examples the exponent value of lower intermediate value may be beyond the valid range of exponent values for single precision floating point format.

The processor may determine based at least in part on the difference between the exponents of operand and intermediate value whether the processor adds operand to upper intermediate value or lower intermediate value to produce resulting upper value or to produce lower value . In floating point arithmetic two floating point values may be added together without shifting either of the two floating point values if the two floating point values have the same exponent. Thus adding two floating values having different exponents may include shifting the mantissa of one of the floating point values so that the two floating point values have the same exponent and adding the resulting mantissas. As such the processor may determine whether to add operand to upper intermediate value or lower intermediate value based at least in part on the amount of shifting required of the mantissa of operand and or the mantissas of intermediate value so that the exponent of operand matches the exponent of upper intermediate value or lower intermediate value .

In some examples if the processor adds operand to upper intermediate value to result in upper value the processor may set lower value to 0.0. In some other examples if the processor adds operand to lower intermediate value to result in lower value the processor may set upper value to the value of upper intermediate value .

In response to producing upper value and lower value the processor may add upper value to lower value . The processor may normalize the sum of upper value and lower value and may round towards nearest even the normalized sum of upper value and lower value to produce resulting value of the emulated FMA operation on operands and .

As discussed above the processor may determine based at least in part on the difference between the exponents of operand and intermediate value whether the processor adds operand to upper intermediate value or lower intermediate value to produce resulting upper value or lower value . is a block diagram illustrating a situation where the difference between the exponent of intermediate value and the exponent of operand is greater than two and where the signs of intermediate value and operand are the same. In other words the exponents of operand is smaller than the exponents of intermediate value by more than 2 so that mantissa of operand may be right shifted by more than two bits and the exponent of operand accordingly incremented by more than two in order for the exponent of operand to equal the exponent of intermediate value . Alternatively the situations shown in may be expressed as where the unsigned value of operand is less than intermediate value divided by 4.

As shown in examples of operand where the difference between the exponents of intermediate value and the exponents of operand is greater than 2 may include scenarios and . In scenario the exponents of intermediate value and the exponents of operand is greater than 48 so that the processor may shift mantissa rightwards by more than 48 bits to increment the exponent of operand by more than 48 so that the exponent of operand is the same value of the exponent of intermediate value .

In scenario the exponents of intermediate value and the exponents of operand is exactly 48 so that the processor may shift mantissa rightwards by exactly 48 bits to increment the exponent of operand by exactly 48 so that the exponent of operand is the same value of the exponent of intermediate value . In scenario the exponents of intermediate value and the exponents of operand is greater than 24 and less than 48 so that the processor may shift mantissa rightwards by more than 24 bits and less than 48 to increment the exponent of operand by more than 24 and less than 48 so that the exponent of operand is the same value of the exponent of intermediate value .

In scenario the exponents of intermediate value and the exponents of operand is exactly 24 so that the processor may shift mantissa rightwards by exactly 24 bits to increment the exponent of operand by exactly 24 so that the exponent of operand is the same value of the exponent of intermediate value . In scenario the exponents of intermediate value and the exponents of operand is greater than 2 and less than 24 so that the processor may shift mantissa rightwards by more than 2 bits and less than 24 bits to increment the exponent of operand by more than 2 and less than 24 so that the exponent of operand is the same value of the exponent of intermediate value .

In each of scenarios and shown in in order to emulate a fused multiply add operation the processor may determine whether the signs of intermediate value and operand is the same. Alternatively the processor may determine whether the signs of intermediate value and operand are the same. In response to determining that the signs of intermediate value and operand is the same and that either the difference between the exponents of intermediate value and the exponents of operand is greater than 2 or the value of operand is less than intermediate value divided by 4 the processor may set upper value to upper intermediate value . The processor may also add operand to lower intermediate value normalize the sum of operand and lower intermediate value and may round to nearest odd the normalized sum of operand and lower intermediate value to produce lower value . The processor may add upper value and lower value normalize the sum of upper value and lower value and round to nearest even the normalized sum of upper value and lower value to produce an emulated fused multiply add result .

Similarly the processor may emulate a fused multiply subtract operation. A fused multiply subtract operation may be a fused multiply addition where the signs of intermediate value and operand are different. In scenarios and the processor may determine if the signs of intermediate value and operand differ. In response to determining that the signs of intermediate value and operand differ the processor may set upper value to upper intermediate value . The processor may also subtract operand from lower intermediate value normalize the difference of lower intermediate value and operand and may round to odd the normalized difference of lower intermediate value and operand to produce lower value . The processor may subtract lower value from upper value normalize the difference of upper value and lower value and round to nearest even the normalized sum of upper value and lower value to produce an emulated fused multiply subtract result .

As shown in examples of operand where the difference between the exponents of intermediate value and the exponents of operand is less than or equal to 2 may include scenarios and . In scenario the exponents of intermediate value and the exponents of operand is exactly 2 so that the processor may shift mantissa rightwards by 2 bits to increment the exponent of operand by exactly 2 so that the exponent of operand is the same value of the exponent of intermediate value .

In scenario the exponents of intermediate value and the exponents of operand is exactly 1 so that the processor may shift mantissa rightwards by 1 bit to increment the exponent of operand by exactly 1 so that the exponent of operand is the same value of the exponent of intermediate value .

In scenario the exponent of intermediate value is the same as the exponents of operand so that the processor does not need to shift mantissa . In scenario the exponents of operand is larger than the exponent of intermediate value by exactly 1 so that the processor may shift the mantissa of intermediate value rightwards by exactly 1 bit to increment the exponent of intermediate value by exactly 1 so that the exponent of operand is the same value of the exponent of intermediate value . In scenario the exponents of operand is larger than the exponent of intermediate value by 2 so that the processor may shift the mantissa of intermediate value rightwards by exactly 2 bits to increment the exponent of intermediate value by exactly 2 so that the exponent of operand is the same value of the exponent of intermediate value .

In scenario the exponents of operand is greater than the exponent of intermediate value by more than 2 and less than 24 so that the processor may shift the mantissa of intermediate value rightwards by more than 2 and less than 24 bits to increment the exponent of intermediate value by more than 2 and less than 24 so that the exponent of operand is the same value of the exponent of intermediate value . In scenario the exponents of operand is larger than the exponent of intermediate value by exactly 24 so that the processor may shift the mantissa of intermediate value rightwards by exactly 24 bits to increment the exponent of intermediate value by exactly 24 so that the exponent of operand is the same value of the exponent of intermediate value .

In scenario the exponents of operand is greater than the exponent of intermediate value by more 24 so that the processor may shift the mantissa of intermediate value rightwards by more than 24 bits to increment the exponent of intermediate value by more than 24 so that the exponent of operand is the same value of the exponent of intermediate value .

In each of scenarios and shown in in order to emulate a fused multiply add operation the processor may determine whether the signs of intermediate value and operand are the same. The processor may also determine whether the exponent of operand is greater than the same or smaller than by no more two the exponent of intermediate value . In response to determining that the signs of intermediate value and operand is the same and in further response to determining that the exponent of operand is greater than the same or smaller than by no more two the exponent of intermediate value the processor may set lower value to 0.0 If operand and intermediate value have the same sign for scenarios and shown in the processor may set lower value to 0.0 regardless of the exponents of operand and intermediate value . The processor may also add operand to upper intermediate value normalize the sum of operand and upper intermediate value and may round to nearest even the normalized sum of operand and upper intermediate value to produce upper value . In this example the multiply stage of the processor sends upper portion and the MSB the second MSB as well as OR of the remaining bits of lower portion to form guard bit round bit and sticky bit respectively. The processor may add upper value and lower value normalize the sum of upper value and lower value and round to nearest even the normalized sum of upper value and lower value to produce an emulated fused multiply add result .

Similarly in scenarios and shown in the processor may emulate a fused multiply subtract operation. As discussed above a fused multiply subtract operation may be a fused multiply addition where the signs of intermediate value and operand are different. In scenarios and the processor may set lower value to 0.0. In scenarios and GPU may set lower value to the lower 22 bits i.e. 21 0 of lower portion .

For scenarios the processor may add operand and upper intermediate value normalize the sum of upper intermediate value and operand and may round to nearest even the normalized sum of upper intermediate value and operand to produce upper value As mentioned above the rounding to produce upper value may be based at least in part on the guard round and sticky bits resulting from the multiply stage. The processor may add upper value and lower value normalize the sum of upper value and lower value and round to nearest even the normalized sum of upper value and lower value to produce an emulated fused multiply add result .

For scenarios and when signs of operand and intermediate value are different the processor may sum upper intermediate value having guard round and sticky bits from lower intermediate value with operand normalize the sum of operand and upper intermediate value and may round to nearest even the normalized sum of operand and upper intermediate value to produce upper value . The processor may add upper value and lower value normalize the sum of upper value and lower value and round to nearest even the normalized sum of upper value and lower value to produce an emulated fused multiply add result .

In each of the scenarios and shown in the processor may also preserve guard bit round bit and sticky bit from normalizing the product of operands and to produce intermediate value so that the result of rounding to nearest even the normalized sum or difference of operand and upper intermediate value to produce upper value may be based at least in part on one or more of the values of one or more guard bit round bit and sticky bit from normalizing the product of operands and . For example in scenarios and rounding to nearest even the normalized sum or difference of operand and upper intermediate value to produce upper value may be based on guard bit round bit and sticky bit from normalizing the product of operands and acting as the guard bit the round bit and the sticky bit respectively for upper intermediate value .

In scenario the value of guard bit from normalizing the product of operands and may be the round bit and the result from ORing round bit from normalizing the product of operands and with sticky bit from normalizing the product of operands and may act as the sticky bit. LSB of becomes the guard bit. The processor may round to nearest even the normalized sum or difference of operand and upper intermediate value to produce upper value based on these guard round and sticky bits.

In scenarios and the result from ORing guard bit from normalizing the product of operands and with round bit from normalizing the product of operands and with sticky bit from normalizing the product of operands and may act as the sticky bit. The processor may round to nearest even the normalized sum of operand and upper intermediate value to produce upper value based on the guard round and sticky bits.

Table 2 summarizes scenarios for both emulated fused multiply addition and emulated fused multiply subtraction 

As shown in Table 2 the scenarios listed correspond to scenarios shown in . Upper Value may represent upper value and lower value may represent lower value uh may represent upper intermediate value ul may represent lower intermediate value and c may represent operand . The ADD columns may represent the values of upper value and or lower value for fused multiply add and the SUB columns may represent the values of upper value and or lower value for fused multiply subtract. The symbol may represent concatenation of one or more bits and the symbol may represent logical OR of one or more bits. g r and s may represent guard bit round bit and sticky bit respectively resulting from normalizing intermediate value that comes from the most significant bits of lower portion .

For scenario if ul c results in a negative value processor may represent the resulting negative value in twos complement form because a multiplier of the processor may not be able to normalize the resulting negative value. For scenarios and the resulting value of uh ul grs c may be so small that the resulting value may be represented as denormal numbers. For example the resulting value may have an exponent value of 127 that becomes 126 after a rounding step.

For scenarios and uh may be shifted right 2 or more bits. Bit of the shifted value acts as round bit . Bit of the shifted value may be ORed with the result of g r s to form sticky bit . Thus in scenarios and uh g r s may stand for ORing the result of ORing guard bit round bit and sticky bit with the LSB of uh to form sticky bit .

Computing device may include additional modules or units not shown in for purposes of clarity. For example computing device may include a speaker and a microphone neither of which are shown in to effectuate telephonic communications in examples where computing device is a mobile wireless telephone or a speaker where computing device is a media player. Computing device may also include a video camera. Furthermore the various modules and units shown in computing device may not be necessary in every example of computing device . For example user interface and display may be external to computing device in examples where computing device is a desktop computer or other device that is equipped to interface with an external user interface or display.

Examples of user interface include but are not limited to a trackball a mouse a keyboard and other types of input devices. User interface may also be a touch screen and may be incorporated as a part of display . Transceiver module may include circuitry to allow wireless or wired communication between computing device and another device or a network. Transceiver module may include modulators demodulators amplifiers and other such circuitry for wired or wireless communication.

Processor may be a microprocessor such as a central processing unit CPU configured to process instructions of a computer program for execution. Processor may comprise a general purpose or a special purpose processor that controls operation of computing device . A user may provide input to computing device to cause processor to execute one or more software applications. The software applications that execute on processor may include for example an operating system a word processor application an email application a spreadsheet application a media player application a video game application a graphical user interface application or another program. Additionally processor may execute GPU driver for controlling the operation of GPU . The user may provide input to computing device via one or more input devices not shown such as a keyboard a mouse a microphone a touch pad or another input device that is coupled to computing device via user input interface . Processor may be configured to perform any of the techniques disclosed herein for emulating FMA operations.

The software applications that execute on processor may include one or more graphics rendering instructions that instruct processor to cause the rendering of graphics data to display . In some examples the software instructions may conform to a graphics application programming interface API such as e.g. an Open Graphics Library OpenGL API an Open Graphics Library Embedded Systems OpenGL ES API a Direct3D API an X3D API a RenderMan API a WebGL API or any other public or proprietary standard graphics API. In other examples the software instructions may conform to other APIs such as an Open Computing Language OpenCL API. In order to process the graphics rendering instructions processor may issue one or more graphics rendering commands to GPU e.g. through GPU driver to cause GPU to perform some or all of the rendering of the graphics data. In some examples the graphics data to be rendered may include a list of graphics primitives e.g. points lines triangles quadrilaterals triangle strips etc. Typically a mathematics function library may be provided by the compiler for functions such as arcsin arctan power and the like. These functions may be implemented using a rational polynomial. Using FMA instructions for such library function implementations may potentially provide higher precision and execution speed.

GPU may be configured to perform graphics operations to render one or more graphics primitives to display . Thus when one of the software applications executing on processor requires graphics processing processor may provide graphics commands and graphics data to GPU for rendering to display . The graphics data may include e.g. drawing commands state information primitive information texture information etc. GPU may in some instances be built with a highly parallel structure that provides more efficient processing of complex graphic related operations than processor . For example GPU may include a plurality of processing elements such as shader units that are configured to operate on multiple vertices or pixels in a parallel manner. The highly parallel nature of GPU may in some instances allow GPU to draw graphics images e.g. GUIs and two dimensional 2D and or three dimensional 3D graphics scenes onto display more quickly than drawing the scenes directly to display using processor .

GPU may in some instances be integrated into a motherboard of computing device . In other instances GPU may be present on a graphics card that is installed in a port in the motherboard of computing device or may be otherwise incorporated within a peripheral device configured to interoperate with computing device . GPU may include one or more processors such as one or more microprocessors application specific integrated circuits ASICs field programmable gate arrays FPGAs digital signal processors DSPs or other equivalent integrated or discrete logic circuitry. GPU may also include one or more processor cores so that GPU may be referred to as a multi core processor. GPU may be configured to perform any of the techniques disclosed herein for emulating FMA operations.

GPU may be directly coupled to graphics memory . Thus GPU may read data from and write data to graphics memory without using a bus. In other words GPU may process data locally using a local storage instead of off chip memory. Such graphics memory may be referred to as on chip memory. This allows GPU to operate in a more efficient manner by eliminating the need of GPU to read and write data via a bus which may experience heavy bus traffic. In some instances however GPU may not include a separate memory but instead utilize system memory via a bus. Graphics memory may include one or more volatile or non volatile memories or storage devices such as e.g. random access memory RAM static RAM SRAM dynamic RAM DRAM erasable programmable ROM EPROM electrically erasable programmable ROM EEPROM Flash memory a magnetic data media or an optical storage media.

In some examples GPU may store a fully formed image in system memory . Display processor may retrieve the image from system memory and output values that cause the pixels of display to illuminate to display the image. Display may the display of computing device that displays the image content generated by GPU . Display may be a liquid crystal display LCD an organic light emitting diode display OLED a cathode ray tube CRT display a plasma display or another type of display device.

GPU may include MUL ADD unit which may be a digital circuit that is configured to perform unfused multiply add operations. MUL ADD unit may include digital circuits hardware multipliers adders hardware logic and the like to perform floating point arithmetic and logical operations necessary to perform unfused multiply add operations. GPU may utilize MUL ADD unit to emulate FMA operations. GPU may also include additional hardware such as hardware for performing rounding shifting and normalization of floating point numbers to assist MUL ADD unit to emulate FMA operations. In some examples processor may also include MUL ADD unit and additional hardware that together may be configured to emulate FMA operations according to the techniques disclosed herein.

In some examples processor or GPU using at least MUL ADD unit may be configured to emulate an FMA operation for a first operand a second operand and a third operand. To emulate the FMA operation processor or GPU may determine an intermediate value based at least in part on multiplying the first operand with the second operand determine at least one of an upper intermediate value or a lower intermediate value wherein determining the upper intermediate value comprises rounding towards zero the intermediate value by a specified number of bits and wherein determining the lower intermediate value comprises subtracting the intermediate value by the upper intermediate value determine an upper value and a lower value based at least in part on adding the third operand to one of the upper intermediate value or the lower intermediate value and determine an emulated fused multiply add result for the first operand the second operand and the third operand by adding the upper value and the lower value.

In some examples the first operand the second operand and the third operand may comprise 32 bit floating point numbers a mantissa of the intermediate value may comprise 48 bits and the specified number of bits may comprise 24 bits.

The process may further include determining by processor or GPU at least one of an upper intermediate value or a lower intermediate value wherein determining the upper intermediate value may comprise rounding towards zero the intermediate value by a specified number of bits and wherein determining the lower intermediate value may comprise subtracting the intermediate value by the upper intermediate value . In some examples determining at least one of an upper intermediate value or a lower intermediate value may further include determining the upper intermediate value and in response to an exponent of the intermediate value being greater than an exponent of the third operand by more than a specified threshold determining the lower intermediate value.

The process may further include determining by processor or GPU an upper value and a lower value based at least in part on adding a third operand to one of the upper intermediate value or the lower intermediate value .

In some examples determining by processor or GPU an upper value and a lower value based at least in part on adding the third operand to one of the upper intermediate value or the lower intermediate value may include in response to a sign of the intermediate value being the same as a sign of the third operand and in response to an exponent of the intermediate value being greater than an exponent of the third operand by more than a specified threshold setting by processor or GPU the upper value to the upper intermediate value and determining by processor or GPU the lower value by rounding to nearest odd a sum of the lower intermediate value and the third operand. In some examples the specified threshold may be 2 because cancellation may happen when the exponent difference is 1 or 0.

In some examples determining by processor or GPU an upper value and a lower value based at least in part on adding the third operand to one of the upper intermediate value or the lower intermediate value may include in response to a sign of the intermediate value being the same as a sign of the third operand and in response to an exponent of the intermediate value being greater than an exponent of the third operand by fewer than a specified threshold or the exponent of the intermediate value being equal to or less than the exponent of the third operand determining by processor or GPU the upper value by rounding to nearest even a sum of the upper intermediate value and the third operand and setting by processor or GPU the lower value to 0.0. In some examples the specified threshold may be 2. In some examples determining the intermediate value may include normalizing by processor or GPU a product of the first operand and the second operand and rounding to nearest even the sum of the upper intermediate value and the third operand may include rounding by processor or GPU to nearest even the sum of the upper intermediate value and the third operand based at least in part on one or more of a guard bit a round bit and a sticky bit resulting from normalizing the intermediate value.

In some examples determining the upper value and the lower value based at least in part on adding the third operand to one of the upper intermediate value or the lower intermediate value may include in response to a sign of the product being different from a sign of the third operand and in response to an exponent of the intermediate value being greater than an exponent of the third operand by a specified threshold setting by processor or GPU the upper value to the upper intermediate value and determining by processor or GPU the lower value by rounding to nearest odd a sum of the lower intermediate value and the third operand.

The process may further include determining by processor or GPU an emulated fused multiply add result for the first operand the second operand and the third operand by adding the upper value and the lower value .

In some examples determining an emulated fused multiply add result for the first operand the second operand and the third operand may further include normalizing by processor or GPU a sum of the upper value and the lower value and rounding by processor or GPU to even the normalized sum of the upper value and the lower value.

Although the present disclosure discusses techniques for emulating an FMA operation for single precision floating point numbers it should be understood that the techniques discussed herein may be equally applicable to half precision floating point numbers double precision floating point numbers any other sized floating point numbers as well as floating point numbers represented in any other applicable floating point format.

In one or more examples the functions described may be implemented in hardware software firmware or any combination thereof. If implemented in software the functions may be stored on or transmitted over as one or more instructions or code on a computer readable medium. Computer readable media may include computer data storage media or communication media including any medium that facilitates transfer of a computer program from one place to another. Data storage media may be any available media that can be accessed by one or more computers or one or more processors to retrieve instructions code and or data structures for implementation of the techniques described in this disclosure. By way of example and not limitation such computer readable media can comprise RAM ROM EEPROM CD ROM or other optical disk storage magnetic disk storage or other magnetic storage devices or any other medium that can be used to carry or store desired program code in the form of instructions or data structures and that can be accessed by a computer. Also any connection is properly termed a computer readable medium. For example if the software is transmitted from a website server or other remote source using a coaxial cable fiber optic cable twisted pair digital subscriber line DSL or wireless technologies such as infrared radio and microwave then the coaxial cable fiber optic cable twisted pair DSL or wireless technologies such as infrared radio and microwave are included in the definition of medium. Disk and disc as used herein includes compact disc CD laser disc optical disc digital versatile disc DVD floppy disk and Blu ray disc where disks usually reproduce data magnetically while discs reproduce data optically with lasers. Combinations of the above should also be included within the scope of computer readable media.

The code may be executed by one or more processors such as one or more digital signal processors DSPs general purpose microprocessors application specific integrated circuits ASICs field programmable logic arrays FPGAs or other equivalent integrated or discrete logic circuitry. Accordingly the term processor and processing unit as used herein may refer to any of the foregoing structure or any other structure suitable for implementation of the techniques described herein. In addition in some aspects the functionality described herein may be provided within dedicated hardware and or software modules configured for encoding and decoding or incorporated in a combined codec. Also the techniques could be fully implemented in one or more circuits or logic elements.

The techniques of this disclosure may be implemented in a wide variety of devices or apparatuses including a wireless handset an integrated circuit IC or a set of ICs i.e. a chip set . Various components modules or units are described in this disclosure to emphasize functional aspects of devices configured to perform the disclosed techniques but do not necessarily require realization by different hardware units. Rather as described above various units may be combined in a codec hardware unit or provided by a collection of interoperative hardware units including one or more processors as described above in conjunction with suitable software and or firmware.

Various examples have been described. These and other examples are within the scope of the following claims.

