---

title: Augmented image display using a camera and a position and orientation sensor
abstract: An augmented image is generated by capturing a visual image of a site with a digital camera, generating a virtual image or associated information from a digital model of the site, and superimposing the virtual image or associated information on the visual image. To register the digital model with the visual image, a sensor pole is introduced into the field of view, and a combined visual image of the site and an optical target on the sensor pole is captured. The position and orientation of the sensor pole with respect to the site reference frame are measured by sensors mounted on the sensor pole; the position and orientation of the digital camera with respect to the sensor pole are calculated from image analysis of the optical target on the sensor pole; and the position and orientation of the digital camera with respect to the site reference frame are calculated.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09367962&OS=09367962&RS=09367962
owner: Topcon Positioning Systems, Inc.
number: 09367962
owner_city: Livermore
owner_country: US
publication_date: 20141009
---
This application claims the benefit of U.S. Provisional Application No. 61 893 971 filed Oct. 22 2013 which is incorporated herein by reference.

The present invention relates generally to digital imaging and more particularly to generating augmented images.

Digital cameras have largely supplanted film cameras. A digital camera can capture a visual image in near real time represent the visual image as data and store the data in an image data file. The image data file can be processed by software and the visual image can be displayed on a display. Furthermore the visual image can be augmented with a computer generated image or with associated information.

In an embodiment of the invention an augmented image of a site is generated by the following method. Combined visual image data representing a combined visual image captured with a digital camera is received the combined visual image data includes site visual image data representing a visual image of the site and optical target visual image data representing a visual image of at least a portion of an optical target disposed on a reference surface of a position and orientation sensor unit.

In the combined visual image data the optical target visual image data is identified. Based at least in part on the optical target visual image data the position and orientation of the digital camera relative to the position and orientation sensor unit is calculated. Measurements of the position and orientation of the position and orientation sensor unit relative to a site reference frame are then received. Based at least in part on the position and orientation of the digital camera relative to the position and orientation sensor unit and based at least in part on the position and orientation of the position and orientation sensor unit relative to the site reference frame the position and orientation of the digital camera relative to the site reference frame are calculated.

Based at least in part on the position and orientation of the digital camera relative to the site reference frame at least one graphical representation is generated from a digital model of the site. A graphical representation includes a virtual image associated with the site information associated with the site or information associated with the virtual image. An augmented image is generated by displaying at least one graphical representation superimposed on a displayed site visual image displaying at least a portion of the visual image of the site or superimposed on a displayed combined visual image displaying at least a portion of the visual image of the site and displaying at least a portion of the visual image of at least a portion of the optical target.

These and other advantages of the invention will be apparent to those of ordinary skill in the art by reference to the following detailed description and the accompanying drawings.

In an embodiment of the invention a visual image of a site captured by a digital camera is augmented with a virtual image generated from a digital model of the site or with associated information generated from a digital model of the site. The virtual image or associated information is superimposed onto the visual image. Registration of the visual image with the digital model of the site is performed by introducing a position and orientation sensor unit into the field of view of the digital camera the precise position and orientation of the sensor unit relative to the site reference frame are measured the position and orientation of the digital camera relative to the sensor unit are determined the position and orientation of the digital camera relative to the site reference frame are determined and the digital model of the site is mapped to the visual image. To simplify the terminology a digital camera is referred to below as a camera.

An image and data processing system includes a camera a display and a computational system . The camera generates a digital visual image still or video of at least a portion of the site the visual image is displayed on the display . A position and orientation sensor unit is placed within the field of view of the camera . A total station which can be a robotic total station is used in conjunction with some embodiments of the position and orientation sensor unit described below . One or more position and orientation sensor units can be deployed and one or more total stations can be deployed.

In the components of the image and data processing system are shown schematically as three separate components. In practice the components can be integrated in various configurations. In one embodiment the camera display and computational system are integrated into a tablet computer or a laptop computer. The camera is forward facing and the display is backward facing as the camera is pointed towards the site the operator user views a visual image on the display. In another embodiment the camera display and computational system are integrated into a head mounted unit with a heads up display.

Other embodiments can have one or more cameras one or more displays or one or more computational systems in various combinations. For example a computer with limited memory limited data storage capacity and limited processor power mounted on an eyeglass frame can communicate with a tablet or laptop computer with more memory more data storage capacity and more processor power. In general the components can communicate with one another via various interfaces wired or wireless. To simplify the figures the interfaces are not shown. Specific embodiments of the position and orientation sensor unit and specific embodiments of the image and data processing system are described below.

Shown in is an arbitrary point P. The vector from the origin Oto Pis R. The projection of Ronto the X Yplane is the vector r. The meridian angle measured from the Z axis to the vector Ris . The azimuthal angle measured counter clockwise from the X axis to the vector ris .

Refer to View A and View B . The sensor pole has a longitudinal axis which is placed coincident with the Z axis . The sensor pole includes a shaft that extends from the top end C to the bottom end D. The shaft has a body portion A that extends from the top end C to the boundary E and a base portion B that extends from the boundary E to the bottom end D. The value of the length of the shaft measured along the Z axis from the top end C to the bottom end D is a design choice. A typical value based on commercial survey poles is about 2 m considerably smaller values can be used see below .

The body portion A has a cylindrical geometry and the base portion B has a conical geometry. The bottom end D corresponds to a pointed tip. shows an enlarged cross sectional view of the body portion A View Z Z . The cross section is orthogonal to the Z axis and the cross sectional view is projected onto the X Yplane and sighted along the Z axis. The periphery is the circle G with a diameter . The value of the diameter is a design choice typical values based on commercial survey poles range from about 0.03 m to about 0.1 m.

Return to and . The sensor pole includes a sensor head mounted on the top end C of the shaft . The sensor head can include one or more position or orientation sensors. The geometry of the sensor head is a design choice. Examples of suitable sensors include a global navigation satellite system GNSS antenna an optical prism used in conjunction with a total station an electronic magnetic compass an inclination sensor and an inertial measurement unit IMU . Different numbers of each type of sensor can be used and different combinations of types of sensors can be used. The sensors can be mounted on or in the sensor head or elsewhere on the shaft .

The value of the length of the sensor head measured along the Z axis from the top surface A to the bottom surface B is a design choice typical values range from about 0.05 m to about 0.2 m. Reference point RP is a reference point on or in the sensor head . The origin O is placed coincident with the reference point RP. For example the height of the sensor head above the ground can be measured with respect to the reference point RP. The distance between the reference point RP and the bottom surface B measured along the Z axis is referred to as the distance .

In an embodiment the sensor head includes a global navigation satellite system GNSS antenna. Examples of GNSSs include the US Global Positioning System GPS and the Russian GLONASS. The GNSS antenna is operably coupled to a GNSS receiver not shown mounted in the sensor head or elsewhere on the shaft . The position of the sensor head can be measured by the GNSS receiver. The position can be transmitted by the GNSS receiver to a field controller for example via a wired or wireless interface mounted on the shaft . In the embodiment shown the field controller is mounted onto the shaft via a bracket or straps not shown . Further details of the field controller are described below.

If two GNSS antennas are mounted in the sensor head the orientation of the sensor pole can also be measured and transmitted by the GNSS receiver to the field controller a single GNSS receiver with two antenna inputs can be used or two GNSS receivers each with a single antenna input can be used. In some embodiments the field controller includes the computational system . In some embodiments the field controller includes an auxiliary computer that transmits GNSS measurements to a separate computational system . In some embodiments a field controller on the sensor pole is not used and the GNSS receiver can transmit measurements to a computational system not mounted on the sensor pole.

In an embodiment the sensor head includes an optical prism. The optical prism is used in conjunction with the total station to measure the position of the sensor head. If two optical prisms are mounted in the sensor head the orientation of the sensor pole can also be measured. The total station transmits its measurements to the field controller or to another computational system depending on the system configuration.

The sensor pole further includes an optical target disposed on at least a portion of the cylindrical surface F of the shaft . The optical target extends from the top boundary A to the bottom boundary B. The distance between the reference point RP and the top boundary A measured along the Z axis is referred to as the distance . The distance between the top boundary A and the bottom boundary B measured along the Z axis is referred to as the distance . In some embodiments the optical target is fabricated on the cylindrical surface F for example the optical target can be painted on or coated on the cylindrical surface. In some embodiments the optical target is fabricated as a label and attached to the cylindrical surface F for example the label can be attached with glue.

Refer to which shows an azimuthal projection map of an embodiment of the optical target . The vertical axis is the Z axis and the horizontal axis is the azimuthal angle axis . In this example the optical target includes two labels label and label . The label extends along the Z axis from the top boundary A Z Z to the boundary B Z Z and the label extends along the Z axis from the boundary C Z Z to the bottom boundary D Z Z . In an embodiment Z Z Z Z 5 cm and Z Z 50 cm. The optical target comprises characteristic features that are mapped to the Z coordinates. In an embodiment the characteristic features include line segments oriented along the Z axis and line segments oriented orthogonal to the Z axis. The number of line segments the orientation of a line segment the width of a line segment the length of a line segment and the spacing between adjacent line segments can all be varied. In an embodiment the characteristic features are high contrast characteristic features for example black line segments against a white background. The function of the optical target is discussed below.

The sensor pole is advantageous because it is based on commercial survey poles. Other embodiments of a sensor pole can be used. Refer to which shows a perspective view of the sensor pole . The sensor pole has a longitudinal axis which is placed coincident with the Z axis . The sensor pole has the shaft and the sensor head as described above coupled to the top end of the shaft .

Refer to which shows a perspective view of details of the geometry of the shaft . The shaft has a top end A a bottom end B and a surface C about the longitudinal axis. For simplicity the shaft is shown with a solid cross section however the shaft can also be a hollow tube or some portions of the shaft can be solid and some portions of the shaft can be hollow.

A portion of the surface C is referred to as the reference surface . In and cylindrical coordinates are used. The reference surface extends from the top boundary A to the bottom boundary B. The Z coordinate measured along the Z axis from the origin O of the top boundary A is Z and the Z coordinate of the bottom boundary is Z. Refer to the reference point RP which has the Z coordinate Z. The cross section is orthogonal to the Z axis.

Refer to which shows an enlarged view of the cross section projected onto the X Yplane and sighted along the Z axis. The reference point RP has the radius coordinate measured from the Z axis r rand the azimuthal angle coordinate measured counter clockwise from the X axis . The reference surface is defined by the function r Z the function is a design choice. An optical target similar to the optical target described above can be disposed on the reference surface . The optical target comprises a set of characteristic features that is mapped to the reference surface that is the set of characteristic features are mapped to r Z coordinates. The optical target is a design choice. Note If the optical target is a label attached to the sensor pole then the reference surface refers to the surface of the label. In practice the attached label can be considered as part of the sensor pole and the reference surface can refer to a region on the surface of the sensor pole.

For simplicity the shaft is shown as a cylinder. In general the geometry is a design choice. Alternative geometries are shown in the perspective views of . shows the shaft A which has the geometry of a frustrated truncated cone. shows the shaft B which has the geometry of a polygonal prism. shows the shaft C which has the geometry of a frustrated pyramid. In general the geometry does not need to be symmetric about the longitudinal axis and does not need to be constant along the longitudinal axis.

One or more sensor poles can be deployed. A sensor pole can be held by an operator or mounted onto a tripod stand or base positioned on the ground or off the ground if the base portion of the shaft is appropriately configured for example with a sufficiently wide pedestal base the sensor pole can be free standing self supporting . Sensors and optical targets not mounted on a sensor pole can also be used. For example sensors and optical targets can be mounted on a tree building fence or tower.

In an advantageous embodiment a single hand held sensor pole is used. Refer to . An operator holds the sensor pole and can walk about the site which includes various landmarks such as the landmark in this instance a tree and the landmark in this instance a building . The operator wears a head mounted unit which includes a camera display and computational system. Head mounted units include for example units mounted on a helmet units mounted on a pair of goggles and units mounted on an eyeglass frame units mounted on an eyeglass frame as depicted pictorially in are advantageous. The operator can readily hold the sensor pole such that at least a portion of the optical target is within the field of view of the head mounted unit . The head mounted unit can communicate with the field controller over a wired or wireless interface a wireless interface is advantageous.

Refer to . The sensor pole T is similar to the sensor pole shown in except that the sensor pole T has a truncated shaft it does not have a base portion resting on the ground.

One embodiment of the image and data processing system includes the field controller and the head mounted unit . and show a schematic diagram of the field controller and a schematic diagram of the head mounted unit respectively.

Refer to . The field controller includes a computational system a user input device and a display . The computational system includes a computer which includes a processor unit memory and a data storage device .

The computational system further includes a user input device interface which interfaces the computer with the user input device . Examples of the user input device include a keypad a touchscreen and a microphone for voice activated input . Data and user commands can be entered into the computer via the user input device interface .

The computational system further includes a display interface which interfaces the computer with the display . Data and images can be displayed on the display .

The computational system further includes a head mounted unit interface which interfaces the computer with the head mounted unit details of which are described below.

The computational system further includes a communications network interface which interfaces the computer with a communications network . Examples of the communications network include a near field communications network a local area network and a wide area network. Data can be downloaded from and uploaded to a network server via the communications network interface . Data can represent for example computer executable code digital models measurements and visual images. A user can access the computer via a remote access terminal communicating with the communications network . A user can transmit data to and receive data from the computer via the remote access terminal . A user can also transmit user commands to the computer via the remote access terminal .

The computational system further includes a GNSS receiver interface which interfaces the computer with a GNSS receiver . The computer can receive measurements from the GNSS receiver via the GNSS receiver interface .

The computational system further includes a total station interface which interfaces the computer with the total station . The computer can receive measurements from the total station via the total station interface . In some embodiments the total station is a robotic total station that can be controlled by the field controller .

The field controller further includes a sensor measurement system interface which interfaces the computer with an additional sensor measurement system . Examples of the sensor measurement system include inclination sensor measurement systems electronic magnetic compass measurement systems and inertial sensor measurement systems.

Refer to . The head mounted unit includes a computational system a user input device a display and a camera . In an advantageous embodiment the head mounted unit is mounted on an eyeglass frame. The computational system includes a computer which includes a processor unit memory and a data storage device .

The computational system further includes a user input device interface which interfaces the computer with the user input device . Examples of the user input device include a touchpad a touchstrip a pushbutton and a microphone for voice activated input . Data and user commands can be entered into the computer via the user input device interface .

The computational system further includes a display interface which interfaces the computer with the display . Data and images can be displayed on the display . In an embodiment the display is a heads up display.

The computational system further includes a camera interface which interfaces the computer with the camera .

The computational system further includes a field controller interface which interfaces the computer with the field controller described above.

The computational system further includes a communications network interface which interfaces the computer with a communications network . Examples of the communications network include a near field communications network a local area network and a wide area network. Data can be downloaded from and uploaded to a network server communicating with the communications network . Data can represent for example computer executable code digital models measurements and visual images. A user can access the computer via a remote access terminal communicating with the communications network . A user can transmit data to and receive data from the computer via the remote access terminal . A user can also transmit user commands to the computer via the remote access terminal .

The computational system further includes a user input output device interface which interfaces the computer with the user input output device . Examples of the user input output device include a keyboard a mouse a touchscreen a microphone for voice activated input and a local access terminal. Data and user commands can be entered into the computer via the user input output device interface .

The computational system further includes a display interface which interfaces the computer with the display . Data and images can be displayed on the display .

The computational system further includes a camera interface which interfaces the computer with the camera .

The computational system further includes a communications network interface which interfaces the computer with a communications network . Examples of the communications network include a near field communications network a local area network and a wide area network. Data can be downloaded from and uploaded to a network server communicating with the communications network . Data can represent for example computer executable code digital models measurements and visual images. A user can access the computer via a remote access terminal communicating with the communications network . A user can transmit data to and receive data from the computer via the remote access terminal . A user can also transmit user commands to the computer via the remote access terminal . An auxiliary computer can communicate with the computer via the communications network . Examples of the auxiliary computer include the computers in the field controller and head mounted unit described above.

The computational system further includes a GNSS receiver interface which interfaces the computer with a GNSS receiver . The computer can receive measurements from the GNSS receiver via the GNSS receiver interface .

The computational system further includes a total station interface which interfaces the computer with the total station . The computer can receive measurements from the total station via the total station interface . In some embodiments the total station is a robotic total station that can be controlled by the image and data processing system .

The computational system further includes a sensor measurement system interface which interfaces the computer with an additional sensor measurement system . Examples of the sensor measurement system include inclination sensor measurement systems electronic magnetic compass measurement systems and inertial sensor measurement systems.

In the field controller the head mounted unit and the image and data processing system each of the interfaces can operate over different physical media. Examples of physical media include wires coax cables optical fibers free space optics and electromagnetic waves typically in the radiofrequency range and commonly referred to as a wireless interface .

In the field controller the head mounted unit and the image and data processing system a single interface of each type was shown and described. In general one or more of each type of interface can be implemented to interface the computer the computer the computer and the computer respectively with one or more of each type of component. For example some embodiments can have one or more display interfaces to interface the computer with one or more displays one or more communications network interfaces to interface the computer with one or more communications networks and one or more GNSS receiver interfaces to interface the computer with one or more GNSS receivers.

In general not all of the types of interfaces need to be implemented. For example some embodiments can have a GNSS receiver interface but not a total station interface other embodiments can have a total station interface but not a GNSS receiver interface. Furthermore components can communicate with the computer via one or more communications network interfaces such as instrumentation buses Universal Serial Bus Ethernet WiFi and Bluetooth instead of via individual dedicated interfaces.

In the field controller and the image and data processing system the term sensor measurement system is inclusive of a GNSS receiver and a total station. A sensor measurement system receives signals from a sensor and transmits data representing measurements by the sensor. In general a sensor and its associated sensor measurement system are operably coupled. In some instances a sensor and its associated sensor measurement system are integrated into a single unit. For example an inertial measurement unit integrates inertial sensors accelerometers gyros or combinations of accelerometers and gyros with a sensor measurement system including electronics firmware and software . In other instances a sensor and its associated sensor measurement system are separate units. For example an antenna sensor is operably coupled to a GNSS receiver sensor measurement system and an optical prism sensor is operably coupled to a total station sensor measurement system .

In the head mounted unit and the image and data processing system the cameras the camera and the camera respectively can be still frame cameras or video cameras. Furthermore the cameras can be single view cameras or stereoscopic view cameras. A stereoscopic view camera can have a single camera body with two lenses or a stereoscopic view camera can be configured from two separate single view cameras.

In the field controller the head mounted unit and the image and data processing system the computational systems the computational system the computational system and the computational system respectively can be constructed by one skilled in the art from various combinations of hardware firmware and software. Furthermore the computational systems can be constructed by one skilled in the art from various electronic components including one or more general purpose microprocessors one or more digital signal processors one or more application specific integrated circuits ASICs and one or more field programmable gate arrays FPGAs .

In the field controller the head mounted unit and the image and data processing system the processor units the processor unit the processor unit and the processor unit respectively can include one or more processors. In some embodiments the processor unit includes a single processor a central processing unit CPU . In other embodiments the processor unit includes multiple processors. The multiple processors can operate in parallel or functions can be distributed across different processors. For example one processor can serve as a CPU and a second processor a graphics processing unit GPU can be dedicated primarily for graphics processing. Numerical computations can be performed by a CPU a math co processor or a GPU. The CPU typically controls operation of the other processors.

In the field controller the head mounted unit and the image and data processing system the memory the memory the memory and the memory respectively can be functionally and physically partitioned for example into system memory and video memory video memory typically has higher speed than system memory. For example the system memory is operably coupled to the CPU and the video memory is operably coupled to the GPU.

In the field controller the head mounted unit and the image and data processing system the data storage devices the data storage device the data storage device and the data storage device respectively include at least one persistent non transitory tangible computer readable medium such as non volatile semiconductor memory or a magnetic hard drive. The data storage devices can include a removable memory card or a removable flash drive. Data stored on the data storage devices can represent for example computer executable code digital models measurements and visual images.

As is well known a computer such as the computer the computer and the computer operates under control of computer software which defines the overall operation of the computer and applications. The processor unit such as the processor unit the processor unit and the processor unit controls the overall operation of the computer and applications by executing computer program instructions that define the overall operation and applications. The computer program instructions can be stored in a data storage device such as the data storage device the data storage device and the data storage device and loaded into memory such as memory memory and memory when execution of the computer program instructions is desired.

The algorithm shown schematically in described below for example can be defined by computer program instructions stored in the data storage device or in memory or in a combination of the data storage device and memory and controlled by the processor unit executing the computer program instructions. For example the computer program instructions can be implemented as computer executable code programmed by one skilled in the art to perform algorithms. Accordingly by executing the computer program instructions the processor unit executes the algorithm shown schematically in .

In an embodiment a computational system contains a two dimensional 2D or three dimensional 3D digital model of a site or at least a portion of interest of the site . Herein to simplify the terminology a digital model refers to a 2D digital model or a 3D digital model. A digital model of a site for example can be a building information model BIM or a computer aided design CAD model. A digital model of a site for example comprises points that are displayed as 2D primitive objects for example squares or triangles or 3D primitive objects for example tetrahedrons or cubes curves of different thicknesses and text labels. In some embodiments one or more digital models of sites are stored in the computational system assuming it has sufficient data storage capacity memory and processor power . In the embodiment shown in for example one or more digital models of sites are stored in the computational system .

In the embodiment shown in and a digital model of a site is stored in the field controller and portions of the digital model of the site are updated and displayed as needed on the head mounted unit . The field controller can also communicate with the network server to receive different digital models of sites. Note that a digital model of a site can represent the existing site or intended changes to the existing site. For example a digital model of a site can represent buildings fences towers roads bridges and other structures to be added to the site. A digital model of a site can also represent structures to be modified or to be removed from the site. As discussed above a site can refer to an interior space as well as an exterior space. Therefore a digital model of a site can also represent for example additions removals or modifications of structures facilities furniture and equipment within a building.

In an embodiment a graphical representation is generated from a digital model of a site. Herein a graphical representation refers to a virtual image information associated with the site information associated with the virtual image or any combination of a virtual image information associated with the site and information associated with the virtual image. Associated information refers to information associated with the site information associated with the virtual image or a combination of information associated with the site and information associated with the virtual image. An augmented image is generated by superimposing a graphical representation onto a visual image.

In image A is a visual image of a first landmark in this instance a tree image B is a visual image of a second landmark in this instance a portion of a road and image I is a visual image of a portion of the sensor pole . In the display image B displays an augmented image in which a virtual image of a planned building is superimposed onto the display image A. In the display image C displays a further augmented image in which associated information is superimposed onto the display image B. For example the dimension A and the dimension B of the planned building in the image is displayed and the distance C between a reference point on the planned building in the image and a reference point on the landmark in the image B is displayed.

Arbitrary information associated with the various landmarks and planned structures can also be displayed. For example information A can include the geographical coordinates height age and species of the tree. Information B can include the geographical coordinates and designation for example US Highway I 78 of the road. Information C can include the geographical coordinates block and lot number and owner of the planned building.

In an embodiment as shown in the image I the visual image of a portion of the sensor pole is removed from the augmented image to simplify the final augmented image.

In image is a visual image of a landmark in this instance an existing building and image I is a visual image of a portion of the sensor pole . In the display image B displays an augmented image in which a virtual image of a planned wall is superimposed onto the display image A. In the display image C displays a further augmented image in which associated information is superimposed onto the display image B. For example the dimension A the dimension B and the dimension C of the building in the image are displayed the dimension E the dimension F and the dimension G of the planned wall in the image are displayed and the distance D between a reference point on the building in the image and a reference point on the planned wall in the image are displayed. Information A associated with the building and information B associated with the planned wall are also displayed. The information B for example can include the geographical coordinates of the planned wall and the construction material of the planned wall.

In an embodiment as shown in the image I the visual image of a portion of the sensor pole is removed from the augmented image to simplify the final augmented image.

In summary in some applications a model of future construction is superimposed for example as wireframes onto a visual image of the existing site a visual image of an existing site is also referred to as a real world image of the existing site . The model for example can include boundaries of buildings roads with pavements and bridges. There are two major modes of survey work that can benefit from an augmented display. In the first stakeout mode modelled data is staked out on the actual ground site . With an augmented display the operator can readily navigate to the points that need to be staked out. In the second survey mode the operator adds data to the existing digital model of the site by measuring objects on the ground such as existing fire hydrants posts roads and buildings . It is advantageous to view a virtual image of the existing digital model of the site superimposed onto the real world image of the site to quickly find out where the existing digital model of the site should be extended or where more details or information should be included . It is also advantageous to see in real time or near real time how newly measured data appears on the superimposed image.

Besides the digital model of the site itself extra positioning information can be superimposed onto the visual image for example speed and direction of movement of the operator and accuracy of positioning such as horizontal and vertical root mean square values standard deviations and error estimates . In the stakeout mode for example basic navigation cues can be displayed as well for example an arrow to display suggested direction of movement to reach a specific target .

Refer to . shows a perspective view of the Cartesian reference frame fixed to the ground site . The Cartesian reference frame is defined by the origin O X axis Y axis and Z axis . The Cartesian reference frame for example can be a local ENU navigation reference frame in which the X axis points East E the Y axis points North N and the Z axis points Up U . In common practice the X Yplane is tangent to the World Geodetic System 1984 WGS 84 Earth ellipsoid however various other orientations can be used. The Cartesian reference frame is also referred to as the site reference frame the real world reference frame and the object reference frame.

Shown is a representative object O which can for example correspond to any one of the landmarks landmark A landmark E described above with reference to in the site . The camera captures a visual image of the site with the object O. The visual image is represented by data stored in the visual image data file .

Refer to . The visual image data file is inputted into the computational system . The computational system processes the visual image data file and generates the display image on the display . The visual image I is a visual image of the object O .

Shown in is the Cartesian reference frame fixed to the display . The Cartesian reference frame is defined by the origin O X axis and Y axis . The Cartesian reference frame is referred to as the display reference frame and the screen reference frame. The display coordinates can be represented in length units such as mm or in pixel units.

The camera the computational system and the display map the object O in the object reference frame to the visual image I in the display reference frame. The mapping occurs in two stages In the first stage the camera maps the object O to the visual image data file . In the second stage the computational system and the display map the visual image data file to the visual image I.

Refer to . The computational system receives the data file which includes a digital model of the site and the input parameters . The computational system generates a virtual image and associated information which are superimposed on the display image to generate the display image . The display image is an augmented image.

A digital model of a site is represented by a two dimensional 2D or three dimensional 3D data set. A rendering process generates a 2D virtual image from the data set the virtual image is then displayed on a 2D display. The rendering process generates a data set that is used as input to a video processor that generates the virtual image on the display. Since the virtual image is to be superimposed onto a visual image captured by a camera and displayed on the display the virtual image is rendered such that it is corresponds to an image captured with the camera. Associated information can also be extracted from the digital model of the site and displayed on the display.

The data set for the digital model of the site is referenced to the site or object reference frame Cartesian reference frame in and the data set for the virtual image is referenced to the display reference frame Cartesian reference frame in . The computational system generates the data set for the virtual image such that the virtual image is equivalent to a visual image captured by the camera . The input parameters are input parameters to image processing algorithms executed in software by digital libraries to generate the data set for the virtual image. For example DirectX or OpenGL 3D Application Programming Interface API software libraries can be used.

The input parameters include three subsets of parameters the camera position and orientation A the camera calibration parameters B and the display calibration parameters C. Details of each subset are discussed below.

First consider the camera calibration parameters. Refer to . The geometry of the camera is described with respect to a Cartesian reference frame fixed to the camera. shows a perspective view of the Cartesian reference frame defined by the origin O X axis Y axis and Z axis .

Refer to which shows a cross sectional view of the camera the cross section is taken through the X Zplane. The camera includes the lens and the image sensor . The lens has an optical axis placed coincident with the Z axis an optical center RP placed coincident with the origin O and a focal point referenced as RP. The image sensor has a front surface A which is the image plane.

Refer to which shows a front view sighted along the Z axis of the front surface A. The geometry is referenced with respect to the 2D Cartesian coordinate system defined by the origin O X axis and Y axis . The Cartesian axes X Y are parallel to the Cartesian axes X Y respectively. The image sensor is represented by a rectangle with a length L measured along the X axis and a length L measured along the Y axis.

Refer back to . The focal length of the lens is the distance f measured along the optical axis between the optical center and the focal point. The image distance S measured along the optical axis is the distance between the optical center and the image plane. The focal length is a key camera calibration parameter. Other camera calibration parameters can characterize lens defects such as distortion across the image plane. The camera calibration parameters for a specific model of camera can be determined by measuring the camera calibration parameters for multiple samples of the specific model of camera the measurements can be averaged and stored weighted averages can be used .

Next consider the camera position and orientation. Refer back to . shows a perspective view of the Cartesian reference frame . The Cartesian reference frame is defined by the origin O X axis Y axis and Z axis . As discussed above the camera has a lens with an optical axis referenced as the Z axis . The origin Ois fixed to a reference point in the camera in this example the reference point is coincident with the optical center RP of the lens which as discussed above is also coincident with the origin O. The Cartesian axes X Y Z are parallel to the Cartesian axes X Y Z respectively. In the Cartesian reference frame the origin Ois translated with respect to the origin Oby the vector A. The components of the vector R X Y Z specify the position coordinates of the camera more specifically the position coordinates of the optical center RP in the Cartesian reference frame .

The orientation of the camera is specified by three angles and . The angles and specify the orientation of the optical axis Z axis where is the meridian angle measured from the Z axis to the Z axis and the angle is the azimuthal angle measured from the X axis to the projection of the Z axis onto the X Yplane. The camera can also rotate about the Z axis by the angle this angle for example would be varied to set the camera in the landscape mode or the portrait mode .

In an embodiment of the invention the camera position and orientation are determined with a position and orientation sensor. Refer to . Shown is the top portion of the sensor pole . shows a perspective view of the Cartesian reference frame . The Cartesian reference frame is defined by the origin O X axis Y axis and Z axis . The Cartesian axes X Y Z are parallel to the Cartesian axes X Y Z respectively.

Also shown is the Cartesian reference frame fixed to the sensor pole with origin O X axis Y axis and Z axis as described earlier the longitudinal axis of the sensor pole is placed coincident with the Z axis . The origin Ois placed coincident with the origin O which as described earlier is placed coincident with the reference point RP .

In the Cartesian reference frame the origin Ois translated with respect to the origin Oby the vector R. The components of the vector R X Y Z specify the position coordinates of the sensor pole more specifically the position coordinates of the reference point RP in the Cartesian reference frame .

The orientation of the sensor pole is specified by three angles and . The angles and specify the orientation of the longitudinal axis Z axis where is the meridian angle measured from the Z axis to the Z axis and the angle is the azimuthal angle measured from the X axis to the projection of the Z axis onto the X Yplane.

The sensor pole can also rotate about the Z axis by the angle . Let the angle represent the azimuthal angle measured from the X axis to the projection of the X axis onto the X Yplane. Then the angle can be calculated from the angle .

The position and the orientation of the sensor pole with respect to the Cartesian reference frame can be measured as described above. The position coordinates of the reference point RP with respect to the origin Oin the Cartesian reference frame can be measured for example with an antenna mounted in the sensor head the antenna is operably coupled to a GNSS receiver or with an optical prism mounted in the sensor head the optical prism is operably coupled to a total station . The meridian angle and the azimuthal angle can be measured for example with two antennas mounted in the sensor head the two antennas are operably coupled to a GNSS receiver or with two optical prisms mounted in the sensor head the two optical prisms are operably coupled to a total station .

The rotation angle can be measured for example with an electronic magnetic compass mounted in the sensor head the electronic magnetic compass is used to measure the azimuthal angle and is calculated from and . The rotation angle can be also be measured for example with an inertial measurement unit IMU mounted in the sensor head the IMU measures changes in from a known initial value of .

In an embodiment the rotation angle is measured with an electronic magnetic compass fine tuned with an IMU. An electronic magnetic compass has the advantage of determining an absolute reading of however an electronic magnetic compass has a slow response time and is susceptible to magnetic interference for example from passing nearby large machinery or vehicles . An IMU has the advantage of a fast response time and is not susceptible to magnetic interference however since it measure changes in the absolute reading of is subject to drift and bias. Therefore an electronic magnetic compass can be used to establish absolute reference values of at periodic time instants and an IMU can be used to measure changes in between the periodic time instants.

Refer to which shows the sensor pole juxtaposed with the camera . The coordinate systems are then transformed by a rotation about the origin O such that the longitudinal axis Z axis is aligned with the Z axis . The resulting view is shown in .

In the Cartesian reference frame the origin Ois translated with respect to the origin Oby the vector R. The components of the vector R X Y Z specify the position coordinates of the camera more specifically the position coordinates of the optical center RP in the Cartesian reference frame . The position can also be specified by the spherical coordinates R . Here R R is the magnitude of the vector R. The angle is the meridian angle measured from the Z axis to the vector R. The angle is the azimuthal angle measured from the X axis to the projection of the Rvector onto the X Yplane.

The orientation of the camera is specified by three angles and . The angles and specify the orientation of the optical axis Z axis where is the meridian angle measured from the Z axis to the Z axis and the angle is the azimuthal angle measured from the X axis to the projection of the Z axis onto the X Yplane. As discussed above the camera can also rotate about the Z axis by the angle .

In an embodiment the position R and the orientation of the camera are determined by image analysis of a visual image of the optical target captured by the camera .

To illustrate the basic principles first consider a simplified example in which 2 2 and is set such that the X axis is parallel to the Z axis. Refer to which shows a plan view sighted along the Z axis. Shown is the shaft with the optical target . As discussed above with reference to the optical target includes features that are mapped to Z . Therefore the angle can be determined from image analysis of the image of the optical target captured by the camera .

Refer to which shows a cross sectional view of the camera in the X Zplane as previously shown in and a view of the sensor pole orthogonal to the Z axis. The closest distance between the optical target and the optical center RP measured along the optical axis is the distance S this distance is referred to as the object distance. The height of the optical target measured along the Z axis from the top boundary A to the bottom boundary D is the height H.

The lens forms an image of at least a portion of the sensor pole on the image plane front surface A of the image sensor . The corresponding height of the image of the optical target measured along the X axis is the height H. From geometrical optics the relationship between Hand His given by the equations in the thin lens approximation 

As discussed above the visual image captured by the camera is represented by data stored in a data file. The data file is then processed by a computational system and displayed on a display. Refer to . The image is stored as data in the visual image data file and displayed as the image in the display . The corresponding height of the image of the optical target measured along the Y axis is the height H. The ratio of Hto His given by the display scaling factor M 

More complex photogrammetry algorithms can be similarly used for multi element lenses and for scenarios in which the camera orientation is arbitrary. Since the optical target comprises a set of characteristic features the object distance and the camera orientation are not calculated from a single value they can be calculated from a set of values and averaged for greater accuracy weighted averages can be used . For example in the simplified example above only the single object height Hwas used to characterize the height of the optical target in practice the spacings measured along the Z axis between multiple pairs of characteristic features can be used. The optical target can also be configured to provide characteristic features of different resolution. For example a coarse set of characteristic features can allow the algorithms to identify the azimuthal angle to for example the nearest 5 deg increment and a fine set of characteristic features disposed about each coarse characteristic feature can allow the algorithms to identify the azimuthal angle to for example the nearest 0.5 deg increment.

Alternatively a digital model of the optical target can be created and virtual images of the optical target can be generated for different object distances and different camera orientations. The set of virtual images of the optical target can be stored in a data set. By comparing the current visual image of the optical target captured with a camera with the set of virtual images of the optical target the best match virtual image of the optical target can be obtained and the best match values of object distance and camera orientation can be obtained. For the scenarios previously shown in and the range of object distance and camera orientation will be limited by the size and movement of typical operators therefore the search space number of virtual images can be restricted.

Alternatively still calibration can be performed by capturing a set of reference visual images of the optical target over a set of known object distances and a set of known camera orientations. The set of reference visual images can be stored in a data set. By comparing the current visual image of the optical target with the set of reference visual images of the optical target the best match reference visual image of the optical target can be obtained and the best match values of object distance and camera orientation can be obtained. For the scenarios previously shown in and the range of object distance and camera orientation will be limited by the size and movement of typical operators therefore the search space number of reference visual images can be restricted.

In summary by analyzing the visual image of at least a portion of the optical target the position and orientation of the camera relative to the sensor pole can be calculated. Since the position and orientation of the sensor pole relative to the site reference frame are known through measurements the position and orientation of the camera relative to the site reference frame can then be calculated.

In various figures a visual image of a sensor pole or at least a portion of a sensor pole containing at least a portion of an optical target is shown for convenience of explanation. In some embodiments a visual image of a sensor pole is displayed on a display as a visual cue to the operator. Image analysis to identify at least a portion of the optical target and to determine the position and the orientation of the camera relative to the sensor pole however does not require a visual image of at least a portion of the optical target to be actually displayed on a display such image analysis can be performed digitally from the data representing the visual image of at least a portion of the optical target.

Refer to . The computational system receives the combined visual image data file and the input parameters . The computational system processes the combined visual image data file and generates the display image on the display . The visual image I is a visual image of the object O and the visual image I is a visual image of a portion of the sensor pole . The computational system analyzes the visual image I and calculates the camera position and orientation.

The display image can display all of the combined visual image stored in the combined visual image data file or the display image can display a portion of the combined visual image stored in the combined visual image data file . For example the computational system can crop scale and rotate the combined visual image these image processing operations are dependent on the display calibration parameters C. The visual image I can correspond to the visual image of a portion of the sensor pole stored in the combined visual image data file or to a portion of the visual image of a portion of the sensor pole stored in the combined visual image data file .

Refer to . The computational system receives the data file which includes a digital model of the site. Using the previously received camera calibration parameters B the previously received display calibration parameters C and the previously calculated camera position and orientation the computational system generates from the digital model of the site a virtual image and associated information which are superimposed on the display image to generate the display image . The display image is an augmented image.

Refer to . In an embodiment the image I the visual image of a portion of the sensor pole is removed from the augmented image to simplify the final augmented image referenced as the display image .

In step the following initial input is received a data file with a digital model of a site a data file with a digital model of the optical target and a set of camera and display calibration parameters.

The process then passes to step in which combined visual image data representing a combined visual image is received. The combined visual image includes a site visual image of a site and an optical target visual image of at least a portion of an optical target on a sensor pole. The combined visual image data includes site visual image data representing the site visual image and optical target visual image data representing the optical target visual image. The combined visual image data can be received as a combined image data file. The combined image data file can represent a single image captured with a still camera or a single videoframe captured with a videocamera.

The process then passes to step in which analysis of the combined visual image data is performed to identify the optical target visual image data. The characteristic features of the optical target are known from the digital model of the optical target and image recognition algorithms are executed to identify the optical target visual image data. As discussed above the combined visual image does not need to be displayed on a display for the image recognition algorithms to be executed. In some embodiments as a visual cue to the operator the combined visual image is displayed on a display.

The process then passes to step in which the optical target visual image data is analyzed to calculate the position and orientation of the camera relative to the sensor pole.

The process then passes to step in which the position and orientation of the sensor pole relative to the site reference frame are received.

The process then passes to step in which the position and orientation of the camera relative to the site reference frame are calculated from the position and orientation of the camera relative to the sensor pole and the position and orientation of the sensor pole relative to the site reference frame.

The process then passes to step in which a data set for a graphical representation is generated. A graphical representation includes a virtual image associated with the site information associated with the site or information associated with the virtual image.

The process then passes to step in which the graphical representation is superimposed on a displayed site visual image displaying at least a portion of the site visual image or superimposed on a displayed combined visual image displaying at least a portion of the site visual image and at least a portion of the optical target visual image.

As discussed above stereoscopic augmented images can be generated. In an embodiment a stereoscopic camera includes two single view cameras operably coupled such that the position and orientation of the second camera relative to the first camera are known. Since the position and orientation of the second camera relative to the first camera are known the method discussed above for determining the position and orientation of a camera relative to the site reference frame needs to be performed for only one camera for example the first camera. Once the position and orientation of the first camera relative to the site reference frame has been calculated the position and orientation of the second camera relative to the site reference frame can be calculated from the position and orientation of the first camera relative to the site reference frame and from the position and orientation of the second camera relative to the first camera.

A first augmented image can be generated from a combined visual image of the site and at least a portion of the optical target on the sensor pole captured by the first camera and a second augmented image can be generated from a visual image of the site or of the site and at least a portion of the optical target on the sensor pole captured by the second camera.

Coupled to the mounting plate are the camera with the lens and the camera with the lens . Fixed to the camera is the Cartesian reference frame defined by the origin O X axis Y axis and Z axis . The origin Ois placed coincident with the optical center of the lens and the Z axis is placed coincident with the optical axis of the lens . The X Y Z axes are parallel to the X Y Z axes respectively.

Similarly fixed to the camera is the Cartesian reference frame defined by the origin O X axis Y axis and Z axis . The origin Ois placed coincident with the optical center of the lens and the Z axis is placed coincident with the optical axis of the lens . The X Y Z axes are parallel to the X Y Z axes respectively. The spacing between the optical axis of the lens and the optical axis of the lens measured along the Y axis is S. The spacing between the optical axis of each lens and the Y axis measured along the X axis is H. Since the position and orientation of the camera is fixed relative to the camera if the position and orientation of the camera relative to the site reference frame are known then the position and orientation of the camera relative to the site reference frame can be calculated from the known position and orientation of the camera relative to the site reference frame and the known position and orientation of the camera relative to the camera .

The foregoing Detailed Description is to be understood as being in every respect illustrative and exemplary but not restrictive and the scope of the invention disclosed herein is not to be determined from the Detailed Description but rather from the claims as interpreted according to the full breadth permitted by the patent laws. It is to be understood that the embodiments shown and described herein are only illustrative of the principles of the present invention and that various modifications may be implemented by those skilled in the art without departing from the scope and spirit of the invention. Those skilled in the art could implement various other feature combinations without departing from the scope and spirit of the invention.

