---

title: Offering network performance guarantees in multi-tenant datacenters
abstract: Methods of offering network performance guarantees in multi-tenant datacenters are described. In an embodiment, a request for resources received at a datacenter from a tenant comprises a number of virtual machines and a performance requirement, such as a bandwidth requirement, specified by the tenant. A network manager within the datacenter maps the request onto the datacenter topology and allocates virtual machines within the datacenter based on the available slots for virtual machines within the topology and such that the performance requirement is satisfied. Following allocation, stored residual capacity values for elements within the topology are updated according to the new allocation and this updated stored data is used in mapping subsequent requests onto the datacenter. The allocated virtual machines form part of a virtual network within the datacenter which is allocated in response to the request and two virtual network abstractions are described: virtual clusters and virtual oversubscribed clusters.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09519500&OS=09519500&RS=09519500
owner: Microsoft Technology Licensing, LLC
number: 09519500
owner_city: Redmond
owner_country: US
publication_date: 20140210
---
This application is a continuation of and claims priority to U.S. patent application Ser. No. 13 176 901 filed Jul. 6 2011 and entitled OFFERING NETWORK PERFORMANCE GUARANTEES IN MULTI TENANT DATACENTERS . The disclosure of the above identified application is hereby incorporated by reference in its entirety as if set forth herein in full.

A portion of the disclosure of this patent contains material which is subject to copyright protection. The copyright owner has no objection to the facsimile reproduction by anyone of the patent document or the patent disclosure as it appears in the Patent and Trademark Office patent file or records but otherwise reserves all copyright rights whatsoever.

There is an increasing demand for datacenters which offer on demand use of computing resources. Such datacenters share resources between multiple tenants and as a result the performance experienced by one tenant may be influenced by the activities of the other tenants of the datacenter and can be highly variable. This variability can have negative consequences for both tenants and datacenter providers. The tenants may experience unpredictable application performance and increased tenant cost because cost is based on the time spent using the resources . This in turn renders such datacenters unsuitable for some applications which rely on predictable performance and the variability further results in reduced datacenter throughput and hence datacenter efficiency and revenue loss for the provider.

The embodiments described below are not limited to implementations which solve any or all of the disadvantages of known methods of managing datacenters.

The following presents a simplified summary of the disclosure in order to provide a basic understanding to the reader. This summary is not an extensive overview of the disclosure and it does not identify key critical elements of the invention or delineate the scope of the invention. Its sole purpose is to present a selection of concepts disclosed herein in a simplified form as a prelude to the more detailed description that is presented later.

Methods of offering network performance guarantees in multi tenant datacenters are described. In an embodiment a request for resources received at a datacenter from a tenant comprises a number of virtual machines and a performance requirement such as a bandwidth requirement specified by the tenant. A network manager within the datacenter maps the request onto the datacenter topology and allocates virtual machines within the datacenter based on the available slots for virtual machines within the topology and such that the performance requirement is satisfied. Following allocation stored residual capacity values for elements within the topology are updated according to the new allocation and this updated stored data is used in mapping subsequent requests onto the datacenter. The allocated virtual machines form part of a virtual network within the datacenter which is allocated in response to the request and two virtual network abstractions are described virtual clusters and virtual oversubscribed clusters.

Many of the attendant features will be more readily appreciated as the same becomes better understood by reference to the following detailed description considered in connection with the accompanying drawings.

The detailed description provided below in connection with the appended drawings is intended as a description of the present examples and is not intended to represent the only forms in which the present example may be constructed or utilized. The description sets forth the functions of the example and the sequence of steps for constructing and operating the example. However the same or equivalent functions and sequences may be accomplished by different examples.

One solution to the problem of performance variability within a cloud computing environment is to provide each tenant with compute instances connected through a dedicated network having a particular guaranteed bandwidth e.g. 10 Gbs . However this solution can lead to inefficient use of cloud computing resources increased provisioning costs for the datacenter provider as they need to build a bulky datacenter network and significantly increased tenant costs e.g. costs of 5 10 times more than standard instances .

The network manager NM is a logically centralized entity which upon receiving a tenant request performs admission control and maps the request to physical machines to allocate VMs. In performing the allocation the NM takes into account available network resources and resources which have been reserved in response to previous tenant requests across the physical network. In order to do this the NM maintains one or more data structures which contain the following information 

The flow diagram in shows an example method of operation of the NM. The NM receives a tenant request which includes the number of virtual machines being requested N and a requested performance characteristic block in contrast with existing cloud datacenters where tenants simply ask for the amount of computing and storage resources they require. The VMs may have varying amounts of CPU memory and storage resources and this may also be specified by the tenant as part of the request . The performance characteristic which is specified by the tenant rather than being a value set by the datacenter provider may be defined in terms of a bandwidth B which is the maximum rate at which any VM can send or receive. In other examples other performance metrics may be used such as delay or jitter. The NM then maps the request to allocate the requested number of VMs on physical machines block . The stored residual capacity data for elements in the network is used in performing this allocation in block and the form of the stored data and the type of elements to which the data relates may be dependent upon the particular performance metric which is used. For example where tenants specify a bandwidth requirement B the residual capacity data may comprise the residual bandwidth for each link in the network.

Having allocated the VMs to satisfy the tenant request the stored data is updated to take into account the new allocation block e.g. by updating the residual capacity values stored for elements in the network and by removing those slots which have been allocated from the list of empty and hence available slots. The tenant is also informed that the request has been accepted block . If the method fails and is unable to allocate VMs to satisfy the request the request is rejected not shown in .

The NM may be considered to operate in a management plane to perform the VM allocation. In some examples there may also be control in a data plane to enforce the performance characteristic specified by the tenant which may be considered in many circumstances to be a performance limit or cap within the network itself and enforcement modules within the network may be used. In an example the performance characteristic which may also be referred to as a performance metric performance requirement performance parameter or performance criterion may be the bandwidth B which specifies the maximum send receive rate from any VM and in such an example an enforcement module within each machine may perform rate limiting or otherwise enforce the bandwidth available at each VM. This is described in more detail below. In other examples an enforcement module may be located elsewhere within the network e.g. at a rack level or in a switch .

In mapping the request received from a tenant in block or to allocate VMs in block the method not only allocates VMs to tenants but through the reserving of network resources also provides a virtual network connecting a tenant s VM instances. The virtual network isolates tenant performance from the underlying infrastructure and provides the performance guarantees requested by the tenant. Use of virtual networks also enables a datacenter provider to modify their physical topology or even completely alter their infrastructure or physical topology without impacting tenants i.e. as long as the existing virtual networks are mapped onto the new physical topology . The tenants will be unaware of any such changes.

With a virtual cluster a tenant request provides the following topology each tenant machine is connected to a virtual switch by a bidirectional link of capacity B resulting in a one level tree topology. The virtual switch has a bandwidth of N B. This means that the virtual network has no oversubscription and the maximum rate at which the tenant VMs can exchange data is N B. However this data rate is only feasible if the communication matrix for the tenant application ensures that each VM sends and receives at rate B. Alternatively if all N tenant VMs were to send data to a single destination VM the data rate achieved will be limited to B.

With a virtual oversubscribed cluster a tenant request includes additional information. Tenant machines are arranged in groups of size S as indicated by the dotted outline in resulting in P N S groups. In the following description all the groups are shown as being of the same size i.e. comprising the same number of virtual machines however in some examples groups within a virtual oversubscribed cluster may be of different sizes and in such an example the request may include this information e.g. . The VMs in a group are connected by bidirectional links of capacity B to a virtual group switch which has a bandwidth of S B . The group switches are further connected using a link of capacity B S B O to a virtual root switch which has a bandwidth of N B O . The resulting topology has no oversubscription for intra group communication through the group switches . However inter group communication has an oversubscription factor O i.e. the aggregate bandwidth at the VMs is O times greater than the bandwidth at the root switch . The oversubscription factor O neither depends upon nor requires physical topology oversubscription and in a similar manner to the size S the oversubscription factor O may be different for different groups.

The maximum data rate with the virtual oversubscribed cluster topology is still N B. Yet the localized nature of the tenant s bandwidth demands resulting from this abstraction allows the provider to fit more tenants on the physical network. Compared to virtual cluster this virtual oversubscribed cluster VOC abstraction does not offer as dense a connectivity but has the potential to significantly limit tenant costs. Hence in effect by incentivizing tenants to expose the flexibility of their communication demands the VOC abstraction achieves better multiplexing which benefits both tenants by reducing costs and providers by improving provider flexibility .

The term virtual switch is used for the switches in both virtual network abstractions because one or more switches in the physical topology may form the virtual switch with the switching functionality of the virtual switch being distributed between these physical switches. This is shown in the topology of where the set of switches and links form a distributed virtual switch for the tenant with the three VMs .

In order to allocate a virtual cluster of VMs in block an allocation algorithm is used and for the purposes of the following description bandwidth B is used as the performance characteristic which is specified within a tenant request. This allocation algorithm identifies which allocations of VMs are valid where validity is defined in terms of two constraints which may also be referred to as validity conditions there must be an available slot for a VM on the relevant machine and the tenant s bandwidth characteristic should be met on all links in the tenant tree or more generally the tenant s performance characteristic must be met at all entities in the tenant tree . Given that as shown in the first example in the tenant s virtual switch has a bandwidth of N B one option would be to ensure that there was N B residual capacity on each link in the tree however this is inefficient as can be described with reference to . In the diagram in a tenant tree is shown by the dashed lines between switches and racks and comprises three VMs . If a particular link in the tree is considered removing this link from the tree leads to two components if the first one contains m VMs the other by definition contains N m VMs. The virtual topology dictates that a single VM cannot send or receive at rate more than B. Hence traffic between these two components is limited to min m N m B or in this particular example to B as m 2 and N 3 . This is the bandwidth required for the tenant on this link and is less than N B .

Each physical machine in the datacenter has K slots where VMs can be placed while each link has capacity C. Further kis the number of empty slots in the sub tree v e.g. in a machine at level 0 k 0 K while Ris the residual bandwidth for link l. Starting with a machine as the base case which may be considered to be one of the leaves in the tree level 0 the number of VMs for request r that can be allocated to a machine v with outbound link l is given by the set M 0 min s.t. min 1 To explain this constraint a scenario may be considered where m 

As shown in and in the pseudo code example below which provides an example implementation of the method of given the number of VMs that can be placed at each level of the datacenter hierarchy the algorithm greedily tries to allocate the tenant VMs to the lowest level possible. To achieve this the method traverses the topology tree starting at the leaves physical machines at level 0 block and determines if all N VMs can fit in any sub tree at this level blocks and lines . This involves determining the number Mof VMs that can fit in each sub tree v at that level level 0 block lines and then comparing the computed value to the required number of VMs N block line . Equation 1 above defines the value of Min this situation. If none of the sub trees at this level i.e. none of the physical machines for level 0 can accommodate all of the required VMs No in block the algorithm moves to the next level block and repeats the process blocks and e.g. to determine if any of the racks at level 1 can accommodate all of the required VMs. Once the algorithm determines a sub tree that can accommodate the VMs Yes in block line it allocates the empty slots on physical machines in the sub tree to the tenant block line using the Alloc function set out in lines . If the algorithm reaches the top level without finding any sub tree that can accommodate all of the required VMs line then the tenant s request is rejected line . As described above and shown in once the assignment of VMs has been done the bandwidth needed for the request is effectively reserved for the tenant by updating the residual bandwidth for each link l as R R B block .

The fact that the physical datacenter network topologies are typically oversubscribed i.e. they have less bandwidth at root than edges guides the algorithm s optimization heuristic. To maximize the possibility of accepting future tenant requests the algorithm allocates a request while minimizing the bandwidth reserved at higher levels of the topology. This is achieved by packing the tenant VMs in the smallest sub tree.

Further when multiple sub trees are available at the same level of hierarchy an implementation of the algorithm chooses the sub tree with the least amount of residual bandwidth on the edge connecting the sub tree to the rest of the topology. This preserves empty VM slots in other sub trees that have greater outbound bandwidth available and hence are better positioned to accommodate future tenants.

The allocation algorithm described above with reference to and the example pseudo code searches for suitable sub trees by traversing the network from bottom level 0 to top. In other examples different heuristics may be used for example the algorithm may be modified to search from the top to the bottom. Furthermore the algorithm may be modified to pack VMs into the datacenter topology in a different way. In an example a utility function that maximizes an aspect of the network performance layout or a combination of such aspects may be used e.g. a packing function which aims to reduce fragmentation across the network a balancing function which balances CPU and network utilization across the network etc. In a further example a packing function may be used which strives for data locality i.e. putting VMs close to where the tenant data is .

If a request with three groups is considered by way of example as with the virtual cluster any physical link in the tenant tree divides the tree into two components. If gdenotes the number of VMs of group i that are in the first component this implies that the rest are in the second component S g . The bandwidth required by the request on the link between the two components is the sum of the bandwidth required by individual groups. shows a schematic diagram which illustrates the bandwidth required by group VMs on a link dividing the tenant tree into two components . In the first component the group traffic on the link in question comprises the intra group traffic to group VMs in the second component and inter group traffic to group and VMs in the second component.

In the first component group VMs cannot send or receive traffic at a rate more than g B. In the second component group VMs cannot receive or send at a rate more than S g B while the rate for VMs of other groups cannot exceed the inter group bandwidth B . The rate D of these other VMs is further limited by the aggregate bandwidth of the group and members in the second component i.e. S g S g B. Hence as shown in the total bandwidth needed by group of request r on link is B min g B S g B D where D min B S g S g B . Finally the total bandwidth required on the link is the sum across all three groups i.e. Generalizing the analysis above the bandwidth required for group i on link l is given by min min 

The bandwidth to be reserved on link l for request r is the sum across all the groups i.e. B B. For the allocation to be valid link must have enough residual bandwidth to satisfy B. Hence B Ris the validity condition.

Allocating an oversubscribed cluster involves allocating a sequence of virtual clusters for individual groups as shown in . This allows reuse of the cluster allocation algorithm as shown by the commonality between . Hence the allocation for a request r proceeds one group at a time. The flow diagram shown in starts with group however for the purposes of generality of the following description it is assumed that groups to i have already been allocated through iterations of the method of and therefore the next allocation to be performed is for the VMs of group i. As with the cluster allocation algorithm described above the number of VMs for this group that can be assigned to each sub tree is determined block and there are a number of constraints which set this value. If a sub tree with outbound link l already containing gmembers of group j j i is considered using the analysis above the conditional bandwidth needed for the jgroup of request r on link l is 1 min min where This bandwidth is conditional since groups i . . . P remain to be allocated. A conservative assumption is to assume that all subsequent groups will be allocated outside the sub tree and as a result link l will have to accommodate the resulting inter group traffic. Hence if gmembers of group i were to be allocated inside the sub tree the bandwidth required by groups i on l is at most ECB i . Consequently the number of VMs for group i that can be allocated to sub tree v designated by the set M is 0 min s.t. 2 Consequently when determining the number of VMs that can fit in each sub tree equation 1 is used for virtual cluster allocation as in and equation 2 is used for allocating a group within the virtual oversubscribed cluster allocation algorithm as in . With this change in definition the example pseudo code given above may also be used in allocation of groups within the virtual oversubscribed cluster allocation algorithm.

Given the number of VMs that can be placed in sub trees as determined in block using equation 2 above at each level of the datacenter hierarchy the allocation algorithm proceeds to allocate VMs for individual groups using the algorithm as described above. If any sub tree at the particular level can accommodate all S VMs in the group Yes in block these are allocated in block . However if there are no sub trees at the particular level that can accommodate all the VMs of the group No in block the analysis blocks and is repeated at the next level block . Once a group has been allocated the algorithm moves to the next group block if the last group has not been allocated No in block and a request is accepted and an acceptance message sent to the tenant in block if all groups are successfully allocated. If it is not possible to allocate all the groups the request is rejected.

The constraints or validity conditions used in the methods described above are based on the number of available slots and the available capacity e.g. bandwidth or other performance metric. These conditions enable the validity checks to be performed very quickly and on a per tenant basis when allocating VMs without requiring the whole traffic matrix and so can be implemented at a large enough scale to be applicable to datacenters.

Irrespective of whether one or both of the abstractions described above are used in a datacenter implementation tenants are provided with simple and intuitive interfaces so that they can express performance guarantees e.g. bandwidth guarantees .

Using the methods described above for either VC or VOC or a combination of the two the NM ensures that the physical links connecting a tenant s VMs have sufficient bandwidth. As mentioned above a datacenter may also include mechanisms to enforce tenant virtual networks i.e. to ensure that tenants do not use more capacity e.g. bandwidth than has been reserved for them. In an example implementation individual VMs are rate limited using explicit bandwidth reservations at switches however the limited number of reservation classes on existing commodity switches implies that such a solution does not currently scale well with the number of tenants.

In another example endhost based rate enforcement may be used in which traffic to and from each VM is limited in accordance with the tenant specified performance characteristic e.g. bandwidth characteristic . For each VM on a physical machine an enforcement module resides in the OS operating system hypervisor. Given a tenant s virtual topology and the tenant traffic rate it is feasible to calculate the rate at which pairs of VMs should be communicating. To achieve this the enforcement module for a VM measures the traffic rate to other VMs block as shown in which shows two example methods in a single flow diagram. In the first example method these traffic measurements from all VMs for a tenant are periodically sent to a tenant VM designated as the controller VM block or to the NM or another entity within the datacenter. The enforcement module at the controller VM or other entity where appropriate then calculates the max min fair share for traffic between the VMs block . These rates are communicated back to other tenant VMs block where the enforcement module for the VM uses per destination VM rate limiters to enforce the rates block . The use of a controller VM or other central controller entity in this way reduces control traffic in the network. Alternatively as shown in the second method in the enforcement modules for an endhost may use a gossip protocol or any suitable protocol to exchange their traffic rates block so that rate limits can be computed locally block and then enforced as before block .

In each of these methods the enforcement modules are effectively achieving distributed rate limits for instance with a cluster request the aggregate rate at which the tenant s VMs can source traffic to a destination VM cannot exceed B. The knowledge of the virtual topology makes it easier to determine the traffic bottlenecks and furthermore as the computation is tenant specific the scale of the problem is reduced and this enables rates for each virtual network to be computed independently. Simulation results show that such an implementation scales well and imposes low communication overhead. The rate computation overhead depends on the tenant s communication pattern. Even for a tenant with 1000 VMs two orders of magnitude more than mean tenant size today and a worst case scenario where all VMs communicate with all other VMs the simulation results showed that the computation takes 395 ms at the 99th percentile. With a typical communication pattern 99th percentile computation time is 84 ms. To balance the trade off between accuracy and responsiveness of enforcement and the communication overhead an example implementation may recompute rates periodically e.g. every 2 seconds . For a tenant with 1000 VMs and worst case all to all communication between the VMs the controller traffic is 12 Mbps 1 Mbps with a typical communication pattern .

In some multi tenant datacenter implementations some tenants may have guaranteed resources provided through allocation of a virtual network and other tenants may not have virtual networks i.e. they may have dedicated VMs but without dedicated resources interconnecting those VMs . These tenants without virtual networks may for example be legacy tenants or a provider may offer a different cost structure for provision of VMs with or without guaranteed resources. In such an instance the datacenter may be controlled such that the network traffic for tenants without guaranteed resources gets a share which may be a fair share of the residual link bandwidth in the physical network. This may be achieved using two level priorities and as existing commodity switches offer priority forwarding switch support for this may be used to provide these two level priorities. Traffic from tenants with a virtual network may be marked as and treated as high priority while other traffic is low priority. This prioritization when combined with the mechanisms above ensures that tenants with virtual networks get the virtual topology and the bandwidth they ask for while other tenants get their fair share of the residual network capacity. In a further optimization of performance for those tenants without virtual networks e.g. so that the performance they experience is not too poor a datacenter provider may limit the fraction of network capacity used for virtual networks e.g. such that even without any tenants with a virtual network the stored value of the residual capacity of a link R is less than the actual capacity of that link .

The allocation algorithms described above assume that the traffic between a given tenant s VMs is routed along a tree. This assumption holds trivially for simple tree physical topologies with a single path between any pair of machines. However datacenters often have richer networks. For instance a commonly used topology involves multiple layer 2 L2 domains inter connected using a couple of layers of routers. The spanning tree protocol ensures that traffic between machines within the same L2 domain is forwarded along a spanning tree. The IP routers are connected with a mesh of links that are load balanced using Equal Cost Multi Path forwarding ECMP . Given the amount of multiplexing over the mesh of links these links can be considered as a single aggregate link for bandwidth reservations. Hence in such topologies with limited path diversity the physical routing paths themselves form a tree and the assumption still holds. The NM only needs to infer this tree to determine the routing tree for any given tenant. This can be achieved using SNMP queries of the 802.1D Bridge MIB Management Information Base on switches e.g. as supported by products like NetView and OpenView or through active probing.

Data intensive workloads in today s datacenters have motivated even richer fat tree topologies that offer multiple paths between physical machines. Simple hash based or randomized techniques like ECMP and Valiant Load Balancing VLB are used to spread traffic across paths. Hence tenant traffic would not be routed along a tree and additional mechanisms may be needed to satisfy the assumption. For the purpose of bandwidth reservations multiple physical links can be treated as a single aggregate link if traffic is distributed evenly across them. Today s ECMP and VLB implementations realize hash based per flow splitting of traffic across multiple links. Variations in flow length and hash collisions can result in a non uniform distribution of traffic across the links. To achieve a uniform distribution a centralized controller may be used to reassign flows in case of uneven load or distribute traffic across links on a per packet basis e.g. in a round robin fashion.

Alternatively the NM may control datacenter routing to actively build routes between tenant VMs and backwards compatible techniques have been proposed to achieve this. With both SecondNet as described in SecondNet A Data Center Network Virtualization Architecture with Bandwidth Guarantees by C. Guo et al and published in Proc. of ACM CoNext 2010 and SPAIN as described in SPAIN COTS Data Center Ethernet for Multipathing over Arbitrary Topologies by J. Mudigonda et al and published in Proc. of NSDI 2010 route computation is moved to a centralized component that directly sends routing paths to endhosts. The NM described above can adopt such an approach to build tenant specific routing trees on top of rich physical topologies. The fact that there are many VMs per physical machine and many machines per rack implies that multiple paths offered by the physical topology can still be utilized.

Failures of physical links and switches in the datacenter will impact the virtual topology for tenants whose routing tree includes the failed element. With today s setup within datacenters providers are not held responsible for physical failures and tenants end up paying for them. However the systems and algorithms described above can be extended to determine the tenant VMs that need to be migrated and reallocate them so as to satisfy the tenant s virtual topology. In such an implementation the NM stores allocation information for existing tenants in addition to the datacenter network topology the residual capacity for elements in the network and details of the slots on each physical machine which are available for allocation of a VM and it is this allocation information which is used when a failure occurs. For instance with a virtual cluster request a failed edge i.e. link will divide a tenant s routing tree into two components. If the NM cannot find alternate links with sufficient capacity to connect the two components it will reallocate the VMs present in the smaller component using the algorithms described above . Further such an extended allocation scheme can also accommodate tenant contraction and expansion wherein tenants want to decrease or increase the size of their virtual topology in an incremental fashion.

Computing based device comprises one or more processors which may be microprocessors controllers or any other suitable type of processors for processing computer executable instructions to control the operation of the device in order to manage a datacenter. In particular to implement the NM functionality or to act as an enforcement module at an endhost. In some examples for example where a system on a chip architecture is used the processors may include one or more fixed function blocks also referred to as accelerators which implement a part of the methods e.g. part of the VM allocation method in hardware rather than software or firmware e.g. the computation of M. Platform software comprising an operating system or any other suitable platform software may be provided at the computing based device to enable application software which may include a network manager or enforcement module to be executed on the device.

The computer executable instructions may be provided using any computer readable media that is accessible by computing based device . Computer readable media may include for example computer storage media such as memory and communications media. Computer storage media such as memory includes volatile and non volatile removable and non removable media implemented in any method or technology for storage of information such as computer readable instructions data structures program modules or other data. Computer storage media includes but is not limited to RAM ROM EPROM EEPROM flash memory or other memory technology CD ROM digital versatile disks DVD or other optical storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other non transmission medium that can be used to store information for access by a computing device. In contrast communication media may embody computer readable instructions data structures program modules or other data in a modulated data signal such as a carrier wave or other transport mechanism. As defined herein computer storage media does not include communication media. Although the computer storage media memory is shown within the computing based device it will be appreciated that the storage may be distributed or located remotely and accessed via a network or other communication link e.g. using communication interface .

The memory may also comprise a data store for storing data which is used in executing the application software . Where the computing based device acts as a network manager the data store may be arranged to store the datacenter network topology the residual capacity data and a record of empty i.e. available VM slots within the datacenter . In some examples as described above the data store may also be arranged to store allocation information for use in the event of failure network reconfiguration or release of resources by a tenant.

The communication interface enables communication between the computing based device and other entities within the datacenter e.g. other endhosts or tenants e.g. to receive the requests in block or . This communication may be over the datacenter network e.g. communication between entities within the datacenter or another network e.g. communication with tenants .

The computing based device may also comprises an input output controller arranged to output display information to a display device which may be separate from or integral to the computing based device . The display information may provide a graphical user interface to the datacenter provider showing the virtual networks allocated within the datacenter. The input output controller may also be arranged to receive and process input from one or more devices such as a user input device e.g. a mouse or a keyboard . The input output controller may also output data to devices other than the display device e.g. a locally connected printing device not shown in .

The virtual network abstractions described above allow tenants to expose their network requirements. This enables a symbiotic relationship between tenants and providers tenants get a predictable environment in shared settings while the provider can efficiently match tenant demands to the underlying infrastructure without muddling their interface. Simulations show that the abstractions are practical can be efficiently implemented and provide significant benefits. The virtual network abstractions can provide a succinct means of information exchange between tenants and providers.

Another aspect of virtual networks is pricing e.g. cloud pricing. Providing tenants with virtual networks means that it is possible to charge for network bandwidth. This represents a fairer charging model with a tenant paying more for a virtual cluster with 500 Mbps than one with 100 Mbps currently a tenant is charged based only on the number of VMs requested . In an example charging model apart from paying for VM occupancy k tenants also pay a bandwidth charge of 

In existing charging models tenants can implicitly be charged for their internal traffic. However by offering bounded network resources to tenants this provides explicit and fairer bandwidth charging. More generally charging tenants based on the characteristics of their virtual networks eliminates hidden costs and removes a key hindrance to cloud adoption. This in effect could pave the way for multi tenant datacenters where tenants can pick the trade off between the performance of their applications and their cost.

Although the present examples are described and illustrated herein as being implemented in a system which uses a specific metric inter VM network bandwidth the system described is provided as an example and not a limitation. As those skilled in the art will appreciate the present examples are suitable for application in a variety of different types of systems and other performance metrics or non performance metrics like reliability may be used. Examples of other performance metrics include bandwidth to the storage service latency between VMs and failure resiliency of the paths between VMs.

Furthermore although the present examples are described and illustrated herein as being implemented in a multi tenant datacenter the methods described herein may be implemented in any datacenter which runs multiple competing jobs where the jobs may be for the same entity or different entities. Datacenters do not need to be multi tenant but may involve an aspect of sharing e.g. between jobs for the same entity or between jobs for different entities and although the datacenter may be a cloud based datacenter in other examples the datacenter may not be cloud based. Examples of datacenters which may use the methods described herein include datacenters providing an internet search engine a cloud datacenter a company datacenter a datacenter for High Performance Computing etc.

The term tenant is used herein to refer to both existing tenants and prospective tenants of the datacenter i.e. where a method refers to receiving a request from a tenant of the datacenter this request may be the first request received from that tenant such that they do not have any existing allocated resources or the tenant may have submitted previous requests and may already have allocated resources within the datacenter. The term user may be used interchangeably with tenant .

The term computer is used herein to refer to any device with processing capability such that it can execute instructions. Those skilled in the art will realize that such processing capabilities are incorporated into many different devices and therefore the term computer includes PCs servers mobile telephones personal digital assistants and many other devices.

The methods described herein may be performed by software in machine readable form on a tangible storage medium e.g. in the form of a computer program comprising computer program code means adapted to perform all the steps of any of the methods described herein when the program is run on a computer and where the computer program may be embodied on a computer readable medium. Examples of tangible or non transitory storage media include computer storage devices comprising computer readable media such as disks thumb drives memory etc and do not include propagated signals. The software can be suitable for execution on a parallel processor or a serial processor such that the method steps may be carried out in any suitable order or simultaneously.

This acknowledges that software can be a valuable separately tradable commodity. It is intended to encompass software which runs on or controls dumb or standard hardware to carry out the desired functions. It is also intended to encompass software which describes or defines the configuration of hardware such as HDL hardware description language software as is used for designing silicon chips or for configuring universal programmable chips to carry out desired functions.

Those skilled in the art will realize that storage devices utilized to store program instructions can be distributed across a network. For example a remote computer may store an example of the process described as software. A local or terminal computer may access the remote computer and download a part or all of the software to run the program. Alternatively the local computer may download pieces of the software as needed or execute some software instructions at the local terminal and some at the remote computer or computer network . Those skilled in the art will also realize that by utilizing conventional techniques known to those skilled in the art that all or a portion of the software instructions may be carried out by a dedicated circuit such as a DSP programmable logic array or the like.

Any range or device value given herein may be extended or altered without losing the effect sought as will be apparent to the skilled person.

Although the subject matter has been described in language specific to structural features and or methodological acts it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather the specific features and acts described above are disclosed as example forms of implementing the claims.

It will be understood that the benefits and advantages described above may relate to one embodiment or may relate to several embodiments. The embodiments are not limited to those that solve any or all of the stated problems or those that have any or all of the stated benefits and advantages. It will further be understood that reference to an item refers to one or more of those items.

The steps of the methods described herein may be carried out in any suitable order or simultaneously where appropriate. Additionally individual blocks may be deleted from any of the methods without departing from the spirit and scope of the subject matter described herein. Aspects of any of the examples described above may be combined with aspects of any of the other examples described to form further examples without losing the effect sought.

The term comprising is used herein to mean including the method blocks or elements identified but that such blocks or elements do not comprise an exclusive list and a method or apparatus may contain additional blocks or elements.

It will be understood that the above description of a preferred embodiment is given by way of example only and that various modifications may be made by those skilled in the art. The above specification examples and data provide a complete description of the structure and use of exemplary embodiments of the invention. Although various embodiments of the invention have been described above with a certain degree of particularity or with reference to one or more individual embodiments those skilled in the art could make numerous alterations to the disclosed embodiments without departing from the spirit or scope of this invention.

