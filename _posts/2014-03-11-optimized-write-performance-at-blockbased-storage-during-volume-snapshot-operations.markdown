---

title: Optimized write performance at block-based storage during volume snapshot operations
abstract: Write optimization for block-based storage performing snapshot operations may be implemented. Write requests for a particular data volume may be received for which a snapshot operation is in progress. A determination may be made as to whether a data chunk of the data volume modified as part of the write request has not yet been stored to a remote snapshot data store as part of the snapshot operation. For a data chunk that is to be modified and that has not yet been stored, the data chunk may be stored in a local in-memory volume snapshot buffer. Once the data chunk is stored in the in-memory volume snapshot buffer, the write request may be performed and acknowledged as complete. The data chunk may be sent to the remote snapshot data store asynchronously with regard to the acknowledgment of the write request.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09405483&OS=09405483&RS=09405483
owner: Amazon Technologies, Inc.
number: 09405483
owner_city: Reno
owner_country: US
publication_date: 20140311
---
The recent revolution in technologies for dynamically sharing virtualizations of hardware resources software and information storage across networks has increased the reliability scalability and cost efficiency of computing. More specifically the ability to provide on demand virtual computing resources and storage through the advent of virtualization has enabled consumers of processing resources and storage to flexibly structure their computing and storage costs in response to immediately perceived computing and storage needs. Virtualization allows customers to purchase processor cycles and storage at the time of demand rather than buying or leasing fixed hardware in provisioning cycles that are dictated by the delays and costs of manufacture and deployment of hardware. Rather than depending on the accuracy of predictions of future demand to determine the availability of computing and storage users are able to purchase the use of computing and storage resources on a relatively instantaneous as needed basis.

Virtualized computing environments are frequently supported by block based storage. Such block based storage provides a storage system that is able to interact with various computing virtualizations through a series of standardized storage calls that render the block based storage functionally agnostic to the structural and functional details of the volumes that it supports and the operating systems executing on the virtualizations to which it provides storage availability. However as network transmission capabilities increase along with greater processing capacity for virtualized resources I O demands upon block based storage may grow. If I O demands exceed the capacity of block based storage to service requests then latency and or durability of block based storage performance for virtualized computing resources suffer.

While embodiments are described herein by way of example for several embodiments and illustrative drawings those skilled in the art will recognize that the embodiments are not limited to the embodiments or drawings described. It should be understood that the drawings and detailed description thereto are not intended to limit embodiments to the particular form disclosed but on the contrary the intention is to cover all modifications equivalents and alternatives falling within the spirit and scope as defined by the appended claims. The headings used herein are for organizational purposes only and are not meant to be used to limit the scope of the description or the claims. As used throughout this application the word may is used in a permissive sense i.e. meaning having the potential to rather than the mandatory sense i.e. meaning must . Similarly the words include including and includes mean including but not limited to.

The systems and methods described herein may implement optimized write performance for block based storage performing snapshot operations according to various embodiments. Data volumes maintained in block based storage may provide a logical presentation of block data that may be accessed by a storage client. Various different storage operations such as input output I O operations e.g. read write requests may be performed with respect to a data volume. It may be desirable to preserve the state of a data volume at certain points in time such as for recovery operations or historical or analytical operations. Snapshot operations may capture a point in time state of a data volume as a snapshot that may be stored separately from the data volume for future use. Multiple snapshot operations may be performed for the same data volume at different times.

Snapshot operations may often prove to be costly with regard to the performance of other storage operations e.g. I O operations for a data volume undergoing a snapshot operation. As the state of the data of the data volume at the time associated with the snapshot may need to be preserved until it is copied into the snapshot changes to the data volume may be blocked dropped or otherwise delayed from completion. Although capturing snapshots of a data volume may be beneficial for other operations the performance impact on I O operations may discourage and or prevent the capture of snapshots operations such as for applications with I O intensive workloads making use of data volumes in block based storage.

As illustrated in data chunks through have already been sent to remote snapshot data store which may be a data store remote from storage node . While the snapshot operation is in progress write requests may also be received at storage node . For write requests to those data chunks that have yet to be stored in remote snapshot data store such as a write request to data chunk data chunk may be stored in a storage slot in in memory volume snapshot buffer which may be a portion of system memory or other memory device local to storage node . Once stored in the in memory volume snapshot buffer the write request may be performed at persistent volume storage to update data chunk and then acknowledged as complete as the version of the data associated with the ongoing snapshot operation is still preserved. By storing the data chunk in the in memory volume snapshot buffer the write request may be completed in various embodiments without blocking the write request until data chunk is stored in remote snapshot data store . As illustrated in in memory volume snapshot buffer has other available storage space to store other data chunks such as free slot and free slot . However if no storage space for a data chunk were to be available in in memory volume snapshot buffer various different actions might be taken as described in further detail below with regard to such as blocking the write request and or aborting the snapshot operation.

As illustrated in scene the snapshot operation may carry on copying data chunks to remote snapshot data store without regard to write requests received at storage node in various embodiments. Data chunk may be sent from in memory volume snapshot buffer to remote snapshot data store asynchronously from performing and acknowledging write request . Once stored in remote snapshot data store the space storing data chunk in in memory volume snapshot buffer may be reclaimed and made free to store another data chunk. discussed in more detail below provides different examples and techniques for selecting and sending data chunks from an in memory volume snapshot buffer to a remote snapshot data store.

Please note that previous descriptions are not intended to be limiting but are merely provided as an example of storage nodes that provide block based storage implementing write optimizations during snapshot operations. For example multiple storage nodes may maintain replicas of a same data volume and implement write optimization at each of the storage nodes maintaining replicas of the data volume. Other differences for example such as the number of data chunks in a data volume or the number of available storage slots for data chunks in the in memory volume snapshot buffer may also be different than illustrated in .

This specification begins with a general description of a provider network implementing a block based storage service for volume clients such as virtual instances of a virtual computing resource provider. Storage nodes for the block based storage service may implement write optimization during snapshot operations in some embodiments. Then various examples of a block based storage service are discussed including different components modules or arrangements of components module that may be employed as part of implementing a block based storage service. A number of different methods and techniques to optimize write performance for block based storage during snapshot operations are then discussed some of which are illustrated in accompanying flowcharts. Finally a description of an example computing system upon which the various components modules systems devices and or nodes may be implemented is provided. Various examples are provided throughout the specification.

As noted above virtual compute service may offer various compute instances to clients . A virtual compute instance may for example comprise one or more servers with a specified computational capacity which may be specified by indicating the type and number of CPUs the main memory size and so on and a specified software stack e.g. a particular version of an operating system which may in turn run on top of a hypervisor . A number of different types of computing devices may be used singly or in combination to implement the compute instances of virtual compute service in different embodiments including general purpose or special purpose computer servers storage devices network devices and the like. In some embodiments instance clients or other any other user may be configured and or authorized to direct network traffic to a compute instance. In various embodiments compute instances may attach or map to one or more data volumes provided by block based storage service in order to obtain persistent block based storage for performing various operations.

Compute instances may operate or implement a variety of different platforms such as application server instances Java virtual machines JVMs general purpose or special purpose operating systems platforms that support various interpreted or compiled programming languages such as Ruby Perl Python C C and the like or high performance computing platforms suitable for performing client applications without for example requiring the client to access an instance. In some embodiments compute instances have different types or configurations based on expected uptime ratios. The uptime ratio of a particular compute instance may be defined as the ratio of the amount of time the instance is activated to the total amount of time for which the instance is reserved. Uptime ratios may also be referred to as utilizations in some implementations. If a client expects to use a compute instance for a relatively small fraction of the time for which the instance is reserved e.g. 30 35 of a year long reservation the client may decide to reserve the instance as a Low Uptime Ratio instance and pay a discounted hourly usage fee in accordance with the associated pricing policy. If the client expects to have a steady state workload that requires an instance to be up most of the time the client may reserve a High Uptime Ratio instance and potentially pay an even lower hourly usage fee although in some embodiments the hourly fee may be charged for the entire duration of the reservation regardless of the actual number of hours of use in accordance with pricing policy. An option for Medium Uptime Ratio instances with a corresponding pricing policy may be supported in some embodiments as well where the upfront costs and the per hour costs fall between the corresponding High Uptime Ratio and Low Uptime Ratio costs.

Compute instance configurations may also include compute instances with a general or specific purpose such as computational workloads for compute intensive applications e.g. high traffic web applications ad serving batch processing video encoding distributed analytics high energy physics genome analysis and computational fluid dynamics graphics intensive workloads e.g. game streaming 3D application streaming server side graphics workloads rendering financial modeling and engineering design memory intensive workloads e.g. high performance databases distributed memory caches in memory analytics genome assembly and analysis and storage optimized workloads e.g. data warehousing and cluster file systems . Size of compute instances such as a particular number of virtual CPU cores memory cache storage as well as any other performance characteristic. Configurations of compute instances may also include their location in a particular data center availability zone geographic location etc. . . . and in the case of reserved compute instances reservation term length.

In various embodiments provider network may also implement block based storage service for performing storage operations. Block based storage service is a storage system composed of a pool of multiple independent storage nodes through e.g. server block data storage systems which provide block level storage for storing one or more sets of data volumes data volume s through . Data volumes may be mapped to particular clients providing virtual block based storage e.g. hard disk storage or other persistent storage as a contiguous set of logical blocks. In some embodiments a data volume may be divided up into multiple data chunks including one or more data blocks for performing other block storage operations such as snapshot operations or replication operations.

A volume snapshot of a data volume may be a fixed point in time representation of the state of the data volume . In some embodiments volume snapshots may be stored remotely from a storage node maintaining a data volume such as in another storage service . Snapshot operations may be performed to send copy and or otherwise preserve the snapshot of a given data volume in another storage location such as a remote snapshot data store in other storage service over various different internal or external communication systems devices or networks implemented as part of provider network .

Block based storage service may implement block based storage service control plane to assist in the operation of block based storage service . In various embodiments block based storage service control plane assists in managing the availability of block data storage to clients such as programs executing on compute instances provided by virtual compute service and or other network based services located within provider network and or optionally computing systems not shown located within one or more other data centers or other computing systems external to provider network available over a network . Access to data volumes may be provided over an internal network within provider network or externally via network in response to block data transaction instructions.

Block based storage service control plane may provide a variety of services related to providing block level storage functionality including the management of user accounts e.g. creation deletion billing collection of payment etc. . Block based storage service control plane may further provide services related to the creation usage and deletion of data volumes in response to configuration requests. Block based storage service control plane may also provide services related to the creation usage and deletion of volume snapshots on other storage service . Block based storage service control plane may also provide services related to the collection and processing of performance and auditing data related to the use of data volumes and snapshots of those volumes.

Provider network may also implement another storage service as noted above. Other storage service may provide a same or different type of storage as provided by block based storage service . For example in some embodiments other storage service may provide an object based storage service which may store and manage data as data objects. For example volume snapshots of various data volumes may be stored as snapshot objects for a particular data volume . In addition to other storage service provider network may implement other network based services which may include various different types of analytical computational storage or other network based system allowing clients as well as other services of provider network e.g. block based storage service virtual compute service and or other storage service to perform or request various tasks.

Clients may encompass any type of client configurable to submit requests to network provider . For example a given client may include a suitable version of a web browser or may include a plug in module or other type of code module configured to execute as an extension to or within an execution environment provided by a web browser. Alternatively a client may encompass an application such as a database application or user interface thereof a media application an office application or any other application that may make use of compute instances a data volume or other network based service in provider network to perform various operations. In some embodiments such an application may include sufficient protocol support e.g. for a suitable version of Hypertext Transfer Protocol HTTP for generating and processing network based services requests without necessarily implementing full browser support for all types of network based data. In some embodiments clients may be configured to generate network based services requests according to a Representational State Transfer REST style network based services architecture a document or message based network based services architecture or another suitable network based services architecture. In some embodiments a client e.g. a computational client may be configured to provide access to a compute instance or data volume in a manner that is transparent to applications implement on the client utilizing computational resources provided by the compute instance or block storage provided by the data volume .

Clients may convey network based services requests to provider network via external network . In various embodiments external network may encompass any suitable combination of networking hardware and protocols necessary to establish network based communications between clients and provider network . For example a network may generally encompass the various telecommunications networks and service providers that collectively implement the Internet. A network may also include private networks such as local area networks LANs or wide area networks WANs as well as public or private wireless networks. For example both a given client and provider network may be respectively provisioned within enterprises having their own internal networks. In such an embodiment a network may include the hardware e.g. modems routers switches load balancers proxy servers etc. and software e.g. protocol stacks accounting software firewall security software etc. necessary to establish a networking link between given client and the Internet as well as between the Internet and provider network . It is noted that in some embodiments clients may communicate with provider network using a private network rather than the public Internet.

Block based storage service may manage and maintain data volumes in a variety of different ways. Different durability schemes may be implemented for some data volumes among two or more storage nodes maintaining a replica of a data volume. For example different types of mirroring and or replication techniques may be implemented e.g. RAID 1 to increase the durability of a data volume such as by eliminating a single point of failure for a data volume. In order to provide access to a data volume storage nodes may then coordinate I O requests such as write requests among the two or more storage nodes maintaining a replica of a data volume. For storage nodes providing write optimization during a snapshot operation additional coordination may be implemented.

In various embodiments master storage node and or slave storage node s may be configured to process various volume client requests according to one or more interfaces communication protocols or other techniques. For example in some embodiments block based storage service may implement an application programming interface API which may allow volume clients to perform various storage operations for a particular data volume. In some embodiments master storage node may be configured to process requests from volume clients via a storage communication protocol such as internet small computer system interface iSCSI . Although illustrated as single entity volume client may be a virtual compute instance such as may be provided by virtual compute service in a client of provider network e.g. requesting a data volume snapshot or other component of block based storage service e.g. block based storage service control plane .

Master storage node and or slave storage node s may in some embodiments be configured to communicate with other storage systems or devices remote from the respective storage nodes such as other storage service . Specifically volume snapshots of data volumes maintained at master storage node and slave storage node s such as data volume may be sent to other storage service to be stored. Thus master storage node and or slave storage node s may be configured to perform snapshot operations. A snapshot operation as discussed below with regard to as well as the various methods and techniques discussed below with regard to may include storing a state of a data volume at a point in time to other data storage . Various different request store write copy or other messages may be sent to other data storage service in order to store volume snapshot s . In various embodiments data volumes may be divided into data chunks such that a snapshot operation may send the respective data chunks of a data volume to the other storage service to be stored as part of a particular volume snapshot . Once all of the data chunks are stored in the snapshot in other storage service then the snapshot operation may be considered complete committed and or acknowledged to a requesting volume client . described in detail below provides examples of various different techniques master storage node and or slave storage node s may implement to determine whether a snapshot operation is complete.

During a snapshot operation such as for data volume write requests for data volume may be received at master storage node from volume client . These write requests may modify data in a data chunk in data volume . If the data chunk remains to be stored in the volume snapshot in other storage then modifying the data chunk would violate the point in time state of the snapshot for data volume . Thus in various embodiments master storage node and or slave storage node s may implement write optimization for write requests received during a snapshot operation. System or other memory implemented at master storage node and slave storage node s may include in memory volume snapshot volume buffers and respectively. In some embodiments in memory volume snapshot buffers and may include one or more slots or other type of allocated storage spaces for data chunks e.g. a 256 MB in memory volume snapshot buffer may allocate 64 slots for 4 MB data chunks . The system or other memory device implementing in memory volume snapshot buffers and may not be persistent storage that is in the event of system failure such as a power failure data in the memory device may be lost . Alternatively in some embodiments the system memory may be persistent such as NV RAM or other type of battery backed RAM as well as various other types of memristor based resistive random access memory ReRAM three dimensional NAND technologies Ferroelectric RAM magnetoresistive RAM MRAM or any of various types of phase change memory PCM may be used to maintain the in memory volume snapshot buffer.

Please note that previous discussions and illustrations are not intended to be limiting as to the various other schemes and or protocols to implement optimized write performance during snapshot operations. For example various messages may be combined such as buffer request and forwarding of the write request . The timing of performing various operations may be different such as the unmodified data chunk may be stored in the in memory volume snapshot buffer once the write request is received at master storage node whether prior to sending the buffer request or after receiving acknowledgment of the buffer request .

In various embodiments master storage node may send an initial query to determine if a snapshot to be sent from an in memory volume snapshot buffer is already stored in other storage service . If yes then the data chunk may be removed from the in memory volume snapshot buffer. As illustrated in if the data chunk is not in the snapshot then master storage node may send a request to store the data chunk in the snapshot to other storage service . If master storage node receives an acknowledgment of the data chunk stored in other storage service then the data chunk may be removed from the in memory volume snapshot buffer . If however no acknowledgment is received in some embodiments master storage node may send another storage request for the data chunk repeatedly until an acknowledgement is received.

Similar to master storage node slave storage node s may communicate with other storage service via an application programming interface API or other protocol such as may be provided by other storage service . Slave storage node s may send an initial query to determine if a snapshot to be sent from an in memory volume snapshot buffer is already stored in other storage service . If yes then the data chunk may be removed from the in memory volume snapshot buffer. As illustrated in if the data chunk is not in the snapshot then slave storage node s may send a request to store the data chunk in the snapshot to other storage service . If slave storage node s receive an acknowledgment of the data chunk stored in other storage service then the data chunk may be removed from the in memory volume snapshot buffer . If however no acknowledgment is received in some embodiments slave storage node s may send another storage request for the data chunk repeatedly until an acknowledgement is received.

In some embodiments master storage node and or slave storage node s may not send an initial query to determine if particular data chunk is stored at other storage service . For example if master storage node and slave storage node s implement a durability scheme such as discussed above with regard to then the a data chunk in one in memory volume snapshot buffer may be the same as a data chunk in another in memory volume snapshot buffer. Thus overwrites of data chunks replacing a same data chunk in the remote snapshot data store may occur without any data loss or consistency error in the snapshot. In some embodiments either master storage node or or alternatively slave storage node s may perform sending data chunks from the in memory volume snapshot buffer to the remote snapshot data store as part of the snapshot operation. For instance slave storage node s may send data chunks from the in memory volume snapshot buffer as part of a failover mechanism where the master storage node has failed and or otherwise lost data in the in memory volume snapshot buffer at the master storage node .

Please note that in some embodiments the role of master and slave storage nodes may be assigned per data volume. For example for one data volume maintained at a storage node the storage node may serve as a master storage node. While for another data volume maintained at the same storage node the storage node may serve as a slave storage node. Moreover in some embodiments write optimization may be implemented at a master only slave only or any other combination of multiple storage nodes. For example in some embodiments write optimization may be disabled at different storage nodes based on network utilization e.g. heavy traffic in order to lessen the amount of network traffic e.g. by reducing the number of storage nodes that send data to the remote snapshot data store .

The examples of optimized write performance at block based storage during snapshot operations discussed above with regard to have been given in regard to a block based storage service. Various other types or configurations of block based storage may implement these techniques. For example a client of block based storage application operating system or other component sending write requests for the data volume may be implemented on a same computing system as the storage node performing a snapshot operation. Different configurations of storage nodes may also implement various numbers of replicas mirroring or other durability techniques that may implement write optimization during a snapshot operation performed by the variously configured storage nodes for the data volume. is a high level flowchart illustrating various methods and techniques for optimized write performance at block based storage during snapshot operations according to some embodiments. These techniques may be implemented using one or more storage nodes or other system component that performs snapshot operations for a data volume in block based storage as described above with regard to .

As indicated at a write request for a data volume maintained in persistent data storage for which a snapshot operation is currently performed may be received in various embodiments. A snapshot operation may be an operation to capture a particular state of a data volume a particular point in time to create a snapshot of the data volume. A data volume may be divided in multiple data chunks which may in turn be groups of smaller units such as data blocks on one or more block based persistent storage devices . In other words data chunks may be logical units of a logical whole the data volume maintained in one or more block based storage devices such as may be implemented at a storage node storage server or other block based storage device that maintains one or more data volumes for clients. The snapshot operation may in various embodiments include multiple operations to transfer data chunks of the data volume to a remote snapshot data store which may be another storage service or device such as described above with regard to .

As data volumes may vary in size with particularly large data volumes including many data chunks a snapshot operation may impact other I O operations directed to the data volume e.g. write requests read requests etc. . For example if a change is to be made to portion of data in a data chunk and the data chunk has not yet been stored in the remote snapshot data store then version of the data in the data chunk may need to be maintained until stored in the snapshot in order to preserve the snapshot of the data volume. Thus a determination may be made as to whether a data chunk modified by the write request has not yet been stored in the remote snapshot data for the snapshot operation as indicated at . is a high level flowchart illustrating various methods and techniques for determining whether to store a data chunk in an in memory volume snapshot buffer according to some embodiments.

As indicated at a data chunk may be identified that is to be modified by a write request. A write request may be directed to a particular range location or portion of the data volume. An identified data chunk may include the particular range location or portion. Once identified a determination may be made as to whether the data chunk is stored in the remote snapshot data as indicated at . For example a snapshot operation may maintain a listing index or other form of metadata describing the progress of a snapshot operation for a data volume. Data chunks already stored in the remote snapshot data store may be marked in the listing as stored. While those data chunks that have not yet been stored may be identified as not stored. If the data chunk is stored in the remotes snapshot data store then as indicated by the positive exit from the write request may be completed as indicated at and may be acknowledged as complete. If the data chunk is not stored in the remote snapshot then the data chunk may be identified for storage in a local in memory volume snapshot buffer as indicated at .

Turning back to the data chunk modified by the write request that has not yet been stored in the remote snapshot data store may in various embodiments be stored in a local in memory volume snapshot buffer as indicated at . As discussed below with regard to a local in memory volume snapshot buffer may be a portion of system memory or other memory device with write and or read speeds similar to system memory for which an allocated portion of the memory e.g. one or more slots or other type of allocated storage spaces stores data chunks e.g. a 256 MB in memory volume snapshot buffer may allocate 64 slots for 4 MB data chunks . The in memory volume snapshot buffer may then maintain the unmodified version of the data chunk until it may be stored in the remote snapshot data store as part of the snapshot operation. In some embodiments the in memory volume snapshot buffer may not be persistent storage that is in the event of system failure such as a power failure data in the memory device may be lost . Alternatively in some embodiments the system memory may be persistent such as NV RAM or other type of battery backed RAM as well as various other types of memristor based resistive random access memory ReRAM three dimensional NAND technologies Ferroelectric RAM magnetoresistive RAM MRAM or any of various types of phase change memory PCM may be used to maintain the in memory volume snapshot buffer.

In various embodiments the data chunk may be written to available space in the in memory volume snapshot buffer. Metadata listing mapping information or other data describing the progress of the snapshot operation and or the contents of the in memory volume snapshot buffer may be updated in response to storing the data chunk. discussed in detail below describes various techniques for handling write requests that result in a data chunk that has not yet been sent to a remote snapshot data store and no available storage in the in memory volume snapshot buffer to store the data chunk.

Once the data chunk is stored in the in memory volume snapshot buffer the write request may be performed to update the data chunk in the persistent storage e.g. block based storage devices for the data volume as indicated at . In response to performing the write request the write request may be acknowledged as complete as indicated at . For example an acknowledgement message may be sent to a client that indicating that the write request is complete.

As indicated at the data chunk may be sent from the in memory volume snapshot data store to the remote snapshot data store asynchronously with regard to the performance of the write request and acknowledgment of the write request. For example the data chunk may be sent to the remote snapshot data store either before during or after the performance of the write request and or acknowledgment of the write request as complete. discussed in more detail below describes various methods and techniques for pushing data chunks stored in the in memory volume snapshot buffer to the remotes snapshot data store.

In some embodiments write requests may be received that are associated with a versions of data stored in particular data chunks that are prior in time to the snapshot operation in progress. In response to receiving such writes affected data chunks stored in the in memory volume snapshot buffer and or persistent data storage may be updated to include the write request and the write request may then be acknowledged in response to completion of the write request at the affected data chunks.

The techniques described above may be implemented upon multiple storage nodes servers systems or devices which may maintain a data volume. As noted above multiple replicas of a same data volume may forward write requests and acknowledgments of write requests to one another in order to provide greater durability for data volumes such as described above in . Similarly asynchronous snapshot operations discussed above with regard to and below with regard to may be performed among the multiple replicas of a same data volume in order to complete a snapshot operation for a data volume.

Snapshot operations may in various embodiments operate independently from other operations performed with regard to a data volume. As noted above performing a snapshot operation may occur as part of a background process at a storage node while input output operations e.g. storing chunks in the in memory volume snapshot buffer performing write read requests acknowledging write requests may be performed as part of a foreground process at a storage node. is a high level flowchart illustrating various methods and techniques for copying data chunks maintained in an in memory volume snapshot buffer to a remote snapshot data store according to some embodiments. Such techniques may be performed before during or after processing incoming read and or write requests. Thus maintaining data chunks in the in memory volume snapshot buffer may allow for asynchronous transfer of data chunks to the remote snapshot data store without regard to acknowledging a write request.

As indicated at a data chunk maintained in an in memory volume snapshot buffer for a data volume to store in a remote snapshot data store as part of a snapshot operation may be identified. Various different schemes or techniques may be implemented to determine which data chunk should be identified for sending to the remote snapshot data store. For example in some embodiments data chunks maintained in memory volume snapshot buffer may be selected according a first in first out FIFO last in first out LIFO or other ordering technique. In some embodiments snapshot operations for all data chunks in a data volume may be performed according an ordering and thus a particular data chunk in the in memory volume snapshot buffer may be identified to be sent to a remote snapshot data store based on this ordering for the snapshot operation. For example snapshot operations may send data chunks in a contiguous order e.g. starting with the lowest or highest address range for a given data chunk and then selecting an adjacent data chunk or may prioritize the selection of certain data chunks e.g. those that are most frequently written i.e. hot over those data chunks that may be accessed less frequently. In some embodiments multiple storage nodes maintaining an in memory volume snapshot buffer e.g. a master and one or more slave storage nodes may identify data chunks in the in memory volume snapshot buffer for the data volume in a different order. For example slave storage nodes may operate in FIFO manner while a master storage node may identify data chunks to send in a LIFO manner.

In some embodiments a check or other operation may be made to determine whether the data chunk is already stored in the remote snapshot data store prior to sending the identified data chunk as indicated at . For example a query request or other message may be sent to the data store identifying the data chunk to be sent and or receiving back some indication from the remote snapshot data store from which it may be determined whether the data chunk is already present and the correct version of the data chunk associated with the snapshot . If the data chunk is already stored in the data store as indicated by the positive exit from then the data chunk may be removed from the in memory volume snapshot data store as indicated at . If the data chunk is not present at the remote snapshot data store as indicated at the negative exit from then the data chunk may be sent to the remote snapshot data store as indicated at to be included in the snapshot. In various embodiments a confirmation or acknowledgment may be received from the remote snapshot data store that a particular data chunk sent to the remote snapshot data store has been stored. The data chunk may then be removed from the in memory volume snapshot buffer as indicate at . Removing the data chunk may make the slot or space occupied by the data chunk available for storing an additional data chunk.

Data chunks as previously noted may not be removed from an in memory volume snapshot buffer until determined to be stored in a remote snapshot data store in some embodiments. Over time the number of data chunks stored in the in memory volume snapshot buffer may grow if for example the data chunks may not be sent to and stored in the remote snapshot data store at the same or faster rate than data chunks are stored in the in memory volume snapshot buffer. is a high level flowchart illustrating various methods and techniques for handling write requests with a full in memory volume snapshot buffer according to some embodiments.

As indicated at a write request may be received for a data volume maintained in persistent data storage for which a snapshot operation is currently being performed. As discussed above with regard to write requests in a write request may write to delete or modify in any way a portion of the data volume. As a data volume may be divided up into multiple different data chunks representing ranges of data in the data volume a data chunk modified by a write request may be identified. A determination may be made as to whether the data chunk has yet to be stored in the remote snapshot data store. For a data chunk that has not yet been sent to the remote snapshot data store as part of a snapshot operation as indicated at the data chunk may be identified to be stored in an in memory volume snapshot buffer so that the write request may be completed to the affected data chunk. However as noted earlier in some instances the in memory volume snapshot buffer may not have any available space. For example in some embodiments the in memory volume snapshot buffer may allocate a fixed number of storage slots. Based on the number of available storage slots or other free space allocated to store the identified data chunk a determination may be made that the in memory volume snapshot buffer has no available storage for the identified data chunk as indicated by the negative exit at . In some embodiments other demands on the memory device implementing the in memory volume snapshot buffer may reduce the space allocated to the in memory volume snapshot buffer. For example if a storage node detects that more system memory is needed to perform other processes storage space may be reallocated away from the in memory volume snapshot buffer.

In response to determining that the in memory volume snapshot buffer has no available storage space various different actions may be taken. For example in some embodiments as indicated at the write request may be blocked e.g. stored in a buffer until the identified data chunk is stored in the remote snapshot data store as part of the snapshot operation. Therefore the write request may be delayed not completed and remain unacknowledged until the data chunk is sent to the remote snapshot data store and acknowledged as stored in some embodiments. Other write requests for other data chunks that have already been stored in the remote snapshot data store may in some embodiments be completed and acknowledged as indicated at . Alternatively all write requests may be blocked delayed not completed and unacknowledged until the write request for the data chunk that remains to be stored in the remote snapshot data store is complete. As also indicted at in some embodiments the write request may be blocked until a storage slot becomes available in the in memory volume storage buffer and then completed such as described above with regard to . In some embodiments blocking may occur until the sooner of either a storage slot in the in memory volume snapshot buffer becomes available or the data chunk is sent to the remote snapshot data store.

In some embodiments instead of blocking a write request that modifies a data chunk that has not yet been stored as part of a snapshot operation for a data volume the snapshot operation itself may be aborted. For multiple storage nodes implementing in memory volume snapshot buffers for a same data volume even one full in memory volume snapshot buffer may trigger the abortion of a snapshot operation for a data volume. Similar to the example given above a selection may be made between multiple different techniques to handle such a write request. For example if the snapshot operation is completed above some threshold e.g. 95 then the snapshot operation may complete and the write may be blocked. If however the snapshot operation is below the threshold e.g. 80 then the snapshot operation may be aborted and the write completed and acknowledged without blocking the write request. Determining which handling technique for an in memory volume snapshot buffer with no available storage space may be selected or identified by a user e.g. a block based storage service control panel or other type of agreement or setting agreed to by a user owner of a data volume persisted at a storage node in some embodiments.

As discussed above with regard to data chunks maintained in an in memory volume snapshot buffer are sent asynchronously to remote snapshot data storage with regard to acknowledging completion of write requests. Once a data chunk is removed from an in memory volume snapshot buffer the data chunk may not be recoverable at the storage node as the data chunk is stored in the in memory volume buffer because it has been modified at the storage node . This is not problematic if the snapshot at the remote snapshot data store maintains a copy of the data chunk. However if the snapshot does not maintain a copy then the snapshot may not be able to be completed. is a high level flowchart illustrating various methods and techniques for determining whether a snapshot operation for which optimized write requests are performed is complete according to some embodiments.

As indicated at an evaluation of data chunks stored in a remote snapshot data store may be performed to determine whether a particular snapshot operation is complete. For example a list or other set of metadata may be maintained for the data volume indicating data chunks that were stored in the in memory volume buffer. The listing information or metadata may also include the particular snapshot of the data volume with which data chunks are associated. For example multiple snapshot operations may be ongoing for a particular data volume for different point in time versions of the data volume . Data chunks may be stored in the in memory volume snapshot buffer that are associated with these different snapshots.

Based on the listing of data chunks associated with a particular snapshot the data chunks stored in the remote snapshot data store may be checked to determine if the data chunk once stored in the in memory volume snapshot buffer is present in the remote snapshot data store. For instance a storage node e.g. a master storage node control plane or other system component that determines whether a snapshot operation is complete may send query messages to the remote snapshot data store that ask for version information for particular data chunks in order to be compared with version information maintained for the data chunks in the snapshot and thus may take inventory of the data chunks stored in the snapshot in the remote snapshot data store. If the version information does not match for a particular data chunk then it may be determined that the particular data chunk in the remote snapshot data store is not the correct data chunk or is not present . In some embodiments the remote snapshot data store itself may be able to confirm or deny the existence of data chunk whether or not it matches e.g. a particular version or whether the data chunk has been modified. Evaluating the completeness of a snapshot operation may be performed after storage nodes may be performed after all data chunks for the data volume are sent to the remote snapshot data store in some embodiments.

As indicated from the positive exit from if the snapshot operation is complete the snapshot may be committed as indicated at . Any post commit processing for a snapshot may then be performed such as notifying a client that the snapshot operation is complete updating any data volume information such as a listing of snapshots or any other operation that is performed with respect to complete snapshots.

As indicated from the negative exit from if the snapshot operation is not complete a check may be performed to determine whether a snapshot time operation timeout has elapsed as indicated at . For example in some embodiments a time limit or timeout may be implemented for a given snapshot operation. If a snapshot operation is not determined to be complete within the given time period then the snapshot operation may be aborted as indicated . For instance at the commencement of a snapshot operation a timer may be started which may then abort the snapshot operation if the timer exceeds a timeout threshold for the snapshot operation. Alternatively in some embodiments a timer may be started after the snapshot operation is first determined to not be complete as illustrated in . For instance the snapshot operation itself may take as long as necessary but once all of the data chunks for a data volume are sent to the remote snapshot data store and an evaluation of the snapshot operation that indicates that the snapshot operation is not complete the timer may start e.g. once all data chunks there may be a 24 hour window for the snapshot stored in the remote snapshot data store to become complete .

The methods described herein may in various embodiments be implemented by any combination of hardware and software. For example in one embodiment the methods may be implemented by a computer system e.g. a computer system as in that includes one or more processors executing program instructions stored on a computer readable storage medium coupled to the processors. The program instructions may be configured to implement the functionality described herein e.g. the functionality of various servers storage nodes control planes managers and or other components such as those that implement the block based storage service described herein . The various methods as illustrated in the figures and described herein represent example embodiments of methods. The order of any method may be changed and various elements may be added reordered combined omitted modified etc.

Embodiments of optimized write performance at block based storage during snapshot operations as described herein may be executed on one or more computer systems which may interact with various other devices. is a block diagram illustrating an example computer system according to various embodiments. For example computer system may be configured to implement storage and or compute nodes of a compute cluster a data stores and or a client in different embodiments. Computer system may be any of various types of devices including but not limited to a personal computer system desktop computer laptop or notebook computer mainframe computer system handheld computer workstation network computer a consumer device application server storage device telephone mobile telephone or in general any type of computing device.

Computer system includes one or more processors any of which may include multiple cores which may be single or multi threaded coupled to a system memory via an input output I O interface . Computer system further includes a network interface coupled to I O interface . In various embodiments computer system may be a uniprocessor system including one processor or a multiprocessor system including several processors e.g. two four eight or another suitable number . Processors may be any suitable processors capable of executing instructions. For example in various embodiments processors may be general purpose or embedded processors implementing any of a variety of instruction set architectures ISAs such as the x86 PowerPC SPARC or MIPS ISAs or any other suitable ISA. In multiprocessor systems each of processors may commonly but not necessarily implement the same ISA. The computer system also includes one or more network communication devices e.g. network interface for communicating with other systems and or components over a communications network e.g. Internet LAN etc. .

In the illustrated embodiment computer system also includes one or more persistent storage devices and or one or more I O devices . In various embodiments persistent storage devices may correspond to disk drives tape drives solid state memory other mass storage devices block based storage devices or any other persistent storage device. Computer system or a distributed application or operating system operating thereon may store instructions and or data in persistent storage devices as desired and may retrieve the stored instruction and or data as needed. For example in some embodiments computer system may host a storage system server node and persistent storage may include the SSDs attached to that server node.

Computer system includes one or more system memories that are configured to store instructions and data accessible by processor s . In various embodiments system memories may be implemented using any suitable memory technology e.g. one or more of cache static random access memory SRAM DRAM RDRAM EDO RAM DDR 10 RAM synchronous dynamic RAM SDRAM Rambus RAM EEPROM non volatile Flash type memory or any other type of memory such as non volatile random access memory NV RAM . System memory may contain program instructions that are executable by processor s to implement the methods and techniques described herein. In various embodiments program instructions may be encoded in platform native binary any interpreted language such as Java byte code or in any other language such as C C Java etc. or in any combination thereof. For example in the illustrated embodiment program instructions include program instructions executable to implement the functionality of a storage node in different embodiments. In some embodiments program instructions may implement multiple separate clients nodes and or other components.

In some embodiments program instructions may include instructions executable to implement an operating system not shown which may be any of various operating systems such as UNIX LINUX Solaris MacOS Windows etc. Any or all of program instructions may be provided as a computer program product or software that may include a non transitory computer readable storage medium having stored thereon instructions which may be used to program a computer system or other electronic devices to perform a process according to various embodiments. A non transitory computer readable storage medium may include any mechanism for storing information in a form e.g. software processing application readable by a machine e.g. a computer . Generally speaking a non transitory computer accessible medium may include computer readable storage media or memory media such as magnetic or optical media e.g. disk or DVD CD ROM coupled to computer system via I O interface . A non transitory computer readable storage medium may also include any volatile or non volatile media such as RAM e.g. SDRAM DDR SDRAM RDRAM SRAM etc. ROM etc. that may be included in some embodiments of computer system as system memory or another type of memory. In other embodiments program instructions may be communicated using optical acoustical or other form of propagated signal e.g. carrier waves infrared signals digital signals etc. conveyed via a communication medium such as a network and or a wireless link such as may be implemented via network interface .

In some embodiments system memory may include data store which may be configured as described herein. In general system memory e.g. data store within system memory persistent storage and or remote storage may store data blocks replicas of data blocks metadata associated with data blocks and or their state configuration information and or any other information usable in implementing the methods and techniques described herein.

In one embodiment I O interface may be configured to coordinate I O traffic between processor system memory and any peripheral devices in the system including through network interface or other peripheral interfaces. In some embodiments I O interface may perform any necessary protocol timing or other data transformations to convert data signals from one component e.g. system memory into a format suitable for use by another component e.g. processor . In some embodiments I O interface may include support for devices attached through various types of peripheral buses such as a variant of the Peripheral Component Interconnect PCI bus standard or the Universal Serial Bus USB standard for example. In some embodiments the function of I O interface may be split into two or more separate components such as a north bridge and a south bridge for example. Also in some embodiments some or all of the functionality of I O interface such as an interface to system memory may be incorporated directly into processor .

Network interface may be configured to allow data to be exchanged between computer system and other devices attached to a network such as other computer systems for example. In addition network interface may be configured to allow communication between computer system and various I O devices and or remote storage . Input output devices may in some embodiments include one or more display terminals keyboards keypads touchpads scanning devices voice or optical recognition devices or any other devices suitable for entering or retrieving data by one or more computer systems . Multiple input output devices may be present in computer system or may be distributed on various nodes of a distributed system that includes computer system . In some embodiments similar input output devices may be separate from computer system and may interact with one or more nodes of a distributed system that includes computer system through a wired or wireless connection such as over network interface . Network interface may commonly support one or more wireless networking protocols e.g. Wi Fi IEEE 802.11 or another wireless networking standard . However in various embodiments network interface may support communication via any suitable wired or wireless general data networks such as other types of Ethernet networks for example. Additionally network interface may support communication via telecommunications telephony networks such as analog voice networks or digital fiber communications networks via storage area networks such as Fibre Channel SANs or via any other suitable type of network and or protocol. In various embodiments computer system may include more fewer or different components than those illustrated in e.g. displays video cards audio cards peripheral devices other network interfaces such as an ATM interface an Ethernet interface a Frame Relay interface etc. 

It is noted that any of the distributed system embodiments described herein or any of their components may be implemented as one or more network based services. For example a compute cluster within a computing service may present computing and or storage services and or other types of services that employ the distributed computing systems described herein to clients as network based services. In some embodiments a network based service may be implemented by a software and or hardware system designed to support interoperable machine to machine interaction over a network. A network based service may have an interface described in a machine processable format such as the Web Services Description Language WSDL . Other systems may interact with the network based service in a manner prescribed by the description of the network based service s interface. For example the network based service may define various operations that other systems may invoke and may define a particular application programming interface API to which other systems may be expected to conform when requesting the various operations. though

In various embodiments a network based service may be requested or invoked through the use of a message that includes parameters and or data associated with the network based services request. Such a message may be formatted according to a particular markup language such as Extensible Markup Language XML and or may be encapsulated using a protocol such as Simple Object Access Protocol SOAP . To perform a network based services request a network based services client may assemble a message including the request and convey the message to an addressable endpoint e.g. a Uniform Resource Locator URL corresponding to the network based service using an Internet based application layer transfer protocol such as Hypertext Transfer Protocol HTTP .

In some embodiments network based services may be implemented using Representational State Transfer RESTful techniques rather than message based techniques. For example a network based service implemented according to a RESTful technique may be invoked through parameters included within an HTTP method such as PUT GET or DELETE rather than encapsulated within a SOAP message.

Although the embodiments above have been described in considerable detail numerous variations and modifications may be made as would become apparent to those skilled in the art once the above disclosure is fully appreciated. It is intended that the following claims be interpreted to embrace all such modifications and changes and accordingly the above description to be regarded in an illustrative rather than a restrictive sense.

