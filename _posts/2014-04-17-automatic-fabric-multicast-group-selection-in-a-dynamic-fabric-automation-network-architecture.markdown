---

title: Automatic fabric multicast group selection in a dynamic fabric automation network architecture
abstract: A method is provided in one example embodiment and includes establishing a pool of multicast group addresses reserved for assignment to Layer 2 (“L2”) and Layer 3 (“L3”) segment IDs of a network comprising an Internet protocol (“IP”) fabric, and assigning a first multicast group address from the pool to an L3 segment ID of a Virtual Routing and Forwarding element (“VRF”) associated with a new partition established in the network. The method further includes pushing the first multicast group address assignment to a database to provide arguments for configuration profiles, and configuring a new tenant detected on a leaf node of the network using the configuration profiles, in which the configuring comprises specifying multicast group to segment ID assignments for the tenant as specified in the configuration profiles.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09419811&OS=09419811&RS=09419811
owner: CISCO TECHNOLOGY, INC.
number: 09419811
owner_city: San Jose
owner_country: US
publication_date: 20140417
---
This disclosure relates in general to the field of computer networking and more particularly to techniques for automatic fabric multicast group selection in a Dynamic Fabric Automation DFA network architecture.

Dynamic Fabric Automation DFA also referred to as Vinci is an architecture for facilitating data center networking. The physical topology of DFA is based on a two tier fat tree also known as a Clos network in which a plurality of leaf nodes which may be implemented as Top of Rack ToR switches or routers connects to each of a plurality of spine nodes implemented as switches or routers and vice versa. To support data forwarding IP fabric is used in one embodiment of DFA. Additional details are provided in VXLAN A Framework for Overlaying Virtualized Layer 2 Networks over Layer 3 Networks draft mahalingam dutt dcops vxlan 02 which is hereby incorporated by reference in its entirety.

Virtual eXtensible Local Area Network VXLAN is a technique for providing an L2 overlay on an L3 network. VXLAN encapsulates native data frames with a VXLAN header and uses UDP IP for transportation. The VXLAN header contains a VXLAN segment ID VXLAN network identifier which is a 24 bit field that identifies virtual network segments for different tenants. Multi destination frames in VXLAN are carried in IP multicast data packets which use group addresses as destination IP addresses. A group address can be dedicated to one segment or shared among multiple segments. Additional details are provided in VXLAN A Framework for Overlaying Virtualized Layer 2 Networks over Layer 3 Networks draft mahalingam dutt dcops vxlan 02 which is hereby incorporated by reference in its entirety. One of the major advantages of VXLAN is that core routers comprising the underlay network can be implemented using generic IP routers that have no knowledge of the overlay encapsulation. In DFA IP fabric spine routers may be VXLAN unaware and are therefore presumed not to perform pruning on VXLAN segment ID.

A method is provided in one example embodiment and includes establishing a pool of multicast group addresses reserved for assignment to Layer 2 L2 and Layer 3 L3 segment IDs of a network comprising an Internet protocol IP fabric and assigning a first multicast group address from the pool to an L3 segment ID of a Virtual Routing and Forwarding element VRF associated with a new partition established in the network. The method further includes pushing the first multicast group address assignment to a database to provide arguments for configuration profiles and configuring a new tenant detected on a leaf node of the network using the configuration profiles in which the configuring comprises specifying multicast group to segment ID assignments for the tenant as specified in the configuration profiles. The method may further include assigning a second multicast group addresses from the pool to an L2 segment ID of a new network established in the network and pushing the second multicast group address assignment to the database to provide additional arguments for the configuration profiles. In some embodiments the configuration profiles include VRF profiles and Virtual eXtensible Local Area Network VXLAN logical interface profiles. In other embodiments the configuring the new tenant involves generating VRF configuration and Virtual eXtensible Local Area Network VXLAN logical interface configuration for the new tenant.

In still other embodiments the method may further include dividing the pool into a common pool and a mobility domain pool and for each multicast group address in the mobility domain pool assigning one of the mobility domain multicast group addresses to at least one non overlapping mobility domain of a group of mobility domains comprising the network if there are fewer mobility domain multicast group addresses than mobility domains otherwise assigning to each of the mobility domains at least one non overlapping multicast group address from the mobility domain pool. Additional embodiments may include for each new partition if the partition contains networks of the same mobility domain assigning one of the multicast group addresses from the mobility domain pool assigned to the mobility domain to an L3 segment ID configured to the partition otherwise selecting a multicast group address from the common pool to be assigned to the L3 segment ID configured to the partition. Still other embodiments may include for each new network selecting one of the multicast group addresses from the mobility domain pool assigned to the mobility domain of the network for assignment to the L2 segment ID configured to the new network. The partition may be a Data Center Network Manager DCNM partition and the network may be a DCNM network.

Multitenancy is an important feature for Vinci IP fabric. Tenant traffic is either switched or routed over the IP fabric encapsulated with VXLAN segment IDs. A tenant may be allocated one or more VLANs on a leaf node to which the virtual machines VMs thereof are connected. Each VLAN is associated with a layer 2 L2 segment ID which is used to encapsulate traffic switched over the fabric. In addition a tenant may be associated with a VRF on the leaf node. The IP packets of a tenant may be forwarded over the IP fabric based on lookups in its VRF. Each VRF is associated with a layer 3 L3 segment ID which is used to encapsulate traffic routed over the fabric. Each segment layer two or layer three will be assigned a multicast group for multi destination traffic within that segment. In view of the fact that the number of segments it typically much larger than the number of groups supported in the fabric multiple segments may share a multicast group.

Simplified fabric management and automatic provisioning are important advantages of Vinci. In one embodiment a network manager element such as Cisco Systems Data Center Network Management DCNM offers a central point of management for ease of operation. A three level hierarchy may be used in DCNM to facilitate automatic provisioning. Such a hierarchy may include one or more DCNM organizations at the highest level DCNM partitions at the middle level and DCNM networks at the lowest level. In particular a Vinci IP fabric may host one or more DCNM organizations. An example of a DCNM organization is a company such as Cisco Systems Inc. Cisco . Each DCNM organization may consist of one or more DCNM partitions. Examples of partitions within an organization may include departments such as Finance Engineering and IT. The combination of a DCNM organization and a DCNM partition uniquely identifies a VRF. Using the previously presented examples Cisco Finance may identify a VRF designated VRFA Cisco Engineering may identify a VRF designated VRFB and Cisco IT may identify a VRF designated VRFC. Within each DCNM partition one or more DCNM networks may be present. Continuing with the previous examples DCNM networks within the DCNM partition Engineering may include Experimental and Research. A DCNM network can be identified by a mobility domain MD and VLAN. A mobility domain defines a scope within which a virtual machine VM may be moved.

In particular a data packet received at a leaf node from a VM in a tenant network may have a header with a data packet identifier. The leaf node may perform an L2 learn event by reading a media access control MAC address in the header of the received packet to discover that the VM and the associated tenant network have not yet been provisioned. The leaf node may use the received data packet identifier such as a VLAN tag along with the mobility domain based on the ingress incoming port to create an identifier that maps to exactly one segment ID. A VLAN identifier alone may be insufficient to map to exactly one segment ID. For example a given virtual data center e.g. VMware vCenter may have up to 4 096 VLANs but there may be multiple linked vCenters with duplicate VLAN identifiers. This may happen when a tenant network with a given subnet runs out of VLANs and thus a linked vCenter with a new pool of 4 096 VLANs is created. For example this will be the case when the same network segment needs to be extended across multiple vCenters since the first vCenter ran out of resources. Thus to create a unique identifier that corresponds to only one segment ID another variable in addition to a VLAN identifier is needed. The mobility domain may provide this unique correspondence.

The mobility domain is an identifier defining the domain within which a VM may be moved or migrated from one physical server to another physical server. Thus the mobility domain is an identifier for a set of VLANs. The mobility domain s movement constraints may be imposed by logical clusters groups of VLANs such that a VM may not be able to be moved outside of a defined logical grouping of VLANs. For example although there may be two linked vCenters and thus duplicate VLAN numbers which may have access to a given segment ID a given virtual machine may only be moved within one vCenter. Thus a vCenter identifier such as a vCenter IP address may be used as the mobility domain which along with a VLAN identifier map uniquely to a given segment ID. The leaf node may provide the data packet identifier such as the VLAN identifier and mobility domain to the DCNM. The DCNM may contain logic to map the data packet identifier and mobility domain to a given segment ID and thus obtain provisioning information that it returns to the leaf switch. While the foregoing explanation considers a VM manager like VMware vCenter the same logic and mechanism may apply to other VM managers such as Microsoft Hyper V s System Center Virtual Machine Manager SCVMM Openstack Controller and others.

As modern data centers become increasingly massive and complex a need has arisen to unify the management plane to enable holistic management of the data center infrastructure. In one embodiment DCNM streamlines provisioning of the unified fabric and proactively monitors network components offering a level of visibility and control via a single graphical user interface GUI .

DCNM provides VM aware path management simplifying the management of the virtual infrastructure by enabling management of the entire path through the physical to the virtual network across the data center environment. DCNM further helps ensure the resiliency of the network infrastructure by monitoring and providing alerts for fabric availability and performance. DCNM s interactive dashboard provides intuitive views into the top fabric users with the capability to view more details of key performance indicators KPIs . Moreover because data center management tools must scale to large and distributed data center deployments DCNM supports deployment of multiple servers will maintaining a consolidated view across distributed data centers.

Referring to illustrated therein is a system comprising a DCNM which may be used alone or in concert with one or more other network elements to create modify provision and or delete one or more tenant networks residing in a leaf spine underlay fabric . As shown in the fabric includes a plurality of leaf nodes A D each of which may be implemented as a ToR switch located in a rack unit not shown that houses one or more network elements such as physical servers A and B. Each leaf node A D is connected to each of a plurality of spine nodes A B which may be implemented using routers or switches and is configured to route communications between physical servers A B in the rack unit and other network elements. Each physical server A B may host one or more virtual switches A and B and VMs A D. Virtual switches A B and virtual machines A D may be created and run on each physical server A B on top of a hypervisor A B.

Each virtual switch A B may be configured to manage communications of VMs in particular virtual networks and or subnetworks subnets . Each virtual switch A B may be embodied by software stored and executed on the corresponding physical server A B. Thus the virtual switch A B performs functions of a physical switch device. Similarly each VM A D may be software stored and executed on the corresponding physical server A B. The VM A D is configured to exchange communications with other VMs via the fabric . It may be appreciated that any number of physical servers hosting any number of virtual switches and VMs may be present in the system . For simplicity illustrates a single physical server A associated with leaf node A and a single physical server B associated with leaf node C. Virtual switch A B may in one example manage communications of multiple VMs of the physical server.

Though not illustrated in the physical server A B associated with leaf switch A C may be housed in a rack unit or rack. Other physical servers similar to the physical server A B may also be housed in the rack. Leaf nodes A D are responsible for managing communications e.g. routing and forwarding originating from and destined for physical servers and virtual machines and virtual switches hosted by the physical servers in the rack. Hence the term top of rack ToR ascribed to leaf nodes A D. Leaf nodes A D may be used to provide redundancy and fault tolerance for communications associated with physical servers virtual machines and virtual switches in the rack. Thus leaf node A is a peer to leaf node B and vice versa. These leaf nodes are configured to communicate with a network controller unit network controller not shown in which is configured to manage communications between leaf nodes in different racks.

As stated above physical server A hosts the virtual switch A and VMs A B. The VM A may exchange communications e.g. data packets with other VMs in the network via leaf nodes A D. Each VM is a member of a tenant network which is a unique L3 subnet that may contain one or more VLANs. For example a tenant Company A may have two tiers tenant networks namely 1.1.1.0 24 and 2.2.2.0 24. A tenant network or subnet can span multiple VLANs. As the tenant network of which VM is a member it may be provisioned with certain network attributes in order to exchange data packets. For example upon instantiation a tenant network and a VM therein may be provisioned with virtual network segmentation resources for example the VM and tenant network may be associated with one or more virtual Local Area Network VLAN identifiers and a subnet identifier. In one example virtual network segmentation resources may be provisioned on a per switch or per port basis e.g. up to four thousand VLANs per switch or four thousand per port of a switch . Thus when a tenant network and VM therein are created a ToR switch may select an unused VLAN for a given segmentation assignment. The virtual segmentation resources may also include a Switch Virtual Interface SVI assignment an Access Control List ACL assignment a Quality of Service QoS assignment a Virtual Routing and Forwarding VRF assignment etc. It may be appreciated that other network information now known or heretofore contemplated may also be assigned to the VM. Each tenant network is also associated with a segment identifier segment ID which is used to uniquely identify the tenant network in the leaf spine fabric . A segment ID is a 24 bit identifier that allows 16 million unique tenant networks to be addressed. VXLAN is a specific MAC over IP UDP encapsulation scheme that also has a VNI virtual network identifier which also happens to be 24 bits. However the term segment as used herein is more generic than a VNI in that it is an identifier but it does not dictate that the encapsulation should be VXLAN or any other encapsulation scheme.

Additionally upon instantiation the VM A may be provisioned with identifier information that may include an Internet Protocol IP address a Media Access Control MAC address a port number associated with the VLAN to which it is assigned etc. Once the VM A is instantiated with resources which may also include assigning one or more network identifiers such as a subnet or VLAN to the VM s virtual network interface card vNIC the VM A becomes capable of exchanging communications with other VMs that have also been properly instantiated in the network.

In order to provision a tenant network and a VM therein at the associated leaf node on the underlay network configuration messages using a control protocol may be exchanged between a virtual switch that manages the VM and the associated leaf node to which the physical device hosting the virtual switch and VM are connected. An example control protocol for sending configuration messages is a Virtual Station Interface VSI Discovery Protocol VDP digital handshake message as defined by the Institute of Electrical and Electronic Engineers IEEE 802.1Qbg standard. VDP is a reliable protocol that enables configuration messages to be exchanged between a VSI and a ToR switch in order to provision a VM managed by the virtual switch with network resources including virtual network segmentation resources . Specifically VDP enables provisioning of network resources on physical switch ports associated with the virtual switch one of which may be associated with the VM. For example the virtual switch may have multiple physical switch ports associated with it and the VM may be configured to exchange communications via one of these ports. As VDP messages are exchanged between the virtual switch and the ToR switch the port that is associated with the VM may be provisioned with the network resources. For the sake of simplicity illustrates a VM connected to a single virtual switch and thus the VDP message exchanges herein are described in association with this example. However it should be appreciated that the techniques described herein may enable network resources to be provisioned for multiple VMs multiple virtual switches and multiple physical servers. It should also be appreciated that VDP is merely used as an example herein and that the techniques described herein are generally applicable to any handshake based provisioning protocol between a virtual switch and a ToR switch.

Referring now to depicted therein is a system including an example deployment of a Vinci IP fabric in accordance with features of one embodiment. As shown in the fabric includes two spine routers respectively designated by reference numerals A and B and four leaf nodes respectively designated by reference numerals A D. In one embodiment both spine nodes A B and leaf nodes A D are implemented using routers although switches or other network devices may alternatively or additionally be used. A DCNM is provided in the system and may be used to manage spine nodes A B and leaf nodes A D in the IP fabric as described above. In the illustrated embodiment four mobility domains designated MD MD MD and MD are configured for VM move. In the embodiment illustrated in mobility domains MD and MD are configured on leaf nodes A and B while mobility domains MD and MD are configured on leaf nodes C and D. As shown in tables A and B on leaf nodes A and B two VRFs designated VRF and VRF have been created and allocated L3 segment IDs and respectively. VRF contains a DCNM network identified by mobility domain MD and VLAN and allocated L2 segment ID while VRF contains two DCNM networks identified by mobility domain MD and VLANs and allocated L2 segment IDs and respectively. These three DCNM networks are instantiated on each of leaf nodes A and B. Similarly as shown in tables C and D on each of leaf nodes C and D two other VRFs respectively designated VRF and VRF have been created and allocated L3 segment IDs and respectively. VRF contains a DCNM network identified by mobility domain MD and VLAN while VRF contains two DCNM networks identified by mobility domain MD and VLANs respectively. These three DCNM networks are instantiated on each of the leaf nodes C and D.

In the example shown in there are five multicast groups respectively designated G G G G and G. The multicast trees of G G G G and G are represented by dashed lines A E respectively. Each L2 or L3 segment should be assigned to one of the five multicast groups G G. depicts a possible arrangement of assignments of groups to segments. For example as shown in group G is assigned to L3 segments and group G is assigned to L3 segments and group G is assigned to L2 segments and group G is assigned to L2 segments and and group G is assigned to L2 segments and . It may be recognized that given a large number of segments in an IP fabric as well as the dynamic nature of VMs the assignment of a multicast group to one or more segments should not be performed manually and or statically.

The multicast group to segment assignments illustrated in result in unnecessary flooding over the IP fabric . For example since multicast group G is shared by segments and which are present in leaf nodes A B and leaf nodes C D respectively the multicast tree A of multicast group G has to reach all four leaf nodes A D. Given that spine nodes do not perform pruning on segment ID unnecessary flooding will occur. For example segment multi destination traffic sourced from leaf node A will reach leaf nodes C and D where it will be dropped. Similarly multicast group G is shared by segments and which are present in leaf nodes A B and leaf nodes C D respectively the multicast tree C of multicast group G has to reach all four leaf nodes A D. As a result of this assignment segment multi destination traffic sourced from leaf node C will reach leaf nodes A and B where it will be dropped. The same will be true with respect to multicast groups G G and G.

In view of the foregoing it is clear that at least two issues exist to be considered regarding the assignment of multicast group to segment ID. One is the issue of how to automatically provision the assignment so that resources can be reserved and released dynamically. Another is the issue of determining the best group to assign to a segment so as to reduce unnecessary flooding of multi destination traffic over the IP fabric. In an IP fabric it is assumed there is a pool of multicast groups or more specifically multicast group addresses that are reserved to be assigned to L2 and L3 segments. The number of group addresses in the pool is limited and a group address in the pool may be shared by multiple segments. For example as previously noted in the embodiment illustrated in the pool consists of five multicast groups G G G G and G. Accordingly one embodiment is an enhancement to the existing DCNM auto configuration to support dynamic and automatic provisioning of multicast group to segment mapping. is a flowchart illustrating a process of automatic provisioning of multicast group to segment ID assignment in accordance with embodiments described herein. Referring to in step a pool of multicast group addresses reserved for assignment to L2 and L3 segment IDs is established. In one embodiment as represented in steps when a DCNM partition is added to a DCNM organization through a DCNM graphical user interface GUI a multicast group in the pool is automatically assigned to the L3 segment ID of the VRF associated with the DCNM partition. In step the assignment is pushed by the DCNM GUI to an LDAP database represented in by an LDAP database to provide arguments for VRF profiles. Additionally as represented in steps when a DCNM network identified by a mobility domain and a VLAN is added to the DCNM partition through the DCNM GUI a multicast group in the pool is automatically assigned to the L2 segment ID of the DCNM network. In step the assignments are pushed by the DCNM GUI to the LDAP database to provide arguments for interface profiles of VXLAN logical interfaces. When a new tenant is detected by a leaf node step in step the auto configuration will retrieve configuration profiles tied to the tenant s DCNM partition and DCNM network from the LDAP database . The profiles are applied to the leaf node to generate VRF configuration and VXLAN logical interface configuration which specify multicast group to segment ID assignments. The leaf node may send out Protocol Independent Multicast PIM join messages upon instantiations of VRF and VXLAN logical interface if it has not yet joined the associated multicast group. As a result the building of the fabric multicast tree for the group is triggered by tenant presence on the leaf node.

The above described procedure accomplishes automatic provisioning of multicast group to segment ID assignment. However optimal selection of a group to which to assign a given segment ID must also be considered. One approach to group selection is a round robin approach. Using this approach group addresses are selected from the pool of group addresses from the first group address sequentially through to the last group address and then the process begins again with the first group address. The assignments depicted in are achieved in a round robin fashion for a pool of five group addresses G G assuming the following sequence of adding DCNM partitions and DCNM networks 1 DCNM partition VRF assigned group G 2 DCNM partition VRF assigned group G 3 DCNM network MD VLAN assigned group G 4 DCNM network MD VLAN assigned group G 5 DCNM network MD VLAN assigned group G 6 DCNM partition VRF assigned group G 7 DCNM partition VRF assigned group G 8 DCNM network MD VLAN assigned group G 9 DCNM network MD VLAN assigned group G and 10 DCNM network MD VLAN assigned group G . As previously discussed with reference to the forgoing assignments are undesirable due to unnecessary flooding in the IP fabric.

In accordance with features of one embodiment mobility domain aware group selection is employed to reduce unnecessary flooding in the IP fabric. Specifically multicast groups are separated into two categories mobility domain specific groups and nonspecific groups. A mobility domain is assigned one or more mobility domain specific groups. If traffic in a segment is scoped by a mobility domain one of the mobility domain specific groups assigned to the mobility domain is selected for the segment. Otherwise a nonspecific group is selected. In this manner the multicast tree of a mobility domain specific group can only reach leafs that are configured with mobility domains that are assigned the group. One embodiment of mobility domain aware group selection is as follows.

A flowchart illustrating an embodiment of a process for performing mobility domain aware group selection is shown in . In step the multicast group addresses that make up the pool for the IP fabric are divided into two sub pools including a common pool and a mobility domain pool. In step with respect to the mobility domain pool if there are fewer groups than mobility domains each group is assigned to one or more non overlapping mobility domains otherwise each mobility domain is assigned one or more non overlapping groups from the mobility domain pool. Additionally a new GUI field is introduced in the DCNM GUI for adding a DCNM partition to a DCNM organization. The new field allows a user to 1 indicate whether or not the DCNM partition will contain DCNM networks of a same mobility domain and 2 if the DCNM partition will contain DCNM networks of a same mobility domain specify the mobility domain. In step when adding a DCNM partition if the user indicates yes meaning that the DCNM partition will contain DCNM networks of the same mobility domain one of the groups assigned to the mobility domain from the mobility domain pool will be selected for the L3 segment ID configured to the DCNM partition. Otherwise a group from the common pool will be selected in a round robin fashion for the L3 segment ID configured to the DCNM partition. In step when a new DCNM network is added to a DCNM partition through the DCNM GUI one of the groups assigned to its mobility domain from the mobility domain pool will be selected for the L2 segment ID configured to the DCNM network.

The above group selection process limits the multicast tree of a group in the mobility domain pool to only reach those leaf nodes configured with mobility domains to which the group is assigned. For example the multicast tree A of the group assigned to mobility domain MD G is limited to only reach leaf nodes A and B both of which are configured with mobility domain MD. Similarly the multicast tree B of the group assigned to mobility domain MD G is also limited to only reach leaf nodes A and B both of which are configured with mobility domain MD. The multicast trees C and D of the groups assigned to mobility domains MD G and MD G respectively are each limited to only reach leaf nodes C and D both of which are configured with mobility domains MD and MD. As a result unnecessary flooding is reduced for groups in the mobility domain pool.

Referring now to illustrated there is another example mobility domain aware group selection. In contrast to in the example shown in and specifically as shown in tables A D it will be assumed that VRF is associated with two DCNM networks identified by MD and MD while VRF is associated with four DCNM networks identified by MD MD MD and MD . VRF and VRF are no longer configured. Since both VRF and VRF span two mobility domains their segment IDs will be assigned the group in the common pool G. In this manner multicast trees of groups in the mobility domain pool will not be affected by VRF and VRF which are present on all four leaf nodes.

A typical case in which each DCNM network has its own unique L2 segment ID has been shown and described. However in some deployments users may choose to configure multiple DCNM networks with the same L2 segment ID that is multiple DCNM networks may be stitched together by a common L2 segment ID. For example as shown in the DCNM network identified by MD is configured with segment ID and thus stitched to the DCNM network identified by MD which is also configured with segment ID . If the stitched DCNM networks belong to different mobility domains the group selected for their common L2 segment ID should be from the common pool. Otherwise it should be from the mobility domain pool. For example as shown in and specifically as shown in tables A D since DCNM networks MD and MD are stitched group G from the common pool should be selected for their common L2 segment ID because they belong to two different mobility domains. To handle DCNM network stitching a new GUI field is introduced in the DCNM GUI that adds a DCNM network to a DCNM partition. This field allows a user to indicate whether the DCNM network being added will be stitched with other DCNM networks of different mobility domains. When adding a DCNM network if the user indicates that the network will be stitched with other DCNM networks of different mobility domains a multicast group from the common pool will be selected in a round robin fashion for the L2 segment ID configured to the DCNM network. Otherwise one of the multicast groups assigned to the mobility domain of the DCNM network will be selected for the L2 segment ID from the mobility domain pool.

Although embodiments have been shown and described using DCNM GUI they are equally applicable in cases in which the DCNM is controlled by an orchestrator e.g. OpenStack through REpresentational State Transfer REST APIs. In the orchestration case REST APIs should be enhanced to add new parameters in a way similar to DCNM GUI enhancements.

In summary the above described embodiments provide enhancements to auto configuration to support dynamic provisioning of multicast group to segment ID assignment or mapping Certain embodiments employ mobility domain aware group selection. As a result IP fabric multicast trees are built based on tenant presence on leaf nodes and unnecessary flooding in the IP fabric is significantly reduced.

In one example implementation various devices involved in implementing the embodiments described herein can include software for achieving the described functions. For example referring to the DCNM may be implemented using one or more computer devices comprising software embodied in one or more tangible media for facilitating the activities described herein. The computer device for implementing the DCNM may also include a memory device or memory element for storing information to be used in achieving the functions as outlined herein. Additionally the computer device for implementing the DCNM may include a processor that is capable of executing software or an algorithm to perform the functions as discussed in this Specification including but not limited to the functions illustrated in and described with reference to . These devices may further keep information in any suitable memory element random access memory RAM ROM EPROM EEPROM ASIC etc. software hardware or in any other suitable component device element or object where appropriate and based on particular needs. Any of the memory items discussed herein should be construed as being encompassed within the broad term memory element. Similarly any of the potential processing elements modules and machines described in this Specification should be construed as being encompassed within the broad term processor. Each of the network elements can also include suitable interfaces for receiving transmitting and or otherwise communicating data or information in a network environment.

Note that in certain example implementations the functions outlined herein and in may be implemented by logic encoded in one or more tangible media e.g. embedded logic provided in an application specific integrated circuit ASIC digital signal processor DSP instructions software potentially inclusive of object code and source code to be executed by a processor or other similar machine etc. . In some of these instances a memory element can store data used for the operations described herein. This includes the memory element being able to store software logic code or processor instructions that are executed to carry out the activities described in this Specification including but not limited to the functions illustrated in and described with reference to . A processor can execute any type of instructions associated with the data to achieve the operations detailed herein in this Specification. In one example the processor could transform an element or an article e.g. data from one state or thing to another state or thing. In another example the activities outlined herein may be implemented with fixed logic or programmable logic e.g. software computer instructions executed by a processor and the elements identified herein could be some type of a programmable processor programmable digital logic e.g. a field programmable gate array FPGA an erasable programmable read only memory EPROM an electrically erasable programmable ROM EEPROM or an ASIC that includes digital logic software code electronic instructions or any suitable combination thereof.

It should be noted that much of the infrastructure discussed herein can be provisioned as part of any type of network element. As used herein the term network element or network device can encompass computers servers network appliances hosts routers switches gateways bridges virtual equipment load balancers firewalls processors modules or any other suitable device component element or object operable to exchange information in a network environment. Moreover the network elements may include any suitable hardware software components modules interfaces or objects that facilitate the operations thereof. This may be inclusive of appropriate algorithms and communication protocols that allow for the effective exchange of data or information.

In one implementation network elements devices can include software to achieve or to foster the management activities discussed herein. This could include the implementation of instances of any of the components engines logic etc. shown in the FIGURES. Additionally each of these devices can have an internal structure e.g. a processor a memory element etc. to facilitate some of the operations described herein. In other embodiments these management activities may be executed externally to these devices or included in some other network element to achieve the intended functionality. Alternatively these network devices may include software or reciprocating software that can coordinate with other network elements in order to achieve the management activities described herein. In still other embodiments one or several devices may include any suitable algorithms hardware software components modules interfaces or objects that facilitate the operations thereof.

It should also be noted that although the examples provided above are described in the context of VXLAN the techniques described herein may be implemented in the context of numerous other types of overlay techniques capable of supporting multiple network tenants including but not limited to Network Virtualization using Generic Routing Encapsulation NVGRE in which case a 24 bit Tenant Network Identifier TNI would be employed in place of the VNID to uniquely identify tenants in the network as described above.

Note that with the example provided above as well as numerous other examples provided herein interaction may be described in terms of two three or four network elements. However this has been done for purposes of clarity and example only. In certain cases it may be easier to describe one or more of the functionalities of a given set of flows by only referencing a limited number of network elements. It should be appreciated that topologies illustrated in and described with reference to the accompanying FIGURES and their teachings are readily scalable and can accommodate a large number of components as well as more complicated sophisticated arrangements and configurations. Accordingly the examples provided should not limit the scope or inhibit the broad teachings of the illustrated topologies as potentially applied to a myriad of other architectures.

It is also important to note that the steps in the preceding flow diagrams illustrate only some of the possible signaling scenarios and patterns that may be executed by or within communication systems shown in the FIGURES. Some of these steps may be deleted or removed where appropriate or these steps may be modified or changed considerably without departing from the scope of the present disclosure. In addition a number of these operations have been described as being executed concurrently with or in parallel to one or more additional operations. However the timing of these operations may be altered considerably. The preceding operational flows have been offered for purposes of example and discussion. Substantial flexibility is provided by communication systems shown in the FIGURES in that any suitable arrangements chronologies configurations and timing mechanisms may be provided without departing from the teachings of the present disclosure.

Although the present disclosure has been described in detail with reference to particular arrangements and configurations these example configurations and arrangements may be changed significantly without departing from the scope of the present disclosure. For example although the present disclosure has been described with reference to particular communication exchanges embodiments described herein may be applicable to other architectures.

Numerous other changes substitutions variations alterations and modifications may be ascertained to one skilled in the art and it is intended that the present disclosure encompass all such changes substitutions variations alterations and modifications as falling within the scope of the appended claims. In order to assist the United States Patent and Trademark Office USPTO and additionally any readers of any patent issued on this application in interpreting the claims appended hereto Applicant wishes to note that the Applicant a does not intend any of the appended claims to invoke paragraph six 6 of 35 U.S.C. section 112 as it exists on the date of the filing hereof unless the words means for or step for are specifically used in the particular claims and b does not intend by any statement in the specification to limit this disclosure in any way that is not otherwise reflected in the appended claims.

