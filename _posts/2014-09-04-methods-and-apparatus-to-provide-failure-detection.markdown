---

title: Methods and apparatus to provide failure detection
abstract: Methods and apparatus to provide failure detection are disclosed herein. An example method to synchronize data operations between multiple workload units in a computing device to facilitate failure detection includes identifying a number of first data operations to write data from a computing node in a first workload unit to locations that are not in a local cache of the computing node and are not in a memory of the first workload unit, the first data operations corresponding to a set of computing instructions that are assigned to the first workload unit and, when a flag in the first workload unit has been set to a first value, synchronizing the first data operations with second data operations by a second workload unit.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09459949&OS=09459949&RS=09459949
owner: Intel Corporation
number: 09459949
owner_city: Santa Clara
owner_country: US
publication_date: 20140904
---
This patent arises from a continuation of U.S. patent application Ser. No. 13 538 596 filed Jun. 29 2012 now U.S. Pat. No. 8 832 505 . The entirety of U.S. patent application Ser. No. 13 538 596 is incorporated herein by reference.

This invention was made with Government support under contract number HR0011 10 3 0007 awarded by the Department of Defense. The Government has certain rights in this invention.

The present disclosure relates generally to computer system platforms and more particularly to methods and apparatus to provide failure detection.

Large scale processing utilizes many processing cores to accomplish processing tasks. The efficiencies of large scale processing machines may be measured at least in part by the amount of energy that is consumed by the machine to perform a number of instructions. Machines that use more energy to accomplish the same number of instructions are considered less efficient by a performance per energy unit measurement.

Although the following discloses example methods apparatus systems and or articles of manufacture including among other components firmware and or software executed on hardware it should be noted that such methods apparatus systems and or articles of manufacture are merely illustrative and should not be considered as limiting. For example it is contemplated that any or all of the firmware hardware and or software components could be embodied exclusively in hardware exclusively in software exclusively in firmware or in any combination of hardware software and or firmware. Accordingly while the following describes example methods apparatus systems and or articles of manufacture the examples provided are not the only way s to implement such methods apparatus systems and or articles of manufacture.

Current processing systems assume that data reads and data writes to memory or remote machines do not fail at the hardware level and make no provision for such reads and writes in the event a failure occurs. When a hardware error is detected a machine check exception is triggered which typically stops the entire processing system from performing any further processing until the system is rebooted. For future large scale machines such as data center machines and exascale computing systems e.g. capable of performing on the order of 1 quintillion floating point operations per second the assumption that data reads and data writes do not fail may not remain true. Current large scale machines such as the petaFLOP machines e.g. capable of performing on the order of 1 quadrillion floating point operations per second have up times e.g. lengths of time in continuous operation measured in hours or small numbers of days before significant failures occur failures that may require restarting the machine or a portion thereof . Future Exascale machines would multiply this problem by at least 1 000 times over petascale machines and may lead to short less useful up times.

Known methods for detecting data errors include providing hardware buffers for every data operation that is to communicate data to a location other than the core performing the data operation e.g. a memory another core and mandating that every such data operation receive an acknowledgement e.g. an ACK message or a nonacknowledgement e.g. a NAK message to indicate a success or failure respectively of the data operation.

Example systems methods and articles of manufacture disclosed herein provide failure detection in computing systems by monitoring expected and observed data operations within a threshold time period. An example method includes executing via a plurality of computing nodes first fenced computing operations storing a count of issued data operations resulting from the first fenced computing operations and determining whether a failure condition exists in the plurality of computing nodes by comparing the count of issued data operations to a count of performed data operations resulting from the first fenced computing operations.

Example systems methods and articles of manufacture disclosed herein overcome the disadvantages of prior art methods by reducing the amount of hardware e.g. circuitry processor features etc. used to perform the failure detection. For machines having large numbers of processors e.g. petascale machines exascale machines etc. the hardware requirements of the prior art can become large and use significant amounts of power and therefore reduce the performance per watt efficiency of such machines. In contrast example systems methods and articles of manufacture disclosed herein may be implemented in software in combination with timers and counters implemented in hardware and thus are more energy efficient than prior art solutions.

The example computing system of includes multiple interconnected computing nodes . The computing nodes of the illustrated example are configured to execute groups of computing operations defined by fences. As used herein a fence refers to a logical barrier in instruction flow such that instructions logically subsequent to the fence e.g. occurring after a fence marker in an execution order are not permitted to be executed until the prior operations e.g. data operations from the instructions occurring prior to the fence have completed. Computing instructions may be contained within a fence to cause the computing nodes executing the fenced instructions to wait until the fenced instructions have been completed before advancing to further computing operations. Fences can guard data read operations data write operations and or other types of traffic and or control streams depending on the type of fence and or the instruction set of the system . A fence may affect one or more computing nodes. In some examples a fence causes the nodes within the fence to monitor and verify data operations e.g. data writes to local storage data writes to remote storage data reads from local storage data reads from remote storage to ensure the correctness of data operations. In the example of the computing nodes are configured in a first workload unit and the computing nodes are configured in a second workload unit . A workload unit refers to a physical and or logical grouping of multiple processing nodes for the purpose of accomplishing a processing task. However the computing nodes are reconfigurable to use any combination s of the nodes as one or more workload unit s to perform computing operations. The terms workload unit workload group computing node tree and tree are used interchangeably throughout this disclosure.

The example computing system of further includes a memory . The memory is a shared memory and may include multiple physical and or logical memories. The computing system may include additional memories. The memory and or portions of the memory may be reserved to a workload unit while performing a set of computing operations e.g. to assure data integrity and consistency of the computing instructions . In some examples the workload unit controlling the memory synchronizes the data operations after a group of fenced computing operations has been performed.

For some sets of computing operations a guarantee that data operations e.g. reading data from the memory and or writing data to the memory have been performed is used to ensure that subsequent computing instructions are being performed using the proper data. If the guaranteed data operations fail and or are not confirmed the computing operations and any resulting data operations may be reversed and or otherwise corrected.

An I O controller performs functions that enable the computing nodes to communicate with peripheral input output I O devices and a network interface . The I O controller may interface with any desired type of I O device such as for example a keyboard a video display or monitor a mouse etc. The network interface may be for example an Ethernet device an asynchronous transfer mode ATM device an 802.11 device a DSL modem a cable modem a cellular modem etc. that enables the processor system to communicate with another processor system. A mass storage memory may include any desired type of mass storage device including hard disk drives optical drives tape storage devices etc.

The example computing system of includes one or more interconnection buses to communicatively couple the nodes the workload units the memory the I O controller the network interface and or the mass storage device . The example nodes may transmit configuration information via the interconnection bus es to for example configure and or reconfigure the workload units . The example nodes additionally transmit data to other ones of the nodes to the memory and or to the mass storage device . Because the memory is coupled to multiple ones of the nodes data conflicts and or failures may arise and or may be caused by hardware failures in the computing system . When such data conflicts and or failures occur while performing computing operations the example computing system detects the failures and or conflicts and performs corrective measures as described in more detail below.

The example nodes the example memory the example I O controller the example network interface the example mass storage device and or any other components in the computing system of are interconnectable and logically separable to accomplish multitasking multithreading and or any other type of specialized computing via the computing system . The examples described below will refer to the example workload unit for example sets of fenced computing operations. However the examples described herein are applicable to other sets of fenced computing operations other workload units e.g. the workload unit other sizes and or compositions of workload units and or any other configuration of the example computing system .

In a first phase of an example failure detection process by the computing system the tree manager node iterates e.g. loops a process of monitoring the nodes in the workload unit monitoring for the nodes to have reported a status. In some examples the nodes report a status by writing e.g. storing a value to a designated location e.g. a register in the memory . The tree manager node maintains a sum e.g. count of the total writes performed by the nodes . When the sum is determined e.g. each of the nodes has reported a value that must be correct because the sum is determined from each node s fenced operations the tree manager node then resets the nodes in the tree to a value e.g. 1 . The tree manager node further sets a flag e.g. a variable accessible to the nodes in the unit e.g. to true . In a second phase of the example failure detection process the nodes respond by determining respective numbers of performed remote data write operations resulting from the performed computing operations. The tree manager node compares the count of data operations observed by the nodes e.g. performed data operations in the second phase to the number of total data operations issued by the nodes in the first phase. If the number of observed data operations is equal to the number of issued data operations the tree manager node reports that the data operations are synchronized and the workload unit may perform subsequent computing operations. On the other hand if a watchdog timer interrupt occurs prior to the tree manager node determining that the number of observed data operations is equal to the number of issued data operations the tree manager node determines that a failure condition exists and initiates corrective action.

Each of the example nodes includes a respective watchdog timer a respective processing core a respective cache and a respective performance manager . The example watchdog timers count down and or up and may be set and or reset to raise an interrupt or other event upon expiration of a threshold time. The threshold time may be set via computing instructions provided by a programmer. The example cores are processing cores that perform computing operations e.g. execute computer instructions and perform input and output e.g. to the cache . The caches store data for access by the respective cores . The performance monitors count data operations e.g. data read operations from local and or remote locations data write operations to local and or remote locations and generate interrupts in response to triggering events e.g. when the count of data operations changes .

The example nodes collectively have access to a memory . Access to the memory of may be reserved to the example workload unit during one or more sets of instructions and then unreserved or reserved to a different computing node tree. Additionally the example workload unit includes a flag . The tree manager node may set the flag to a value as described below. In the example of the flag represents a Boolean variable e.g. has a value of true or false . The example nodes read the state of the flag and perform actions based on the state as described in more detail below. A tree counter stores a count of a number of data operations issued by the nodes while performing a fenced set of computing instructions.

In some examples the value s to which the local watchdog timer s are set are based on a time within which a set of computing operations is expected to be executed barring any failures.

Upon receiving a set of fenced computing operations the example nodes participating in the workload unit attempt to drain e.g. perform any pending data operations to establish a known state for the workload unit . To establish the known state each of the example nodes in the workload unit performs the following 1 set a watchdog timer to a first value 2 execute fenced computing operations while monitoring data operations if a watchdog timer trigger interrupt occurs a resiliency failure may be implied by the node in response to the watchdog trigger interrupt a failure correction or recovery task e.g. a higher privilege function a check point roll back self diagnostics etc. is performed 3 if the fenced computing operations complete before the watchdog timer is triggered e.g. expires reset the watchdog timer to the first value or to a different value and 4 perform a synchronization process.

Each example node has an assigned location in the memory . In the example of each assigned location is a register capable of storing a signed 64 bit integer in memory but may be a different size or type. When the workload unit is formed e.g. when the nodes receive instructions to form the workload unit each location is initialized with a value. In some examples the memory locations are initialized with a value of 1. The example nodes begin performing the set of computing operations as the operations are assigned to the nodes . While performing the computing operations the performance monitors monitor the numbers of data operations e.g. one or more types of data operations issued by the cores as a result of performing the computing operations. After each node completes the computing operations assigned to it the node executes a fence operation sequence. If the watchdog timers of the nodes are not triggered the nodes may be instructed to synchronize its data operations with other nodes before the fence is released. A fence is not released until each node in the example workload unit has synchronized.

To perform the synchronization sequence each example node 1 writes e.g. stores to its assigned location a number of data operations e.g. data reads and or data writes that the node performed while executing the set of computing operations and 2 moves to a spin wait e.g. idle state until the flag is set to a value.

If the example tree manager node has any computing operations to perform the tree manager node performs those computing operations before performing tree manager functions. When the tree manager node completes any computing operations assigned to it the tree manager node resets the watch dog timer . The example tree manager node repeatedly monitors the other nodes in the workload unit to determine whether the nodes have reported a status. In the example of the nodes report a status by writing a number of data operations to an assigned location in the memory . The example tree manager node determines a sum of the total write operations performed by the nodes . When the nodes in the workload unit have reported corresponding numbers of data operations the tree manager node stores the sum in the tree counter . The tree manager node then resets the values stored at the locations for the nodes in the workload unit to an initialized value e.g. 1 . The example tree manager node sets the flag to a value e.g. true .

At the time the tree manager node sets the flag the tree counter stores a sum of the data operations e.g. data write operations that have occurred e.g. been issued based on executed computing operations within the workload unit . When the tree manager node sets the flag e.g. to true the other nodes in the workload unit enter a second phase and begin reporting a number of data operations that the nodes observe being performed. If the computing operations and the resulting data operations are completed successfully the tree manager node identifies the number of data operations reported by the nodes in the second phase as matching the sum of the data operations stored by the tree manager node in the tree counter .

To report the number of observed data operations in the second phase the example nodes each perform the following 1 set a trigger value for the performance monitor to a trigger threshold e.g. 1 and set an alarm or interrupt signal to activate when any remote generated e.g. non local interconnect delivered data operation e.g. data write arrives at the node 2 determine a count of remote generated data operations observed by the performance monitor of the node using an atomic exchange operation to atomically e.g. simultaneously from the perspective of other components of the workload unit reset the count of the performance monitor e.g. to 0 3 write to the assigned location for the node the number of remote generated data operations the node has observed since the beginning of the most recent set of computing instructions and 4 move to a spin wait e.g. idle state until the flag is set to a value e.g. true .

While the nodes perform the above steps the tree manager node monitors the locations in the memory corresponding to the nodes . The tree manager node determines a sum of the data operations reported by the nodes e.g. an observed value and compares the sum to the stored value in the tree counter e.g. an expected value . While the reported e.g. observed data operations do not match the sum of the data operations issued by the nodes during performance of the computing operations then there are still data operations to be performed somewhere in the computing system e.g. in flight data operations and or a failure has occurred somewhere in the system. While the observed value does not equal the expected value any remote data operation at a node that has not been previously observed triggers the corresponding performance monitor trigger event that was previously set. In response to a trigger of the performance monitor the node corresponding to the triggered performance monitor updates the value at the assigned location for that node in the memory . In some examples the core updates the value at the assigned location while in some other examples the performance monitor updates the value. The node also resets the performance monitor event trigger and places the node back into the spin wait e.g. idle mode. In some examples each of the nodes iterates second phase steps 2 4 above while the flag is not set e.g. while the observed value is not equal to the expected value .

If any of the watchdog timers of any of the nodes elapses before the count of observed data operations e.g. writes matches the sum of the data operations from the computing operations e.g. the expected value the nodes determine and or otherwise identify that a failure has occurred. In response to determining a failure the example nodes may initiate recovery and or error correction procedures e.g. diagnostics checkpoint replays any appropriate method s for the particular system . On the other hand if the count of observed data operations matches the sum of data operations from the computing operations e.g. the expected value the tree manager node sets the flag to indicate that the computing operations have become visible and that no failures have occurred. The flag further indicates that the workload is allowed to resume forward progress e.g. a new set of computing operations may be undertaken by the node s and or the workload unit .

In some examples the nodes update respective locations in the memory with a location e.g. an address of a register to wake up that specific node . In other examples the locations in the memory are the same respective locations in memory e.g. the same 64 bit registers used by the nodes to update the numbers of data operations. Once a node has written a number of data operations into the tree the node may then choose to clock gate itself off directly and or via request to higher privilege layers . The tree manager node may then wake up the nodes when the workload unit has reported a number of data operations and or to return control to the nodes e.g. to perform individual node processing to perform post synchronization overhead etc. . In such examples the performance monitors are operational in clock gated behavior. Additionally the performance monitors are able to wake up the respective nodes e.g. if incoming memory operations arrive for service .

In some examples in which explicit guarantees of behavior within a macro block of nodes e.g. a block of nodes different from the workload unit and including one or more of the nodes or larger subset of the system are provided by hardware a manager of the macro block is included in the workload unit . The block manager observes the total numbers of data operations included in and out of the block of nodes by monitoring e.g. accessing and or summing the ingress tables and or egress tables within the block. The performance monitors located at the edges of the block of nodes after fenced computing operations on the nodes in the block have data operation counts that accurately reflect the total data operations of the nodes in the block. The block manager enables a reduction in the size of the workload unit and in the total messages exchanged between nodes in the workload unit .

While an example manner of implementing the computing platform of has been illustrated in one or more of the elements processes and or devices illustrated in may be combined divided re arranged omitted eliminated and or implemented in any other way. Further the example nodes the example watchdog timers the example cores the example caches the example performance monitors the example memory the example flag the example tree counter and or more generally the example workload unit of may be implemented by hardware software firmware and or any combination of hardware software and or firmware. Thus for example any of the example nodes the example watchdog timers the example cores the example caches the example performance monitors the example memory the example flag the example tree counter and or more generally the example workload unit of could be implemented by one or more circuit s programmable processor s application specific integrated circuit s ASIC s programmable logic device s PLD s and or field programmable logic device s FPLD s etc. When any of the appended apparatus claims are read to cover a purely software and or firmware implementation at least one of the example nodes the example watchdog timers the example cores the example caches the example performance monitors the example memory the example flag and or the example tree counter are hereby expressly defined to include a tangible medium such as a memory DVD CD etc. storing the software and or firmware. Further still the example nodes the example watchdog timers the example cores the example caches the example performance monitors the example memory the example flag the example tree counter and or more generally the example workload unit of may include one or more elements processes and or devices in addition to or instead of those illustrated in and or may include more than one of any or all of the illustrated elements processes and devices.

Alternatively some or all of the example processes of may be implemented using any combination s of application specific integrated circuit s ASIC s programmable logic device s PLD s field programmable logic device s FPLD s discrete logic hardware firmware etc. Also some or all of the example processes of may be implemented manually or as any combination s of any of the foregoing techniques for example any combination of firmware software discrete logic and or hardware. Further although the example processes of are described with reference to the flow diagrams of other methods of implementing the processes of may be employed. For example the order of execution of the blocks may be changed and or some of the blocks described may be changed eliminated sub divided or combined. Additionally any or all of the example processes of may be performed sequentially and or in parallel by for example separate processing threads processors devices discrete logic circuits etc.

The example process begins with the nodes executing first fenced computing operations block . Example fenced computing operations include computing operations in which data operations e.g. reads from the memory writes to the memory are monitored and or computing operations in which a computing resource e.g. the memory is reserved to the nodes . The example node e.g. the tree manager node stores a count of data operations resulting from the first fenced computing operations e.g. an expected count block . Storing the count may include for example temporarily storing the count in a cache or other short term memory and or storing the count in a mass storage device.

The example tree manager node determines whether a count of data operations e.g. the expected count a count of attempted data operations is equal to a count of performed data operations e.g. an observed count block . For example the tree manager node may determine a count of performed data operations observed by the nodes and compare the count of performed data operations e.g. the observed count to the stored count of expected data operations. If the count of expected data operations is not equal to the count of performed e.g. observed data operations block the example tree manager node determines that a failure condition exists block . On the other hand if the tree manager node determines that the expected count of data operations is equal to the count of performed e.g. observed data operations block the tree manager node reports a data synchronization block . A data synchronization may for example signal that the workload unit has successfully executed a set of computing instructions and successfully performed data operations associated with the computing instructions.

The example process begins by setting a watchdog timer e.g. the watchdog timer of block . The watchdog timer may be set to a value that enables the core to complete a set of computing instructions. The example node e.g. via the core performs computing instructions block . For example the node may be assigned a subset of computing instructions that are assigned to the workload unit .

The example performance monitor reads a number of remote write data operations block . For example the core increments a counter for the node e.g. a counter in the cache and or a counter in the performance monitor . The example performance monitor may read the value in the counter and or reset the counter. The performance monitor stores the count of remote write data operations block . For example the performance monitor stores the count of writes to an assigned location in the memory of .

The example performance monitor determines whether a flag is set block . For example the node may enter a spin wait or idle state while the performance monitor determines whether the flag of has been set e.g. to a value true by the tree manager node . If the flag is set block the example node performs synchronization e.g. data synchronization block . An example process to perform the synchronization is described below with reference to .

If the flag has not been set block the example performance monitor determines whether the watchdog timer has expired block . If the watchdog timer has expired block the performance monitor reports a potential failure condition block . The potential failure condition may result in determining whether the watchdog timer was not set appropriately and or may result in error correcting failure reporting and or recovery actions being taken by the node and or by a computer system of which the node is a part.

If the watchdog timer has not expired block control returns to block to determine if the flag is set. The example node may therefore remain in a spin wait mode while waiting for the flag to be set or for the watchdog timer of the node to expire.

The example process begins by setting a watchdog timer e.g. the watchdog timer of block . The watchdog timer may be set to a value that enables the data operations from previously executed computing instructions to be successfully performed. The example performance monitor reads a number of remote write operations e.g. writing data to locations other than the cache or the memory block . The number of remote write operations may be stored in a location such as the cache or in a register in the performance monitor and incremented by the core when a remote write operation is performed or observed by the core . The number of remote write operations is monitored by the performance monitor and stored in a location such as the cache or in a register in the performance monitor . The performance monitor stores the number of writes block . For example the performance monitor may store the number of writes in an assigned e.g. designated location in the memory . In the example of the example node is assigned a particular signed 64 bit register in the memory to which the performance monitor stores the number of writes.

The example performance monitor of the node determines whether the remote write count has changed since the count was last checked by the performance monitor block . For example the performance monitor may check the remote write count e.g. in the cache in a register etc. periodically aperiodically at particular times in response to events and or otherwise non continuously. If the remote write count has changed block control returns to block to store the updated number of writes e.g. in the assigned location in memory . In the example process the performance monitor overwrites the location in memory with the updated count of remote writes. In some examples the performance monitor issues an interrupt to cause the core to store and or update the count of remote writes.

If the remote write count has not changed block the example performance monitor determines whether the flag has been set block . For example the flag is set when the tree manager node for the workload unit to which the node belongs determines that the count of data operations performed is equal to the number of data operations that were issued during execution of a set of computing instructions. If the flag has been set block the example node reports a data synchronization block . In some examples the node reports the data synchronization by becoming available to receive a configuration and or to execute further computing instructions.

If the flag is not set block the example performance monitor determines whether the watchdog timer for the node has expired block . If the watchdog timer has expired block the example performance monitor reports a potential failure condition block . The potential failure condition may result in determining whether the watchdog timer was not set appropriately and or may result in error correcting failure reporting and or recovery actions being taken by the node and or by a computer system of which the node is a part. If the watchdog timer has not expired block control returns to block to determine whether the remote write count has changed.

The example process begins by setting a watchdog timer block . For example the watchdog timer is set to a value that permits the core to perform any computing instructions that are assigned to the tree manager node for execution. The example tree manager node performs computing work assigned to the tree manager node as a computing node block . For example the node executes computing instructions in addition to acting as the tree manager for the workload unit .

The example node then resets the watchdog timer block . The watchdog timer may be reset to a value that enables the nodes in the workload unit to perform assigned computing instructions and to report respective numbers of data operations. The example tree manager node e.g. via the performance monitor and or via the core checks the locations assigned to the computing nodes to determine counter values block . For example the tree manager node may check each location in memory that is assigned to a node to determine whether any of the nodes has not reported a counter value e.g. block of .

The example tree manager node determines whether all of the computing nodes e.g. in the workload unit have reported data operations block . For example the tree manager node may determine whether any locations assigned to the nodes in the memory still have a default value e.g. 1 or another value that indicates a node has not modified the value . If all of the computing nodes in the workload unit have reported the data operations block the example tree manager node sums the counter values of all the computing nodes performing fenced computing operations to determine a count of issued data operations block . The count of data operations performed in block may additionally or alternatively be considered a number of expected data operations. In some examples the count of issued data operations includes particular type s of data operations such as remote data write operations and excludes other type s of data operations. In some examples the tree manager node stores the determined count of issued data operations in the tree counter .

Having determined the count of issued data operations the example tree manager node resets the node counters block . For example the tree manager node may reset the locations assigned to the nodes in the memory to a default e.g. initial value such as 1. The example tree manager node then resets the flag block . Resetting the flag is observed by the nodes e.g. block of to cause the nodes to perform a data synchronization.

If any computing nodes have not reported data operations block the tree manager node determines whether the watchdog timer has expired block . If the watchdog timer has expired block the example tree manager node reports a potential failure condition block . The failure condition may cause an evaluation of the watchdog timer to determine whether an appropriate value was set and or may result in error correcting failure reporting and or recovery actions being invoked.

The example process begins by setting a watchdog timer e.g. the watchdog timer of block . The watchdog timer may be set to a value that enables the data operations from previously executed computing instructions to be successfully performed. The example tree manager node determines performed e.g. observed data operations from the computing nodes block . For example the tree manager node may monitor the locations assigned to the nodes in the memory . The tree manager node sums the performed data operations to determine a count of performed data operations block . For example the tree manager node may determine the count of performed data operations by summing the values stored in the locations assigned to the nodes in the memory . The count of performed data operations may be considered the observed operations. In some examples the tree manager node updates the count of performed data operations based on updated values in the locations 

The example tree manager node determines whether the count of performed e.g. observed data operations e.g. block is equal to a count of issued data operations e.g. the count determined in block of a value stored in the tree counter of block . For example the core or the performance monitor of the tree manager node may compare the count of performed data operations to a value stored in the tree counter . If the count of count data operations is equal to the count of issued data operations block the tree manager node reports that data operations are synchronized block . The example workload unit proceeds to a next set of computing operations block . In some examples block may be performed by setting the flag to a value e.g. true . The next set of computing operations may be performed by the same workload unit or by one or more of the nodes in the workload unit as part of the same workload unit or a different workload group.

If the count of performed data operations is not equal to the count of data operations block the example tree manager node determines whether the watchdog timer e.g. the watchdog timer of the tree manager node has expired block . If the watchdog timer has not expired block control returns to block to determine the numbers of performed data operations from the computing nodes . If the watchdog timer has expired block the example tree manager node reports a potential failure condition block . The failure condition may cause an evaluation of the watchdog timer to determine whether an appropriate value was set and or may result in error correcting failure reporting and or recovery actions being invoked.

Example methods include executing via a plurality of computing nodes first fenced computing operations storing a count of issued data operations resulting from the first fenced computing operations and determining whether a failure condition exists in the plurality of computing nodes by comparing the count of issued data operations to a count of performed data operations resulting from the first fenced computing operations. Some example methods further include setting a watchdog timer to a first value and determining that the failure condition exists when the watchdog timer reaches a threshold value prior to the count of issued data operations being equal to the count of performed data operations. In some example methods the data operations include at least one of reading data from a storage remote to the plurality of computing nodes or writing data to the storage remote to the plurality of computing nodes.

Some example methods further include causing at least one of the plurality of computing nodes to enter an idle state prior to monitoring the count of performed data operations and setting a flag when the plurality of computing nodes have performed the first fenced computing operations the monitoring to occur in response to setting the flag. In some examples storing the count of issued data operations includes determining that each of the plurality of computing nodes has stored a respective number of issued data operations resulting from a respective portion of the first fenced computing operations where the count includes a sum of the respective numbers of the issued data operations from the plurality of computing nodes. Some example methods further include monitoring the count of performed data operations by identifying a change in a second count of performed data operations for a first one of the plurality of computing nodes storing the second number in response to the change and updating the count of performed data operations based on the second number. Some example methods further include reporting that a data synchronization has occurred when the count of issued data operations is equal to the count of performed data operations.

Disclosed example systems include a plurality of first computing nodes to cooperate to perform first fenced computing operations to store respective first numbers of issued data operations resulting from the first fenced computing operations and to monitor second numbers of performed data operations resulting from the first fenced computing operations and a second computing node to determine a count of issued data operations based on the first numbers of issued data operations to determine a count of performed data operations based on the second numbers and to determine whether a failure condition exists by comparing the count of issued data operations to the count of performed data operations. Some example systems further include a memory where the first numbers of issued data operations include at least one of data read operations from the memory by the computing nodes or data write operations to the memory by the computing nodes.

In some example systems the count of performed data operations includes at least one of data read operations from the memory by third computing nodes other than the first computing nodes or the second computing node or data write operations to the memory by the third computing nodes. In some example systems the first plurality of computing nodes are to store the respective first numbers of issued data operations in respective locations in the memory.

In some examples a first one of the first plurality of computing nodes includes a processing core to identify a performed data operation and a performance monitor to in response to the processing core identifying the data operation perform at least one of storing one of the second numbers of performed data operations in a designated memory location or updating the one of the second numbers of performed data operations in the designated memory location. In some example systems a first one of the first plurality of computing nodes includes a processing core to perform a computing operation and to issue one or more data operations based on the computing operation and a performance monitor to in response to the processing core issuing the data operation determine a number of the one or more data operations and store the determined number in a designated memory location.

Some example systems further include a flag where the second computing node is to set the flag to a value in response to determining the count of issued data operations and the plurality of first computing nodes are to monitor the second numbers of performed data operations in response to the second computing node setting the flag to the value. In some examples the second computing node is to set the flag to the value or a second value in response to comparing the count of issued data operations to the count of the performed data operations the plurality of first computing nodes to determine that a data synchronization has occurred in response to the second computing node setting the flag to the value or the second value.

Disclosed example tangible computer readable storage media store instructions that when executed cause a machine to at least execute via a plurality of computing nodes first fenced computing operations store a count of issued data operations resulting from the first fenced computing operations and determine whether a failure condition exists in the plurality of computing nodes by comparing the count of issued data operations to a count of performed data operations resulting from the first fenced computing operations. In some examples the instructions further cause the machine to set a watchdog timer to a first value and determine that the failure condition exists when the watchdog timer reaches a threshold value prior to the count of issued data operations being equal to the count of performed data operations.

In some examples the data operations include at least one of reading data from a storage remote to the plurality of computing nodes or writing data to the storage remote to the plurality of computing nodes. Some example instructions further cause the machine to cause at least one of the plurality of computing nodes to enter an idle state prior to monitoring the count of performed data operations and set a flag when the plurality of computing nodes have performed the first fenced computing operations the monitoring to occur in response to setting the flag. In some examples storing the count of issued data operations includes determining that each of the plurality of computing nodes has stored a respective number of data operations resulting from a respective portion of the first fenced computing operations where the count includes a sum of the respective numbers of the data operations from the plurality of computing nodes.

In some examples monitoring the count of performed data operations includes identifying a change in a second count of performed data operations for a first one of the plurality of computing nodes storing the second number in response to the change and updating the count of performed data operations based on the second number. In some examples the instructions further cause the machine to report that a data synchronization has occurred when the count of issued data operations is equal to the count of performed data operations.

Although certain methods apparatus and articles of manufacture have been described herein the scope of coverage of this patent is not limited thereto. To the contrary this patent covers all methods apparatus and articles of manufacture fairly falling within the scope of the appended claims either literally or under the doctrine of equivalents.

