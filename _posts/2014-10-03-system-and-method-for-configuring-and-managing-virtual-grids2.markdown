---

title: System and method for configuring and managing virtual grids
abstract: Some embodiments provide a hosting system for provisioning and managing servers (e.g., virtual servers, dedicated servers). In some embodiments, the hosting system receives a configuration for one or more servers for a particular entity (e.g., customer, user). The hosting system then identifies a grid identity and a set of virtual local area network (VLAN) identities for the particular entity. The hosting system then deploys the set of servers on one or more of the hardware nodes using the grid identity and the set of VLAN identities. In some embodiments, the set of VLAN identities includes a first VLAN identity for a private network and a second VLAN identity for a public network.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09647854&OS=09647854&RS=09647854
owner: GOGRID, LLC
number: 09647854
owner_city: Jersey City
owner_country: US
publication_date: 20141003
---
Hosting services provide a means whereby multiple users can implement custom server configurations without the overhead costs associated with purchasing upgrading and maintaining the equipment needed to implement the configuration. In some cases a hosting service provider maintains and provisions a grid of hardware nodes that are shared amongst the multiple users. More specifically resources of a single node can be partitioned and each of these partitions can be allocated to host a server configuration of a different user.

Virtualization provides the means for partitioning the hardware resources amongst the multiple server configurations. Virtualization creates the fa ade that each server configuration is individually hosted on dedicated equipment with a particular set of resources. Two or more server configurations are provided non conflicting sets of resources of the same hardware node such that a guaranteed amount of processing resources is available to each such configuration. In other words a single physical resource is partitioned to operate as multiple logical resources.

In some cases a hosting service may lease dedicated equipment for users to implement their custom server configurations. The dedicated equipment in some instances may provide higher reliability increased performance and greater security as its hardware resources are not shared amongst multiple users. For instance dedicated servers may be ideal for running applications that users do not want on a multi tenant environment. One example of such an application is a database application that requires Payment Card Industry PCI Data Security Standard compliance.

To facilitate the hosting services users typically place orders for hardware configurations requiring certain functionality. Users fill out forms or place telephone calls to specify their configurations. At the hosting service site system operators review the requests and manually determine which nodes or dedicated equipment to distribute the configurations. The operators then configure the nodes or equipment and install software as specified within the order requests.

In some cases a hosting service may include multiple grids supporting server configurations for different users. However limitations of virtual local area network VLAN protocol e.g. 802.1Q may cause problems when deploying network configurations of servers on one switched network. For instance the VLAN protocol may specify that a VLAN identification IDs includes 12 bits of data. This limits the maximum number of unique VLAN IDs to around 4096 2 12 per switched network. As a result once the available VLAN IDs are utilized the servers of different users may not be able to be bridged on to the same switched network as it will break the logical division of the users network configurations.

Reserving one or more switches for servers on a per grid basis adversely affects scalability manageability and capacity planning and results in suboptimal resource utilization. Furthermore the problem of configuring and managing separate network switches for different grids may escalate as new grids are added to the hosting service. Similarly reserving a hardware node for servers on a per grid basis adversely affects scalability manageability and capacity planning and results in suboptimal resource utilization.

Some embodiments provide a hosting system for provisioning and managing servers e.g. virtual servers dedicated servers . In some embodiments the system includes a front end user interface UI that allows user to configure provision and control virtual and dedicated servers through UI elements. For instance the front end UI may include different UI controls that can be used to define configurations for a dedicated server. Examples of such configurations include hardware specifications e.g. memory CPU storage image specifications e.g. operating system applications network specifications e.g. IP address etc.

When a server configuration is received through the front end UI the hosting system in some embodiments sends the server configuration to its back end logic and automatically deploys the server configuration. In some embodiments the back end portion of the system includes different deployment managers that perform different provisioning tasks. For example a virtual server may be logically partitioned and configured on a particular node in a grid of hardware resources through one deployment manager while a dedicated server may be configured through another different deployment manager. In addition one datacenter at a first location may have a different set of deployment managers than another datacenter at a second location.

To interface with different types of deployment managers the hosting system of some embodiments includes a resource management module. In some embodiments the resource management module 1 receives a user request from the front end UI 2 identifies a deployment manager that can fulfill the user request and 3 sends the user request to the identified deployment manager. The resource management module may also identify a datacenter location of the deployment manager.

In some embodiments the hosting system receives a configuration for one or more servers for a particular entity e.g. customer user . The hosting system then identifies a grid identity and a set of virtual local area network VLAN identities for the particular entity. The hosting system then deploys the set of servers on one or more of the hardware nodes using the grid identity and the set of VLAN identities. In some embodiments the set of VLAN identities includes a first VLAN identity for a private network and a second VLAN identity for a public network.

In some embodiments the hosting system determines whether a grid identity has been assigned to the customer and assigns the grid identity upon determining that a grid identity has not been assigned. The hosting system of some embodiments determines whether private and public VLAN identities have been assigned to the customer. To make this determination the hosting system may access a customer database that stores data for the customer. When the determination is made that the VLAN identities have not been assigned to the customer the hosting system assigns the VLAN identities and marks those VLAN identities as identities that are not available for other customers.

As mentioned above the VLAN protocol places a limit on the number of available VLANs. In some embodiments the hosting system extends the number of available VLANs beyond the limitations of the VLAN protocol by using another protocol. Different embodiments can use different protocols to extend the range of available VLANs. For example some embodiments utilize an 802.1QinQ protocol. In some such embodiments the grid identity is used as the outer VLAN tag and the VLAN identity e.g. public VLAN identity or private VLAN identity is used as the inner VLAN tag. In some embodiments the hosting system defines a universal hardware node based on its network architecture. To define the universal hardware node the hosting of some embodiments performs a mapping of the grid identity and the VLAN identity e.g. the public VLAN identity or private VLAN identity to a VLAN identity of a switch that is coupled to the node.

The preceding Summary is intended to serve as a brief introduction to some embodiments of the invention. It is not meant to be an introduction or overview of all inventive subject matter disclosed in this document. The Detailed Description that follows and the Drawings that are referred to in the Detailed Description will further describe the embodiments described in the Summary as well as other embodiments. Accordingly to understand all the embodiments described by this document a full review of the Summary Detailed Description and the Drawings is needed. Moreover the claimed subject matters are not to be limited by the illustrative details in the Summary Detailed Description and the Drawings but rather are to be defined by the appended claims because the claimed subject matters can be embodied in other specific forms without departing from the spirit of the subject matters.

In the following description numerous details are set forth for the purpose of explanation. However one of ordinary skill in the art will realize that the invention may be practiced without the use of these specific details. In other instances well known structures and devices are shown in block diagram form in order not to obscure the description of the invention with unnecessary detail.

Some embodiments provide a hosting system for provisioning and managing servers e.g. virtual servers dedicated servers . In some embodiments the system includes a front end user interface UI that allows user to configure provision and control virtual and dedicated servers through UI elements. For instance the front end UI may include different UI controls that can be used to define configurations for a dedicated server. Examples of such configurations include hardware specifications e.g. memory CPU storage image specifications e.g. operating system applications network specifications e.g. IP address etc.

When a server configuration is received through the front end UI the hosting system in some embodiments sends the server configuration to its back end logic and automatically deploys the server configuration. In some embodiments the back end portion of the system includes different deployment managers that perform different provisioning tasks. For example a virtual server may be logically partitioned and configured on a particular node in a grid of hardware resources through one deployment manager while a dedicated server may be configured through another different deployment manager. In addition one datacenter at a first location may have different set of deployment managers than another datacenter at a second location.

To interface with different types of deployment managers the hosting system of some embodiments includes a resource management module. In some embodiments the resource management module 1 receives a user request from the front end UI 2 identifies a deployment manager that can fulfill the user request and 3 sends the user request to the identified deployment manager. The resource management module may also identify a datacenter location of the deployment manager.

In some embodiments the hosting system receives a configuration for one or more servers for a particular entity e.g. customer user . The hosting system then identifies a grid identity and a set of virtual local area network VLAN identities for the particular entity. The hosting system then deploys the set of servers on one or more of the hardware nodes using the grid identity and the set of VLAN identities. In some embodiments the set of VLAN identities includes a first VLAN identity for a private network and a second VLAN identity for a public network.

As mentioned above the VLAN protocol places a limit on the number of available VLANs. In some embodiments the grid identity is used to extend the number of available VLANs. For example by using another protocol such as an 802.1QinQ protocol the hosting system can extend the number of available VLANs by utilizing the grid identity as an outer VLAN tag and the VLAN identity as an inner VLAN tag. In some such embodiments the hosting system maps the grid and VLAN identities e.g. public VLAN identity private VLAN identity to a switch VLAN identity.

Several more detailed embodiments of the invention are described in the sections below. Section I provides an overview of a multi server control panel according to some embodiments. Sections II provides a conceptual architecture diagram of the hosting system of some embodiments. Section III describes an example configuration of the hosting system to provide virtual grids of hardware resources. Section IV describes an example configuration of the hosting system to provide grid independent hardware nodes. Finally Section V describes a computer system which implements some embodiments of the invention.

Some embodiments provide a graphical user interface GUI that allows users to manage servers e.g. virtual servers dedicated servers . Several examples of such a GUI are given below. In several of these examples the GUI is referred to as a multi server control panel because it allows the users to configure provision and control the servers through UI elements.

In some embodiments the multi server control panel provides UI elements that allow users to provision or configure servers by specifying parameters that define or redefine the attributes of the servers. The multi server control panel of some embodiments displays representations of the servers organized into several tiers where each tier represents a layer in a server configuration. In other words each tier represents a logical application layer e.g. a load balancing layer a web server layer an application server layer a database server layer a storage layer etc. in a multi server configuration.

In the example illustrated in the display area is organized into columns that represent several tiers. The display area includes a load balancer tier a web server tier a database server tier and a storage tier . The tier organization allows a user to assess a server topology. This tier organization allows the user to scale the server topology by adding one or more servers to or deleting one or more servers from a particular tier using the multi server control panel. For example a user can scale the system topology by adding a second web server to support a first web server. The user can also scale the system topology by adding another tier e.g. by adding an application server to a multi server configuration that includes a load balancer a web server and a database .

In some embodiments this tier organization allows the user to scale the server topology by adding one or more storages e.g. cloud storages as represented by the storage tier . For instance with the multi server control a user can easily allocate a particular amount of storage that he or she intends to use and offload storage maintenance tasks to the hosting service. As a result the user does not have to buy upgrade and maintain physical storages.

Another way in which this tier organization allows the user to scale the server topology is by allowing the users to increase allocated resources e.g. memory storage bandwidth CPU for any server in the server topology. That is the user is not limited to increasing the server topology only vertically e.g. along the tier organization of the display area but may also increase the server topology horizontally by allocating additional resources for one or more servers in the server topology. Some embodiments of the multi server control panel provide UI elements that allow a user to specify one or more attributes of a server e.g. one or more attributes of a load balancer a web server an application server a database server etc . Examples of such attributes include the amount of memory the OS of the server and the name of the server.

Sections B and C below provide several more detailed examples of how a user can use the multi server control panel to configure and add servers to a server topology. In particular Section B describes adding a virtual server to the server topology and Section C describes adding a dedicated server to the server topology.

As shown in stage the object selection window has a list of selectable icons and a datacenter field . The list of selectable icons represents different server configuration components or objects e.g. server load balancer storage that a user can add to a server configuration. In the example illustrated in the list of selectable icons includes icons for a cloud server dedicated server cloud storage and load balancer. Here the cloud server represents either a web server or a database server. As will be described below by reference to in some embodiments a server is defined as a web server or database server based on the application selected for the server. For example a server may be defined as a database server when an image selected for the server includes an operating system that is preconfigured with a database application e.g. SQL server .

The datacenter field allows a user to select a datacenter to host the server configuration. In the example shown in stage the user can select either US East 1 which represents a datacenter located in the Eastern United States or US West which represents a datacenter located in the Western United States. However additional user selectable items representing other locations may be available depending on the locations of datacenters of the hosting system e.g. hosting service provider . The datacenter field may also list datacenters differently. For instance the datacenter field may list each datacenter with more specific location information such as state city street address etc.

In some embodiments the selection of a datacenter e.g. US West modifies the available selectable icons in the list of selectable icons . That is several selectable icons may be presented or removed based on the services provided by the selected datacenter. For instance a selection of a particular datacenter may cause an icon corresponding to the cloud storage to be removed from or presented in the list of selectable icons .

When the user scrolls through the object list the selected icon may be highlighted. This is shown in the fourth stage with the icon for the cloud server highlighted while the icons for the dedicated server cloud storage and load balancer are not highlighted. The user can select any of the icons in the object list e.g. by clicking on them or by scrolling to them and pressing the enter key . When the user selects the cloud server icon in the object list the user is presented with an image list window as illustrated in .

In some embodiments the cloud server is defined as a web server database server or application server based on one or more applications that is installed or preconfigured on the operating system. For example a server may be defined as a database server when an image selected for the server includes an operating system that is preconfigured with a database application e.g. SQL server . Also a server may be defined as a web server when an image having an operating system preconfigured with a web server or application server is selected for the server. Furthermore a server may be defined by default as a web server application server or database server when an operating system is not preconfigured with any application.

As shown in the first stage the image selection window includes an image list and a filter tool . The image list is an area in the window that lists all available images from which the user can choose the selected cloud server. In some embodiments the list of images represents images in one or more image repositories or libraries. The list may include images provided by the hosting service. The list may further include images provided by other users e.g. customers general public etc . Alternatively the list may include only images provided by other users in some embodiments.

In the example illustrated in several different selectable images are displayed. Several of these images include Linux distributions while others include Windows operating systems. The images are also classified as either a web server or a database server. Also several of the listed images are only available for dedicated severs and others are available for all types of servers. The list may be sequentially organized by the name of the operating system the type of server e.g. web server database server the type of operating system architecture e.g. 32 bit 64 bit price date updated and owner.

In some embodiments the images may also be organized or classified by system requirements. In other words different images may have different system requirements. These requirements may include memory storage processor etc. For instance some images may be available for a web server that has a minimum of one gigabyte of random access memory RAM . Also some images may support a maximum of sixteen gigabytes of RAM. As shown in the first stage the list is alphabetically organized by name based on a sorting tool .

The filter tool is a user interface item provided in the image selection window that allows the user to search or filter the image list based on one or more criteria. In the example illustrated in the user can filter the image list based on the name of the operating system and architecture. The user can also filter the image list based on different types of servers. For instance the image list may be filtered to only display images that are defined as a web server or database server. Also the user can reset the filter tool by selecting a reset button.

Having described the image selection window the operations of selecting an image will now be described by reference to the state of this window at the four stages . In the first stage the image list lists several images from which the user can choose the cloud server. The second stage shows the user filtering the image list based on the architecture of the operating system. Specifically a field of the filter tool is selected to reveal a drop down list of different architecture filters i.e. 32 bit 64 bit . The user chooses the 64 bit filter which causes the image list to display only those operating systems matching the filter as illustrated in the third stage .

In the third stage as the user scrolls through the list of images the selected image is highlighted. Here the user selects an image containing a Windows operating system that is defined as a web server. Lastly the fourth stage show the user s selection of the Next button to proceed with configuring the web server. Optionally the user can cancel the process of adding the web server by selecting the Cancel button . When the user selects the next button the user is presented with a cloud server form as illustrated in .

The name field allows the user to specify a descriptive name or site name e.g. Web Server 1 www.abc.com for the web server. In some embodiments the name field is automatically populated. For example when a user specifies a site name during a sign up process the name field is automatically populated with the site name provided by the user.

The description field is an optional field that allows the user to describe the web server. For example through the description field the user can input self describing information such as the date the web server was added the content of the web pages provided by the web server etc.

The IP address field allows the user to specify an IP address for the web server. In some embodiments the IP address field is implemented as a drop down menu that opens to provide a list of IP addresses that are available for a user to choose as an IP address for the web server. In some embodiments the available IP addresses are based on a specified hosting plan. For instance if a user signs up for a particular hosting plan the multi server control panel might display ten IP addresses for the servers in the configuration. However if the user signs up for a different hosting plan the multi server control panel might display twenty IP addresses for the servers. In some embodiments the IP address may be from an IP subnet allocated to a customer s virtual local area network VLAN .

The memory field allows the user to specify the amount of memory e.g. RAM in some embodiments that the user wants to allocate to the web server. Different embodiments allow the user to specify this amount differently. For instance some embodiments allow a user to enter a numerical amount for the memory. Other embodiments allow the user to enter a percentage that specifies the percentage of an overall amount of memory that the user has purchased for his entire configuration or a particular tier of his configuration. For instance a user might select a hosting plan with one hundred gigabytes of memory. In such a case a user might then enter 10 in the memory field. This entry then allocates ten gigabytes of memory to the web server. If the user subsequently changes to a different hosting plan that includes more or less memory the allocated memory for the web server is automatically adjusted to reflect the change in the hosting plan. In some embodiments this field is implemented as a pull down menu that opens to provide a list of selectable memory values from which the user can choose the web server.

Instead of or in conjunction with the memory field other embodiments might include fields for other resources in the web server form . Examples of such other resources include physical resources e.g. storage space number of CPUs CPU cycles etc. and network resources e.g. data transfer .

Having described the elements of the cloud server form the operations of configuring a web server will now be described by reference to the state of this form at the four stages . In the first stage the cloud server form displays several indications related to the previously selected options. Specifically the datacenter label indicates that the selected datacenter is US East 1 and the image label indicates that the selected image includes a Windows operating system that is 64 bit. In the first stage the name field is selected e.g. through a cursor click operation through a touch operation etc. to allow the user to input a name for the web server.

Stage two shows the cloud server form after the user has specified a name for the web server. Here the IP address field is selected to reveal a drop down list of different IP addresses from which the user can choose the web server. As the user scrolls through the list the selected IP address is highlighted. Similarly in stage three the user specifies the amount of memory to allocate to the web server using the memory field . In this example the user selects 4 GB from a drop down list of the memory field . The fourth stage shows the user s selection of the Save button to proceed with configuring the web server. Alternatively the user can cancel the process of adding the web server by selecting the Cancel button .

The graphical element includes a web server icon and a resource meter . The web server icon is a graphical representation of the web server. In some embodiments the web server icon provides an indication of the operating system installed on the web server. For instance if the user selects an operating system image that includes a particular Linux distribution the web server icon may display a representation of the particular distribution. As shown in the web server icon displays an indication that the operating system selected for the web server is a Windows operating system.

The resource meter is a meter that displays usage of several resources e.g. CPU and memory in real time. In the example illustrated in the top resource meter represents CPU usage and the bottom resource meter represent memory usage. Each meter displays the real time usage by fluctuating e.g. moving a bar within the meter in accord with the real time usage of the corresponding server. In some embodiments the fluctuating graphical display is indicative of usage of the resource at different instances in time and or is indicative of real time or near real time usage of the resource.

Also the fluctuating graphical display changes color in some embodiments when the usage of the particular resource exceeds a particular threshold. For instance in some embodiments the bar within a meter changes color when resource usage goes over a predetermined threshold for the resource. One such example is when the memory usage exceeds 50 percent of an allotted memory capacity the bottom resource meter might change from one color to another color e.g. from green to yellow . The threshold in some embodiments is an expected usage rate over a duration of time based on the amount of a particular resource that is assigned to the particular user. Hence the top and bottom meters may display different colors at different instances in time to indicate excess usage of the resource. These fluctuating meter bars and changing colors provide a quick visual indication of whether the CPU and memory is being overloaded or thrashed. Hence these icons are referred to as thrash o meters in some embodiments. Instead of or in conjunction with CPU and memory some embodiments of the multi server control panel provide real time usage of other resources. These other resources include network resources e.g. network traffic data transfer and other physical resources e.g. storage space .

The name field description field and IP address field are similar to those discussed above by reference to with respect to the cloud server form . Specifically the name field allows the user to specify a descriptive name or site name for the dedicated server. The description field is an optional field that allows the user to describe the dedicated server. The IP address field allows the user to specify an IP address for the dedicated server.

The configuration list allows the user to select or specify a hardware configuration for the dedicated server. Specifically it lists several different configurations for the dedicated server based on processor memory and storage. For instance a first configuration indicates that the dedicated server includes one multiple core processor 8 GB of memory i.e. RAM and two 320 GB RAID storages. The first configuration also includes prices for monthly or annual plans. As shown the configuration list lists several other configurations including a second and third configuration with additional processor cores memory and storage.

Alternatively or conjunctively other embodiments might allow the user to select from other resources in the configuration list . Examples of such other resources include hardware resources such as manufacturer and type of CPU CPU cycles memory type storage type etc. and network resources such as data transfer . Different embodiments allow the user to specify the dedicated server configuration differently. For instance instead of selecting a particular configuration from a list of configurations some embodiments allow a user to customize a dedicated server by selecting different hardware components. This allows the user to more gradually define the dedicated server that will be added to the server configuration. In some embodiments the configuration list is implemented as a pull down menu that opens to provide a list of selectable configurations from which the user can choose for the dedicated server.

Having described the elements of the dedicated server form the operations of configuring a dedicated server will now be described by reference to the state of this form at the four stages . In the first stage the datacenter field indicates that the selected datacenter for the dedicated server is US East 1 . Also selecting e.g. through a cursor click operation through a touch operation etc. the name field allows the user to input a name for the dedicated server.

Stage two shows the dedicated server form after the user has specified a name for the dedicated server. Here the IP address field is selected to reveal a drop down list of different IP addresses from which the user can choose an IP address. As the user scrolls through the list the selected IP address is highlighted.

In stage three the user selects a radio button corresponding to the third configuration in the configuration list . As shown in the figure the third configuration includes two multiple core processor 24 GB of memory and five 146 GB RAID storages. The fourth stage shows the user s selection of the Next button to proceed with configuring the dedicated server. In some embodiments the user can cancel the process of adding the dedicated server at any time by selecting the Cancel button . When the user selects the next button the user is presented with an image selection window as illustrated in .

In some embodiments a dedicated server is defined as a web server database server or application server based on one or more applications that are installed or preconfigured on the operating system. For example the dedicated server may be defined as a database server when an image selected for the server includes an operating system that is preconfigured with a database application e.g. SQL server . Also the dedicated server may be defined as a web server when an image having an operating system preconfigured with a web server or application server is selected as the server. Furthermore the dedicated server may be defined by default as a dedicated server application server or database server when an operating system is not preconfigured with any application.

As shown in the first stage the image selection window includes an image list and a filter tool . The image list is an area in the window that lists all available images from which the user can choose the selected dedicated server. In some embodiments the list of images represents images in one or more image repositories or libraries. The list may include images provided by the hosting service. The list may further include images provided by other users e.g. customers general public etc . Alternatively the list may include only images provided by other users in some embodiments.

In the example illustrated in several different selectable images are displayed. Several of these images include Linux distributions while others include Windows operating systems. The images are also classified as either a web server or a database server. Also several of the listed images are only available for dedicated severs and others are available for all types of servers. The list may be sequentially organized by the name of the operating system the type of server e.g. web server database server the type of operating system architecture e.g. 32 bit 64 bit price date updated and owner.

In some embodiments the images may also be organized or classified by system requirements. In other words different images may have different system requirements. These requirements may include memory storage processor etc. For instance some images may be available for a dedicated server that has a minimum of one gigabyte of random access memory RAM . Also some images may support a maximum of sixteen gigabytes of RAM. As shown in the first stage the list is alphabetically organized by name based on a sorting tool.

The filter tool is a user interface item provided in the image selection window that allows the user to search or filter the image list based on one or more criteria. In the example illustrated in the user can filter the image list based on the name of the operating system and architecture. The user can also filter the image list based on different types of servers. For instance the image list may be filtered to only display images that are defined as a web server or database server. Also the user can reset the filter tool by selecting a reset button.

Having described the image selection window the operations of selecting an image will now be described by reference to the state of this window at the four stages . In the first stage the image list lists several images from which the user can choose the dedicated server. The second stage shows the user filtering the image list based on the architecture of the operating system. Specifically a field of the filter tool is selected to reveal a drop down list of different architecture filters i.e. 32 bit 64 bit . The user chooses the 64 bit filter which causes the image list to display only those operating systems matching the filter as illustrated in the third stage .

In the third stage as the user scrolls through the list of images the selected image is highlighted. Here the user selects an image containing a Linux operating system that is defined as a web server. The fourth stage shows the user s selection of the Next button to proceed with specifying an operating system for the dedicated server. In some embodiments the user can cancel the process of specifying an operating system for the dedicated server by selecting the Cancel button .

In some embodiments when the user selects the next button the user is presented with a dialog window that inquires whether to proceed with provisioning the dedicated server. The dialog window may list the configuration settings e.g. selected hardware image datacenter etc. for the dedicated server. The dialog window may also list hosting plan details e.g. contract related pricing etc . In some embodiments the dialog window includes an accept button to confirm the provisioning request and a cancel button to cancel the request.

The graphical element includes a dedicated server icon and a resource meter . The dedicated server icon is a graphical representation of the dedicated server. In some embodiments the dedicated server icon provides an indication of the operating system installed on the dedicated server. For instance if the user selects an operating system image that includes a particular Windows operating system the dedicated server icon may display a representation of the particular operating system. As shown in the dedicated server icon displays an indication that the operating system selected for the dedicated server is a Linux distribution.

The resource meter is a meter that displays usage of several resources e.g. CPU and memory in real time. In the example illustrated in the top resource meter represents CPU usage and the bottom resource meter represent memory usage. Each meter displays the real time usage by fluctuating e.g. moving a bar within the meter in accord with the real time usage of the corresponding resource by the server. In some embodiments the fluctuating graphical display is indicative of usage of the resource at different instances in time and or is indicative of real time or near real time usage of the resource.

Also the fluctuating graphical display changes color in some embodiments when the usage of the particular resource exceeds a particular threshold. For instance in some embodiments the bar within a meter changes color when resource usage goes over a predetermined threshold for the resource. One such example is when the memory usage exceeds 50 percent of an allotted memory capacity the bottom resource meter might change from one color to another color e.g. from green to yellow .

The threshold in some embodiments is an expected usage rate over a duration of time based on the amount of a particular resource that is assigned to the particular user. Hence the top and bottom meters can indicate different colors at different instances in time to specify excess usage of the resource. These fluctuating meter bars and the changing colors provide a quick visual indication of whether the CPU and memory are being overloaded or thrashed. Hence these icons are referred to as thrash o meters in some embodiments. Instead of or in conjunction with the CPU and memory some embodiments of the multi server control panel provide real time usage information of other resources. These other resources include network resources e.g. network traffic data transfer and other physical resources e.g. storage space .

The front end provisioning system 1 receives communications e.g. service requests from external users through a network and 2 routes the communications to different datacenters e.g. datacenters and . In the example illustrated in the front end provisioning system includes a web server an application programming interface API server a core and a resource management system .

The web server communicates to a user through a network such as the Internet. Specifically the user accesses the hosting system through the web browser or which may be executed on the user s desktop computer portable notebook computer personal digital assistant PDA digital cellular telephone or other electronic communication devices. For instance when the user logs onto the hosting service s website or portal the user may be presented with the multi server control panel as discussed above by reference to .

In some embodiments the web server is responsible for generating a graphical interface through which users specify graphical representations e.g. the multi server control described in Section I above for various server configurations. In conjunction with or instead of the web server some embodiments implement the API server that interfaces with different custom applications e.g. a custom application UI through the network . The custom applications may operate on different operating systems or communication devices. In some embodiments the custom application may be a program or an applet that executes in a web browser.

In some embodiments the core acts as a controller that contains the executable code or logic required to perform different operations related to the multi server control panel. These operations may include operations related to creating user accounts enforcing access privileges e.g. authenticating and authorizing a user billing monitoring resources etc. For instance on an initial communication the web server may pass the user communication to the core for user verification and authentication. Accordingly the core may receive identification information from the user and determine whether the user has already created an account with the system. Also the core may authenticate and authorize the user based on data stored in the customer database . In addition the core may utilize an asset database to track available resources e.g. hardware resources . In some embodiments the core interacts with the resource management system to facilitate management of servers e.g. virtual servers dedicated servers at different datacenters.

The resource management system receives different requests e.g. provisioning tasks restart request from the core and routes these requests to the back end provisioning system. In some embodiments the resource management system 1 receives a change request from the core 2 identifies a particular deployment manager that can fulfill the change request and 3 sends a message to the particular deployment manager. The resource management system may also identify a datacenter location from the change request. For instance the resource management system may receive a request for a virtual server at a datacenter located in the Eastern United States. The resource management system may then send a message to a deployment manager that deploys virtual severs at the datacenter location.

The resource management system may serialize a message or data structure into a format that is understandable by a deployment manager that operates at a particular datacenter. In some embodiments the serialization allows objects or data structures containing information to be sent and understood by different parts or modules of the provisioning system e.g. the front end provisioning system the back end provisioning system . For instance different modules of the provisioning system that are defined by different programming languages e.g. C Java etc. may interoperate by exchanging messages that are serialized.

The deployment manager e.g. or is a component of the back end system that receives a request e.g. provisioning task and translates the request to control or manage hardware resources such as hardware nodes dedicated machines storage area networks etc. Each datacenter location e.g. datacenter or may have one or more deployment managers for different tasks. For instance a datacenter may have one deployment manager that deploys virtual machines and another deployment manager that deploys dedicated machines. The datacenters may also have one or more other deployment managers to monitor or control e.g. restart shutdown hardware resources.

To configure network details for the virtual server the message includes several other parameters . As shown the message includes an eth0vlan parameter that represents a customer s VLAN identify for the public network and an eth1vlan parameter that represents the customer s VLAN identify for the private network. In the example illustrated in the message also includes an eth0net parameter that defines the subnet and submask of eth0vlan and an eth1net parameter that defines the subnet and submask of eth1vlan . Also the message includes an eth0ip parameter that defines the IP subnet and submask for the pubic network.

In some embodiments the deployment manager receives the message and facilitates deployment of the virtual server according to the server configuration. Different embodiments deploy virtual servers differently. For example the deployment manager may operate in conjunction with a scheduler to schedule and deploy the virtual server on an available hardware node. Also the deployment manager may operate in conjunction with one or more other network components or modules e.g. switch programs in order to define the public and private VLANs for the virtual server according to the network details described above.

Having described the example architectural components of the hosting system several example network components will be described in Sections III and IV below. Specifically Section III describes an example configuration of the hosting system to provide virtual grids of hardware resources. Section IV then describes an example configuration of the hosting system to provide grid independent hardware nodes.

As mentioned above the VLAN protocol e.g. 802.1Q specifies a VLAN identification ID that includes 12 bits of data. This limits the maximum number of unique VLAN IDs to around 4096 2 12 per grid. Due to this limitation of the VLAN protocol e.g. 802.1Q some hosting service providers organize hardware resources e.g. hardware nodes dedicated servers into different grids of physical grids in order to provision and manage server configurations of multiple different entities e.g. customers users . In other words the service providers are forced to organize their hardware resources into different physical grids when all available VLAN IDs are utilized. However as the hosting service providers serve more and more entities the limitations of this organization make further growth expensive and difficult to manage.

As shown in Grid includes two access switches and one backend switch a set of hardware nodes and a top of rack switch . Similarly Grid includes two access switches and one backend switch a set of hardware nodes and a top of rack switch . As each grid includes its own set of hardware resources and switches Grid and Grid are two separate switched networks.

In the example illustrated in each customer s server has access to the public network and private network. Here the public network is configured with one or more public Internet Protocol IP addresses. This allows the server on the network of a particular grid to be visible to devices outside of the network e.g. from the Internet or another network . On the other hand the private network is configured such that the devices outside of the network cannot access the customer server. This private network defines different broadcast domains for different customers using unique VLAN IDs. For example by assigning one or more matching VLAN IDs for servers e.g. virtual server dedicated server of one customer the customer s servers can communicate with one another through the private network.

The access switches and perform routing and switching operations to provide access to public networks. The backend switches and as well as the top of rack switches and are for switching data to the private network . In the example of the access switch or and the backend switch or represents industrial switches e.g. a same type of industrial switch . Because of the large amount of routing tasks involved and relative weak routing capacities on these industrial switches each grid includes multiple access switches e.g. to provide the public interface to Internet via the border .

The sets of hardware nodes and represent hardware resources that can be shared amongst multiple different customers. Each set of hardware nodes and is coupled to the corresponding back end switch and so that a customer s servers can communicate with one another over the private network. Here each set of hardware nodes connects to network switches that are located within the same grid. As such each set of hardware nodes is tied to a particular grid and cannot be used for another grid. For example the set of hardware nodes can only be associated with VLANs in Grid .

There are several problems with the grid architecture illustrated in . For example due to VLAN protocol limitations the number of unique VLAN IDs can be exhausted in both Grid and Grid . For example a customer s server can be assigned a public VLAN ID and a private VLAN ID. This limits the number of different customers to around 2000 e.g. per grid. When the maximum number is reached in both grids the hosting system has to provide a new physical grid with another set of switches and hardware nodes.

Accordingly this method of scaling the hosting system is quite expensive to manage. It is also cumbersome to manage resources for multiple grids separately. For example the set of hardware nodes e.g. in Grid cannot be utilized for customers assigned to Grid . With each additional grid the capacity requirements multiply as separate pools of hardware nodes need to be maintained for each grid. When hardware nodes need to be moved between grids these nodes have to be physically moved from one area of the datacenter to another area of the datacenter.

Moreover in the example illustrated in there are several single points of failure. For instance each switch e.g. access switches and and backend switch is a single point of failure. In some cases the failure of one of these switches will potentially affect many customers e.g. thousands of customers as it will cause a communication failure to the set of nodes that is connected to that switch. Having so many single points of failure can be potentially revenue affecting for a hosting service provider. Furthermore it is expensive to deploy a separate set of switches e.g. industrial switches for each grid. It is also expensive to maintain a separate pool of hardware nodes for each grid. Furthermore moving hardware nodes between grids is labor intensive and potentially costly.

The switched network core is a set of core switches that manage data communication for all grids. The set of routers are for routing traffic to the Internet. Each router is configured to serve one or more grid. In this example routers and are configured to serve grid routers and are configured to serve grid and routers and are configured to serve grid . If the demand for routing increases the hosting system can simply add additional routers to meet the increased demand.

The set of hardware racks are standardized enclosures for mounting multiple hardware resources. Each hardware rack has several hardware nodes and one or more top of rack switch. For instance hardware rack has three hardware nodes and one top of rack switch . The top of rack switch is an inexpensive switch that sits on the very top or near the top of a hardware rack in a data center. The top of rack switch switches data to different grids for hardware nodes . As the demand for hardware resources increases and more hardware racks are added the hosting system can simply add additional top of rack switches to meet the increased demand.

The hosting system organizes hardware resources into logical or virtual grids rather than physical grids. Accordingly the same switching infrastructure can be leveraged for multiple grids. Instead of having three expensive switches for each physical grid as illustrated in the hosting system uses a single set of switches and routers to manage switching and routing for multiple virtual grids. In addition the hosting system can scale well beyond 4K VLANs on the same switched network infrastructure as will be described below by reference to .

Because the routing and switching are scaled independently the hosting system can scale the routing and switching infrastructure without any significant limits from the other. The hosting system can also scale the switched network linearly in a cost effective manner as the number of customers increases and without requiring any significant upfront deployment expenses.

Rather than using the same switch for both routing and switching as described in the hosting system uses specialized routers and switches for routing and switching respectively. This improves the routing and switching performance. The hosting system does not have to deploy a whole new switch just to scale routing resources. As a result the hosting system can add routing resources in a more cost effective and architecturally simpler manner. In addition by using multiple inexpensive routers and switches in a single switched network the hosting system increases its availability by significantly reducing and isolating any potential outages.

In the example network architecture illustrated in each hardware node is tied to a particular virtual grid which is actually a set of VLANs. All nodes are connected in a single switched network. The design is flexible and extensible such that the hosting system is able to deploy any customer on any node.

In order to scale beyond 4K VLANs on a single switching infrastructure the hosting system has to adopt a new VLAN protocol that provides extra bits for identifying virtual grids. illustrates an example of using 802.1QinQ protocol to identify logical grids in a hosting system.

As illustrated in an Ethernet frame is mainly constituted of three parts destination MAC address source MAC address and data . Current VLAN protocol e.g. 802.1Q inserts a customer VLAN ID into an Ethernet frame. However current VLAN protocol has its limitations. For instance 802.1Q frames only reserve 12 bits of data for VLAN IDs. This limits the maximum number of unique VLAN IDs to around 4096 per grid. A new VLAN protocol like 802.1QinQ 802.1AD allows an outer VLAN tag to be inserted into an 802.1Q frame to serve as grid identification ID for virtual grids. This essentially allows support for 4096 4096 or 16 777 216 VLANs.

By using a new VLAN protocol like 802.1QinQ the hosting system still divides hardware resources into grids. However these grids are no longer physical grids as they used to be. Instead grids are virtual now because they are purely logical. The division of hardware resources into grids has nothing to do with where they are located or which switch they connect to. The hosting service provider configures the switch port to which a particular node connects and that will determine which virtual gird that particular hardware node belongs to.

The hosting system can leverage a new VLAN protocol like 802.1QinQ to segregate each virtual grid s VLANs. The separation of routing and switching layers allows the hosting system to do this. Each virtual grid can be assigned a different grid ID in the outer VLAN tag . The routers hardware nodes and load balancers establish 802.1Q trunks just as they do in the past. However on the switches instead of configuring 802.1Q trunks the hosting system configures 802.1QinQ tunnel ports. VLAN tagged 802.1Q frames entering 802.1QinQ tunnel ports on the switch get an additional outer tag . Frames exiting 802.1QinQ tunnel ports have the outer tag removed and the frames are sent with a single VLAN tag that represents the customer VLAN ID. The outer VLAN tag represents the grid and the inner VLAN tag represents the customer. While the invention has been described with reference to 802.1QinQ protocol one of ordinary skill in the art will recognize that the invention can be embodied in other network protocols e.g. 802.1AH Mac in Mac without departing from the spirit of the invention.

The switched network core is a set of core switches that manage data communication for all virtual grids. The routers and are for routing traffic to the Internet. Each router is configured to server one or more virtual grids. In this example router is configured to serve grid by connecting to tunnel port . Router is configured to serve grid by connecting to tunnel port .

The top of rack switches and switch traffic to different virtual grids for hardware nodes . Hardware nodes represents physical resources e.g. memory storage CPU etc. that can be shared amongst different customers. These hardware nodes connect to switches so that they can communicate with other entities in the switched network. The hosting system configures the switch port connecting a hardware node to assign the hardware node to a particular virtual grid. In the example illustrated in for instance since hardware node connects to switch through a tunnel port that is configured for grid hardware node can only provide its hardware resources to VLANs in grid . Similarly since hardware node connects to switch through a tunnel port that is configured for grid hardware node belongs to grid .

By configuring 802.1QinQ tunnel ports on the switches the hosting system is able to segregate traffic from different grids on the same switching infrastructure. Single VLAN tagged frames entering 802.1QinQ tunnel ports on the switch get an additional outer tag to identify the grid. Frames exiting 802.1QinQ tunnel ports have the outer tag removed and the frames are sent with a single VLAN tag that identifies the customer. The outer VLAN tag represents the grid and the inner VLAN tag represents the customer.

For example frames entering the switch from router will get an outer tag of to indicate they belong to grid . Among these frames those destined for hardware node will go through the switched network core and the top of rack switch and will exit the switch through tunnel port . When exiting tunnel port the outer grid tag of will be removed and the frames will be sent to hardware node with a single VLAN tag that identifies the customer.

Even though a hardware node is tied to a logical grid in the hosting system it provides benefits over a hosting system in which each hardware node is tied to a physical grid. In a hosting system where a hardware node is tied to a physical grid for example if a node has to be moved between physical grids it has to be unplugged un racked moved to another area of the datacenter and racked again. However the hosting system adopts a network architecture of virtual grids instead of physical grids. As a result all the hosting service provider needs to do is change the 802.1QinQ tunnel access VLAN ID on the switch to move a hardware node from one grid to another. Instead of physical separation between the grids the hosting system has a logical separation. Significant cost savings can be realized as the same switching infrastructure can be leveraged for all grids in a datacenter. There would be no need to deploy three expensive switches per grid as described in above. Instead two equivalent switches will probably be able to serve all the grids in the hosting system . Switching scales simply by addition of top of rack switches as more racks are added.

For a hosting system conceptually illustrates a process for provisioning a virtual server on a hardware node. Specifically this figure illustrates defining a public and private network for a customer s server based on several VLAN identities and grid identity. In some embodiments the process is performed by one or more components of the hosting system e.g. the core the resource management system the deployment manager .

The process first receives at a server configuration for a customer. The process then determines at whether there is a grid ID assigned to the customer. In some embodiments the process performs this task by querying the customer database and or the asset database . When there is no grid ID assigned to the customer the process assigns at a grid ID to the customer. When the customer has been previously assigned a grid ID the process identifies at this grid ID.

The process then determines at whether there is any VLAN assigned to the customer. In some embodiments the process performs this task by querying the customer database and or the asset database . When there is no VLAN assigned to the customer the process assigns at available VLANs for the server s public and private networks.

The process then marks at each assigned VLAN as not available. In some embodiments the process performs the marking by modifying the contents in the customer database and or the asset database . This is to prevent the same VLAN ID from being used by multiple customers. When the determination is made at that the VLANs have been previously assigned to the customer the process identifies at those public and private VLANs of the customer. The process then deploys at the virtual server based on the grid ID and the public and private VLANs. For example the private VLAN identity and the grid ID can be used to define a separate broadcast domain or private network such that the customer s servers can communicate with one another.

The switched network represents a set of switches that manage data communication of all virtual grids inside of the hosting system . This set of switches may include one or more core switches and several top of rack switches. Hardware nodes are computers that provide hardware resources e.g. memory storage CPU etc. to be shared by customers. These hardware nodes connect to the switched network to communicate with other entities in the hosting system .

A hardware node connects to the switched network through a particular port on a switch. How the hosting system configures that particular port will determine which grid the hardware node belongs to. In the example illustrated in for instance since hardware node connects to the switched network through a tunnel port that is configured for grid hardware node can only provide hardware resources to VLANs in grid . Similarly hardware node belongs to grid while hardware nodes and belong to grid .

When a frame from hardware node enters into the switched network through the tunnel port the hosting system inserts a grid ID into the frame as an outer tag. As a result the altered frame will have two VLAN tags. One is the newly inserted grid ID. The other is an existing inner customer VLAN ID . When the altered frame exits the switched network through a tunnel port the outer grid ID will be removed and the resulting frame will return to its original form with only the customer VLAN ID.

Similarly the hosting system will insert a grid ID into a frame when it enters the switched network through a tunnel port resulting in an altered frame . The hosting system will remove the grid ID from the altered frame when it exits the switched network resulting in frame that has a single VLAN tag that identifies the customer.

The switched network represents a set of switches that manage data communication for all virtual grids inside of the hosting system . This set of switches may include one or more core switches and several top of rack switches. Hardware nodes and are computers that provide hardware resources e.g. memory storage CPU etc. to be shared by customers. These hardware nodes connect to the switched network so that they can communicate with other entities in the hosting system . The routers and are for routing traffic to the Internet.

A hardware node connects to the switched network through a particular port on a switch. How the hosting system configures that particular port will determine which grid a hardware node belongs to. In the example illustrated in for instance since hardware node connects to the switched network through a tunnel port that is configured for grid hardware node can only provide hardware resources to VLANs in grid . Similarly hardware node belongs to grid . A router can connect to multiple grids. However in this example router connects only to grid and router connects only to grid .

When a frame from router enters into the switched network through the tunnel port the hosting system inserts a grid ID into the frame as an outer tag. As a result the altered frame will have two VLAN tags. One is the newly inserted grid ID . The other is an existing inner customer VLAN ID . When the altered frame exits the switched network through a tunnel port the outer grid ID will be removed. The frame reaching the hardware node will return to its original form with only the customer VLAN ID .

Similarly the hosting system will insert a grid ID into a frame from the hardware node when it enters the switched network through a tunnel port resulting in an altered frame . The hosting system will remove the grid ID from the altered frame when it exits the switched network resulting in frame that has only one VLAN tag that identifies customer VLAN . The frame will then go on to router .

The core switches and manage data communication for all virtual grids. The virtual grid network architecture significantly simplifies the configuration on the core switches. Instead of managing 4K customer VLANs only the number of virtual grids needs to be managed by the core switches. This greatly reduces the load on the core switches and which can now be used for simply switching high volumes of traffic with minimal latency.

Core redundancy technologies enable clustering the two distinct physical switches and into a single logical switch . All devices that connect to the core switches and will connect as if they are connecting to a single switch . This will reduce the likelihood of physical loops in network connections. Core redundancy will also simplify management of the hosting system . One IP address and configuration file controls both core switches and . The control plane is active on one switch only while the other switch is in non controlling standby mode. However the data plane is active on both switches.

Routers connect to the logical core switch to route traffic to the Internet. Load balancers and connect to the logical core switch to provide a single service from multiple servers. The cloud storage switch connects and switches data between the cloud storage node the caching node and the logical core switch . The set of hardware racks are standardized enclosures for mounting multiple hardware resources. Each hardware rack could contain several hardware nodes and one or more top of rack switch. For instance hardware rack contains one hardware node and one top of rack switch . The top of rack switch is an inexpensive switch that sits on the very top or near the top of a rack in a data center. The top of rack switch switches data for hardware node .

The same switching infrastructure can be leveraged for the private networks by tunneling the private network traffic of a particular grid into its own outer grid ID. For example private network traffic from grid can be assigned an outer grid ID and the private network traffic from grid can be assigned an outer grid ID . A separate backend router is for routing the traffic between private networks and the cloud storage environment.

The management switch can serve multiple grids in switching data for the management network. The management network is for the hosting service providers to manage hardware nodes e.g. adding a VLAN removing a VLAN loading a virtual server etc.

The hosting system allows scaling of routing by addition of routers or interfaces on existing routers as the number of customers increases. Private network routing can be scaled similarly. The hosting system can scale management switch by stacking additional switches. Similarly the hosting system can scale cloud storage switch by stacking additional switches.

As long as individual non core components in the hosting system are sized appropriately there should be no performance issues. The core switches and will be lightly configured with a few VLANs and minimal routing configuration. Their control plane is not likely to be loaded at all. The core switches and should be able to provide ample data forwarding capacity.

In the hosting system it is easy to provide redundancy with almost all network components. The only single point of failure is the top of rack switches . Any failure of top of rack switches will likely be isolated to a small set of customers. For instance the failure of top of rack switch will only affect customers using the hardware node .

The upfront cost of deploying the hosting system is likely to be high. However incremental costs will be low. It will be much cheaper to deploy additional grids as compared to deploying whole new physical grids as described in . The hosting system can leverage the same routers for multiple grids. There is no need to deploy two routers per grid for redundancy. One router can serve as a backup for all the other routers.

As the separation of grids is virtual instead of physical moving unused resources such as hardware nodes load balancers etc. can be done by simply changing the VLAN ID of the tunnel port on the switch. The devices do not need to be physically moved.

The switched network core is a set of core switches that manage data communication for all virtual grids. The top of rack switch is an inexpensive switch connecting the hardware node to the rest of the switched network. The hardware node is a grid specific node which means it is tied to a particular virtual grid e.g. grid . The hosting system configures the port as a tunnel port that inserts an outer grid ID into frames coming from the hardware node and removes the outer grid ID from frames leaving for the node. As a result the hardware node can only provide its hardware resources to customers in grid .

The two to one VLAN translation switch makes the hardware node grid independent. As shown in the hardware node connects to the switch through a 802.1Q trunk port rather than a 802.1QinQ tunnel port. This means that any frame to and from the hardware node will have only one inner VLAN tag. Since frames arriving at the switch from other parts of the switched network have two VLAN tags the switch has to do a two to one mapping to enable the two sides to communicate with each other. In the two to one mapping table in a customer from grid with a customer VLAN ID is mapped to a local VLAN ID . Likewise a customer from hardware node with only a local VLAN ID is mapped to grid ID and customer VLAN ID . As a result of this mapping the hardware node can be leveraged to serve customers in multiple grids. This enables hardware resource sharing between multiple grids and makes the hosting system more cost effective.

Every frame coming in to and out of node e.g. has an outer grid ID and inner customer VLAN ID. For frame the inner VLAN ID is 30 because it originated from or is destined for customer VLAN on node . When switch receives frame it will map the outer grid ID and inner VLAN ID to a single VLAN ID resulting frame . The converted frame will then reach local VLAN on the grid independent node . Similarly frame from node will be converted to frame when it goes through switch and reaches node .

On the other side in grid a frame e.g. that originated from VLAN on node of grid is converted to frame when going through switch where the outer grid ID and inner customer VLAN ID is mapped to a local VLAN ID . Similarly frame will be converted to frame when it goes from hardware node to node through switch . As a result of these conversions customers from grid and grid can share the resources on the same hardware node .

In some cases a customer may have different accounts with the same hosting service provider for a number of reasons e.g. account segregation security billing access control etc. Those different accounts may wind up being associated with two different grids. But the customer may still want deployments for those separate accounts to use the same physical hardware resources for better performance less latency or other reasons. The method illustrated in makes it possible for a customer s multiple accounts on different grids to share resources on the same hardware node.

In some embodiments a two to one VLAN translation switch is configured to translate VLAN ID tags in headers e.g. 802.1Q headers and 802.1QinQ headers of all frames of data going between the switch and any upstream switches. For instance the two to one VLAN translation may be done at the switch s port level. That is the switch may not be aware of a virtual server s grid ID and customer VLAN ID prior to the translation. However when the switch identifies data e.g. frame of data going to a particular MAC address e.g. of the virtual server the switch may replace the local VLAN ID in the header with the virtual server s grid ID and customer VLAN ID.

For a hosting system conceptually illustrates a process for provisioning a virtual server on a hardware node. Different from the process of the process facilitates deployment of the virtual server by specifying a mapping of the public and private VLANs to VLANs of a particular switch. In some embodiments the process is performed by one or more components of the hosting system e.g. the core the resource management system the deployment manager .

As shown in operations to are identical to operations of . Specifically the process first receives at a server configuration for a customer. The process then determines at whether there is a grid ID assigned to the customer. When there is no grid assigned to the customer the process assigns at a grid ID to the customer. When the customer has been previously assigned a grid ID the process identifies at this grid ID.

The process then determines at whether there is any VLANs assigned to the customer. When there is no VLANs assigned to the customer the process assigns at available VLANs for the server s public and private networks. The process then marks at each assigned VLAN as not available. When there are VLANs previously assigned to the customer the process identifies at the customer s public and private VLANs.

As shown in the process then identifies at a hardware node to deploy the virtual server. Here the process also identifies the hardware node s switch. The process then determines at whether the hardware node is a grid specific node or a grid independent universal node. In some embodiment the process makes this determination by querying one or more databases e.g. the asset database .

When the process determines at that the hardware node is a grid specific node the process then deploys at the virtual server based on the grid ID and the public and private VLANs. For example the private VLAN identity and the grid ID can be used to define a separate broadcast domain or private network such that the customer s servers can communicate with one another.

When the node is a grid independent node the process determines at whether one or more switch VLANs e.g. TOR VLANs has been assigned to the customer. In some embodiments the process determines whether the customer has been assigned public and private VLANs on the switch that was identified at .

When the determination is made that the switch VLANs has not been assigned the process assigns at available public and private switch VLANs to the customer. The process then marks at each assigned switch VLAN as not available. This prevents the same TOR VLANs from being assigned to a different customer. In some embodiments the process performs the marking by modifying data in the customer database and or the asset database.

When the determination is made that the switch VLANs has been assigned the process identifies at the customer s public and private switch VLANs. At the process specifies a mapping from the public and private VLANs to the TOR VLAN. In some embodiments the grid identity is also used to perform a two to one mapping as described above by reference to . Finally the process deploys at and configures the network details of the virtual server by using the public and private switch VLANs.

Many of the above described features and applications are implemented as software processes that are specified as a set of instructions recorded on a computer readable storage medium also referred to as computer readable medium . When these instructions are executed by one or more processing unit s e.g. one or more processors cores of processors or other processing units they cause the processing unit s to perform the actions indicated in the instructions. Examples of computer readable media include but are not limited to CD ROMs flash drives RAM chips hard drives EPROMs etc. The computer readable media does not include carrier waves and electronic signals passing wirelessly or over wired connections.

In this specification the term software is meant to include firmware residing in read only memory or applications stored in magnetic storage which can be read into memory for processing by a processor. Also in some embodiments multiple software inventions can be implemented as sub parts of a larger program while remaining distinct software inventions. In some embodiments multiple software inventions can also be implemented as separate programs. Finally any combination of separate programs that together implement a software invention described here is within the scope of the invention. In some embodiments the software programs when installed to operate on one or more electronic systems define one or more specific machine implementations that execute and perform the operations of the software programs.

The bus collectively represents all system peripheral and chipset buses that communicatively connect the numerous internal devices of the electronic system . For instance the bus communicatively connects the processing unit s with the read only memory the GPU the system memory and the permanent storage device .

From these various memory units the processing unit s retrieves instructions to execute and data to process in order to execute the processes of the invention. The processing unit s may be a single processor or a multi core processor in different embodiments. Some instructions are passed to and executed by the GPU . The GPU can offload various computations or complement the image processing provided by the processing unit s .

The read only memory ROM stores static data and instructions that are needed by the processing unit s and other modules of the electronic system. The permanent storage device on the other hand is a read and write memory device. This device is a non volatile memory unit that stores instructions and data even when the electronic system is off Some embodiments of the invention use a mass storage device such as a magnetic or optical disk and its corresponding disk drive as the permanent storage device .

Other embodiments use a removable storage device such as a floppy disk flash drive or ZIP disk and its corresponding disk drive as the permanent storage device. Like the permanent storage device the system memory is a read and write memory device. However unlike storage device the system memory is a volatile read and write memory such a random access memory. The system memory stores some of the instructions and data that the processor needs at runtime. In some embodiments the invention s processes are stored in the system memory the permanent storage device and or the read only memory . For example the various memory units include instructions for processing multimedia clips in accordance with some embodiments. From these various memory units the processing unit s retrieves instructions to execute and data to process in order to execute the processes of some embodiments.

The bus also connects to the input and output devices and . The input devices enable the user to communicate information and select commands to the electronic system. The input devices include alphanumeric keyboards and pointing devices also called cursor control devices . The output devices display images generated by the electronic system. The output devices include printers and display devices such as cathode ray tubes CRT or liquid crystal displays LCD . Some embodiments include devices such as a touchscreen that function as both input and output devices.

Finally as shown in bus also couples electronic system to a network through a network adapter not shown . In this manner the computer can be a part of a network of computers such as a local area network LAN a wide area network WAN or an Intranet or a network of networks such as the Internet. Any or all components of electronic system may be used in conjunction with the invention.

Some embodiments include electronic components such as microprocessors storage and memory that store computer program instructions in a machine readable or computer readable medium alternatively referred to as computer readable storage media machine readable media or machine readable storage media . Some examples of such computer readable media include RAM ROM read only compact discs CD ROM recordable compact discs CD R rewritable compact discs CD RW read only digital versatile discs e.g. DVD ROM dual layer DVD ROM a variety of recordable rewritable DVDs e.g. DVD RAM DVD RW DVD RW etc. flash memory e.g. SD cards mini SD cards micro SD cards etc. magnetic and or solid state hard drives read only and recordable Blu Ray discs ultra density optical discs any other optical or magnetic media and floppy disks. The computer readable media may store a computer program that is executable by at least one processing unit and includes sets of instructions for performing various operations. Examples of computer programs or computer code include machine code such as is produced by a compiler and files including higher level code that are executed by a computer an electronic component or a microprocessor using an interpreter.

While the above discussion primarily refers to microprocessor or multi core processors that execute software some embodiments are performed by one or more integrated circuits such as application specific integrated circuits ASICs or field programmable gate arrays FPGAs . In some embodiments such integrated circuits execute instructions that are stored on the circuit itself.

As used in this specification and any claims of this application the terms computer server processor and memory all refer to electronic or other technological devices. These terms exclude people or groups of people. For the purposes of the specification the terms display or displaying means displaying on an electronic device. As used in this specification and any claims of this application the terms computer readable medium computer readable media and machine readable medium are entirely restricted to tangible physical objects that store information in a form that is readable by a computer. These terms exclude any wireless signals wired download signals and any other ephemeral signals.

While the invention has been described with reference to numerous specific details one of ordinary skill in the art will recognize that the invention can be embodied in other specific forms without departing from the spirit of the invention. In addition a number of the figures including conceptually illustrate processes. The specific operations of these processes may not be performed in the exact order shown and described. The specific operations may not be performed in one continuous series of operations and different specific operations may be performed in different embodiments. Furthermore the process could be implemented using several sub processes or as part of a larger macro process. Thus one of ordinary skill in the art would understand that the invention is not to be limited by the foregoing illustrative details but rather is to be defined by the appended claims.

