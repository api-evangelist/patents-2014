---

title: System, computer program product, and method for generating a lightweight source code for implementing an image processing pipeline
abstract: A system, method, and computer program product for generating a lightweight source code for implementing an image processing pipeline is disclosed. The method comprises receiving a specification for an image processing pipeline based on configuration settings associated with a user interface of a viewer application, generating a graphics language (GL) representation of the image processing pipeline based on the specification, and code for causing the GL representation to be compiled via a compile service to generate a binary executable instantiation of the image processing pipeline for execution on one or more graphics processing unit (GPU) cores.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09448771&OS=09448771&RS=09448771
owner: Duelight LLC
number: 09448771
owner_city: Sunnyvale
owner_country: US
publication_date: 20141017
---
The present invention relates generally to digital photographic systems and more specifically to a system method and computer program product for generating a lightweight source code for implementing an image processing pipeline.

A typical digital camera focuses an optical image of a scene onto an image sensor which samples the optical image to generate an electronic representation of the scene. The electronic representation is then processed and stored as a digital photograph. Many digital cameras and other hand held devices such as smart phones implement a software based image processing pipeline for processing the raw data captured by the image sensor to generate the digital photograph.

As digital cameras and other consumer electronic devices become more complex certain aspects of the image processing pipeline may become more configurable. Additionally the availability of more and more filters means that the executable code for the image processing pipeline may grow quite large and complex. At the same time the processing capacity of chips used in digital cameras and other hand held devices may be limited.

As such there is a need for addressing issues associated with the image processing pipeline for digital cameras and other hand held devices and or other related issues associated with the prior art.

A system method and computer program product for generating a lightweight source code for implementing an image processing pipeline is disclosed. The method comprises receiving a specification for an image processing pipeline based on configuration settings associated with a user interface of a viewer application generating a graphics language GL representation of the image processing pipeline based on the specification and code for causing the GL representation to be compiled via a compile service to generate a binary executable instantiation of the image processing pipeline for execution on one or more graphics processing unit GPU cores.

In one embodiment a digital photographic system such as digital photographic system of is configured to perform method . The digital photographic system may be implemented within a digital camera such as digital camera of or a mobile device such as mobile device of .

The method begins at step where a specification for an image processing pipeline is received. In one embodiment the specification may be based on configuration settings associated with a user interface of a viewer application. Additionally the viewer application may be a process or processes executed by a host processor. The viewer application may enable a user to view and manipulate images captured by for example a digital photographic system that includes the host processor. In such an embodiment the viewer application may be associated with a user interface displayed on a display device in order to present options for processing images to a user.

In one embodiment a digital photographic system may generate a digital image or simply image of a photographic scene subjected to either ambient illumination strobe illumination or a combination thereof. In such an embodiment digital photographic systems may include without limitation digital cameras and mobile devices such as smart phones that are configured to include a digital camera module and a strobe unit. Further in one embodiment a given photographic scene may be a portion of an overall scene sampled by the digital photographic system.

In another embodiment two or more images are sequentially sampled by the digital photographic system to generate an image set. Each image within the image set may be generated in conjunction with different strobe intensity different exposure parameters or a combination thereof. In one embodiment exposure parameters may include sensor sensitivity ISO parameter exposure time shutter speed aperture size f stop and focus distance. In other embodiments one or more exposure parameters such as aperture size may be constant and not subject to determination. For example aperture size may be constant based on a given lens design associated with the digital photographic system. At least two of the images comprising the image set may be sampled in conjunction with a strobe unit such as a light emitting diode LED strobe unit configured to contribute illumination to the photographic scene.

In one embodiment exposure parameters are initially determined and held constant for each image in the image set. The exposure parameters may be initially determined based on ambient lighting conditions. If insufficient ambient lighting is available such as for extremely dark scenes then exposure parameters may be determined based on mid range strobe intensity. For example mid range strobe intensity may be selected as fifty percent of maximum strobe intensity for the strobe unit.

The strobe unit may be configured to modulate strobe intensity to provide a range of illumination contribution among the images within the image set. For example in one embodiment the image set may comprise ten images each with monotonically increasing illumination from the strobe unit. The first often images within the image set may be relatively under exposed with respect to strobe illumination while the tenth image may be over exposed with respect to strobe illumination. However one or more images between the first image and the tenth image will likely be appropriately exposed with respect to strobe illumination. An image with appropriate illumination may be selected automatically or manually selected by a user. The image with appropriate illumination may then be a resulting image for the photographic scene.

In one embodiment the digital photographic system may include a viewer application. The viewer application may reside in a memory of the digital photographic system and be associated with a user interface presented on a display of the digital photographic system. In another embodiment the user interface may include various menus settings and user interface elements that enable a user to modify one or more images stored in the memory of the digital photographic system. For example in one specific embodiment the user interface may include a slider that enables a user to manually select an image within the image set as the resulting image. The user interface may also include menus that enable a user to select filters to include in an image processing pipeline checkboxes for selecting particular options associated with the image processing pipeline and textboxes or drop down menus for selecting values associated with the image processing pipeline such as a size of a filter to apply a threshold value to use with a filter and so forth.

Still yet in one embodiment the user may select various options for processing an image or images. By selecting or deselecting certain user interface elements or by providing values or changing the state of certain user interface elements a user may configure the manner by which the images are processed by an image processing pipeline. The various configuration changes made by the user may be reflected in the specification by the viewer application. For example in one embodiment the user interface may be configured to store a data structure that includes the current state of each of the user interface elements in the user interface. The specification may be generated from this data structure. In another embodiment an application or process may query the user interface to determine the current state of each of the user interface elements and generate the specification based on the current state of the various user interface elements.

In the context of the present description configuration settings may refer to the combined state of one or more user interface elements in the user interface used to configure the image processing pipeline. For example a user may select which filters to apply to a given image or images and in what order using the user interface such that a resulting image is displayed by the viewer application by processing one or more images via the custom image processing pipeline. In one embodiment a list of available filters may include but is not limited to no filter e.g. the output is the same as the input a white balance filter a noise reducing filter a smoothing filter or blur filter an unsharp filter a color correction filter and the like.

At step a graphics library GL representation of the image processing pipeline is generated based on the specification. In one embodiment a code generator module may create the GL representation by stitching together code segments associated with each of the filters included in the image processing pipeline as defined in the specification. In one embodiment the GL representation may be an object or data structure that represents a plain text string of source code configured according to an OpenGL language specification and may include calls to an OpenGL application programming interface API . For example in one embodiment the GL representation may represent a shader program such as a fragment shader comprising a combination of one or more filters applied to the pixels of an image where the shader program includes one or more OpenGL functions with corresponding executable mappings. Exemplary executable mappings include source code arithmetic operations mapping to corresponding executable arithmetic instructions or corresponding sequences of instructions source code variable access operations mapping to executable load store operations and the like. In other embodiments the GL representation may target OpenCL CUDA or any other technically feasible multi threaded language.

In one embodiment the user interface elements included in the user interface may enable a user to specify which filters to include in the image processing pipeline and an order for those filters to be applied to the image s . Further a code generator module may receive the specification and generate the GL representation by stitching together code segments associated with each of the elements of the pipeline. The code generator module may also be configured to control the flow of data between nodes in the image processing pipeline by specifying how the output of one node or filter is tied to the input of the next node or filter . In one embodiment the intermediate data may be consumed immediately by the subsequent node or may be stored temporarily in a buffer as specified by the code generator module.

In addition the user interface elements may also specify values or settings for applying those filters such as by enabling the user to customize threshold values associated with the filters a window size associated with the filters and the like. The code generator module may include references to uniform variables in the source code of the GL representation. As defined by the OpenGL standard a uniform variable is a read only variable that has the same value for all processed vertices or fragments but can be changed outside of the shader program via an external application executing on the host processor. In other words the uniform variable may reference a location in memory that stores a value that can be updated between passes of the image processing pipeline by for example the viewer application. By including references to uniform variables in the GL representation and passing certain values to the GL representation in a shared memory some values specified by the user interface elements can be changed at a later time without explicitly changing the source code included in the GL representation. For example the user may change the value of a slider UI element associated with a uniform causing a new resulting image to be generated according to a new uniform value. Similarly a different texture map may be passed to the binary executable instantiation of the image processing pipeline causing a new resulting image to be generated without changing the binary executable instantiation of the image processing pipeline.

At step the GL representation is compiled via a compile service to generate a binary executable instantiation of the image processing pipeline. In one embodiment the compile service may be associated with a given hardware for executing the image processing pipeline and may be provided by the manufacturer of the hardware for compiling source code that can be executed by that hardware. For example the image processing pipeline may be executed on one or more graphics processing unit GPU cores. The manufacturer of the GPU may provide a compile service module for compiling source code configured for the GPU as either a stand alone application or as a function call provided via an API implemented by another application e.g. as a function call made to a driver for the GPU etc. . In one embodiment the binary executable instantiation of the image processing pipeline may be executed by the GPU to process one or more images according to the image processing pipeline.

Additionally in various embodiments the GL representation may comprise a source code including one or more GL functions with a corresponding executable mapping. The GL function may comprise an OpenGL function. Additionally an image set may be received that includes a first image of a photographic scene based on a first set of sampling parameters and a second image of the photographic scene based on a second set of sampling parameters at least one image in the image set may be processed via the binary executable instantiation of the image processing pipeline 

In other embodiments the first set of sampling parameters may comprise ambient exposure parameters for capturing images illuminated by ambient light and wherein the second set of sampling parameters comprises exposure parameters for capturing images illuminated by a strobe unit. Additionally the specification may comprise a data structure that identifies at least one filter included in the image processing pipeline and an order for the at least one filter in the image processing pipeline.

In one embodiment the viewer application in response to user input associated with the user interface may be configured to modify the specification generate an updated GL representation of the image processing pipeline based on the modified specification and cause the updated GL representation to be compiled via the compile service to generate an updated binary executable instantiation of the image processing pipeline for execution on the one or more GPU cores.

In another embodiment the GL representation may include at least one reference to a uniform variable and the viewer application in response to user input associated with the user interface may assign values to the at least one reference to the uniform variable.

In one embodiment the image processing pipeline may comprise a plurality of filters applied to at least one image in an image set. Additionally one or more images in an image set may be captured and a resulting image may be displayed generated by processing the one or more images via the binary executable instantiation of the image processing pipeline.

Further in another embodiment the user interface may include at least one of a pull down menu a checkbox a radio button and a text box wherein the at least one of the pull down menu the checkbox the radio button and the text box may define at least part of the image processing pipeline or may specify values for references to uniform variables included in the GL representation.

In one embodiment a digital photographic system such as digital photographic system of is configured to perform method . The digital photographic system may be implemented within a digital camera such as digital camera of or a mobile device such as mobile device of .

The method begins at step where user input associated with a user interface is detected. In one embodiment method is performed to generate a lightweight source code for the image processing pipeline and then that source code is compiled to generate a shader program for execution on specialized hardware such as one or more GPU cores. One or more images are then processed by the image processing pipeline. However when a user makes changes to the configuration settings for the image processing pipeline the binary executable instantiation of the image processing pipeline may need to be updated. For example in one embodiment a user may change the number of filters included in the image processing pipeline or may change a particular setting associated with one of the filters. Such changes to the configuration settings made by the user using one or more user interface elements should be reflected in the binary executable instantiation of the image processing pipeline.

At step the viewer application determines whether the updated configuration setting is associated with a uniform variable. In one embodiment some configuration settings may specify which filters are included in the image processing pipeline and in what order while other configuration settings may simply specify certain values or settings associated with those filters. These values or settings may be passed to the filter included in the binary executable instantiation of the image processing pipeline via references to uniform variables in the source code. If the configuration setting modified by the user is associated with a uniform variable in the GL representation then the GL representation does not need to be modified and recompiled. Instead at step the viewer application may simply modify a value in a memory associated with a reference to the uniform variable to adjust the image processing pipeline for processing subsequent images. In one embodiment a change in any uniform variable triggers execution of the binary executable instantiation of the image processing pipeline to generate a new resulting image but no change is made to the binary executable. After step the method proceeds to step discussed below.

Returning to step if the configuration setting modified by the user is not associated with a uniform variable then at step the viewer application may modify the specification based on the user input. Such user input may for example specify a different number and or order of the filters to be applied to the images by the image processing pipeline. Such modifications to the configuration settings may not simply be effected by changing the value of a uniform variable. Instead the specification may be modified to define the configuration of the image processing pipeline based on the new configuration settings selected by the user which may require an updated GL representation of the image processing pipeline. In certain embodiments code that executes outside the image processing pipeline to manage the image processing pipeline may be configured to adapt accordingly to changes in the specification.

At step an updated GL representation of the image processing pipeline is generated based on the modified specification. In one embodiment a code generator module may stitch together various code segments for the one or more filters included in the image processing pipeline and control the flow of data between nodes in the image processing pipeline. At step the GL representation is compiled via a compile service to generate a binary executable instantiation of the image processing pipeline. In one embodiment the binary executable may be transmitted to the GPU and used to process one or more images according to the updated image processing pipeline. At step either the method is terminated or the method returns to step to detect another user input associated with the user interface.

More illustrative information will now be set forth regarding various optional architectures and uses in which the foregoing method may or may not be implemented per the desires of the user. It should be strongly noted that the following information is set forth for illustrative purposes and should not be construed as limiting in any manner. Any of the following features may be optionally incorporated with or without the exclusion of other features described. While techniques and method disclosed herein are described with respect to altering uniform variables without regenerating the image processing pipeline texture maps and any other data that does not alter the structure of the image processing pipeline may also be changed without requiring the image processing pipeline or binary executable instantiation of the image processing pipeline to be regenerated. Such changes may or may not trigger computing a new resulting image.

As shown the digital photographic system may include a processor complex coupled to a camera module via an interconnect . In one embodiment the processor complex is coupled to a strobe unit . The digital photographic system may also include without limitation a display unit a set of input output devices non volatile memory volatile memory a wireless unit and sensor devices each coupled to the processor complex . In one embodiment a power management subsystem is configured to generate appropriate power supply voltages for each electrical load element within the digital photographic system . A battery may be configured to supply electrical energy to the power management subsystem . The battery may implement any technically feasible energy storage system including primary or rechargeable battery technologies. Of course in other embodiments additional or fewer features units devices sensors or subsystems may be included in the system.

In one embodiment a strobe unit may be integrated into the digital photographic system and configured to provide strobe illumination during an image sample event performed by the digital photographic system . In another embodiment a strobe unit may be implemented as an independent device from the digital photographic system and configured to provide strobe illumination during an image sample event performed by the digital photographic system . The strobe unit may comprise one or more LED devices a gas discharge illuminator e.g. a Xenon strobe device a Xenon flash lamp etc. or any other technically feasible illumination device. In certain embodiments two or more strobe units are configured to synchronously generate strobe illumination in conjunction with sampling an image. In one embodiment the strobe unit is controlled through a strobe control signal to either emit the strobe illumination or not emit the strobe illumination . The strobe control signal may be implemented using any technically feasible signal transmission protocol. The strobe control signal may indicate a strobe parameter e.g. strobe intensity strobe color strobe time etc. for directing the strobe unit to generate a specified intensity and or color of the strobe illumination . The strobe control signal may be generated by the processor complex the camera module or by any other technically feasible combination thereof. In one embodiment the strobe control signal is generated by a camera interface unit within the processor complex and transmitted to both the strobe unit and the camera module via the interconnect . In another embodiment the strobe control signal is generated by the camera module and transmitted to the strobe unit via the interconnect .

Optical scene information which may include at least a portion of the strobe illumination reflected from objects in the photographic scene is focused as an optical image onto an image sensor within the camera module . The image sensor generates an electronic representation of the optical image. The electronic representation comprises spatial color intensity information which may include different color intensity samples e.g. red green and blue light etc. . In other embodiments the spatial color intensity information may also include samples for white light. The electronic representation is transmitted to the processor complex via the interconnect which may implement any technically feasible signal transmission protocol.

In one embodiment input output devices may include without limitation a capacitive touch input surface a resistive tablet input surface one or more buttons one or more knobs light emitting devices light detecting devices sound emitting devices sound detecting devices or any other technically feasible device for receiving user input and converting the input to electrical signals or converting electrical signals into a physical signal. In one embodiment the input output devices include a capacitive touch input surface coupled to a display unit . A touch entry display system may include the display unit and a capacitive touch input surface also coupled to processor complex .

Additionally in other embodiments non volatile NV memory is configured to store data when power is interrupted. In one embodiment the NV memory comprises one or more flash memory devices e.g. ROM PCM FeRAM FRAM PRAM MRAM NRAM etc. . The NV memory comprises a non transitory computer readable medium which may be configured to include programming instructions for execution by one or more processing units within the processor complex . The programming instructions may implement without limitation an operating system OS UI software modules image processing and storage software modules one or more input output devices connected to the processor complex one or more software modules for sampling an image stack through camera module one or more software modules for presenting the image stack or one or more synthetic images generated from the image stack through the display unit . As an example in one embodiment the programming instructions may also implement one or more software modules for merging images or portions of images within the image stack aligning at least portions of each image within the image stack or a combination thereof. In another embodiment the processor complex may be configured to execute the programming instructions which may implement one or more software modules operable to create a high dynamic range HDR image.

Still yet in one embodiment one or more memory devices comprising the NV memory may be packaged as a module configured to be installed or removed by a user. In one embodiment volatile memory comprises dynamic random access memory DRAM configured to temporarily store programming instructions image data such as data associated with an image stack and the like accessed during the course of normal operation of the digital photographic system . Of course the volatile memory may be used in any manner and in association with any other input output device or sensor device attached to the process complex .

In one embodiment sensor devices may include without limitation one or more of an accelerometer to detect motion and or orientation an electronic gyroscope to detect motion and or orientation a magnetic flux detector to detect orientation a global positioning system GPS module to detect geographic position or any combination thereof. Of course other sensors including but not limited to a motion detection sensor a proximity sensor an RGB light sensor a gesture sensor a 3 D input image sensor a pressure sensor and an indoor position sensor may be integrated as sensor devices. In one embodiment the sensor devices may be one example of input output devices .

Wireless unit may include one or more digital radios configured to send and receive digital data. In particular the wireless unit may implement wireless standards e.g. WiFi Bluetooth NFC etc. and may implement digital cellular telephony standards for data communication e.g. CDMA 3G 4G LTE LTE Advanced etc. . Of course any wireless standard or digital cellular telephony standards may be used.

In one embodiment the digital photographic system is configured to transmit one or more digital photographs to a network based online or cloud based photographic media service via the wireless unit . The one or more digital photographs may reside within either the NV memory or the volatile memory or any other memory device associated with the processor complex . In one embodiment a user may possess credentials to access an online photographic media service and to transmit one or more digital photographs for storage to retrieval from and presentation by the online photographic media service. The credentials may be stored or generated within the digital photographic system prior to transmission of the digital photographs. The online photographic media service may comprise a social networking service photograph sharing service or any other network based service that provides storage of digital photographs processing of digital photographs transmission of digital photographs sharing of digital photographs or any combination thereof. In certain embodiments one or more digital photographs are generated by the online photographic media service based on image data e.g. image stack. HDR image stack image package etc. transmitted to servers associated with the online photographic media service. In such embodiments a user may upload one or more source images from the digital photographic system for processing by the online photographic media service.

In one embodiment the digital photographic system comprises at least one instance of a camera module . In another embodiment the digital photographic system comprises a plurality of camera modules . Such an embodiment may also include at least one strobe unit configured to illuminate a photographic scene sampled as multiple views by the plurality of camera modules . The plurality of camera modules may be configured to sample a wide angle view e.g. greater than forty five degrees of sweep among cameras to generate a panoramic photograph. In one embodiment a plurality of camera modules may be configured to sample two or more narrow angle views e.g. less than forty five degrees of sweep among cameras to generate a stereoscopic photograph. In other embodiments a plurality of camera modules may be configured to generate a 3 D image or to otherwise display a depth perspective e.g. a z component etc. as shown on the display unit or any other display device.

In one embodiment a display unit may be configured to display a two dimensional array of pixels to form an image for display. The display unit may comprise a liquid crystal LCD display a light emitting diode LED display an organic LED display or any other technically feasible type of display. In certain embodiments the display unit may be able to display a narrower dynamic range of image intensity values than a complete range of intensity values sampled from a photographic scene such as within a single HDR image or over a set of two or more images comprising a multiple exposure or HDR image stack. In one embodiment images comprising an image stack may be merged according to any technically feasible HDR blending technique to generate a synthetic image for display within dynamic range constraints of the display unit . In one embodiment the limited dynamic range may specify an eight bit per color channel binary representation of corresponding color intensities. In other embodiments the limited dynamic range may specify more than eight bits e.g. 10 bits 12 bits or 14 bits etc. per color channel binary representation.

As shown the processor complex includes a processor subsystem and may include a memory subsystem . In one embodiment processor complex may comprise a system on a chip SoC device that implements processor subsystem and memory subsystem comprises one or more DRAM devices coupled to the processor subsystem . In another embodiment the processor complex may comprise a multi chip module MCM encapsulating the SoC device and the one or more DRAM devices comprising the memory subsystem .

The processor subsystem may include without limitation one or more central processing unit CPU cores a memory interface input output interfaces unit and a display interface unit each coupled to an interconnect . The one or more CPU cores may be configured to execute instructions residing within the memory subsystem volatile memory NV memory or any combination thereof. Each of the one or more CPU cores may be configured to retrieve and store data through interconnect and the memory interface . In one embodiment each of the one or more CPU cores may include a data cache and an instruction cache. Additionally two or more of the CPU cores may share a data cache an instruction cache or any combination thereof. In one embodiment a cache hierarchy is implemented to provide each CPU core with a private cache layer and a shared cache layer.

In some embodiments processor subsystem may include one or more graphics processing unit GPU cores . Each GPU core may comprise a plurality of multi threaded execution units that may be programmed to implement without limitation graphics acceleration functions. In various embodiments the GPU cores may be configured to execute multiple thread programs according to well known standards e.g. OpenGL WebGL OpenCL CUDA etc. and or any other programmable rendering graphic standard. In certain embodiments at least one GPU core implements at least a portion of a motion estimation function such as a well known Harris detector or a well known Hessian Laplace detector. Such a motion estimation function may be used at least in part to align images or portions of images within an image stack. For example in one embodiment an HDR image may be compiled based on an image stack where two or more images are first aligned prior to compiling the HDR image.

As shown the interconnect is configured to transmit data between and among the memory interface the display interface unit the input output interfaces unit the CPU cores and the GPU cores . In various embodiments the interconnect may implement one or more buses one or more rings a cross bar a mesh or any other technically feasible data transmission structure or technique. The memory interface is configured to couple the memory subsystem to the interconnect . The memory interface may also couple NV memory volatile memory or any combination thereof to the interconnect . The display interface unit may be configured to couple a display unit to the interconnect . The display interface unit may implement certain frame buffer functions e.g. frame refresh etc. . Alternatively in another embodiment the display unit may implement certain frame buffer functions e.g. frame refresh etc. . The input output interfaces unit may be configured to couple various input output devices to the interconnect .

In certain embodiments a camera module is configured to store exposure parameters for sampling each image associated with an image stack. For example in one embodiment when directed to sample a photographic scene the camera module may sample a set of images comprising the image stack according to stored exposure parameters. A software module comprising programming instructions executing within a processor complex may generate and store the exposure parameters prior to directing the camera module to sample the image stack. In other embodiments the camera module may be used to meter an image or an image stack and the software module comprising programming instructions executing within a processor complex may generate and store metering parameters prior to directing the camera module to capture the image. Of course the camera module may be used in any manner in combination with the processor complex .

In one embodiment exposure parameters associated with images comprising the image stack may be stored within an exposure parameter data structure that includes exposure parameters for one or more images. In another embodiment a camera interface unit not shown in within the processor complex may be configured to read exposure parameters from the exposure parameter data structure and to transmit associated exposure parameters to the camera module in preparation of sampling a photographic scene. After the camera module is configured according to the exposure parameters the camera interface may direct the camera module to sample the photographic scene the camera module may then generate a corresponding image stack. The exposure parameter data structure may be stored within the camera interface unit a memory circuit within the processor complex volatile memory NV memory the camera module or within any other technically feasible memory circuit. Further in another embodiment a software module executing within processor complex may generate and store the exposure parameter data structure.

In one embodiment the digital camera may be configured to include a digital photographic system such as digital photographic system of . As shown the digital camera includes a camera module which may include optical elements configured to focus optical scene information representing a photographic scene onto an image sensor which may be configured to convert the optical scene information to an electronic representation of the photographic scene.

Additionally the digital camera may include a strobe unit and may include a shutter release button for triggering a photographic sample event whereby digital camera samples one or more images comprising the electronic representation. In other embodiments any other technically feasible shutter release mechanism may trigger the photographic sample event e.g. such as a timer trigger or remote control trigger etc. .

In one embodiment the mobile device may be configured to include a digital photographic system e.g. such as digital photographic system of which is configured to sample a photographic scene. In various embodiments a camera module may include optical elements configured to focus optical scene information representing the photographic scene onto an image sensor which may be configured to convert the optical scene information to an electronic representation of the photographic scene. Further a shutter release command may be generated through any technically feasible mechanism such as a virtual button which may be activated by a touch gesture on a touch entry display system comprising display unit or a physical button which may be located on any face or surface of the mobile device . Of course in other embodiments any number of other buttons external inputs outputs or digital inputs outputs may be included on the mobile device and which may be used in conjunction with the camera module .

As shown in one embodiment a touch entry display system comprising display unit is disposed on the opposite side of mobile device from camera module . In certain embodiments the mobile device includes a user facing camera module and may include a user facing strobe unit not shown . Of course in other embodiments the mobile device may include any number of user facing camera modules or rear facing camera modules as well as any number of user facing strobe units or rear facing strobe units.

In some embodiments the digital camera and the mobile device may each generate and store a synthetic image based on an image stack sampled by camera module . The image stack may include one or more images sampled under ambient lighting conditions one or more images sampled under strobe illumination from strobe unit or a combination thereof.

In one embodiment the camera module may be configured to control strobe unit through strobe control signal . As shown a lens is configured to focus optical scene information onto image sensor to be sampled. In one embodiment image sensor advantageously controls detailed timing of the strobe unit though the strobe control signal to reduce inter sample time between an image sampled with the strobe unit enabled and an image sampled with the strobe unit disabled. For example the image sensor may enable the strobe unit to emit strobe illumination less than one microsecond or any desired length after image sensor completes an exposure time associated with sampling an ambient image and prior to sampling a strobe image.

In other embodiments the strobe illumination may be configured based on a desired one or more target points. For example in one embodiment the strobe illumination may light up an object in the foreground and depending on the length of exposure time may also light up an object in the background of the image. In one embodiment once the strobe unit is enabled the image sensor may then immediately begin exposing a strobe image. The image sensor may thus be able to directly control sampling operations including enabling and disabling the strobe unit associated with generating an image stack which may comprise at least one image sampled with the strobe unit disabled and at least one image sampled with the strobe unit either enabled or disabled. In one embodiment data comprising the image stack sampled by the image sensor is transmitted via interconnect to a camera interface unit within processor complex . In some embodiments the camera module may include an image sensor controller which may be configured to generate the strobe control signal in conjunction with controlling operation of the image sensor .

In one embodiment the camera module may be configured to sample an image based on state information for strobe unit . The state information may include without limitation one or more strobe parameters e.g. strobe intensity strobe color strobe time etc. for directing the strobe unit to generate a specified intensity and or color of the strobe illumination . In one embodiment commands for configuring the state information associated with the strobe unit may be transmitted through a strobe control signal which may be monitored by the camera module to detect when the strobe unit is enabled. For example in one embodiment the camera module may detect when the strobe unit is enabled or disabled within a microsecond or less of the strobe unit being enabled or disabled by the strobe control signal . To sample an image requiring strobe illumination a camera interface unit may enable the strobe unit by sending an enable command through the strobe control signal . In one embodiment the camera interface unit may be included as an interface of input output interfaces in a processor subsystem of the processor complex of The enable command may comprise a signal level transition a data packet a register write or any other technically feasible transmission of a command. The camera module may sense that the strobe unit is enabled and then cause image sensor to sample one or more images requiring strobe illumination while the strobe unit is enabled. In such an implementation the image sensor may be configured to wait for an enable signal destined for the strobe unit as a trigger signal to begin sampling a new exposure.

In one embodiment camera interface unit may transmit exposure parameters and commands to camera module through interconnect . In certain embodiments the camera interface unit may be configured to directly control strobe unit by transmitting control commands to the strobe unit through strobe control signal . By directly controlling both the camera module and the strobe unit the camera interface unit may cause the camera module and the strobe unit to perform their respective operations in precise time synchronization. In one embodiment precise time synchronization may be less than five hundred microseconds of event timing error. Additionally event timing error may be a difference in time from an intended event occurrence to the time of a corresponding actual event occurrence.

In another embodiment camera interface unit may be configured to accumulate statistics while receiving image data from camera module . In particular the camera interface unit may accumulate exposure statistics for a given image while receiving image data for the image through interconnect . Exposure statistics may include without limitation one or more of an intensity histogram a count of over exposed pixels a count of under exposed pixels an intensity weighted sum of pixel intensity or any combination thereof. The camera interface unit may present the exposure statistics as memory mapped storage locations within a physical or virtual address space defined by a processor such as one or more of CPU cores within processor complex . In one embodiment exposure statistics reside in storage circuits that are mapped into a memory mapped register space which may be accessed through the interconnect . In other embodiments the exposure statistics are transmitted in conjunction with transmitting pixel data for a captured image. For example the exposure statistics for a given image may be transmitted as in line data following transmission of pixel intensity data for the captured image. Exposure statistics may be calculated stored or cached within the camera interface unit .

In one embodiment camera interface unit may accumulate color statistics for estimating scene white balance. Any technically feasible color statistics may be accumulated for estimating white balance such as a sum of intensities for different color channels comprising red green and blue color channels. The sum of color channel intensities may then be used to perform a white balance color correction on an associated image according to a white balance model such as a gray world white balance model. In other embodiments curve fitting statistics are accumulated for a linear or a quadratic curve fit used for implementing white balance correction on an image.

In one embodiment camera interface unit may accumulate spatial color statistics for performing color matching between or among images such as between or among an ambient image and one or more images sampled with strobe illumination. As with the exposure statistics the color statistics may be presented as memory mapped storage locations within processor complex . In one embodiment the color statistics are mapped in a memory mapped register space which may be accessed through interconnect within processor subsystem . In other embodiments the color statistics may be transmitted in conjunction with transmitting pixel data for a captured image. For example in one embodiment the color statistics for a given image may be transmitted as in line data following transmission of pixel intensity data for the image. Color statistics may be calculated stored or cached within the camera interface .

In one embodiment camera module may transmit strobe control signal to strobe unit enabling the strobe unit to generate illumination while the camera module is sampling an image. In another embodiment camera module may sample an image illuminated by strobe unit upon receiving an indication signal from camera interface unit that the strobe unit is enabled. In yet another embodiment camera module may sample an image illuminated by strobe unit upon detecting strobe illumination within a photographic scene via a rapid rise in scene illumination. In one embodiment a rapid rise in scene illumination may include at least a rate of increasing intensity consistent with that of enabling strobe unit . In still yet another embodiment camera module may enable strobe unit to generate strobe illumination while sampling one image and disable the strobe unit while sampling a different image.

In one embodiment the camera module may be in communication with an application processor . The camera module is shown to include image sensor in communication with a controller . Further the controller is shown to be in communication with the application processor .

In one embodiment the application processor may reside outside of the camera module . As shown the lens may be configured to focus optical scene information onto image sensor to be sampled. The optical scene information sampled by the image sensor may then be communicated from the image sensor to the controller for at least one of subsequent processing and communication to the application processor . In another embodiment the controller may control storage of the optical scene information sampled by the image sensor or storage of processed optical scene information.

In another embodiment the controller may enable a strobe unit to emit strobe illumination for a short time duration e.g. less than one microsecond etc. after image sensor completes an exposure time associated with sampling an ambient image. Further the controller may be configured to generate strobe control signal in conjunction with controlling operation of the image sensor .

In one embodiment the image sensor may be a complementary metal oxide semiconductor CMOS sensor or a charge coupled device CCD sensor. In another embodiment the controller and the image sensor may be packaged together as an integrated system or integrated circuit. In yet another embodiment the controller and the image sensor may comprise discrete packages. In one embodiment the controller may provide circuitry for receiving optical scene information from the image sensor processing of the optical scene information timing of various functionalities and signaling associated with the application processor . Further in another embodiment the controller may provide circuitry for control of one or more of exposure shuttering white balance and gain adjustment. Processing of the optical scene information by the circuitry of the controller may include one or more of gain application amplification and analog to digital conversion. After processing the optical scene information the controller may transmit corresponding digital pixel data such as to the application processor .

In one embodiment the application processor may be implemented on processor complex and at least one of volatile memory and NV memory or any other memory device and or system. The application processor may be previously configured for processing of received optical scene information or digital pixel data communicated from the camera module to the application processor .

In one embodiment the network service system may be configured to provide network access to a device implementing a digital photographic system. As shown network service system includes a wireless mobile device a wireless access point a data network data center and a data center . The wireless mobile device may communicate with the wireless access point via a digital radio link to send and receive digital data including data associated with digital images. The wireless mobile device and the wireless access point may implement any technically feasible transmission techniques for transmitting digital data via digital a radio link without departing the scope and spirit of the present invention. In certain embodiments one or more of data centers may be implemented using virtual constructs so that each system and subsystem within a given data center may comprise virtual machines configured to perform specified data processing and network tasks. In other implementations one or more of data centers may be physically distributed over a plurality of physical sites.

The wireless mobile device may comprise a smart phone configured to include a digital camera a digital camera configured to include wireless network connectivity a reality augmentation device a laptop configured to include a digital camera and wireless network connectivity or any other technically feasible computing device configured to include a digital photographic system and wireless network connectivity.

In various embodiments the wireless access point may be configured to communicate with wireless mobile device via the digital radio link and to communicate with the data network via any technically feasible transmission media such as any electrical optical or radio transmission media. For example in one embodiment wireless access point may communicate with data network through an optical fiber coupled to the wireless access point and to a router system or a switch system within the data network . A network link such as a wide area network WAN link may be configured to transmit data between the data network and the data center .

In one embodiment the data network may include routers switches long haul transmission systems provisioning systems authorization systems and any technically feasible combination of communications and operations subsystems configured to convey data between network endpoints such as between the wireless access point and the data center . In one implementation a wireless the mobile device may comprise one of a plurality of wireless mobile devices configured to communicate with the data center via one or more wireless access points coupled to the data network .

Additionally in various embodiments the data center may include without limitation a switch router and at least one data service system . The switch router may be configured to forward data traffic between and among a network link and each data service system . The switch router may implement any technically feasible transmission techniques such as Ethernet media layer transmission layer switching layer routing and the like. The switch router may comprise one or more individual systems configured to transmit data between the data service systems and the data network .

In one embodiment the switch router may implement session level load balancing among a plurality of data service systems . Each data service system may include at least one computation system and may also include one or more storage systems . Each computation system may comprise one or more processing units such as a central processing unit a graphics processing unit or any combination thereof. A given data service system may be implemented as a physical system comprising one or more physically distinct systems configured to operate together. Alternatively a given data service system may be implemented as a virtual system comprising one or more virtual systems executing on an arbitrary physical system. In certain scenarios the data network may be configured to transmit data between the data center and another data center such as through a network link .

In another embodiment the network service system may include any networked mobile devices configured to implement one or more embodiments of the present invention. For example in some embodiments a peer to peer network such as an ad hoc wireless network may be established between two different wireless mobile devices. In such embodiments digital image data may be transmitted between the two wireless mobile devices without having to send the digital image data to a data center .

The image set includes two or more source images which may be generated by capturing images with the digital photographic system using different exposure parameters.

In one embodiment the resulting image represents a source image that is selected from the image set . The source image may be selected according to any technically feasible technique. For example in one embodiment a given source image may be selected automatically based on based on exposure quality. In an alternative embodiment a given source image may be selected manually through a UI control discussed in greater detail below in . The UI control generates a selection parameter that indicates the manual selection of a particular source image in the image set . An image processing subsystem is configured to generate the resulting image by processing the selected source image s via an image processing pipeline. In certain embodiments the image processing subsystem automatically selects a source image and transmits a corresponding recommended setting to the UI control . The recommended setting indicates through the position of a control knob of the UI control which source image was automatically selected. In one embodiment a user may keep the recommended setting or select a different source image to use for generating the resulting image using the UI control .

In an alternative embodiment the viewer application may be configured to combine two or more source images to generate a resulting image . Further the two or more source images may be mutually aligned by the image processing subsystem prior to being combined. The relative position of the control knob between any two discrete positions of UI control may specify a weight assigned to each of two source images . The weight may be used to perform a transparency opacity blend known as an alpha blend between the two source images .

In certain implementations viewer application may include an image cache configured to include a set of cached images corresponding to the source images . The image cache may provide images that may be used to readily and efficiently generate or display resulting image in response to real time changes to the user interface. In one embodiment the cached images may be the result of processing one or more of the source images according to a particular image processing pipeline associated with the current configuration settings of the user interface. That way when a user adjusts one of the UI elements such as UI control between two particular discrete positions of the UI control the viewer application can retrieve the cached images from the image cache rather than processing the source images via the image processing pipeline. In one embodiment the cached images are rendered to a resolution substantially identical to the screen resolution of display unit . Caching images may advantageously reduce power consumption associated with rendering a given source image for display.

The viewer application also includes a UI software module . In one embodiment the UI software module may be configured to render an application window for display on a display device. The application window may include a number of UI elements described in more detail below in . In addition the UI software module may be configured to detect user input with respect to the user interface such as when a user modifies one of the configuration settings for a particular UI element in the user interface.

In one embodiment when the UI software module detects that user input has modified one of the configuration changes the UI software module may update the specification accordingly. In another embodiment the specification is an object or data structure stored in a memory that represents the current configuration settings for the image processing pipeline.

As shown in the UI software module generates an application window for presentation on a display device such as display unit of the digital photographic system . The application window may include graphical representations of a resulting image a UI control and one or more additional UI elements . The application window may comprise a plurality of pixel information stored in e.g. a frame buffer in a memory and scanned out to the display device as a video signal. As discussed above the resulting image may be a combination of one or more images from the image stack as processed by the image processing pipeline. The resulting image may be generated based on the configuration settings for the UI control and or the UI elements .

In one embodiment the UI control may comprise a linear slider control with a control knob configured to slide along a slide path . A user may position the control knob by performing a slide gesture. For example in one embodiment the slide gesture may include touching the control knob in a current position and sliding the control knob to a new position. Alternatively in another embodiment the user may touch along the slide path to move the control knob to a new position defined by a location of the touch.

In one embodiment positioning the control knob into a discrete position along the slide path may cause a corresponding source image to be processed by the image processing pipeline to generate the resulting image . For example in one embodiment a user may move control knob into discrete position to indicate that source image is selected. The UI software module may indicate to the image processing subsystem that one or more configuration settings have been modified and update the specification if necessary. In one embodiment the control knob may be configured to snap to a closest discrete position when released by a user withdrawing their finger.

In an alternative embodiment the control knob may be positioned between two discrete positions to indicate that resulting image should be generated based on two source images corresponding to the two discrete positions . For example if the control knob is positioned between discrete position and discrete position then the image processing subsystem generates resulting image from source images and . In one embodiment the image processing subsystem generates resulting image by aligning source images and and performing an alpha blend between the aligned images according to the position of the control knob . For example if the control knob is positioned to be one quarter of the distance from discrete position to discrete position along slide path then an aligned image corresponding to source image may be blended with twenty five percent opacity over a fully opaque aligned image corresponding to source image .

In one embodiment UI control may be configured to include a discrete position for each source image within a given image set being viewed. Each image set stored within the digital photographic system of may include a different number of source images and UI control may be configured to establish discrete positions to correspond to the source images for a given image set .

In addition to UI control the application window may include one or more UI elements that enable a user of the viewer application to adjust configuration settings associated with the image processing pipeline. Although only shown conceptually in the UI elements may take any form such as drop down menus text boxes check boxes combo boxes radio buttons icons and the like. In addition to UI elements displayed on the application window additional UI elements may be displayed within dialog boxes that are displayed based on user input such as by selecting one of UI elements .

In one embodiment the UI elements may be associated with configuration settings that define the image processing pipeline. For example one UI element may comprise a drop down menu that lists a number of filters to include in the image processing pipeline. A user may select each filter that they want to include in the image processing pipeline by selecting the items in the drop down menu. As a particular filter is selected other UI elements may be used to specify various configuration settings associated with that filter or to specify other filters to be included in a filter pipeline. For example in one embodiment one UI element may be a checkbox for specifying whether the filter should be included in the image processing pipeline. Another UI element may be a textbox or drop down menu for selecting a particular value used when applying that filter such as a color value a window size etc. Another UI element may be a series of radio buttons for selecting one of a pre configured set of values associated with the filter. It will be appreciated that the UI elements described herein are only for illustrative purposes and any UI elements capable of receiving user input to specify a value or configuration setting associated with the image processing pipeline is within the scope of the embodiments.

As discussed above in response to user input that modifies the setting of one of the UI elements the UI software module may be configured to update the specification based on the state of the UI element . In one embodiment the specification is only updated based on a subset of UI elements that are related to a change of the image processing pipeline not associated with a reference to a uniform variable. In such an embodiment a modification to the UI element may be reflected by changing a value in the memory corresponding to the reference to the uniform variable. In contrast other UI elements may be related to a change of the image processing pipeline that is not associated with a reference to a uniform variable. In one embodiment such changes may be fundamental to the image processing pipeline such as the addition or removal of a filter a change in the filter ordering and so forth such that the UI software module may update the specification and notify the image processing subsystem that the image processing pipeline needs to be updated.

As shown in the image processing pipeline receives one or more source images and generates a resulting image . At least one source image is processed by a filter e.g. etc. and the resulting filtered image is forwarded to the next filter e.g. etc. . As image data makes its way down the pipeline eventually the image data is processed by the last filter in the image processing pipeline such as filter and the resulting image data corresponds to the resulting image . In one embodiment each filter is implemented as a fragment shader configured to execute as a multi threaded program where each thread generates one pixel of the resulting image . In one embodiment at least one source image is passed to the image processing pipeline as a texture map.

It will be appreciated that the types of filters implemented in image processing pipelines are numerous. Each manufacturer of a digital camera may implement a different image processing pipeline that is tailored to their specific hardware. Examples of the types of filters implemented in image processing pipelines include but are not limited to white balance filters Debayer filters gamma correction filters color correction filters blur filters unsharp mask filters noise reducing filters chroma subsampling filters and compression filters. It will be appreciated that any type of image processing filter may be implemented as a node of the image processing pipeline .

In many digital cameras on the market today the image processing pipeline is pre configured. For example an image may be captured by the image sensor and sampled in a predefined way to generate raw image data. The raw image data is then demosaiced i.e. Debayered to generate a digital image in an RGB color space. A white balance function may be applied. A filter may also be applied e.g. Gaussian blur etc. . Gamma correction may be applied. The image may be converted from one color space to another color space. Finally the image may be subsampled and compressed to be stored in a particular image format. This image processing pipeline may be implemented in hardware software or a combination of hardware and software. In other embodiments the processing pipeline may not be changed except that certain filters may be bypassed depending on the cameras settings such as by not applying the white balance filter .

These image processing pipelines are typically limited to a small number of conventional filters used to make the images have a uniform look. In other words more complex image processing filters are typically not included in the pre configured image processing pipelines. Instead users must transfer the digital images to a desktop computer and edit the images using photo editing software such as Adobe Photoshop. The ability to implement the large number of filters available in the photo editing software in a particular image processing pipeline would be impractical and the limitations of the hardware would quickly be reached by trying to implement a pre configured image processing pipeline with tens or even hundreds of different filters. By contrast embodiments of the present invention enable greater flexibility by allowing a user to select specific filters from a set of available filters and an order for performing filter operations using the selected filters. For example as shown below in an image processing pipeline can be configured on the fly in order to implement only a subset of filters selected from a large number of available filters selected by a user. As such a large variety of image processing filters may be made available to a user while not being constrained by the hardware of the device.

As used herein a graphics language refers to any system of software language semantics API functions and related functions used to implement graphics operations. In one embodiment a graphics language may refer to the OpenGL standard graphics library e.g. which may define a standard set of functions that comprise an application programming interface etc. . In one embodiment specialized graphics hardware such as the GPU cores of may be utilized to implement the functions defined by the GL and API.

In one embodiment the manufacturer of the graphics hardware may supply a device driver that executes on a host processor where the device driver may implement the API calls defined in the OpenGL standards or some other graphics library like Direct3D . An application program such as the viewer application may then include calls to the API functions that are linked to the device driver. In one embodiment when the application makes an API call the driver is configured to implement the corresponding operation typically with the use of the specialized graphics hardware.

In another embodiment GPUs may be specifically designed to process a large amount of data in parallel. This may be particularly useful in image processing applications. Most filters used to process an image can be executed in parallel for each pixel in the image thereby performing the operation much faster than if each pixel of the image was processed serially. For example in one embodiment a blur function will compute a filtered pixel value for each pixel in the image based on a number of pixel values surrounding that particular pixel. These filters may be implemented by creating what is referred to as a shader program or more specifically a pixel or fragment shader program.

In the context of the present description the shader program is defined using source code that includes calls to functions defined in the graphics library. The shader program may be written as a small code segment or group of instructions that will be applied to each pixel in the image being processed. The output of the shader program may be a pixel value e.g. an rgb vector an rgba vector etc. and may correspond to a particular pixel in the intermediate results or resulting image .

It will be appreciated that the filters included in the image processing pipeline may be implemented as shader programs that are executed on one or more GPU cores of the processor complex of . Therefore the abstract image processing pipeline of may be implemented by combining a number of shader programs that implement each of the filters and then controlling the flow of data through the shader programs to generate the resulting image .

As shown in a code generator module may be implemented within the viewer application . The code generator module receives the specification that defines the image processing pipeline . As discussed above the specification may be a data structure that includes the configuration settings associated with the user interface associated with the viewer application . The code generator module analyzes the specification to determine an order of filters included in the image processing pipeline .

In one embodiment once the code generator module determines which filters to include in the image processing pipeline the code generator module may fetch one or more code segments corresponding to each of the filters . In one embodiment the code segments may be stored in a non volatile memory such as NV memory of the digital photographic system . In another embodiment the code segments as well as metadata associated with the code segments may be stored in a database and fetched in response to a query to the database. The database may be stored in the non volatile memory of the device or stored remotely and accessed via a web service. In one embodiment if the database is stored remotely then the code generator module may be configured to fetch code segments the first time they are included in the image processing pipeline and then cache the code segments locally for reuse in later implementations of the image processing pipeline . In such an embodiment this may allow a large number of filters to be made available on the Internet but only those filters that are used to be stored on the local device.

Each code segment may include a plain text version of a shader program for a particular filter . In other words the code segment for a particular filter may include the source code for the filter including any graphics library functions needed to implement the filter on the graphics hardware. For example as shown below Table 1 illustrates the source code included in a first code segment for a no filter operation i.e. the input pixel is returned as the output pixel and Table 2 illustrates the source code included in a second code segment for a white balance operation.

In one embodiment the code segments shown above may be stored as objects e.g. files etc. in the non volatile memory. In addition the code segments may be associated with metadata that describes the code segments such as a length of the code segment in bytes the names of the parameters passed to the filter in the code segment whether the code segment is associated with any uniform variables and so forth. Metadata may include any useful information about the code segment that may be utilized by the code generator module or the viewer application .

Again it will be appreciated that the code segments shown above are for illustrative purposes and other types of code segments are within the scope of the present disclosure. For example the code segments may include source code for a Debayer operation an unsharp mask operation a noise reducing operation an edge detection operation a color correction operation a blur operation and so forth.

Once the code generator module has fetched the required code segments the code generator module may stitch the code segments together to generate a single GL representation of the image processing pipeline . More specifically the code generator module may create a single plain text file or text string with source code for instantiating each of the filters included in the image processing pipeline using the graphics hardware. In one embodiment the code generator module may include each of the fetched code segments in the GL representation . The code generator module also may define references to uniform variables such that one or more values included in the configuration settings may be passed to the filters during execution.

In one embodiment references to uniform variables are included such that certain values specified by the UI elements may be changed without requiring a new GL representation to be generated and compiled. For example in Table 2 above the white balance operation takes a parameter called tilt that may be changed within the UI to adjust the manner that white balance is performed. The tilt parameter allows for compensation of the color temperature and a green magenta shift of the light used to illuminate a scene.

Finally the code generator module includes a function in the GL representation for specifying the processing order for each of the defined filters . In one embodiment the function may take a pixel value as an input parameter process the pixel value based on a first filter process the resulting pixel value based on a second filter and so forth until the pixel value has been processed by each of the filters included in the image processing pipeline . Table 3 illustrates a GL representation of an image processing pipeline having two filters the no filter operation and the white balance operation shown above in Tables 1 and 2 respectively.

In one embodiment the code generator module may instantiate a stitcher object from a stitcher class. In one embodiment the stitcher class may include code for adding references to filters to the stitcher object and code for generating the GL representation based on the filters associated with the stitcher object. The main body of the code generator module may include a loop for parsing the specification fetching the code segments for the filters identified by the specification querying the metadata associated with the code segments to determine whether any uniform variables are defined for the filters and adding the code segments and unique uniform variable definitions to the GL representation . The main body of the code generator module may also include an instruction for adding a main function to the GL representation that instantiates the processing order of each of the filters included in the image processing pipeline . Alternatively the code segments may be instantiated and stitched as in line code within the GL representation . In other embodiments more than one image processing pipelines may be similarly generated each configured to perform certain operations that may operate together to independently within the context of an application such as viewer application .

As shown in the viewer application includes the UI software module and the image processing subsystem which includes the code generator module . The UI software module receives user input that changes at least one configuration setting associated with at least one UI element in the user interface of viewer application . In response the UI software module updates the specification to reflect the image processing pipeline defined by the current configuration settings. The code generator module receives the updated specification and generates the GL representation of the image processing pipeline . Then the image processing subsystem transmits the GL representation to a compile service to compile the GL representation and generate a binary executable instantiation of the image processing pipeline . The image processing subsystem then loads the binary executable instantiation of the image processing pipeline on the graphics hardware such as one or more GPU cores of the digital photographic system . Then the image processing subsystem causes one or more images such as source images to be processed according to the image processing pipeline on the graphics hardware to generate the resulting image .

In one embodiment the compile service may comprise a function included in a device driver for the graphics hardware and may be executed by a host processor. In one embodiment the compile service may generate the binary executable instantiation of the image processing pipeline in a matter of milliseconds. Consequently a resulting image may be generated from stored source images in a very small time allowing new image processing pipelines to be defined and compiled in real time in response to user input associated with the user interface. Thus a user may quickly define new image processing pipelines that include different filters to process a given image using a subset of simple UI elements such as menus and checkboxes and the like. These customizable image processing pipelines may be generated and executed in a matter of milliseconds in order to show the user the response of their changes to the resulting image .

In one embodiment the UI software module may monitor the user interface to detect the user input . If any modifications are made the UI software module may determine whether the changes require a new image processing pipeline to be generated. If so the code generator module creates a modified GL representation that represents the new image processing pipeline . The modified GL representation may be compiled by the compile service and loaded into the graphics hardware to re process the source image s .

One advantage of the present invention is that a user may specify a customizable image processing pipeline for use on mobile devices that may have scaled down or power limited hardware. The code for each distinct image processing pipeline is kept lightweight and only includes the code for the included filters. Thus this code may be easily executed on limited graphics hardware without experiencing undue delays. In conventional systems the image processing pipeline would typically be implemented with all available filters and a manner to bypass some filters based on configuration settings in the device or the filters may be modular but require intermediate buffers to store materialized intermediate images. However because the source code includes the instructions for every available filter processing images with the pipeline would take a long time and practical limitations required that only a small number of filters would be included in the image processing pipeline. Other filters would need to be manually added later using image editing software. With the system disclosed herein a user may add a large variety of filters directly into an image processing pipeline of a device such that those filters are immediately applied to the images captured by the device. Furthermore the device enables real time modifications to be made to the image processing pipeline so that a user may determine the best possible configuration for their application.

While various embodiments have been described above it should be understood that they have been presented by way of example only and not limitation. Thus the breadth and scope of a preferred embodiment should not be limited by any of the above described exemplary embodiments but should be defined only in accordance with the following claims and their equivalents.

