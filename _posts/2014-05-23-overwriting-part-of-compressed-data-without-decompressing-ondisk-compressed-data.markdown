---

title: Overwriting part of compressed data without decompressing on-disk compressed data
abstract: Partially overwriting a compression group without decompressing compressed data can consumption of resources for the decompression. A storage server partially overwrites the compression group when a file block identifier of a client's write request resolves to the compression group. The compression group remains compressed while the partial overwriting is performed.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09477420&OS=09477420&RS=09477420
owner: NetApp, Inc.
number: 09477420
owner_city: Sunnyvale
owner_country: US
publication_date: 20140523
---
This application is a Continuation of and claims the priority benefit of U.S. application Ser. No. 13 099 283 filed May 2 2011.

At least one feature of the disclosure pertains to data storage systems and more particularly to overwriting part of compressed data without decompressing on disk compressed data in an extent based storage architecture.

A portion of the disclosure of this patent document contains material which is subject to copyright protection. The copyright owner has no objection to the facsimile reproduction by anyone of the patent document or the patent disclosure as it appears in the Patent and Trademark Office patent file or records but otherwise reserves all copyright rights whatsoever. The following notice applies to the software and data as described below and in the drawings hereto Copyright 2011 NetApp Inc. All Rights Reserved.

Various forms of network based storage systems exist today. These forms include network attached storage NAS storage area networks SAN s and others. Network based storage systems are commonly used for a variety of purposes such as providing multiple users with access to shared data backing up critical data e.g. by data mirroring etc.

A network based storage system typically includes at least one storage server which is a processing system configured to store and retrieve data on behalf of one or more client processing systems clients . The data is stored and retrieved as storage objects such as blocks and or files. A block is a sequence of bytes or bits of data having a predetermined length. A file is a collection of related bytes or bits having an arbitrary length. In the context of NAS a storage server operates on behalf of one or more clients to store and manage file level access to data. In the context of NAS a storage server may be a file server which is sometimes called a filer . A filer operates on behalf of one or more clients to store and manage shared files. The files may be stored in a storage system that includes one or more arrays of mass storage devices such as magnetic or optical disks or tapes by using a data storage scheme such as Redundant Array of Inexpensive Disks RAID . Additionally the mass storage devices in each array may be organized into one or more separate RAID groups. In a SAN context a storage server provides clients with block level access to stored data rather than file level access. Some storage servers are capable of providing clients with both file level access and block level access such as certain storage servers made by NetApp Inc. NetApp of Sunnyvale Calif.

WAFL aggregate is a physical storage container that can store data in the WAFL file system. Flexible volume is a logical volume that allows the virtualization of the allocation of volumes on physical storage . Thereby multiple independently managed flexible volumes can share the same physical storage e.g. physical storage . The virtualization requires mapping between virtual volume block numbers VVBNs used by flexible volume and physical volume block numbers PVBNs used by WAFL aggregate to access data stored in physical storage . A PVBN as used herein refers disk blocks that have been abstracted into a single linear sequence in the aggregate. Each volume container corresponds to a flexible volume . Volume container contains all the data blocks for a corresponding flexible volume .

As used herein a block offset or an offset refers to a distance in blocks from the beginning of a storage object such as a volume file extent etc. Block addresses used within flexible volume refer to block offsets within volume container . Since volume container contains every block within flexible volume there are two ways to refer to the location of a particular block. The PVBN specifies the location of a block within WAFL aggregate . The VVBN specifies the offset of the block within the container file. When a block in a file is requested flexible volume translates the file offset into a VVBN. The VVBN is passed from flexible volume to volume container . Volume container translates the VVBN to a PVBN. The PVBN is then used to access the requested block in physical storage . Once a VVBN has been translated into a PVBN the block pointer for the PVBN in flexible volume is updated to include e.g. in a cache the PVBN for the VVBN. Thereby the next time the requested block is required the flexible volume can use the stored PVBN to access physical storage .

Current examples of WAFL define a file as a tree of indirect blocks. Each indirect block in the tree has a fixed span a fixed number of entries each pointing to another block in the tree. Extents are represented using an entry for each block within the extent. An extent as used herein refers a contiguous group of one or more blocks. As a result the amount of indirect block metadata is linear with respect to the size of the file. Additionally disk gardening techniques such as segment cleaning file reallocation etc. are complicated by caching PVBN pointers in VVBN blocks.

Storage systems often use a predetermined block size for all internal operations. For example WAFL uses 4 KB e.g. 4096 bytes blocks for both VVBN and PVBN as do client side file systems for file block numbers FBN . Block boundaries are expected to occur every 4 KB from an initial offset e.g. FBN 0 . Since file systems usually offset individual files based on these block boundaries application writers take advantage of a file system s block size and alignment to increase the performance of their input output I O operations for example always performing I O operations that are a multiple of 4 KB and always aligning these operations to the beginning of a file. Other file systems or applications such as a virtual machine may use a block boundary of a different size e.g. a virtual machine environment in which an initial master boot record block of 512 bytes is followed by the expected 4 KB blocks resulting in misalignment between FBN s and PVBN s. Additionally multiple virtual machines may share a single volume container and each virtual machine may misaligned by a different amount.

Compression groups data blocks together to make a compression group. The data blocks in the compression group are compressed in a smaller number of physical data blocks than the number of logical data blocks. A typical compression group requires 8 eight logical data blocks to be grouped together such that compressed data can be stored in less than 8 physical data blocks. This mapping between physical data blocks and logical data blocks requires the compression groups to be written as a single data block. Therefore the compression group is written to disk in full.

When a compression group is partially written by a user e.g. one logical data block is modified in a compression group of 8 logical data blocks all physical data blocks in the compression group are read the physical data blocks in the compression group are uncompressed and the modified data block is merged with the uncompressed data. If the system is using inline compression then compression of modified compression groups is performed immediately prior to writing out data to a disk and the compressed groups are all written out to disk. If a system is using background compression then the compression of a modified compression group is performed in the background once the compression group has been modified and the compressed data is written to disk. Random partial writes partial writes to different compression groups can therefore greatly affect performance of the storage system. In fact write performance can be up to 15 times slower for compressed volumes than for uncompressed volumes. Therefore although compression provides storage savings the degradation of performance may be disadvantageous enough to not do compression in a storage system.

Overwriting part of compressed data without decompressing on disk compressed data includes receiving a write request for a block of data in a compression group from a client wherein the compression group comprises a group of data blocks that is compressed wherein the block of data is uncompressed. The storage server partially overwrites the compression group wherein the compression group remains compressed while the partial overwriting is performed. The storage server determines whether the partially overwritten compression group including the uncompressed block of data should be compressed. The storage server defers compression of the partially overwritten compression group if the partially overwritten compression group should not be compressed. The storage server compresses the partially overwritten compression group if the partially overwritten compression group should be compressed.

In the following detailed description reference is made to the accompanying drawings in which like references indicate similar elements and in which is shown by way of illustration specific features of the disclosure that may be practiced. These features are described in sufficient detail to enable those skilled in the art to practice the features of the disclosure and it is to be understood that other features may be utilized and that logical mechanical electrical functional and other changes may be made without departing from the scope of the disclosure. The following detailed description is therefore not to be taken in a limiting sense and the scope of the disclosure is defined only by the appended claims.

As set forth in further detail below features are described for overwriting part of compressed data without decompressing on disk compressed data. Overwriting part of compressed data without decompressing on disk compressed data can include receiving a write request for a block of data in a compression group from a client wherein the compression group comprises a group of data blocks that is compressed wherein the block of data is uncompressed. The storage server partially overwrites the compression group wherein the compression group remains compressed while the partial overwriting is performed. The storage server determines whether the partially overwritten compression group including the uncompressed block of data should be compressed. The storage server defers compression of the partially overwritten compression group if the partially overwritten compression group should not be compressed. The storage server compresses the partially overwritten compression group if the partially overwritten compression group should be compressed.

Overwriting part of compressed data without decompressing on disk compressed data allows the compressed data to remain compressed while the overwrite occurs. Thus overwriting part of compressed data does not require uncompressing the data prior to overwriting it. Thus the partial overwrite of compressed data has almost the same performance as overwrites in a non compressed volume. Therefore compression can be used more easily in storage servers allowing data to be stored more efficiently.

Storage of data in storage units is managed by storage servers which receive and respond to various I O requests from clients directed to data stored in or to be stored in storage units . Data is accessed e.g. in response to the I O requests in units of data blocks which in some aspects are 4 KB in size although other data block sizes e.g. 512 bytes 2 KB 8 KB etc. may also be used. In one aspect 4 KB as used herein refers to 4 096 bytes. For alternative aspects 4 KB refers to 4 000 bytes. Storage units constitute mass storage devices which can include for example flash memory magnetic or optical disks or tape drives illustrated as disks A B . The storage devices can further be organized into arrays not illustrated of a Redundant Array of Inexpensive Disks Devices RAID scheme whereby storage servers access storage units using one or more RAID protocols. RAID is a data storage scheme that divides and replicates data among multiple hard disk drives e.g. in stripes of data. Data striping is the technique of segmenting logically sequential data such as a single file so that segments can be assigned to multiple physical devices hard drives. Redundant parity data is stored to allow problems to be detected and possibly fixed. For example if one were to configure a hardware based RAID 5 volume using three 250 GB hard drives two drives for data and one for parity the operating system would be presented with a single 500 GB volume and the example single file may be stored across the two data drives. Although illustrated as separate components a storage server and storage unit may be a part of housed within a single device.

Storage servers can provide file level service such as used in a network attached storage NAS environment block level service such as used in a storage area network SAN environment a service which is capable of providing both file level and block level service or any other service capable of providing other data access services. Although storage servers are each illustrated as single units in a storage server can in other aspects constitute a separate network element or module an N module and disk element or module a D module . According to one feature the D module includes storage access components for servicing client requests. In contrast the N module includes functionality that enables client access to storage access components e.g. the D module and may include protocol components such as Common Internet File System CIFS Network File System NFS or an Internet Protocol IP module for facilitating such connectivity. Details of a distributed architecture environment involving D modules and N modules are described further below with respect to and aspects of a D module and a N module are described further below with respect to .

In yet other aspects storage servers are referred to as network storage subsystems. A network storage subsystem provides networked storage services for a specific application or purpose. Examples of such applications include database applications web applications Enterprise Resource Planning ERP applications etc. e.g. executable by a client. Examples of such purposes include file archiving backup mirroring etc. provided for example on archive backup or secondary storage server connected to a primary storage server. A network storage subsystem can also include a collection of networked resources provided across multiple storage servers and or storage units.

In the example illustrated in one of the storage servers e.g. storage server A functions as a primary provider of data storage services to client . Data storage requests from client are serviced using disks A organized as one or more storage objects. A secondary storage server e.g. storage server B takes a standby role in a mirror relationship with the primary storage server replicating storage objects from the primary storage server to storage objects organized on disks of the secondary storage server e.g. disks B . In operation the secondary storage server does not service requests from client until data in the primary storage object becomes inaccessible such as in a disaster with the primary storage server such event considered a failure at the primary storage server. Upon a failure at the primary storage server requests from client intended for the primary storage object are serviced using replicated data i.e. the secondary storage object at the secondary storage server.

It will be appreciated that in other aspects network storage system may include more than two storage servers. In these cases protection relationships may be operative between various storage servers in system such that one or more primary storage objects from storage server A may be replicated to a storage server other than storage server B not shown in this figure . Secondary storage objects may further include protection relationships with other storage objects such that the secondary storage objects are replicated e.g. to tertiary storage objects to protect against failures with secondary storage objects. Accordingly the description of a single tier protection relationship between primary and secondary storage objects of storage servers should be taken as illustrative only.

Nodes may be operative as multiple functional components that cooperate to provide a distributed architecture of system . To that end each node may be organized as a network element or module N module A B a disk element or module D module A B and a management element or module M host A B . In one aspect each module includes a processor and memory for carrying out respective module operations. For example N module may include functionality that enables node to connect to client via network and may include protocol components such as a media access layer Internet Protocol IP layer Transport Control Protocol TCP layer User Datagram Protocol UDP layer and other protocols known in the art.

In contrast D module may connect to one or more storage devices via cluster switching fabric and may be operative to service access requests on devices . In one aspect the D module provides an extent based storage architecture and a partial overwrite module as will be described in greater detail below. In one aspect the D module includes storage access components such as a storage abstraction layer supporting multi protocol data access e.g. Common Internet File System protocol the Network File System protocol and the Hypertext Transfer Protocol a storage layer supporting storage protocols e.g. RAID protocol and a driver layer supporting storage device protocols e.g. Small Computer Systems Interface protocol for carrying out operations in support of storage access operations. In the aspect shown in a storage abstraction layer e.g. file system of the D module divides the physical storage of devices into storage objects. Requests received by node e.g. via N module may thus include storage object identifiers to indicate a storage object on which to carry out the request.

Also operative in node is M host which provides cluster services for node by performing operations in support of a distributed storage system image for instance across system . M host provides cluster services by managing a data structure such as a RDB RDB A RDB B which contains information used by N module to determine which D module owns services each storage object. The various instances of RDB across respective nodes may be updated regularly by M host using conventional protocols operative between each of the M hosts e.g. across network to bring them into synchronization with each other. A client request received by N module may then be routed to the appropriate D module for servicing to provide a distributed storage system image.

It should be noted that while shows an equal number of N and D modules constituting a node in the illustrative system there may be different number of N and D modules constituting a node in accordance with various features of overwriting part of compressed data without decompressing on disk compressed data. For example there may be a number of N modules and D modules of node A that does not reflect a one to one correspondence between the N and D modules of node B. As such the description of a node comprising one N module and one D module for each node should be taken as illustrative only.

The processor is the central processing unit CPU of the storage server and thus control its overall operation. The processor accomplishes this by executing software stored in memory . For one feature multiple processors or one or more processors with multiple cores are included in the storage server . For one feature individual adapters e.g. network adapter and storage adapter each include a processor and memory for carrying out respective module operations.

Memory includes storage locations addressable by processor network adapter and storage adapter for storing processor executable instructions and data structures associated with an extent based storage architecture. Storage operating system portions of which are typically resident in memory and executed by processor functionally organizes the storage server by invoking operations in support of the storage services provided by the storage server . It will be apparent to those skilled in the art that other processing means may be used for executing instructions and other memory means including various computer readable media may be used for storing program instructions pertaining to the inventive techniques described herein. It will also be apparent that some or all of the functionality of the processor and executable software can be provided by hardware such as integrated currents configured as programmable logic arrays ASICs and the like.

Network adapter comprises one or more ports to couple the storage server to one or more clients over point to point links or a network. Thus network adapter includes the mechanical electrical and signaling circuitry needed to couple the storage server to one or more client over a network. The network adapter may include protocol components such as a Media Access Control MAC layer Common Internet File System CIFS Network File System NFS Internet Protocol IP layer Transport Control Protocol TCP layer User Datagram Protocol UDP layer and other protocols known in the art for facilitating such connectivity. Each client may communicate with the storage server over the network by exchanging discrete frames or packets of data according to pre defined protocols such as TCP IP.

Storage adapter includes a plurality of ports having input output I O interface circuitry to couple the storage devices e.g. disks to bus over an I O interconnect arrangement such as a conventional high performance FC or SAS link topology. Storage adapter typically includes a device controller not illustrated comprising a processor and a memory for controlling the overall operation of the storage units in accordance with read and write commands received from storage operating system . In one aspect the storage operating system provides an extent based storage architecture and a partial overwrite module as will be described in greater detail below. As used herein data written by a device controller in response to a write command is referred to as write data whereas data read by device controller responsive to a read command is referred to as read data. 

User console enables an administrator to interface with the storage server to invoke operations and provide inputs to the storage server using a command line interface CLI or a graphical user interface GUI . In one aspect user console includes a monitor and keyboard.

When included as a node of a cluster such as cluster of the storage server further includes a cluster access adapter shown in phantom broken lines having one or more ports to couple the node to other nodes in a cluster. In one aspect Ethernet is used as the clustering protocol and interconnect media although it will be apparent to one of skill in the art that other types of protocols and interconnects can by utilized within the cluster architecture.

Multi protocol engine includes a media access layer of network drivers e.g. gigabit Ethernet drivers that interface with network protocol layers such as the IP layer and its supporting transport mechanisms the TCP layer and the User Datagram Protocol UDP layer . A file system protocol layer provides multi protocol file access and to that end includes support for the one or more of the Direct Access File System DAFS protocol the NFS protocol the CIFS protocol and the Hypertext Transfer Protocol HTTP protocol . A VI layer provides the VI architecture to provide direct access transport DAT capabilities such as RDMA as required by the DAFS protocol . An iSCSI driver layer provides block protocol access over the TCP IP network protocol layers while a FC driver layer receives and transmits block access requests and responses to and from the storage server. In certain cases a Fibre Channel over Ethernet FCoE layer not shown may also be operative in multi protocol engine to receive and transmit requests and responses to and from the storage server. The FC and iSCSI drivers provide respective FC and iSCSI specific access control to the blocks and thus manage exports of LUNs to either iSCSI or FCP or alternatively to both iSCSI and FCP when accessing data blocks on the storage server.

The storage operating system also includes a series of software layers organized to form a storage server that provides data paths for accessing information stored on storage devices. Information may include data received from a client in addition to data accessed by the storage operating system in support of storage server operations such as program application data or other system data. Preferably client data may be organized as one or more logical storage objects e.g. volumes that comprise a collection of storage devices cooperating to define an overall logical arrangement. In one aspect the logical arrangement may involve logical volume block number VBN spaces wherein each volume is associated with a unique VBN.

The file system includes a virtualization system of the storage operating system through the interaction with one or more virtualization modules illustrated as a SCSI target module . The SCSI target module is generally disposed between drivers and the file system to provide a translation layer between the data block LUN space and the file system space where LUNs are represented as data blocks. In one aspect the file system provides a WAFL file system having an on disk format representation that is block based using e.g. 4 KB blocks and using a data structure such as index nodes inodes to identify files and file attributes such as creation time access permissions size and block location . File system uses files to store metadata describing the layout of its file system including an inode file which directly or indirectly references points to the underlying data blocks of a file.

For one feature the file system includes an extent based architecture as an extension to WAFL. Operationally a request from a client is forwarded as a packet over the network and onto the storage server where it is received at a network adapter. A network driver such as layer or layer processes the packet and if appropriate passes it on to a network protocol and file access layer for additional processing prior to forwarding to file system . There file system generates operations to load retrieve the requested data from the disks if it is not resident in core i.e. in memory . If the information is not in memory file system in cooperation with the extent based architecture accesses an indirect volume to retrieve an extent identifier accesses an extent to physical block map to retrieve a PVBN as described in greater detail with reference to . For one aspect the file system passes the PVBN to the RAID system . There the PVBN is mapped to a disk identifier and device block number disk DBN and sent to an appropriate driver of disk driver system . The disk driver accesses the DBN from the specified disk and loads the requested data block s in memory for processing by the storage server. Upon completion of the request the node and operating system returns a reply to the client over the network.

It should be noted that the software path through the storage operating system layers described above needed to perform data storage access for the client request received at the storage server adaptable to the teachings of the disclosure may alternatively be provided in hardware. That is according to alternative aspects a storage access request data path may be provided as logic circuitry embodied within a field programmable gate array FPGA or an application specific integrated circuit ASIC . This type of hardware can increase the performance of the storage service provided by the storage server in response to a request issued by a client. Moreover in another alternate aspect the processing elements of adapters may be configured to offload some or all of the packet processing and storage access operations respectively from processor to thereby increase the performance of the storage service provided by the storage server. It is expressly contemplated that the various processes architectures and procedures described herein can be provided in hardware firmware or software.

When included in a cluster data access components of the storage operating system may be embodied as D module for accessing data stored on disk. In contrast multi protocol engine may be embodied as N module to perform protocol termination with respect to a client issuing incoming access over the network as well as to redirect the access requests to any other N module in the cluster. A cluster services system may further include an M host e.g. M host to provide cluster services for generating information sharing operations to present a distributed file system image for the cluster. For instance media access layer may send and receive information packets between the various cluster services systems of the nodes to synchronize the replicated databases in each of the nodes.

In addition a cluster fabric CF interface module CF interface modules A B may facilitate intra cluster communication between N module and D module using a CF protocol . For instance D module may expose a CF application programming interface API to which N module or another D module not shown issues calls. To that end CF interface module can be organized as a CF encoder decoder using local procedure calls LPCs and remote procedure calls RPCs to communicate a file system command to between D modules residing on the same node and remote nodes respectively.

Although features are shown herein to provide an extent based architecture within the illustrated components and layers of a storage server it will be appreciated that an extent based architecture may be provided in other modules or components of the storage server according to other aspects. In addition an extent based architecture may include a combination of a software executing processor hardware or firmware within the storage server. As such an extent based architecture may directly or indirectly interface with modules of the storage operating system in accordance with teachings of the disclosure.

As used herein the term storage operating system generally refers to the computer executable code operable on a computer to perform a storage function that manages data access and may provide data access semantics of a general purpose operating system. The storage operating system can also be a microkernel an application program operating over a general purpose operating system such as UNIX or Windows or as a general purpose operating system with configurable functionality which is configured for storage applications as described herein.

In addition it will be understood to those skilled in the art that the features described herein may apply to any type of special purpose e.g. file server or storage serving appliance or general purpose computer including a standalone computer or portion thereof embodied as or including a storage system. Moreover the teachings of this disclosure can be adapted to a variety of storage system architectures including but not limited to a network attached storage environment a storage area network and disk assembly directly attached to a client or host computer. The term storage system should therefore be taken broadly to include such arrangements in addition to any subsystems configured to perform a storage function and associated with other equipment or systems. It should be noted that while this description is written in terms of a write anywhere file system the teachings of the disclosure may be utilized with any suitable file system including conventional write in place file systems.

The extent based entries of the extent to physical block map provide per aggregate indirection. In contrast virtual volume block numbers VVBN of volume containers provide per volume indirection. A per aggregate extent based entry as used herein refers to an extent being unique across volume boundaries within an aggregate. A per volume indirect entry refers to an entry being unique within volume boundaries. For per aggregate indirection when the storage server copies moves or makes other changes to physical blocks the changes are reflected within the aggregate layer in the extent to physical block map . These changes however do not need to be propagated into the volume layer because the extent identifier does not need to change. This enables compression decompression sharing and the termination of sharing of extents to occur without communication with the volume layer . Blocks can be easily shared across volume boundaries enabling cross volume deduplication. Segment cleaning and related disk gardening techniques can be performed on the extent to physical block map in a single pass all without having to propagate changes up into the volume layer .

For one aspect the lengths of extents vary. For one aspect the length of an extent is expressed as the number of data blocks of a fixed size within the extent. For example an extent containing only a single 4 KB block would have a length of 1 an extent containing two 4 KB blocks would have a length of 2 etc. For one aspect extents have a maximum length driven by user I O or write allocation e.g. extents having a maximum length of 64 blocks . For an alternate aspect the length of extents may be consistently defined e.g. 8 blocks .

For an example utilizing an extent based tree with variable length extents the height of the tree is variable even between two files of the same size. For one aspect the span of an internal node is also variable. As used herein the span of an indirect block refers to the number of blocks to which that indirect block refers. As a comparison in examples of WAFL the span of an indirect block is fixed the span of a tradvol indirect block is 1024 blocks the span of a flexvol indirect block is 510 blocks e.g. as stored in flexible volume and the span of a 64 bit flexvol indirect block is 255 blocks e.g. as stored in flexible volume .

Additionally in examples of WAFL a contiguous extent containing N blocks would use the same amount of indirect space as N randomly located blocks because each data block of the extent would be represented by a separate indirect entry in the volume layer. An extent based sorted data structure however greatly reduces the amount of indirect space used because volume layer entries are per extent rather than per block. For example consider a 64 bit flexvol storing a file containing 532 685 800 bytes of data approximately 508 MB as provided in examples of WAFL. The flexvol includes indirect blocks having 255 entries a span of 255 and each entry refers to a 4 KB block. The flexvol represents the 508 MB file using two level 2 indirect blocks pointing to 510 level 1 indirect blocks pointing to 130050 4 KB level 0 data blocks. In an extent based sorted data structure instead of using one entry for each 4 KB block the storage server uses one entry for each extent. Extents can be longer than a single 4 KB block. For example an extent is a contiguous group of one or more 4 KB blocks. Using an extent based sorted data structure with 16 block long extents and 127 entries per block the storage server represents the 130050 4 KB with only 8129 leaf nodes and 65 internal nodes resulting in an 87 savings in indirect block metadata.

For one aspect the storage server uses an extent based sorted data structure to provide an indirect volume . For one aspect the storage server provides each indirect volume as a B tree. shows an example volume layer indirect entry for a leaf node of an extent based data structure used to provide an indirect volume . The volume layer indirect entry stores an FBN a corresponding extent identifier and a length of the extent . The storage server uses the FBN as the primary sorting key to navigate the extent based sorted data structure and find the extent identifier that corresponds to the FBN . For one aspect the FBN is 48 bits the extent identifier is 48 bits and the length is 8 bits. Alternatively the storage server uses different sizes for one or more of the FBN extent identifier or length . For example the extent identifier may be 64 bits long in an alternate aspect e.g. to provide for 512 byte granularity in the offset of blocks . For one aspect extent lengths vary. For an alternate aspect extent lengths are fixed.

For one aspect the FBN is 51 bits to provide for 512 byte granularity in the offsets of blocks where a 48 bit FBN provides for 4 KB byte granularity of FBN offsets . Because the storage server stores indirect blocks using an extent based sorted data structure FBN s do not need to be aligned based upon block size e.g. 512 byte offset alignment and 4 KB blocks . The extent based sorted data structure stores an entry for an entire extent based upon an FBN and length of the extent. The extent based sorted data structure does not store only the block at that FBN and then require subsequent entries to correspond to each subsequent FBN. For example given two adjoining extents that are each 16 blocks in length the entries in the extent based sorted data structure for these two extents will have FBN s that are offset by at least 16 blocks. In traversing the extent based sorted data structure the storage server does not need to assume that each entry is separated by the same offset or that an entry s FBN is offset by a whole number multiple of the block size. Additionally the savings in indirect metadata resulting from using an extent based sorted data structure compensates for the use of three additional bits for each FBN . Providing the 512 byte offset granularity within the volume layer eliminates the previously described complications resulting from misalignment between blocks in FBN space and blocks in aggregate space. Once an FBN is mapped to an extent identifier the extent identifier can be mapped to an extent as described below without concern of misalignment because the aggregate layer maintains a consistent block sized alignment of offsets within the aggregate.

The storage server allocates extent identifiers during write allocation. For one aspect the storage server allocates extent identifiers from a finite pool. Alternatively extent identifiers are monotonically increasing values that never wrap.

For one aspect the length of an extent is used for a consistency check as described with reference to below.

The per volume container files of examples of WAFL are not used in an extent based sorted data structure used to provide an indirect volume . Instead of per volume container files the storage server uses an extent to physical block map. As described above the use of the extent to physical block map can result in reduced indirect metadata. The indirect volume blocks however no longer contain cached pointers to PVBN s. Accesses to an extent involves the storage server looking up an extent identifier in the indirect volume and looking up the PVBN e.g. by way of a pointer in the extent to physical block map . The computational overhead of this additional I O look up is offset by some of the features of extent based architecture . For example I O accesses are per extent rather than per block and therefore multiple blocks are accessed by a single I O access of each the indirect volume and the extent to physical block map . Additionally the extent based architecture gains advantages in compression deduplication segment cleaning etc. which can be performed with altering the extent identifier . Actions such as deduplication can easily span the aggregate rather than just a single volume and many changes to blocks e.g. resulting from compression and segment cleaning do not need to be propagated up to the indirect volume e.g. to correct cached indirect pointers as in examples of WAFL .

For one aspect the storage server uses an extent based sorted data structure to provide an extent to physical block map . For one aspect the storage server provides an extent to physical block map as a B tree. shows an example extent map entry for a leaf node of an extent based sorted data structure used to provide an extent to physical block map . Leaf nodes of an extent based sorted data structure used to provide an extent to physical block map store extent identifiers references such as a pointers to PVBN s or other extent identifiers offsets for the extents lengths for the extents and compressed bits indicating whether the extent is compressed. As used herein an offset for an extent is a distance in blocks from the first block of the contiguous group of blocks that make up an extent. For one aspect the extent identifier is 48 bits the pointer extent identifier is 48 bits the offset is 8 bits the length is 8 bits and the compressed bit is 1 bit. For an alternate aspect different numbers of bits are used for each portion of an extent map entry .

For one aspect each extent map entry includes either a pointer or other reference directly to a PVBN or to another extent identifier that directly references a PVBN. For one aspect each PVBN is owned by only one extent and any other extent that references the PVBN does so by way of referencing the owner extent. As a result the maximum additional look up for a given extent to get to a PVBN should be no more than one. This maximum prevents the level of indirect references in extent map entries from becoming arbitrarily deep and taking an arbitrary amount of time as measured in terms of disk I O operations assuming that each extent entry is likely to be stored within a different disk block . As a result of extents having a single owner the storage server can use the owner extent identifier as a tag unique number or other context for the purpose of lost write detection.

For an alternate aspect all extent identifiers map directly to a PVBN and PVBN s can be owned by more than one extent. For a feature including lost write detection the storage server creates a context tag or unique number e.g. via a separate table that is separate different from the extent identifiers due to the possibility of multiple extent identifiers referencing a single PVBN.

For one aspect the storage server checks data consistency by comparing the length of an extent as stored in the volume layer with the length of the extent as stored in the aggregate layer .

For one aspect the storage server utilizes a finite number of extent identifiers. If an extent identifier is a candidate to be reused e.g. upon a request to delete the extent the storage server first determines whether or not other extents refer to that extent identifier. If one or more extents reference the candidate extent identifier the storage server ensures that the one or more extents continue to point to the same data e.g. by altering one of the extents to directly reference the corresponding PVBN and the other extents to reference that altered extent . For one aspect the storage server maintains e.g. in one or more metafiles reference counts for references by extents to each extent and by extents to each PVBN. Reference counts enable the storage server to be aware of whether or not other extents would be affected by operations performed on an extent PVBN e.g. reallocation segment cleaning etc. . The storage server tracks increments and decrements of the reference count in one or more log files. For example the storage server would increment a reference count when a new extent PVBN is allocated when an extent identifier is shared e.g. via clone creation snapshot creation or deduplication etc. . For one aspect the storage server accumulates increments and decrements using a log file and makes batch updates to reference count metafiles e.g. at a consistency point. For one aspect the storage server increments a reference count from 0 to 1 for a PVBN directly bypassing the log file when allocating a new extent PVBN and executes all other increments and decrements of the reference counts via the respective reference count log file.

At block the storage server groups a predetermined number of FBNs into a compression group. In one aspect the predetermined number of FBNs is 8 FBNs. At processing instruction block the storage server compresses the physical blocks corresponding to the FBNs in the compression group. The compression is performed by one or more methods commonly known in the art. For example methods such as Huffman encoding Lempel Ziv methods Lempel Ziv Welch methods algorithms based on the Burrows Wheeler transform arithmetic coding etc. At processing instruction block the storage server allocates an extent identifier for the compression group. At processing instruction block the storage server creates a new extent map entry including the newly allocated extent identifier a reference to the stored data blocks in the compression group an offset from the reference where the compression group begins the length in blocks of the compression group and the compressed bit set to on.

At processing instruction block the storage server uses the allocated extent identifier as a key to traverse the aggregate layer extent based sorted data structure and adds new extent map entry for the compression group. At processing instruction block the storage server overwrites the existing extent identifier with the allocated extent identifier in the existing entry in the volume layer extent based sorted data structure associated with the compression group.

At processing instruction block the storage server receives a write request including an FBN and one or more data blocks in a compression group to be overwritten from a client . For one aspect the client provides the data to be written. Alternatively the client provides a reference to the data to be written. At processing instruction block the storage server partially overwrites the compression group without uncompressing the compression group. At processing instruction block the storage server determines if the partially overwritten compression group should be recompressed. If the storage server determines that the partially overwritten compression group should not be recompressed the recompression of the compression group is deferred at processing instruction block . If the storage server determines that the partially overwritten compression group should be recompressed the compression group is recompressed at processing instruction block . The compression is performed by one or more methods commonly known in the art. In one aspect the storage server returns an indication to the client that the overwrite request for the compression group was successfully processed.

At processing instruction block the method allocates an extent identifier for a partial extent to be written. At processing instruction block the method determines if the offset between the FBN provided with the write request and the FBN for the overlapping extent is zero. At processing instruction block if there is a non zero offset the method creates a new extent map entry including the newly allocated extent identifier a reference to the existing extent identifier equal to zero a length of the existing data blocks that are not being overwritten e.g. the value of the offset between the FBN provided with the write request and the FBN for the existing extent and the compressed bit to off. The compressed bit is set to off because although the compression group is still compressed the newly store data blocks corresponding to the partially overwritten compression group are not compressed. The newly stored data block may be later compressed as described below with reference to .

At processing instruction block if the offset of the FBN provided with the write request from the FBN for the overlapping extent is zero or after creating a new extent map entry for an initial set of blocks not being overwritten the method creates a new extent map entry including the newly allocated extent identifier a reference to the stored data blocks provided with the overwrite request an offset from the reference where the newly stored data blocks begin the length in blocks of the new data and the compressed bit to off.

At processing instruction block the method determines if the overwrite process has reached the end of the existing extent. For one aspect the method determines if the sum of the offset from the start of the existing extent for the new data blocks and the length of the new data blocks is greater or equal to length of the existing extent to determine if the end of the existing extent has been reached after completing the overwrite portion of the method .

At processing instruction block if the overwrite has not reached the end of the existing extent the method creates a new extent map entry including the newly allocated extent identifier a reference to the existing extent identifier an offset equal to the first block of the remainder of existing blocks that are not being overwritten the offset from the beginning of the existing extent to the first block to be overwritten the length of the new data a length of the remainder of the existing data blocks that are not being overwritten and the compressed bit to off.

At processing instruction block the method uses the allocated extent identifier as a key to traverse the aggregate layer extent based sorted data structure and adds the one or more new extent map entries . At processing block the method overwrites the existing extent identifier with the allocated extent identifier in the existing entry in the volume layer extent based sorted data structure associated with the FBN for the overlapping extent.

Therefore the data blocks of the existing extent corresponding to the original compression group do not need to be read and uncompressed prior to performing the partial overwrite of the compression group because the compression group itself is not overwritten. For one aspect if an extent map entry refers to the existing extent map entry for the compression group that extent remains unchanged by the partial overwrite because the existing extent still refers to the original compression group and includes the original offset length for the compression group and compressed bit set to on. A new extent map entry is created by the partial overwrite of the compression group including a reference to the newly stored blocks the length for the newly stored blocks and compressed bit set to off because those newly stored blocks are not compressed.

At block the method determines if the storage server has a system usage guarantee. In one aspect a system usage guarantee is a policy that includes a minimum system usage to guarantee that the usage of the central processing unit CPU in the storage server will always be above a predetermined percentage e.g. above 20 . This minimum system usage guarantee guarantees that the CPU is always being utilized rather than sitting idle. If the minimum system usage guarantee is being violated at processing instruction block the storage server is not being utilized efficiently and therefore the storage server has bandwidth to perform tasks such as compression. Therefore the physical blocks in the compression group are compressed at processing instruction block using one or more methods commonly known in the art. In some aspects the system usage guarantee guarantees that the usage of the CPU in the storage server will always be below a predetermined percentage e.g. below 70 . This guarantees that the CPU is not being over utilized. If the maximum system usage guarantee is violated the physical blocks in the compression group will not be compressed because the CPU usage is already above the predetermined threshold.

If there is no system usage guarantee for the storage sever at processing instruction block or the minimum system usage guarantee is not violated at processing instruction block the method determines if the storage server has a space usage guarantee at processing instruction block . In one aspect a space usage guarantee is a policy that includes a maximum space usage to guarantee that the usage of the storage devices e.g. disks accessible to the storage server is not above a certain percentage e.g. disks are not more than 70 full . If this maximum space usage guarantee is being violated at processing instruction block the space usage of the storage server is too high and the data in the storage devices must be compressed. Therefore the physical blocks in the compression group are compressed at processing instruction block using one or more compression methods commonly known in the art. In some aspects the space usage guarantee includes a minimum space usage. If the space usage guarantee includes a minimum space usage the physical blocks in the compression group will not be compressed if the minimum space usage is met. This will avoid the storage server being utilized for compression when the space usage is minimal in the storage server .

If there is no space usage guarantee for the storage sever at processing instruction block or the space usage guarantee is not violated at processing instruction block the method determines if the storage server is to perform a snapshot of data within a predetermined amount of time at processing instruction block . Prior to a snapshot being performed it is beneficial to compress the data that will be used in the snapshot such that the snapshot takes less space once it is taken. If the snapshot is to be performed within the predetermined amount of time e.g. 1 hour the method compresses the physical blocks in the compression group at processing instruction block using one or more compression methods commonly known in the art. A storage server generally performs different types of snapshots such as an hourly snapshot a daily snapshot a weekly snapshot and a monthly snapshot. In some aspects the method may further determine the type of snapshot that is to be taken within a predetermined amount of time and may only compress the physical blocks if the snapshot is a certain type of snapshot e.g. daily snapshot a weekly snapshot and a monthly snapshot .

If there is no snapshot within a predetermined amount of time at processing instruction block the method determines if the data in the compression group is cold data at processing instruction block . Cold data is data that is not overwritten often such as log files. In contrast hot data is data that is overwritten often such as a file that is currently open and being modified by a user. The data in the compression group is found to be cold data by determining the last created time the last modified time and or the last accessed time of the data. The last created time is the last time the data was created. The last modified time is the last time the data was modified. The last accessed time is the last time the data was accessed. If the determined time is more than a predetermined time period in the past e.g. 1 day then the data is determined to be cold data. The determined time is compared to the current time of the system to determine if the determined time is more than the predetermined time period in the past. In some aspects the last created time the last modified time and or the last accessed time are extracted from metadata of the data blocks in the compression group. The current time of the system is determined by accessing a clock associated with the system. If the data is cold data the method compresses the physical blocks in the compression group at processing instruction block using one or more compression methods commonly known in the art.

If the data is not cold data at processing instruction block the method determines if it is a predetermined time of day at processing instruction block . The time of day is determined by checking a system clock in the storage server . The compression may be performed at the same time every day e.g. midnight . If it is the predetermined time of day the method compresses the physical blocks in the compression group at processing instruction block using one or more compression methods commonly known in the art.

If it is not a predetermined time of day the method determines if user defined criteria exists for compression in the storage server at processing instruction block . The user defined criteria allow a user to define when compression should be triggered in the storage server . For example the user can specify that certain files should be selected more often for compression e.g. word processing files . In some aspects a user may enter criteria for compression using a graphical user interface GUI .

If there are no user defined criteria the method ends at processing instruction block . If there are user defined criteria the method determines if the user defined criteria has been met at block processing instruction . If the user defined criteria are met at processing instruction block the method compresses the physical blocks in the compression group at processing instruction block using one or more compression methods commonly known in the art. If the user defined criteria are not met at processing instruction block the method ends at processing instruction block .

Thus an extent based architecture can be included in a computer system as described herein. The methods and may constitute one or more programs made up of computer executable instructions. The computer executable instructions may be written in a computer programming language e.g. software or may be embodied in firmware logic or in hardware circuitry. The computer executable instructions to provide a persistent cache may be stored on a machine readable storage medium. A computer readable storage medium or a non transitory computer readable storage medium as the terms are used herein include any mechanism that provides i.e. stores and or transmits information in a form accessible by a machine e.g. a computer network device personal digital assistant PDA manufacturing tool any device with a set of one or more processors etc. . A non transitory computer readable storage medium as the term is used herein does not include a signal carrier wave etc. The term RAM as used herein is intended to encompass all volatile storage media such as dynamic random access memory DRAM and static RAM SRAM . Computer executable instructions can be stored on non volatile storage devices such as magnetic hard disk an optical disk and are typically written by a direct memory access process into RAM memory during execution of software by a processor. One of skill in the art will immediately recognize that the terms machine readable storage medium and computer readable storage medium include any type of volatile or non volatile storage device that is accessible by a processor. For example a machine readable storage medium includes recordable non recordable media e.g. read only memory ROM random access memory RAM magnetic disk storage media optical storage media flash memory devices etc. .

Although the disclosure has been described with reference to specific examples it will be recognized that other examples are possible and can be practiced with modification and alteration within the spirit and scope of the appended claims. Accordingly the specification and drawings are to be regarded in an illustrative sense rather than a restrictive sense.

