---

title: Large receive offload for virtual machines
abstract: A network interface controller (NIC) that includes a set of receive NIC queues capable of performing large receive offload (LRO) operations by aggregating incoming receive packets is provided. Each NIC queue turns on or off its LRO operation based a set of LRO enabling rules or parameters, whereby only packets that meet the set of rules or parameters will be aggregated in the NIC queue. Each NIC queue is controlled by its own set of LRO enabling rules such that the LRO operations of the different NIC queues can be individually controlled.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09384033&OS=09384033&RS=09384033
owner: VMware, Inc.
number: 09384033
owner_city: Palo Alto
owner_country: US
publication_date: 20140311
---
Large Receive Offload LRO has become a feature on almost all network adapters or network interface controllers NICs . This feature is typically turned on for end nodes terminating TCP traffic in order to get a boost in throughput to the application terminating the connection on that node. However blindly forwarding a large LRO aggregated packet would require downstream fragmentation of packets leading to performance degradation. Furthermore traffic being forwarded out of a host machine must comply with Maximum Segment Size MSS but MSS is a parameter that is visible only on the TCP layer and not available to a forwarding VM. Performing LRO aggregation on forwarded traffic would therefore likely to create oversized packets that exceed the MSS requirement and results in fragmentation.

In most NICs LRO is a Boolean feature that is simply turned on or off. However a host machine in a network virtualization environment can host one or more virtual machines VMs some of which may be forwarding traffic rather than terminating traffic. In some host machines a VM may terminate some types of traffic while forwarding other types of traffic. In order to avoid fragmentation of packets on forwarded traffic many host machines in network virtualization environment simply elect to turn off the LRO feature in the NIC.

What is needed is a host machine that is able to fully utilize the LRO capability of its NIC for maximizing throughput and performance. Such a host machine should be able to enable LRO aggregation on traffic being terminated by a VM while disabling LRO aggregation on traffic being forwarded by a VM. Such a host machine should also be able to maximize throughput even on forwarded traffic by LRO aggregation without causing unnecessary fragmentation downstream by violating the MSS requirement.

Some embodiments of the invention provide a network interface controller NIC that includes a set of receive NIC queues capable of performing large receive offload LRO operations by aggregating incoming receive packets. In some embodiments each NIC queue turns on or off its LRO operation based a set of LRO enabling rules or parameters whereby only packets that meet the set of rules or parameters will be aggregated in the NIC queue. In some embodiments each NIC queue is controlled by its own LRO enabling rule such that the LRO operations of the different NIC queues can be individually controlled.

In some embodiments the NIC described above is a physical NIC PNIC . The PNIC has several receive NIC queues each NIC queue controlled by its own set of LRO enabling rules such that the LRO operations of the different NIC queues can be individually controlled. In some embodiments at least some of the operations of the PNIC are controlled by a PNIC driver which in turn provides an application programming interface API to the virtualization software for controlling the LRO operations and other PNIC operations. The API allows the virtualization software and other software components of the host machine to set the LRO enabling rules of the individual NIC queues in the PNIC.

In some embodiments a LRO rule for a NIC queue is a destination address filter that enables LRO operation for a specific destination address MAC address IP address or other types of destination address . In some embodiments the LRO rule for a NIC queue specifies a particular flow or microflow for which the LRO operation is to be enabled. In some of these embodiments the flow is specified by a set of parameters that specifies a network session or a transport connection e.g. the five tuple parameters of a TCP IP connection .

Some embodiments perform LRO aggregation on packets being forwarded by a VM. Some of these embodiments segment the LRO aggregated packet according to the Maximum Segment Size MSS of the TCP protocol before forwarding the segmented packets to their destination. Some embodiments snoop the packets being forwarded for its MSS parameter before using the snooped MSS parameter to perform Transmit Segmentation Offload TSO operation. In some embodiments the segmentation operation that uses the extracted MSS parameter is performed by a PNIC of the host machine. In some of these embodiments the PNIC performs both the aggregation operation LRO and the segmentation TSO within its own hardware without consuming CPU cycles at the host machine. In some embodiments the PNIC receives the MSS parameter from the network stack as a metadata that accompanies a LRO aggregated packet.

The preceding Summary is intended to serve as a brief introduction to some embodiments of the invention. It is not meant to be an introduction or overview of all inventive subject matter disclosed in this document. The Detailed Description that follows and the Drawings that are referred to in the Detailed Description will further describe the embodiments described in the Summary as well as other embodiments. Accordingly to understand all the embodiments described by this document a full review of the Summary Detailed Description and the Drawings is needed. Moreover the claimed subject matters are not to be limited by the illustrative details in the Summary Detailed Description and the Drawings but rather are to be defined by the appended claims because the claimed subject matters can be embodied in other specific forms without departing from the spirit of the subject matters.

In the following description numerous details are set forth for the purpose of explanation. However one of ordinary skill in the art will realize that the invention may be practiced without the use of these specific details. In other instances well known structures and devices are shown in block diagram form in order not to obscure the description of the invention with unnecessary detail.

Some embodiments of the invention provide a network interface controller NIC that includes a set of receive NIC queues capable of performing large receive offload LRO operations by aggregating incoming receive packets. In some embodiments each NIC queue turns on or off its LRO operation based a set of LRO enabling rules or parameters whereby only packets that meet the set of rules or parameters will be aggregated in the NIC queue. In some embodiments each NIC queue is controlled by its own LRO enabling rule such that the LRO operations of the different NIC queues can be individually controlled.

In some embodiments LRO is turned on or off in the hardware or software based upon flow information programmed on the component performing the LRO aggregation. The flow or micro flow could be L2 or L3 L4 flow with wildcards. Hence for example some embodiments enable LRO for all traffic destined to a particular VM s MAC address. This way the VM and virtualization software can gain in efficiency and throughput as this reduces the number of packets hitting through the VM.

The host machines in some embodiments is a computing device managed by an operating system e.g. Linux that is capable of creating and hosting VMs. The host machine provides the computing resources such as CPU cores and memories needed for performing the computing tasks of the VMs. The host machine also provides the network communication resources needed for allowing each VM to participate in the network traffic of the physical network .

The VMs are virtual machines operating in the host machine . The VMs executes on top of a hypervisor not shown which in some embodiments includes the network virtualization layer. Network virtualization will be further described by reference to below. In some embodiments the VMs are each assigned a set of network addresses e.g. a MAC address for L2 an IP address for L3 etc. and can send and receive network data to and from other network elements such as other VMs. In some embodiments at least some of the VMs act as traffic terminals of network traffic that generates or consume network traffic. In some embodiments at least some of the VMs act as forwarding elements of network traffic that forward received data packets on to other elements in the network. In the example host machine illustrated in the VMs and are operating as forwarding elements while the VMs and are operating as network traffic terminals.

In some embodiments a forwarding VM both forwards and consumes i.e. uses the network traffic that it receives. In some embodiments a forwarding VM replicates the received network traffic to multiple recipients. In some embodiments a forwarding VM does not use or consume the network traffic that it receives and just forwards. In some of these embodiments the host machine is operating a multi layered network stack for each VM and a forwarding VM performs packet forwarding at lower levels of the network stack e.g. below TCP level . In some embodiments a forwarding VM is an instance of a logical routing element LRE that performs L3 routing between different IP subnets. Description of LREs can be found in U.S. patent application Ser. No. 14 137 862 filed Dec. 20 2013 now published as U.S. Patent Publication 2015 0106804.

The host machine performs various functions and operations between the NIC and the VMs . These functions and operations are collectively referred to as the network TX and RX processing in . Different embodiments implement the TX and RX process differently. Some embodiments implement the network TX and RX processing as one module within the host machine while some other embodiments implement the network TX and RX processing in several modules.

In some embodiments the TX and RX processing includes network stacks for the VMs . In some embodiments at least some of the network stacks are implemented according to a multi layer networking model such as TCP IP. In some embodiments each of the network stacks includes an I O chain not illustrated that performs the layered network operations as well as other operations. Network stacks will be further described by reference to below.

In some embodiments the TX and RX processing includes L2 switching operations and or L3 routing operations. In some of these embodiments these switching and or routing operations are performed by an instance or instances of software forwarding elements such as logical forwarding elements LFEs logical switching elements LSEs or aforementioned LREs. Software forwarding elements will be further described by reference to .

The NIC module is the interface to the physical network for the host machine . As illustrated the NIC module includes a number of queues for queuing incoming network traffic packets from the physical network . The NIC module also includes a queue assignment sub module for assigning incoming data packets to the queues . In some embodiments each of the NIC queues is assigned to a computing resource e.g. processor core or a processing thread in the host machine and the queue assignment sub module assigns incoming packet to the queues according to the computing resource assignment. In some embodiments each of the NIC queues is associated with a VM. In some of these embodiments each VM is addressable by a MAC address or IP address and the queue assignment sub module filters incoming network packets into the queues based on the MAC address or the IP address of the VMs.

As illustrated the NIC queues are associated with the VMs respectively. In some embodiments a VM is associated with a NIC queue because the NIC queue receives only data packet filtered for that VM. In some embodiments a VM is associated with a NIC queue because the NIC queue is assigned to a processor core or a CPU that is running the network stack or the processing threads of the VM. In some embodiments a NIC queue is not associated with any particular VM but received data packets will be distributed to their correct destination VMs according to their destination IP address or MAC address.

The NIC queues are for buffering incoming data packets received from the physical network . Furthermore each of the NIC queues is capable of performing LRO operations i.e. aggregating smaller incoming network packets into larger data packets for efficient processing and delivery to the VMs. The LRO operations of each NIC is individually controlled by a LRO rule or a set of LRO rules for that NIC queue and the NIC queue enables LRO operations on packets that comply with the LRO rule for that queue i.e. aggregates a smaller data packet in the NIC queue into a aggregated LRO packet when the smaller data packet meets the criteria set forth by the LRO rule . In some embodiments such rules are supplied by a network controller that controls the networking operations of the host machines include that are connected to the physical network .

In some embodiments a LRO rule for a NIC queue is a destination address filter that enables LRO operation for a specific destination address MAC address IP address or other types of destination address . In some embodiments the LRO rule for a NIC queue specifies a particular flow or microflow for which the LRO operation is to be enabled. In some of these embodiments the flow is specified by a set of parameters that specifies a network session or a transport connection e.g. the five tuple parameters of a TCP IP connection . In other words the LRO rule specifies that packet aggregation is to be turned on for data packets of a particular network session or connection but not for others. Flow based LRO control will be further described in Section II below.

As mentioned in some embodiments the LRO operations of the different NIC queues are individually enabled or disabled. In other words LRO operations can be enabled for some NIC queues while disabled for others. illustrates the individual enable disable of the LRO operations in queues of the NIC . In the NIC the LRO operations of the NIC queue are controlled by LRO control modules respectively. As illustrated the LRO control receives a LRO rule that enables LRO aggregation in the NIC queue while the LRO control receives a different LRO rule that disables LRO aggregation in the NIC queue . The NIC queues and are likewise respectively disabled and enabled by each s own LRO controls and . As mentioned some embodiments enable LRO aggregation for a particular destination VM hence all packets in the NIC queue of the particular destination VM will be aggregated by LRO operation. For some embodiments that enable LRO for a particular flow e.g. a TCP connection only packets in the NIC queue that belongs to that particular flow will be aggregated.

By allowing the LRO operations of NIC queues to be individually enabled or disabled the host machine in some embodiments allows effective control of LRO operations for different VMs. For example in some embodiments it is desirable to turn on LRO operations for traffic terminating at a VM while turning off LRO operations for traffic that are to be forwarded. This is at least partly because packets that are to be forwarded out of the host machine must comply with a maximum size requirement and hence it is desirable to turn off LRO operations for those VMs e.g. VMs and that are forwarding packets out of the host machine. This avoids creating LRO aggregated packets that may exceed the maximum size limit e.g. Ethernet MTU which require downstream segmentation operations e.g. TSO operations or fragmentation to dissolve the aggregated packets into smaller segments or fragments. On the other hand for VMs that consume the incoming network packets and do not forward those packets e.g. the VMs and it is advantageous to perform LRO aggregation for reducing overhead because LRO aggregated packets that exceed the maximum size requirement of the physical network would not need to be segmented or fragmented later. Some embodiments allow further effective control of the LRO operations by enabling and disabling LRO operations in each NIC queue on a connection by connection or session by session basis.

The process then identifies at a set of LRO rules for the host machine. In some embodiment such LRO rules are identified in order to optimize the performance of the host machine. For example some embodiments set the LRO rules so that packets that will be consumed by a VM within the host machine will be aggregated while packets that will be forwarded by a VM would not be aggregated. This is done to minimize overhead in processing small packets and to avoid having to segment or fragment oversized packet. Some embodiments identify transport connections that are forwarded or consumed by VMs in the host machine. The process then enables LRO aggregation for transport connections that terminate at VMs in this host machine while disable LRO aggregation for transport connections that are to be forwarded by VMs in this host machine.

Next the process identifies at NIC queues that are needed for implementing those LRO rules. In some embodiments an LRO rule can be for packets destined for a particular VM so to perform LRO on those packets requires identifying the NIC queues that buffers the packets for that particular VM. In some embodiments an LRO rule can be for a particular type of packet e.g. of a particular TCP connection identifiable by a five tuple and the process would identify a NIC queue that is assigned to hold that particular type of packets.

Once a NIC queue is identified for a particular LRO rule the process applies at the particular LRO rule to the identified NIC queue. In the example of this operation includes sending the LRO rule to the LRO control module of the identified NIC queue. In some embodiments in which the NIC is a physical NIC PNIC the operation includes setting the LRO rules at the queues of the PNIC through the API of the driver of the PNIC.

The process then determines at if there are other LRO rules to implement in the NIC queues. If so the process returns to to identify a NIC queue for another LRO rule. If not the process ends.

The solution described above allows effective turning on of LRO on micro flow level so that traffic destined to a particular VM can have LRO enabled in hardware or software. This allows the virtualization software to efficiently use the resources available to it.

Several more detailed embodiments of the invention are described below. Section I describes individually enabled NIC queues in a physical NIC. Section II describes controlling LRO operations using various LRO aggregation rules. Section III describes snooping maximum segment size parameter from forwarded packets. Finally section IV describes an electronic system with which some embodiments of the invention are implemented.

In some embodiments the NIC described above is a physical NIC PNIC in a host machine of a virtualized network environment. The PNIC is a network adaptor that has dedicated network interfacing hardware for processing incoming and outgoing network traffic without consuming processor i.e. CPU cycles of the host machine. The host machine operates virtualization software which allows multiple VMs to simultaneously operate in the host machine and to have network access through the PNIC to the physical network. The PNIC has several receive NIC queues each NIC queue controlled by its own set of LRO enabling rules such that the LRO operations of the different NIC queues can be individually controlled. In some embodiments at least some of the operations of the PNIC are controlled by a PNIC driver which in turn provides an application programming interface API to the virtualization software for controlling the LRO operations and other PNIC operations. The API allows the virtualization software and other software components of the host machine to set the LRO enabling rules of the individual NIC queues in the PNIC.

The PNIC is a physical hardware component dedicated to performing the function of a NIC. In some embodiments the host machine offloads network interfacing tasks from its CPUs processors to the PNIC . As illustrated the PNIC includes receive queues for buffering incoming packets from the physical network each of these queues capable of LRO packet aggregation operations as described above. The LRO operations of each of these queues are controlled by its own LRO control module respectively . In some embodiments the queuing of incoming network traffic as well as the aggregating of data packets LRO are handled by the PNIC without consuming CPU processor cycles in the host machine . In some of these embodiments the LRO rules to the different NIC queues are provided by the virtualization software through the API while the PNIC uses the API to inform the virtualization software that a particular NIC queue has completed aggregating a packet under the particular NIC queue s LRO aggregation rule. In some embodiments the virtualization software in turn fetches the received data packets from the PNIC either aggregated if LRO is turned on or not aggregated if LRO is turned off . As illustrated the API also allows LRO operations to be controlled by an external network controller which in some embodiments pushes down configuration or control data that includes rules for LRO aggregation to the host machine .

The PNIC also includes a queue assignment sub module and a RSS receive side scaling sub module . The queue assignment sub module determines to which NIC queue does an incoming data packet from the physical network goes into while the RSS sub module assigns each NIC queue to a computing resource e.g. processor core or processing thread in the host machine. In some embodiments the RSS assigns network traffic to processing threads and the threads can be assigned to different CPUs for load balancing purposes. In some of these embodiments each VM is associated with a processing thread and the RSS ensures that a thread of a VM stays on a CPU for the duration of a network connection session. In some embodiments the queue assignment sub module assigns packets to the different queues by simply hashing the incoming packets e.g. by hashing on the destination MAC address or IP address the RSS sub module in turn distributes packets from the NIC queues based on the same hashing function.

In some embodiments each of the NIC queues is directly associated with a VM and the queue assignment sub module filters incoming network packets into the queues based on the MAC address or the IP address of the VMs. In some embodiments a NIC queue in the PNIC is not associated with any particular VM but the received data packets will be distributed to their correct destination VMs according to their destination IP address or destination MAC address by a software forwarding element. In some embodiments the queue assignment sub module filters incoming network packets into the queues based on flows or micro flows that are associated with connection sessions. For example a NIC queue can have a filter that accepts only packets having a particular five tuple identifier belonging to a particular TCP connection.

In some embodiments the queue assignment sub module assigns packets from multiple different VMs to a same NIC queue. This is the case for some host machines operate more VMs than NIC queues such that at least some of the NIC queues necessarily serve multiple VMs. A host machine that operates more VMs than NIC queues will be further described below by reference to . In some embodiments the queue assignment sub module assigns packets for a particular VM to only one particular NIC queue such that all of the packets heading to that particular VM will be subject to a same set of LRO rules being applied to that particular queue. In some embodiments the queue assignment sub module can assign packets for a same VM across different queues. This can occur if the queue assignment sub module uses criteria other than destination address for assigning packets to a queue e.g. by hashing or by connection session identified by five tuple . Assigning packets for a same VM across different NIC queues will be further discussed by reference to below.

As discussed above different embodiments assign incoming packets to queues based on different types of criteria MAC address five tuple simple hashing etc. . In some embodiments different queues in a NIC can be programmed to accept packets based on different types of criteria i.e. a NIC can use a mixture of types of criteria for assigning packets into queues. For example in some embodiments a NIC can have some queues that use MAC filters for accepting incoming packets some queues that use connection five tuple filters while other queues receive packets solely based on a hashing function.

Though not illustrated a PNIC in some embodiments have different types of NIC queues in which some of the NIC queues are assigned to computing resources while others are assigned to corresponding destination VMs by setting MAC filters. In some embodiments some of the queues in the PNIC are dedicated to specialized hardware functions. For example some of the NIC queues have specialized hardware support for performing LRO operations and can be programmed to perform LRO aggregation for any destination VM.

The PNIC also includes a command data interface for handling the communication between the PNIC and the processor core of the host machine . The PNIC driver communicates with the PNIC when routines of the API are invoked at the processor core of the host machine e.g. by the virtualization software . The command data interface translates signals received from the process core into data packets for the network or into control signals for various components of the PNIC . Among these control signals are the LRO aggregation rules for each of the NIC queues where different LRO control modules are controlled by different control signals from the command data interface . In some embodiments the processor is able to implement a LRO aggregation rules at a particular NIC queues by invoking an API routine that addresses the control signals of a particular LRO control module.

In some embodiments the command data interface also support communication with the driver by interrupt or by polling. For example when there is an aggregated packet ready for delivery to one of the VMs from one of the NIC queues the command data interface updates a set of corresponding status bits so the processor would know that there is a packet ready for retrieval when it polls the status bits. The processor then invokes an API routine to retrieve the packet from the PNIC through the command data interface .

The virtualization software manages the VMs . Virtualization software may include one or more software components and or layers possibly including one or more of the software components known in the field of virtual machine technology as virtual machine monitors VMMs hypervisors or virtualization kernels. Because virtualization terminology has evolved over time and has not yet become fully standardized these terms do not always provide clear distinctions between the software layers and components to which they refer. As used herein the term virtualization software is intended to generically refer to a software layer or component logically interposed between a virtual machine and the host platform.

In some embodiments the virtualization software assigns the computing resources of the host machine e.g. CPU cycles to the VMs . In some embodiments the virtualization software also conducts network traffic between the PNIC and the VMs as well as among the VMs themselves. In some of these embodiments the virtualization software includes one or more software forwarding element for forwarding data packets to and from the VMs in the host machine. In addition the host machine also operates a network stack or protocol stack for each of the VMs. For some of these embodiments illustrates the host machine operating a software forwarding element and network stacks between the PNIC and the VMs .

As illustrated in in addition to the PNIC driver and the PNIC the host machine is operating a software forwarding element and network stacks for the VMs . In some embodiments the software forwarding element and the network stacks are part of the virtualization software running on the host machine e.g. the virtualization software . Each network stack connects to its VM and the software forwarding element which is shared by all the network stacks of all the VMs. Each network stack connects to the software forwarding element through a port of the software forwarding element. In some embodiments the software forwarding element maintains a single port for each VM.

The software forwarding element connects to the PNIC through the PNIC driver to send outgoing packets and to receive incoming packets. In some embodiments the software forwarding element is defined to include an uplink through which it connects to the PNIC to send and receive packets. The software forwarding element performs packet processing operations to forward packets that it receives on one of its ports to another one of its ports or through the uplink and the physical network to another host machine. For example in some embodiments the software forwarding element tries to use data in the packet e.g. data in the packet header to match a packet to flow based rules and upon finding a match performs the action specified by the matching rule.

In some embodiments software forwarding elements executing on different host devices e.g. different computers are configured to implement different logical forwarding elements LFEs for different logical networks of different tenants users departments etc. that use the same shared computing and networking resources. For instance two software forwarding elements executing on two host devices can perform L2 switch functionality. Each of these software switches can in part implement two different logical L2 switches with each logical L2 switch connecting the VMs of one entity. In some embodiments the software forwarding elements provide L3 routing functionality and can be configured to implement different logical routers with the software L3 routers executing on other hosts.

In the virtualization field some refer to software forwarding elements as virtual forwarding elements as these are software elements. However in some embodiments the software forwarding elements are referred to as physical forwarding elements PFEs in order to distinguish them from logical forwarding elements LFEs which are logical constructs that are not tied to the physical world. In other words the software forwarding elements are referred to as PFEs because they exist and operate in the physical world whereas logical forwarding elements are simply a logical representation of a forwarding element that is presented to a user. Examples of software forwarding elements such as software switches software routers etc. can be found in U.S. patent application Ser. No. 14 137 862.

Each network stack processes network traffic from the PNIC to its corresponding VM across the different layers of network protocols. In some embodiments this includes handling network protocols at link layer e.g. Ethernet or MAC network layer e.g. IP transport layer e.g. TCP and or application layer e.g. HTTP . In some embodiments one or more of the layered protocols of a network stack is handled by the corresponding VM.

In some embodiments an LRO aggregated packet is a TCP layer packet i.e. having TCP headers and specifying a TCP port as destination . Such an LRO aggregated packet destined for a particular VM is processed at the transport layer of the particular VM s network stack according to the TCP protocol. In some embodiments the network stack communicates with the PNIC through the PNIC s API in order to retrieve LRO aggregated packets from the NIC queue.

In some embodiments each network stack is operated by processing threads running on the host machine s processor core s . In some of these embodiments each thread manages a queue in the PNIC . Whenever a queue in the PNIC has a packet ready e.g. a LRO packet for delivery to the corresponding network stack the PNIC generates an interrupt to the processor core that executes the network stack s processing thread. The PNIC sends this interrupt through the API which in turn passes the interrupt to the processor core. In some embodiments each time a queue s thread is invoked for this operation the core that manages the queue and executes its thread has to interrupt another task that it is performing to execute the thread so that it can retrieve the packets from the queue. Such interruptions affect the processor s operational efficiency. By performing LRO operations and aggregating many small packets into fewer larger packets some embodiments increase the operational efficiency of the processor by reducing the number of interrupts that the processor has to handle.

In some embodiments LRO aggregation is not implemented in the PNIC but is instead implemented within the virtualization software. illustrates a host machine that is running virtualization software that performs LRO aggregation. The virtualization software is operating VMs and receiving data packets from a physical network through a PNIC .

The virtualization software includes network stacks for the VMs respectively. The virtualization software also includes a software forwarding elements for forwarding packets between the PNIC and the VMs. The software forwarding element has several ports each port of the virtualization software is connected to a network stack of a VM . The software forwarding element receives data packets from the PNIC through a set of queues . Like the NIC queues in the PNIC as described above by reference to the queues are for buffering incoming data packets received from the physical network. And like the NIC queues in the PNIC each queue in the set of queues is capable of performing LRO aggregation operation based on its own LRO aggregation rule . In some embodiments the LRO aggregation rules are specified by an external network controller . The virtualization software also includes a queue assignment module for assigning incoming data packets from the PNIC to the queues .

Unlike the software forwarding element in the virtualization software the forwarding element does not receives LRO aggregated packets. Rather each port of the software forwarding element forwards received data packet to a queue that is capable of performing LRO aggregation before reaching a network stack for a VM. As illustrated the queues are situated at the ports of the software forwarding element and are for performing LRO aggregation on packets destined to the VMs respectively. The network stacks receives data packets from the queues and these data packets may be LRO aggregated or not based on the LRO aggregation rule of each of the queues . Since the LRO capable queues receive data packets that are already sorted based on destination address the virtualization software does not include a queue assignment module like for assigning receive data packets to those queues.

In some embodiments some or all of the queues handle data packets and hence aggregation for multiple different VMs. This is particularly the case when the host machine implementing the NIC queues is operating fewer NIC queues than VMs. For some embodiments illustrates a host machine that assigns packets for different VMs into a same queue.

As illustrated in the host machine is operating eight different VMs VM A through H . The host machine also has computing resources for operating the VMs . The host machine also includes a RX processing module four incoming data buffers and a queue assignment module . In some embodiments the incoming data buffers are NIC queues in a PNIC not illustrated and the queue assignment module is part of the PNIC. In some embodiments the incoming data buffers and the queue assignment module are implemented in a virtualization software not illustrated running on the host machine .

The RX processing module encapsulate functions performed by the host machine that retrieve process and forward packets to the VMs . In some embodiments the RX processing module includes a software forwarding element for forwarding packets to the VM as well as network stacks for processing network protocols for the VMs . In some embodiments the RX processing module represents a collection of software modules performed by the virtualization software running on the host machine .

The queue assignment module receives incoming packets from the physical network and assigns the received packets to the incoming data buffers . As mentioned different embodiments assign incoming packets to queues based on different types of criteria MAC address five tuple simple hashing etc. . Furthermore in some embodiments different queues in a NIC can be programmed to accept packets based on a mixture of different types of criteria.

As illustrated at least some of the incoming data buffers have packets from different queues. Specifically the queue is assigned packets for VMs and VMs A and B the queue is assigned packets for VMs and VMs C D and E the queue is assigned packets for VM VM F and the queue is assigned packets for VM and VMs G and H . In this particular example the sharing of at least some of the queues by multiple VMs is necessary because there are more VMs eight than there are queues four . In some embodiments each incoming data buffer or queue is associated with a computing resource e.g. a thread or a CPU core and VMs that operates on a same computing resource would share a same queue regardless of whether there are more VMs than queues. In this example the VMs are operating on a same computing resource so the data packets for the VMs are assigned to a same queue .

As illustrated the LRO operations of the queues are governed by LRO aggregation rules respectively. Thus the rule governs the LRO aggregation operations of the VMs the rule governs the LRO aggregation operations of the VMs the rule governs the LRO aggregation operations of the VM and the rule governs the LRO aggregation operations of the VMs .

In some embodiments the assignment of packets to queues is such that packets for a particular VM may end up in different queues. This can occur if the queue assignment is at least partly based on simple hash or if the queue assignment is at least partly based on flow or micro flow e.g. specified by five tuple identifiers for a TCP connection session filtering that does not correspond directly to a VM in the host machine. Consequently in some of these embodiments LRO rules may be applicable to only some packets of a VM e.g. belonging to a particular connection session but not to other packets of the same VM e.g. not belonging to the particular connection session .

As illustrated the queue assignment module is applying a connection session filter on the queue and another connection session filter on the queue such that packets for the VM ends up in both the queue and the queue . In contrast the queue assignment module is applying a MAC filter to the queue that allows only packets for the VM to enter the queue . The packets for the VM are not distributed across different queues.

Because packets for a same VM can be in different queues packets for one particular VM are simultaneously aggregated under different LRO rules associated with these different queues. In some embodiments a same set of LRO rules are applied across different queues such that packets of a same VM being assigned to different queues may be aggregated under that same set of rules. In some embodiments multiple LRO rules are applied to one queue such that packets for different VMs can have different LRO aggregation rules even though they share the same queue. Examples of LRO rules that are applied across different queues will be further described by reference to below.

As mentioned LRO aggregation of incoming data packets to VMs of a host machine can be turned on or off based on LRO aggregation rules. In some embodiments the LRO aggregation rules are implemented on individual NIC queues of a PNIC as discussed above by reference to . In some embodiments the LRO aggregation rules are implemented within a virtualization software of a host machine as discussed above by reference to .

In some embodiments a LRO aggregation rule is implemented as a destination address filter that enables LRO operation for specific destination address MAC address IP address or other types of destination address . In some embodiments a LRO aggregation rule specifies a particular flow or microflow for which LRO operation is to be enabled. In some of these embodiments the flow is specified by a set of parameters that specifies a network session or connection e.g. the five tuple parameters of a TCP IP connection . In other words the LRO rule specifies that packet aggregation is to be turned on for data packets of a particular network session or transport connection but not for others.

For instance some embodiments use the five tuple IP data in the L3 and L4 packet header to classify the packet payload. The five tuple data include source port identifier destination port identifier source IP address destination IP address and the protocol. Using these five identifiers some embodiments can selectively turn on or off LRO aggregation for IP packets of different types such as VOIP packet video packet audio packet FTP packet HTTP packet HTTPS packet Remote Desktop packet PCoIP VNC RDP management packet authentication server health monitoring time synchronization E mail packet POP3 SMTP etc.

The examples provided below illustrates how the five tuples can be used to differentiate web traffic VoIP video streaming remote desktop management e mails by using the following notation Protocol src ip dst ip src port dest port with denoting wildcard match. In these examples it is assumed that that a VM is the client that requests the service data service from the server.

For some embodiments illustrates different queues or incoming packet buffers that are each enabled to perform LRO aggregation under different five tuple filtering. In some embodiments the queues are NIC queues inside a PNIC. In some embodiments these queues are implemented within the virtualization software of the host machine. Each queue is enabled to perform LRO aggregation under its own LRO aggregation rule such that a packet received from a physical network arriving at a particular queue will be aggregated into a LRO packet only if it meets the criteria set forth in the LRO aggregation rule of the particular queue. As illustrated the queues are enabled to perform LRO aggregation under LRO aggregation rule respectively.

The LRO aggregation rule is a five tuple filter that does not have any wild cards. It specifies that LRO operation would only be performed for packets that come from a specific sender source IP 192.168.10.2 and to a specific recipient destination IP 10.10.3.1 . It further specifies that LRO operation would only be performed for packets of a particular transport connection source transport port and destination transport port and that the transport protocol is TCP. Consequently the queue would accumulate only data packets with headers having the five tuple of TCP 192.168.10.2 10.10.3.1 1111 2222. All other packets arriving at the queue will not be aggregated into an LRO packet.

The LRO aggregation rule is a five tuple filter having several wild cards. In fact it only specifies only that the protocol used is TCP and that the source transport port be i.e. VoIP . In other words the LRO aggregation rule states that any packet with source transport and protocol TCP will be aggregated into LRO packets. When applied to the queue the LRO aggregation rule causes packets assigned to the queue to be aggregated into LRO packet if it has source transport and protocol TCP i.e. VoIP .

The LRO aggregation rule is also a five tuple filter having several wild cards. It specifies only that the protocol be TCP and that the destination transport port be i.e. POP3 . In other words the LRO aggregation rule states that any packet with destination transport port and protocol TCP will be aggregated into LRO packets. When applied to the queue the LRO aggregation rule causes packets assigned to the queue to be aggregated into LRO packet if it has destination transport port and protocol TCP i.e. POP3 .

Different embodiments implement queues for buffering incoming receive data packets differently and the LRO aggregation rules are applied differently in those different embodiments when creating LRO aggregated packets. illustrates the application of different LRO aggregation rules to different queues in the host machine where a LRO aggregation rule applied to a particular queue creates LRO aggregated packet by aggregating only packets assigned to that particular queue. As mentioned above in some embodiments each NIC queue is assigned to a computing resource such as a processor core or a processing thread. An LRO aggregation rule applied to such a queue is therefore applicable to the VM or the network stack that is being processed by that assigned computing resource.

For some embodiments illustrates LRO aggregation rules applied to queues that are assigned to computing resources in a host machine . As illustrated the host machine has computing resources labeled as CPUs that are used to operate VMs and network stacks . Specifically the computing resource is for operating the VM and the network stack the computing resource is for operating the VM and the network stack etc. The host machine also has NIC queues in a PNIC not illustrated for buffering incoming data packets from a physical network . The host machine includes a queue assignment module for assigning each incoming data packets from the physical network into one of the queues in the PNIC. The host machine also includes a RSS module receive side scaling for assigning network traffic from the NIC queues to the computing resources.

As illustrated each NIC queue receives its own LRO aggregation rule and performs LRO aggregation on data packets assigned that queue based on the received rule. Since the network traffic from the queues are distributed by the RSS module to one of the computing resources the LRO aggregation rule applied to a particular queue is applicable to the computing resource that is selected by the RSS to receive data packets LRO aggregated or not from that particular queue. In some embodiments the RSS selects computing resources to receive data from the queues in a manner to balance the computational loads between the different computing resources.

In some embodiments each VM and its corresponding network stack e.g. the VM and the network stack are handled by a same computing resource such as a same CPU a same CPU core or a same processing thread of a CPU . A LRO aggregation rule applied to a queue that is assigned to a computing resource is therefore applied to the VM that is performed by that computing resource. For example if the traffic from NIC queue is assigned to the computing resource then the LRO aggregation rule would be producing LRO aggregated packets for the VM . In some embodiments the RSS ensures that a thread of a VM stays on a CPU for the duration of a network connection session and thus an LRO aggregation rule that enables LRO aggregation for a particular five tuple would remain applicable for the VM for the duration of the network connection session according to that particular five tuple.

As illustrated the VM has a MAC address MAC1 and the VM has MAC address MAC2 . The queue receives only packets destined for VM because the filter is a MAC filter that allows only data packets destined for address MAC1 to enter the queue . Likewise the queue receives only packets destined for VM because the filter is a MAC filter that allows only data packets destined for address MAC2 to enter the queue . Consequently the LRO aggregation rule applied to the queue is applicable only to packets destined for the VM and the aggregated packet produced under LRO aggregation rule is always destined for VM regardless of whether the LRO aggregation rule actually specifies the destination address e.g. by having wild card on the destination IP part of the five tuple. Likewise is true for the LRO aggregation rule applied to the queue and the VM .

Though the example of uses L2 MAC address of a VM for filtering the incoming packets into the VM s corresponding queue one of ordinary skill would understand that other address schemes that uniquely address a VM can also be used for filtering packets into the VM s queue. For example some embodiments use the L3 IP address of the VM as filter for the VM s queue.

As mentioned in some embodiments NIC queues or incoming packet buffers are not necessarily tied to VMs. In some of these embodiments the LRO aggregation rule or rules applicable to a NIC queue would be applied to all incoming packets to that queue. The host machine would then forward the packets from the queue aggregated and non aggregated to their destination based on the destination address e.g. destination IP or destination MAC address in the packet headers.

For some embodiments illustrates an LRO aggregation rule applied to a queue that is not bound to any specific VMs. illustrates a host machine that is operating VMs and . The host machine has a queue for buffering incoming data packets from a physical network . The host machine also has software forwarding element for forwarding packets from the queue as well as other incoming packet buffers not illustrated to the VMs and others not illustrated . An LRO aggregation module receives a LRO aggregation rule for determining whether to aggregate packets in the queue into LRO aggregated packets. In some embodiments the LRO aggregation rule is a five tuple flow or microflow.

As illustrated the queue receives incoming data packets from the physical network the incoming data packets including some packets with destination address MAC1 the MAC address of VM and some packets with destination address of MAC2 the MAC address of VM . These packets arrive at the queue and the LRO aggregation module applies the LRO aggregation rule to create aggregated packets with destination address MAC1 and aggregated packets with destination address MAC2 . Packets that do not meet the requirement of LRO aggregation rule remain non aggregated non aggregated packets for MAC1 and non aggregated packets for MAC2 . The packets whether aggregated or non aggregated are then forwarded by the software forwarding element to their respective destinations the VM or the VM based on the destination address in the header.

In some embodiments the LRO aggregation rule specifies only the destination address. In other words the LRO aggregation rule enables LRO aggregation only for a VM having a particular address IP address or MAC address while packets for any other VMs will not be aggregated. For some embodiments illustrates an LRO aggregation rule that specifies only a destination address for the queue in the host machine .

The LRO aggregation rule is a rule that specifies that LRO aggregation is to take place for packets with destination MAC address MAC1 while all other packets i.e. packets with other destination MAC address will not be aggregated. Consequently all packets packets being forwarded to the VM by the software forwarding element are LRO aggregated and packets being forwarded to the VM packets by the software forwarding element are not LRO aggregated.

In some embodiments each particular LRO aggregation rule is applied not only to a one particular queue or incoming packet buffer but is instead applied to all incoming data packets stored in all queues. Furthermore multiple LRO aggregation rules are actively simultaneously to aggregate LRO packets under different rules and the LRO operations of at least one of the queues or some or all of the queues are governed by multiple LRO aggregation rules.

The host machine has three different LRO aggregation rules that are applied to the queues . Each of the LRO aggregation rules is applied to all three queues . Each LRO aggregation rule has a different effect on different queues depending on the packets being held in each queue.

The LRO aggregation rule is a 5 tuple rule that does not specify a specific destination address i.e. having wild cards in destination IP . The rule therefore affects all VMs and all queues and both queues and have LRO aggregated packets under rule aggregated packets and . However since the rule does require that the destination transport port be 110 and the protocol be TCP any packet that does not have the matching transport port ID or protocol required by the five tuple in will not be aggregated under this rule.

The LRO aggregation rule is a MAC filter it enable LRO aggregation only for the VM with MAC address MAC2 . The rule therefore affects only a queue that is holding data packets destined for MAC address MAC2 i.e. the VM . In the example of only the queue is holding packets destined for VM and therefore only the queue has LRO aggregated packet created under the rule . Furthermore in this example the queue holds only packets destined for the VM and therefore all of the data packets in the queue are aggregated under the rule aggregated packets and .

The LRO aggregation rule is a completely specified five tuple filter with a specified destination IP address 10.10.3.1 . The rule therefore affects only a queue that is holding data packets destined to IP address 10.10.3.1 i.e. the VM . In the example of only the queue is holding packets destined for the VM and therefore only the queue has LRO aggregated packet created under the rule aggregated packet . Furthermore since the rule also requires that the protocol be TCP UDP the source 192.16.10.2 the source transport port ID be 1111 the destination transport port ID be 2222 any packets failing to meet the these requirement will not be aggregated under the rule .

For some embodiments conceptually illustrates a process for applying LRO aggregation rules to packets in NIC queues or incoming packet buffers. In some embodiments the process is performed by a PNIC for each of its NIC queues. In some embodiments the process is performed by a virtualization software implementing LRO aggregation in its software implemented incoming packet buffers.

The process starts when it receives at a packet from the physical network. The process then determines at whether the packet is for this queue. In some embodiments the process applies a destination address filter e.g. a MAC filter that allows only packets with certain destination address or addresses into the queue. In some embodiments the process applies other criteria such performing hashing to determine whether the incoming data packet is to be assigned to a particular CPU that is assigned to the queue. If the packet is not for this queue the process ignores at the packet and let the packet be assigned to one of the other queues or incoming packet buffer and the process ends. If the packet is for this queue the process proceeds to .

At the process determines whether LRO aggregation is enabled for this packet. For a queue that uses five tuple microflow as LRO aggregation rule to determine whether to perform LRO aggregation the process examines whether the incoming packet meets the requirement of the five tuple. For a queue that uses another type of LRO aggregation rule such as MAC filtering the process examines the packet under the other criteria to determine whether to perform LRO aggregation. If the packet meets the requirement of the LRO aggregation rule the process proceeds to . If the packet does not meet the requirement of the LRO aggregation rule the process proceeds to .

At the process passes or sends the packet onto the VM without aggregation. In some embodiments the process notifies e.g. by interrupt the processor core of the host machine to let it know that that a packet is ready to be retrieved. In some embodiments the packet is stored in a memory area awaiting retrieval and the process uses an API to notify the host machine processor core of the memory location of the data packet. The process then ends.

At the process aggregates or adds the received packet into a current LRO aggregated packet that is still being aggregated. The process then determines at if the aggregation of the current LRO aggregated packet is complete. In some embodiments the process compares the size of the LRO aggregated packet against a threshold size usually larger than the MSS of TCP or MTU of Ethernet to determine if the LRO aggregated packet is large enough for delivery retrieval. If the LRO aggregated packet is complete and ready for retrieval the process proceeds to . If the LRO aggregated packet is incomplete and can aggregate more incoming received packet the process proceeds to to continue aggregation and ends.

At the process passes or sends the aggregated packet onto the VM. In some embodiments the process notifies e.g. by interrupt the processor core of the host machine to let it know that that a packet is ready to be retrieved. In some embodiments the packet is stored in a memory area awaiting retrieval and the process uses an API to notify the host machine processor core of the location in memory of the LRO aggregated packet. The process then ends.

As mentioned some embodiments turn off LRO operations on VMs that are forwarding packets partly because packets that are to be forwarded must comply with a maximum size requirement. Creating LRO aggregated packets that exceed the maximum size limit e.g. Ethernet MTU would require downstream segmentation operations e.g. TSO operations or fragmentation to dissolve the aggregated packets into smaller segments. However some embodiments do perform LRO aggregation on packets being forwarded by a VM. Some of these embodiments then segment the LRO aggregated packet according to the Maximum Segment Size MSS of the TCP protocol before forwarding the segmented packets to their destination.

In some embodiments for packets being forwarded through a VM with LRO having being turned on for the flows being handled by the VM the VM in its forwarding path can snoop on the TCP traffic for MSS and maintain stateful table for these flows and mark the large packet for TSO processing based upon the MSS that it snooped. Doing so avoids breaking the OSI model and still takes advantage of the hardware assist offload that is available in PNIC for tasks such as packet aggregation and segmentation and the VM does not have to do fragmentation on the packet in the forwarding path because of maximum transmission unit MTU limitation.

The MSS is the largest amount of data specified in bytes that TCP is willing to receive in a single segment. For best performance some embodiments set the MSS small enough to avoid IP fragmentation which can lead to packet loss and excessive retransmissions. Some embodiments announce MSS when the TCP connection is established. In some of these embodiments MSS is derived from the MTU size of the data link layer of the networks to which the sender and receiver are directly attached. In some embodiments the MSS is set to be smaller than the MTU to ensure that a TCP segment complying with the MSS size requirement at TCP layer would not be further segmented or fragmented at Ethernet data link layer.

The MSS is a parameter found in the TCP header of only certain types of packets e.g. a TCP Syn Ack packet not just any TCP header. The MSS parameter is typically set by the TCP layer of the protocol stack that originates the connection and then used by the TCP layer of the protocol stack that terminates the connection. The MSS parameter is typically not available to the forwarding VM since the network stack of the forwarding VM does not process forwarded traffic at TCP layer or above.

Some embodiments therefore snoop the packets being forwarded for its MSS parameter before using the snooped MSS parameter to perform Transmit Segmentation Offload TSO operation. conceptually illustrates the snooping of MSS parameter and the use of the snooped MSS parameter for packet segmentation on packets being forwarded by a VM. In the example the packets are being forwarded by a VM of a host machine from a source network node to a destination network node over a physical network . The host machine is also operating a virtualization software for hosting the VM and other VMs not illustrated .

In some embodiments packets being forwarded by a VM do not traverse all the way up in the network stack of the VM but are rather handled at lower layers of the network stack only. Specifically for packets being forwarded by a VM some embodiments only handle the network protocol of the link layer Ethernet layer and the Internet layer IP layer but not for the TCP transport layer and or above. Since MSS parameter is in TCP layer header but the forwarded packet is never processed by the forwarding VM s TCP layer some embodiments snoop and extract the MSS parameter from the TCP header when the lower layers of the network stack forwards the syn ack packet.

As illustrated the packet is a packet with several layers of encapsulation in which successive higher layer header and payload are encapsulated as a lower layer payload along with a lower layer header. The packet encapsulates a TCP layer packet under IP layer and Ethernet layer and consequently includes an Ethernet layer header an IP layer header and a TCP layer header and TCP payload. An MSS parameter is included in the TCP header since the encapsulated TCP layer packet is a TCP syn ack packet. In some embodiments the packet can be an LRO aggregated packet.

In some embodiments the segmentation operation that uses the extracted MSS parameter is performed by the NIC of the host machine . In some of these embodiments the NIC is a PNIC that is performs both the aggregation operation LRO and the segmentation TSO within its own hardware without consuming CPU cycles at the host machine . In some embodiments the PNIC receives the MSS parameter from the network stack as a metadata that accompanies a LRO aggregated packet.

For some embodiments illustrates the segmentation TSO operation by the PNIC based on a MSS parameter extracted by the network stack in the host machine . As illustrated the NIC in the host machine is a PNIC. It performs LRO aggregation on the incoming data packets and TSO segmentation the forwarded packet into the segmented packets . In some embodiments the PNIC performs LRO aggregation on the incoming packets at a NIC queue based on a LRO aggregation rule as described above in Sections I and II.

As mentioned in some embodiments a VM in its forwarding path can snoop on the TCP traffic for MSS and maintains stateful tables for these flows and mark the large packet for TSO processing based upon the MSS that it snooped. A stateful VM keeps track of the state of network connections such as TCP streams or UDP communication and is able to hold significant attributes of each connection in memory. These attributes are collectively known as the state of the connection and may include such details as the IP addresses and ports involved in the connection and the sequence numbers of the packets traversing the connection. Stateful inspection monitors incoming and outgoing packets over time as well as the state of the connection and stores the data in dynamic state tables. This cumulative data is evaluated so that context that has been built by previous connections and or previous packets belonging to the same connection would be available.

As illustrated the host machine is receiving packets from various source nodes. These received packets includes packets in different TCP connections A B C rectangles labeled A B and C in . In some embodiments each TCP connection includes traffic in both directions such as SYN and ACK packets such that packets of a particular connection can come from either end of the TCP connection.

The LRO aggregation module applies LRO aggregation rules on the received packets and produces packets . Since all of the received packets are to be processed and forwarded by the VM some embodiments assign all of the received packets to a same NIC queue in the PNIC and the LRO module represents the LRO aggregation operation that takes place in one NIC queue. In some embodiments packets destined for a same VM can be assigned to different queues and the LRO aggregation module represents LRO aggregation operations performed at different queues of the NIC. In some embodiments the LRO aggregation rules are flow based rules that target one or more TCP connections i.e. only packets belonging to certain TCP connections will be aggregated. Though not indicated in the figure some of the packets outputted by the LRO aggregation module are aggregated as they satisfy the criteria specified by the LRO rule while some of the packets are not aggregated as they do not meet the LRO rule.

The packets are delivered to the VM and its corresponding network stack for processing and forwarding. The VM operates a snooper module that snoops the content of the packets being forwarded. The snooper keeps track of the states of each TCP connection in stateful tables and . Among the state information being maintained is the MSS parameter of each TCP connection. Specifically MSS A is the MSS of TCP connection A MSS B is the MSS of TCP connection B. MSS C is the MSS of TCP connection C. The VM marks some of the packets as requiring TSO segmentation operation and the MSS parameters of the corresponding TCP connections are passed to the TSO module along with the marked packets as metadata. The marked packets are then sent to the TSO module as packets .

The marked packets forwarded by the VM reaches the TSO module which segments some or all of the packets according to the MSS parameter of the TCP connection that the packet belongs to. For example the packet belongs to TCP connection A and the TSO module would segment it according to MSS A i.e. each segmented packet is less than or equal to MSS A . Likewise the packet would be segmented according to the MSS of connection B and the packet would be segmented according to the MSS of connection C. In some embodiments each packet requiring TSO segmentation would arrive at the TSO module with a metadata that indicates its MSS i.e. the MSS of its TCP connection and the TSO module then segments the packet based on the MSS embedded in the metadata of the packet. The segmented packets are then delivered to their destinations as packets .

In some embodiments conceptually illustrates a process for snooping MSS parameter and a process for performing segmentation on LRO aggregated packets based on the snooped MSS parameter. The process uses the MSS parameter snooped by the process for performing segmentation. In some embodiments the processes and are performed by a host machine having a PNIC. In some embodiments the two processes are performed in the same host machine in parallel.

The process starts when it receives at an incoming packet from a network. The process then determines at if MSS information is available in the packet. In some embodiments not all packets have TCP headers and not all TCP headers specify MSS. Some embodiments examine the packet for a particular type of TCP header e.g. syn ack to determine if MSS parameter is available. If MSS parameter is available the process proceeds to to extract and store the MSS and ends. If MSS parameter is not available the process ends. The MSS parameter if extracted is made available to process .

The process starts whenever there are incoming packets from the network. The process performs at LRO aggregation on the incoming packets. In some embodiments this aggregation is enabled according to LRO aggregation rules such as five tuple microflows as discussed above in Section II. The process then performs at TSO segmentation on outgoing packets based on the extracted MSS. In some embodiments an extracted MSS is specific to a TCP connection and the process performs segmentation on a packet based on the MSS of the TCP connection that the packet belongs to. After performing segmentation on the aggregated packet the process ends.

Many of the above described features and applications are implemented as software processes that are specified as a set of instructions recorded on a computer readable storage medium also referred to as computer readable medium . When these instructions are executed by one or more processing unit s e.g. one or more processors cores of processors or other processing units they cause the processing unit s to perform the actions indicated in the instructions. Examples of computer readable media include but are not limited to CD ROMs flash drives RAM chips hard drives EPROMs etc. The computer readable media does not include carrier waves and electronic signals passing wirelessly or over wired connections.

In this specification the term software is meant to include firmware residing in read only memory or applications stored in magnetic storage which can be read into memory for processing by a processor. Also in some embodiments multiple software inventions can be implemented as sub parts of a larger program while remaining distinct software inventions. In some embodiments multiple software inventions can also be implemented as separate programs. Finally any combination of separate programs that together implement a software invention described here is within the scope of the invention. In some embodiments the software programs when installed to operate on one or more electronic systems define one or more specific machine implementations that execute and perform the operations of the software programs.

The bus collectively represents all system peripheral and chipset buses that communicatively connect the numerous internal devices of the electronic system . For instance the bus communicatively connects the processing unit s with the read only memory the system memory and the permanent storage device .

From these various memory units the processing unit s retrieves instructions to execute and data to process in order to execute the processes of the invention. The processing unit s may be a single processor or a multi core processor in different embodiments.

The read only memory ROM stores static data and instructions that are needed by the processing unit s and other modules of the electronic system. The permanent storage device on the other hand is a read and write memory device. This device is a non volatile memory unit that stores instructions and data even when the electronic system is off. Some embodiments of the invention use a mass storage device such as a magnetic or optical disk and its corresponding disk drive as the permanent storage device .

Other embodiments use a removable storage device such as a floppy disk flash drive etc. as the permanent storage device. Like the permanent storage device the system memory is a read and write memory device. However unlike storage device the system memory is a volatile read and write memory such a random access memory. The system memory stores some of the instructions and data that the processor needs at runtime. In some embodiments the invention s processes are stored in the system memory the permanent storage device and or the read only memory . From these various memory units the processing unit s retrieves instructions to execute and data to process in order to execute the processes of some embodiments.

The bus also connects to the input and output devices and . The input devices enable the user to communicate information and select commands to the electronic system. The input devices include alphanumeric keyboards and pointing devices also called cursor control devices . The output devices display images generated by the electronic system. The output devices include printers and display devices such as cathode ray tubes CRT or liquid crystal displays LCD . Some embodiments include devices such as a touchscreen that function as both input and output devices.

Finally as shown in bus also couples electronic system to a network through a network adapter not shown . In this manner the computer can be a part of a network of computers such as a local area network LAN a wide area network WAN or an Intranet or a network of networks such as the Internet. Any or all components of electronic system may be used in conjunction with the invention.

Some embodiments include electronic components such as microprocessors storage and memory that store computer program instructions in a machine readable or computer readable medium alternatively referred to as computer readable storage media machine readable media or machine readable storage media . Some examples of such computer readable media include RAM ROM read only compact discs CD ROM recordable compact discs CD R rewritable compact discs CD RW read only digital versatile discs e.g. DVD ROM dual layer DVD ROM a variety of recordable rewritable DVDs e.g. DVD RAM DVD RW DVD RW etc. flash memory e.g. SD cards mini SD cards micro SD cards etc. magnetic and or solid state hard drives read only and recordable Blu Ray discs ultra density optical discs any other optical or magnetic media and floppy disks. The computer readable media may store a computer program that is executable by at least one processing unit and includes sets of instructions for performing various operations. Examples of computer programs or computer code include machine code such as is produced by a compiler and files including higher level code that are executed by a computer an electronic component or a microprocessor using an interpreter.

While the above discussion primarily refers to microprocessor or multi core processors that execute software some embodiments are performed by one or more integrated circuits such as application specific integrated circuits ASICs or field programmable gate arrays FPGAs . In some embodiments such integrated circuits execute instructions that are stored on the circuit itself.

As used in this specification the terms computer server processor and memory all refer to electronic or other technological devices. These terms exclude people or groups of people. For the purposes of the specification the terms display or displaying means displaying on an electronic device. As used in this specification the terms computer readable medium computer readable media and machine readable medium are entirely restricted to tangible physical objects that store information in a form that is readable by a computer. These terms exclude any wireless signals wired download signals and any other ephemeral signals.

While the invention has been described with reference to numerous specific details one of ordinary skill in the art will recognize that the invention can be embodied in other specific forms without departing from the spirit of the invention. In addition a number of the figures including conceptually illustrate processes. The specific operations of these processes may not be performed in the exact order shown and described. The specific operations may not be performed in one continuous series of operations and different specific operations may be performed in different embodiments. Furthermore the process could be implemented using several sub processes or as part of a larger macro process. Thus one of ordinary skill in the art would understand that the invention is not to be limited by the foregoing illustrative details but rather is to be defined by the appended claims.

