---

title: Method and system for collecting and pre-processing quality of service data in a storage system
abstract: Methods and systems for collecting and processing quality of service (QOS) data are provided. A collection module receives the QOS data from a storage operating system for a plurality of storage volumes at time t, when a process for collecting the QOS data began at time t such that t>t. The collection module estimates a QOS data value for time t and provides the estimated QOS data value to a performance manager that uses the estimated QOS data value for monitoring QOS for the plurality of storage volumes using a plurality of resources for processing input/output (I/O) requests.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09542293&OS=09542293&RS=09542293
owner: NETAPP, INC.
number: 09542293
owner_city: Sunnyvale
owner_country: US
publication_date: 20140114
---
The present disclosure relates to collecting and pre processing quality of service QOS data in a storage system.

Various forms of storage systems are used today. These forms include direct attached storage DAS network attached storage NAS systems storage area networks SANs and others. Network storage systems are commonly used for a variety of purposes such as providing multiple clients with access to shared data backing up data and others.

A storage system typically includes at least a computing system executing a storage operating system for storing and retrieving data on behalf of one or more client computing systems may just be referred to as client or clients . The storage operating system stores and manages shared data containers in a set of mass storage devices.

Quality of Service QOS is used in a storage environment to provide certain throughput in processing input output I O requests as well as a response time i.e. latency within which I O requests are processed. QOS may also include processing certain number of I O requests per second IOPS which is associated with throughput. Throughput means an average rate at which data is transferred for I O requests. Different QOS levels may be provided to different clients depending on client service levels.

To process an I O request to read and or write data various resources are typically used within a storage system for example network resources processors storage devices and others. The different resources perform various functions for reading and writing information. The use of resources impact QOS for clients. For example if a client overuses a certain resource then it may delay I O processing for a client which may lower the QOS for client .

As storage systems continue to expand in size and operating speeds it is desirable to efficiently monitor resource usage within the storage system and analyze QOS data so that any incidents based on abnormal QOS data can be identified and handled appropriately. The storage operating system typically maintains QOS data regarding various storage volumes that use the resources of the storage system. Continuous efforts are being made to efficiently collect and pre process QOS data so that the data can be efficiently analyzed for identifying abnormal incidents that may impact overall I O processing in compliance with QOS policies.

As a preliminary note the terms component module system and the like as used herein are intended to refer to a computer related entity either software executing general purpose processor hardware firmware and a combination thereof. For example a component may be but is not limited to being a process running on a hardware processor a hardware based processor an object an executable a thread of execution a program and or a computer.

By way of illustration both an application running on a server and the server can be a component. One or more components may reside within a process and or thread of execution and a component may be localized on one computer and or distributed between two or more computers. Also these components can execute from various computer readable media having various data structures stored thereon. The components may communicate via local and or remote processes such as in accordance with a signal having one or more data packets e.g. data from one component interacting with another component in a local system distributed system and or across a network such as the Internet with other systems via the signal .

Computer executable components can be stored for example at non transitory computer readable media including but not limited to an ASIC application specific integrated circuit CD compact disc DVD digital video disk ROM read only memory floppy disk hard disk EEPROM electrically erasable programmable read only memory memory stick or any other storage device in accordance with the claimed subject matter.

In one aspect a performance manager module is provided for analyzing quality of service QOS data maintained by a storage operating system for processing input output I O requests for writing and reading data to and from storage devices. The storage system uses various resources to process the I O requests. The QOS data may include a throughput rate average number of IOPS that are processed by the storage operating system average response time a service time a wait time a visit time and a number of visits at each of the resources for processing the I O requests. The performance manager uses the QOS data to predict an expected range or threshold value for future QOS data. Future QOS data can be compared with the expected range to detect abnormal behavior.

The performance manager needs QOS data for specific times or time intervals for predicting the expected range. In one aspect a collection module is provided that interfaces with the storage operating system and polls the storage operating system for the QOS data for time intervals requested by the performance manager. The actual data however is received after a polling interval has begun. For example if polling starts at time t0 then certain QOS data may not be received until time t1 where t1 to. In one aspect as described below in detail the collection module uses the data received at t1 to provide an estimate of what the data would have been at t0. Thus the performance manager receives the estimated data for the specific polling interval. This allows the performance manager to efficiently perform its analysis without having to pre process the QOS data.

QOS provides a certain throughput latency and an average number of IOPS I O requests processed in a second for clients A N. The I O operations may be read and or write requests for reading and or writing data at storage devices. Throughput means average amount of data transferred within certain duration for example a second. Latency means a delay in processing an I O request and may be measured by an average response time in processing client I O requests.

Collection module interfaces with the storage operating system for receiving QOS data. The QOS data may include throughput information an average response time service time wait time and a number of visits at a plurality of storage system resources used for processing I O requests or any other QOS data type. The QOS data is pre processed and provided to the performance manager that stores the pre processed QOS data at a local data structure . The performance manager then analyzes the QOS data for predicting abnormal behavior.

The storage operating system maintains a large amount of QOS data for various storage volumes defined below and their associated clients. The collection module starts the collection process based on a polling interval that is specified by the performance manager . However the data that is received from the storage operating system does not exactly match the time and the polling interval. The collection module receives the QOS data and as described below in detail estimates i.e. pre processes or transforms what the data would have been if it was received at the time requested by the performance manager . This allows the performance manager to simply take the pre processed QOS data from the collection module and perform its analysis to ensure that abnormal incidents are being addressed.

In one aspect the storage system has access to a set of mass storage devices A N may be referred to as storage devices or simply as storage device within at least one storage subsystem . The storage devices may include writable storage device media such as magnetic disks video tape optical DVD magnetic tape non volatile memory devices for example solid state drives SSDs including self encrypting drives flash memory devices and any other similar media adapted to store information. The storage devices may be organized as one or more groups of Redundant Array of Independent or Inexpensive Disks RAID . The various aspects disclosed are not limited to any particular storage device type or storage device configuration.

In one aspect the storage system provides a set of logical storage volumes may be interchangeably referred to as volume or storage volume for providing physical storage space to clients A N or virtual machines A N . A storage volume is a logical storage object and typically includes a file system in a NAS environment or a logical unit number LUN in a SAN environment.

Each storage volume may be configured to store data files or data containers or data objects scripts word processing documents executable programs and any other type of structured or unstructured data. From the perspective of one of the client systems each storage volume can appear to be a single drive. However each storage volume can represent storage space in at one storage device an aggregate of some or all of the storage space in multiple storage devices a RAID group or any other suitable set of storage space.

When a storage volume is created a QOS policy may be associated with the storage volume such that requests associated with the storage volume can be managed appropriately. The QOS policy may be a part of a QOS policy group referred to as Policy Group and shown as QOS data structure that is used to manage QOS for different storage volumes. QOS at the storage system level may be implemented by a QOS module that maintains the QOS data structure . QOS module maintains various QOS data types that are provided to the collection module as described below.

The storage operating system organizes physical storage space at storage devices as one or more aggregate where each aggregate is a logical grouping of physical storage identified by a unique identifier and a location. The aggregate includes a certain amount of storage space that can be expanded. Within each aggregate one or more storage volumes are created whose size can be varied. A qtree sub volume unit may also be created within the storage volumes. For QOS management each aggregate and the storage devices within the aggregates are considered as resources that are used by storage volumes.

The storage system may be used to store and manage information at storage devices based on an I O request. The request may be based on file based access protocols for example the Common Internet File System CIFS protocol or Network File System NFS protocol over the Transmission Control Protocol Internet Protocol TCP IP . Alternatively the request may use block based access protocols for example the Small Computer Systems Interface SCSI protocol encapsulated over TCP iSCSI and SCSI encapsulated over Fibre Channel FCP .

In a typical mode of operation a client or a virtual machine transmits one or more I O request such as a CFS or NFS read or write request over connection system to the storage system . Storage operating system receives the request issues one or more I O commands to storage devices to read or write the data on behalf of the client system and issues a CIFS or NFS response containing the requested data over the network to the respective client system.

Although storage system is shown as a stand alone system i.e. a non cluster based system in another aspect storage system may have a distributed architecture for example a cluster based system that is described below in detail with respect to .

System may include a virtual machine environment where a physical resource is time shared among a plurality of independently operating processor executable virtual machines VMs . Each VM may function as a self contained platform running its own operating system OS and computer executable application software. The computer executable instructions running in a VM may be collectively referred to herein as guest software. In addition resources available within the VM may be referred to herein as guest resources. 

The guest software expects to operate as if it were running on a dedicated computer rather than in a VM. That is the guest software expects to control various events and have access to hardware resources on a physical computing system may also be referred to as a host platform or host system which maybe referred to herein as host hardware resources . The host hardware resource may include one or more processors resources resident on the processors e.g. control registers caches and others memory instructions residing in memory e.g. descriptor tables and other resources e.g. input output devices host attached storage network attached storage or other like storage that reside in a physical machine or are coupled to the host system.

In one aspect system may include a plurality of computing systems A N may also be referred to individually as host platform system or simply as server communicably coupled to the storage system executing the storage operating system via a connection system such as a local area network LAN wide area network WAN the Internet or any other interconnect type. As described herein the term communicably coupled may refer to a direct connection a network connection a wireless connection or other connections to enable communication between devices.

Host system includes a processor executable virtual execution environment executing a plurality of VMs A N that may be presented to client computing devices systems A N. VMs A N execute a plurality of guest OS A N may also be referred to as guest OS that share hardware resources . As described above hardware resources may include processors memory I O devices storage or any other hardware resource.

In one aspect host system interfaces with a virtual machine monitor VMM for example a processor executed Hyper V layer provided by Microsoft Corporation of Redmond Wash. a hypervisor layer provided by VMWare Inc. or any other type. VMM presents and manages the plurality of guest OS A N executed by the host system . The VMM may include or interface with a virtualization layer VIL that provides one or more virtualized hardware resource to each OS A N.

In one aspect VMM is executed by host system with VMs A N. In another aspect VMM may be executed by an independent stand alone computing system often referred to as a hypervisor server or VMM server and VMs A N are presented at one or more computing systems.

It is noteworthy that different vendors provide different virtualization environments for example VMware Corporation Microsoft Corporation and others. The generic virtualization environment described above with respect to may be customized to implement the aspects of the present disclosure. Furthermore VMM or VIL may execute other modules for example a storage driver network interface and others the details of which are not germane to the aspects described herein and hence have not been described in detail.

System may also include a management console that executes a processor executable management application for managing and configuring various elements of system . Application may be used to manage and configure VMs as well as configure resources that are used by VMs according to one aspect. It is noteworthy that although a single management console is shown in system may include other management consoles performing certain functions for example managing storage systems managing network connections and other functions described below.

In one aspect application may be used to present storage space that is managed by storage system to clients A N or VMs A N . The clients may be grouped into different service levels where a client with a higher service level may be provided with more storage space than a client with a lower service level. A client at a higher level may also be provided with better QOS vis vis a client at a lower level.

Before describing the details of process steps executed by the collection module the following provides a description of a clustered storage system from where QOS data is collected.

The clustered storage system includes a plurality of nodes . . a cluster switching fabric and a plurality of mass storage devices . . may be referred to as and similar to storage device . Each of the plurality of nodes . . is configured to include an N module a D module and an M Module each of which can be implemented as a processor executable module. Specifically node . includes an N module . a D module . and an M Module . node . includes an N module . a D module . and an M Module . and node . includes an N module . a D module . and an M Module ..

The N modules . . include functionality that enable the respective nodes . . to connect to one or more of the client systems . .N over the computer network while the D modules . . connect to one or more of the storage devices . .. Accordingly each of the plurality of nodes . . in the clustered storage server arrangement provides the functionality of a storage server.

The M Modules . . provide management functions for the clustered storage system . The M Modules . . collect storage information regarding storage devices .

Each node may execute or interface with a QOS module shown as . . that is similar to the QOS module of . The QOS module may be executed for each node or a single QOS module may be used for the entire cluster. The aspects disclosed herein are not limited to the number of instances of QOS module that may be used in a cluster.

A switched virtualization layer including a plurality of virtual interfaces VIFs is provided to interface between the respective N modules . . and the client systems . .N allowing storage . . associated with the nodes . . to be presented to the client systems . .N as a single shared storage pool.

The clustered storage system can be organized into any suitable number of virtual servers also referred to as vservers or storage virtual machines in which each vserver represents a single storage system namespace with separate network access. Each vserver has a client domain and a security domain that are separate from the client and security domains of other vservers. Moreover each vserver is associated with one or more VIFs and can span one or more physical nodes each of which can hold one or more VIFs and storage associated with one or more vservers. Client systems can access the data on a vserver from any node of the clustered system through the VIFs associated with that vserver. It is noteworthy that the aspects described herein are not limited to the use of vservers.

Each of the nodes . . is defined as a computing system to provide application services to one or more of the client systems . .N. The nodes . . are interconnected by the switching fabric which for example may be embodied as a Gigabit Ethernet switch or any other type of switching connecting device.

Although depicts an equal number i.e. 3 of the N modules . . the D modules . . and the M Modules . . any other suitable number of N modules D modules and M Modules may be provided. There may also be different numbers of N modules D modules and or M Modules within the clustered storage system . For example in alternative aspects the clustered storage system may include a plurality of N modules and a plurality of D modules interconnected in a configuration that does not reflect a one to one correspondence between the N modules and D modules.

Each client system . .N may request the services of one of the respective nodes . . . and that node may return the results of the services requested by the client system by exchanging packets over the computer network which may be wire based optical fiber wireless or any other suitable combination thereof.

Collection module interfaces with the various nodes and obtains QOS data that is pre processed and provided to the performance manager for analysis. Details regarding the various modules of collection module are now described with respect to .

As an example system A shows two clusters A and B both similar to cluster described above. Each cluster includes the QOS module for implementing QOS policies that are established for different clients applications.

Cluster A may be accessible to clients . and . while cluster B is accessible to clients . .. Both clusters have access to storage subsystems and storage devices . .N.

Clusters A and B communicate with the collection module for providing QOS data. Collection module may use a Zephyr Application Programming Interface ZAPI or any other interface type to communicate with clusters A and B. The various aspects disclosed herein are not limited to any particular interface type.

Collection module includes one or more acquisition modules for collecting the QOS data from the clusters. The data is pre processed by the pre processing module and stored as pre processed QOS data at a storage device. Pre processing module formats the collected QOS data for the performance manager as described below in detail. Pre processed QOS data is then provided to a collection module interface of the performance manager . QOS data received from collection module is stored as QOS data by performance manager at a storage device not shown .

Performance manager includes a plurality of modules for example a forecasting module that predicts an expected range for the pre processed QOS data a detection module that detects an abnormal incident and an incident analysis module that analyzes the QOS data for abnormal incidents and reports the incidents to a client system via a GUI . Performance manager may also recommend a corrective action plan to client .

In block B the collection module may initiate QOS data collection for a first poll at time t poll . In block B the actual data for the first poll is received by the collection module at time t A such that t A is greater than t poll .

In block B a second poll for the QOS data is initiated at time t poll . The actual data for the second poll is received in block B at time t A where again t A is greater than t poll .

In block B the collection module estimates the data that would have been received at time t poll . In one aspect the collection module estimates the data by performing a linear extrapolation operation using the data received at t A and t A .

In block B the collection module initializes collection for a third poll at time t poll . The actual data is received in block B at time t A where t A is greater than t poll .

In block B the collection module uses the actual data received at time t A and actual data received at time t A to estimate the data value that would have been received at t poll . The collection module uses linear extrapolation for estimating the data value that would have been received at t poll .

Thereafter in block B the collection module estimates a data value that would have been received at t poll . For example if throughout is the QOS data that is being collected for performance manager then the following equation may be used to estimate the reported throughput value at t poll Throughput poll 3 Estimated Value at poll 3 Estimated Value at poll 2 Polling Interval poll 3 poll 2 

Thereafter in block B the collection module reports the estimated value at t poll to the performance manager .

Process repeats steps B B for receiving actual data for next polling intervals i.e. t poll t poll and so forth and estimating the data for times that the performance manager is expecting the data.

Process has advantages because the collection module collects and pre processes QOS data for the performance manager so that the performance manager can perform its analysis for identifying abnormal incidents without having to collect and pre processing collected data.

In one aspect the collection module maintains a plurality of counters for collecting QOS data from storage operating system . When null data is received from the storage operating system indicating that there was no change from a previous interval is not reported to the performance manager . This may be referred to as zero suppression . This is efficient for performance manager that does not have to store null data. Considering that performance manager gets a large amount of data for different QOS data types and different storage volumes zero suppression can result in reduction of significant data processing time.

Performance manager uses a certain minimal amount of QOS data for example QOS data for 3 hours or 30 data samples . Performance manager measures the performance of different workloads on a cluster based dynamically generated threshold values or an expected range based on historical QOS data. Performance manager compares actual workload QOS data to the expected range for the QOS data. A workload that reaches the threshold value may then trigger an incident which is then analyzed for corrective action.

Various resources are used within a cluster to process I O requests. As an example there are two types of resources a service center and a delay center. The service center is a resource category that can be represented by a queue with a wait time and a service time for example a processor that processes a request out of a queue . The delay center may be represented by a queue that does not include service time and instead only represents wait time. The distinction between the two resource types is that for a service center the QOS data includes a number of visits wait time per visit and service time per visit for incident detection and analysis. For the delay center the number of visits and the wait time per visit at the delay center are used as described below in detail.

Table I below provides an example of the various service and delay centers whose QOS data is used to track workload performance. Table I also identifies the resource type i.e. utilization and or latency type .

Performance manager uses the concept of workloads for analyzing QOS data for incident detection and analysis. Workloads are defined based on incoming I O requests. As an example a workload may include a plurality of streams. Each stream may have a plurality of requests. The requests may be generated by any entity for example an external entity like a client system and or an internal entity for example a replication engine that replicates storage volumes at one or more storage location.

A request may have a plurality of attributes for example a source a path a destination and I O properties. The source identifies the source from where a request originates for example an internal process a host or client address a user application and others.

The path defines the entry path into the storage system. For example a path may be a logical interface LIF or a protocol such as NFS CIFS iSCSI and Fibre Channel protocol.

A destination is the target of a request for example storage volumes LUNs data containers and others.

In one aspect streams may be grouped together based on client needs. For example if a group of clients make up a department on two different subnets then two different streams with the source restrictions can be defined and grouped within the same workload. Furthermore requests that fall into a workload are tracked together by performance for efficiency. Any requests that don t match a user or system defined workload may be assigned to a default workload.

In one aspect workload streams may be defined based on the I O attributes. The attributes may be defined by clients. Based on the stream definition performance manager tracks workloads.

Table II below provides a listing of the various objects with multiple instances that are used by the performance manager for incident detection and analysis 

Collection module collects data using a plurality of counter objects. Data for these various object counters is collected from storage operating system . Table III below provides an example of the workload object counters 

Table IV below provides an example of the details associated with the object counters that are collected by the collection module as part of the QOS data.

Processors A B may be or may include one or more programmable general purpose or special purpose microprocessors digital signal processors DSPs programmable controllers application specific integrated circuits ASICs programmable logic devices PLDs or the like or a combination of such hardware devices. The local storage comprises one or more storage devices utilized by the node to locally store configuration information for example in a configuration data structure . The configuration information may include information regarding storage volumes and the QOS associated with each storage volume.

The cluster access adapter comprises a plurality of ports adapted to couple node . to other nodes of cluster . In the illustrative aspect Ethernet may be used as the clustering protocol and interconnect media although it will be apparent to those skilled in the art that other types of protocols and interconnects may be utilized within the cluster architecture described herein. In alternate aspects where the N modules and D modules are implemented on separate storage systems or computers the cluster access adapter is utilized by the N D module for communicating with other N D modules in the cluster .

Each node . is illustratively embodied as a dual processor storage system executing a storage operating system similar to that preferably implements a high level module such as a file system to logically organize the information as a hierarchical structure of named directories and files at storage .. However it will be apparent to those of ordinary skill in the art that the node . may alternatively comprise a single or more than two processor systems. Illustratively one processor A executes the functions of the N module on the node while the other processor B executes the functions of the D module.

The memory illustratively comprises storage locations that are addressable by the processors and adapters for storing programmable instructions and data structures. The processor and adapters may in turn comprise processing elements and or logic circuitry configured to execute the programmable instructions and manipulate the data structures. It will be apparent to those skilled in the art that other processing and memory means including various computer readable media may be used for storing and executing program instructions pertaining to the disclosure described herein.

The storage operating system portions of which is typically resident in memory and executed by the processing elements functionally organizes the node . by inter alia invoking storage operation in support of the storage service implemented by the node.

The network adapter comprises a plurality of ports adapted to couple the node . to one or more clients . .N over point to point links wide area networks virtual private networks implemented over a public network Internet or a shared local area network. The network adapter thus may comprise the mechanical electrical and signaling circuitry needed to connect the node to the network. Each client . .N may communicate with the node over network by exchanging discrete frames or packets of data according to pre defined protocols such as TCP IP.

The storage adapter cooperates with the storage operating system executing on the node . to access information requested by the clients. The information may be stored on any type of attached array of writable storage device media such as video tape optical DVD magnetic tape bubble memory electronic random access memory micro electro mechanical and any other similar media adapted to store information including data and parity information. However as illustratively described herein the information is preferably stored at storage device .. The storage adapter comprises a plurality of ports having input output I O interface circuitry that couples to the storage devices over an I O interconnect arrangement such as a conventional high performance Fibre Channel link topology.

In one example storage operating system may include several modules or layers executed by one or both of N Module and D Module . These layers include a file system manager that keeps track of a directory structure hierarchy of the data stored in storage devices and manages read write operation i.e. executes read write operation on storage in response to client . .N requests.

Storage operating system may also include a protocol layer and an associated network access layer to allow node . to communicate over a network with other systems such as clients . .N. Protocol layer may implement one or more of various higher level network protocols such as NFS CIFS Hypertext Transfer Protocol HTTP TCP IP and others.

Network access layer may include one or more drivers which implement one or more lower level protocols to communicate over the network such as Ethernet. Interactions between clients and mass storage devices . . or are illustrated schematically as a path which illustrates the flow of data through storage operating system .

The storage operating system may also include a storage access layer and an associated storage driver layer to allow D module to communicate with a storage device. The storage access layer may implement a higher level storage protocol such as RAID redundant array of inexpensive disks while the storage driver layer may implement a lower level storage device access protocol such as Fibre Channel or SCSI. The storage driver layer may maintain various data structures not shown for storing information regarding storage volume aggregate and various storage devices.

As used herein the term storage operating system generally refers to the computer executable code operable on a computer to perform a storage function that manages data access and may in the case of a node . implement data access semantics of a general purpose operating system. The storage operating system can also be implemented as a microkernel an application program operating over a general purpose operating system such as UNIX or Windows XP or as a general purpose operating system with configurable functionality which is configured for storage applications as described herein.

In addition it will be understood to those skilled in the art that the disclosure described herein may apply to any type of special purpose e.g. file server filer or storage serving appliance or general purpose computer including a standalone computer or portion thereof embodied as or including a storage system. Moreover the teachings of this disclosure can be adapted to a variety of storage system architectures including but not limited to a network attached storage environment a storage area network and a storage device directly attached to a client or host computer. The term storage system should therefore be taken broadly to include such arrangements in addition to any subsystems configured to perform a storage function and associated with other equipment or systems. It should be noted that while this description is written in terms of a write any where file system the teachings of the present disclosure may be utilized with any suitable file system including a write in place file system.

The processing system includes one or more processor s and memory coupled to a bus system . The bus system shown in is an abstraction that represents any one or more separate physical buses and or point to point connections connected by appropriate bridges adapters and or controllers. The bus system therefore may include for example a system bus a Peripheral Component Interconnect PCI bus a HyperTransport or industry standard architecture ISA bus a small computer system interface SCSI bus a universal serial bus USB or an Institute of Electrical and Electronics Engineers IEEE standard 1394 bus sometimes referred to as Firewire .

The processor s are the central processing units CPUs of the processing system and thus control its overall operation. In certain aspects the processors accomplish this by executing instructions stored in memory . Processor may be or may include one or more programmable general purpose or special purpose microprocessors digital signal processors DSPs programmable controllers application specific integrated circuits ASICs programmable logic devices PLDs or the like or a combination of such devices.

Memory represents any form of random access memory RAM read only memory ROM flash memory or the like or a combination of such devices. Memory includes the main memory of the processing system . Instructions implement the process steps described above with respect to may reside in and executed by processors from memory . Instructions may also be used to implement the forecasting module detection module and incident analysis module according to one aspect.

Also connected to the processors through the bus system are one or more internal mass storage devices and a network adapter . Internal mass storage devices may be or may include any conventional medium for storing large volumes of data in a non volatile manner such as one or more magnetic or optical based disks. The network adapter provides the processing system with the ability to communicate with remote devices e.g. storage servers over a network and may be for example an Ethernet adapter a Fibre Channel adapter or the like.

The processing system also includes one or more input output I O devices coupled to the bus system . The I O devices may include for example a display device a keyboard a mouse etc.

Thus a method and apparatus for collecting and pre processing QOS data have been described. Note that references throughout this specification to one aspect or an aspect mean that a particular feature structure or characteristic described in connection with the aspect is included in at least one aspect of the present disclosure. Therefore it is emphasized and should be appreciated that two or more references to an aspect or one aspect or an alternative aspect in various portions of this specification are not necessarily all referring to the same aspect. Furthermore the particular features structures or characteristics being referred to may be combined as suitable in one or more aspects of the disclosure as will be recognized by those of ordinary skill in the art.

While the present disclosure is described above with respect to what is currently considered its preferred aspects it is to be understood that the disclosure is not limited to that described above. To the contrary the disclosure is intended to cover various modifications and equivalent arrangements within the spirit and scope of the appended claims.

