---

title: System and method for continuous analytics run against a combination of static and real-time data
abstract: A system for continuous analytics comprises an in-memory storage and a processor. The processor receives a query that indicates an analytic function to be performed on a combination of static data and real-time data. The processor loads the in-memory storage from a first source external to the processor with data that was stored as the static data. At the same time that the in-memory storage is already loaded with the static data, the processor continuously receives ephemeral real-time data as it is being generated by a second source external to the processor. The processor runs the analytic function from the query against the in-memory storage loaded with the first data and the real-time data that is being continuously received, in combination, to produce a result of the analytic function. The result is stored and time-stamped in the in-memory storage as an analytic cube.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08977600&OS=08977600&RS=08977600
owner: Software AG USA Inc.
number: 08977600
owner_city: Reston
owner_country: US
publication_date: 20140523
---
This application claims the benefit of the following Provisional applications 61 827 194 filed May 24 2013 61 827 223 filed May 24 2013 61 827 249 filed May 24 2013 61 827 278 filed May 24 2013 61 827 322 filed May 24 2013 61 827 355 filed May 24 2013 61 827 387 filed May 24 2013 and 61 827 412 filed May 24 2013 all of which are expressly incorporated herein by reference.

The technical field relates in general to computer languages and more specifically to computer languages which process data.

 Large data is a collection of data sets that are so large that they are difficult to process using traditional database tools. Large data is sometimes referred to as big data . Data sets tend to result from combinations of separate smaller sets of data. A typical approach to handling big data is massively parallel software running on multiples of servers for example using a MapReduce programming model. However this approach does not work for all applications. The problems associated with large data are of particular concern when dealing with analytics on the data.

Referring now to a flow chart illustrating a conventional procedure for generating analytics will be discussed and described. is a representation of a conventional process for handling large data and using analytics on the data. A data source provides data which has an a priori known data format such as from a stock market. A process to generate conventional analytics inputs in step a defined model for the data such as a model for stock market data. In step the process runs the data into the pre determined model which is known to be appropriate for the data. In step a user manually prepares queries which can be run on data in the pre determined model. The queries are run and the query results are displayed to the user in step .

The problems emanating from this conventional process can be explained by considering two distinct areas addressed by conventional mechanisms.

The first area is Traditional Business Intelligence BI . Traditional BI style systems extract data into a data warehouse or read data from a database and then analyze the highly structured data. Traditional BI systems or database systems are characterized by several problematic qualities. In these systems data typically resides in a single highly structured source such as a database or data warehouse. Additionally the data and data structure are tightly coupled.

Another key factor in these systems is that they required significant preparation such as data collection aggregation and loading into some repository to prepare for analysis. In many cases a large amount of data cleansing will also be required. Most of these steps are done manually.

Another concern with BI systems is that they produce static results. Analytic visualizations are bound to static data and are no longer live. Analysis and exploration are no longer attached to the original data source but instead to a snapshot of the data.

BI systems also exhibit a lack of extensibility. Analytics are limited to what is provided out of the box and cannot be dynamically extended.

BI systems are also limiting because real time support does not exist. These systems cannot analyze real time data that is constantly updated and pushed from the source systems.

The second area that is addressed by conventional mechanisms is Streaming Analytics. Streaming analytics systems analyze data in motion event based and are not designed to simultaneously analyze data in motion and data at rest. These systems are problematic because analytics is only performed on streaming or data in motion. Another issue with these systems is that there is an inability to efficiently process real time data with data at rest. A third limitation is that these systems use cases for real time data and are uniquely different from traditional BI analysis.

In short conventional analytic systems are devised to either handle snapshot of transactional data or streaming data but not both simultaneously.

One or more embodiments discussed herein can address the aforementioned problems with traditional mechanisms by not only resolving the problems and issues of performing continuous and dynamic analytics on a combination of static and real time data but also by resolving problems that occur when the data involved is exceptionally large.

Accordingly one or more embodiments provide a system method and or non transitory computer readable medium for continuous analytics run against a combination of static and real time data.

Accordingly an embodiment provides a system for a system for continuous analytics run against a combination of static and real time data. The system comprises an in memory storage and a processor. The processor is operably connected to the in memory storage. The processor is programmed to receive in a query engine from a client a query that indicates an analytic function to be performed on a combination of static data and real time data in response to receipt of the query that indicates the analytic function to be performed on the combination of static data and real time data load the in memory storage from a first source external to the processor with data which was stored as the static data at the same time that the in memory storage is already loaded with the static data continuously receive real time data as it is being generated by a second source external to the processor the real time data being ephemeral in response to the real time data which is continuously received subsequent to receipt of the query to temporally correlate the real time data with existing static data in the in memory storage and to continuously run the analytic function from the query against in combination both the real time data which is being continuously received and the existing static data which is loaded in the in memory storage and which is temporally correlated to the real time data to continuously produce a result of the analytic function.

According to another embodiment the first source of the static data is from a different source than the second source of the real time data the combination of static data and real time data is referenced in the query as a single variable and the processor is further configured to determine a source of the data the data being referenced in the single variable and to process the data differently based on the source of the data.

According to another embodiment the static data has a data structure different from the real time data.

According to another embodiment at least one of the static data and the real time data is originated by the first or second sources respectively in a hierarchical format.

According to another embodiment the processor is further configured to analyze the real time data and the existing static data which are temporally correlated in a moving window of the real time data as streaming data and discard the real time data which has been analyzed and temporally correlated to make room for the real time data which continues to be received.

According to another embodiment the processor is further configured to store information about a running calculation being made by the analytic function to re use the information about the running calculation in continuously performing the analytic function with the real time data which is newly received.

According to another embodiment the combination of static and real time data indicated in the query which is received and on which the analytic function is performed indicates at least one of 1 a combination of different types of static data 2 combinations of different types of real time data and 3 combinations of different types of static data and real time data and the processor is further configured to normalize the at least one of 1 the combination of different types of static data 2 the combinations of different types of real time data and 3 the combinations of different types of static data and real time data into normalized data which is stored in the in memory storage in the form of a tuple. The analytic function which is run is applied against the normalized data stored in the in memory storage.

According to another embodiment the processor is further configured to store the result of the analytic function in the in memory storage as an analytic cube for use in subsequent queries together with a time stamp in association with the result of the analytic function as part of the analytic cube and in response to a subsequent analytic function for the combination of static data and real time data having a time period which includes the time stamp re use the result of the analytic function from the in memory storage based on the time stamp associated therewith as a source of data for the subsequent analytic function.

Another embodiment provides a method for continuous analytics run against a combination of static and real time data. The method comprises receiving in a query engine from a client a query that indicates an analytic function to be performed on a combination of static data and real time data in response to receiving the query that indicates the analytic function to be performed on the combination of static data and real time data loading by a processor an in memory storage from a first source external to the processor with data which was stored as the static data at the same time that the in memory storage is already loaded with the static data continuously receiving by the processor real time data as it is being generated by a second source external to the processor the real time data being ephemeral in response to the real time data which is continuously received subsequent to receipt of the query temporally correlating by the processor the real time data with existing static data in the in memory storage and continuously running by the processor the analytic function from the query against in combination both the real time data which is being continuously received and the existing static data which is loaded in the in memory storage and which is temporally correlated to the real time data to continuously produce a result of the analytic function.

Yet another embodiment provides a method for continuous analytics run against a combination of static and real time data comprising by a processor in response to a command that indicates an analytic function to be performed on at least real time data continuously receiving by the processor real time data as it is being generated by a source external to the processor the real time data being ephemeral in response to the real time data which is continuously received subsequent to the command continuously running by the processor the analytic function from the command against the at least real time data which is being continuously received to continuously produce a result of the analytic function storing by the processor the result of the analytic function in an in memory storage as an analytic cube for use in subsequent calculations together with a time stamp in association with the result of the analytic function as part of the analytic cube and in response to a subsequent calculation for the real time data having a time period which includes the time stamp re using by the processor the result of the analytic function from the in memory storage based on the time stamp associated therewith together with the real time data which is newly received as a source of data for the subsequent calculation.

According to another embodiment the command that indicates both the analytic function to be performed and a combination of static data and real time data on which to perform the analytic function a source of the static data is different from a source of the real time data the combination of static data and real time data is referenced in the command as a single variable and further comprising determining by the processor a source of the data being analyzed by the analytic function the data being referenced in the single variable and processing by the processor the data differently based on the source of the data.

According to another embodiment the subsequent calculation is a running calculation being made by the analytic function further comprising continuously performing the analytic function.

According to another embodiment the subsequent calculation is made in response to a subsequent command that indicates a second analytic function to be performed on the at least real time data.

Another embodiment provides a non transitory computer readable medium which can perform a method according to one or more of these embodiments.

One or a combination of more than one or all of the above embodiments can be combined and provided in a single embodiment.

Moreover the purpose of the foregoing abstract is to enable the U.S. Patent and Trademark Office and the public generally and especially the scientists engineers and practitioners in the art who are not familiar with patent or legal terms or phraseology to determine quickly from a cursory inspection the nature and essence of the technical disclosure of the application. The abstract is neither intended to define the invention of the application which is measured by the claims nor is it intended to be limiting as to the scope of the invention in any way.

In overview the present disclosure concerns computer systems sometimes referred to as client server networks such as may be associated with computer systems providing apps. Such computer systems often involve running an app on a user s computer that invokes a web service providing live data. More particularly various inventive concepts and principles are embodied in systems devices and methods therein for providing a computer language and or underlying implementation and or architecture useful for data analysis optionally including discovery of data structure and or analytics of such data for example where the data is big data dynamic real time streaming hierarchical and or the data structure is not known a priori.

The detailed description of the mechanism is explained through a detailed description of an embodiment involving the use of RAQL and EMML. While not limiting the mechanism such discussion of RAQL and EMML is believed to provide the greatest clarity in explaining the mechanism. RAQL is merely a convenient example of a query language the principles discussed herein can be applied to other query languages EMML is an example of an XML markup language which is useful for creating software applications particularly for mashups that consume data from a variety of sources.

The present disclosure concerns a mechanism to continuously analyze data which is based on a combination of static non changing temporal data and real time temporal data data that is continuously changing . Real world operational decisions need decisions that are based on analytics derived from data that is changing and data at rest. 

Temporal data is data which contains a timestamp. The timestamp can represent the date time of the data creation date time of the data collection and or the date time of the data analysis. As will be discussed in detail for non changing data the system dynamically retrieves data from source such as a database or web service time stamps the data and places the data in an in memory store.

A query can be done and if intermediate result sets are acquired they can be stored into the memory with time stamps on the information. Thus if dealing with data which changes constantly data can be compared analyzed on a temporal basis. Every time data is stored in the memory it can be time stamped. Every record and every chunk of record can be time stamped. Further queries can be run on a temporal basis. For example an initial query is done and the original time stamped results are stored a later query is done which specifies for example time to time. Consequently temporal analyses and comparisons can be performed. To enable the continuous analysis of both real time and static together the mechanism can employ a Real Time Analytic Query Language RAQL which will be described in detail later a query engine and optimizer connectors to the data sources and a declarative markup language Enterprise Mashup Markup Language EMML . At the highest level the RAQL query language is similar in form to SQL but is unique in at least three important ways

First the data that is being processed can be hierarchical such as XML not just tabular. Second the source of the data represented in the query within the from clause can be any data source such as a database webservice and or a data that is in memory or even a messaging queue topic. Third the source and structure of the data does not have to be known at design time it can be discovered and realized at query run time.

EMML can serve as a declarative wrapper for the all the information needed to connect and to query the data to perform analytics and both RAQL and EMML are described in the Welcome to the Presto Library for 3.7 in the section Presto Analytics currently accessible at the following weblink http mdc.jackbe.com prestodocs v3.7 raql getStarted.html. EMML and RAQL can be written and run or dynamically generated and run.

The instant disclosure is provided to further explain in an enabling fashion the best modes of performing one or more embodiments. The disclosure is further offered to enhance an understanding and appreciation for the inventive principles and advantages thereof rather than to limit in any manner the invention. The invention is defined solely by the appended claims including any amendments made during the pendency of this application and all equivalents of those claims as issued.

It is further understood that the use of relational terms such as first and second and the like if any are used solely to distinguish one from another entity item or action without necessarily requiring or implying any actual such relationship or order between such entities items or actions. It is noted that some embodiments may include a plurality of processes or steps which can be performed in any order unless expressly and necessarily limited to a particular order i.e. processes or steps that are not so limited may be performed in any order.

Much of the inventive functionality and many of the inventive principles when implemented are best supported with or in software or integrated circuits ICs such as a digital signal processor and software therefore and or application specific ICs. It is expected that one of ordinary skill notwithstanding possibly significant effort and many design choices motivated by for example available time current technology and economic considerations when guided by the concepts and principles disclosed herein will be readily capable of generating such software instructions or ICs with minimal experimentation. Therefore in the interest of brevity and minimization of any risk of obscuring principles and concepts further discussion of such software and ICs if any will be limited to the essentials with respect to the principles and concepts used by the exemplary embodiments.

The primary problems associated with handling analytics and large data center around four main issues. First it can be problematic to handle analytics on hierarchical data in the form of for example XML extensible markup language or JSON JavaScriptObject Notation . Second it can be challenging to deal with large data due to the nature of analytics. Third it can be problematic to perform analytics on large data using a steaming approach where some mashups for example exceed their architectural limits when there are large amounts of data or steaming data being processed. Fourth it can be difficult to run temporal based analytics on large data sets. Conventional approaches offer no practical solutions to these problems.

Various inventive principles and combinations thereof are advantageously employed to provide a new analytics language that can handle large data. The language and implementation aspects to support the language and its data handling are quite different from conventional SQL.

The disclosure provides various principles that disclose the mechanism based in part on the new query language its architecture implementation and abilities. The new query language may be considered to appear to be close to SQL because the new language is somewhat SQL standard oriented. However the implementation is totally different from SQL.

The new query language can follow a standard format of SQL statements albeit with some differences. Specifically the new query language can follow ANSI SQL for example SQL 2003 or other variants and evolutions such as SQL 1999 SQL 2006 SQL 2008 SQL 2011. However a new query language is provided herein which can resemble ANSI SQL language structure.

The new query language can be extended to hierarchical data instead of just tabular data as is handled by SQL. An embodiment can use SQL like language with a slightly different syntax to query documents and streaming data. This can cover the ability to introduce a query against hierarchical data.

There are other differences of the new query language in comparison to SQL. For example the UDF user defined function syntax and the namespace as used in the new query language is not part of SQL. There is a framework provided in the new query language to support this.

The new query language may be considered to be SQL like or partially SQL compliant in that it can use at least some of the constructs of SQL. However there are at least two major constructs that are significantly different from SQL. First a FROM clause against hierarchical data streaming data is used in the new query language. Second the new query language can obtain data from a variable versus the actual data source. For example a streaming variable can be queried.

The new query language has an engine underneath. In an example of 10 mb of data a tree has to be created in a conventional approach and then it can be queried. This puts lots of stress on the JVM Java virtual machine and memory especially for big data. In the new query language chunks of data can be processed instead which avoids requiring lots of memory. The chunks can be discarded when done.

An understanding of the features in the query language may help to clarify the mechanism. The following section discusses aspects of features one or more of which can be included in embodiments 1 the new query language itself for example new queries new formats 2 implementation underlying the new query language 3 the ability to handle user defined functions UDF in memory 4 interaction with in memory data by the new query language 5 the ability to support hierarchical and streaming data 6 a functionality that works with static and real time data 7 the ability to run the new query language dynamically including the ability of the new query language to discover the data format without being aware of a priori the data structures 8 analytic data cubes for the new query language and 9 an expanded discussion of the mechanism.

The commands can work on hierarchical data streaming data and or large data sets. The format can be similar to SQL. The SQL like statements in the new query language have a similar structure to SQL but operate on non SQL like data i.e. on non tabular data. In SQL one can SELECT columns from a table as a normal statement . In comparison in the new query language it is not a table but a hierarchical document or streaming data that is continuously changing. The style of SQL looks similar but the function of SQL typically is for databases. An intention of the new query language is to be a real time analytical engine that works on variables and hierarchical data. It can also handle temporal data streaming data and or large data.

With temporal data there can be a real time nature of the data data can be real time can be large data sets and or can be arbitrary data sets in memory . There can be a time stamp of the record or a single time stamp of a set of records and or a single time stamp on a chunk of records also referred to as a record chunk . As a consequence the mechanism using the new query language is not tied to the data source.

For the time stamping when querying large data sets the results are stored into in memory stores. A query is done and if intermediate result sets are acquired they are stored into the memory with time stamps on the information. Thus if dealing with data which changes constantly data can be compared analyzed on a temporal basis. Every time data is stored in the memory it is time stamped. Every record and every chunk of record can be time stamped. Further queries can be run on a temporal basis. For example an initial query is done and the original time stamped results are stored a later query is done which specifies for example time to time. Consequently temporal analyses and comparisons can be performed.

The in memory storage may be characterized as a table that has a virtual column for time stamp. The time stamp column is updated every time that record is updated. Every time that group of data is updated the group level time stamp is updated also.

Note that the new query language puts its own time stamp in the new query language has no expectation that data which is retrieved explicitly has its own time information.

The new query language uses a different syntax. For example the FROM statement FROM variable name path path is optional differs from conventional mechanisms. In SQL there is a table name instead of a variable name FROM table name . The FROM clause has been extended. The path provides the hierarchy. For example customer customer path . . . 

The new query language can query XML documents or a collection of XML documents or JSON data for example.

The new query language can be used for querying one XML or JSON document as an example of a simple case . The new query language can also be used for querying across multiple documents i.e. a collection of entities . By contrast in SQL the statement operates on one entity.

As another example consider a mashup that snapshots data at a period of time. The new query language can enable the mechanism to query across thousands of snapshots as they are snapshotted or while they are being snapshotted without having to load data into a database as with traditional database systems.

Internally when the data is received it can be normalized into a standard format for example a tuple format or a format that includes a tuple in which additional data is stored as part of the tuple. For example the XML data is changed into tuples and then stored into memory by the engine and the query is then applied on it. The tuple ordered list of named values also contains a timestamp for temporal queries. A single tuple or a set of tuples can be processed by the query language or stored in memory and then processed by the query language.

To recap data for example XML data CSV comma separated values data and or JSON data and or other data may come in to the scope of the new query language. They can be internally normalized for example to a tuple format. Then the new query language operation can be applied to the stored data. These normalized forms can be saved in the in memory database and then later queried against.

A tuple can be as an example without being limited to Java a normalized Java object which is the data model for the new query language engine . Inside the tuple is time stamp name value for example a hash map .

No a priori structure is expected there need be no required structure or canonical structure that the new query language puts data into. The tuple is not really a structure it is an ordered list. This gives the mechanism the ability to handle any data with any structure. In SQL by comparison a database is both expected and required.

The new query language is schema free also referred to as model free . The new query language can remember relevant meta data about the data as it is coming in. In SQL schemas must be first created and then the data can be queried in the pre determined schema by the SQL command. In the new query language by contrast the schema does not need to be created before the data is queried by the new query language command. The discovery of data source happens at the query. No pre determined data format is stored. However there can be a schema helper that stores small metadata to help later. As the data arrives the new query language can discover the data on the fly and then query it. That also allows the new query language to represent the hierarchical nature in the language.

There can be a data syntax inference engine. Regarding a hierarchical record path the element path is optional. An inference engine can infer an element path. The high performance structure inference engine can infer the data path. In the FROM clause for querying a hierarchical document a path can be specified to whatever the record is. Sometimes these documents paths are straightforward. However if the path is not given the system tries to walk the hierarchy and can try to infer what the record path is most likely to be for this query. If the path is not given in the FROM clause it tries to infer the path from the document. This is new for a query language. To infer the structure the root of the path can be known and then the hierarchy can be walked if something repeats itself periodically heuristics are used. This is not always correct.

For example in the query discussed above SELECT NAME ADDRESS FROM congress legislators legislator path has a full path. However SELECT NAME ADDRESS FROM congress does not have full path. In this example the repeating elements can be used as the record to infer the path.

UDF is an extension mechanism. The UDF in the new query language can work similar to an SQL UDF command. The user would implement a UDF and deploy it as a Java class to make those functions available in a UDF command. Once the function is plugged in for example installed the user can access their logic. A SQL UDF is written typically in C or PostScript.

The UDF can be hot deployed. That is it can be automatically detected and loaded without a need for server downtime.

An example of a UDF is SELECT modulename.function name. In this case function name is a user written function. The functions can be written in accordance with known techniques. For example there are generally three types of functions 1 simple scalar 2 analytic aggregate functions and 3 analytic windowing functions. The new query language can handle UDF by using a framework which enables users to write for the UDF. The function which is written is put into a framework. The framework does the work to deploy the functions. The functions can be written in Java or other languages they can be a call to another service or they can be a call to libraries or similar. Any of these can be conventional or known. This is a very different style of doing a UDF from that performed by SQL SQL s UDF must operate within a known database.

In the new query language the function is placed into the UDF framework and then it can be called as a UDF function in an SQL style functional call and then it operates on the new query language type of data. This can be the tuple formatted data in which the grouping and temporal functions have been done to organize the data and present it to the analytic function.

In an alternate embodiment there is a streaming UDF function which can handle streamed data. The data can be streamed in and bottlenecks are avoided. In another embodiment there can be a facility for intermediate interim in memory storage. In yet another embodiment there is hot deploy capability.

Referring now to a table of core RAQL functions will be discussed and described. is a table that includes several examples of core RAQL functions. The functions can be categorized into one of three types of functions for example simple aggregate and window . These types of functions are discussed further below.

When the data is being streamed out of the new query language the data can be re directed to an in memory store sometimes referred to as an in memory cache . The re direction can be accomplished using a command for example an EMML statement . Later the system can load the data from the in memory store using a LOAD statement and the load from data being read from the in memory can be directed to new query language. A command for example an EMML statement called can be used to load the information from the in memory store. Initially there may be a large amount of data. After the data is stored into the in memory store the new query language based query analytics can be performed on the in memory data.

 In memory refers to in memory processing everything happens in memory. In memory can refer to off heap memory memory that is managed outside a Java JVM or outside the Java JVM process. In trying to overcome the bottleneck inside a process when a process runs out of memory and fails memory can be obtained outside the process and managed by the process itself. The memory is no longer limited to process available memory. The in memory store does not use the hard drive or outside drives. It uses only high speed memory for example RAM. The in memory store can span multiple RAMs and or multiple machines.

A secondary index can be used for the in memory store. The in memory can be indexed so that the in memory can be queried. For example time stamping allows ready access of for example the last 10 minutes of data. The in memory structure can incorporate the time based index which can help in fast retrieval of queries that include a time component.

The EMML streaming support is unique. The LOAD TO and STORE FROM commands are unique to the new query language. The in memory usage can be done with for example EMML.

Typically the term hierarchical data refers to an XML document JSON document or the like. Streaming data can arrive as large data sets that do not need to be all loaded into memory at one time. The mechanism wants to carry out queries on the streaming data but it also wants to avoid having all of the data in memory because dealing with a huge amount of data will cause it to run out of memory.

Iteration and chunking can be used to avoid loading all of the data. Consider an example of 1 GB of large data 1 million records . The system can bring the big data into memory in chunks operate on the chunk send the operated on chunk to the client and bring in the next chunk. This is referred to as streaming because it does not need to have all data in memory at the same time. This can be extremely useful for certain operations.

Some other operations do need all of the data in memory. For example for sorting and grouping of data when large amounts of memory are needed it is stored so that it won t occupy all of the JVM. Also when doing aggregate or window functions the system can leverage streaming along with in memory. The excess data can be stored in in memory. The goal is to avoid for certain types of operations for example filter having all of the data in memory.

With respect to chunking consider 1 million records and a filter operation. The system can read each record see if it validates and if not successful the record is discarded not stored anywhere. The chunk can be just one record. If the data is stored in memory the user can specify the chunk size partition size for example 10 000 records. The system can read in and store the records in buckets of 10 000 tuples. The operation for example a filter operation will be read in 10 000 chunks operate on them and then push them out to the client.

That is the system can repeatedly validate discard or forward depending on validation results. As a variation the system can perform chunking before validating.

Continuous analytics can be run against a combination of static and real time data. In this functionality the in memory is loaded with data or a database or a portion thereof . At the same time real time data is arriving and being processed for example using functionality discussed herein.

Continuous analytics can also be run against 1 combinations of different types of static data 2 combinations of different types of real time data or 3 combinations of different types of both static and real time data.

The new query language can be dynamically generated and executed. The new query language does not need to know the data format for example the data schema in advance. In comparison SQL knows the data structure before the query is run. A part of this is a model free functionality in which the structure of the data can be discovered at run time. The new query language can be generated on the fly. The new query language does not need to know the data format in advance. In comparison SQL knows the data structure. The new query language can discover the data structure on the fly. This can come in handy if a user is doing something with a tool or application. The queries do not have to be written. The system can infer what the user is trying to do and then can generate the query. The new query language engine or server has no pre existing knowledge about the query itself.

There can be an engine that executes new query language statements. The server can run the new query language queries. The server can handle the real time data that is coming in. If there is a database it is just another data source recall that new query language can obtain data from other sources the system can invoke and get data from any other source which becomes a data source for the new query language to run on. The new query language only needs a data source to be pointed to. Another way to point to the data source for example a web service or data base is to run an actual conventional SQL to fetch the results which can then be stored in memory to provide a data source for the new query language to start with.

It is dynamically generated and model free. The new query language does not need any schema. The new query language can import data. Data coming from a database for example is not originally in a tuple format. The data is then placed into tuple format for the query engine. For example the data arriving from different disparate data sources can be normalized into the above discussed tuple format. Also the data can be fetched in pre defined sizes as chunks.

There can be adapters in the new query language engine to perform the formatting for the fetched data convert JDBC to tuple convert XML to tuple convert JSON to tuple and the like. Then the converted data can be operated on by the new query language engine. In context the adapter knows that the data comes from for example XML or example from the query or from the query sub type. The system can do an inference from the sub type from the statement and the like.

Regardless of where the data is coming from for example spread sheet data base web service or elsewhere the system can figure out the data structure can adapt the data to the normalized format for the new query language and can then run the query on the data in the normalized format for the new query language.

The system can pre compute and store small sets of analytics about a data source that is repeatedly requested by a user. Analytic cubes can be created dynamically or in advance. Analytic cubes can be thought of as analytics which are stored in memory for direct queries or as data provided for subsequent analytic queries. Analytic cubes are fully in memory and are treated as an intermediate source of data for subsequent queries. For example consider a list of transactions for stock purchases the system can repeatedly aggregate average stock price computer total stock price and the like. For the analytic functions the system can aggregate this information into a structure which is sometimes referred to herein as an analytic data cube or a mini cube which are the same thing. Then instead of going through all of the data in response to a subsequent query on the data source the system can just examine the mini cube and retrieve the information from the mini cube. The information stored in the mini cube may not be 100 current but it is good enough for further aggregations or analytics on the data source.

Consider another example the analytics command is COUNT . Then this command is repeated a minute later and the count result which counts the data is stored into the mini cube . The second time the command performed on the same data set can retrieve the count result from the mini cube and not go to re count the data set. The result of the analytics command is stored in the mini cube.

The analytics cubes can be stored as temporal data. That is when the data is stored in one of the analytics cubes a time stamp can be include in the analytic cube. The records themselves in the analytics cubes can also be time stamped.

Referring now to an example of a RAQL query listing and an example of an output table from a RAQL query listing respectively will be discussed and described. The RAQL query listing in illustrates the RAQL query that calculates the running moving average labeled aunningAvgPricePerHouf of a stock data and re calculates the running average every hour. Line A shows the data that will be extracted. Line A shows the aggregation avg price which will be calculated for each time window partition is shown beginning in line A. Lines A to A create the partition window by element and time. These lines show that the query should partition the aggregation shown in A by symbol year month and hour which basically equates to the partition occurring every hour. Line A shows the order by clause with symbol and datetime and tells the analytic engine to calculate a running average for each symbol by datetime. Line Anames the partition result RunningAvgPricePerHour and in this example is calculated for a single stock which has four pricings per hour as illustrated in the table in . The table is an example of two hours worth of data.

Referring now to a block diagram of a first batch of data retrieved from a database in a first hour and a second hour will be discussed and described. illustrates the first batch of data of Hour 1 and Hour 2 being retrieved from a database in step 1a . Pull Data . The retrieval gets the full eight data elements in one batch and the query is run to calculate the aggregate data in step . Calculate . In step . Push to Analytic Cube the results are pushed into the Virtual In memory Analytic Cube which includes a time stamp for each element. The data set in this example represents data up to Hour 2 and Minute 59.

Referring now to a block diagram illustrating the process of data streaming into the Analytic Engine will be discussed and described. illustrates how data begins streaming into the analytic engine . As the data stream begins the analytic engine is aware that the query is part of a continuous query RAQL Listing and the prior data and calculations Table 501 are already stored in a Virtual In memory Analytic Cube and as part of the Query Optimizer the analytic engine knows to retrieve prior calculated values as required as shown in step . Retrieve Calculation of the block diagram in .

Since the data begins after the batch calculated data Time Hour 2 Minute 59 and the query partitions by hour the first stream element Time Hour 3 Minute 5 is processed by the analytic engine and compared against prior calculated data and pushes the results into the analytic cube . In stream element at Time Hour 3 Minute 15 the next data element arrives and the analytic engine processes the new data against the prior data in the analytic cube and pushes the new results into the analytic cube . This continues as long as the stream continues to arrive as shown in stream elements and

Referring now to a block diagram illustrating a query that takes data from the Virtual In Memory Data Cube will be discussed and described. includes a query which instead of calculating the data in its current place retrieves the data from the Virtual In Memory Data Cube .

Data cubes are modeled in EMML. The cubes specify assorted data sources which may include but are not limited to data that is purely static files updatable datasets SQL Service and or real time streaming channels. Data cubes measure aggregates and machine learning functions kmeans linear logistic regression to be computed. The measures can be computed both in batch and incrementally. Caches such as BigMemory cache can be associated with data cubes.

The stocks data source elements semantically bind the cube to all three data sources in the above sample snippet i.e. stocks.xml datafile service that returns stock info and stocksStreams channel .

Based on the cube definition aggregates are computed using dimensions and measures specifications. This involves computation of a whole aggregates for static datasets and b incremental aggregates for a hybrid of static and streaming datasets.

The BigMemory cache named in the cube is used for storing all states associated with the cube. This includes computed aggregates and partial computation states that aid in incremental aggregate calculation. The cube is not limited to a single cache mechanism such as the aforementioned BigMemory cache.

Regarding incremental aggregations the UDF Framework supports but is not limited to Map Reduce style paradigms to compute aggregations. This can be used for computing both whole aggregations and incremental aggregations. It facilitates incremental aggregations by having functions store partial computation state that can used in subsequent calculations.

With regard to the querying cube the cube is referenced as a data source in RAQL for querying data. Examples include 

In sampling mode aggregations are performed on a sampled dataset instead of the whole dataset. This mode may be used for calculating approximate aggregates under heavy loads.

First variation. The analytic can be run on the data set behind the scenes as data is changing and the results can be stored in mini cubes. This avoids doing any calculations at run time because the results have previously been calculated on the fly.

Second variation. The values for an analytic on a data source are pre computed and stored in an analytics cube and the process is repeated to create multiple analytics cubes. That means the results are temporal and analytics can be performed which possible analyze temporal variations. The system can trend how the values are changing over time by having a series of analytic data cubes associated with a data source. This can provide a series of snapshots of the analytics cubes themselves. The series of snapshots as history can be reviewed backward and forward to determine what is trending.

RAQL can reference a variable in its queries. The variable acts as a pointer to the source of the data that is referenced by RAQL to perform the queries. The following are examples of using the same variable constructs to reference different data types and data that is at rest and streaming in motion.

The data variable that references the data can take the following forms which are provided by way of example 

Data variable a. is an example of a variable which references a static data file which can be CSV XML or JSON. The file can be located locally or remotely as long as it is accessible to the analytic engine. The variable is given a name in this case stocks and references a CSV data file named stocks.csv. All that may be required for variables that reference static data files is to include the datafile reference to the actual file name which contains the data.

Data variable b. is an example of a variable which references a data source which requires invocation such as a database Webservice REST ATOM feed and or a Mashup. In this example it is a two step declaration. The first step is declaring the variable in this example named myNews . The next step is invoking the service data source by name. In this example the service name is CNN News its endpoint feed rss.cnn.com rss cnn mostpopular.rss was previously registered and given the name CNN News for easier reference by the analytic engine. It also provides the ability for the analytic engine to apply authorization security since the service is registered by an alias name not its actual endpoint. The output of the data can be held in the variable named myNews .

Data variable c. is an example of a variable which references a database directly as a data source using SQL. This also is a two step declaration. The first step is declaring the variable named myInventory. The next step is to use the sql construct and issue a query of which results are held in the variable named myInventory. 

Data variable d. is an example of a variable which references data that is in memory. This too can use two steps the first is to declare the variable named InMemoryData the second is to use the loadFrom construct to load data from in memory. This example loads data from a cache named sensorDataCache into a variable named InMemoryData. 

Data variable e. is an example of a variable which references streaming data. It uses the channel construct which can reference a messaging Queue Topic and or any data source that provides streaming data and is referenced by the variable streamingData. 

The point in the above explanation is to illustrate the fact that different data formats sources and types streaming data file and data source oriented can all be referenced as a single variable with a name.

Once the data variable is attached to data RAQL can perform its real time queries and can transparently handle real time data and static data using the same query mechanism.

RAQL takes the form of Select . . . from . . . . The is any variable as described above. A characteristic of analytics against static and streaming data is demonstrated best using temporal data.

In this example the RAQL queries work the same whether data source is A. a Data File B . a Data Service or C . a Streaming Data Channel as defined in the following. All three sources in this example will be based on stock prices.

The following RAQL queries behave the same running against the stocks variable irrespective of the source and the style of data streaming or static .

Each of the above queries produces the exact same results if the dataset contains the same data over the same time period when running against the stocks variable. The underlying implementation of the analytic engine processes the data differently based on the source without requiring changes to the query itself. Specifically the analytic engine identifies the source of the runtime data and selects the data scanner that is appropriate for data retrieval.

Referring now to a block diagram illustrating the process flow of the analytic engine when it selects the proper data scanner referenced in the from variable of the RAQL query will be discussed and described. shows the process flow of the analytic engine selecting the proper data scanner referenced in the from variable of the RAQL query. When the analytic engine processes the query it identifies the variable type based on runtime determination of the underlying data source. Then it selects the proper data scanner which can either be a data file scanner a data service scanner or a streaming data scanner .

If the analytic engine chooses the data file scanner the engine creates a File Reader and iterates over each line in the file if CSV or each element if XML or JSON using respective CSV Scanner JSON Scanner and pr XML Scanner and converts it to a Tuple format and hands off processing to the analytic engine. If the analytic engine chooses the data service scanner the data source is further analyzed and the specific data scanner is selected here represented by DB Scanner Webservice Scanner RSS ATOM Scanner and REST Scanner. If the DB Scanner is selected a JDBC connection is established and the JDBC record iterator is handed to the analytic engine for processing. In the case of the Webservice Scanner RSS ATOM Scanner and REST scanner an HTTP or HTTPS connection is established and an iterator is handed to the analytic engine for processing. Finally if the analytic engine detects a data variable which references streaming data a future determination is made as to whether the source is a JMS source or an Event source which could be a proprietary event driven API.

The processing is similar between the data file scanner and the data service scanner . The analytic engine synchronously processes the source data until the result set is complete. However the streaming data scanner processes a continuous stream of data with a scanner herein represented by a JMS Scanner and an Event Scanner and terminates the scan either after the stream is fully processed or a time window is defined. An example of a time window is based on the query time partition but could be a time partition as small as a few seconds or as large as a minute hour or days.

The in memory is loaded with data or a database or a portion thereof. At the same time data is arriving as it is being generated. The static data is data that has already been loaded in and it can come from databases real time data which is in memory or other data. The system can run the new query language over the data that is not changing and the new data that is coming through. Conventionally analytic engines only work on static data and CEP type analytic engines only work on data in motion. This system can do both at the same time.

An example of doing both at the same time could be machine to machine data. Machines are monitored by sensors and the data is collected in real time to show the health there is also historical information.

Using this a user can say in a situation that power consumption information is collected for example show me whether my power consumption in the last 5 minutes is greater than the power consumption in the last 3 months.

Referring now to a block diagram illustrating in memory real time analytics using a combination of static and real time data will be discussed and described. Entitled In Memory Real Time Analytics illustrates Axeda which is an example real time M2M cloud that is taking data off of machines being monitored. That data is coming directly into in memory . Presto is an application that is accessing the four illustrated databases and pushing that into the in memory. The new query language can run analytics on the data that is in memory which is both static and real time using static historical data from the four databases and real time data which is being pushed into the in memory .

Referring now to a block diagram illustrating real time in memory analytics using a combination of static and real time data will be discussed and described. The example in entitled Real Time In Memory Analytics illustrates historical transactional data retrieved from plural databases which is static data and stored into in memory the live data as real time data is pushed into the in memory from an M2M source for example as the live data is collected from the source and Big Data is streamed in as it occurs as real time data . The analytics can be executed on a combination of the static and real time data which exists in the in memory area to produce the results of the analytics in this illustration displayed on the User Interfaces .

Referring now to an example of a user interface illustrating real time in memory analytics using a combination of static and real time data will be discussed and described. Some data is coming from historical transactional data and some is coming from live data. This user interface illustrates output of analytics on a combination of historical and real time data for Spark Plug Analysis. This example has 3 months of data loaded into in memory. The data is being examined for an hourly average and an average by the minute. But there is also live data that is coming in in this example from an M2M source as it is being collected at the source. The system is able to look at the historical information and run the analytics against the live data and the historical data in combination to provide for example plug voltage grouped by hour and power output by hour in addition to statistical charts with hourly minimum hourly maximum with latest value and hourly average and also number of readings and average by measurement ID. As real time data is newly provided the analytics can provide new results based on the newly provided data and the user interface can be updated to reflect the new results.

In this context Live data can mean data that is coming in which is being reviewed and applied against historical information live data has not been stored as historical data. Live data is not necessarily streaming data.

 Streaming data streaming allows the data to be run over and processed as it arrives vs. going to get the entire data set and processing it. Streaming refers to the way the data is processed. Streaming data is continuous it continuously arrives as it continues to be generated in some instances as a practical matter the streaming data does not stop coming. An example could be Twitter data.

 Chunking is independent of the streaming. For example in memory data is not streamed because the data is already stored there. What the mechanism does is to partition the large data set into chunks which have a pre determined size as pre defined by the user. When it is read from the large data set in memory the mechanism can achieve that in memory behavior by storing in chunks to simulate streaming when processing that data it reads the first chunk of the pre determined size from in memory and then process that data reads the second chunk from in memory and process that data and so on. That will simulate streaming behavior from data that flows from in memory into the new query language engine. When the chunk has been processed by new query language it can be discarded from the in memory.

The streaming data is processed as it arrives. The data is transient is captured and then analyzed and correlated with existing static data. The real time data is correlated to the static data. Static data can be for example information about the devices that are being monitored such as device off on device characteristics and the like. It does not get updated on a regular periodicity. The real time data is updated changed frequently for example what is the current voltage at this moment in time.

The new query language can be run on the streaming data and or the data can be loaded into the in memory and then the analytic is processed directly from the stream and or from in memory. The new query language provides the ability for the mechanism to run analytics on both static data and real time data as it arrives simultaneously.

Regarding continuous aggregation the analytic engine handles the new data and the static data and stores information about the running calculation being made by the analytic engine and the current results of the analytic. What is stored might depend on the type of the analytic.

Reference is made to a block diagram illustrating portions of a computer system. The computer system may include a communication port and or transceiver or the like for optional communication with a data source such as a source of streaming data a data set with data of undetermined format and or a database with a fixed format a processor a memory a display interface a display an input interface and or a user input device such as a keyboard trackball mouse joystick pointing device and or similar.

The computer system can receive data of an undetermined format for example from one or more web services as an information providing service as an example of streaming data from a database or other means to obtain data from a data source.

The illustrated communication port transceiver is representative of one or more receiver and or transmitter and or transceiver communication ports for wired or wireless communication over a computer network or communication network.

The processor may comprise one or more microprocessors and or one or more digital signal processors. The memory may be coupled to the processor and may comprise a read only memory ROM a random access memory RAM a programmable ROM PROM and or an electrically erasable read only memory EEPROM . The memory may include multiple memory locations for storing among other things an operating system data and variables for programs executed by the processor computer programs for causing the processor to operate in connection with various functions such as a client and a query engine the client can comprise a user interface CLI or the like in which a new query language query is run which indicates data set s and an analytic to perform on the data set s and the query initiates a call to the query engine the query engine can perform functions such as to receive a query from the client for an analytic to be performed on a combination of static and real time data to load in memory storage with data stored as static data to continuously receive real time data as it is generated to run the analytic function against a combination of the ephemeral real time data as it is received and the static data to produce a result of the analytic which relies on both the ephemeral data which will be discarded and the static data which will remain in the in memory storage to store a running calculation as it is continuously being made by the analytic for further use in a next iteration of the running calculation an in memory storage which can be JVM Java virtual machine and or off heap memory and a data storage location for other information used by the processor . The computer programs may be stored for example in ROM or PROM and may direct the processor in controlling the operation of the computer system . Each of these functions is considered in more detail elsewhere herein.

The user may invoke functions accessible through the user input device and may interface with the processor through an input interface . The user input device may comprise one or more of various known input devices such as a keyboard and or a pointing device such as a mouse the keyboard may be supplemented or replaced with a scanner card reader voice control or other data input device the pointing device may be a mouse touch pad control device track ball device or any other type of pointing device and the input interface can be a known interface thereof to communicate with the processor .

The text and or image display is representative of a display that may present information to the user by way of a conventional liquid crystal display LCD or other visual display and or by way of a conventional audible device for playing out audible messages. The display interface can be a known interface thereof to communicate between the processor and the display .

Responsive to signaling from the user input device in accordance with instructions stored in memory or automatically upon receipt of certain information via the communication port and or transceiver the processor may direct the execution of the stored programs.

The computer system discussed here or elsewhere in this document can include a central processing unit CPU with disk drives not illustrated symbolic of a number of disk drives that might be accommodated by the computer. Typically these might be one or more of the following a floppy disk a hard disk a CD ROM a digital video disk an optical disk a removable flash memory or the like or variations thereof. The number and type of drives may vary typically with different computer configurations. Disk drives may be options and for space considerations may be omitted from the computer system used in conjunction with the processes described herein. The computer may also include a CD ROM reader and CD recorder which are interconnected by a bus along with other peripheral devices supported by the bus structure and protocol not illustrated . The bus can serve as the main information highway interconnecting other components of the computer and can be connected via an interface to the computer. A disk controller not illustrated can interface disk drives to the system bus. These may be internal or external.

It should be understood that is described in connection with logical groupings of functions or resources. One or more of these logical groupings may be omitted from one or more embodiments. Likewise functions may be grouped differently combined or augmented without parting from the scope. Similarly the present description may describe various databases or collections of data and information. One or more groupings of the data or information may be omitted distributed combined or augmented or provided locally and or remotely without departing from the scope.

Referring now to a flow chart illustrating a procedure supporting continuous analytics run against a combination of static and real time data will be discussed and described. The procedure can be executed on the system of or other hardware appropriately configured. The procedure can include both a query engine procedure and a client procedure . The client procedure can receive in accordance with conventional techniques a query . If the client procedure determines that the query is in the query language format the client procedure can pass the query to the query engine for handling. The query engine procedure can receive a query passed to it such as from the client procedure . The query engine procedure can load a storage with data indicated in the query on which the analytic function is to be run the data is stored as static data. Static data can mean that the data will not be updated in the storage into which the query engine procedure has stored it and which the query engine procedure will be using for the analytic. The query engine procedure can start receiving and continue to receive real time data from a data source which is also indicated in the query as another source of data for the analytic function. The query engine procedure can run the analytic function from the query against the storage loaded with the static data and the real time data which is being continuously received to produce the result of the analytic. The query engine procedure can continue to provide the result of the analytic as the real time data continuously is received and thus might be changing the result of the analytic. Note the real time data is continuously received in parallel with the query engine procedure iteratively running the analytic function on the static and real time data.

The claims may use the following terms which are defined to have the following meanings for the purpose of the claims herein. Other definitions may be specified in this document.

The term computer system or computer used herein denotes a device sometimes referred to as a computer laptop personal computer personal digital assistant notebook computer personal assignment pad server client mainframe computer or evolutions and equivalents thereof. As one example the computer system may be a general purpose computer or a specially programmed special purpose computer. It may be implemented as a distributed computer system rather than a single computer. Similarly a communications link may be World Wide Web a modem over a POTS line data links and or any other wired or wireless method of communicating between computers and or users. Moreover the processing could be controlled by a software program on one or more computer system or processors or could even be partially or wholly implemented in hardware.

The term communication networks used herein denotes those that transmit information in packets for example those known as packet switching networks that transmit data in the form of packets where messages can be packetized and routed over network infrastructure devices to a destination. Such networks include by way of example the Internet intranets local area networks LAN wireless LANs WLAN wide area networks WAN cellular telephone networks general packet radio service GPRS services GSM global system for mobile communications cellular network and others and can be supported by networking protocols such as TCP IP Transmission Control Protocol Internet Protocol and UDP UP Universal Datagram Protocol Universal Protocol and or other protocol structures and variants and evolutions thereof. Such networks can provide wireless communications capability and or utilize wireline connections such as cable and or a connector or similar. Any appropriate communication protocol may be used.

The term app is short for application and denotes a computer executable software program that performs a function to benefit the user. Typically the term app is used to refer to discrete applications that provide a single function and a simple user interface. The term app is sometimes used to refer to programs such as GoogleMaps. An app is a way to visualize a mashup. An app is different from an operating system that runs the computer.

The term mashup used herein is defined as a software application that combines pre existing components from one or more information providing services into a single tool which can comprise a server side and a client side application where the components used by the mash up are visually presented to a user on a display at the client side in a manner which is different from the pre determined presentation of the information providing service and is configured in accordance with standards such as Enterprise Mashup Markup Language EMML XML interchanged as REST or Web Services RSS Atom and other evolutions and variations of mashup standards. A mashup is to be distinguished from a portal in which content is presented side by side in the manner which is the same as the pre determined presentation of the information providing service. The designation component as used in this paragraph refers to data which is retrieved by a mashup in real time from an information providing service. A mashup is frequently made by access to open APIs and other data sources to produce results that were not the original reason for producing the raw source data. An example of a mashup is the use of cartographic data from Google Maps to add location information to real estate data thereby creating a new and distinct Web service that was not originally provided by either source.

The term service sometimes referred to herein as an information providing service is used herein expressly to refer to an information providing service that provides data from a server in a visual presentation on a display to a user typically an application programming interface API or web API that can be accessed over a computer network and executed on a remote system hosting the requested services in accordance with Extensible Markup Language messages that follow the Simple Object Access Protocol SOAP standard such as SOAP Version 1.2 specification Web Services Description Language WSDL such as Web Services Description Language Version 2.0 Specification Representational State Transfer REST constraints and variations and evolutions thereof. An example of a service is Google Maps a Web service or an RSS feed.

The term live data as used herein is defined as data that is coming in which is being reviewed and applied against historical information. Live data has not been stored as historical data. Live data is not necessarily streaming data.

The term streaming data as used herein is defined as data that is delivered and processed as soon as it arrives versus going to get the entire data set and processing it. Streaming refers to the way the data is processed. Streaming data is continuous. It continuously arrives as it continues to be generated. In some instances as a practical matter the streaming data does not stop coming. An example could be Twitter data.

The term chunking as used herein is independent of the streaming. For example in memory data is not streamed because the data is already stored there. The mechanism partitions the large data set into chunks which have a pre determined size as pre defined by the user. When it is read from the large data set in memory the mechanism can achieve that in memory behavior by storing in chunks to simulate streaming. Upon processing that data it reads the first chunk of the pre determined size from in memory and then process that data reads the second chunk from in memory and process that data and so on. That will simulate streaming behavior from data that flows from in memory into the new query language engine. When the chunk has been processed by new query language it can be discarded from the in memory.

The term hierarchical data as used herein refers to the data format to be contrasted to tabular format data.

The phrase automatically without user intervention if used in a claim is defined to mean that the particular step occurs after the step is initiated until limitations recited in the step are finished without requiring a user to provide input to a processor.

The detailed descriptions which appear herein may be presented in terms of program procedures executed on a computer or a network of computers. These procedural descriptions and representations herein are the means used by those skilled in the art to most effectively convey the substance of their work to others skilled in the art.

Further an embodiment has been discussed in certain examples as if it is made available by a provider to a single customer with a single site. An embodiment may be used by numerous users if preferred and the users can be at one or more sites.

A computer readable storage medium is tangible and non transitory a computer readable storage medium can be any of the memory or disks such as those examples described herein or other removable or fixed storage medium.

One or more displays for the system may be developed in connection with HTML display format. Although HTML may be the preferred display format it is possible to utilize alternative display formats for interacting with a user and obtaining user instructions.

The system used in connection herewith may rely on the integration of various components including as appropriate and or if desired hardware and software servers applications software database engines server area networks firewall and SSL security production back up systems and or applications interface software. The configuration may be preferably network based and optionally utilizes the Internet as an exemplary interface with the user for information delivery.

The various databases may be in for example a relational database format but other standard data formats may also be used. Windows 2007 for example may be used but other standard operating systems may also be used. Optionally the various databases include a conversion system capable of receiving data in various standard formats.

The detailed description includes many specific details. The inclusion of such detail is for the purpose of illustration only and should not be understood to limit the invention. In addition features in one embodiment may be combined with features in other embodiments of the invention. Various changes may be made without departing from the scope of the invention as defined in the following claims.

A procedure is generally conceived to be a self consistent sequence of steps leading to a desired result. These steps are those requiring physical manipulations of physical quantities. Usually though not necessarily these quantities take the form of electrical or magnetic signals capable of being stored on non transitory computer readable media transferred combined compared and otherwise manipulated. It proves convenient at times principally for reasons of common usage to refer to these signals as bits values elements symbols characters terms numbers or the like. It should be noted however that all of these and similar terms are to be associated with the appropriate physical quantities and are merely convenient labels applied to these quantities.

Further the manipulations performed are often referred to in terms such as adding or comparing which are commonly associated with mental operations performed by a human operator. While the discussion herein may contemplate the use of an operator a human operator is not necessary or desirable in most cases to perform the actual functions described herein the operations are machine operations.

Various computers or computer systems may be programmed with programs written in accordance with the teachings herein or it may prove more convenient to construct a more specialized apparatus to perform the required method steps. The required structure for a variety of these machines will be apparent from the description given herein.

Terms as used herein are intended to be interpreted as understood to one of skill in the art of computer languages which process data and secondarily if not thus interpretable then in the art of data processing and or computer science instead of as interpreted by a more general dictionary.

Furthermore the networks of interest for communicating between computers onto which some embodiments may be distributed include by way of example but not limitation data and or packet communications networks which can provide wireless communications capability and or utilize wireline connections such as cable and or a connector or similar. Any appropriate communication protocol may be used.

This disclosure is intended to explain how to fashion and use various embodiments in accordance with the invention rather than to limit the true intended and fair scope and spirit thereof. The invention is defined solely by the appended claims as they may be amended during the pendency of this application for patent and all equivalents thereof. The foregoing description is not intended to be exhaustive or to limit the invention to the precise form disclosed. Modifications or variations are possible in light of the above teachings. The embodiment s was chosen and described to provide the best illustration of the principles of the invention and its practical application and to enable one of ordinary skill in the art to utilize the invention in various embodiments and with various modifications as are suited to the particular use contemplated. All such modifications and variations are within the scope of the invention as determined by the appended claims as may be amended during the pendency of this application for patent and all equivalents thereof when interpreted in accordance with the breadth to which they are fairly legally and equitably entitled.

This section reproduces a guide to RAQL which is the convenient example of a query language discussed herein. This guide was included as part of the provisional applications mentioned in the Related Applications section. This section is included for the sake of completeness. This section should not be interpreted to limit the embodiments to RAQL.

Presto Analytics provides a simple method to work with big data along with existing real time historical and transactional data to produce useful insights and intelligence delivered in visual apps that go anywhere your users need. You get the power and flexibility of mashups a new query language to easily support your analytic needs and the support for performant access to data using streaming and memory management for large datasets. Try Getting Started with Presto Analytics.

Business intelligence daily seems to encompass more data from very different sources social media for your customers increasing volumes of real time data for monitoring operations or managing risk plus all the traditional historical and transactional data of your organization. Managing this fire hose of data and making sense of it requires 

Presto Analytics leverages many of the existing features in Presto to do this and adds features to let you solve this problem 

Presto Analytics In Memory Store to handle big data with good performance using streaming access and BigMemory from Terracotta as an add on to Presto to provide straight forward extensible fast in memory data management.

BigMemory can use heap and off heap memory either in the local host for Presto in a separate server and host or in an array of distributed servers. Server arrays provide an easily extended architecture while off heap memory can provide better performance with no Java garbage collection to cause bottle necks. Streaming large datasets also removes other performance bottlenecks such as keeping the entire dataset in Presto memory with a DOM to support XPath queries. Data is streamed in chunks partitions instead.

The Real Time Analytics Query Language RAQL and Analytics Engine to query large datasets in a simple but powerful way without the overhead of a DOM and easily apply analytic functions. Datasets use a flat table like structure to support queries. RAQL pronounced is a SQL like query language that provides performant access and handles streaming datasets. It includes a set of built in analytics functions and a straight forward way to provide your own user defined functions to meet new or unique analysis needs.

You use RAQL queries within mashups thus leveraging the power and flexibility of mashups. Presto Analytics has extended EMML the language for mashups to provide

streaming access to common sources for datasets as well as store datasets in or load dataset from the In Memory Store.

Mashup Tools and the RAQL Explorer use the RAQL Explorer to easily explore the features of RAQL syntax and the datasets you need to work with. You can also create RAQL queries for mashups in Wires using simple graphic modelling and drag and drop blocks. Or use the full power of EMML RAQL in the Mashup Editor.

Once you have the mashup and RAQL queries you need to work with a dataset simply add views and create apps or workspace apps using Presto s point and click wizards. New views have been added to Presto in this release as well as new capabilities for existing views covering visualization needs in more depth. Or developers can create pluggable views and add them to Presto to meet your specific needs.

Datasets you have already loaded in the Presto Analytics In Memory Store Users work with the final visual analytics that are published as apps or workspace apps. These apps are based on the mashups with RAQL queries and the views you choose to include in the app.

When users find and use these apps the associated mashup is processed in Mashup Server using the EMML Engine the Analytics Engine and any analytics functions defined in RAQL queries. They may work with datasets in the Presto Analytics In Memory Store or directly access datasets from any supported data source.

For your next steps see Getting Started with Presto Analytics RAQL Queries and Working with the Presto Analytics In Memory Store.

This topic presents basic examples to help you get comfortable with the features of Presto Analytics and the Real Time Analytics Query Language RAQL . Additional query techniques for RAQL are discussed in RAQL Queries and Working with the Presto Analytics In Memory Store.

Many of the example datasets used in this topic or other topics illustrating RAQL are available as either 

In a few cases such as examples for snapshots you must provide some initial configuration or perform some steps in Presto Hub to make the datasets used in the example available.

The example datasets used in this topic do not necessarily represent actual load or throughput requirements. Base memory settings for Presto may require tuning to provide adequate performance for actual loads. For more information see Working with the Presto Analytics In Memory Store. First let s take a look at a simple RAQL query.

You use mashups in Presto to access and work with small or large datasets. To work with large datasets mashups use EMML extension statements specifically designed for Presto Analytics and the RAQL query language.

Since data in large datasets may come from many different sources the data must be in a format and structure that is supported by RAQL. Valid data formats include 

The structure of the dataset must also be flat like a database table containing two or more rows records . Each row must contain at least one column with simple data. Each column must have a unique name.

JDBC result sets from database queries are always in the correct structure. For CSV and XML however this imposes some specific restrictions. See Supported Data Formats for RAQL for details.

Once you have the dataset loaded you can use RAQL queries to analyze and manipulate the data. RAQL is very similar to the Structured Query Language SQL used to access data in a database. It is 

We will explore a simple use of each query clause in this topic. See RAQL Queries for a synopsis of the valid expressions for each of these clauses along with links to other examples.

The Select clause determines which columns to include in the result and can also perform analysis when it is used with either the Over clause or the Group By clause. The From clause determines which dataset to query or can define a subquery to use as the source of data.

The Over and Group By clauses both group dataset rows into different sets based on an expression. These groups determine the scope of rows that are used in analytic functions in the Select clause.

Over and Group By are mutually exclusive as they have different affects on the data returned by the query. The Over clause performs calculations and adds the calculations as additional columns to each row. Group By instead performs calculations and returns just the calculations for each group.

Most RAQL clauses also support the use of functions within their expressions. RAQL functions come in two varieties 

Aggregate analytic functions use all rows in the current scope such as sum . While window analytic functions use specific rows such as rownumber . These functions include simple arithmetic as well as statistical functions machine learning functions or other analysis algorithms.

RAQL provides a set of built in functions plain and analytical as well as a way for you to define your own functions. See Built In RAQL Functions for more information. Let s try another query using CSV data and some conditions to filter rows.

Accessing files can be useful but in most cases the datasets you want to work with come from databases or from other systems or applications. If applications provide a REST or Web Service interface you can access and load data using and the appropriate URL.

We re going to load a CSV dataset with information on global manufacturing plants that is accessible from http raw.github.com jackbe raql master data mfgplants.csv. Then we will use the Where clause to filter the rows the mashup should work with.

This is the RAQL Explorer that you can use to explore RAQL queries. You can use this tool to play with results and queries when your dataset is accessible as a file or from a URL. See Explore RAQL with the Presto RAQL Explorer for more information about this tool.

As the results show this contains a list of manufacturing plants by country and name along with latitude longitude information and statistics on the production lines at each site.

You can apply plain functions to individual columns in a dataset stream in any clause in a RAQL query. Plain functions can update data in each row in the Select clause help to filter rows in Where conditions or help to sort results in Order By. They can also be used along with analytic functions in Over or Group By clauses or in subqueries in From clauses.

Presto includes a set of built in plain functions. See Built In RAQL Functions for details. You may also have additional plain user defined functions available. The first example uses the built in plain function split part in the Select clause to split longitude and latitude data into two columns for the Manufacturing Plants dataset shown previously in Load Data with and Filter Rows.

The Select clause uses the split part function on the Location column to split the data into two separate pieces before and after a comma delimiter. The first call extracts the latitude and the second call extracts the longitude. In each case the result of the function is added to the query results as a separate column using as alias to provide the name of the new column. The next example uses the decimal casting function in a Where clause to ensure that the Active Production Lines column is treated as a number for filtering. Casting functions simply cast the data in the column to an appropriate datatype.

The last example uses the sample dataset for US legislators previously introduced in A Basic RAQL Query. It sorts legislators by state and district using the decimalcasting function to ensure that their district is sorted numerically 

The plain functions used to cast the datatype of a column in the previous section point out one issue to be aware of for RAQL queries in many cases the RAQL Engine that runs the query has no datatype information for a given column. This is particularly true for data in CSV or XML formats. When datatype information is not available the RAQL Engine considers column data to be untyped. For most purposes this means that the data is treated as a string unless you explicitly cast it to another datatype.

Instead of using casting functions you can provide datatype information for datasets in your mashup. See Providing Dataset Path and Datatype Information in a Schema for more information.

The examples so far have used an EMML statement to load the data as a stream used RAQL extension statements to query the stream and then returned the query results as the mashup results. Currently however mashups cannot return streams as their results. As the following figure shows streams must be converted to a document before being returned from the mashup.

You must also convert query result streams to documents to use EMML statements that are not RAQL extensions with query results. Any EMML statement that uses XPath expressions requires that the stream first be converted to a document.

Because of the volume of data it is a good practice to avoid using the statement with RAQL query results.

The statements used in earlier examples implicitly converted the query results to a document type variable by using result as the output variable and not setting the streaming mode. You can have query results returned in a stream when needed by setting the stream attribute on the RAQL query. For example 

In general if a mashup statement or the receiving variable does not set stream true then the dataset will be treated as a document not a stream.

So far the examples have actually not had a large set of data that would overload the memory available to Presto. The mashups have loaded all the data directly into the mashup and worked with it there without using Terracotta BigMemory or the Presto Analytics In Memory Store.

The previous figure also illustrates the basic flow when you do need an in memory store to handle large amounts of data 

Mashups that store datasets in the Presto Analytics In Memory Store use the EMML extension statement. Datasets are streamed to the in memory store and stored with a unique key that other mashups can use to load this dataset. The dataset to store must also be the results of a RAQL query although the query can simply select the entire dataset.

The following example mashup retrieves performance data for stocks from a URL using . It uses a RAQL query to package all the data for storage with and includes stream true to treat the query results as a stream.

The mashup finally stores the data selected in the query in the default in memory store under the key stocks2011. This key is a simple string but dataset keys can have different scopes or use other techniques to ensure they are unique. See Set Unique Keys for Datasets for more information.

This example did not filter or adjust the dataset in any way before storing it. But you can also use a RAQL query to preprocess the data you want to store in the Presto Analytics In Memory Store. If you have multiple in memory stores defined you can also identify the store by name.

Once you store a dataset other mashups can use the EMML extension statement to load this dataset stream for queries and other processing. The following example shows a mashup to retrieve this stock dataset and return just the first 10 rows 

This stored dataset shown below will be used in other examples in Getting Started to discuss Group By and Over query clauses for grouping and analysis 

To analyze the data in a dataset stream you can use the Group By clause or the Over clause. Group By as in SQL categorizes rows into sets based on the unique values of one or more columns. The analysis then is performed on each group defined by the analytic function s that are used in the query s Select clause. In this example we group the stocks dataset that was stored in the Presto Analytics In Memory Store in the previous section and determine the highest price for each stock symbol in each year. This mashup uses to retrieve the stock dataset stream from the in memory store and then issues the RAQL query.

The Group By clause uses a list of column expressions to determine how rows are grouped. This can be as simple as one column although it is quite common to group by two or more. Unique values from the combination of columns then determine which group a given row belongs to.

This query uses the plain function year to extract the year for each row from the date and the max analytic function in the Select clause to discover the highest price for all rows in each group. Because Group By returns a single row for each group you must use aggregate analytic functions which perform calculations for all values in the current scope group in this case and return a single value.

RAQL has a set of built in analytic functions that you can use in Group By clauses or you can write and add your own analytical functions. See Built In RAQL Functions for more information.

The results of this query using Group By shown here contain one row for each symbol year combination 

The other RAQL query clause that you can use to perform analysis is the Over clause. Like Group By the Over clause segments the rows of the dataset into different groups known as partitions. The primary differences between Group By and Over are 

All rows of the dataset that meet the conditions of the Where clause if any are returned from a query with an Over clause rather than just one row per group.

You can also define windows within a partition. A window consists of the current row within a partition and the number of preceding and following rows you define.

Analytic functions are applied to either the full partition or to each window within the partition. The results of the analysis is added as a new column to either each row in the partition or to the current row for each window.

The results of analytic functions can also be running calculations such as running totals including the current row and all preceding rows.

Let s look at a simple partition example. The following mashup loads the stock dataset from the Presto Analytics In Memory Store stored earlier in Getting Started .

The Select clause selects each column that will be used in the calculation or in the definition of the partition symbol openand close .

Lastly Select uses the built in correlation analytic function to determine if there is a linear correlation between opening and closing prices for each symbol. The Over clause defines the partitions that this analytic function is applied to.

The Where clause filters the rows that are included in each partition to specific stock symbols based on a name pattern. With this dataset this limits the results to the symbols DISH and NFLX.

The results of this query shown below include each row for the selected symbols and a new column coefficient that includes the result of this analysis function 

RAQL has a set of built in analytic functions that you can use in Over clauses or you can write and add your own analytic functions. See Built In RAQL Functions for more information.

This finishes the basic examples of RAQL queries and how to work with large datasets using Presto Analytics. For examples of other query techniques or advanced capabilities such as dynamic queries see RAQL Queries and Working with the Presto Analytics In Memory Store.

For more information on the extensions that RAQL adds to EMML see RAQL Extension to EMML Statements for Mashups.

The basics of using RAQL to query and analyze large datasets is discussed in Getting Started with Presto Analytics. For specific techniques on loading large datasets or using specific RAQL query clauses see the links in these sections 

See also Escape Characters for RAQL Queries RAQL Operators RAQL Datatypes and Data Formats Built In RAQL Functions and Create and Add User Defined Functions for RAQL Queries.

The RAQL Explorer lets you easily load large datasets from files or URLs and then play with different RAQL queries. You can save mashups based on queries you are happy with or open and update existing queries. See how to 

Once you have useful results you can save a query as a mashup. Simply click Save As and enter a name for the mashup. The new mashup appears in the list of queries. You can also find the mashup in Presto Hub search results see it in Mashboard Wires the Mashup Editor or from other common links in Presto Hub.

Click a query from the query list to open it. Then click the Query Name to open the mashup s artifact page where you can 

You enter RAQL queries as the value of the queryattribute on the element in a mashup. Because the query is in an XML file the mashup you must use the escaped form of the following XML delimiters when they appear in a RAQL query 

You can also load a dataset directly from a database using the statement in EMML and then use RAQL to perform analysis. To query a dataset from a database you must 

If you do not already have a datasource defined in Presto for the database containing the data you want to work with have your Presto administrator add one. See Add a Data Source for instructions.

The example shown in this topic is from a MySQL database with machine sensor data. Every half second various readings are added to this database for different devices. The dataset shown below in XML format includes the value data value for a reading a code data item id for the type of reading a code for the device device id and both a timestamp and a date time in milliseconds when the reading was taken.

Once you have an available datasource to connect to the database use the statement in a mashup to query the database for the dataset and set streaming on with the streamattribute. The name of the datasource is set in the name attribute.

The following example queries for sensor data for a single type of reading across all devices. This also sets a fetchsize that is used to stream the dataset to Presto.

Use to further query and analyze the dataset just as you would with XML or CSV datasets. The results of the database query are already in a flat table structure that matches the RAQL data model so no additional path information is needed.

You can load datasets for analysis with RAQL from any Presto mashable or mashup using . Mashable or mashup results must be a document.

The following example loads results from the Yahoo local search mashable one of the sample mashable you may register when you install Presto 

Streaming is not set for either the searchResults variable that holds the results of invoking Yahoo local search or for the statement itself RAQL executes the query even though the result dataset is not streamed. To help clarify the exact XML elements in the mashable results that are considered a row for the RAQL query the From clause uses both the variable name for the dataset and the full path to the element that is a row. Paths are sometimes useful to clarify rows when datasets are XML. See Dataset Paths Names and Datatypes for more information.

All of the direct children of in the Yahoo local search results are accessible as columns in the RAQL query. Currently however the following types of XML content are not accessible 

This uses a local variable named snapshots to hold the dataset stream which is only in scope for this RAQL query. See Load Snapshots Anonymously for an example.

Queries to retrieve snapshots are SQL queries that require a Select From and Where clause to define which snapshots to include in the dataset. You identify snapshots in the Where clause by one of these conditions 

This query creates a variable named coffee to stream the selected snapshots and then use in further RAQL queries. The snapshot query retrieves all snapshots for a single mashable the sample Yahoo local search mashable identified by ID 

Once the dataset has been loaded the RAQL query can act on the dataset or it can be stored in the Presto Analytics In Memory Store.

You can also load snapshots as a dataset into a local anonymous variable with the snapshotquery attribute in . This example is identical to the named example shown previously except that the query to load snapshots in a dataset stream is specified on .

Add the distinct keyword in Select clauses to retrieve only one row for each distinct value of a column. For example 

You can change column names or supply column names for calculations using as in the Select clause. As example of changing a column name is shown here 

Examples of queries using as to provide column names for calculations are shown in Use Plain Functions to Update Select or Sort Rows Group and Analyze Rows and Group and Analyze Rows with Row Detail.

The alias name for a column is defined in either the Select or Over clause. In general you cannot use this alias to refer to the column in other RAQL clauses. This RAQL query for example will result in an error 

You can duplicate the original column such as this example select Id Descr decimal Qty as Quantity from items wheredecimal Qty 5

This can be cumbersome or in some cases impossible where the syntax is complex. The solution is to define the alias in a subquery so that you can refer to it in an outer query. For an example of this syntax see From Subqueries.

Columns in XML datasets can sometimes contain HTML markup such as tags. This is quite common in data from RSS or Atom web feeds. These HTML tags in column data can cause information to be missing from RAQLqueries or cause other problems because the tags are incorrectly interpreted. To overcome these errors you can have RAQL escape the column content with HTML tags using CDATA sections by using a column alias in the following form original column name as original column name cdata

With a column named title for example the alias name to escape any HTML markup in the column content would be title cdata.

Plain functions can be used in any RAQL query clause. You may use any Presto built in plain function see Built In RAQL Functions for a list or user defined plain functions that you or other Presto developers have added.

Simple examples of using plain functions in Select clauses are shown in Use Plain Functions to Update Select or Sort Rows. Plain functions can also be nested such as this example 

When working with XML or CSV datasets there are three potentially troublesome areas that you can improve with specific techniques 

The data model for XML datasets is frequently hierarchical including additional metadata beyond the flat rows of interest to RAQL and adding additional layers of structure.

To simplify queries RAQL automatically attempts to detect which elements in an XML dataset should be considered rows. This allows you to refer to rows in the dataset in RAQL queries using only the name of the variable containing the dataset such as 

In some cases this default may not be the dataset elements you actually need to work with or query results may be incomplete. You can override this default by Adding Paths to Clarify RAQL Row Detection or Providing Dataset Path and Datatype Information in a Schema.

In some cases you may also need to alter column names to make them valid for RAQL or for EMML. See Valid Names for Columns and Paths for information.

With XML or CSV data RAQL has no metadata about the datatypes for each column so the data is treated as a string. You can fill this gap to simplify the need for casting functions by Providing Dataset Path and Datatype Information in a Schema.

If you need to override the default XML elements in a dataset that RAQL treats as rows you can add a specific path to the elements you need to the variable in the From clause. For this example 

The path uses XPath syntax starting with a slash and separating each level of elements within the hierarchy with a slash. See Load Data with for an example of a From clause using paths.

See also Providing Dataset Path and Datatype Information in a Schema for an alternative way to set path information.

Both RAQL and EMML have the following rules for valid column names and the names within paths to dataset rows 

The names in paths used with XML datasets must follow XML name rules. They can contain letters numbers dashes and underscores   .

RAQL automatically fixes some of these problems. With CSV datasets for example RAQL will replace spaces with an underscore   in column names or change numeric column names to column original number.

Schemas provide three types of metadata to simplify or improve how RAQL queries interacts with XML or CSV datasets 

The scope of this schema is based on where you define it within a single mashup or as a global Presto attribute that can be used in any number of mashups. See Dataset Schemas Defined in Mashups and Global Dataset Schemas as Presto Attributes for information.

Schemas for datasets define a variable name for the schema a set of columns with datatype and optional format information and an optional set of options for a dataset. The schema syntax is in the form 

define dataset variable name column name datatype format column name datatype format . . . with options option name value option name value . . . 

define dataset stocks symbol string date datetime yyyy MM dd open decimal close decimal high decimal low decimal volume decimal with options record stocks stock 

The format metadata for date or time type columns accepts any lexical pattern that is valid for the Java SimpleDate class. For the most common patterns you can use see the Date Formatter function for the Transformer block in Wires.

record path to row identifies the elements within the dataset starting from the root that should be used as rows. This uses the same syntax as paths you specify in a From clause excluding the variable name.

You can declare a dataset schema in the mashup that loads a dataset using the EMML statement and a type of schema. For example 

The variable named stockType defines a schema for the stock dataset introduced in Use the Presto Analytics In Memory Store to Store and Load Datasets in Getting Started. This variable is then referenced in the variable named stocks using a type of variable stockType that will hold the dataset once it is loaded. The type identifies the named variable containing the schema for this dataset.

The primary advantage of having the dataset defined is that RAQL queries now know datatypes so that filter conditions in Where sorting criteria in Order By and functions or calculations in Over or Group By clauses work seamlessly without having to cast columns to the right datatype.

Sorting is defined on a numeric column but because no datatype information is available from the original XML source the sort order in the result is wrong. But run the same query with schema information now available and the results are now sorted correctly 

If a dataset will be used in many RAQL queries you can define a schema for the dataset as a Presto global attribute that can be easily used in different mashups.

Presto administrators can create global attributes in the Admin Console. See Managing Presto Global Attributes for instructions. For dataset schemas the value of the Presto global attribute is the full definition of the schema. In the following example 

The attribute name is yahooSearchSchema and the full definition of the schema is a single string as the attribute value.

Once you have the dataset schema defined as Presto global attribute you can use it in a mashup in a the statement with a name in the form global.attribute name and a type of schema. This allows the mashup to use the global attribute to supply the schema definition. The following example retrieves the schema defined above from the Presto global attribute named yahooSearchSchema 

Then add the variable to hold the dataset and reference the schema variable using a typeof variable global.attribute name that will hold the dataset once it is loaded. The typeidentifies the named variable containing the schema for this dataset In this example the variable searchResults has a type that pulls in the global.yahooSearchSchema global attribute containing the schema definition.

In this example query when no schema is used the mashup shows results of a single row even though the query to Yahoo Local Search asked for up to 20 results 

When the schema is added supplying specific path information to rows in Yahoo s results the query now retrieves all 20 results 

You can use subqueries in RAQL in the From clause only. The following example has two levels of subqueries based on the stocks dataset introduced in Use the Presto Analytics In Memory Store to Store and Load Datasets in Getting Started.

The innermost query groups stocks data by year and quarter and calculates the average volume for each group. These results are used in the middle subquery to retrieve the previous quarter s average volume for each row and then calculate the percentage of change using a simple math expression.

Also of interest in the intermediate query is the use of the lag analytical function to add the previous quarter value to each row. The nvl plain function in the equation for the percentage of change handles the first row where the previous quarter value is null.

The final outer query then filters the results to include only those rows where the percentage of change is greater than 15 .

The example shown above also illustrates the use of subqueries to allow the use of aliases in the Where clause. The intermediate query defines prev qtr as an alias. This alias is then used within that Select clause for the very next column pct change. This works because the alias reference is in the Select clause.

However the use of pct change in the Where clause of the outer query works only because pct change exists from the subquery. If the query attempted to use this in Where in the intermediate query it would fail with an error.

You can use the limit keyword at the end of RAQL queries to limit the number of rows to return. One common use is to do an initial query that retrieves all columns but a small number of rows to get a look that the dataset you are working with. For example 

Where clauses can use logical operators comparison operators or arithmetic operators combined with parenthese to define complex conditions for queries. See RAQL Operators for a complete list of valid operators for Where clauses.

Use the or logical operator to define conditions where multiple matches are allowed. You can also combine and or to define more complex conditions. Use parenthese to indicate the appropriate precedence. This example selects senators from Texas New York or California 

You can use the common math comparison operators to compare numbers or dates. In many cases you also need to cast column data to an appropriate datatype for the comparison to be successful.

You can perform common arithmetic operations within the Where clause. Use parenthese to resolve any precedence issues with math operators.

This example selects plants based on the sum of currently active plants and plants under construction 

This example selects legislators whose birthdate is Jan. 1 1960 or later and whose first name is not John In many cases however the not logical operator is more flexible. This example query finds all senators whose last name does not begin with A or B 

You can define filter conditions in the Where clause for string columns based on a simple matching pattern using the like pattern keyword. The Like pattern uses the symbol as a wildcard to represent zero to any number of characters.

You can find an example of Like with a pattern using wildcards at the end in Group and Analyze Rows with Row Detail. This selects rows based on the stock symbol starting with either D or N .

You can define filter conditions for string columns to match rows based on an enumerated set of values using the in value1 value2 . . . keyword and set definition.

Parameters in Where Clauses RAQL does not currently support the parm name syntax in Where clauses to supply values for filter conditions from named parameters. You can however make RAQL queries dynamic by 

See Creating Dynamic RAQL Queries for an example and more information. This example shows a simple Where clause where the threshold for a filter is supplied by an input parameter to the mashup 

Plain functions can be used in any RAQL query clause. You may use any Presto built in plain function see Built In RAQL Functions for a list or user defined plain functions that you or other Presto developers have added.

You can find a simple example of using plain functions in a Where clause in Use Plain Functions to Update Select or Sort Rows. See also Comparing Dates or Numbers for an example of using casting functions in Where clauses.

This example query uses nested functions to select manufacturing plants that are in the southern hemisphere by comparing the latitude in the data 

By default rows included in query results are sorted in ascending order based on the column expressions in the Order By clause. You can change the sort order for specific columns by using the desc or asc keywords in the column expression.

For string values sorting depends on both the character set of the text and the sort collation defined for that character set. For example many European languages have upper case and lower case letters which are sorted separately. Many Asian languages use ideographs where this concept does not apply for sorting.

Both the character sets that RAQL can work with and the collations used for string sorting are defined by the version of Java used by the application server that Presto is hosted in.

Multiple levels of sorting are defined simply by the columns included in the Order By column expression.

A simple two column sort is shown in Group and Analyze Rows in Getting Started. There is no specific limit for sorting levels however. This example shows a three level sort 

Plain functions can be used in any RAQL query clause. You may use any Presto built in plain function see Built In RAQL Functions for a list or user defined plain functions that you or other Presto developers have added.

The Group and Analyze Rows with Row Detail example in Getting Started introduced a simple partition using the Over clause to perform some analysis and add these results to each row in the dataset. This topic includes examples of 

The column expression you use in a partition definition for Over clauses can use a list of multiple columns to define multi level partitions just like column expressions for Group By clauses. The following example segments US legislators into partitions for each chamber of Congress state and political party to determine the number of legislators for each party in each state and legislative chamber 

Windows define subsets of rows within a partition that are relative to the current row based on row position.

This example is centered where the number of preceding and following rows is equal. Windows can be asymmetric using different numbers of preceding and following rows or omitting either. Analytic functions are applied to just those rows within the window based on the current row and the result is added to the current row.

Windows are useful for time based datasets although they are not limited to this. With time based datasets each row represents a different slice of data for a specific time. The following example uses windows with a time based dataset to calculate moving averages 

The moving average is calculated over the rows in the window relative to the current row so each row potentially has a different moving average. Moving averages typically show a smoother trend for the column.

With window analytic functions such as lag or firstvalue you can select values for specific siblings for each row using window definitions. These functions return the column value for a sibling as shown in this example 

The lag function selects the column value of the preceding sibling while firstvalue selects the column value of the first preceding sibling based on the partition or window definition. In this example Prev is added to each row using lag to get the last sensor reading for the current row. First is also added to each row and gets the value 5 seconds previous based on the window size 10 preceding rows and readings every half second.

You can get cumulative calculations also known as running totals using aggregate analytic functions over partitions or windows defined in Over clauses. Normally aggregate analytic functions return a single value for all rows in a partition or window such as the Multi Level Partitions example. If you add an Order By clause within the partition or window definition however aggregate functions return cumulative values based on the current row and all previous rows in the partition or window.

The following example uses the sum aggregate analytic function to provide a running total of volumes for the stocks dataset introduced in Use the Presto Analytics In Memory Store to Store and Load Datasets in Getting Started. The dataset is segmented into partitions by symbol plus the year in the date column and then ordered by the same column expression.

The runningTotal column is added to each row and calculated as a running sum for each symbol year combination.

You can use any of these built in Presto analytic window functions to rank or number rows in partitions 

To support numbering you must include an Order By clause within the partition definition. The first column in the internal Order By clause is used to determine row or ranking order.

This example uses all of these built in numbering functions with the same partition legislators within each chamber of the US congress and sort order ordered by state to illustrate the different effects of each function 

Analytic functions are used in Select clauses to perform analysis for partitions or windows defined in Over clauses. You can use either aggregate analytic or window analytic functions. See Built In Analytic Functions Aggregate and Window for a list of the Presto built in analytic functions available to you. User defined analytic functions may also be available.

At its simplest dynamic queries use input parameters to provide the values used in Where clause conditions using dynamic mashup expressions. See Parameters in Where Clauses for an example.

But sometimes you need more flexibility to make other clauses be dynamic. For these more demanding cases you can build entire RAQL queries using the EMML statement the concat XPath function and other EMML statements.

The following example builds the entire RAQL query based on an input parameter that chooses the time period to use for grouping stock volumes from the example dataset introduced in Getting Started.

This example uses that parameter to determine both the functions and aliases for fields to use in the Select clause as well as functions used in the Group By and Order By clauses. To do this the mashup 

Depending on the value of the queryScope parameter the actual RAQL query that this mashup uses is either 

select symbol year date as yr quarter date as qtr decimal mean volume as this qtr from stocksgroup by year date quarter date order by year date quarter date 

decimal mean volume as this month from stocksgroup by year date month date order by year date month date 

JDBC results sets can contain columns with complex data such as BLOB CLOB or IMAGE and these columns can be included in the query result. However the query itself cannot access complex data to perform comparisons or calculations.

The division operator uses integer division unless the datatype of the data is decimal. You can set the datatype using casting functions or by additing datatype information in a schema. See Built In Plain Functions and Providing Dataset Path and Datatype Information in a Schema for more information.

 or like for matching text based on a pattern. Use to indicate zero to any number of characters. For example where lastname like A to find any last name starting with a capital A. in to match values against an enumerated set of values defined within brackets. For example where direction in N NW W 

Operators are not case sensitive. You can use parentheses to build complex logical expressions such as where service level in gold silver and rating or overdue 

Presto provides both Built In Plain Functions and Built In Analytic Functions Aggregate and Window that you may use in RAQL queries. You can also define and add your own plain or analytic functions to RAQL. See Create and Add User Defined Functions for RAQL Queries for instructions.

Plain functions can be used in Select Over Where Order By and Group By clauses in RAQL queries. They typically either cast change the datatype of column values extract part of the values or transform values in some way.

Plain functions are applied individually to each value in the column specified without access to values in other rows.

Analytic functions most commonly perform calculations using sets of rows within a dataset. This may be the entire dataset or specific sets of rows defined as groups partitions or windows.

The latest Java Development Kit 6. See JDK1.6 to download and install this if needed. A folder for your user defined function library with this structure 

User defined functions are packaged and deployed as named libraries in Presto. The library name also uniquely defines your user defined functions from built in functions or user defined functions in other libraries that have been deployed in Presto. Library names must match the name of the library folder containing your source code.

Valid library names must be unique for a Mashup Server. They can contain letters numbers dashes or underscores   .

The name analytics is reserved for the Presto UDF function library. See External UDF Library Deployment Folder for more information.

You can organize user defined functions however you need. User defined functions can be packaged in multiple libraries. Each library can contain one or more Java packages. Each package can contain multiple classes. For plain functions each class can contain multiple functions. For analytic functions each function is packaged as a class.

The RaqlFunc annotation identifies which methods should be associated with RAQL functions. If you omit the name parameter the name of the method becomes the name of the RAQL function within this UDF library. Use name alias to use a different name for the function from the method name. Using annotations to configure the methods for user define functions in RAQL is a best practice.

You can however skip the annotations in your Java classes and instead provide configuration that maps methods to user defined functions in the lib.jsonconfiguration file for your UDF libraries. See External UDF Library Deployment Folder for more information.

This example identifies three packages for this library. The classes property must be present in configuration but can be an empty array like this example. The RAQL Engine will search each class within the identified packages for function annotations to find the functions to add.

You can also optionally identify classes functions and methods in this configuration. See UDF Library Configuration for more information.

Compile your Java class with user defined functions being sure to include the raqljar file and any third party libraries you used in this class in the classpath. For example 

 JAVA HOME bin javac classpathc Presto3.6 appserver apache tomcat7.0.27 webapps presto WEB INF lib raql.jar d classessrc com MyOrg raqlUdf MyStringFuncsjava

javac classpath Applications Presto3.6 appserver apache tomcat7.0.27 webapps presto WEB INF lib raql.jar d classessrc com MyOrg raqlUdf MyStringFuncs.java

This will add the compiled class to the classes folder in your development folder for this library. Deploy the classes and any third party libraries for this library to the Mashup Server. Copy the following folders to the root deployment folder for your library 

Restart the Mashup Server. See Starting and Stopping the Presto Mashup Server for instructions. Use the RAQL Explorer or write mashups that use these new user defined functions to test them. You refer to user defined functions in the form library name.functionname arg . . . to identify both the library name and the function name. For example 

The lib.json file contains configuration that identifies the Java packages with user defined functions for RAQL. It can also optionally contain configuration that identifies the specific classes and methods in these packages and the function names to map to methods.

The following example includes method and function mapping configuration along with the required package configuration information 

A default external UDF Library Deployment folder is created when you install Presto at prestoinstall raql udfs where you can deploy all your user define functions. The Presto analytics library with built in Presto functions is also automatically deployed in this folder.

In clustered environments you may want to create a shared external folder for Presto configuration and move all user defined functions including the Presto built in function library to this shared location for all members of the cluster. The structure would look something like this 

If you move user defined functions from the default UDF Library Deployment folder you must also update an environmental variable for each Mashup Server 

With window analytic functions each function is a single class that extends the org.raql.funcs.WindowAnalyticFunctionbase class in the Presto RAQL User Defined Function API. This API uses a map reduce paradigm to handle large datasets.

Window analytic functions unlike plain functions have access to all rows or tuples within the current partition or window which they can use to perform calculations. Unlike aggregate analytic functions however they provide a different calculation for each tuple.

initialize FunctionContext fnctxt is optional. The FunctionContext stores the result for the function and any interim state needed. Use this method to initialize the result interim state or to validate function arguments.

You set up your development environment for window analytic functions just the same as for plain functions. See Set Up Your Development Environment for details.

We re going to use two examples. The first example shows the basics of a window analytics function and how to use the function context to track state and set the function result. This example implements the built in analytics.discretize method.

And Configure Compile Deploy and Test User Defined Functions. For the complete code see Complete Discretize Example.

The second example implements the Presto built in lead function which illustrates techniques to Work with Specific Tuples in Window Calculations using the current position of a tuple.

This example is available in the sample user defined functions package at TBD. It implements the built in discretize Number column double min double max intbinCount function which assigns each tuple to one of a number of discrete bins based on their values in a column.

Your window analytic function class imports the classes in the Presto RAQL User Defined Function API and extends the org.raql.funcs.WindowAnalyticFunctionabstract base class 

This example overrides the default implementation for initializeto validate the function s argument. It uses the getArg method in FunctionContext to access these arguments. The FunctionContext object is also used to track the result and interim state for the function.

You implement the map method with the core logic for your window analytic function. This has three arguments the FunctionContext with the current state and results for the function the current Tuplein this partition or window and a List of all the Tuples in this partition or window.

The use of the setInterimValueand getInterimValue methods on the function context for interim state properties. These are Java properties of any type needed for the function.

The use of the setResult method on the function context to store the results for the function for the current tuple. This is the value that RAQL adds to the current row of the dataset.

This example is a window analytics function similar to the Presto built in leadfunction which returns the value for the specified column for a row a tuple that follows the current row by a specific offset 

To work with a specific tuple relative to the current tuple this function uses the getCurrentPos method in the function context to get the position of the current tuple within the partition or window. It uses the size method to get the number of tuples in the partition or window.

This example also uses the evalExpr method inherited from the BaseAnalyticFunctionabstract class to evaluate the column passed as an argument for the tuple relative to the current tuple specified by the offset. This base class contains several helper methods to evaluate expressions that can be useful for window analytics.

Aggregate analytic functions are a single class that extends the org.raql.funcs.AggregateAnalyticFunction base class in the Presto RAQL User Defined Function API. This API uses a map reduce paradigm to handle large datasets.

Like window analytic functions aggregate analytic functions have access to all rows or tuples in the current group partition or window. They perform an aggregate calculation that uses all tuples to return a single result. Depending on where they are used in RAQL queries this single result may be all the query returns or it may be returned as a column on every row.

initialize FunctionContext fnctxt is optional. The FunctionContext stores the result for the function and any interim state needed. Use this method to initialize the result interim state or to validate function arguments.

map FunctionContext fnctxt Tuple current List window to perform any intermediate calculations required and store them in the FunctionContext.

reduce FunctionContext fnctxt to perform the core logic for the aggregate calculation based on interim calculations performed in the map method.

You set up your development environment for aggregate analytic functions just the same as for plain functions. See Set Up Your Development Environment for details. Write the class for your aggregate analytic function and then Configure Compile Deploy and Test User Defined Functions.

We re going to use two examples. The first example shows the basics of an aggregate analytics function how to use the function context to track state and then perform the final calculation. My Average Aggregate Example is an implementation of the built in avg Number column method.

The second example Kurtosis Using a Third Party Library uses methods in the Apache Commons Math library to calculate the kurtosis for a column. This is an example of how to use the mapmethod to restructure column values and store them as an interim value in the function context to use then with statistics implementations from a third party library.

This example is available in the sample user defined function package at TBD. It implements an aggregate function myavg Number column similar to the Presto built in average function 

Kurtosis is a statistical measure of peakedness in the values for a dataset compared to a normal distribution. This indicates how closely the distribution matches the rounded bell shape of a normal distribution.

In this example we will use an implementation of kurtosis provided in the Apache Commons Math library version 2.2. The method to calculate kurtosis in the DescriptiveStatistics class in the Apache Library expects the values to use as the probability distribution to be primitive values in an array.

To support this the map method builds an array from the column values for tuples in a group partition or window. The reduce method then uses this array to perform the calculation. As always the function context object is used to hold state for both methods.

The map method uses the FunctionContext to get the values for the column passed to the function convert these values appropriately and add them to a ListArray stored in the result in FunctionContext.

map also uses convenience methods in TypeConv to cast values for the column to an appropriate numeric type. This method also throws a RuntimeExceptionfor casting errors which ensures the UDF function fails correctly in a RAQL query.

The reduce method gets the array with all values for the column for this group partition or window. It converts these values to primitive values in a simple array and uses methods in the DescriptiveStatistics class from Apache Commons Math to calculate the kurtosis. The final calculation for the group partition or window is then set as the FunctionContext result.

To compile this example you must include the Apache Commons Math library version 2.2 in the classpath. You may include the jar file for this library in the lib folder for the user defined function library. This specific library is also used in Presto so you also simply add the jar file for this library to the classpath. See Statistics and Analytics Third Party Libraries for information.

Presto Analytics includes the following third party libraries with many common statistics and machine learning algorithms 

User defined analytics functions can leverage these libraries directly. Simply include the associated jar file in the classpath along with the jar for RAQL when you compile your UDF classes.

This in memory store allows you to store large datasets for quick access or retrieve them for analysis using RAQL. Getting Started with Presto Analytics includes some very simple examples of how to do this.

You can also control the scope of key names append data to an existing key configure the partition size used for streaming to name just a few techniques when you store or load datasets. For more information see Store Data in Presto Analytics In Memory Store and Load Data from the Presto Analytics In Memory Store.

Installation for Presto optimizes memory configuration and configuration for the Presto Analytics In Memory Store based on 

In many cases this base configuration is sufficient to work with Presto in development environments with individual user computers. You may need to adjust this configuration to provide adequate performance with large datasets when you install Presto in staging or production environments or when you receive your full Presto license. See About BigMemory and the Presto Analytics In Memory Store for an overview and links to configuration tasks.

By default the Presto Analytics In Memory Store stores datasets in memory with no expiration so they remain in memory indefinitely. The store uses no persistence however so Mashup Server restarts clear all datasets.

The In Memory Store also does not overflow to disk if the datasets you store exceed available memory. If the memory allocated to BigMemory is full datasets are evicted based on least recent use to make room for new datasets.

BigMemory manages memory and data for both the Presto Analytics In Memory Store and all Presto caches. The initial configuration for memory when you install Presto may use only heap memory or it may also include off heap memory as shown in the following figure 

This is a high level summary of memory use and storage tiers available with BigMemory. For more detailed information see the Configuration Storage Tiers section in BigMemory documentation.

You can deploy one BigMemory server optionally with a mirror that is paired with one Mashup Server such as the following figure 

Presto uses heap memory from the local host as usual. Some local host memory is allocated for off heap which is combined with a much large allocation of off heap memory from the host for BigMemory.

The data for the Presto Analytics In Memory Store and Presto caches are distributed across both local and external off heap memory that is managed by BigMemory. And of course some memory is allocated for the operating system or other applications on both the Presto and BigMemory hosts.

You can also install BigMemory in a cluster to provide more memory capacity or support failover and other high availability features.

This is a high level summary of memory use and storage tiers available with BigMemory. For more detailed information see the Configuration Storage Tiers section in BigMemory documentation.

How you update memory allocations and optionally apply Presto and BigMemory licenses depends on how Presto and BigMemory are deployed 

When you receive your full Presto license you can move the BigMemory Add On to one or more separate hosts to provide additional memory or added reliability using mirrors to handle failover. You must also update both your Presto license and the BigMemory license for Presto.

For Presto edit the presto install prestoenv.bat file for Windows systems or the prestoinstall prestoenv.sh file for Linux OS X or UNIX systems. This property is defined in the BIGMEM OPTS environmental variable.

For BigMemory see Operations topics in BigMemory documentation for more information on how to update startup properties.

Remove the comment markers and change the url attribute to the host or IP address and port for the BigMemory server s you installed. For example 

Find the element with a name of RAQL DATA CACHE in ehcache.xml. Remove the comment markers around the line inside this cache with . This configuration should now look something like this 

This element allows the In Memory Store to use memory in both the local host and the BigMemory host. This combined memory is managed by the external BigMemory server.

Start the BigMemory server s that you installed. See Operations topics in BigMemory documentation for more information.

If needed adjust memory configuration for the local Presto host. See Memory Configuration for the Mashup Server for instructions.

The basics of storing a dataset in the Presto Analytics In Memory Store is covered in Use the Presto Analytics In Memory Store to Store and Load Datasets in Getting Started. You can also 

Keys for datasets must be unique strings within each In Memory Store whether the store serves one Mashup Server or a cluster. If two mashups store different datasets under the same key name the dataset is overwritten each time one of the mashups is run.

Although the In Memory Store has a single namespace RAQL supports two namespace layers separated by scope . You specify what namespace layer is used to find or create a key with the scope attribute in and statements.

Global keys have no prefix added to the key name you set so the name of each key must be unique. This is the default namespace if you omit a scope. Or you can set scope global to put a key in this namespace.

The keys used in Use the Presto Analytics In Memory Store to Store and Load Datasets Group and Analyze Rows and Group and Analyze Rows with Row Detail are all examples of global key names that do not include a domain name.

The user namespace for keys prepends the Presto username of the current user to the key name when a key is created in the In Memory Store. Thus keys are automatically unique.

Each user can load data from the key with their username as prefix but no other user can so this namespace is also private to each user. To use this namespace set scope user in mashups that store a dataset stream and the corresponding mashups that load data from that key.

This namespace is useful if the mashup that stores the dataset allows a user to dynamically preprocess the dataset before it is stored based on input parameters. This ensures that each user sees the data they wanted in later analysis. For example 

This mashup allows users to set the specific stock symbols they want to work with using an input parameter for the mashup. Each time this mashup is run by a different user it retrieves the dataset filters it by the symbols that user has chosen and stores this in the In Memory Store with a key in the form username stocks2011.

If the dataset is not customized when it is stored however the user key namespace can result in multiple copies of exactly the same data for different users.

Datasets are streamed in sets of rows with the maximum number of rows defined as the partition size. For more information on streaming partitions see Stream Partitions.

You set the partition size in when you store the dataset in the In Memory Store. If no partition size is set RAQL uses a default partition size of 10 000 rows. The following example sets the partition size for this dataset to 20 000 rows 

To load data from the Presto Analytics In Memory Store datasets must first be stored using see Store Data in Presto Analytics In Memory Store for more information and examples . The basics of using to load a dataset stored with a global key are covered in Group and Analyze Rows and Group and Analyze Rows with Row Detail in Getting Started.

To load datasets that are stored in the In Memory Store with a global key you simply enter the key name in . No scope information is required. Group and Analyze Rows and Group and Analyze Rows with Row Detail in Getting Started are examples of loading datasets from global keys. Keys with a user scope have a username added as a prefix to the key value. To get the current user s username for this prefix you must set the scope attribute so that Presto can find the correct key such as the example 

In many cases analysis is only interested in the most recent updates to a dataset. You can load specific rows from a dataset stream stored in the In Memory Store based on a time interval starting back from now using the period attribute in the statement.

You specify the recent time period for the rows you want to load as a number of seconds minutes hours days or weeks. See for the syntax to use.

The following example works in conjunction with the example shown in Append Query Results to illustrate the effect of retrieving dataset rows based on a recent time period 

It retrieves all rows from the storeAppendPlants in memory key that were added within the last 10 minutes and then selects the list of distinct countries within those rows. The following examples of results for different time periods shows which rows were loaded 

To try this example use the following EMML code for this loadLastPeriod mashup and open it in Mashup Editor. Also create or open the storeAppend mashup shown in Append Query Results. Then run storeAppend. Wait a few seconds or minutes update the value of period in loadLastPeriod and run it to see which rows it loads.

If the dataset you need to load has been stored in an In Memory Store that is not the default you must identify which In Memory Store holds the dataset by name using the cache attribute in the statement.

The Presto Analytics In Memory Store can be split into multiple stores with unique names to manage specific datasets differently.

The following example loads earthquake data from a USGS Atom feed that is stored in an In Memory Store named RAQL NONCRITICAL DATA 

If you do not specify the name of an In Memory Store in Presto Analytics looks for the dataset in the default In Memory Store as expected.

Errors can occur in mashups that load datasets from an In Memory Store key if the key is missing unexpectedly. One way to avoid this is to have a mashup optionally store the dataset if it is not present using the and statements in EMML. These EMML statements allow you to test to see if the key exists and handle both cases appropriately.

The following example uses a memorization pattern to try to load the key and if not found then store the key and work with the data.

The loop tries to retrieve the dataset stream from the mashup scoped key. If the key already exists it executes a simple RAQL query.

The portion of the loop will catch the exception thrown if the key is not found in the loop. It then invoke the REST mashable query the results and load the dataset to the mashup scoped key.

If you run this mashup in the Mashup Editor look at the Console section to see the messages output from the 

Presto Analytics has a single In Memory Store by default. You can define additional In Memory Stores with unique names to use with Presto Analytics. This can be useful if you need to manage memory for specific datasets differently.

