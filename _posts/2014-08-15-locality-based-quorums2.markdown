---

title: Locality based quorums
abstract: Disclosed are various embodiments for distributing data items within a plurality of nodes. A data item that is subject to a data item update request is updated from a master node to a plurality of slave notes. The update of the data item is determined to be locality-based durable based at least in part on acknowledgements received from the slave nodes. Upon detection that the master node has failed, a new master candidate is determined via an election among the plurality of slave nodes.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09588851&OS=09588851&RS=09588851
owner: Amazon Technologies, Inc.
number: 09588851
owner_city: Seattle
owner_country: US
publication_date: 20140815
---
This application is a continuation of and claims priority to co pending U.S. Patent Application entitled LOCALITY BASED QUORUMS filed Jun. 19 2013 and assigned application Ser. No. 13 921 938 which is a continuation of U.S. Patent Application entitled LOCALITY BASED QUORUMS filed on Dec. 14 2010 and assigned application Ser. No. 12 967 187 now issued as U.S. Pat. No. 8 473 775 on Jun. 25 2013 all of which are incorporated herein by reference in their entirety.

A data store such as for example a non relational database a relational database management system RDBMS or other data systems may be implemented as a distributed system. Distributed systems can offer improved reliability and availability better fault tolerance increased performance and easier expansion. Some distributed models employ single master replication where data written to a master data store is replicated to one or more secondary stores. Distributed data stores may experience difficulties if the master data store fails.

The present disclosure relates to failover recovery in a distributed data store. In one embodiment a distributed data store can employ a single master replication model that provides for a master data store and one or more slave data stores. The master data store can receive updates to data items stored in the distributed data store received from client systems and propagate the updates to the slave data stores. Upon propagating the update to a requisite number of slave data stores the master data store can then consider the update as successful durable and or committed to the distributed data store. To provide data durability or integrity from a client or user point of view any update to a data item acknowledged to the user as successful in a distributed data store according to embodiments of the disclosure should be able to survive the failure of the master data store. In such a scenario a slave data store in the distributed data store can be designated as the new master data store. To provide such failover capability to the distributed data store the new master data store previously a slave data store must be able to determine at least the last successful updates committed to the distributed data store and acknowledge as successful to a client in order to properly assume its role as the master.

With reference to shown is a networked environment according to various embodiments. The networked environment includes one or more computing devices . . . N in data communication with one or more client devices by way of a network . The network includes for example the Internet intranets extranets wide area networks WANs local area networks LANs wired networks wireless networks or other suitable networks etc. or any combination of two or more such networks.

Each of the computing devices . . . N may comprise for example a server computer or any other system providing computing capability. Alternatively a plurality of computing devices . . . N may be employed that are arranged for example in one or more server banks or computer banks or other arrangements. A plurality of computing devices . . . N together may comprise for example a cloud computing resource a grid computing resource and or any other distributed computing arrangement. Such computing devices . . . N may be located in a single installation or may be distributed among many different geographical locations. For purposes of convenience the computing device is referred to herein in the singular. Even though the computing device is referred to in the singular it is understood that a plurality of computing devices . . . N may be employed in the various arrangements as described above.

Various applications and or other functionality may be executed in the computing device according to various embodiments. Also various data is stored in a respective data store . . . N that is accessible to the computing device . The respective data store . . . N may be representative of a plurality of data stores as can be appreciated. The data stored in the data store for example is associated with the operation of the various applications and or functional entities described below. The data stored in a data store includes for example replicated data and potentially other data. The replicated data includes any data maintained in the data store that can be durably persisted across a distributed data store implemented by the various computing devices in the system.

The components executed on the computing device for example include a data store management application and other applications services processes systems engines or functionality not discussed in detail herein. When a computing device is designated as a master data store for a distributed data store implemented by computing devices . . . N the data store management application takes on a master role and is thus executed to manage the data store and to facilitate replication of data to one or more data stores accessible to computing devices that are designated as slave data stores. In a master role the data store management application may obtain data item update requests from the client device and respond with data item update confirmations . The updates may take the form of writes to the data store for example. The master data store management application may also generate and send data item replication requests to the slave data store management applications and obtain data item replication confirmations from the slave data store management applications .

When a computing device is designated as a slave data store for a distributed data store implemented by computing devices . . . N the data store management application takes on a slave role and is thus executed to receive data item replication requests from a master data store management application and cause the corresponding data item to be stored in the data store managed by the slave data store management applications . In other words the slave data store management applications are each configured to obtain data item replication requests from the master data store management application . In response to the data item replication requests the slave data store management application is configured to commit data item updates to its respective data store . . . N and then generate and send data item replication confirmations to the master data store management application .

The client device is representative of a plurality of client devices that may be coupled to the network . The client device may comprise for example a processor based system such as a computer system. Such a computer system may be embodied in the form of a desktop computer a laptop computer a personal digital assistant a cellular telephone a set top box a music player a video player a media player a web pad a tablet computer system a game console or other devices with like capability.

The client device may be configured to execute various applications such as a data store client application and other applications. The data store client application may be executed in a client device for example to facilitate interaction with the data store management application . In one embodiment the data store client application may be configured for example to access and render network pages such as web pages or other network content served up by the computing device a web server a page server or other servers for the purpose of interfacing with the data store management application . The client device may be configured to execute applications beyond the data store client application such as for example browser applications email applications instant message applications and or other applications.

In various embodiments the data store client application may comprise a thin client application a thick client application or another type of client application. Some embodiments may include a graphical user interface and or a command line interface. In some embodiments the client device can be configured to interact with a distributed data store provided by the computing devices . . . N via an application programming interface API provided by the data store management application executed in a master data store or slave data store.

A data item update request is generated by a data store client application . Although the data store client application is described as executed in a client device it is understood that the client device may correspond to a server computer that processes business logic generates network pages and or performs other tasks. Thus although a user may generate a data item update request through a user interface a data item update request may also be generated automatically by business logic applications workflow engines network page generation applications and or other applications.

The data item update request may correspond to a portion of another application such as for example a module a library etc. in various embodiments. The data item update request may be sent over the network to the data store management application using hypertext transfer protocol HTTP simple object access protocol SOAP remote procedure call RPC remote method invocation RMI representational state transfer REST Windows Communication Foundation and or other frameworks and protocols. In various embodiments the data item update request may describe updates to data items by using for example structured query language SQL extensible markup language XML JavaScript object notation JSON yet another markup language YAML and or other formats.

Turning now to shown is another view of the networked environment . Where focused on structure of the components focuses on how the computing devices . . . N are distributed among physical locations. The computing devices . . . N may be referred to herein as nodes or replicated nodes . Together nodes function as a distributed data store . Each computing device resides at a particular physical location and these locations can be grouped into availability zones. A collection of computing devices which all reside at the same physical location e.g. building campus etc. is commonly referred to as data center. The example networked environment of includes three data centers . Availability zones and or data centers are geographically separated to some degree but the degree of separation may vary. That is availability zones and or data centers can be distributed across a town across a city across a country across the world etc. Such distribution provides for greater stability of a distributed data store so that if a catastrophic event occurs in one location and may affect a subset of the nodes in the distributed data store the catastrophic event does not jeopardize the system as a whole.

As noted above at any point in time one node acts as a master and the other nodes act as slaves. In the example networked environment of node is the master node while nodes and are slave nodes. The master node is located at data center as is slave node . Slave node is located at data center and slave node is located at data center . It should be appreciated that a networked environment can include any number of data centers a data center can include any number of nodes and the master node can reside at any data center .

An overview of the operation of distributed data store will now be provided. A data store client application executing on a client device generates a data item update request . The data item update request is received by the master node . The master node sends a data item replication request to the slave nodes and . The data item update request may be an actual replica of the originally received data item update request a separate request including some or all of the information in the originally received data item update request or other variations as should be appreciated.

After processing the data item replication request the slave nodes and each send a data item replication confirmation back to the master node . After receiving a predefined quorum of acknowledgements the master node responds to the data store client application with a data item update confirmation . The quorum required to send out this data item update confirmation is a locality based durability quorum described below in connection with and . Because the durability quorum is defined in terms of location and the location of a node does not change changes in the node membership does not change the composition of the durability quorum.

The distributed data store includes features which facilitate recovery upon failure of the master node . A failure can be represented by a hardware failure of some kind an abnormal termination of the data store management application and or other failure as can be appreciated. Therefore the remaining computing devices executing an instance of the data store management application can elect a new master node by employing a consensus algorithm. In some embodiments the data store management application executed in the various computing devices can be configured to collectively employ a Paxos election scheme in order to determine the identity of the computing device that will serve as the master. The election of a master among the various computing devices in the distributed data store can also be determined by other methods of reaching consensus in a distributed system of peers as can be appreciated. The quorum required in the election of a new master is a locality based failover quorum described below in connection with and . The use of locality based quorums for durability and for election allows the new master to synchronize with a smaller number of slave nodes as compared to a simple majority quorum which provides a faster failover that is also fault tolerant.

Referring next to shown is a flowchart that provides one example of the operation of a portion of the data store management application according to various embodiments. In particular the flowchart of illustrates aspects of a process in which a data write requested by a client is replicated in a distributed data store. It is understood that the flowchart of provides merely an example of the many different types of functional arrangements that may be employed to implement the operation of the portion of the data store management application as described herein. As an alternative the flowchart of may be viewed as depicting an example of steps of a method implemented in the computing device according to one or more embodiments.

Beginning at box the data store management application executing on the master node replicates a data item update request to the slave nodes . Next at box the master data store management application waits for the data item update request to obtain a locality based durability quorum. As used herein reaching a locality based durability quorum means that a master action such as an update to the replicated data store is not considered durable until the update is acknowledged by at least one node located in each of K data centers where K is a configurable durability requirement for the distributed data model. K is less than N where N is the total number of data centers . Gaining K data center durability guarantees barring a double fault scenario that if the master node fails the succeeding master will know about and have processed the update.

Though shows a single replicate and acknowledge path in boxes and some embodiments of master data store management application support handling multiple replicates in parallel. The master data store management application repeats the operation of boxes and until an event indicates that a failure of the master node has been detected. The event may take the form of a timeout a message from the master node a message from another node or any other suitable implementation.

Upon failure detection the data store management application executing on a node other than the failed master node begins operating at box . At box a new master candidate is determined by an election among nodes other than the failed master node . The election employs a consensus algorithm as described above. Next at box the data store management application on the newly elected master node waits for consensus among a locality based failover quorum before acting as the master e.g. before receiving data item update requests from clients . As used herein locality based failover quorum is defined as participation from all nodes in N K 1 data centers . Having seen full participation from the N K 1 quorum of data centers at box the newly elected master is guaranteed to know about all of the updates that have gained locality based durability. To this end at box the newly elected master ensures that all data discovered during the wait for consensus box is locality based durable.

At box having ensured that the new master candidate knows about all locality based durable updates the data store management application executing on the newly elected master node transitions from a new master candidate to the master. As such the data store management application receives data item update requests from clients and processes them according to boxes and . The use of a locality based failover quorum together with a locality based durability quorum means that the failover at box is guaranteed to occur safely without any loss of client updates. This is true because the newly elected master node is guaranteed to have seen the most recent update that the failed master had successfully completed as well as all writes prior to that.

Turning now to shown is a flowchart that provides additional detail for the write replication operations of according to various embodiments. It is understood that the flowchart of provides merely an example of the many different types of functional arrangements that may be employed to implement the operation of the portion of the data store management application as described herein. As an alternative the flowchart of may be viewed as depicting an example of steps of a method implemented in the computing device according to one or more embodiments.

Beginning at box the data store management application executing on the master node receives a data item update request from a data store client application . The data item update request includes data to be updated. Next at box the master data store management application applies the update by sending a data item replication request to each of the slave data store management applications . Having requested replication by the slave data stores the master data store management application now waits for the write action to be K data center durable.

At box the master data store management application receives a data item replication confirmation from a particular slave data store management application acknowledging that the slave has committed the data item to its data store. As described above in connection with and each slave data store management application executes on a particular node where that node resides at a particular data center . Next at box the master data store management application determines whether the write is locality based durable i.e. whether write acknowledgements have been received at from least one node located in each of K data centers where K is a value less than the number of data centers represented as N. If at box it is determined that the write is not K data center durable then processing returns to block where the master data store management application waits for write acknowledgements from additional nodes .

If at box it is determined that the write is K data center durable then processing moves to block . At box the master data store management application sends a confirmation to the data store client application which sent the data item update request in box . The confirmation sent in box indicates that the data item update request received in box was successfully performed. The process of is then complete. In some embodiments a durability timeout is used such that if locality based durability is not achieved in a predetermined amount of time a failure code is send to the data store client application instead of a confirmation.

Moving on to shown is a flowchart that provides additional detail for the new master transition operations of according to various embodiments. In particular the flowchart of illustrates aspects of a single master failover in a distributed data store. It is understood that the flowchart of provides merely an example of the many different types of functional arrangements that may be employed to implement the operation of the portion of the data store management application as described herein. As an alternative the flowchart of may be viewed as depicting an example of steps of a method implemented in the computing device according to one or more embodiments.

Beginning at box a failure of the master node is detected. Such a failure can be represented by a hardware failure of some kind an abnormal termination of the master data store management application and or other failure as can be appreciated. At box the remaining computing devices that are executing an instance of the data store management application vote in an election for a new master node and a new master is elected.

At this point the distributed data store is not yet consistent if the previous now failed master had received one or more data item update request from a clients and had sent corresponding replication request to some slaves but not to the slave which is now the new master. Accordingly at box the data store management application executing on the newly elected master node recovers replicated client data as necessary from other slave nodes to maintain data consistency. Notably during this recovery process the newly elected master node does not begin acting as master e.g. receiving update requests from clients until the distributed system can guarantee that the failover can occur without losing any previous writes from the failed master. To this end at box the newly elected master node determines whether the data writes recovered in box include participation from all nodes in a predetermined quorum. Specifically at box the newly elected master node determines whether recovered data has been received from a locality based failover quorum. As noted above the locality based failover quorum is defined as N K 1 data centers.

If at box it is determined that the locality based failover quorum has been reached processing continues at block . At box the data store management application executing on the newly elected master node ensures that data recovered in box is locality based durable. Having determined that the recovered data is locality based durable at box the data store management application executing on the newly elected master node transitions to a state in which the application is ready to receive data item update requests from clients. Determining that the locality based failover quorum has been reached and that recovered data is locality based durable guarantees that the failover can occur safely by guaranteeing that the newly elected master node has recovered the most recent write that the failed master had successfully completed as well as all writes prior to that. The process of is then complete.

If at box it is instead determined that the locality based failover quorum has not been reached the data store management application executing on the newly elected master node returns to box to wait for additional client data to be recovered from other slave nodes . As described above when the locality based failover quorum is reached the master node transitions in box and the process ends.

Moving on to shown is a schematic block diagram of the computing device according to an embodiment of the present disclosure. The computing device includes at least one processor circuit for example having a processor and a memory both of which are coupled to a local interface . To this end the computing device may comprise for example at least one server computer or like device. The local interface may comprise for example a data bus with an accompanying address control bus or other bus structure as can be appreciated.

Stored in the memory are both data and several components that are executable by the processor . In particular stored in the memory and executable by the processor are the data store management application and potentially other applications. Also stored in the memory may be a data store and other data. In addition an operating system may be stored in the memory and executable by the processor . While not illustrated the client device also includes components like those shown in whereby data store management application is stored in a memory and executable by a processor.

It is understood that there may be other applications that are stored in the memory and are executable by the processors as can be appreciated. Where any component discussed herein is implemented in the form of software any one of a number of programming languages may be employed such as for example C C C Objective C Java Javascript Perl PHP Visual Basic Python Ruby Delphi Flash or other programming languages.

A number of software components are stored in the memory and are executable by the processor . In this respect the term executable means a program file that is in a form that can ultimately be run by the processor . Examples of executable programs may be for example a compiled program that can be translated into machine code in a format that can be loaded into a random access portion of the memory and run by the processor source code that may be expressed in proper format such as object code that is capable of being loaded into a random access portion of the memory and executed by the processor or source code that may be interpreted by another executable program to generate instructions in a random access portion of the memory to be executed by the processor etc. An executable program may be stored in any portion or component of the memory including for example random access memory RAM read only memory ROM hard drive solid state drive USB flash drive memory card optical disc such as compact disc CD or digital versatile disc DVD floppy disk magnetic tape or other memory components.

The memory is defined herein as including both volatile and nonvolatile memory and data storage components. Volatile components are those that do not retain data values upon loss of power. Nonvolatile components are those that retain data upon a loss of power. Thus the memory may comprise for example random access memory RAM read only memory ROM hard disk drives solid state drives USB flash drives memory cards accessed via a memory card reader floppy disks accessed via an associated floppy disk drive optical discs accessed via an optical disc drive magnetic tapes accessed via an appropriate tape drive and or other memory components or a combination of any two or more of these memory components. In addition the RAM may comprise for example static random access memory SRAM dynamic random access memory DRAM or magnetic random access memory MRAM and other such devices. The ROM may comprise for example a programmable read only memory PROM an erasable programmable read only memory EPROM an electrically erasable programmable read only memory EEPROM or other like memory device.

Also the processor may represent multiple processors and the memory may represent multiple memories that operate in parallel processing circuits respectively. In such a case the local interface may be an appropriate network that facilitates communication between any two of the multiple processors between any processor and any of the memories or between any two of the memories etc. The local interface may comprise additional systems designed to coordinate this communication including for example performing load balancing. The processor may be of electrical or of some other available construction.

Although the data store management application and other various systems described herein may be embodied in software or code executed by general purpose hardware as discussed above as an alternative the same may also be embodied in dedicated hardware or a combination of software general purpose hardware and dedicated hardware. If embodied in dedicated hardware each can be implemented as a circuit or state machine that employs any one of or a combination of a number of technologies. These technologies may include but are not limited to discrete logic circuits having logic gates for implementing various logic functions upon an application of one or more data signals application specific integrated circuits having appropriate logic gates or other components etc. Such technologies are generally well known by those skilled in the art and consequently are not described in detail herein.

The flowcharts of show the functionality and operation of an implementation of portions of the data store management application . If embodied in software each block may represent a module segment or portion of code that comprises program instructions to implement the specified logical function s . The program instructions may be embodied in the form of source code that comprises human readable statements written in a programming language or machine code that comprises numerical instructions recognizable by a suitable execution system such as a processor in a computer system or other system. The machine code may be converted from the source code etc. If embodied in hardware each block may represent a circuit or a number of interconnected circuits to implement the specified logical function s .

Although the flowcharts of show a specific order of execution it is understood that the order of execution may differ from that which is depicted. For example the order of execution of two or more blocks may be scrambled relative to the order shown. Also two or more blocks shown in succession in and may be executed concurrently or with partial concurrence. Further in some embodiments one or more of the blocks shown in may be skipped or omitted. In addition any number of counters state variables warning semaphores or messages might be added to the logical flow described herein for purposes of enhanced utility accounting performance measurement or providing troubleshooting aids etc. It is understood that all such variations are within the scope of the present disclosure.

Also any logic or application described herein including the data store management application that comprises software or code can be embodied in any non transitory computer readable medium for use by or in connection with an instruction execution system such as for example a processor in a computer system or other system. In this sense the logic may comprise for example statements including instructions and declarations that can be fetched from the computer readable medium and executed by the instruction execution system. In the context of the present disclosure a computer readable medium can be any medium that can contain store or maintain the logic or application described herein for use by or in connection with the instruction execution system. The computer readable medium can comprise any one of many physical media such as for example magnetic optical or semiconductor media. More specific examples of a suitable computer readable medium would include but are not limited to magnetic tapes magnetic floppy diskettes magnetic hard drives memory cards solid state drives USB flash drives or optical discs. Also the computer readable medium may be a random access memory RAM including for example static random access memory SRAM and dynamic random access memory DRAM or magnetic random access memory MRAM . In addition the computer readable medium may be a read only memory ROM a programmable read only memory PROM an erasable programmable read only memory EPROM an electrically erasable programmable read only memory EEPROM or other type of memory device.

It should be emphasized that the above described embodiments of the present disclosure are merely possible examples of implementations set forth for a clear understanding of the principles of the disclosure. Many variations and modifications may be made to the above described embodiment s without departing substantially from the spirit and principles of the disclosure. All such modifications and variations are intended to be included herein within the scope of this disclosure and protected by the following claims.

