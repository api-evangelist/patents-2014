---

title: Creation and application of audio avatars from human voices
abstract: A subject voice is characterized and altered to mimic a target voice while maintaining the verbal message of the subject voice. Thus, the words and message are the same as in the original voice, but the voice that conveys the words and message in the altered voice is different. Audio signals corresponding to the altered voice are output, for example to an application for playback to a user, or to another application or device for subsequent playback by the user or someone else. In one embodiment, the altered voice is posted to a social network. In other embodiments, the altered voice is used by other software applications or consumer electronics applications, such as GPS guidance systems, ebook readers, voice-based intelligent personal assistants, chat applications, and/or others that use voice as an input or output.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09324318&OS=09324318&RS=09324318
owner: Nookster, Inc.
number: 09324318
owner_city: Pasadena
owner_country: US
publication_date: 20141014
---
This disclosure relates generally to mimicking a target human voice and more particularly to consumer electronics applications that use voice inputs and or outputs.

Small form factor electronic devices such as smartphones smartwatches and other wearable devices often lack full keyboards and frequently have a limited screen size. Accordingly conventional user interfaces that rely on text or touch input can be difficult to implement on these devices. Such devices are increasingly relying on voice inputs and voice outputs to interface with users.

With the increased emphasis on voice as a user interface some companies have added voice altering capabilities to their products for entertainment purposes. Such voice altering capabilities include changing the speed of the voice by slowing down or speeding up the rate of playback of an audio file containing the voice and changing the frequency of the voice so that the voice sounds higher or lower than the original.

Embodiments of the invention characterize a subject voice and alter the subject voice to sound like a target voice. A subject voice is received as input. For example a user records her own voice speaking a message of her choice. A sample of a target voice is also received as input. A voice analysis and altering module characterizes the subject voice and the sample of the target voice and then alters the subject voice to mimic the target voice while maintaining the verbal message of the subject voice. Thus the words and message are the same as in the original recording but the voice that conveys the words and message is different. Audio signals corresponding to the altered voice are output for example to an application for playback to a user or to another application or device for subsequent playback by the user or someone else.

In one embodiment a file comprising the output audio signal is posted to a social network. In one implementation the social network is an anonymous social network. Other users can retrieve the posted voice file for playback but they will not be able to identify the user s voice in real life from the altered voice without access to the subject voice and target voice. In other embodiments the output audio signal is used by other software applications or consumer electronics applications such as global positioning system GPS guidance application ebook readers voice based intelligent personal assistants chat applications and or others that use synthetic or natural voice as an input or output or both. The ability to transform one voice to mimic another voice can be used for enhancing user experience engagement for entertaining purposes for adding anonymity or for enabling users to better express their authentic selves without being confined to the voice that their biology dictates.

The features and advantages described in the specification are not all inclusive and in particular many additional features and advantages will be apparent to one of ordinary skill in the art in view of the drawings specification and claims. Moreover it should be noted that the language used in the specification has been principally selected for readability and instructional purposes and may not have been selected to delineate or circumscribe the inventive subject matter.

The Figures FIGS. and the following description describe certain embodiments by way of illustration only. One skilled in the art will readily recognize from the following description that alternative embodiments of the structures and methods illustrated herein may be employed without departing from the principles described herein.

Embodiments of the invention alter a subject voice to mimic a target voice. In one embodiment the subject voice is changed through an audio signal processing technique to mimic a target voice of a user s choice referred to herein as an audio avatar. For ease of explanation embodiments of the invention are described below in the context of an audio file for example a voice message. However it is noted that the audio file can also be streamed audio an audio recording or audio track of a video.

The server system authenticates user devices analyzes and alters voices captured in audio samples by those user devices or other captured voices and outputs digital audio signals corresponding to the altered voices. The server system further stores user data including account information and may store past samples. In some embodiments the server system is implemented as a single server while in other embodiments it is implemented as a distributed system of multiple servers. The server system includes an application interaction module a user account module a voice analysis and altering module and a data store .

The application interaction module manages the interactions between the server system and the user devices . Specifically the application interaction module receives voices from the user devices and sends altered voices to user devices for playback. In one embodiment the application interaction module also communicates the selection of an audio avatar by a user from the user device to the server system and the selection of user preferences user credentials account information or other commands related to functions managed by the server system .

The user account module receives user credentials for example from the application interaction module to authenticate a user operating a user device to the server system and enable access to the user s stored data. The user account module may also store user preferences user profile information and other administrative data for each respective account into data store .

The voice analysis and altering module receives source voices in audio files including voice audio signals for example from streamed audio from an audio recording or from the audio track of a video. The source voices include subject voices that users want to alter and target voices that users want to use as audio avatars. In one embodiment the source voices are from a user using a user device or from the data store . In one embodiment the voice analysis and altering module characterizes the subject voice alters the subject voice to mimic a selected target voice and outputs digital audio signals corresponding to the altered voice. In another embodiment the voice analysis and altering module converts text input into a natural or synthesized voice. The voice analysis and altering module is further described with reference to the block diagram of and the flowcharts of below.

The data store of the server system stores user data for access by the server system and in some cases for distribution to user devices . The user data may be for example data collected by the user account module such as user preferences user profile information and other administrative data for each respective account as well as the user s voice patterns characterized by the voice analysis and altering module and previous voice samples and the respective audio avatars chosen. The data store may further store audio avatars corresponding to other source voices that have been characterized. These audio avatars in data store can be included among the choices from which a user may select a target voice. In some embodiments the data store is a distributed data store and in some embodiments some of the data described as stored in data store as part of the server system can be alternatively or additionally stored on a user device .

The user device A is a computing device such as a desktop laptop or tablet computer or a smart phone or other mobile computing device. The user device A is used to record voices make audio avatar selections and listen to altered voices. The user device A executes an application .

The application is a software application for example running within the operating system of the user device . The software application contains program modules to implement the voice altering functionality described herein. In one particular embodiment the software application implements the functionality of voice based anonymous social network including posting audio messages listening to messages and responding to messages of other users of the social network by posting text audio or video comments. In other embodiments the software application is a GPS guidance application an ebook reader a voice based intelligent personal assistant a voice based chat application and or other application that uses voice as an input or output. In one embodiment the application is used to modify a synthetic or natural voice to sound like a target voice. Specifically as illustrated in this example the application includes a server interaction module a user interface module a voice capture module and an audio avatar module .

The server interaction module of the application A manages the interactions of the application with the server system . The server interaction module communicates data between the user device and the server system via the network . The server interaction module relays to the server system subject voices and selections of audio avatars and the server interaction module relays from the server system altered voices.

In situations in which the systems discussed here collect personal information about users or may make use of personal information the users may be provided with an opportunity to control whether programs or features collect user information e.g. information about social actions or activities profession a user s preferences or a user s current location or to control whether and or how to receive content from the server system that may be more relevant to the user. In addition certain data may be treated in one or more ways before it is stored or used so that personally identifiable information is removed. For example a user s identity may be treated so that no personally identifiable information can be determined for the user or a user s geographic location may be generalized where location information is obtained such as to a city ZIP code or state level so that a particular location of a user cannot be determined. Thus the user may have control over how information is collected about the user and used by the server system .

The user interface module presents the user interface of the software application to the user and receives the user s input through the user device such as through a touchscreen of the user device displaying a graphical user interface. Examples of a user interface of the application implemented as an anonymous social networking application will be described below with reference to .

The voice capture module uses a microphone or a camera and microphone combination of the user device to capture a voice. The voice capture module may optionally format compress or otherwise prepare the audio file for transmission to the server system via the network according to any technique known to those of skill in the art.

The audio avatar module receives a user s selection of a target voice. In one embodiment the audio avatar module presents an array of audio avatars for possible selection by the user for example from local storage on the device or from data store receives the user s selection of an audio avatar to apply to a subject voice and in one embodiment conveys the selected audio avatar to the server interaction module for communication to the server system. In an alternative embodiment the audio avatar module may perform the audio signal processing described below as with reference to the voice analysis and altering module of the server system in order to perform the voice altering on the user device .

The user device B is a computing device such as a smartphone or other mobile computing device connected to a wearable electronic device such as a smartwatch or glasses. The wearable device can be used to perform many of the functions described above such as recording a voice selecting an audio avatar and playing back altered voices. In an alternative embodiment the wearable device may also perform the audio signal processing described below as with reference to the voice analysis and altering module of the server system in order to apply the audio avatar to the subject voice. The wearable device may communicate with the user device B according to any protocol known to those of skill in the art.

The network provides a communication infrastructure between the server system and the client devices . The network is typically the Internet but may be any network including but not limited to a Local Area Network LAN a Metropolitan Area Network MAN a Wide Area Network WAN a mobile wired or wireless network a private network or a virtual private network.

The cache temporarily stores a voice to be analyzed and altered by module for operational convenience. The cache may also temporarily store altered voices or slices of it after it has been processed by the voice analysis and altering module before it is stored in data store .

The voice characterization module characterizes source voices from voice samples. The voice sample may be a subject voice that a user desires to alter or the voice sample may be a sample of a target voice selected to be mimicked. The voice characterization module includes a slicing module a transform module a peak analysis module and a cluster module .

The slicing module slices the audio file containing a voice to be characterized into short periods of a few to tens of milliseconds. Each slice may overlap the previous slice. Some overlap of slices for example half of the slice period is preferred to maximize the fidelity of the processed audio however no overlap is required. Regarding slice length if the slices are too long then more than one sound will be captured in a slice and if the slice is too short then the entirety of one sound is not captured. In both of these cases the quality of the audio processing will be diminished.

The transform module extracts the frequency content of each slice. For example the transform module can apply a Fast Fourier Transform to each slice. Alternatively a filter bank of tuned filters can be used to extract the intensity of each slice at each of the filter center frequencies. This yields the frequency content of each slice. The transform module outputs the normalized levels for all frequencies in the slice.

The peak analysis module extracts the N most significant frequency peaks determined by the size of the peak for each slice. In one embodiment the peak search is only performed within the frequency range of human voice so that the dominant frequencies present in the slice are more likely to correspond to human voice than to background noise. In one embodiment N is selected to be a value between 10 and 15 but higher or lower values of N can be used. The larger N is the more likely that at least some of the peaks correspond to noise. The lower the value of N the less fidelity to the original voice signals. The peak analysis module stores N descriptions of frequency intensity and phase values for the N most significant frequency peaks for the slice which is referred to herein as the slice pattern.

The cluster module clusters slices together according to slice patterns for example using k means clustering or x means clustering or any other clustering or classification algorithm known to those of skill in the art. The clustering results in a set of M slice patterns that correspond to the fundamental sounds present in the audio file. The number M of clustered slice patterns is chosen to optimize the fidelity of the representation and minimize the amount of data that needs to be stored. There is a tradeoff between optimizing the fidelity and minimizing the amount of data. In one embodiment M is on the order of 100 whereas M being on the order of 10 is too few and M being on the order of 1000 is unnecessarily detailed. M can be thought of as the number of distinct or atomic sounds that a given voice makes. This set of M slice patterns is referred to herein as the voice pattern for a characterized voice.

The voice changing module takes as input the slice pattern of the subject from the peak analysis module and the voice patterns of the target that have been output from the voice characterization module . The voice changing module alters the voice from an audio file of a subject to sound like a target voice. The voice changing module includes a pattern matching module a substitution module and a generation module .

The pattern matching module matches each slice pattern from the set of M slice patterns from the subject to the closest voice pattern from the target. One example of an algorithm to perform this matching begins with normalizing the subject s pattern and the target s pattern for example by setting the first frequency f 1 to 1 and the remaining frequencies expressed as a multiple of the first frequency f 1 . So if the pattern is 1000 Hz 1.0 and the second is 1200 Hz 0.5 then the normalized values are 1.0 1.0 and 1.2 0.5 . After all of the frequencies have been normalized the pair of patterns is examined term by term and the root mean square RMS difference between the a frequencies and b intensities are computed. The distance between the two patterns is then calculated as the root mean square frequency difference multiplied by the root mean square intensity difference. Of course for two identical patterns the difference calculated will be zero. The closest match corresponds to the minimum calculated distance.

The substitution module replaces each slice pattern from the subject with the matching voice pattern from the target. The resulting set of slice patterns can be saved temporarily to the cache .

The generation module generates a superposition of sine waves in the time domain over the period of the slice according to the target voice pattern substituted for the subject s slice pattern which can then be output as digital audio signals corresponding to the altered voice.

The text to voice module is optionally present in the voice analysis and altering module or on a user device . The text to voice module takes any input text i.e. any text word phrase command sentence message email or any other text content and converts it to voice output i.e. audio signals corresponding to the input text read aloud in a natural human voice or synthetic voice according to any technique known to those of skill in the art. The voice output from the text to voice module can then be used as the subject to be altered using an audio avatar. Accordingly in some embodiments of the invention by applying an audio avatar to text messages such as instant messages and TWEETS input text messages can be made audible in a voice of a user s choice.

The storage device is a non transitory computer readable storage medium such as a hard drive compact disk read only memory CD ROM DVD or a solid state memory device. The memory holds instructions and data used by the processor . The input interfaces may include a touch screen interface a mouse track ball or other type of pointing device a keyboard a microphone a camera or some combination thereof and is used to input data into the computer . In some embodiments the computer may be configured to receive input e.g. commands from the input interface via gestures from the user. Gestures are movements made by the user while contacting a touch screen interface. For example tapping a portion of the screen touching a portion of the screen and then dragging the touched portion in a particular direction etc. The computer monitors gestures made by the user and converts them into commands e.g. dismiss maximize scroll etc. In other embodiments the computer may be configured to receive input such as audio signals or subject voice audio files from a microphone or camera and microphone combination. The computer may also include one or more speakers to playback audio. The graphics adapter displays images and other information on the display . The network adapter couples the computer to one or more computer networks such as network .

The computer is adapted to execute computer program modules for providing functionality described herein. As used herein the term module refers to computer program logic used to provide the specified functionality. Thus a module can be implemented in hardware firmware and or software. In one embodiment program modules are stored on the storage device loaded into the memory and executed by the processor .

The types of computer used by the entities of can vary depending upon the embodiment and the processing power required by the entity. For example the server system may include multiple computers communicating with each other through a network to provide the functionality described herein. Such computers may lack some of the components described above such as graphics adapters displays speakers and may also lack some types of input interfaces .

In step the subject voice is characterized. Likewise in step the target voice is characterized based on the sample of the target voice. An example process for characterizing a voice is described below in detail with reference to the flowchart of .

In step the subject voice is altered to mimic the target voice while maintaining the verbal message of the subject voice. An example process for altering a subject voice to mimic a target voice is described below in detail with reference to the flowchart of .

In step the digital audio signals corresponding to the altered voice are output. For example the altered voice may be output to an audio file that is posted to a social network so that other users of the social network may access it and play it back on their user devices . In one particular implementation the users of the social network are not able to identify the individual who contributed the post to the social network without access to the subject voice and target voice. Thus in this implementation the contributor can remain anonymous because the contributor s voice has been disguised to sound like the voice of an audio avatar. Thus even the person s real life friends and family who are quite familiar with the person s regular voice will not be able to identify the person by the altered voice that is posted to the social network. Optionally in response to the posted file comments may be received from other users of the social network. Examples of comments include text audio files and video files. If the comment includes a voice steps can be repeated to alter the voice present in the comment to protect the anonymity of the contributor of the comment. Thus in this implementation the anonymous social network allows online discussions through message threads about whatever is on users minds by harnessing the convenience of voice contributions without compromising the user s anonymity. The use of audio avatars allows users of the social network to express themselves in whatever voice they choose regardless of their normal speaking voice.

In other embodiments the altered voice may be output to other software applications or electronic devices in order to apply an audio avatar to the default voice of the software application or electronic device. For example the altered voice may be output to a GPS guidance application so that directions are delivered in the voice an audio avatar such as in the user s own voice rather than in the default voice. As another example the altered voice may be output to an ebook reader so that a story read aloud by the ebook reader can be read aloud in the voice of a loved one such as a child s parent rather than in a default voice. As another example the altered voice may be output to a voice based intelligent personal assistant so that the intelligent personal assistant speaks in a target voice of a user s choice. Similarly the altered voice may be communicated to any consumer application such as a chat application that sends peer to peer communications or peer to group communications for example for entertainment purposes. The altered voice can be output to any application that uses voice as an input and or output. The ability to transform one voice to mimic another voice can be used for enhancing user experience engagement for entertaining purposes for adding anonymity or for enabling users to better express their authentic selves without being confined to the voice that their biology dictates.

In another example embodiment the method of can be used for the creation and application of audio avatars from human voices as follows. To create an audio avatar in step the sample of the target voice is captured by a user device . For example a user speaks into a microphone of the user device to capture his own voice or records his friend s voice on the user device . In step the sample of the target voice is characterized. Once captured and characterized the target voice can be made into an audio avatar to apply to any voice in the future. In fact the sample of the target voice may be captured just once while the audio avatar of the voice can be applied many times to many different subject voices in the future. Thus in one embodiment step and may be executed far in advance of step and which refer to receiving the subject voice and characterizing the subject voice.

In this example after steps and have been completed in step a subject voice is received for example from any other electronic device or application through an application programming interface API or from a text to voice module creating the sample. In step the subject voice is characterized as described above. Optionally a selection of an audio avatar to apply to the subject voice is also received before concurrently with or after the receipt of the subject voice. The selection of the audio avatar may be communicated through the API.

Then steps and may execute substantially as described in the examples above and below in order to alter the subject voice to mimic the target voice and maintain the verbal message of the subject voice. However in step the digital audio signals can be output to any software application or electronic device capable of outputting the digital audio signals for example through an API to the software application or electronic device to replace the default voice of that software application or electronic device. Thus the user can enjoy hearing voice communications from the software application or electronic device in the target voice of his choice e.g. the audio avatar corresponding to his own voice another audio avatar he has created or an audio avatar created by someone else and shared through the server system rather than a default voice or no voice at all. In addition through the user creating and applying audio avatars user metadata is captured to further enhance the user profile stored for example by a user account module .

In step the voice signal is sliced into a plurality of slices for example by a slicing module of the voice characterization module of the voice analysis and altering module . As described above each slice is a short period which ranges from a few to tens of milliseconds and each slice may overlap the previous slice.

In step each slice is analyzed separately either in series or parallel for example by the voice characterization module . In step the frequency content of the slice is extracted. For example a Fast Fourier Transform is performed on the slice for example by the transform module of the voice characterization module . Alternatively a filter bank of tuned filters can be used the extract the intensity of each slice at each of the filter center frequencies. In step the N most significant frequency peaks determined by the intensity levels of the peaks are extracted for example by the peak analysis module of the voice characterization module . Then in step the N descriptions frequency intensity and phase corresponding to the N most significant frequency peaks are stored as the slice pattern. The voice characterization module iterates steps over each slice of the voice signal to accumulate a large number of slice patterns.

In step the slices are clustered according to the slice patterns into a set of M slice patterns where each slice pattern in the set of M slice patterns corresponds to a fundamental sounds present in the voice signal. The clustering is performed for example by a cluster module executing a clustering algorithm such as k means clustering or x means clustering or any other clustering or classification algorithm known to those of skill in the art. In this case M represents a reduced set of patterns from a great number of N descriptions from the slice patterns. This set of M slice patterns is the voice pattern for the characterized voice.

In step each slice pattern of a set of M slice patterns characteristic of the subject voice is analyzed separately either in series or parallel for example by a voice changing module of a voice analysis and altering module . In step the slice pattern of the subject is matched with the closest slice pattern from the voice pattern of the target. As discussed above one example of an algorithm to perform this matching begins with normalizing the subject s pattern and the target s pattern for example by setting the first frequency f 1 to 1 and the remaining frequencies expressed as a multiple of the first frequency f 1 . After all of the frequencies have been normalized the pair of patterns are examined term by term and the root mean square RMS difference between the a frequencies and b intensities are computed. The distance between the two patterns is then calculated as the root mean square frequency difference multiplied by the root mean square intensity difference. Thus the closest pattern corresponds to the minimum calculated distance. In step the slice pattern of the subject voice is replaced by the matching slice pattern from the target for example by the substitution module of the voice changing module . Then in step a superposition of sine waves in the time domain are generated over the time period of the slice corresponding to the slice pattern based on the replacement slice pattern. The voice changing module iterates steps over each slice pattern in the set of M slice patterns in the voice pattern for a subject voice.

In one embodiment the target voice data is processed and short segments of sound information are recorded in memory where methods may be applied to transform the subject voice data in real time or near real time. The target voice data is received as input in step . The target voice generally comprises an audio file or audio stream comprising various words or phrases spoken by a person. The words and phrases in the target voice should be comprehensive enough to include all of the atomic sounds normally produced by the subject voice. The target voice is then digitized if applicable and parsed to generate multiple sequential time segments referred to as slices each slice being on the order of 10 to 50 milliseconds in duration. The slices may be acquired at intervals less than the slice duration in order to produce overlapping slices. This overlap between successive slices helps to produce continuity in the frequency profiles of those slices.

Each slice is then transformed into the frequency domain using a Fast Fourier Transform FFT algorithm or Infinite Impulse Response IIR filter banks for example. After this processing each slice is represented as a spectrum including a range of frequencies each frequency given by a particular intensity and phase optionally . For the FFT a windowing scheme may be performed on the time domain samples before the FFT is calculated in order to reduce aliasing and other unwanted artifacts. The frequency extraction from the FFT result is limited to a range somewhat larger than the known range of the human voice typically 50 5000 Hz. In other embodiments a low pass filter or band pass filter may be applied to limit the bandwidth to useful frequencies in a range of interest.

Each spectrum is normalized using the integrated intensity of the spectrum after filtering. A predetermined number N of dominant peaks in each spectrum are identified from the normalized spectrum. The dominant peaks correspond to the frequencies exhibiting the greatest intensity. Only the dominant peaks of a slice are retained and used to form a slice pattern which is recorded as a vector of data pairs each data pair comprising a representation of the frequency and intensity of a dominant peak or data triplets if phase is used . The frequency of the first pair referred to herein as the base frequency is recorded in Hertz. The remaining frequencies of the vector are represented as a ratio each ratio being the frequency of that peak divided by the base frequency. In the preferred embodiment a predetermined number of peaks e.g. sixteen to forty are recorded for each slice pattern. The remaining peaks being the dominant frequencies produce the signature sound of the target voice data. These peaks may also be used to identify the age and or gender of the voice.

Clustering is then employed to obtain a representative subset of frequencies with which to model sounds in the target voice data. The slice patterns of the target voice are compared to one another to identify similar patterns i.e. slices with similar frequency composition and structure. In particular a plurality of similar slice patterns are identified and grouped based on the similarity of the dominant frequencies as well as the intensities of those dominant frequencies using k means clustering or x means clustering or any other clustering or classification algorithm for example. K means clustering effectively collects the slice patterns into groups where each group is defined by a cluster region or boundary in a multi dimensional frequency intensity space. In general each group includes a plurality of slice patterns that are clustered near one another in that multi dimensional space. For each group the centroid of the plurality of slice patterns is used to generate an individual slice pattern representative of the plurality of slice patterns grouped in the cluster. The set of representative slice patterns are referred to herein as voice patterns. In other embodiments a group of slice patterns are combined by averaging the slice patterns of a group or by identifying center peaks of clusters of dominant peaks present in the slice patterns. Reduction of multiple slice patterns into a single voice pattern has many benefits namely 1 it reduces the number of slice patterns necessary to transform the subject voice data into the target voice 2 it reduces the processing time necessary to transform the subject voice to the target voice and 3 it reduces noise in the slice patterns.

Like the target voice data the subject voice data may be an audio file or an audio stream containing voice and or non voice sounds. The subject voice data is received as input in step . The subject voice data is parsed to generate a plurality of slices and each slice is transformed into the frequency domain and filtered. The spectrum of each slice is normalized using the integrated intensity of the spectrum after filtering. The dominant peaks of the spectra are identified and used to generate slice patterns as described above.

Each slice pattern of the subject voice is then matched to the most similar target slice pattern namely the most similar voice pattern. In one embodiment a match is identified using a distance metric in a multi dimensional hyperspace representing the frequencies and intensities of dominant peaks as well as phases prosody in some embodiments. Each voice pattern corresponds to a single point in the multi dimensional hyperspace in which each axis corresponds to one possible frequency represented by based frequency and frequency ratios in the slice spectra. The distance metric is then used to identify the voice pattern closest to the subject slice pattern. The closest point may be the nearest neighbor determined based on the Euclidian distance or using a Manhattan distance algorithm for example. Once the nearest neighbor is identified the voice pattern is selected to substitute for the corresponding subject slice pattern. The matching process and substitution process are repeated for each subject slice pattern of the subject voice data. An audio file is generated from the sequence of voice patterns which are transformed from the frequency domain back to the temporal domain the slices concatenated in the sequence in which they were originally parsed and the corresponding audio file outputted to an audio player. The effect is that the voice in the subject voice data will sound like that of the target voice but the message and meaning in the original subject voice will be preserved.

Some portions of the above description describe the embodiments in terms of algorithmic processes or operations. These algorithmic descriptions and representations are commonly used by those skilled in the data processing arts to convey the substance of their work effectively to others skilled in the art. These operations while described functionally computationally or logically are understood to be implemented by computer programs comprising instructions for execution by a processor or equivalent electrical circuits microcode or the like. Furthermore it has also proven convenient at times to refer to these arrangements of functional operations as modules without loss of generality. The described operations and their associated modules may be embodied in software firmware hardware or any combinations thereof.

As used herein any reference to one embodiment or an embodiment means that a particular element feature structure or characteristic described in connection with the embodiment is included in at least one embodiment. The appearances of the phrase in one embodiment in various places in the specification are not necessarily all referring to the same embodiment.

As used herein the terms comprises comprising includes including has having or any other variation thereof are intended to cover a non exclusive inclusion. For example a process method article or apparatus that comprises a list of elements is not necessarily limited to only those elements but may include other elements not expressly listed or inherent to such process method article or apparatus. Further unless expressly stated to the contrary or refers to an inclusive or and not to an exclusive or. For example a condition A or B is satisfied by any one of the following A is true or present and B is false or not present A is false or not present and B is true or present and both A and B are true or present .

In addition use of the a or an are employed to describe elements and components of the embodiments herein. This is done merely for convenience and to give a general sense of the disclosure. This description should be read to include one or at least one and the singular also includes the plural unless it is obvious that it is meant otherwise.

Upon reading this disclosure those of skill in the art will appreciate still additional alternative structural and functional designs. Thus while particular embodiments and applications have been illustrated and described it is to be understood that the described subject matter is not limited to the precise construction and components disclosed herein and that various modifications changes and variations which will be apparent to those skilled in the art may be made in the arrangement operation and details of the method and apparatus disclosed herein.

