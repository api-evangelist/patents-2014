---

title: Systems and methods for efficient screen capture
abstract: Systems and methods are provided for efficient screen capture and video recording on mobile and other devices. The video is recorded using a graphics rendering pipeline that includes a primary frame buffer, a secondary frame buffer, and a video writer module that encodes and writes data to a video file. The frame buffers include multiple textures to which graphics data can be rendered or copied, with at least the secondary frame buffer textures backed with memory that is quickly accessible by a central processing unit. In operation, a frame is rendered into a texture in the primary frame buffer, and the contents of the rendered frame are copied to a texture of the secondary frame buffer as well as to a default graphics rendering pipeline for output to a display. The contents of the rendered frame are then provided from the secondary frame buffer to the video writer for output to a video file.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09161006&OS=09161006&RS=09161006
owner: Kamcord, Inc.
number: 09161006
owner_city: San Francisco
owner_country: US
publication_date: 20141205
---
The present disclosure relates generally to digital video recording and more particularly to systems and methods for efficient screen capture of a mobile application for the creation of digital video recordings via an integrated software library.

Mobile devices such as smartphones have specialized hardware and software that implicate unique considerations when providing for video processing functionality as compared to desktop and laptop computers running for example the MICROSOFT WINDOWS operating system or the APPLE OS X operating system. There currently are however no third party video recording solutions for mobile games and applications that support the intricacies of these mobile integrated graphical solutions while providing for minimal performance overhead.

Systems and methods are described for efficient screen capture using a third party library integrated into a mobile application. As one example a mobile game developer can obtain the library and integrate it into a game by for example utilizing an application programming interface provided by the library. In execution the functionality provided by the library allows game sessions to be manually or automatically recorded on behalf of the user. Upon completion of a recording a user interface can allow the player to review and edit the created video and upload the video to the internet for sharing via social media.

In one aspect a method for efficient screen capture includes the steps of providing a video recording pipeline comprising a primary frame buffer comprising a first plurality of textures a secondary frame buffer comprising a second plurality of textures each comprising memory accessible by a central processing unit and a video writer module for writing data to a video file receiving into a texture of the primary frame buffer a rendering of a frame copying contents of the rendered frame to a texture of the secondary frame buffer and to a default graphics rendering pipeline for output to a display and providing the contents of the rendered frame from the secondary frame buffer to the video writer for output to a video file.

The primary frame buffer and or the secondary frame buffer can be configured as a circular buffer of textures and the first and or second pluralities of textures can include OPENGL textures.

The rendered frame can be received from an application on a mobile device and the video recording pipeline can be provided as a library that interfaces with the mobile device application.

In one implementation providing the rendered frame to the default graphics rendering pipeline includes copying contents of the rendered frame to a default frame buffer in the default graphics rendering pipeline.

In another implementation the method includes switching to a next one of the textures in the primary frame buffer such that a next received frame is rendered into the next texture of the primary frame buffer. The method can further include switching to a next one of the textures in the secondary frame buffer such that contents of the next received rendered frame are copied into the next texture of the secondary frame buffer.

In a further implementation the video writer performs a color conversion on the contents of the rendered frame.

The details of one or more implementations of the subject matter described in the present specification are set forth in the accompanying drawings and the description below. Other features aspects and advantages of the subject matter will become apparent from the description the drawings and the claims.

Described herein are various implementations of methods and supporting systems for providing high performance screen capture for recording videos in mobile applications. The techniques described herein can be implemented in any appropriate hardware or software. If implemented as software the processes can execute on a system capable of running one or more commercial operating systems such as the MICROSOFT WINDOWS operating systems the APPLE OS X operating systems the APPLE IOS platform the GOOGLE ANDROID platform the LINUX operating system and other variants of UNIX operating systems and the like. The software can be implemented on a general purpose computing device in the form of a computer including a processing unit a system memory and a system bus that couples various system components including the system memory to the processing unit.

Processors suitable for the execution of software include by way of example both general and special purpose microprocessors and any one or more processors of any kind of digital computer. Generally a processor will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a processor for executing instructions and one or more memory devices for storing instructions and data. Information carriers suitable for embodying computer program instructions and data include all forms of non volatile memory including by way of example semiconductor memory devices e.g. EPROM EEPROM and flash memory devices magnetic disks e.g. internal hard disks or removable disks magneto optical disks and CD ROM and DVD ROM disks. One or more memories can store media assets e.g. audio video graphics interface elements and or other media files configuration files and or instructions that when executed by a processor form the modules engines and other components described herein and perform the functionality associated with the components. The processor and the memory can be supplemented by or incorporated in special purpose logic circuitry.

Referring to a video of a game or other user activity or experience can be recorded on user device . User device can be for example a smartphone such as an APPLE IPHONE device or an ANDROID based device tablet laptop palmtop television gaming device music player information appliance workstation personal digital assistant wireless device or other computing device that is operated as a general purpose computer or a special purpose hardware device that can execute the functionality described herein. User device can include a display such as an integrated or separate display screen touchscreen monitor or other visual output feature. User device can also include an audio output feature such as speakers. A video recorded on the user device can capture video and or audio output.

User device includes application video recording library and graphics API which can be implemented for example using a plurality of software modules stored in a memory and executed on one or more processors. The modules can be in the form of a suitable programming language or framework which is converted to machine language or object code or interpreted to allow the processor or processors to read the instructions. Application can be a game web browser video player native application web application or other application that can interface with video recording library for the recording and creation of a video file. For example video recording library can be a software library integrated into application and having an application programming interface API accessed by application when rendering graphical data for display on an output screen of the user device . Video recording library sits between application and a native graphics API used by the user device to render graphics. In one instance the graphics API is an implementation of the OPENGL API such as the OPENGL API for embedded systems OPENGL ES and video recording library replaces intercepts or works in conjunction with OPENGL function calls to provide for the video recording functionality described herein. In other instances the graphics API includes APIs other than the OPENGL API such as the IOS METAL interface.

User device can communicate with a server through communications network . Server can be implemented in any appropriate hardware or software and can provide for example an interface through which user device can upload videos created using the functionality described herein. In some instances server includes a web server and social medium that allows users to share videos with each other. In one implementation server includes an audio video processing server that performs video encoding compression color correction audio processing and or other functions on audio video data provided by user device . In one example user device encodes a video and uploads it to server which can re encode the video in various bitrates prior to making it available to other users.

Communication over network can occur via any media such as standard telephone lines LAN or WAN links e.g. T1 T3 56 kb X.25 broadband connections ISDN Frame Relay ATM wireless links 802.11 Bluetooth GSM CDMA etc. and so on. The network can carry TCP IP protocol communications and HTTP HTTPS requests made by a web browser and the connection between user device and server can be communicated over such TCP IP networks. The type of network is not a limitation however and any suitable network and protocol s can be used.

Method steps of the techniques described herein can be performed by one or more programmable processors executing a computer program to perform functions by operating on input data and generating output. Method steps can also be performed by and apparatus can be implemented as special purpose logic circuitry e.g. an FPGA field programmable gate array or an ASIC application specific integrated circuit . Modules can refer to portions of the computer program and or the processor special circuitry that implements that functionality.

It should also be noted that the present implementations can be provided as one or more computer readable programs embodied on or in one or more articles of manufacture. The article of manufacture can be any suitable hardware apparatus such as for example a floppy disk a hard disk a CD ROM a CD RW a CD R a DVD ROM a DVD RW a DVD R a flash memory card a PROM a RAM a ROM or a magnetic tape. In general the computer readable programs can be implemented in any programming language. The software programs can be further translated into machine language or virtual machine instructions and stored in a program file in that form. The program file can then be stored on or in one or more of the articles of manufacture.

In one implementation the video recording library enables recording of gameplay or other application activity on the user device by redirecting the application engine to draw or render graphical data to a frame buffer containing a special texture instead of the default frame buffer otherwise used by the device . As used herein a texture can refer to an object which contains an image such as an OPENGL texture object that can store one or more images of various types. A special texture can refer to a texture allocated using memory accessible by a central processing unit CPU such that the CPU can quickly read and access the bytes of a frame rendered to the special texture. The enhanced access speed allows for efficient reading of the texture data and writing of the texture data to a video file. In one instance a texture is a special texture if its bytes are directly accessible by a CPU using a pointer to memory that was made available when the texture was instantiated.

In one instance for an OPENGL based application instead of using native OPENGL interfaces to access the data of the frames being drawn by the application the library directs the application to render to a texture backed by non OPENGL interfaces e.g. Core Video can be used to allocate the texture on an IOS based device and EGL can be used to allocate the texture on an ANDROID based device . Doing so allows CPU code to efficiently access the rendered graphics frames such that writing those frames to a video file is performed relatively quickly. The advantage of this approach is that gameplay or other application performance while recording is barely impacted whereas prior approaches of using OPENGL interfaces to read frame data significantly and noticeably impact performance making such solutions unviable for practical purposes.

The creation of a special texture that can be quickly accessed by the CPU and a video data writing component generally differs for each platform. In one implementation on the IOS platform with the OPENGL API a special texture can be created using Core Video framework as follows 

In another implementation on the IOS platform with the METAL interface a special texture can be created using CoreVideo.framework and Metal.framework 

In another implementation on the ANDROID platform a special texture can be created using EGL extensions 

The contents of the primary frame buffer are then copied to a secondary frame buffer which is backed by a special texture that allows the CPU to quickly and efficiently read the bytes that represent a rendered frame. The use of the special texture helps to avoid unacceptable performance degradation that would otherwise accompany video recording during gameplay or other application use. Further the use of the secondary frame buffer with the primary frame buffer also acts as a double buffering scheme to prevent tearing in recorded video. That is if the contents of the primary frame buffer were fed directly to video writer it is possible for the application to begin drawing the next frame while the video writer is attempting to process and record the previous frame thereby causing an undesirable tearing effect.

After being copied into secondary frame buffer the special texture data bytes are sent to video writer using for example a background thread or process which asynchronously encodes and writes the frame to video file . Further while the rendered data is proceeding along the video recording pipeline the frame contents can be copied from the primary frame buffer to the default frame buffer for output to screen i.e. along the standard pipeline .

In another implementation shown in a graphics rendering pipeline omits a secondary frame buffer. Instead the primary frame buffer is backed with a special texture that allows for fast access and reading of the allocated texture bytes by a CPU. Sending the contents of the primary frame buffer directly to video writer rather than copying the data to a secondary frame buffer as shown in provides for better performance i.e. less delay when recording video compared to rendering pipeline . However as noted above the use of a single frame buffer can result in tearing.

Referring now to a graphics rendering pipeline provides the performance advantages of the pipeline shown in while preventing possible tearing at the cost of additional memory use. Pipeline includes two frame buffers and each backed by special textures that allow for fast CPU access such that video writer can efficiently write frames to video file . Here application alternates between frame buffer and frame buffer when drawing frames. For example application can draw all even numbered frames to frame buffer and can draw all odd numbered frames to frame buffer . Once each frame is drawn the pipeline continues in a similar fashion to pipelines described above. That is the contents of each frame can be copied to default frame buffer for display on screen and the contents of each frame can be sent to video writer for output to video file .

Referring to both in one example pipeline operates as follows with the individual textures in the primary frame buffer being referred to as P0 P1 and P2 and the individual textures in the secondary frame buffer being referred to as S0 and S1. Assume that P0 and S0 are currently bound to their respective frame buffers and . In STEP application renders a first frame into texture P0 of primary frame buffer . The contents of the first frame are copied from the default frame buffer for display on an output screen STEP . When the first frame is finished rendering and after it has been copied to the default frame buffer P0 is detached from the primary frame buffer and P1 is attached in its place STEP . P1 is then ready to receive the next rendering of a frame. The contents of the first frame are copied from P0 to texture S0 in secondary frame buffer STEP . In STEP S0 is detached from the secondary frame buffer and S1 is attached in its place allowing for S1 to receive the next rendered frame.

In some implementations the contents of the frame in S0 are processed by performing a color conversion from RGB to YUV space. In STEP the converted bytes are provided to a video encoder and writer. The process continues for each subsequent frame and the textures in both the primary frame buffer and secondary frame buffer are rotated forward one at a time in the manner described above. For example for the next frame P1 is attached to the primary frame buffer and S1 is attached to the secondary frame buffer . The contents of the frame in P1 are then copied into S1 and both frame buffers can be rotated to texture P2 and S0 respectively. Of note the frame buffers can operate independently of each other and rotate their respective textures after the contents of a frame have been fully rendered to or copied to a particular buffer . In some instances if there are delays in the rendering pipeline the primary buffer and or secondary buffer can drop frames i.e. not pass frames along the pipeline in order to accommodate arriving frames.

Although the primary and secondary frame buffers are shown as containing three and two textures respectively other numbers of textures are possible depending on the implementation. For example it may be necessary to balance memory usage more textures uses more memory with the desire to avoid dropped frames more textures allows for greater ability to queue frames . A sufficient number of textures allows the application to continue rendering frames independently of the rest of the video processing pipeline i.e. the secondary frame buffer and onwards . For example a sufficient texture queue allows the application to render to P1 even if P0 has yet to be copied to the secondary frame buffer or even to P2 if the rendering pipeline falls further behind. Using three textures in the primary frame buffer and two textures in the secondary frame buffer has shown to be sufficiently robust to record video without dropping too many frames and further has shown to prevent both flickering on the screen and tearing in the final recorded video . In other implementations a similar effect can be achieved by including additional frame buffers in the pipeline . Further it should be noted that the above steps need not be performed in precisely the same order depending on the implementation for example the rotation of buffer textures and copying of frame data can occur at differing times in the process and at different points with respect to each other.

In one example referring again to the graphics rendering pipeline of the contents of rendered frames in the textures of the secondary frame buffer can be color converted from RGB to YUV by color converter . Each color converted frame can then be provided as input into video encoder . For instance a frame can be input to the encoder after it has finished processing the previous frame. After a frame has been encoded it is sent to the video muxer where it is combined with an audio track if any and written to video file .

As noted above the functionality described herein can be provided as a video recording library or an integrated third party software development kit SDK . This approach provides game and other application developers with the ability to simply download the SDK and easily integrate it into their applications saving the developers from having to build the entire gameplay recording software themselves. Furthermore game players and other application users do not have to purchase third party recording tools nor do they have to actively think about when they want to record. Recording sessions can be automatically programmed by the application developer and the recording watching and sharing process can be naturally built into the application s flow and user experience.

In one implementation on the IOS platform the library is compiled and built into the application e.g. a game . When the game is launched from a mobile phone the library detects the game engine among a variety of known game engines and automatically integrates itself into the graphics rendering pipeline of the game engine. The integration can occur on application load using a method known as swizzling where existing methods are replaced with methods provided by the library that add certain functionality as described herein. This change allows for gameplay recording of the video and audio of the game.

In the case however that the application developer uses a game engine that is not automatically detected by the library the developer can use API hooks to integrate the gameplay recording technology provided by the library into the game engine. One example of this API for an OPENGL based game engine in Objective C is as follows 

By inserting the above function calls in the appropriate places in an IOS based game engine s rendering pipeline gameplay recording functionality can be provided in a way that resembles the graphics rendering pipeline shown in .

Referring to the METAL based functions above the configureLayer method allows the layer and device to be saved for later use and also changes the layer s mode so that created textures can be accessed and sampled. The setCurrentDrawawble method updates the current METAL drawable such that the drawable and texture to read from is made known. In some implementations this method is unnecessary and can be removed by a proper application of swizzling on the CAMetalLayer nextDrawable method. The addMetalCommands method determines if the current frame is being recorded and if so copies the screen s texture to a special texture that can be accessed by the CPU and provided to the video writer .

In another implementation on the ANDROID platform with the OPENGL interface the library is compiled and built into the game. When the game is launched the library automatically integrates into the game engine s rendering lifecycle using for example a method known as function hooking which allows the library to intercept and replace function calls with its own function calls. Specific OPENGL and EGL methods can be replaced to change the game engine s rendering pipeline in a way that resembles the graphics rendering pipeline shown in .

This technique can be used to modify the behavior of OPENGL functions so that graphics drawn by the application are automatically directed to a frame buffer which can be captured as described herein. In one example in OPENGL ES 2.0 on the ANDROID platform in native C code compiled for ARM the function glBindFramebuffer can be intercepted as follows 

In this example the word 0xe51ff004 is an ARM machine code instruction which redirects execution to the address in the following word. The above technique can be used whenever the instruction set of the patching code and the instruction set of the function being modified are both full 32 bit ARM instructions. With respect to glBindFramebuffer specifically for instance the function it is replaced with i.e. patched glBindFramebuffer is written to redirect the OPENGL function to bind the frame buffer in the added video recording pipeline as opposed to the default frame buffer of the application most of the time . This way the scene being drawn can be appropriately captured.

In other example to patch a THUMB function e.g. eglSwapBuffers an additional instruction can be used as follows 

In this example the word 0x4b434778 is two THUMB instructions which switch the processor to ARM mode just in time to use the ARM patching mechanism in the following word. Patching eglSwapBuffers can be used to determine when the application has finished drawing a frame. In this manner per frame work is made possible such as performing a texture copy binding certain frame buffers etc. without the application developer having to make an explicit per frame call.

The performance gains provided by the techniques described herein in particular the use of special textures are significant and make gameplay video recording a possibility for both the IOS platform and ANDROID platform. Table 1 and Table 2 show the performance impacts of these recording integrations with OPENGL based games on various platforms.

The columns that are most relevant in the tables above are the overhead columns which indicate the performance cost of using the present techniques. Notably the overhead is very small. Specifically for the IOS platform most of the overhead values are near zero and for the ANDROID platform most of the numbers are low single digits. This is a massive improvement over the standard frame copying approach of using the glReadPixels function which for the IOS platform can take 10 20 ms frame and for the ANDROID platform can take over 200 ms frame. This performance improvement is a quantitative and qualitative difference which enables video recording to be practically used in games as otherwise the glReadPixels approach would render the game unplayable.

The terms and expressions employed herein are used as terms and expressions of description and not of limitation and there is no intention in the use of such terms and expressions of excluding any equivalents of the features shown and described or portions thereof. In addition having described certain implementations in the present disclosure it will be apparent to those of ordinary skill in the art that other implementations incorporating the concepts disclosed herein can be used without departing from the spirit and scope of the invention. The features and functions of the various implementations can be arranged in various combinations and permutations and all are considered to be within the scope of the disclosed invention. Accordingly the described implementations are to be considered in all respects as illustrative and not restrictive. The configurations materials and dimensions described herein are also intended as illustrative and in no way limiting. Similarly although physical explanations have been provided for explanatory purposes there is no intent to be bound by any particular theory or mechanism or to limit the claims in accordance therewith.

