---

title: System for partitioning batch processes
abstract: A system for processing a batch job comprises a processor and a memory. The processor is configured to receive a job name for a job submitted to execute, to receive one or more job parameters, and to determine one or more nodes to run the job. The processor is configured to determine one or steps, where for each step: a step is executed on a node using a state of data associated with a start state of the step; and upon completion of executing the step, a result is stored to a durable storage. The durable storage stores the state of data associated with the start state of the step and the completion state of the step and are accessible by other execution processes as associated with either the start state of the step or the completion state of the step. The memory of the system is coupled to the processor and configured to provide processor with instructions.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09495200&OS=09495200&RS=09495200
owner: Workday, Inc.
number: 09495200
owner_city: Pleasanton
owner_country: US
publication_date: 20140523
---
This application is a continuation of co pending U.S. patent application Ser. No. 13 156 278 entitled SYSTEM FOR PARTITIONING BATCH PROCESSES filed Jun. 8 2011 which is incorporated herein by reference for all purposes.

An enterprise database system can process large volumes of data. Performing complex business calculations on data sets representing large organizations can take a great deal of time due to the large number of operations involved. Such jobs typically have a large number of independent operations to perform and thus lend themselves naturally to parallelization. However designing software for parallel computing is challenging even for experienced programmers and errors can be very difficult to find. It is therefore difficult to adapt the parallel computing model to business computing where new business calculations need to be designed by a business administrator not by an experienced software designer yet still guaranteed to work.

For a database software company providing a software as a service SaaS model a further complication arises. In a SaaS architecture a number of tenants clients to the database software company use shared hardware including shared processors and shared storage. In the case of a single tenant utilizing a large parallel processor it can utilize as much of the processor as necessary in order to complete the job as quickly as possible. However when multiple tenants are sharing hardware the processing power needs to be divided between the tenants in an appropriate way such that each job completes in a reasonable amount of time. Clients differ in size and requirements complicating the division of processing power between them.

The invention can be implemented in numerous ways including as a process an apparatus a system a composition of matter a computer program product embodied on a computer readable storage medium and or a processor such as a processor configured to execute instructions stored on and or provided by a memory coupled to the processor. In this specification these implementations or any other form that the invention may take may be referred to as techniques. In general the order of the steps of disclosed processes may be altered within the scope of the invention. Unless stated otherwise a component such as a processor or a memory described as being configured to perform a task may be implemented as a general component that is temporarily configured to perform the task at a given time or a specific component that is manufactured to perform the task. As used herein the term processor refers to one or more devices circuits and or processing cores configured to process data such as computer program instructions.

A detailed description of one or more embodiments of the invention is provided below along with accompanying figures that illustrate the principles of the invention. The invention is described in connection with such embodiments but the invention is not limited to any embodiment. The scope of the invention is limited only by the claims and the invention encompasses numerous alternatives modifications and equivalents. Numerous specific details are set forth in the following description in order to provide a thorough understanding of the invention. These details are provided for the purpose of example and the invention may be practiced according to the claims without some or all of these specific details. For the purpose of clarity technical material that is known in the technical fields related to the invention has not been described in detail so that the invention is not unnecessarily obscured.

A system for processing a batch job is disclosed. The system comprises a processor and a memory. The processor is configured to receive a batch job comprising a sequential or parallel flow of operations wherein each operation has a defined input type and a defined output type. The processor is further configured to verify that the batch job can run successfully wherein verifying includes checking that a first operation output defined type is compatible with a second operation input defined type when a first operation output is connected to a second operation input and wherein verifying includes checking that a parameter used by a calculation in an operation is input to the operation. The memory is coupled to the processor and configured to provide the processor with instructions.

In some embodiments the operation is performed on a set of data objects For example a set of employees a set of organizations a set of pay values etc. In some embodiments verifying comprises checking that the operation does not operate on a simple data type for example where the simple data type comprises an integer a string or a real number. In some embodiments the batch job has a time bound. In some embodiments the batch job is executed.

A system for executing a job is disclosed. The system comprises a processor and a memory. The processor is configured to receive a job name for a job submitted to execute. The processor is configured to receive one or more job parameters. The processor is configured to determine one or more nodes to run the job. The processor is configured to determine one or steps where for each step a step is executed on a node using a state of data associated with a start state of the step and upon completion of executing the step a result is stored to a durable storage. The durable storage stores the state of data associated with the start state of the step and the completion state of the step and are accessible by other execution processes as associated with either the start state of the step or the completion state of the step. For example a process can access the state of data by requesting the data as in the state associated with a given start state of a step or a process can access the state of data by requesting the data as in the state associated with a given completion of a given step. This access of the state of data continues even after the process step is complete and the data is further processed by other steps. The memory of the system is coupled to the processor and configured to provide processor with instructions.

In various embodiments the job name comprises an alphanumeric name a number a job identifier or any other appropriate name. In various embodiments wherein determining the one or more nodes is based on one or more of the following a user requesting the job a customer size a customer service level a priority of the job a size of the job a current resource availability a current number or type of executing jobs an overall load on the system or any other appropriate way of determining a number of nodes. In some embodiments executing the step comprises splitting the execution into one or more partitions. In various embodiments the partitions have a maximum size are executed in parallel within one node are executed as a number of parallel threads are executed as a number of parallel threads based in part on the number of cores or any other appropriate characteristic and or behavior associated with the partitions. In various embodiments the step comprises on or more of the following a calculation step an update step an aggregation step a fork step a join step or any other appropriate type of step.

A system for error checking of process definitions for batch processes is disclosed. The system includes a user interface for specifying a new job e.g. a business calculation or a business process. A user uses the system to define a sequential or parallel flow of operations to create the desired job. As the user specifies the job the system verifies that it can run successfully and will not result in any syntactic errors. In order to ensure that the job can be parallelized for a distributed architecture in an automated manner jobs defined through the system operate on sets of data objects rather than on simple data types. For instance an operation is performed on a set of employees a set of organizations a set of pay values etc. Each operation takes a set of data as input and produces a set of data as output. The output of each operation is connected to the input of one or more operation. The input of each operation is connected to the output of one or more operation. The system verifies that each operation selected operates on a set of data e.g. a set of employees a set of employee s salaries etc. not on a simple data type e.g. an integer a string a real number etc. . The system additionally verifies that anywhere an operation output is connected to an operation input the set data types the output and input that are defined are compatible.

When a user specifies a job he additionally specifies a set of input parameters that the job requests of a job user when the job is run. For instance a job may calculate the total budget for an organization and take as input the organization to run on or may calculate the number of employees under a given manager and take as input the manager to calculate for. In order to guarantee a job can run successfully when the job is defined the system verifies that operations only request parameters that are defined in the specified set of input parameters. In some embodiments the batch job has a time bound.

In some embodiments the application developer is able to add their own validations that run at runtime to prevent the batch from running in the event that the conditions are not met. For example the validations validate that the pay period has already been completed or whether all time sheets have not been properly entered before payroll etc.

In some embodiments the application developer is able to handle events that are raised by the job runtime process. For example the application developer can register an event handler for events such as Job Start Job Complete Success Job Complete Failure Job Complete Job Validation Failure etc. and the event handler can be configured to handle the events with an application developer designated course of action.

A system for partitioning batch processes is disclosed. The system includes hardware and software for coordinating parallel processing of the job in a distributed multi tenanted architecture e.g. an architecture in which there are more than one tenant for example more than one customer using the system and the tenants are kept isolated from each other both in processing and stored data . When a job defined using the user interface is executed the operations executing on sets are executed in parallel e.g. the operation is conducted on each member of the set independently with a user specified level of parallelism. A master computing device coordinates delivery of computing operations and portions of a set to slave computing devices. When the slaves have completed the operation results are persisted in a durable storage and the status of the operation is sent back to the master so that the slave can be given a new computing task. In some embodiments the results are delivered back to the master.

In some embodiments a job comprises one or more steps. A step defines an operation to perform. A step is partitioned based on the data. For example Run Payroll is a job. Step 1 is select pay groups to calculate. The operation in step 1 is a method to find the pay groups. Step 2 is select the employees in the pay groups selected in step 1. Step 3 is calculate payroll for each employee in partition sizes of X employees. So if there are 1000 employees for a partition size of 100 employees there will be 10 partitions of 100 employees each.

In some embodiments a processor is configured to receive a job name for a job submitted to execute receive one or more job parameters e.g. Pay Groups Organization Specific Employees determine one or more nodes to run the job e.g. the number of nodes determined is based on the nodes allocated to the cluster of nodes on nodes that are available for example not busy on other jobs and or the service level agreements SLA for the job for example the job is only allowed to run on 1 then 2 nodes would not be allocated determine one or more steps e.g. starting with the first step then as steps are completed additional steps that have yet to be completed will be determined and allocated if there is enough data for a subsequent step to proceed the node can be allocated to run that step even though all partitions for the current step have not yet been completed for each step execute a step on a node using a state data associated with the start state of the step. In addition for each step upon completion of executing the step store a result to a durable storage wherein the durable storage. The durable storage stores a state of data associated with the completion state of the step. The state of data associated with the start state of the step and with the completion state of the step. The state data associated with the start state of the step and the completion state for the step are always accessible by other execution processes.

In some embodiments the system comprises a processor configured to execute an algorithm. The algorithm is stored in a memory which is configured to provide the processor instructions.

The system architecture includes a large number of interchangeable slave devices available to a set of tenants. Any of the tenants may request a job at any time thus the master cannot assign all of the slave devices to any one job. However the large amount of resources available allows a degree of flexibility to the system where the resources allocated to the job can be adapted based on the situation. The master coordinates a job to use an appropriate number of slave devices depending on factors including the size of the job the priority of the job the priority of the tenant executing the job the load on the system etc. For instance a tenant may negotiate an agreement where typical jobs are conducted on three parallel slave devices but monthly large accounting jobs are conducted on ten parallel slave devices.

Machine interface system comprises a system for interacting with a machine over a network e.g. a public API. In various embodiments a machine uses machine interface system for automatically initiating jobs on database system automatically receiving data to upload to database system automatically providing data to download from database system automatically translating the format of data automatically processing data or any other appropriate use. In some embodiments database system comprises more than one machine interface system and executes large jobs e.g. receiving a large amount of data to upload to database system in parallel. Object management system comprises a system for managing database system . Object management system comprises software for organizing data retrieving data processing data storing data preparing reports from data or any other appropriate function. Data is stored in database system storage . In some embodiments object management system and database storage system comprise an object oriented database. In some embodiments object management system and database storage system comprise a distributed database.

The one or more object reporting system s comprise systems for processing data. In some embodiments the one or more object reporting system s comprise systems for preparing reports based on data. In some embodiments the one or more object reporting system s comprise general purpose computing devices performing data processing for the object management system. In some embodiments the one or more object reporting system s are able to process data in parallel. Data processing in the one or more object reporting system s is read only transactions that involve writing to the database storage system are conducted by object transactional system . In various embodiments a database system user e.g. database system user of or database system user of is allocated a certain number of object reporting systems based on the size of the database needed by the user the data processing needs of the user the agreement negotiated by the user or any other appropriate determination. In some embodiments a database user is allocated a flexible number of object reporting systems where the number is able to change based on the needs of the user.

Object caching system comprises a caching system for storing data by object transactional system and the one or more object reporting system s . In some embodiments object caching system is used for efficient storage and recovery of data. In some embodiments object caching system additionally comprises search index data and can provide searching of cached data. Master server comprises a master server for coordinating jobs. In some embodiments master server delivers jobs or sub jobs to one or more object reporting system s . In some embodiments master server coordinates parallel processing. In various embodiments master server tracks the progress of a job tracks the number of object reporting system s that are processing a job manage that all tasks delegated to object reporting system s are completed properly controls restarting jobs in the event of failure or any other appropriate job coordination task. In some embodiments master server coordinates parallel processing on a plurality of machine interface systems e.g. machine interface system of .

When the parent job runtime object is executed the job entry moment is determined. The job entry moment corresponds to the change ID e.g. change identifier in the database system storage e.g. database system storage of at the time the parent job runtime object is executed. Whenever a method queries stored data during job processing it queries it relative to the job entry moment. If further changes to the database occur they are disregarded by the job. Some job steps update the database e.g. job steps of type update or UPDT . When a job step updates the database the job entry moment is updated to the change ID in the database system storage corresponding to the database update.

In nodes are allocated. In some embodiments nodes comprise object reporting systems e.g. one or more object reporting system s of . In some embodiments nodes comprise machine interface systems e.g. machine interface system of . In some embodiments nodes comprise object transactional systems e.g. object transactional system of . In some embodiments allocating nodes comprises reserving nodes for the job being executed. In various embodiments the number of nodes allocated depends on the user requesting the job on the priority of the job on the size of the job on the overall load on the system or on any other appropriate determining factor. In some embodiments a user requesting a job can additionally specify that the job be run on a single node. In the static step runtime object is received from the job manager. In some embodiments the static step runtime object comprises instructions for the first step of the job. The static step is executed with input from the job runtime parameters as specified by the user scheduling requesting the job. The job manager coordinates parallelization of a job by assigning partitions of its input set to different nodes. Since the static step is executed without any input it is not easily parallelizable and thus is executed on only a single node. In the static step runtime object is sent to a node.

In a step complete message is received from a node indicating completion of the static step by the node. The output of the static step is a set of objects or data which have been stored by the node as they were calculated. In some embodiments the output objects or data are stored in a caching system e.g. object caching system of as they are calculated. In some embodiments the output objects or data are stored in a non volatile memory as they are calculated so they can be recovered in the event of device or network failure. In some embodiments the output objects are stored in a durable database system. In the step complete message is forwarded to the job manager. In the next step runtime object and data partitions are received from the job manager. In some embodiments a data partition is a subset of the stored data set output from the previous step. In some embodiments a data partition has a predetermined size e.g. number of elements of the data set output for each step. In some embodiments the data partition size can change automatically. In some embodiments if the data set output from the previous step does not divide evenly into partitions of the predetermined partition size as many partitions of the predetermined partition size as possible are formed and a final smaller partition is formed with the remaining data. For example partitions are based on the size of the data 1000 employees with a partition size of 100 will result in 10 partitions of 100 each. However different job steps may have different partition sizes based on historic observations for the optimal partition size based on performance testing and observation of production runs of the job.

In the step runtime object and data partitions are sent to nodes. As many nodes as have been allocated in each are sent the step runtime object and a data partition. In some embodiments the step runtime object comprises instructions for executing the next step in the job. In some embodiments executing the next step in the job comprises processing the data received in the data partition. In some embodiments executing the next step in the job comprises storing the data received in the data partition. If the data set output from the previous step divides into a number of partitions smaller than the number of nodes allocated in some of the nodes are left idle. If the data set output from the previous step divides into a number of partitions greater than the number of nodes allocated in some of the partitions wait to be processed until nodes have finished processing the first partition they receive. In some embodiments the node processes multiple elements of the partition received in parallel e.g. if the node has 16 processors it can process 16 elements of the partition at once. In some embodiments if the user requesting a job has additionally specified that the job be run on a single node the user can additionally specify that the job run sequentially on that node and only process a single element at a time.

In a step complete message is received from a node indicating completion of the step by the node. The output of the step is a set of objects or data which have been stored by the node as they were calculated. In some embodiments the output objects or data are stored in a caching system as they are calculated. In some embodiments the output objects or data are stored in a non volatile memory as they are calculated so they can be recovered in the event of device or network failure. In some embodiments after a set of objects is calculated the set is checked to verify that it does not contain any repeated objects. Any repeated elements found are eliminated. In some embodiments when a node completes processing it merges its set of objects or data with the set of objects or data calculated by the other nodes processing partitions from the same input data set. In some embodiments when a set of new objects calculated by a node is merged with a set objects previously calculated by other nodes processing partitions from the same input data set the set of new objects is checked to verify that it does not contain any objects already written to the previously calculated set. Any repeated elements found are eliminated. In it is determined whether there are more partitions to process in the current step. If there are more partitions to process in the current step e.g. there are more partitions than nodes allocated control passes to and processing of the partitions continues. If there are no more partitions to process the step is complete. In the step complete message is sent to the job manager. In it is determined if there are more steps in the current job. If there are more steps in the current job control passes to . If there are no more steps in the current job the process ends.

Operation box comprises an operation type box and a method box. In the example shown the operation type selected is CALC e.g. a calculation and the method selected is Determine Workers in Paygroup . A calculate operation performs a method on each member of the input set and produces an output set. Operation box corresponds to the first operation in the job therefore operation box corresponds to the static step and takes no input. When the operation indicated in operation box is executed the Determine Workers in Paygroup method is executed by a single node and creates an output set according to the method. Operation box comprises an operation type box a method box and a partition size box. In the example shown the operation type selected is CALC the method selected is Determine Pay For Worker and the partition size is 50. The Determine Pay For Worker method operates on each of the output set produced by the Determine Workers in Paygroup method of operation box producing an output set of pay values. The operation is processed by one or more nodes depending on the number of nodes allocated to the job e.g. the number of nodes allocated in of and the number of objects in the set output by the operation indicated by operation box . The one or more nodes process the operation in parallel.

Operation box comprises an operation type box indicating a FORK operation. No additional information is required for a FORK operation. A FORK operation indicates that the job splits into two paths that are executed in parallel. Operation box comprises an operation type box a method box and a partition size box. In the example shown the operation type selected is UPDT. An UPDT operation comprises an operation that updates the database storage e.g. database storage system of . In some embodiments UPDT operations are executed by an object transactional system e.g. object transactional system of . In some embodiments UPDT operations are executed by more than one object transactional system in parallel. The output set from an UPDT operation is an empty set. The method selected is Save Pay . In the example shown the Save Pay method persists the pay values input to it. Operation box comprises an operation type box a method box and a partition size box. In the example shown the operation type box comprises an AGGR operation e.g. an aggregation operation . An AGGR operation performs a function to aggregate values in the input set. In the example shown the Sum Pay method is selected. In the example shown the Sum Pay method sums the pay values in the input set and produces an output set comprising one element the total pay value. Operation box comprises an operation type box a method box and a partition size box. In the example shown the operation type selected is UPDT and the method selected is Save Total Pay . The Save Total Pay method persists the total pay value in the set input to it.

Operation box comprises an operation type box indicating a JOIN operation. A JOIN operation joins two paths split by a FORK operation. In some embodiments the JOIN operation merges the output sets from the final operations of the two paths split by the FORK operation. In some embodiments the JOIN operation forces job processing to wait for all processing of both paths to complete before finishing. End box indicates that the job ends. New step button allows a user to create a new operation to extend the job.

In it is determined whether every method input parameter is defined. In some embodiments determining whether every method input parameter is defined comprises determining the method selected for each operation in the job that requires a method e.g. each CALC UPDT and AGGR operation querying the method for its input parameters and checking that those input parameters are defined e.g. in step of . If it is determined that every method input parameter is defined control passes to . If it is determined that the job includes a method with an input parameter that is not defined control passes to . In the process returns success and ends. The process returning success indicates that the job is allowed to run. In the process returns failure and ends. The process returning failure indicates that the job is not allowed to run.

In some embodiments the system disclosed is used for batch processing of payments for example payroll payments. Enterprise systems process large volumes of data. At some point the volume of data causes the processing time to go beyond a point where it is reasonable for a human to wait for the process to complete. These processes are then structured in a manner to allow for offline or background processing. This requires various job management and job monitoring facilities that allow business users to start jobs and receive notifications upon completion. Although the jobs no longer need to be monitored by a human background processing does not solve the processing time issue. Users no longer have to be present for a job to complete but the job runtime can still increase as data volumes grow. Most business processes are time bound by real world business situations e.g. fiscal period end pay period close enrollment period end etc so running the job in the background is not sufficient it must run within a bound time window as well. A common solution for reducing the runtime is to introduce distributed processing of the job and parallelization of the work. However distributed computing and parallel computing is difficult for even the most senior programmers. Moving to a distributed or parallel model introduces bugs that can be difficult to find and behavior that can be difficult to understand. Thus the application developer s job becomes more difficult.

In some embodiments a distributed runtime architecture can be difficult to design monitor and maintain. Historically this type of architecture was intended to run programs for a single customer or tenant. Service providers that host solutions for many customers typically replicate entire technology stacks for each customer. Replicating a distributed computing architecture would quickly become unmanageable. A system is disclosed that provides a job definition language and runtime framework that allows an application developer to focus on the business logic of the job and not focus on the runtime distribution or parallelization of the work. The definition framework and runtime system provide a set of consistent rules and runtime guards that make it impossible for the application developer to create concurrency issues and in which the job runtime automatically manages distribution of the jobs and handles failure scenarios. Furthermore the definitional language requires no coding in a text based grammar as is common with many programming languages. Programmers can only add and modify nodes in a syntax tree that provides guarantees that the program is correctly defined. The runtime handles job and tenant level isolation and physical e.g. hardware resource management across multiple jobs of different types for different customers. This may include customers of different sizes customers with different job service level agreements etc. all running on a common distributed architecture. For example a service level agreement states that payroll for Customer A must finish in under 1 hour whereas another service level agreement states that payroll for Customer B must finish in under 2 hours where Customer A may have for example paid a higher premium for a higher level of service.

In some embodiments a job definition consists of an ordered sequence of one or more job steps. At runtime each step runs on the context calculated from the previous step essentially creating a for each loop on the elements of the context set. The first step is a special step in that there is no context is available so this step needs to run in a static context. Generally this step performs a query type operation to produce the first context set. For example a simple job may be defined as 

In the above job step one has no context so the calculation needs to be defined as a static query against the data in the system. Step 2 has the context of a set of organization objects or instances so the calculation step will be defined assuming an organization. Step 3 has a context of Worker and so on.

The job definition language can guarantee that each step is correctly defined in such a way that the calculation context is correct. For example an application developer would see an error message if the calculation in step 2 was defined to accept a worker context rather than an organization. This avoids common runtime application programming errors by catching invalid logic at design time.

Note that in the example Job above the first two steps result in sets of instances e.g. objects that are persisted business entities. The calculation of these steps would typically consist of various filter operations applied to an input set. However the third step is a set of instances e.g. objects that represent some derived or computed data based on the persisted data. In this example the job immediately persists this computation but the computed instances could also be used as input for a subsequent calculation step.

In some embodiments application developers can also specify a set of input parameters that are required for the Job. For example a payroll job may be defined by an application developer to require a business site or a pay group as input to the process. So in the above example job definition the United States Business Sites can be parameterized as Input Business Site that is provided at runtime either by a human operator initiating the process or by a scheduled job definition.

The job definition language allows an application developer to choose one of the following job step types 

In some embodiments as different jobs may run more efficiently with different partition sizes the application developer can specify the partition size for each of the steps at the step level. The partition size governs the maximum number of instances objects that will be processed in a single partition e.g. sent to each node at runtime . For example if there are 1000 members of a set and the partition size for the step is 100 there will be 10 partitions of 100 members. In the above job through testing or through historical observations of prior runs we may observe that the second step performs optimally at a partition size of 100 but the third at a partition size of 50. In addition to performance considerations the partition size determines the granularity at which a job may checkpoint progress and restart in the case of a runtime failure.

To allow for jobs to be run with or without human control there are various job scheduling options available to end users e.g. business administrators 

In some embodiments job runtime execution comprises the following a job manager determines the job to be run interprets the job definition and manages execution of the steps. The job manager then starts execution by creating a runtime object to manage the state of the job and the state of each step. Since the job definition is deterministic we can compute all the steps that will need to be executed and pre create the runtime objects for each step. The job manager then executes the steps in the defined order generally following this logic 

In some embodiments the first determination to be made is the type of execution a particular instance of a job will take. There are 3 options 

Note that partitions are calculated for each step in the process based on the number of instances produced from the prior step and the partition size for that step. Partitions are the unit of distribution across runtime nodes.

In addition to distributing partitions across multiple runtime nodes the partition execution is parallelized within a runtime node based on the number of threads available to the node. For example a partition for a given step may have 50 workers in it. A single node is executing that partition and can run the computation for each worker in parallel up to the number of processing threads available. The processing threads can be configured based on the number of physical processing cores e.g. hardware level . For example a 16 core node may allocate 16 compute threads allowing 16 workers to be calculated concurrently within a partition.

As steps are completed the results are written to a durable store e.g. typically a database . This allows for job execution to pick up at any point in the process in the event of failure of any of the execution nodes or even the master nodes. If the master node detects node failure it queries the job state to determine the job step that was running on that node and resubmits the job step to an available node. If the master itself fails upon restart of the master e.g. either manually or through a watchdog process the master can query the job state to determine which jobs are in progress and transition jobs to the next step.

In order to achieve distribution and parallelization of work without application developer input some consistency guarantees need to be put in place. When a job starts the view of the data is locked at a point in time consistent with the start of the job. This ensures that all initial calculation steps see the same view of data and they are isolated from ongoing update activity in a running system. Additionally calculation steps are not allowed to mutate data within a calculation read step. The calculations cannot mutate the state of the system however each calculation has its own state containing the result of the calculation but this is local only to the individual calculation i.e. not shared. A locked moment in time provides isolation in the face of ongoing system updates and consistency for transactions running across multiple nodes. This allows us to run calculations concurrently without the need for complex locking or reasoning about concurrent execution. There are no side effects allowed in the calculation phase so we can guarantee that the execution can be run in parallel and do not have to synchronize resources across concurrent executions. Note that the entry moment is described as a change or transaction ID that describes a modification point including time i.e. don t have to concern with clock skew on nodes 

Update steps are the only steps that are allowed to mutate the state of the system. Update steps generally take the result of a computation that was executed in distributed parallel manner and persist the calculated results to the durable store. After an update step is completed the entry moment view point is reset to allow for subsequent calculations to view the data persisted data of the update step. The moment remains locked for all subsequent calculation steps until another update step is encountered.

Note that forks may result in separate execution paths that result in the different paths having different view moments due to the number and order of calculation and update steps within the fork branch. At a join point the job execution will reset to a common moment based on the latest update moment of any of the fork branches. For example suppose there are two fork paths A and B. Path A has UpdateA1 ReadA1 UpdateA2 ReadA2. Path B has UpdateB1 ReadB1. ReadA1 is guaranteed visibility of the data that has been updated by UpdateA1 but not necessarily any updated data by UpdateB1. Similarly ReadB1 is guaranteed visibility of data updated in UpdateB1 but not necessarily UpdateA1 since these are on independent fork paths. The system locks the view of the data based on the last committed transaction prior to the step the step cannot control this visibility point.

The XpressO runtime deals with sets of instances. Instances are the unique objects in the Workday system that represent business entities. Calculations operate on mathematical sets of instances where membership in the set is determined by the unique ID assigned to each instance. That is a set will never contain two elements with the same IDs. Since calculation steps produce sets of instances we need to ensure that the result of a calculation step results in a well defined set. This is trivial in a sequentially executing program where the set is maintained in memory a single process. However since the set is being calculated across a set of distributed nodes the set needs to be maintained across calculations and unique membership also needs to be ensured. For example a calculation may produce a set of Organizations and then for each Organization get the set of workers for the Organization. Finally pay is calculated for all workers. Since a worker may be in more than one Organization it should be ensured that each worker is only processed once. This is achieved by maintaining the set of IDs produced from each calculation step in a set that is accessible by all calculation nodes. As calculation nodes emit elements for the set they are added to this shared set if the add succeeds then the element has not already been seen. The set can then later be partitioned by a subsequent step for processing.

Security enforced upon the submission of the job e.g. Payroll Administrator can only submit payroll job . Enforced for each step on distributed nodes e.g. ensure that there are user credentials supplied with the job step request and these credentials match a user and that the job was in fact submitted by the user and the job is still in process .

In a save pay result for each pay is shown which is indicated to be processing e.g. the circle with cross . In the step is indicated to be an update step e.g. a dash dot dot box and underlined word save .

In the progress and related information is shown associated with . In shows available actions including view intermediate results view step status timings view historic step timings view partitions status abort cancel process or step and view errors warnings. In some embodiments available actions relate to module related actions favorite related actions instance related actions integration ID s related actions metadata related actions module related object related actions OMS known instance related actions reporting related actions and reports related actions. In some embodiments clicking on a triangle allows viewing details associated with an available action for example viewing step timings has a view start time and a view end time or for example view partition status has a view completed partitions and a view in progress partitions. In step progress is shown e.g. 50 progress for example 50 of 100 partitions complete .

In two processes fork e.g. to and . In for each pay aggregate tax data by locality is shown. In the step is indicated to be an aggregation step e.g. a dash box and underlined word aggregate . In for each tax data save tax results is shown. In the step is indicated to be an update step e.g. a dash dot dot box and underlined word save . In for each pay calculate accounting data is shown. In the step is indicated to be a calculation step e.g. a dotted box and underlined word calculate . In for each accounting data save accounting results is shown. In the step is indicated to be an update step e.g. a dash dot dot box and underlined word save . In and in the processes input to . In join two processes are shown.

Although the foregoing embodiments have been described in some detail for purposes of clarity of understanding the invention is not limited to the details provided. There are many alternative ways of implementing the invention. The disclosed embodiments are illustrative and not restrictive.

