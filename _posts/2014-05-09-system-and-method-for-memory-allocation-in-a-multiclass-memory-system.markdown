---

title: System and method for memory allocation in a multiclass memory system
abstract: A system for memory allocation in a multiclass memory system includes a processor coupleable to a plurality of memories sharing a unified memory address space, and a library store to store a library of software functions. The processor identifies a type of a data structure in response to a memory allocation function call to the library for allocating memory to the data structure. Using the library, the processor allocates portions of the data structure among multiple memories of the multiclass memory system based on the type of the data structure.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09377954&OS=09377954&RS=09377954
owner: Advanced Micro Devices, Inc.
number: 09377954
owner_city: Sunnyvale
owner_country: US
publication_date: 20140509
---
This invention was made with government support under Prime Contract Number DE AC52 07NA27344 Subcontract Number B600716 awarded by the Department of Energy DOE . The Government has certain rights in this invention.

The present disclosure relates generally to memory systems and more particularly to memory systems employing multiple memories.

Processing systems may implement multiple types or levels of memory e.g. combinations of volatile and non volatile memory architectures or in package and external memory to satisfy a variety of design requirements. For example multilevel memory may be used to take advantage of increased bandwidth capacity and expandability by combining memories that offer one or more of these features. Allocation of data structures among the memories of a multilevel memory system having a unified memory address space can impact the system performance. Conventionally the operating system or the hardware of the system determines how to allocate data structures among the memories of the multilevel memory system based on static predefined conditions or based on a seemingly arbitrary allocation. This often can result in an inefficient or ineffective utilization of the different memories of the multilevel memory system.

Each of the memories within the unified memory address space classified into its respective memory class denoted class I and II based on its level type or both. As such in some embodiments the memories may be classified such that memories within the same class share one or more of the same level the same type and other operational characteristics such as access time bandwidth data transfer rate and the like. To illustrate the memories may be classified as class I as they both are at the same level e.g. in package and the memories may be classified as class II as they both are at the same level e.g. outside package or the memories may be classified as class I as they both implement for example DRAM architectures whereas the memories may be classified as class II as they both implement for example SRAM architectures and the like.

While the memory hierarchy is illustrated in the embodiment of as two in package memories and two outside package memories other embodiments may employ any number of memories spanning at least two classes. Additionally in some embodiments the memory hierarchy may comprise any combination of in package and outside package memories including all outside package memory and all in package memories. Some embodiments of the memory hierarchy may implement die stacked memory to increase capacity or otherwise take advantage of multiple memories while maintaining a smaller overall footprint. Die stacked memory may be implemented in a vertical stacking arrangement using through silicon via TSV or other vertical interconnect technologies or in a horizontal arrangement whereby the memory dies are stacked horizontally relative to the processor or one another such that they are connected via an interposer. In the embodiment of the in package memories are illustrated as being of the same class denoted class I and the outside package memories are illustrated as being of the same class denoted class II . Further the multiclass memory system of other embodiments may comprise memories of different levels different types or a combination thereof. For example in at least one embodiment the multiclass memory system comprises memories all of the same level but of different types.

The processor comprises processor cores and a memory controller . While the illustrated embodiment depicts a memory controller implemented at the processor in other embodiments the memory controller may be implemented elsewhere for example at a memory interface of a stacked memory device implementing one or more of the memories . Further in some embodiments the processor comprises more than one memory controller . The memory controller retrieves data from the memories in response to a memory address request based on an address space allocation. Thus in the illustrated embodiment the memory controller and the processing system treats the memories as a single flat unified memory address space . As a result the different classes I II of memories are still logically part of the same level of the traditional memory hierarchy in that they are all part of the same main or system memory and are therefore all accessible through the same unified flat physical memory address space.

Conventionally the operating system or the hardware of the system determines how to allocate data structures among the memories of a multiclass memory system based on static predefined conditions or based on a seemingly arbitrary allocation. Since these conventional approaches cannot take advantage of higher level e.g. software data structure algorithm etc. semantic or domain specific knowledge of how data will be accessed frequently accessed portions of data structures are often allocated to lower performance memories leading to decreased efficiency and overall degraded performance.

In contrast in the illustrated embodiment a library store comprises a library which provides data structures algorithms and other services through an Application Programming Interface API to a programmer or other user such that the back end implementation of the library dynamically handles memory allocation decisions. This allows for allocation decisions based on higher level semantic or domain specific knowledge of how data will be accessed. For example in some embodiments the library may use a multilevel memory aware software interface to selectively allocate data structures to the memories of the multiclass memory system or it may maintain its own pools of memory pages from the different memory levels and explicitly handle the allocation of the data structure to these pages as it sees fit. The library may be any library that transparently manages the memory allocation for example the C standard template library STL Java standard libraries C and the .NET framework custom libraries domain specific libraries and the like. Based on the memory allocation decision of the library an operating system of the processing system allocates a unified flat address space to the memories .

In the illustrated embodiment the processor core executes a software program comprising a memory allocation function call to the library to allocate memory to a data structure . The software program accesses the library via the API . In at least one embodiment the library references a data structure type table to determine how to allocate the data structure among the memories of the multiclass memory system based on the type of the data structure to be allocated. The data structure type table may comprise static allocation rules may maintain heuristics updated based on memory access history or other information or the like. The data structure may be any of a variety of data structures for example a linked list a map structure a binary tree a graph structure an array a tuple and the like. Based on the type of the data structure the library may decide that the operating system is to allocate different portions of the data structure to different memories of the multiclass memory system in an effort to maintain efficient performance of the processor .

For example in the illustrated embodiment the library indicates that the operating system is to allocate a first portion of the data structure to memory and a second portion of the data structure to memory . The library may make such a decision based on the dynamic access patterns of the type of data structure e.g. more frequently used portions should be allocated to memories with faster access times the amount of memory available in each memory or class e.g. as much of the data structure as possible should be allocated to the memories with faster access times as long as they have available memory space a combination of these and the like. In at least one embodiment the portions represent the metadata and data respectively of the data structure such that the metadata portion is allocated to a first set of memories of the multiple memories and the data portion is allocated to a second set of memories of the multiple memories . In the illustrated example the first portion such as the metadata of the data structure is allocated to memory of class I that provides faster access than the memory to which the second portion such as the data of the data structure is allocated. Such an allocation may be made to improve performance of the processor because the metadata is smaller than the data of the data structure because the metadata is accessed more frequently than the data a combination of these and the like.

While the illustrated embodiment depicts the library dividing the data structure into two portion to be allocated among two memories respectively other embodiments may divide the data structure into more portions allocate the data structure among more memories or allocate the data structure without dividing it into portions. Further in some embodiments the library allocates the portions of the data structure to specific memory classes I II for the operating system to distribute subsections of the portions among the memories of a specific class e.g. if portion is allocated to class I the operating system distributes subsections of portion among memories of class I evenly arbitrarily or based on one or more heuristics. Further portions and subsections may represent any portion or subsection of the data structure and need not be contiguous.

In some embodiments the library may provide any of a variety of interfaces or hooks which may be optional to allow the programmer to provide input or direction on how the data structure is to be allocated among the memories of the multiclass memory system . For example in at least one embodiment the library allows the programmer or other user to provide a parameter with the memory allocation function call such that the operating system allocates portions of the data structure among multiple memories of the multiclass memory system based on the parameter or plurality of parameters . The parameter may indicate for example the type of the data structure how the data structure is to be divided into its portions how many memories are to be used which memories are to be used which classes I II are to be used one or more limits e.g. only allocate the metadata separately from the data for the first n lines or the like.

In at least one embodiment library comprises a domain specific library. Some examples of domain specific libraries are routines that are specialized for basic linear algebra such as basic linear algebra subprograms BLAS automatically tuned linear algebra software ATLAS portable extensible toolkit for scientific computation PETSc and application markup language APPML . ATLAS for instance is a self optimizing library that searches the optimization parameter space blocking factor unrolling and implementation algorithms to generate highly optimized hardware specific linear algebra routines. An example of such libraries is the use of blocking for matrix matrix multiply. One implementation includes configuring the library to assume different blocking mechanisms for each level of the memory hierarchy and to move data from the lower levels to the upper levels which also correspond to the innermost loop. Such routines assume access to DRAM is a fixed cost but in a system comprising multiple classes of memories the algorithms would have to be refactored for the faster memory. Sparse matrix vector multiplies SpMV is another example of an algorithm that is important in the performance of many high performance computing HPC applications. SpMV are generally represented using compressed row format CSR . In CSR the nonzero row elements are stored in a values array the column indices are stored in a column array and the index into column array for the start of each row is stored in a row index array. In one embodiment the library allocates storage of the index arrays in the faster memory e.g. class I and the large values array in slower memory e.g. class II to allow for faster searching. In addition to static optimization for a multiclass memory system these libraries can insert profile guided dynamic optimization to move components of data structures between different memory levels during execution.

Responsive to the memory allocation function call to the library via the API for allocating memory to the data structure the library identifies the data structure as the linked list data structure . Based on the type table one or more parameters provided by the program or the data structure itself the library determines how the linked list data structure is to be divided into portions and allocated among the multiclass memory system . In the illustrated embodiment two portions are depicted the first representing an initial segment of the linked list data structure and the second representing a final segment of the linked list data structure . Since the nodes of the initial segment will be accessed at least as frequently and likely more frequently than the nodes of the final segment the operating system allocates the initial segment to memory class I comprising memories with relatively faster access times and the final segment to memory class II comprising memories with relatively slower access times. As a result a memory access of node will only require accessing one or more memories of class I while a memory access of node will require accessing one or more memories of class I for nodes as well as one or more memories of class II for nodes . Since nodes are accessed from memories having relatively faster access times the processor is able to traverse the initial segment of the list relatively quickly allowing for more efficient memory accesses of the linked list data structure . This linked list memory allocation technique may be applied to any type of linked list for example a singly linked list doubly linked list and the like. The linked list data structure may or may not be allocated contiguously within a given memory.

While the illustrated example depicts the linked list data structure divided into two portions representing the initial segment and the final segment allocated to two different memory classes I II the library may determine any number of portions of the linked list data structure and may allocate the portions to any number of memory classes or individual memories. Further in some embodiments the library may make its allocation decisions based on one or more parameters provided by the program . For example the parameter may indicate how to divide the linked list data structure into portions how many portions should be created which memory classes I II to use which memories to use which portions of the linked list data structure should be allocated to which memories or which classes I II the initial node of the linked list data structure or the like.

Responsive to the memory allocation function call to the library via the API for allocating memory to the data structure the library identifies the data structure as the map data structure . Based on the type table one or more parameters provided by the program or the data structure itself the library determines how the map data structure is to be divided into portions and allocated among the multiclass memory system . In the illustrated embodiment two portions are depicted the first representing a key portion of the map data structure and the second representing a value portion of the map data structure . The operating system allocates the key portion to memory class I comprising memories with relatively faster access times and the value portion to memory class II comprising memories with relatively slower access times. As a result the key lookup operations may proceed quickly and then the memory controller may retrieve the corresponding value from a memory with slower access times. The processing system will further realize the efficiencies of such a memory allocation in situations involving multiple lookups. Further allocation to a memory having a slower access time but an increased capacity may be beneficial if the map data structure comprises one or more values of a relatively large size. This map data structure memory allocation technique may be applied to any type of map or other associate array data. The keys and values of the map data structure may or may not be allocated contiguously within a given memory.

While the illustrated example depicts the map data structure divided into two portions representing the key portion and the value portion allocated to two different memory classes I II the library may determine any number of portions of the map data structure and may allocate the portions to any number of memory classes or individual memories. Further in some embodiments the library may make its allocation decisions based on one or more parameters provided by the program . For example the parameter may indicate how to divide the map data structure into portions how many portions should be created which memory classes I II to use which memories to use which portions of the map data structure should be allocated to which memories or which classes I II or the like.

Conventional memory allocation arbitrarily allocates the node metadata and the node data among the memories of the multiclass memory system such that nodes that will be traversed consecutively according to the traversal scheme may be allocated to separate memories such that transversal of the binary tree data structure may require each of the separate memories to be accessed. These conventional approaches introduce inefficiencies as multiple memories may need to be accessed multiple times to reach the requested node and frequently accessed portions of the binary tree data structure may be stored at memories with slower access times. In contrast in the illustrated example the operating system allocates portions of the binary tree data structure such that the node metadata of the binary tree data structure is allocated to memories with faster access times while the corresponding node data of the binary tree data structure is allocated to memories with slower access times.

Responsive to the memory allocation function call to the library via the API for allocating memory to the data structure the library identifies the data structure as the binary tree data structure . Based on the type table one or more parameters provided by the program or the data structure itself the library determines how the binary tree data structure is to be divided into portions and allocated among the multiclass memory system . In the illustrated embodiment two portions are depicted the first representing a node metadata portion of the binary tree data structure and the second representing a node data portion of the binary tree data structure . For ease of illustration the node metadata portion and node data portion only indicate select nodes of the binary tree data structure however the node metadata portion represents all of the node metadata and the node data portion represents all of the node data .

The operating system allocates the node metadata portion to memory class I comprising memories with relatively faster access times and the node data portion to memory class II comprising memories with relatively slower access times. As a result the traversal of the binary tree data structure may proceed quickly since the node metadata will be accessed from one or more memories with faster access times and then the memory controller may retrieve the requested node data from a memory with slower access times. Further allocation to a memory having a slower access time but an increased capacity may be beneficial to nodes of the binary tree data structure comprising node data of a relatively large size.

In another embodiment the operating system allocates portions of the binary tree data structure based on the traversal order of the nodes such that segments having nodes that are earlier in the traversal order according to the traversal scheme of the binary tree data structure are allocated to memories with faster access times while segments having nodes later in the traversal order according to the traversal scheme of the binary tree data structure are allocated to memories with slower access times. For example in the context of a level order traversal scheme since the node metadata of higher levels i.e. closer to the root node will be accessed at least as frequently and likely more frequently than the node metadata of lower levels i.e. closer to the branches the operating system may allocate the first three levels comprising node metadata to memory class I comprising memories with relatively faster access times and the branch level included metadata to memory class II comprising memories with relatively slower access times. As a result a memory access of node data will only require accessing one or more memories of class I while a memory access of node data will require accessing one or more memories of class I for node metadata as well as one or more memories of class II for node metadata . Since node metadata is accessed from memories having relatively faster access times the processor is able to traverse the first three levels relatively quickly allowing for more efficient memory accesses of the binary tree data structure .

These binary tree data structure memory allocation technique may be applied to any type of graph data structure for example a ternary tree structure a B tree structure a directed acyclic graph DAG or the like. Further the node metadata and the node data may or may not be allocated contiguously within a given memory. While the illustrated example depicts the binary tree data structure divided into two portions representing the node metadata portion and the node data portion allocated to two different memory classes I II the library may determine any number of portions of the binary tree data structure and may allocate the portions to any number of memory classes or individual memories. Further in some embodiments the library may make its allocation decisions based on one or more parameters provided by the program . For example the parameter may indicate how to divide the binary tree data structure into portions how many portions should be created which memory classes I II to use which memories to use which portions of the binary tree data structure should be allocated to which memories or which classes I II the traversal scheme or the like.

At block the processing system accesses the library via the API . The library provides data structures algorithms and other services through the API to the programmer or other user such that the back end implementation of the library dynamically handles memory allocation decisions. This allows for allocation decisions based on higher level semantic or domain specific knowledge of how data will be accessed. For example in some embodiments the library may use a multilevel memory aware software interface to selectively allocate data structures to the memories of the multiclass memory system or it may maintain its own pools of memory pages from the different memory levels and explicitly handle the allocation of the data structure to these pages as it sees fit. The library may be any library that transparently manages the memory allocation for example the C standard template library STL Java standard libraries C and the .NET framework custom libraries domain specific libraries and the like.

At block the library identifies the type of the data structure based on for example one or more parameters included with the memory allocation function call heuristics or the like. In at least one embodiment the library references the data structure type table to determine information related to allocation of the data structure among the memories of the multiclass memory system . For example the library may use the type table to identify portions of the data structure in accordance with block . The library identifies portions of the data structure based on the data structure type. In some embodiments the library identifies portions of the data structure based on one or more parameters provided by the program . The portions may be determined based on access frequency data size or the like. The library indicates to the operating system how the data structure is to be allocated based on the portions .

At block the operating system allocates the portions of the data structure among multiple memories of the multiclass memory system . The allocation may be based on the dynamic access patterns of the type of data structure e.g. more frequently used portions should be allocated to memories with faster access times the amount of memory available in each memory or class e.g. as much of the data structure as possible should be allocated to the memories with faster access times as long as they have available memory space a combination of these and the like. In at least one embodiment the portions represent the metadata and data respectively of the data structure such that the metadata portion is allocated to a first set of memories of the multiple memories and the data portion is allocated to a second set of memories of the multiple memories . Such an allocation may be made to improve performance of the processor because the metadata is smaller than the data of the data structure because the metadata is accessed more frequently than the data a combination of these and the like.

In some embodiments the API includes a general memory allocation function call that accepts parameters including the data structure depicted as DS in function call to be allocated and a memory indicator I 1 to indicate which memory the data structure is to be allocated to. In some embodiments the memory indicator I 1 may indicate a memory class I II multiple memories multiple memory classes or the like. Further different embodiments may allow or require any of a number of parameters for example data structure type data structure portions allocation size limits and the like. As illustrated when the processor core executes the software program comprising either of the memory allocation function calls to the library via the API the library indicates that the operating system is to allocate the data structure to the memory identified by the memory indicator I 1 . While the illustrated embodiment uses the standard C library malloc for the memory allocation function calls these techniques may easily be applied to other programming languages and their respective memory allocation interfaces as well. In some embodiments a directive or other annotation like syntax is used to specify a memory allocation by specifying a particular memory or memory class directly to the compiler via a memory indicator. For example in some embodiments the directive is processed by a compiler and the information is passed to the library or the operating system .

Different embodiments may employ different conventions for handling the allocations. For example in some embodiments memory allocation specified by the memory indicator of the function call or directive is a strict requirements such that if the indicated memory does not have enough available memory space to satisfy the memory allocation request the allocation would fail e.g. a NULL pointer may be returned by the function calls with an I 1 memory indicator . In other embodiments the memory allocation specified by the function call or the directive is treated more as a suggestion such that if the indicated memory does not have enough available memory space to satisfy the memory allocation request the operating system allocates the data structure other than specified for example according to other heuristics arbitrarily or the like. In at least one embodiment if the memory allocation specified by the function call or directive is not followed the processing system returns additional information to the programmer or other user regarding the actual allocation.

Some embodiments of the library provide a realloc or remap function call that instructs or suggests to the OS that an existing allocation should be reallocated to a new level of memory optionally resizing the size of the allocation at the same time . Variants may include an interface to allow subsets or regions of an existing memory allocation to be remapped. Further some embodiments of the library provide additional interface functions to help differentiate where an allocation came from. For example in one embodiment the function call type whichMemory ptr returns I 1 if ptr is associated with a physical memory location in memory . In some embodiments these memory allocation techniques are used in combination with Non Uniform Memory Access NUMA based memory allocation schemes.

At block the processing system accesses the library via the API . The library provides data structures algorithms and other services through the API to the programmer or other user such that it acts as an interface for the software to communicate memory locality information preferences and the like to the underlying system software. As such the library facilitates a programmer or other user to specify memory allocation via the function calls.

At block the processing system identifies the memory indicator depicted as I 1 in of the function call to determine the specified location for the allocation. For example the memory indicator I 1 may specify one or more memories one or more classes I II one or more memory levels one or more memory types or the like. The memory indicator I 1 may comprise a parameter passed via the function call a syntax indicator separate from the function call or the function call itself.

At block the processing system identifies portions of the data structure based on parameters of the function call . In some embodiments the parameter may specify the portions of the data structure by identifying the type of the data structure the boundaries of the portions the data size of the portions the data type for the portions or the like. The data structure may be divided into any number of data portions of any size including a single portion representing the entire data structure .

At block the operating system allocates portions of the data structure among multiple memories of the multiclass memory system based on the memory indicator I 1 . For example in response to the function call comprising memory indicator I 1 and parameter DS the operating system allocates the entire data structure to the first memory of class I. In some embodiments the processing system may treat the function call and its specified memory indicator I 1 and parameters as a suggestion rather than a requirement. Generally the method facilitates efficient utilization of a multiclass memory system by allowing programmers or other users including application software to manage the allocation of data structures among the multiple memories of the multiclass memory system using function calls or directives comprising a memory indicator.

In some embodiments certain aspects of the techniques described above may implemented by one or more processors of a processing system executing software. The software comprises one or more sets of executable instructions stored or otherwise tangibly embodied on a non transitory computer readable storage medium. The software can include the instructions and certain data that when executed by the one or more processors manipulate the one or more processors to perform one or more aspects of the techniques described above. The non transitory computer readable storage medium can include for example a magnetic or optical disk storage device solid state storage devices such as Flash memory a cache random access memory RAM or other non volatile memory device or devices and the like. The executable instructions stored on the non transitory computer readable storage medium may be in source code assembly language code object code or other instruction format that is interpreted or otherwise executable by one or more processors.

Note that not all of the activities or elements described above in the general description are required that a portion of a specific activity or device may not be required and that one or more further activities may be performed or elements included in addition to those described. Still further the order in which activities are listed are not necessarily the order in which they are performed. Also the concepts have been described with reference to specific embodiments. However one of ordinary skill in the art appreciates that various modifications and changes can be made without departing from the scope of the present disclosure as set forth in the claims below. Accordingly the specification and figures are to be regarded in an illustrative rather than a restrictive sense and all such modifications are intended to be included within the scope of the present disclosure.

Benefits other advantages and solutions to problems have been described above with regard to specific embodiments. However the benefits advantages solutions to problems and any feature s that may cause any benefit advantage or solution to occur or become more pronounced are not to be construed as a critical required or essential feature of any or all the claims. Moreover the particular embodiments disclosed above are illustrative only as the disclosed subject matter may be modified and practiced in different but equivalent manners apparent to those skilled in the art having the benefit of the teachings herein. No limitations are intended to the details of construction or design herein shown other than as described in the claims below. It is therefore evident that the particular embodiments disclosed above may be altered or modified and all such variations are considered within the scope of the disclosed subject matter. Accordingly the protection sought herein is as set forth in the claims below.

