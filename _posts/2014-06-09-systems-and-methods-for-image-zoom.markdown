---

title: Systems and methods for image zoom
abstract: A method is provided that includes operating a first camera to capture a first image stream and operating a second camera to capture a second image stream. The method further includes initially using the first image stream to display a first field of view in a live-view interface of a graphic display and, while displaying the first image stream in the live-view interface, receiving an input corresponding to a zoom command. The method further includes, in response to receiving the input: (a) switching from using the first image stream to display the first field of view in the live-view interface to using a combination of the first image stream and the second stream to display a transitional field of view of the environment in the live-view interface and (b) subsequently switching to using the second image stream to display the second field of view in the live-view interface.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09360671&OS=09360671&RS=09360671
owner: Google Inc.
number: 09360671
owner_city: Mountain View
owner_country: US
publication_date: 20140609
---
Unless otherwise indicated herein the materials described in this section are not prior art to the claims in this application and are not admitted to be prior art by inclusion in this section.

Computing devices such as personal computers laptop computers tablet computers cellular phones and countless types of Internet capable devices are increasingly prevalent in numerous aspects of modern life. Over time the manner in which these devices are providing information to users is becoming more intelligent more efficient more intuitive and or less obtrusive.

The trend toward miniaturization of computing hardware peripherals as well as of sensors detectors and image and audio processors among other technologies has helped open up a field sometimes referred to as wearable computing. In the area of image and visual processing and production in particular it has become possible to consider wearable displays that place a graphic display close enough to a wearer s or user s eye s such that the displayed image appears as a normal sized image such as might be displayed on a traditional image display device. The relevant technology may be referred to as near eye displays. 

Wearable computing devices with near eye displays may also be referred to as head mountable displays HMDs head mounted displays head mounted devices or head mountable devices. A head mountable display places a graphic display or displays close to one or both eyes of a wearer. To generate the images on a display a computer processing system may be used. Such displays may occupy a wearer s entire field of view or only occupy part of wearer s field of view. Further head mounted displays may vary in size taking a smaller form such as a glasses style display or a larger form such as a helmet for example.

Emerging and anticipated uses of wearable displays include applications in which users interact in real time with an augmented or virtual reality. Such applications can be mission critical or safety critical such as in a public safety or aviation setting. The applications can also be recreational such as interactive gaming. Many other applications are also possible.

This disclosure includes methods and systems for image zoom. An imaging system may include a first camera having a wide field of view of an environment and a second camera having a narrow field of view of the environment. The cameras may continuously capture image streams representing the first and second fields of view. The imaging system may include a live view interface configured to display image frames of the first or second image streams or representations thereof . In one example the imaging system may initially be displaying a first image stream representing the wide field of view. The imaging system may then receive an input indicating that the real time display should transition from displaying the wide angle first image stream to displaying the narrow angle second image stream. Before displaying the second image stream the imaging system may display one or more composite image frames e.g. a transitional field of view representing visual information from both the first and second image streams. The display of the composite image frames may yield a perceived effect of substantially continuous zoom between the wide field of view first image stream and the narrow field of view second image stream respectively captured by the first and second cameras.

An example method may include operating a first camera having a first field of view of an environment to capture a first image stream representing the first field of view and operating a second camera having a second field of view of the environment to capture a second image stream representing the second field of view. The first field of view may be wider than the second field of view. The method further includes initially using the first image stream to display the first field of view in a live view interface of a graphic display and while displaying the first image stream in the live view interface receiving an input corresponding to a zoom command. The method may further include in response to receiving the input a switching from using the first image stream to display the first field of view in the live view interface to using a combination of the first image stream and the second stream to display a transitional field of view of the environment in the live view interface and b subsequently switching to using the second image stream to display the second field of view in the live view interface.

Another example includes a non transitory computer readable medium storing instructions that when executed by a computing device cause the computing device to perform functions. The functions comprise operating a first camera having a first field of view of an environment to capture a first image stream representing the first field of view and operating a second camera having a second field of view of the environment to capture a second image stream representing the second field of view. The first field of view may be wider than the second field of view. The functions further include initially using the first image stream to display the first field of view in a live view interface of a graphic display and while displaying the first image stream in the live view interface receiving an input corresponding to a zoom command. The functions further include in response to receiving the input a switching from using the first image stream to display the first field of view in the live view interface to using a combination of the first image stream and the second stream to display a transitional field of view of the environment in the live view interface and b subsequently switching to using the second image stream to display the second field of view in the live view interface.

In still another example a system is provided that comprises a processor and a non transitory computer readable medium storing instructions that when executed by the processor cause the system to perform functions. The functions comprise operating a first camera having a first field of view of an environment to capture a first image stream representing the first field of view and operating a second camera having a second field of view of the environment to capture a second image stream representing the second field of view. The first field of view may be wider than the second field of view. The functions further include initially using the first image stream to display the first field of view in a live view interface of a graphic display and while displaying the first image stream in the live view interface receiving an input corresponding to a zoom command. The functions further include in response to receiving the input a switching from using the first image stream to display the first field of view in the live view interface to using a combination of the first image stream and the second stream to display a transitional field of view of the environment in the live view interface and b subsequently switching to using the second image stream to display the second field of view in the live view interface.

In yet another example a system is provided that includes a means for operating a first camera having a first field of view of an environment to capture a first image stream representing the first field of view and operating a second camera having a second field of view of the environment to capture a second image stream representing the second field of view. The first field of view may be wider than the second field of view. The system further includes means for initially using the first image stream to display the first field of view in a live view interface of a graphic display and while displaying the first image stream in the live view interface receiving an input corresponding to a zoom command. The system further includes means for in response to receiving the input a switching from using the first image stream to display the first field of view in the live view interface to using a combination of the first image stream and the second stream to display a transitional field of view of the environment in the live view interface and b subsequently switching to using the second image stream to display the second field of view in the live view interface.

Another example method may include operating a first camera having a first field of view of an environment to capture a first image stream representing the first field of view and operating a second camera having a second field of view of the environment to capture a second image stream representing the second field of view. The second field of view may be wider than the first field of view. The method further includes initially using the first image stream to display the first field of view in a live view interface of a graphic display and while displaying the first image stream in the live view interface receiving an input corresponding to a zoom command. The method may further include in response to receiving the input a switching from using the first image stream to display the first field of view in the live view interface to using a combination of the first image stream and the second stream to display a transitional field of view of the environment in the live view interface and b subsequently switching to using the second image stream to display the second field of view in the live view interface.

In yet another example a system is provided that includes means for operating a first camera having a first field of view of an environment to capture a first image stream representing the first field of view and operating a second camera having a second field of view of the environment to capture a second image stream representing the second field of view. The second field of view may be wider than the first field of view. The system further includes means for initially using the first image stream to display the first field of view in a live view interface of a graphic display and while displaying the first image stream in the live view interface receiving an input corresponding to a zoom command. The system may further include means for in response to receiving the input a switching from using the first image stream to display the first field of view in the live view interface to using a combination of the first image stream and the second stream to display a transitional field of view of the environment in the live view interface and b subsequently switching to using the second image stream to display the second field of view in the live view interface.

In another example a mobile device is provided that includes a first camera arranged on a surface of the mobile device and having a first field of view and a second camera arranged on the surface of the mobile device and having a second field of view. The first field of view is wider than the second field of view. An optical axis of the second camera is substantially parallel to an optical axis of the first camera. The mobile device further includes a graphic display and a control system that is operable to operate the first camera to capture a first image stream operate the second camera to capture a second image stream and use the first image stream to display an image stream of the first field of view in a live view interface. The live view interface is displayed on the graphic display. The control system is further operable to while displaying the image stream of the first field of view in the live view interface receive an input corresponding to a zoom command and in response to receiving the zoom command switch to use of the second image stream to display an image stream of the second field of view in the live view interface.

These as well as other aspects advantages and alternatives will become apparent to those of ordinary skill in the art by reading the following detailed description with reference where appropriate to the accompanying drawings.

Example methods and systems are described herein. It should be understood that the words example exemplary and illustrative are used herein to mean serving as an example instance or illustration. Any embodiment or feature described herein as being an example being exemplary or being illustrative is not necessarily to be construed as preferred or advantageous over other embodiments or features. The example embodiments described herein are not meant to be limiting. It will be readily understood that the aspects of the present disclosure as generally described herein and illustrated in the figures can be arranged substituted combined separated and designed in a wide variety of different configurations all of which are explicitly contemplated herein.

In the context of an optical system it is often desirable to have zoom capability. Zoom effects can be displayed within a viewfinder of the optical system before image capture or as an image processing technique after an image is captured by the optical system. Optical zoom involves changing an effective focal length of the optical system before image capture which also changes a field of view of the optical system. Optical zoom is typically performed by changing relative displacement between lenses of the optical system changing refractive indices of the lenses or deforming surfaces of the lenses. These techniques can require various mechanical means e.g. moving optical elements which can add to the complexity or cost of the optical system. On the other hand digital zoom involves pixel interpolation within a viewfinder of the optical system or post image capture processing. Digital zoom can provide a somewhat increased level of detail for the viewer but has computational resource costs and is limited in how much zoom can be provided. Further since digital zoom essentially involves cropping an image and reduces the amount of the image sensor that is used for image capture digital zoom may reduce the amount of detail that is captured as compared to optical zoom.

In an example embodiment a device such as mobile phone may include multiple fixed focal length cameras having differing fields of view or perhaps a single camera system having multiple fixed focal length lenses . These cameras may be oriented substantially the same direction so that they capture different fields of view of the same environment e.g. narrower or wider fields of view . According to an example embodiment the device may operate multiple fixed focal length cameras simultaneously to provide a responsive and perhaps instantaneous optical zoom feature in a live view interface and or in other applications. As an example instead of altering optical characteristics of a single camera via optical zoom or manipulating images via digital zoom image processing two cameras with fixed optical elements e.g. lenses can be used to provide a continuous spectrum of zoom.

In some embodiments a first camera may be configured to capture an image of a first field of view of an environment while a second camera is configured to capture a narrower second field of view of the environment. The first and second cameras may share an optical axis or have distinct and parallel optical axes. The first camera may capture a first image stream and the second camera may capture a second image stream. In a live view interface the optical system may initially display the wide angle first image stream. The system may receive an input related to a zoom command and in response may switch from displaying the first image stream to displaying a transitional image stream of composite image frames that combine portions of frames from the first wide angle image stream and the second narrow angle image stream.

The transitional image stream may include composite image frames that include a border region captured with the first wide angle camera and a center region captured with the second narrow angle camera. Images captured by the first camera and the second camera may be processed resized and merged to form the composite image. The center regions of the composite image frames include increased levels of detail when compared to the border regions while the border regions widen the narrow field of view represented by the composite image frame. Within the transitional image stream border regions representing the first field of view may appear to recede and center portions representing the second field of view may appear to expand as each successive composite image frame is displayed giving the appearance of a smooth zoom effect between the wider first field of view and the narrower second field of view.

Systems and devices in which example embodiments may be implemented will now be described in greater detail. In general an example system may be implemented in or may take the form of a wearable computer also referred to as a wearable computing device . In an example embodiment a wearable computer takes the form of or includes a head mountable device HMD .

An example system may also be implemented in or take the form of other devices such as a mobile phone among other possibilities. Further an example system may take the form of non transitory computer readable medium which has program instructions stored thereon that are executable by at a processor to provide the functionality described herein. An example system may also take the form of a device such as a wearable computer or mobile phone or a subsystem of such a device which includes such a non transitory computer readable medium having such program instructions stored thereon.

An HMD may generally be any display device that is capable of being worn on the head and places a display in front of one or both eyes of the wearer. An HMD may take various forms such as a helmet or eyeglasses. As such references to eyeglasses or a glasses style HMD should be understood to refer to an HMD that has a glasses like frame so that it can be worn on the head. Further example embodiments may be implemented by or in association with an HMD with a single display or with two displays which may be referred to as a monocular HMD or a binocular HMD respectively.

Each of the frame elements and and the extending side arms may be formed of a solid structure of plastic and or metal or may be formed of a hollow structure of similar material so as to allow wiring and component interconnects to be internally routed through the HMD . Other materials may be possible as well.

One or more of each of the lens elements may be formed of any material that can suitably display a projected image or graphic. Each of the lens elements may also be sufficiently transparent to allow a user to see through the lens element. Combining these two features of the lens elements may facilitate an augmented reality or heads up display where the projected image or graphic is superimposed over a real world view as perceived by the user through the lens elements.

The extending side arms may each be projections that extend away from the lens frames respectively and may be positioned behind a user s ears to secure the HMD to the user. The extending side arms may further secure the HMD to the user by extending around a rear portion of the user s head. Additionally or alternatively for example the HMD may connect to or be affixed within a head mounted helmet structure. Other configurations for an HMD are also possible.

The HMD may also include an on board computing system an image capture device a sensor and a finger operable touch pad . The on board computing system is shown to be positioned on the extending side arm of the HMD however the on board computing system may be provided on other parts of the HMD or may be positioned remote from the HMD e.g. the on board computing system could be wire or wirelessly connected to the HMD . The on board computing system may include a processor and memory for example. The on board computing system may be configured to receive and analyze data from the image capture device and the finger operable touch pad and possibly from other sensory devices user interfaces or both and generate images for output by the lens elements and .

The image capture device may be for example a camera that is configured to capture still images and or to capture video. In the illustrated configuration image capture device is positioned on the extending side arm of the HMD however the image capture device may be provided on other parts of the HMD . The image capture device may be configured to capture images at various resolutions or at different frame rates. Many image capture devices with a small form factor such as the cameras used in mobile phones or webcams for example may be incorporated into an example of the HMD .

Further although illustrates one image capture device more image capture devices may be used and each may be configured to capture the same view or to capture different views. For example the image capture device may be forward facing to capture at least a portion of the real world view perceived by the user. This forward facing image captured by the image capture device may then be used to generate an augmented reality where computer generated images appear to interact with or overlay the real world view perceived by the user.

The sensor is shown on the extending side arm of the HMD however the sensor may be positioned on other parts of the HMD . For illustrative purposes only one sensor is shown. However in an example embodiment the HMD may include multiple sensors. For example an HMD may include sensors such as one or more gyroscopes one or more accelerometers one or more magnetometers one or more image sensors one or more infrared sensors and or one or more microphones. Other sensing devices may be included in addition or in the alternative to the sensors that are specifically identified herein.

The finger operable touch pad is shown on the extending side arm of the HMD . However the finger operable touch pad may be positioned on other parts of the HMD . Also more than one finger operable touch pad may be present on the HMD . The finger operable touch pad may be used by a user to input commands. The finger operable touch pad may sense at least one of a pressure position and or a movement of one or more fingers via capacitive sensing resistance sensing or a surface acoustic wave process among other possibilities. The finger operable touch pad may be capable of sensing movement of one or more fingers simultaneously in addition to sensing movement in a direction parallel or planar to the pad surface in a direction normal to the pad surface or both and may also be capable of sensing a level of pressure applied to the touch pad surface. In some embodiments the finger operable touch pad may be formed of one or more translucent or transparent insulating layers and one or more translucent or transparent conducting layers. Edges of the finger operable touch pad may be formed to have a raised indented or roughened surface so as to provide tactile feedback to a user when the user s finger reaches the edge or other area of the finger operable touch pad . If more than one finger operable touch pad is present each finger operable touch pad may be operated independently and may provide a different function.

In a further aspect HMD may be configured to receive user input in various ways in addition or in the alternative to user input received via finger operable touch pad . For example on board computing system may implement a speech to text process and utilize a syntax that maps certain spoken commands to certain actions. In addition HMD may include one or more microphones via which a wearer s speech may be captured. Configured as such HMD may be operable to detect spoken commands and carry out various computing functions that correspond to the spoken commands.

As another example HMD may interpret certain head movements as user input. For example when HMD is worn HMD may use one or more gyroscopes and or one or more accelerometers to detect head movement. The HMD may then interpret certain head movements as being user input such as nodding or looking up down left or right. An HMD could also pan or scroll through graphics in a display according to movement. Other types of actions may also be mapped to head movement.

As yet another example HMD may interpret certain gestures e.g. by a wearer s hand or hands as user input. For example HMD may capture hand movements by analyzing image data from image capture device and initiate actions that are defined as corresponding to certain hand movements.

As a further example HMD may interpret eye movement as user input. In particular HMD may include one or more inward facing image capture devices and or one or more other inward facing sensors not shown sense a user s eye movements and or positioning. As such certain eye movements may be mapped to certain actions. For example certain actions may be defined as corresponding to movement of the eye in a certain direction a blink and or a wink among other possibilities.

HMD also includes a speaker for generating audio output. In one example the speaker could be in the form of a bone conduction speaker also referred to as a bone conduction transducer BCT . Speaker may be for example a vibration transducer or an electroacoustic transducer that produces sound in response to an electrical audio signal input. The frame of HMD may be designed such that when a user wears HMD the speaker contacts the wearer. Alternatively speaker may be embedded within the frame of HMD and positioned such that when the HMD is worn speaker vibrates a portion of the frame that contacts the wearer. In either case HMD may be configured to send an audio signal to speaker so that vibration of the speaker may be directly or indirectly transferred to the bone structure of the wearer. When the vibrations travel through the bone structure to the bones in the middle ear of the wearer the wearer can interpret the vibrations provided by BCT as sounds.

Various types of bone conduction transducers BCTs may be implemented depending upon the particular implementation. Generally any component that is arranged to vibrate the HMD may be incorporated as a vibration transducer. Yet further it should be understood that an HMD may include a single speaker or multiple speakers. In addition the location s of speaker s on the HMD may vary depending upon the implementation. For example a speaker may be located proximate to a wearer s temple as shown behind the wearer s ear proximate to the wearer s nose and or at any other location where the speaker can vibrate the wearer s bone structure.

The lens elements may act as a combiner in a light projection system and may include a coating that reflects the light projected onto them from the projectors . In some embodiments a reflective coating may not be used e.g. when the projectors are scanning laser devices .

In alternative embodiments other types of display elements may also be used. For example the lens elements themselves may include a transparent or semi transparent matrix display such as an electroluminescent display or a liquid crystal display one or more waveguides for delivering an image to the user s eyes or other optical elements capable of delivering an in focus near to eye image to the user. A corresponding display driver may be disposed within the frame elements for driving such a matrix display. Alternatively or additionally a laser or LED source and scanning system could be used to draw a raster display directly onto the retina of one or more of the user s eyes. Other possibilities exist as well.

As shown in the HMD may include a single display which may be coupled to the device. The display may be formed on one of the lens elements of the HMD such as a lens element described with respect to and may be configured to overlay computer generated graphics in the user s view of the physical world. The display is shown to be provided in a center of a lens of the HMD however the display may be provided in other positions such as for example towards either the upper or lower portions of the wearer s field of view. The display is controllable via the computing system that is coupled to the display via an optical waveguide .

The HMD may include a single display which may be coupled to one of the side arms via the component housing . In an example embodiment the display may be a see through display which is made of glass and or another transparent or translucent material such that the wearer can see their environment through the display . Further the component housing may include the light sources not shown for the display and or optical elements not shown to direct light from the light sources to the display . As such display may include optical features that direct light that is generated by such light sources towards the wearer s eye when HMD is being worn.

In a further aspect HMD may include a sliding feature which may be used to adjust the length of the side arms . Thus sliding feature may be used to adjust the fit of HMD . Further an HMD may include other features that allow a wearer to adjust the fit of the HMD without departing from the scope of the invention.

In the illustrated example the display may be arranged such that when HMD is worn display is positioned in front of or proximate to a user s eye when the HMD is worn by a user. For example display may be positioned below the center frame support and above the center of the wearer s eye as shown in . Further in the illustrated configuration display may be offset from the center of the wearer s eye e.g. so that the center of display is positioned to the right and above of the center of the wearer s eye from the wearer s perspective .

Configured as shown in display may be located in the periphery of the field of view of the wearer when HMD is worn. Thus as shown by when the wearer looks forward the wearer may see the display with their peripheral vision. As a result display may be outside the central portion of the wearer s field of view when their eye is facing forward as it commonly is for many day to day activities. Such positioning can facilitate unobstructed eye to eye conversations with others as well as generally providing unobstructed viewing and perception of the world within the central portion of the wearer s field of view. Further when the display is located as shown the wearer may view the display by e.g. looking up with their eyes only possibly without moving their head . This is illustrated as shown in where the wearer has moved their eyes to look up and align their line of sight with display . A wearer might also use the display by tilting their head down and aligning their eye with the display .

Other image capture devices or digital cameras may be standalone devices or integrated with other devices. As an example illustrates the form factor of a digital camera device . Digital camera device may be for example a mobile phone a tablet computer or a wearable computing device. However other embodiments are possible. Digital camera device may include various elements such as a body a front facing camera a multi element display a shutter button and other buttons . Front facing camera may be positioned on a side of body typically facing a user while in operation or on the same side as multi element display .

Digital camera device could further include two rear facing cameras and . Rear facing cameras and may be positioned on a side of body opposite front facing camera . Note that referring to the cameras as front and rear facing is arbitrary and digital camera device may include multiple cameras positioned on various sides of body . Further the lenses of rear facing cameras and are arranged on the upper corner on the back of digital camera device and are oriented in substantially the same direction. Note that herein references to cameras being oriented in the same direction should be understood to mean that the lenses of the cameras point in substantially the same direction. 

It should be understood that other multi camera arrangements are possible. In particular the lenses of two or more cameras which are all oriented in substantially the same direction may be arranged in different formations on a surface of the phone.

As noted above the functions of digital camera device or another type of digital camera may be integrated into or take the form of a computing device such as a mobile phone tablet computer laptop computer and so on. For purposes of example is a simplified block diagram showing some of the components of an example computing device that may include camera components . Camera components may include multiple cameras such as cameras and .

By way of example and without limitation computing device may be a cellular mobile telephone e.g. a smartphone a still camera a video camera a fax machine a computer such as a desktop notebook tablet or handheld computer a personal digital assistant PDA a home automation component a digital video recorder DVR a digital television a remote control a wearable computing device or some other type of device equipped with at least some image capture and or image processing capabilities. It should be understood that computing device may represent a physical camera device such as a digital camera a particular physical hardware platform on which a camera application operates in software or other combinations of hardware and software that are configured to carry out camera functions.

As shown in computing device may include a communication interface a user interface a processor data storage and camera components all of which may be communicatively linked together by a system bus network or other connection mechanism .

Communication interface may function to allow computing device to communicate using analog or digital modulation with other devices access networks and or transport networks. Thus communication interface may facilitate circuit switched and or packet switched communication such as plain old telephone service POTS communication and or Internet protocol IP or other packetized communication. For instance communication interface may include a chipset and antenna arranged for wireless communication with a radio access network or an access point. Also communication interface may take the form of or include a wireline interface such as an Ethernet Universal Serial Bus USB or High Definition Multimedia Interface HDMI port. Communication interface may also take the form of or include a wireless interface such as a Wifi BLUETOOTH global positioning system GPS or wide area wireless interface e.g. WiMAX or 3GPP Long Term Evolution LTE . However other forms of physical layer interfaces and other types of standard or proprietary communication protocols may be used over communication interface . Furthermore communication interface may comprise multiple physical communication interfaces e.g. a Wifi interface a BLUETOOTH interface and a wide area wireless interface .

User interface may function to allow computing device to interact with a human or non human user such as to receive input from a user and to provide output to the user. Thus user interface may include input components such as a keypad keyboard touch sensitive or presence sensitive panel computer mouse trackball joystick microphone and so on. User interface may also include one or more output components such as a display screen which for example may be combined with a presence sensitive panel. The display screen may be based on CRT LCD and or LED technologies or other technologies now known or later developed. User interface may also be configured to generate audible output s via a speaker speaker jack audio output port audio output device earphones and or other similar devices.

In some embodiments user interface may include a display that serves as a viewfinder for still camera and or video camera functions supported by computing device . Additionally user interface may include one or more buttons switches knobs and or dials that facilitate the configuration and focusing of a camera function and the capturing of images e.g. capturing a picture . It may be possible that some or all of these buttons switches knobs and or dials are implemented as functions on a touch or proximity sensitive panel.

Processor may comprise one or more general purpose processors e.g. microprocessors and or one or more special purpose processors e.g. digital signal processors DSPs graphics processing units GPUs floating point units FPUs network processors or application specific integrated circuits ASICs . In some instances special purpose processors may be capable of image processing image alignment and merging images among other possibilities. Data storage may include one or more volatile and or non volatile storage components such as magnetic optical flash or organic storage and may be integrated in whole or in part with processor . Data storage may include removable and or non removable components.

Processor may be capable of executing program instructions e.g. compiled or non compiled program logic and or machine code stored in data storage to carry out the various functions described herein. Therefore data storage may include a non transitory computer readable medium having stored thereon program instructions that upon execution by computing device cause computing device to carry out any of the methods processes or functions disclosed in this specification and or the accompanying drawings. The execution of program instructions by processor may result in processor using data .

By way of example program instructions may include an operating system e.g. an operating system kernel device driver s and or other modules and one or more application programs e.g. camera functions address book email web browsing social networking and or gaming applications installed on computing device . Similarly data may include operating system data and application data . Operating system data may be accessible primarily to operating system and application data may be accessible primarily to one or more of application programs . Application data may be arranged in a file system that is visible to or hidden from a user of computing device .

Application programs may communicate with operating system through one or more application programming interfaces APIs . These APIs may facilitate for instance application programs reading and or writing application data transmitting or receiving information via communication interface receiving and or displaying information on user interface and so on.

In some vernaculars application programs may be referred to as apps for short. Additionally application programs may be downloadable to computing device through one or more online application stores or application markets. However application programs can also be installed on computing device in other ways such as via a web browser or through a physical interface e.g. a USB port on computing device .

Camera components may include but are not limited to an aperture shutter recording surface e.g. photographic film and or an image sensor lens and or shutter button. Camera components may be controlled at least in part by software executed by processor . Further camera components may include multiple camera systems which each include an aperture shutter recording surface lens image sensor processor and or shutter button.

When multiple camera systems are included there may be some components that are shared between the systems and other components that are not shared. For example each camera could include its own aperture lens and image sensor while sharing other components such as a processor and a shutter button. As another example each camera could include its own lens but share the same image sensor. Alternatively each camera system s components may be utilized only for that camera system and not shared with other camera systems.

It should be noted that dimensions of the dual imaging system or the object represented in may not be to scale and are for illustrative purposes only. It should also be noted that any depicted angles and or directions of refraction or reflection depicted in are purely for illustrative purposes and are not necessarily to scale.

The optical axis may define an axis of rotational symmetry or other axis of symmetry for image sensors A and B the aperture stops A and B the first reflector A the lens B the second reflector A and the dual imaging system as a whole. For example a light ray such as the light ray that travels along the optical axis may pass through a second aperture defined by the aperture stop B pass through the lens B and reach the second image sensor B without being refracted i.e. having its direction of travel changed . The second aperture may be a disc shaped portion of a plane that is surrounded by the second aperture stop B.

The image sensors A and B may be configured to capture images of light incident upon the image sensors A and B from a common viewpoint and to provide data to a computing system i.e. via an input output interface representing the respective captured images. The image sensors A and B may include a CMOS complementary metal oxide semiconductor sensor or a CCD charge coupled device sensor among other possibilities. The image sensors A and B may be aligned perpendicularly to the optical axis and face the same direction upward in this example . In this example the first image sensor A is located below the second image sensor B and along the optical axis .

The first wall A may provide structural support for the dual imaging system . For example the first image sensor A may be mounted to a bottom interior portion of the first wall A and or side portions of the first wall A. The first reflector A and the second reflector A may also be mounted to a bottom or side portion of the first wall A. Or the second reflector A may be mounted to the first wall A with narrow radial support beams or to a window located coplanar with the first aperture. Structural descriptions included herein are included for illustrative purposes only. Other structural examples are possible.

The first imaging system comprising the first image sensor A the first aperture stop A the first reflector A and the second reflector A may be configured to capture images of the object . For example the light ray may travel from the first end A of the object past the second imaging system and be reflected by the first reflector A toward the second reflector A. The light ray may further be reflected by the second reflector A and become incident upon the first image sensor A.

The first reflector A may be a curved section of glass or other material coated with a smooth reflective metal layer on one or more surfaces among other possibilities. The first reflector A may be a parabolic or concave reflector configured to reflect incident light rays travelling downward with respect to the optical axis toward a focus of the first reflector A. The focus of the first reflector A may be located above the second reflector A but is not depicted in . The focus of the first reflector A may also be a first focus of the second reflector A. The first reflector A may be symmetrically aligned along the optical axis and located below the second image sensor B.

The second reflector A may also be a curved section of glass or other material coated with a smooth reflective metal layer on one or more surfaces among other possibilities. The second reflector A may be a hyperbolic or convex reflector configured to reflect incident light rays toward a second focus not shown of the second reflector A. The second focus of the second reflector A may be below the first image sensor A. The first reflector A may include an opening centered along the optical axis that allows light rays reflected by the second reflector A to pass through the opening to the first image sensor A.

Similar to the light ray the light ray may travel from the second end B of the object past the second imaging system and be reflected by the first reflector A toward the second reflector A. The second reflector A may reflect the light ray so that it is incident upon the first image sensor A. In this way a real image of the object may be formed upon the first image sensor A. Rays of light that originate from a point on the optical axis and travel past the second imaging system such as light ray may be directed by components of the first imaging system to a point on the first image sensor A that is on the optical axis .

The first aperture stop A may surround a first aperture of the first imaging system and the second aperture stop B may surround a second aperture of the second imaging system. The second aperture may be located between the first aperture and the second image sensor B. In another example the first aperture may be located between the second image sensor B and the second reflector A or between the second aperture and the second reflector A. Both the first aperture and the second aperture may be disc shaped portions of planes respectively surrounded by the first and second aperture stops A and B. Light that reaches the first image sensor A may cross the first aperture while light that reaches the second image sensor B may cross the first and second apertures. The diameter of the first aperture may vary as the inner diameter of the first aperture stop A is varied.

In another sense the first aperture stop A and the second aperture stop B may together define a third annular aperture corresponding to the first imaging system. Light that reaches the first image sensor A may cross the annular aperture. See for more detail of the annular aperture. The aperture stops A and B may be adjustable so that the first and second image sensors A and B are configured to capture an image with a common viewpoint and or with a common field of view.

The second imaging system with respect to the optical axis may be located radially within the first wall A of the first imaging system. The second imaging system could be located anywhere along the optical axis with respect to the first imaging system. For example the second aperture defined by the second aperture stop B could be located in front of or behind the first aperture defined by the first aperture stop A. Also the second image sensor could be located in front of or behind the first aperture defined by the first aperture stop A.

The second image sensor B may be located at an image plane of the lens B that corresponds to the object . The location of the image plane of the lens B may be determined by equation 1 1 1 1 1 

In equation 1 and S may represent a distance between the object and the lens B along the optical axis S may represent a distance between the image plane i.e. the second image sensor B and the lens B along the optical axis while f may represent a focal length of the lens B.

The lens B may be a piece of glass or other transparent material machined and or polished to focus light in accordance with embodiments disclosed herein. For example the lens B may be configured to focus light incident upon the lens B to produce a real image of the object upon the second image sensor B. Two of the light rays making up the real image of the object are light rays and which respectively represent the first end A and the second end B of the object .

The second aperture stop B may define the second aperture through which light may pass and be captured by the second image sensor B. The second aperture stop B may have a fixed diameter or may be adjustable to create second apertures of varying diameters. The second aperture corresponding to the second aperture stop B may be a disc shaped portion of a plane parallel to the second aperture stop B. In other embodiments the second aperture stop B may define other shapes of apertures such as a non circular aperture. An adjustable second aperture stop B may define a variable field of view of the second imaging system.

The second wall B may provide structural support for the second imaging system. For example the second image sensor B may be mounted to a bottom portion of the second wall B and or side portions of the second wall B. The lens B may be mounted to side portions of the second wall B. The second wall B may be mounted to a top non reflective surface of the second reflector A. Structural descriptions included herein are included for illustrative purposes only. Other structural examples are possible.

The lens B may be configured to refract light incident upon the lens B onto the second image sensor B so that the second image sensor B may capture images. The second image sensor B may lie along the image plane of the lens B.

It should be noted that dimensions of the imaging system or the object represented in may not be to scale and are for illustrative purposes only. It should also be noted that any depicted angles and or directions of refraction or reflection depicted in are purely for illustrative purposes and are not necessarily to scale.

The optical axis may define an axis of rotational symmetry or other axis of symmetry for image sensors A and B the aperture stops A and B the lens B the first reflector A and the dual imaging system as a whole. For example a light ray such as the light ray that travels along the optical axis may pass through the lens B and reach the second image sensor B without being refracted i.e. having its direction of travel changed .

The image sensors A and B may be configured to capture images of light incident upon the image sensors A and B and to provide data to a computing system i.e. via an input output interface representing the respective captured images. The image sensors A and B may include a CMOS complementary metal oxide semiconductor sensor or a CCD charge coupled device sensor among other possibilities. The image sensors A and B may be aligned perpendicularly to the optical axis but facing opposite directions. In this example the first image sensor A may face downward while the second image sensor B may face upward.

The first wall A may provide structural support for the imaging system. Components of the dual imaging system may be mounted i.e. mechanically coupled to bottom or side interior portions of the first wall A. The first reflector A may be mounted to a bottom or side portion of the first wall A. Structural descriptions included herein are included for illustrative purposes only. Other structural examples are possible.

The first image sensor A the first aperture stop A and the first reflector A may be configured to capture images of the object . For example the light ray may travel from the first end A of the object past the second imaging system and be reflected by the first reflector A and become incident upon the first image sensor A.

The first reflector A may be a curved section of glass coated with a smooth reflective metal layer on one or more surfaces among other possibilities. The first reflector A may be a parabolic or concave reflector configured to reflect incident light rays toward a focus of the first reflector located above the first image sensor A. However light rays may become incident upon the first image sensor A before the rays reach the focus of the first reflector A. The first reflector A may be symmetrically aligned along the optical axis and located below the first image sensor A.

Similar to the light ray the light ray may travel from the second end B of the object past the second imaging system and be reflected by the first reflector A toward the first image sensor A. In this way a real image of the object is formed upon the first image sensor A. Rays of light that originate from a point on the optical axis and travel past the second imaging system such as light ray may be reflected by the first reflector A to a point on the first image sensor A that is on the optical axis .

The first aperture stop A may surround a first aperture of the first imaging system and the second aperture stop B may surround a second aperture of the second imaging system. The second aperture may be located between the first aperture and the second image sensor B. Both the first aperture and the second aperture may be disc shaped portions of planes respectively surrounded by the first and second aperture stops A and B. Light that reaches the first image sensor A may cross the first aperture while light that reaches the second image sensor B may cross the first and second apertures. The diameter of the first aperture may vary as the inner diameter of the first aperture stop A is varied.

In another sense the first aperture stop A and the second aperture stop B may together define a third annular aperture corresponding to the first imaging system. Light that reaches the first image sensor A may cross the annular aperture. See for more detail of the annular aperture. The aperture stops A and B may be adjustable so that the first and second image sensors A and B are configured to capture an image with a common viewpoint and or with a common field of view.

The second imaging system with respect to the optical axis may be located radially within the first wall A of the first imaging system. The second imaging system could be located anywhere along the optical axis with respect to the first imaging system. For example the second aperture defined by the second aperture stop B could be located in front of or behind the first aperture defined by the first aperture stop A. Also the second image sensor could be located in front of or behind the first aperture defined by the first aperture stop A.

The second image sensor B may be located at an image plane of the lens B that corresponds to the object . The location of the image plane of the lens B may be determined by equation 1 discussed above. In equation 1 and S may represent a distance between the object and the lens B along the optical axis S may represent a distance between the image plane i.e. the second image sensor B and the lens B along the optical axis while f may represent a focal length of the lens B.

The lens B may be a piece of glass or other transparent material machined to focus light in accordance with embodiments disclosed herein. For example the lens B may be configured to focus light incident upon the lens B to produce a real image of object upon the second image sensor B. Two of the rays making up the real image of the object are rays and which respectively represent the first end A and the second end B of the object .

The second aperture stop B may define a second aperture through which light may pass and be captured by the second image sensor B. The second aperture stop B may have a fixed diameter or may be adjustable to create second apertures of varying diameters. The second aperture corresponding to the second aperture stop B may be a disc shaped portion of a plane parallel to and surrounded by the second aperture stop B. In other embodiments the second aperture stop B may define other shapes of apertures such as a non circular aperture. An adjustable second aperture stop B may also define a variable field of view of the second imaging system.

The second wall B may provide structural support for the dual imaging system i.e. components of the dual imaging system may be mechanically coupled to the second wall B . For example the second image sensor B may be mounted to a bottom portion of the second wall B and or side portions of the second wall B. The lens B may be mounted to side portions of the second wall B. The second wall B may be mounted to a top non light sensitive surface of the first image sensor A. Structural descriptions included herein are included for illustrative purposes only. Other structural examples are possible.

The lens B may be configured to refract light incident upon the lens B onto the second image sensor B so that the second image sensor B may capture images. The second image sensor B may lie along the image plane of the lens B.

As shown in the second image sensor B may be mechanically coupled to a structure that includes the second aperture stop B and the second wall B. The second aperture stop B may be configured to increase or decrease its inner diameter as shown at . Such changes in the inner diameter of the second aperture stop B may change an amount of light that reaches the second image sensor B and a field of view of the second imaging system. The image of the object of may be made up of light rays and of among other light rays. Toward the outer edge of the imaging system is the first aperture stop A. The first aperture stop A may also be adjustable and define an annular aperture of the first imaging system with an outer radius defined by the first aperture stop A and the inner radius defined by the second wall B or the second aperture stop B.

It should be noted that dimensions of the imaging system or the object represented in may not be to scale and are for illustrative purposes only. It should also be noted that any depicted angles and or directions of refraction or reflection depicted in are purely for illustrative purposes and are not necessarily to scale.

The optical axis may define an axis of rotational symmetry or other axis of symmetry for the image sensors A and B the aperture stops A and B the plurality of lenses B the lens A the first reflector A the second reflector A the lenses A and A and the dual imaging system as a whole. For example a light ray such as the light ray that travels along the optical axis may pass through the plurality of lenses B and reach the second image sensor B without being refracted i.e. having its direction of travel changed .

The image sensors A and B may be configured to capture images of light incident upon the image sensors A and B and to provide data to a computing system i.e. via an input output interface representing the respective captured images. The image sensors A and B may include a CMOS complementary metal oxide semiconductor sensor or a CCD charge coupled device sensor among other possibilities. The image sensors A and B may be aligned perpendicularly to the optical axis facing upward with respect to the optical axis .

The first wall A may provide structural support to the dual imaging system . Components of the dual imaging system may be mounted i.e. mechanically coupled to bottom or side interior portions of the first wall A. Components of the first imaging system such as the lenses A A and A the first reflector A and the second reflector A may also be mounted to a bottom or side portion of the first wall A. Structural descriptions included herein are included for illustrative purposes only. Other structural examples are possible.

The first image sensor A the first aperture stop A the lenses A A and A the first reflector A and the second reflector A may be configured to capture images of the object . For example the light ray may travel from the first end A of the object be refracted by the lens A travel past the second imaging system be reflected sequentially by the first reflector A and the second reflector A be refracted by the lenses A and A and become incident upon the first image sensor A.

The reflectors A and A may be curved sections of glass or other material coated with a smooth reflective metal layer on one or more surfaces among other possibilities. The reflectors A and A may also be dual purpose reflector refractors each having a front refractive element and a back reflective surface. For example light may be refracted through the front refractive element be reflected by the back reflective surface and be refracted by the front refractive element as the light passes through the refractive element again. The lenses A A and A may be pieces of glass or other translucent material machined to focus light in accordance with embodiments disclosed herein.

Similar to the light ray the light ray may travel from the second end B of the object be refracted by the lens A travel past the second imaging system be reflected sequentially by the first reflector A and the second reflector A be refracted by the lenses A and A and become incident upon the first image sensor A. In this way a real image of the object is formed upon the first image sensor A. Note that rays of light that originate from a point on the optical axis and travel past the second imaging system such as light ray may be redirected by the first imaging system to a point on the first image sensor A that is on the optical axis .

The first aperture stop A may surround a first aperture of the first imaging system and the second aperture stop B may surround a second aperture of the second imaging system. The second aperture may be located between the first aperture and the second image sensor B. In other examples the first aperture may be located between the second aperture and the second reflector A. Both the first aperture and the second aperture may be disc shaped portions of planes respectively surrounded by the first and second aperture stops A and B. Light that reaches the first image sensor A may cross the first aperture while light that reaches the second image sensor B may cross the first and second apertures. The diameter of the first aperture may vary as the inner diameter of the first aperture stop A is varied.

In another sense the first aperture stop A and the second aperture stop B may together define a third annular aperture corresponding to the first imaging system. Light that reaches the first image sensor A may cross the annular aperture. See for more detail of the annular aperture. The aperture stops A and B may be adjustable so that the first and second image sensors A and B are configured to capture an image with a common viewpoint and or with a common field of view.

The second imaging system with respect to the optical axis may be located radially within the first wall A of the first imaging system. The second imaging system could be located anywhere along the optical axis with respect to the first imaging system. For example the second aperture defined by the second aperture stop B could be located in front of or behind the first aperture defined by the first aperture stop A. Also the second image sensor could be located in front of or behind the first aperture defined by the first aperture stop A.

The second image sensor B may be located at an image plane of the plurality of lenses B that corresponds to the object . For example the plurality of lenses B may be configured to focus light incident upon the plurality of lenses B to produce a real image of object upon the second image sensor B. Two of the rays making up the real image of the object are rays and which respectively represent the first end A and the second end B of the object .

The second aperture stop B may define a second aperture through which light may pass and be captured by the second image sensor B. The second aperture stop B may have a fixed diameter or may be adjustable to create second apertures of varying diameters. The second aperture corresponding to the second aperture stop B may be a disc shaped portion of a plane parallel to and surrounded by the second aperture stop B. In other embodiments the second aperture stop B may define other shapes of apertures such as a non circular aperture. An adjustable second aperture stop B may define a variable field of view of the second imaging system. In this example the second aperture is located between the first aperture and the second image sensor B.

The second wall B may provide structural support for the dual imaging system i.e. components of the dual imaging system may be mechanically coupled to the second wall B . For example the second image sensor B may be mounted to a bottom portion of the second wall B and or side portions of the second wall B. The plurality of lenses B may be mounted to side portions of the second wall B. The second wall B may be mounted to a top non light sensitive surface of the second reflector A. Structural descriptions included herein are included for illustrative purposes only. Other structural examples are possible.

The plurality of lenses B may be configured to refract light incident upon the plurality of lenses B onto the second image sensor B so that the second image sensor B may capture images. The second image sensor B may lie along an image plane of the plurality of lenses B corresponding to the object .

For example the user may wish to view an image frame that corresponds with the narrower field of view of the second camera. In this case the user may touch a designated area of a touchscreen display indicating that the imaging system should display a zoomed in image frame or provide such an input in another way. Instead of instantaneously switching the live view interface to display image frames captured by the second camera the live view interface may display a transitional field of view that facilitates a transition from the wide field of view of the first camera to the narrower field of view of the second camera. To accomplish this transition the imaging system may perform image processing methods described below with the image frame s captured by the first and second cameras.

Equation 1 could be used if the composite image frame being generated is a grayscale image. Equation 1 describes a weighted average of brightness values of corresponding respective pixels of the expanded image frame and the reduced image frame . For example consider pixel which is located far within the stitching boundary . According to the algorithm a feathering factor FF to be applied at pixel may be 0.1. As an example a brightness value of the pixel may be 50 B on the expanded image frame and 70 B on the reduced image frame yielding a brightness value of 68 for the pixel of the composite image frame Brightness 0.7 1 0.1 0.1 0.5 68 . Accordingly since the pixel lies much closer to the center than the stitching boundary the algorithm determines the brightness of the pixel of the composite image frame to have a value that is much closer to the brightness 70 of the pixel of the reduced image frame rather than the brightness 50 of the pixel of the expanded image frame .

Similarly at pixel along the stitching boundary the feathering algorithm may result in a brightness value of 60 Brightness 0.7 1 0.5 0.5 0.5 60 . According to the algorithm pixels of the composite image frame that lie on the stitching boundary will have a brightness equal to an unweighted average of the brightness of corresponding pixels of the expanded image frame and the reduced image frame .

The algorithm may similarly determine red green and blue color values for pixels of the composite image frame using equations 2 4. For example on an 8 bit color scale of 0 256 pixel of the reduced image frame may have a blue color value of 150 while pixel of the expanded image frame may have a blue color value of 200. According to the algorithm the blue color value of pixel of the composite image frame is determined to be Blue Blue 1 FF FF Blue 150 1 0.1 0.1 200 155 . Since the pixel is much closer to the center of the composite image than the stitching boundary it takes on a blue color value closer to that of the reduced image frame than that of the expanded image frame . On the other hand pixel of the composite image frame may have a blue value of 175 Blue Blue 1 FF FF Blue 150 1 0.5 0.5 200 175 . Since the pixel is on the stitching boundary the pixel takes on a blue color value equally weighted between that of the expanded image frame and the reduced image frame . Red and green color weighting of pixels of the composite image frame are generated in a similar manner using equations 2 and 3. Feathering factors provided for pixels and are arbitrary and included as examples only.

As the first image frame is being displayed within the live view interface the user may provide input indicating that the live view interface should transition to displaying a second image stream captured by a second camera of the imaging system having a second field of view that is narrower than the first field of view.

In response to receiving the input the imaging system may perform image processing steps described above to image frames of the first and second image streams. For instance illustrates a composite image frame made up of an expanded image frame B representing the first image stream and a reduced image frame B representing the second image stream. A stitching boundary may represent a boundary between the reduced image frame B and the expanded image frame B that make up the composite image frame . The composite image frame represents a narrower field of view than that of the first image frame . In this way the live view interface begins to transition from displaying a wide angle first image frame to displaying a narrower field of view represented by the composite image frame . In one aspect composite images such as the composite image could represent a transitional field of view that is displayed in the live view display between display of the first field of view and the second field of view. Note that in and in an entire image frame of the second image stream is displayed in the live view interface while only a portion of an image frame of the first image stream is displayed in the live view interface. Blurring techniques such as those depicted in may be applied to the composite image frame as shown at a blurred region B that lies within and near the stitching boundary .

The transition between the live view interface displaying the first image stream and the second image stream is further illustrated in . When compared to the composite image frame the field of view of the composite image frame is even narrower. As compared to the reduced image frame B and the expanded image frame B the reduced image frame C makes up a larger proportion of the composite image frame and the expanded image frame C makes up a smaller proportion of the composite image frame . Blurring techniques such as those depicted in may be applied to the composite image frame as shown at a blurred region C that lies within and near the stitching boundary .

The transition between the live view interface displaying the first image stream and displaying the second image stream is further illustrated in . When compared to the composite image frame the field of view of the composite image frame is even narrower. As compared to the reduced image frame C and the expanded image frame C the reduced image frame D makes up a larger proportion of the composite image frame and the expanded image frame D makes up a smaller proportion of the composite image frame . Blurring techniques such as those depicted in may be applied to the composite image frame as shown at a blurred region D that lies within and near the stitching boundary .

As shown in the transition between the live view interface displaying the first image stream and displaying the second image stream is complete. The live view interface now displays the second image frame .

Note that in a common instance of time is depicted in the composite image frames for ease of illustration. However in some examples the composite image frames could respectively represent a real time sequence of image frames that depict successive and distinct moments in time. Also while depict a sequence of image frames representing a zoom in sequence a zoom out sequence could easily be performed by starting with display of the second image stream and reversing the sequence of the display.

At block the method includes operating a first camera having a first field of view of an environment to capture a first image stream representing the first field of view. For instance any camera or imaging system of any of the preceding description could capture the first image stream.

At block the method includes operating a second camera having a second field of view of the environment to capture a second image stream representing the second field of view. The first field of view is wider than the second field of view. By way of example any camera or imaging system of any of the preceding description could capture the second image stream.

At block the method includes initially using the first image stream to display the first field of view in a live view interface displayed on a graphic display. See for example. 

At block the method includes while displaying the first image stream in the live view interface receiving an input corresponding to a zoom command. The input could be received at a designated zoom button area of a touchscreen of the graphic display for example. A zoom command could also be received using a slider bar interface in which a user touches and drags an indicator horizontally or vertically across the touchscreen a distance that indicates a given degree of zoom desired. In another example a single tap or two taps in quick succession could represent a command to instantaneously or incrementally zoom from the first image stream to the second image stream or vice versa . For example one tap could indicate a 33 zoom while another tap indicates another 33 zoom and so on. Or a tap and hold input could initiate a continuous zoom that is halted when the tap and hold input is released. A rate of zoom could be determined by prior user input as well. Inputs could also be received via mechanical buttons or via a voice command module and microphone listening for keywords or voice commands. Other examples are possible.

Generally systems and methods disclosed here may include the application of various effects to a live view representation viewable from a display. depicts a graphical user interface according to an illustrative embodiment. Scenario could include a soccer player preparing to kick a ball toward a soccer goal and goalkeeper. Scenario may also represent a live view representation viewable by a user of the HMD or another display type. As such the live view representation may include a user interface. The user interface may include icons words and or other indicators e.g. lights and or sounds that may be associated with effects applicable to the live view representation. For example the icons may include a slow motion icon a zoom icon a select icon a bokeh icon and an other icon . The live view representation may further include a soccer player a ball a goalkeeper and a net .

User interaction via the touch based control interface may cause an effect to be selected. Such a selection may cause the corresponding icon to be highlighted illuminated underlined enlarged or otherwise identified. For example the slow motion icon may have a lighted outline as depicted in .

Referring back to at block the method includes in response to receiving the input a switching from using the first image stream to display the first field of view in the live view interface to using a combination of the first image stream and the second stream to display a transitional field of view of the environment in the live view interface and b subsequently switching to using the second image stream to display the second field of view in the live view interface. Displaying the transitional field of view may include displaying a sequence of composite image frames wherein each successive frame of the sequence represents a narrower field of view than a previous frame in the sequence. For example each image frame displayed in the sequence could represent an entirety of the second field of view and a portion of the first field of view that is narrower than a portion of the first field of view represented by a preceding image frame of the sequence. See for example. 

At block the method includes operating a first camera having a first field of view of an environment to capture a first image stream representing the first field of view.

At block the method includes operating a second camera having a second field of view of the environment to capture a second image stream representing the second field of view. The second field of view is wider than the first field of view.

At block the method includes initially using the first image stream to display the first field of view in a live view interface. The live view interface is displayed on a graphic display. See for example. 

At block the method includes while displaying the first image stream in the live view interface receiving an input corresponding to a zoom command.

At block the method includes in response to receiving the input a switching from using the first image stream to display the first field of view in the live view interface to using a combination of the first image stream and the second stream to display a transitional field of view of the environment in the live view interface and b subsequently switching to using the second image stream to display the second field of view in the live view interface. For example each image frame displayed in the sequence could represent an entirety of the first field of view and a portion of the second field of view that is wider than a portion of the second field of view represented by a preceding image frame of the sequence. See in reverse order for example. 

At block the method includes cropping the first image frame to generate a cropped image frame representing a field of view wider than the second field of view. See for example. 

At block the method includes using the cropped image frame to generate an expanded image frame representing a i field of view equal to the field of view represented by the cropped image frame and ii a pixel resolution greater than a pixel resolution of the cropped image frame. See for example. 

At block the method includes simultaneous to capturing the first image frame capturing a second image frame using the second camera. Respective pixel resolutions of the first and second image frames may be equal. See for example. 

Capturing the first and second image frames may include disparity mitigation techniques that cause the first and second image frames to represent a common viewpoint even if the first and second cameras that respectively capture the first and second image frames do not share an optical axis. For instance a precursor image may be captured with the first camera. Various techniques may be used to determine a disparity between the captured precursor image frame and the second image frame. For instance various known depth from stereo techniques may be used to determine disparity and or extract depth information from the precursor image frame and the second image frame. Such techniques may utilize spatial optical flow between the precursor image frame and the second image frame to determine disparity information. Techniques that utilize feature matching to determine the disparity between the precursor image frame and the second image frame may also be utilized. Any of these techniques may be used to generate a first image frame that shares a viewpoint with the second image frame. Other techniques are also possible.

At block the method includes using the second image frame to generate a reduced image frame of pixel resolution less than a pixel resolution of the second image frame wherein a field of view represented by the reduced image is equal to the field of view of the second camera. See for example. 

At block the method includes overlaying the reduced image frame upon the expanded image frame to form a composite image frame of the environment. The composite image frame represents a field of view narrower than the first field of view and wider than the second field of view. In one example the reduced image frame may be overlaid upon the expanded image frame such that a center of the reduced image frame is aligned with a center of the expanded image frame within the composite image frame. See for example. 

In the figures similar symbols typically identify similar components unless context indicates otherwise. The illustrative embodiments described in the detailed description figures and claims are not meant to be limiting. Other embodiments can be utilized and other changes can be made without departing from the scope of the subject matter presented herein. It will be readily understood that the aspects of the present disclosure as generally described herein and illustrated in the figures can be arranged substituted combined separated and designed in a wide variety of different configurations all of which are explicitly contemplated herein.

With respect to any or all of the message flow diagrams scenarios and flow charts in the figures and as discussed herein each step block and or communication may represent a processing of information and or a transmission of information in accordance with example embodiments. Alternative embodiments are included within the scope of these example embodiments. In these alternative embodiments for example functions described as steps blocks transmissions communications requests responses and or messages may be executed out of order from that shown or discussed including in substantially concurrent or in reverse order depending on the functionality involved. Further more or fewer steps blocks and or functions may be used with any of the message flow diagrams scenarios and flow charts discussed herein and these message flow diagrams scenarios and flow charts may be combined with one another in part or in whole.

A step or block that represents a processing of information may correspond to circuitry that can be configured to perform the specific logical functions of a herein described method or technique. Alternatively or additionally a step or block that represents a processing of information may correspond to a module a segment or a portion of program code including related data . The program code may include one or more instructions executable by a processor for implementing specific logical functions or actions in the method or technique. The program code and or related data may be stored on any type of computer readable medium such as a storage device including a disk drive a hard drive or other storage media.

The computer readable medium may also include non transitory computer readable media such as computer readable media that stores data for short periods of time like register memory processor cache and or random access memory RAM . The computer readable media may also include non transitory computer readable media that stores program code and or data for longer periods of time such as secondary or persistent long term storage like read only memory ROM optical or magnetic disks and or compact disc read only memory CD ROM for example. The computer readable media may also be any other volatile or non volatile storage systems. A computer readable medium may be considered a computer readable storage medium for example or a tangible storage device.

Moreover a step or block that represents one or more information transmissions may correspond to information transmissions between software and or hardware modules in the same physical device. However other information transmissions may be between software modules and or hardware modules in different physical devices.

