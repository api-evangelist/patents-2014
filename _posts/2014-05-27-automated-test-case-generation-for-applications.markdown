---

title: Automated test case generation for applications
abstract: Some implementations include receiving an application binary file for an application to be tested. One or more static analysis operations may be performed on the application binary file to identify application parameters. In some cases, keywords may be associated with individual application parameters, and the keywords may be used to query a test case repository in order to identify test cases. The identified test cases may be used to generate a test plan, and at least a portion of the test plan may be automatically executed in some cases. A test report may be generated that includes a list of test case failures and potential solutions, and the test report may be sent e.g., to a third-party developer or an approval engineer for review.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09268672&OS=09268672&RS=09268672
owner: Amazon Technologies, Inc.
number: 09268672
owner_city: Seattle
owner_country: US
publication_date: 20140527
---
There may be numerous challenges associated with developing and testing a software application particularly in cases where the application is being developed by a third party developer. For example it may be difficult for a third party developer to know how to develop an application on a particular platform what to test and how to fix issues in the application before submitting the application for launch e.g. via an application store . Further it may be difficult for a tester e.g. an approval engineer to verify compatibility of an application developed by a third party developer prior to launch. For example the tester may have a large number of applications developed by third party developers for review and the tester may not have low level information on the application. As such it may be difficult for the tester to determine what to test for a particular application without potentially devoting a large amount of time to the particular application.

This disclosure includes in part techniques and arrangements for automatic test case generation. In some cases test case generation may include generating possible test cases for execution on a given application and may include identifying how to fix issue s if a particular test case fails in testing. This may be useful to testing teams that may be involved in testing external i.e. third party applications prior to launch e.g. via an application store . Additionally this may be useful for the third party application developers in order to know how to develop an application on a particular platform what to test and how to fix issues in the application before submitting the application for launch e.g. via the application store .

While the present disclosure may refer to particular types of applications for illustrative purposes the concepts described may be applicable in a variety of contexts. For example the system of the present disclosure may be applicable to various platform providers for external developers. As another example the system of the present disclosure may be applicable to entities that provide a large number of applications. That is the concepts described in the present disclosure may be applicable not only to application stores but also to device manufacturers social networking sites Internet browsers gaming consoles or smart televisions among other alternatives.

In some cases automated application test case generation may be useful for approval of applications developed by third parties to be launched via a platform such as an application store. In the context of an application store there may be a large volume of applications developed by third party developers to be tested prior to launch e.g. for compliance with particular criteria. The automated application testing of the present disclosure may improve testing efficiency by providing a clear set of test cases to be executed for a particular application. This process may add value because an approval engineer may not be familiar with an application that was developed by a third party developer. Automated application analysis and test case generation may assist the approval engineer to identify what to test. Further testing efficiency may be improved by assigning particular applications to different approval engineers that may have different skill sets. For example an application that uses an in application purchase function may be assigned to an approval engineer with experience with testing in application purchase functionality.

In the example of the user interface may include a first option that is selectable by the user to identify an application binary file associated with an application to be tested. In response to selection of the first option another interface not shown may be displayed that may allow the user to choose the application binary file e.g. from a list of files that are accessible via the electronic device .

In some cases the application binary file can be an application package file APK format file that is used to distribute and install application software and middleware onto Android operating system and certain other operating systems such as Blackberry OS. To make an APK file a program for Android is compiled and then all of its parts are packaged into one file. An APK file contains all of that program s code such as .dex files resources assets certificates and the manifest file. APK files can have any name needed provided that the file name ends in .apk . A manifest file enumerates the files which are included in the distribution either for processing by various packaging tools or for human consumption. The APK file can also include i a directory containing the compiled code that is specific to a software layer of a processor ii a directory containing resources not compiled into resources.arsc iii a directory containing applications assets which can be retrieved by AssetManager iv an additional Android manifest file describing the name version access rights referenced library files for the application. This file may be in Android binary XML that can be converted into human readable plaintext XML with tools such as AXMLPrinter2 apktool or Androguard v the classes compiled in the dex file format understandable by the Dalvik virtual machine and vi resources.arsc a file containing pre compiled resources such as binary XML for example.

The electronic device may be implemented as any of a number of electronic devices such as a smart phone an eBook reader a media player a tablet computing device a portable gaming device a portable digital assistant a laptop or netbook computer and so forth. Furthermore the electronic device may not necessarily be a mobile or portable device and thus in some implementations may include a display of a desktop or other computing device a gaming system a television other home electronics devices and so forth.

The application binary file identified by the user may be sent to one or more computing devices via at least one network . The one or more computing devices may include an analysis module to perform static analysis on the application binary file . For example the static analysis may include determining basic details about the application such as user permission s being used e.g. a RECEIVE SMS permission for an application that monitors incoming SMS messages application programming interface s being used hardware software features being used or layout object s and or style details among other alternatives. That is static analysis may represent a mechanism to read the application binary file and identify parameter s that may provide the list of things that the application may be using.

The analysis module may be configured to identify one or more keywords that are applicable to the application binary file based on the static analysis of the application binary file . A particular embodiment of identifying the keyword s is described in further detail with respect to . In some cases multiple tools e.g. open source tools may be available for analyzing the application binary file . In some contexts static analysis may be used for checking code quality dead code nullness and or use of restricted APIs. However the present disclosure describes the use of static analysis to gather a list of various possible test areas applicable for that application. In some cases apart from the automated static analysis manual analysis may also be performed in order to check the features and or test areas to be tested for the given application.

The one or more computing devices may further include a test case repository that includes a plurality of test cases that may be tagged with keyword s for use in determining the applicability of a particular test case to a particular application. As an illustrative non limiting example the test case s related to camera may be tagged with a Camera keyword in the test case repository . As another example test case s related to in application purchase APIs may be tagged with an IAP keyword in the test case repository . In the embodiment illustrated in the test case repository includes N test cases including a first test case identified as Test Case in a second test case identified as Test Case in up to an Nth test case identified as Test Case N in .

In some embodiments the test cases in the test case repository may be defined based upon applicability to particular devices as well. As an example the CAMERA feature may not be available on a first generation of a device from a particular manufacturer while the CAMERA feature may be available on a subsequent generation of the device. Therefore the list of test cases for the same application on these devices may be different. To illustrate the list of test cases for the first generation of the device may be related to the graceful handling of the absence of the CAMERA feature. By contrast the list of test cases for the subsequent generation of the device may be normal camera related test cases. The same concept may apply for various other hardware and software features associated with different versions generations of a particular device. That is the list of test cases for a particular device may represent test cases that are particularly designed for execution on the application on the particular device.

The analysis module may be configured to query the test case repository based on the one or more keywords in order to identify one or more test cases that are applicable to the application binary file . The test case s may include not only the applicable test steps but also the associated test case description success criteria priority among other details. Further an individual test case may provide a technical blurb to suggest to the user what to do if the particular test case is failing for the application. Therefore the test case s may provide numerous details regarding what to test how to test and how to fix a problem in the event that a problem is detected. Further the test case s may include the tests generic guidelines such as privacy performance and or security. In some cases those test cases may be included for the applications by default.

The one or more computing devices may further include a test module . In some embodiments the test module may be used to create a test plan based on the test case s from the test case repository . The test module may be further configured to generate test information to be sent to the electronic device e.g. via the network . Some of the test case s may be executed automatically while some of the test case s may be executed manually. As such the test module may automatically execute one or more automatic tests on the application and capture the results and may provide information regarding the manual test cases to be executed with the associated results to be manually recorded see e.g. the test plan of .

Once the automatic and manual test runs are completed the test module may generate a test report see e.g. the test report of . That is based on the results of the test cases executed the test report may be generated automatically with the list of test case failures and optionally technical blurbs to help in fixing those issues. In some embodiments this report can be directly sent to the user e.g. the application developer potentially allowing the developer to more quickly fix the identified issues.

In the embodiment illustrated in the user interface may include a second option that is selectable by the user to generate a static analysis report a third option to generate a test plan see e.g. a fourth option to add test cases see e.g. a fifth option to edit test cases and a sixth option to add devices objects and or features. Thus in the example of a user e.g. a third party developer may submit an application binary file for automatic static analysis and automated generation of one or more test cases.

Referring to an example process for automated test case generation is illustrated and generally designated . In the example of the process includes performing static analysis on an application binary file determining keyword s for querying a test case repository to identify test case s that are associated with the keyword s . In some cases the process may include causing display of a test plan generation user interface see e.g. that includes information associated with the identified test case s for use e.g. by a tester of the application or by a developer of the application.

At the process includes performing static analysis on an application binary file to identify one or more application parameters. For example referring to the user may send the application binary file to the one or more computing devices via the network . The analysis module may perform static analysis on the application binary file to identify application parameter s associated with the application binary file . To illustrate in some cases the application parameter s associated with the application binary file may include user permission s being used application programming interface s being used hardware software features being used or layout object s and or style details among other alternatives.

In some embodiments the application binary file may include an archived file e.g. an apk file that may be analyzed by the analysis module to identify the application parameter s . For example the analysis module may un archive the archived file and analyze one or more associated files e.g. a manifest file that presents various types of information regarding an application before a system runs the application s code one or more layout files that may define a visual structure for a user interface such as the user interface for an activity or application widget and a classes file that in some cases may represent files that are converted into a particular virtual machine format before installation on a device . As an illustrative non limiting example analyzing the manifest file may include determining a version of an operating system determining one or more software development kit SDK versions and determining one or more user permissions. As another illustrative example analyzing the layout files may include capturing the type of layout e.g. relative linear webview and list of various layout objects e.g. buttons menus text fields grid media player scroll bars etc. that may be used by the application. As a further example analyzing the classes file may include determining information associated with the application such as APIs and corresponding versions packages classes method names call graph among other alternatives.

At the process includes determining one or more keywords that are associated with the individual application parameter s . For example referring to the analysis module may determine the keyword s that are associated with the individual application parameters.

Based on the list of features objects permissions etc. gathered from the static analysis different tags may be defined. As an illustrative example for a particular application platform multiple user permissions may be available. Each of the user permissions may be mapped to a different keyword tag. As an illustrative non limiting example a RECEIVE SMS permission may allow the application to get short messaging service SMS messages. Therefore the test cases to test the SMS receiving feature may be tagged with a RECEIVE SMS keyword. As another illustrative example multiple APIs e.g. an in application purchase API a maps API etc. may be provided by a particular platform. In this case static analysis may identify these APIs and the test cases may be mapped to the particular APIs. Upon static analysis each keyword may be captured. Further in some cases one or more default keywords may be applied to the application. Illustrative examples of such keywords may include performance privacy security or fluidity among other alternatives.

At the process includes querying a test case repository to identify one or more test cases that are associated with the keyword s . For example referring to the analysis module may query the test case repository to identify the test case s that are associated with the keyword s . As an illustrative non limiting example the test case s related to camera may be tagged with a Camera keyword in the test case repository . As another example test case s related to in application purchase APIs may be tagged with an IAP keyword in the test case repository .

At the process includes causing display of a test plan generation user interface that includes information associated with the identified test case s . In some embodiments the test plan generation user interface may identify various possible test areas that may be applicable for the application. For example the possible test areas may include one or more devices for testing one or more features for testing or one or more objects for testing among other alternatives.

Referring to another example process for automated test case generation is illustrated and generally designated . In the example of the process includes performing static analysis on an application binary file determining keyword s for querying a test case repository generating a test plan based on test case s identified in the test case repository and executing the test plan. further illustrates a particular case in which a test report is generated that identifies test case failure s and potential solution s for use e.g. by a third party developer or a tester.

At the process includes performing static analysis on an application binary file to identify one or more application parameters at . For example referring to the user may send the application binary file to the one or more computing devices via the network . The analysis module may perform static analysis on the application binary file to identify application parameter s associated with the application binary file .

At the process includes determining one or more keywords that are associated with the individual application parameter s . For example referring to the analysis module may determine the keyword s that are associated with the individual application parameters.

At the process includes querying a test case repository to identify one or more test cases that are associated with the keyword s . For example referring to the analysis module may query the test case repository to identify the test case s that are associated with the keyword s .

At the process includes generating a test plan based on the identified test case s . At the process includes executing the test plan. For example referring to the test module may generate a test plan based on the identified test case s and may execute at least a portion of the test plan e.g. the automatically executable test cases . As described above one or more of the test cases may represent manually executable test cases see e.g. the test plan of .

At the process includes generating a test report that includes a list of test case failure s and potential solution s . For example referring to the test module may generate the test information that may include a list of test case failure s and potential solution s . As an illustrative non limiting example the test report of includes an example of a test case failure e.g. Application exits on selecting camera option as well as an example of potential solution s e.g. the How to fix it area .

For some companies that utilize an application store with applications from third party developers testing the bulk of such applications in a timely manner may be difficult. For example a tester may spend a significant amount of time to determine what to test for a particular application and what not to test for the particular application. In some cases the tester may miss certain test areas cases that may be important. illustrate that the system of the present disclosure may improve productivity by automatically determining particular test cases that are to be performed. Further the test report provides the technical blurb to help the developer on how to fix the issue. In some cases this may result in a faster turnaround time which is useful not only to the company but also to the developer.

Thus with respect to business value the system of the present disclosure may improve productivity in terms of testing the applications submitted for approval to the application store. As such more applications may be launched in less time. Further with respect to business value the system of the present disclosure may provide better test reports to developers potentially allowing developers to fix issues faster and potentially improving the developer s trust in the company that provides the application store. Further with respect to business value by sharing the automated test case generation tool with a developer the developer may develop an application faster and ensure that the application meets a quality threshold before submission. Further sharing the tool may result in faster time to market and potentially better business not only for the third party developers but also for the company that provides the marketplace for the application e.g. the application store .

The example processes described herein are only examples of processes provided for discussion purposes. Numerous other variations will be apparent to those of skill in the art in light of the disclosure herein. Further while the disclosure herein sets forth several examples of suitable frameworks architectures and environments for executing the processes implementations herein are not limited to the particular examples shown and discussed.

Referring to a particular example of a test plan generation user interface is illustrated. In some cases the test plan generation user interface may be provided to a tester e.g. an approval engineer associated with a particular platform . Alternatively the test plan generation user interface may be provided directly to the third party developer of the application under test. In the particular example of the user is provided with a first selectable option to generate a test plan and a second selectable option to export a test plan e.g. in a spreadsheet format .

In some embodiments the test plan generation user interface of may be automatically generated and displayed based on the results of the static analysis of the application binary file as described above with respect to . That is in some cases the test plan generation user interface may be presented with one or more automatically selected options for testing a particular application based on the static analysis results of the application binary file associated with the particular application. Thus illustrates that the test plan generation user interface may allow the user to review the automatically selected option s as well as select additional items for testing e.g. a particular device on which the application is to be tested .

In alternative embodiments the test plan generation user interface of may be generated and displayed in response to user selection of the generate test plan option presented via the user interface of . For example if the application to be tested is not already available the user may directly select the generate test plan option and manually select one or more tags to generate the appropriate test cases for the application to be tested.

In the example of the test plan generation user interface identifies a name of an application e.g. ImageFunGame v 3.5 a test plan name e.g. IFGv35 and various possible test areas that may be applicable for the application. For example the possible test areas may include one or more devices for testing one or more features for testing or one or more objects for testing among other alternatives. The test plan generation user interface further includes a first selectable option to generate a test plan. Further in the particular embodiment illustrated the test plan generation user interface also includes a second selectable option to export a test plan e.g. in a spreadsheet format .

In the example illustrated in a checkmark icon is used to illustrate that a particular device feature or object has been selected. However this is for illustrative purposes only and various other methods of identifying selected items may be employed e.g. highlighting underlining font change etc. . illustrates a particular example in which the selected devices include a first device identified as Device in and a second device identified as Device in . further illustrates that the selected features include a Camera feature a Performance feature and an in application purchase IAP feature . also illustrates that the selected objects include a Timer object and an Internet object . It will be appreciated that a tester may select to add and or remove particular selected devices features or objects as desired in order to automatically generate a test plan with multiple test cases that are relevant for the particular selected devices features and or objects.

Referring to a particular example of a test plan is illustrated. In some cases the test plan may be provided to a tester e.g. an approval engineer associated with a particular platform . Alternatively the test plan may be provided directly to the third party developer of the application under test. In some cases the test plan of may be generated in response to selection of the first selectable option of i.e. the Generate Test Plan option .

In the example of the test plan identifies various types of information for multiple test cases. To illustrate for individual test cases the test plan may identify a test case number a test case identifier ID a description a priority an expected behavior . further illustrates a particular embodiment in which a tester may be presented with an option to delete one or more of the test cases. In some cases a first selectable option e.g. a No option may represent a default selection while a second selectable option e.g. a Yes option may be used to remove a particular test case from the test plan . further illustrates that the tester may be presented with a selectable option e.g. a Save Test Plan control to save the test plan .

As an illustrative example for the first test case in the test plan i.e. the test case with 1 as the test case number the test case ID includes the number 1 and the description indicates that the first test case is designed to check installation of the application. Further the priority associated with the first test case is identified as P0 and the expected behavior indicates that the application should install properly without any exceptions errors and application should launch and that installation should not take more than 15 seconds. A tester may save the test plan by selecting the Save Test Plan control .

For example in the embodiment illustrated in a result associated with execution of a particular test case may include a first result e.g. a Pass result or a second result e.g. a Fail result . In the example illustrated in the result associated with a particular test case e.g. with the description of To Test Application Behavior on Selecting Camera Option is identified as having failed.

Referring to a particular example of a test report is illustrated. Based on the results of one or more test cases that are executed the test report may be generated automatically with a list of test failures and optionally technical blurbs that may help in identifying and fixing the cause of the test case failures. In some cases the test report may be provided to a tester e.g. an approval engineer associated with a particular platform . Alternatively the test report may be provided directly to the third party developer of the application under test.

In some embodiments the test report may include an issue identifier ID assigned to an issue a priority associated with the issue a summary of the issue one or more devices that were selected for testing one or more steps to reproduce the issue and one or more observed results . In the example of the summary of the issue indicates that the application exits on selecting camera option. The test report further indicates that the application was tested on Device and Device . The test report further identifies that the steps to reproduce the issue i.e. the application exiting on selecting the camera option include opening the application under test i.e. step 1 and then selecting the option to open the camera to take a photograph i.e. step 2 .

The test report further identifies the observed result s e.g. the application exits . For example referring to the test module may store the observed result s as part of the test case data associated with one of the particular test case s in the test case repository . When a tester marks a test case FAIL the test module may automatically store the details associated with the failure in the test case repository . Additional comment s from the tester may also be stored in the observed result s as part of the test case data in the test case repository .

The test report further includes one or more expected results for comparison to the observed result s . For example in this case the expected results indicate that the application must not freeze or crash. The test report further includes one or more recommendations . For example in the recommendation s indicate that some devices do not have the camera feature and that the lack of this feature may be the cause of the application crashing. In such cases camera actions should be gracefully handled. That is for devices that do not have the camera feature the recommendation s for preventing the application from crashing may include disabling the camera option or displaying an appropriate message e.g. No camera feature available to indicate that the device does not have the feature.

In some embodiments the test report may further include one or more technical blurbs regarding how to fix the issue e.g. How to fix it . For example in some cases the test report may include the blurb s when the test report is presented to the third party application developer while in other cases the test report may not include such blurb s when the test report is presented to another tester. In the example illustrated the blurb s may indicate to the third party developer that he or she can detect device features for the particular device s in several different ways. For example the user may query a package manager perform reflection or check the result of convenience methods. Further the blurb s may indicate to the third party developer that he or she can determine if camera intent is available using PackageManager to determine if the camera software is installed on the device.

In some cases the one or more technical blurbs may include sample code to fix the issue. For example in comments indicate that the sample code may determine if the current device can handle an image capture action. The sample code may return a result of true if the device can handle the image capture action and may return a result of false if the device cannot.

Referring to a particular example of a user interface for adding a test case is illustrated. In some embodiments the user interface of may be displayed in response to selection by the user of the add test cases option presented via the user interface of . That is in some cases the user interface may allow a third party application developer to manually specify one or more parameters associated with a test case to be added. Alternatively the user interface may be presented to another tester e.g. a tester associated with an application store .

In the example of the user interface allows the user to identify a test case version e.g. version 1.0 and to assign a priority to the new test case e.g. by selecting a priority value of P0 from a dropdown window . In some cases a priority value of P0 may represent a test case that a particular platform may specify as a test case that the application is to pass prior to launch of the application while other priority values such as P1 or P2 may represent optional test cases that may improve a user experience but that represent a lower priority for testing purposes. Further the user interface allows the user to provide a short summary of the test case and to identify one or more test case steps to execute the test case. To illustrate in the example of the short summary includes Camera feature in the app should be gracefully handled while the steps to execute the test case include a first step of opening the application under test and a second step of selecting the option to open the camera to take a photograph.

The user interface further allows the user to identify expected result s e.g. the application must not freeze or crash . Further in some cases the user interface may allow the user to provide recommendation s and to provide a technical blurb that may include sample code for executing the new test case. For ease of illustration the expected result s and the recommendation s identified in correspond to the expected result s and the recommendation s described with respect to . Further the technical blurb and the sample code correspond to the technical blurb and the sample code of respectively.

The user interface further allows the user to define one or more tags to be associated with the new test case by selecting various possible test areas that may be applicable for the application. To illustrate the possible test areas may include one or more devices for testing one or more features for testing or one or more objects for testing among other alternatives.

In the example illustrated in a checkmark icon is used to illustrate that a particular device feature or object has been selected for the test case to be added. However this is merely for illustrative purposes only. illustrates a particular example in which two devices and are selected for testing identified as Device and Device in a single feature is selected for testing e.g. a Camera feature and none of the objects are selected for testing. further illustrates that in some cases the user may scroll through a list of the devices that are available for testing using a first selectable scroll option . further illustrates that in some cases the user may scroll through a list of the features that are available for testing using a second selectable scroll option . further illustrates that in some cases the user may scroll through a list of the objects that are available for testing using a third selectable scroll option .

The electronic device may be implemented as any of a number of electronic devices. Furthermore in some implementations the electronic device may include a display of a desktop or other computing device a gaming system a television other home electronics devices and so forth.

Depending on the configuration of the electronic device the computer readable media may be an example of non transitory computer storage media and may include volatile and nonvolatile memory and or removable and non removable media implemented in any type of technology for storage of information such as computer readable instructions data structures program modules or other data. Such computer readable media includes but is not limited to RAM ROM EEPROM flash memory or other computer readable media technology CD ROM digital versatile disks DVD or other optical storage magnetic cassettes magnetic tape solid state storage magnetic disk storage RAID storage systems storage arrays network attached storage storage area networks cloud storage or any other medium that can be used to store information and which can be accessed by the processor directly or through another computing device. Accordingly the computer readable media may be computer readable media able to maintain instructions modules or components executable by the processor .

The computer readable media may be used to store any number of functional components that are executable by the processor . In some implementations these functional components comprise instructions or programs that are executable by the processor and that when executed implement operational logic for performing the actions attributed above to the electronic device . Functional components of the electronic device stored in the computer readable media may include the analysis module and the test module as described above with respect to which may be executed on the processor . Other functional components may include an operating system for controlling and managing various functions of the electronic device . Depending on the type of the electronic device the computer readable media may also optionally include other functional components such as other modules which may include applications programs drivers and so forth.

The computer readable media may also store data data structures and the like that are used by the functional components. For example the computer readable media may store the test case repository as described above with respect to . The electronic device may also include other data which may include for example data used by the operating system and the other modules . Further the electronic device may include many other logical programmatic and physical components of which those described are merely examples that are related to the discussion herein.

One or more communication interfaces may support both wired and wireless connection to various networks such as cellular networks radio WiFi networks short range or near field networks e.g. Bluetooth infrared signals local area networks wide area networks the Internet and so forth. The communication interface s may further allow a user to access storage on another device such as a user s computing device a network attached storage device or the like.

The electronic device may further be equipped with various other input output I O components . Such I O components may include a touchscreen and various user actuatable controls e.g. buttons a joystick a keyboard a mouse etc. speakers a microphone a camera connection ports and so forth. For example the operating system of the electronic device may include suitable drivers configured to accept input from a keypad keyboard or other user actuatable controls and devices included as the I O components . For instance the user actuatable controls may include page turning buttons navigational keys a power on off button selection keys and so on. Additionally the electronic device may include various other components that are not shown examples of which include removable storage a power source such as a battery and power control unit a global positioning system GPS device a PC Card component an accelerometer and so forth.

Various instructions methods and techniques described herein may be considered in the general context of computer executable instructions such as program modules stored on computer storage media and executed by the processors herein. Generally program modules include routines programs objects components data structures etc. for performing particular tasks or implementing particular abstract data types. These program modules and the like may be executed as native code or may be downloaded and executed such as in a virtual machine or other just in time compilation execution environment. Typically the functionality of the program modules may be combined or distributed as desired in various implementations. An implementation of these modules and techniques may be stored on computer storage media or transmitted across some form of communication media.

Although the subject matter has been described in language specific to structural features and or methodological acts it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described. Rather the specific features and acts are disclosed as example forms of implementing the claims.

