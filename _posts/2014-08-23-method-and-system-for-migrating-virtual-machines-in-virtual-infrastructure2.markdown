---

title: Method and system for migrating virtual machines in virtual infrastructure
abstract: A method of migrating a data compute node (DCN) in a datacenter comprising a plurality of host physical computing devices, a compute manager, and a network manager is provided. The method by the network manager configures a plurality of logical networks and provides a read-only configuration construct of at least one of the plurality of logical networks to the virtualization software of each host. The construct of each logical network includes a unique identification of the logical network. The method by the compute manager obtains the unique identification of one or more of the logical networks, determining that a particular logical network is available on first and second hosts based on the unique identification of the particular logical network, and configures a first DCN to move from the first host to the second host based on the determination that the first and second hosts are on the particular logical network.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09547516&OS=09547516&RS=09547516
owner: NICIRA, INC.
number: 09547516
owner_city: Palo Alto
owner_country: US
publication_date: 20140823
---
Today the network presents a significant management overhead especially in the virtual infrastructure. Programmable central management of networks ports and network services is critical to the success of allowing enterprise customers to evolve their operations to cloud scale. This raises an issue that is front and center in the vision of software defined datacenter.

Today s network management constructs allow distributed configuration and scale out of the virtual network but are tightly coupled to the compute management elements. This implicitly binds any solution to artificial constraints which are far exceeded by today s datacenter requirements of scale configuration and distribution for cloud networking. This expansion of virtual networks in harmony with network management orchestration is targeting the goal of a truly elastic datacenter.

One of the drawbacks of the current networking architecture is its tight coupling to the physical compute infrastructure and compute management layers. Today if network administrators would like to manage the physical infrastructure they must login into several physical switches. Today in the virtual infrastructure if a customer would like to manage the deployment of networks they must log into several different compute manager instances. There is no central management portal for virtual networks virtual ports and network services across the virtual infrastructure.

Methods and systems for virtual switch management are provided to assist the creation and consumption of workflows for logical networks also referred to as virtual networks and network services configured through any cloud management system CMS while remaining independent of the compute manager and virtualization software vendor and version. In some embodiments the edge of the logical network infrastructure is controlled by a single networking platform that is decoupled from the compute manager and the CMS. In these embodiments the logical network infrastructure is controlled by a higher level networking specific construct better equipped to present network centric viewpoint than the compute manager. The networking platform adds a layer of indirection that allows decoupling of datacenter infrastructure software.

In addition once the logical network is created in a transport zone the logical network is made visible to all hosts that are in the transport zone to allow the logical network interface controllers or virtual network interface controllers VNICs of the hosts virtual machines VMs to connect to the logical network. A transport zone is created to delineate the width of an overlay network. An overlay network is a network virtualization technology that achieves multi tenancy in cloud computing environment. The workflow of publishing a logical network on a host is facilitated in some embodiments by the central network management entity sending a message regarding the newly created logical network to the virtualization software of each host where the logical network is available.

The network manager assigns a universally unique identifier UUID to the logical network and sends the UUID to the hosts. The UUID is locally stored at the host virtualization software s node management server. This data is periodically re synched and a heartbeat mechanism maintains state synch. In essence the network configuration is made available to the compute manager while the network manager remains the source of truth and maintains network configuration details. This allows the compute and network management entities to remain decoupled yet integrated.

The preceding Summary is intended to serve as a brief introduction to some embodiments of the invention. It is not meant to be an introduction or overview of all inventive subject matter disclosed in this document. The Detailed Description that follows and the Drawings that are referred to in the Detailed Description will further describe the embodiments described in the Summary as well as other embodiments. Accordingly to understand all the embodiments described by this document a full review of the Summary Detailed Description and the Drawings is needed. Moreover the claimed subject matters are not to be limited by the illustrative details in the Summary Detailed Description and the Drawing.

In the following detailed description of the invention numerous details examples and embodiments of the invention are set forth and described. However it will be clear and apparent to one skilled in the art that the invention is not limited to the embodiments set forth and that the invention may be practiced without some of the specific details and examples discussed.

Virtualization is the ability to simulate a hardware platform such as a server storage device or network resource in software. A virtual machine VM is a software implementation of a machine such as a computer. Logical networks are abstractions of a physical network. VMs may communicate using logical networks. conceptually illustrates a legacy virtualized infrastructure domain that uses a legacy logical forwarding element LFE model that relies on compute manager as the source of truth for the LFE. The compute manager is a component of the cloud management system CMS that is used to create and configure computing resources such as the VMs and the storage in the CMS.

The virtualized infrastructure domain includes a set of host machines hosting multiple tenants. Each tenant has one or more VMs. For simplicity the VMs of only one tenant are shown. As shown each host includes virtualization software sometimes referred to as a hypervisor . The virtualization software shown in this figure are representative of the various types of virtualization software that may operate on hosts in such a virtualized infrastructure e.g. virtual machine monitor etc. . The VMs of each tenant form a logical network also referred to as private network or virtual network . The logical network is identified by a logical network identifier also known as virtual network identifier or VNI . Each logical network is configured by a tenant. The logical network is an abstraction of a physical network and may provide a virtual Layer 2 or data link layer for services such as encapsulation and decapsulation of network layer data packets into frames frame synchronization medial access control etc. The logical network may span one or more physical networks and be organized independent of the underlying physical topology and organization of the physical networks.

In some embodiments the virtualization software includes a physical forwarding element PFE such as a virtual switch. In the virtualization field some refer to software switches as virtual switches as these are software elements. However in this specification the software forwarding elements are referred to as physical forwarding elements PFEs in order to distinguish them from logical forwarding elements LFEs which are logical constructs that are not tied to the physical world. A PFE forwards packets in a physical network whether or not it is implemented in software while a LFE forwards packets in a logical network which is logically decoupled or abstracted from the physical network. In other words the software forwarding elements are referred to as PFEs because they exist and operate in the physical world whereas an LFE is a logical representation of a forwarding element that is presented to a user when designing a logical network.

In some embodiments several PFEs are distributed throughout the network implement tenant s LFEs where each PFE is a local instantiation or a proxy of an LFE that operate across different host machines and can perform L3 packet forwarding between VMs on the host machine or on different host machines. An LFE is sometimes referred to as a virtual distributed switch VDS .

In each host the LFE connects to one or more physical network interface controllers PNICs to send outgoing packets and to receive incoming packets through a physical network . As shown the LFE in is defined to include one or more ports or a port group through which it connects to uplinks and the physical NICs to send and receive packets. Each LFE is also defined to have a virtual port group to connect to VMs through virtual NICs VNICs to the LFE. An uplink is a module that relays packets between the LFE and the physical NIC in order to perform various packet processing functions on incoming and outgoing traffic.

LFEs were traditionally defined by the compute manager or by virtualization software and managed by the compute manager which made them tightly coupled with the compute manager platform. As such any new network feature or enhancement had to be tied with an update to the compute manager. shows a legacy LFE model that relies on the compute manager as the source of truth i.e. where the LFE can be defined and modified for the LFE. As shown the compute manager creates the LFE along with an uplink port group and one or more virtual port groups .

Each LFE is associated to a logical network. The compute manager sends as shown by a copy of the LFE and the port groups and to each host where the logical network is available. The compute manager remains the source of truth for the LFE. For example if modifications are made directly on the host to the LFE the compute manager overwrites the modifications when it re syncs at a periodic interval.

The compute manager daemon is the intermediary between the compute manager and the virtualization software on the host which handles service requests that the virtualization software receives from the compute manager. The virtualization software stores a copy or a proxy of the LFE locally at the host. The network manager then pushes as shown by additional configuration such as firewall rules overlay technology etc. for the network through the network daemon . The configurations received from the network manager in are modifiers rather than the creator of the logical network and the LFE. The compute manager remains the source of truth and also does periodic re synch with the logical network and the LFE.

Some embodiments provide methods and systems to remove LFE s dependency on compute manager such that the LFE data path i.e. the LFE proxy in the virtualization software is managed by a single networking platform and is decoupled from the compute manager. In addition some of these embodiments provide the traditional LFEs defined by the compute manager. These embodiments provide two different ways of defining LFEs 1 the traditional way of defining LFEs using the compute manager e.g. as described above by reference to and 2 defining the LFEs by the networking platform that is decoupled from the compute manager and exposing the LFEs for compute manager or host virtualization software consumption while maintaining the configuration of the LFEs only in the networking platform. Through the second approach the LFEs managed by network manager fully decouple the compute manager dependency for LFE feature configuration.

The decoupled LFE port group no longer uses an LFE that is created by the compute manager on the host as the back plane i.e. to connect to . Instead the decoupled LFE uses a port group that is created out of band through a network manager communication channel to the virtualization software. The compute manager is provided a read only access to the decoupled LFE. Some embodiments provide two main types primarily for consumption purposes. A host level LFE type referred to herein as an opaque LFE and a network type referred to in this document as an opaque network .

In some embodiments each LFE or logical network of opaque type is identified by a name and a unique identifier such as a universally unique identifier UUID . This allows updating the network manager without any changes to the compute manager and the virtualization software.

The network manager created LFE and logical networks in some embodiments are identified opaque e.g. by associating them with an isOpaque flag and setting the flag to true in the list of the virtualization software LFEs thus they will be recognized as an opaque switch or opaque network by the compute manager as opposed to a compute manager created LFE proxy or logical network that are subject to the compute manager s changes. This flag indicates to the compute manager that this is an independently managed switch or network that cannot be changed by the compute manager.

Each transport zone is created to connect several transport Nodes. Examples of transport nodes include host virtualization software virtual gateways providing L2 and L3 services and hardware L2 Gateways. For instance the host virtualization software might connect to one network for management traffic while connecting to a different network for VM traffic. The combination of a transport zone and the transport nodes provides the network manager the information to form tunnels between different hosts virtualization software to provide logical connectivity.

The network manager sends as shown by arrows the logical network and LFE information to virtualization hosts that are connected to the same transport zone. The host flags the logical network and the LFE as opaque network and opaque LFE to prevent the compute manager from attempting to modify them. The virtualization software of each host maintains a proxy of the opaque LFE.

As shown each VM connects to the opaque LFE proxy through the virtual NICs . The opaque LFE proxy includes a set of standalone opaque ports which are created and managed by the network manager . The Opaque LFE proxy connects to the physical NICs through the uplinks . The physical NICs connect to the tenant s overlay network in the network . The overlay network traffic between a tenant s VMs that are hosted on separate hosts is passed through a tunnel of the overlay network through the communication network . The communication network may include the Internet local area networks LANs wide area networks WANs different hardware equipment such as cables routers switches etc.

As shown in a network manager agent is installed on each host s virtualization software . The network manager agent receives the configuration information for the opaque networks and the opaque LFE. The network manager agent communicates with an asynchronous switching kernel in the virtualization software to create or use an existing an opaque LFE proxy in the virtualization software . This process in some embodiments is performed through a switch management module within the network manager agent that orchestrates bootstrapping of the switching layer managing physical NICs and the base infrastructure.

The network manager agent is the intermediary between network manger and the virtualization software . The network manager agent passes the opaque LFE information to the asynchronous switching kernel using e.g. link control protocol LCP .

The network manager agent is developed and deployed separately than the virtualization software. The network manager agent provisions the network constructs that are used by the virtualization software. The virtualization software is an entity that is configured and deployed by the compute manager while the network manager agent that is deployed on the virtualization software is configured and deployed by the network manager. In some embodiments the entire virtual network functionality used by the virtualization software is on the network manager agent and not on the virtualization software. The benefit of this arrangement is the network manager agent can be updated independent of the virtualization software. For instance if a new port configuration to support an additional security or firewall feature is developed the network manager agent can be redeployed on the virtualization software without a need to deploy a new version of the virtualization software.

The virtualization software provides read only information about the opaque LFE and the opaque networks to the compute manager e.g. through the switching kernel and the compute manager daemon . The compute manager uses the information to create VMs and connect them to the opaque LFE and the opaque networks without creating or changing the configuration of the opaque networks and opaque LFE. In the embodiment of the network manager is the only entity that creates and configures the opaque LFE and opaque network. The compute manager and the host virtualization software only have read only access to the configuration information.

Next the network manager agent provisions at the set of logical networks on the host for the use by the virtualization software of the host. The network manager agent then provides at a copy of all or a potion of the read only configuration construct of each logical network to a compute manager of the datacenter. The network manager agent in some embodiments pushes the information to the compute manager. In other embodiments the network manager sends the information to the compute manager on demand i.e. the information is pulled by the compute manager . The compute manager utilizes the information to connect one or more VMs of the host to the logical network. The process then ends.

In some embodiments the network manager and the compute manager create different LFEs and logical networks and deploy them to the same host. These embodiments provide compatibility with the traditional datacenters where the LFEs are created and managed by the compute manager. These embodiments provide two different ways of defining LFEs 1 the traditional way of defining LFEs using the compute manager e.g. as described above by reference to and 2 defining the LFEs by the network manager of the networking platform that is decoupled from the compute manager e.g. as described above by reference to .

The opaque LFE proxy is the proxy of the opaque LFE that is created by the network manager. The opaque LFE includes several ports that are configured and managed by the network manager. As shown several VMs are configured by the compute manager to connect via VNICs to the ports that are created and managed by the network manager. These VMs are connected to the physical NICs through the opaque LFE and the uplinks .

The virtualization software also includes an LFE proxy that is the proxy of an LFE that is created and managed by the compute manager. The LFE proxy includes a port group which is created and managed by the compute manager. The port group includes a set of ports. The LFE proxy also includes an uplink port group which is created and managed by the compute manager.

As shown several VMs are configured by the compute manager to connect to the port group that is created and managed by the compute manager. These VMs are connected to the physical NICs through the VNICS LFE proxy and the uplinks . As shown in the virtualization software of the host at the same time includes two different LFEs that are created and managed by two different management stacks namely the network manager and the compute manager. In the embodiment of the compute manager only has read only access to the opaque LFE and the ports . The compute manager uses the read only access to connect the VMs to the opaque LFE and the opaque logical networks that are created and managed by the network manager. On the other hand the compute manager has full control to create delete and configure the LFE proxy the port group and the unlink port group .

The opaque LFE proxy is the proxy of the opaque LFE that is created by the network manager. The opaque LFE proxy includes several ports that are configured and managed by the network manager. As shown some of the VMs such as VM are configured by the compute manager to connect via VNIC to the port that is created and managed by the network manager. VM is connected to the physical NIC through the opaque LFE and the uplink .

The virtualization software also includes an LFE proxy that is the proxy of an LFE that is created and managed by the compute manager. The LFE proxy includes a port group which is created and managed by the compute manager and includes a set of ports. The LFE proxy also includes an uplink port group which is created and managed by the compute manager.

As shown VM is configured by the compute manager to connect via VNIC to the port group that is created and managed by the network manager. VM is connected to physical NICs through the LFE proxy and the uplinks . In addition VM is configured by the compute manager to concurrently connect to the opaque LFE proxy and the LFE proxy . Although shows only one VM connected only to opaque LFE proxy only one VM connected to both opaque LFE proxy and LFE proxy and only one VM connected to only LFE proxy it should be understood that any number of VMs may be so connected.

As shown VM includes two VNICs and . VNIC of VM is connected to the opaque LFE proxy through the port which is created and managed by the network manager. The VNIC of VM is connected through the port group to the LFE proxy which is created and managed by the compute manager.

As shown in the virtualization software of the host at the same time includes two different LFEs that are created and managed by two different management stacks namely the network manager and the compute manager. In the embodiment of the compute manager only has read only access to the opaque LFE and the ports . The compute manager uses the read only access to connect VM to the opaque LFE and the opaque logical networks that are created and managed by the network manager. On the other hand the compute manager has full control to create delete and configure the LFE proxy the port group and the unlink port group . The compute manager uses its own knowledge of the LFE proxy and the port group to connect VM to the LFE proxy and the logical networks that are created and managed by the compute manager.

The process then configures at a first VM on the host by the compute manager to connect to a logical network that is configured and managed by the network manager through a port of the first LFE. The port of the first LFE is configured and managed by the network manager. The process then configures at a second VM on the host by the compute manager to connect to a logical network that is configured and managed by the compute manager through a port of the second LFE. The port of the second LFE is configured and managed by the compute manager.

Next the process optionally configures at a third VM on the host by the compute manager to connect i to the logical network that is configured and managed by the network manager through a port of the first LFE and ii to the logical network that is configured and managed by the compute manager through a port of the second LFE. The process then ends.

An LFE in some embodiments no longer uses a port group that is created and managed by the compute manager such as port groups and in to connect to a network. Instead as shown in a VM virtual NIC uses an opaque LFE that is created and managed by the network manager to connect to a network. Some embodiments provide a create port and attach port API call to network manager. This occurs via an inter process communication IPC callback e.g. from the compute management daemon shown in . In the create port call the network manager creates a port on the opaque LFE through signaling via the network manager agent . In the attach port call the network manager pushes the port to the host virtualization software via a controller messaging channel. The created port for example port shown in is a standalone port i.e. not contained in a traditional port group that is created and managed by the compute manager . However the standalone port does continue to contain any required configuration i.e. traditional LFE port properties .

The network manager management plane sends a request at to the network manager control plane to attach the VIF by providing the LFE identifier which in some embodiments is the same as the logical network UUID the VIF identifier e.g. the VIF UUID and the logical port s identifier. The management plane of the network manager is the desired state engine and is used to create logical networks or specify new policies e.g. firewall policy for the network. The network manager control plane is the realized state engine that maintains media access control MAC learning table for networking learns MAC IP address associations handles address resolution requests ARPs for VMs etc. The network manager control plane resides alongside or within the network manager.

The network manager control plane also receives a message at from the network manager agent to inquire for the port ownership. The message is an inquiry to know which ports are on a host. The network manager control plane sends at a message to the network manager agent to attach the port to the VIF i.e. pushes the port to the virtualization software of the host . The network manager agent sends at a request to the network manager management plane to get the port configuration. Finally the network manager management plane sends at the port configuration to the network manager agent . The network manager agent coordinates with the switching kernel item shown in on the host to attach the VIF to the requested port i.e. to attach the VM corresponding to the VIF to the requested port on the opaque LFE proxy item shown in .

The network manager agent on the host sends at a message to network manager management plane to attach the VIF to the opaque LFE. The network manager management plane sends a request at a request to create the port to the network manager agent . The network manager management plane also sends a request at to the network manager control plane to attach the VIF by providing the opaque LFE identifier e.g. the opaque network UUID the VIF identifier e.g. the VIF UUID and the logical port identifier. The network manager control plane sends a message to provide at the VIF port flow table to the network manager agent . This message passes the opaque port state for feature configuration i.e. the desired state to the host for actually being realized on the port instance.

A cloud management system e.g. Openstack that prefers to handle ports directly can set a flag e.g. portAttachMode flag as manual in the opaque network. For a manual mode opaque network the compute manager has to make explicit create port and attach port API calls on the network manager. The compute manager daemon item in will only update the VM configuration and will not call the network manager.

The attach port operation in the network manager uses a VIF UUID that is set on the LFE port and external identification property of the virtual NIC. At VM power on the virtualization software does a reverse look up with the external identification property of the virtual NIC to find the LFE port and connect to it.

Opaque networks make it possible to decouple network management plane and compute manager plane. Opaque networks create an abstraction to present a logical network that can embody different technology or implementation. The opaque switch LFE creates an abstraction for the collection of physical NICs used by an opaque network data path abstracting the physical layer from the logical space but maintaining the relationship. For instance since an LFE spans across hosts regardless of whether one host is connected physically with two 10 GB physical interfaces or another host is connected with only one 1 GB physical interface the LFE is still going to have connectivity on the given network. This is the layer of abstraction the LFE is providing from the physical infrastructure for connectivity.

The presence of such an opaque network on virtualization software helps compute management plane determine the VM deployment scope in case the VM is using the network. This is helpful for solutions such as distributed resource scheduling DRS where placement can be influenced by the network manager without actually changing the placement logic with the compute manager. In prior art the network management plane had to resort to moving ports into a blocked state in such a scenario. Using the opaque networks and LFE the network management plane can influence the compute manager to instead place the VM on a virtualization software with the required networking reducing disruption for the end user. For instance if the network manager detects that a specific logical network is having issues the network manager can choose to hide or pull back the network from the proxy. As a result solutions such as DRS do not see the network anymore and will avoid moving existing VMs or placing new VMs requiring the specific network on the host. This is the out of band signaling power that is gained from having an asynchronous network centric control of specific network reflected into the compute manager.

The presence of an opaque switch allows compute construct feature configuration to be reflected into the networking layer. For example in order to provide quick failover or high availability the network manager is able to associate a mirror or shadow port with a primary VM port to maintain the high availability association. This displays the loosely coupled nature of the system while allowing the integration for compute features. In order to provide quick failover wherever the standby VM is running a mirror or shadow of the active network port can be used to mirror the configurations for quick turnaround in the event of a failover from the primary VM to the standby VM.

For a cloud management service the opaque network allows maintaining the associations between the network layer and the compute layer from the VM inventory mappings rather than keeping a fully decoupled stack where neither system is aware of the other s presence. For bandwidth optimization and migration optimal path calculations the opaque network allows further integration and influence of the network stack into the compute manager.

In some embodiments the network manager provides configuration construct of the logical networks and LFE as a set of read only parameters to the virtualization software and the compute manager. In some embodiments the compute manager receives these parameters directly from the network manager. In other embodiments the compute manager receives these parameters from the virtualization software of each host. The compute manager utilizes the parameters to configure VMs to connect to the logical network as well as providing other features such as moving a VM from one host to another supporting fault tolerant VMs supporting failover etc.

The network manager abstracts the logical network and provides the abstraction parameters to the virtualization software compute manager to utilize the logical network. The abstraction parameters are utilized to determine e.g. which host is connected to a logical network e.g. which host is on which transport zone and therefore facilitate connecting the VMs to the logical network powering up or moving VMs to the hosts etc.

In some embodiments the network manager provides the following information about an opaque network to the virtualization software and the compute manager. The network manager provides the opaque network unique identifier. In some embodiments the unique identifier of the opaque network is the UUID of the opaque network.

The information provided by the network manager in some embodiments further includes the opaque network name. The information further includes the network type. Examples of the network type include VXLAN NVGRE etc. as described above. The information further includes the unique identifier of the transport zone associated with the opaque network. In some embodiments the unique identifier of the transport zone is the UUID of the transport zone.

In some embodiments the network manager provides the following information about an opaque LFE to the virtualization software and the compute manager. The network manager provides the list of physical NICs PNICs connected to the opaque LFE. The information also includes the list of transport zone unique identifiers. In some embodiments the unique identifier of a transport zone is the UUID of the transport zone. The information also includes the status of the opaque LFE score e.g. a numerical or multi level health score of the LFE . The health score in some embodiments is calculated by the load performance and or synchronization state of the specific logical network. For instance if the load on a specific network on a specific host is getting over a threshold to manage the health score deteriorates. Similarly if an unusual number of packet drops are being detected or if there is a loop causing issues for the network the health score will similarly drop. Another example is when a logical network has a configuration changed in the management plane that has not yet propagated to all of the LFE instances across the network the VMs using that network can be signaled not re balance until the sync state is completed.

When a host joins a transport zone all logical networks in the transport zone are inserted to the host as available opaque networks. In VM migration or power on placement in DRS compute manager checks whether the destination host has the required opaque network available on the host. VMs that are connected to the same opaque network are deemed to have L2 connectivity between them i.e. the opaque network is an embodiment of virtual L2 . The opaque network is associated with a transport zone UUID that can be correlated to one of transport zones associated with the opaque LFE which the compute manager can use to find out the physical NICs used to provide the networking to the opaque network.

Providing the opaque networks and LFE information to the compute manager facilitates the following features by the compute manager. Compute manager features like DRS quick failover fault tolerance moving a VM from one host to another depend on opaque network spans to make sure VM is placed where it can maintains network connectivity.

Compute manager features like network load balancing depends on the physical NIC information on the opaque switch correlated to the opaque network used in virtual NIC via transport zone UUID to do network bandwidth resource planning Compute manager features like the number of VMs that can concurrently be moved from one host to another which is controlled by the bandwidth of physical NIC connected the virtual NIC depends on opaque switch to provide physical NIC information for an opaque network. For instance when there are only two physical links on a given host both physical links are used for all traffic e.g. to provide quick failover . The physical NIC information can be used to prioritize the traffic. For instance the tenant VM traffic gets the highest priority followed by the control traffic and the management traffic.

Compute manager features like VM migration compatibility checks depend on the opaque switch status to prevent VM from losing network by moving to a host whose opaque switch status is unavailable .

In the virtualization software the following data are communicated maintained. In a VM provisioning operation the virtual NIC and opaque network information are sent to the network manager agent. The network manager agent then allocates the port attaches the port to the VNIC VIF UUID and creates the port in the virtualization software.

In the virtualization software the port to VIF attachment is maintained persisted on the virtualization software in a mapping table. In VM power on virtual NIC device connected and link up virtualization software does reverse look up in the mapping table based on the virtual NIC VIF UUID to find the port and connects the port with the virtual NIC. In the virtualization software the network manager agent sets the port s network configuration policy based on what is provisioned on the network manager. This allows new features to be added to the port without virtualization software platform changes.

The network manager then assigns at a name to the LFE. Next the network manager assigns at a universal identifier such as a UUID to the LFE. The network manager then associates at the LFE to a set of physical NICs.

Next the network manager assigns at the LFE to a set of transport zones. A transport zone is a representation of the physical network and defines which transport nodes a network spans. Transport nodes include VMs virtualization software hypervisors gateways etc. Transport nodes and transport zones make a transport network. The network manager then assigns at a status to the LFE e.g. a numerical or multi level health score .

The network manager then tags at the LFE as opaque. The network manager then sends at the LFE information to all hosts where the transport zones associated with the LFE are available. The hosts save at the LFE information. The compute manager can obtain LFE information from hosts or in alternate embodiments the network manager sends or also sends at the LFE information directly to compute manager . The compute manager also saves at the LFE information.

The network manger then creates at the logical network. The network manager then assigns at a name to the logical network. Next the network manager assigns at a network type e.g. VXLAN NVGRE etc. as described above to the logical network. The network manager then assigns at a universal identifier such as a UUID to the logical network. The network manager then associates at the logical network to a of transport zone. The network manager then tags at the logical network as opaque. The network manager then sends at the logical network information to all hosts on the same transport zone.

The hosts save at the logical network information. The network manager may also send at the logical network information to compute manager . The compute manager also saves at the LFE information.

The network manager sends at the information of all identified LFEs to the host. For instance the LFE information in some embodiments includes the list of PNICs connected to the LFE the list of identifiers e.g. UUIDs of the transport zones associated with the LFE and the health status of the LFE. The host saves at the LFEs information.

The DRS determines at the VM s opaque logical network. As described above when a host joins a transport zone the host receives the information about the opaque LFEs assigned to the transport zone. The DRS determines e.g. by looking up the VM s configuration information or receiving information from the network manager the opaque LFE that is connected to the VM s VNIC and the UUID of the transport zone connecting to the VNIC through the LFE. The DRS uses the UUID of the transport zone to identify the opaque network as described above when an opaque network is created the hosts and the compute manager receive information about the opaque network including the UUID of the associated transport zone . In other words the compute manager looks up a given VM s VNIC connection state and discovers the LFE on the host to which the VNIC is connected representing the logical network. The LFE is then mapped to the logical network by the logical network UUID stored on the host.

The DRS then identifies at a host that has the same opaque logical network available. The DRS then determines at whether the identified host satisfies other criteria for the placement of the VM. For instance the hosts in some embodiments are score for placement of VMs based on whether the hosts have enough processing power storage or communication bandwidth available for hosting the VM. Another factor used in some embodiments to score the hosts is the availability of ports on the LFEs. The availability of the logical network and LFE constructs on the virtualization software of each host allows the score of each host to be determined and stored locally at the host. The local availability of the score enables rapid determination of whether a host is eligible to receive a VM.

If not the process proceeds to which was described above. Otherwise the DRS sends at the VM s information to the identified host . The host then starts at the VM. The host then informs at the compute manager of the successful start of the VM. The DRS receives at the indication that the VM has successfully started on the host.

The opaque networks are network manager controller entities starting from the creation and through the subsequent lifecycle management. Since the opaque network is not owned or published by the compute manager the compute manager utilizes the read only logical network constructs provided by the network manager to the virtualization software of the hosts to determine whether a given logical network operates on a specific host. If a set of hosts each contains a network construct with the same unique identifier a VM can be moved between the set of hosts which are members of this network.

Live migration of a VM which entails migrating a VM that is running on a first host to a second without any downtime in the operation of the VM is a known and pervasive technology in modern datacenters and provides several advantages. For instance all VMs operating on a first host can migrate to a second host without any downtime in anticipation of conducting hardware maintenance on the first host. VMs can be moved from one host to another for load balancing the hosts resources. VMs can be proactively moved away from failing or underperforming hosts. For instance one or more VMs can migrate from a host when utilization of a certain resource such as processing storage or communication bandwidth on the host exceeds a predetermined threshold. The VMs can migrate to one or more other hosts with extra resource capacities. The VMs can also be consolidated on fewer hosts when activity i.e. resource requirements is low allowing the extra hosts to be turned off or place them in standby and save power.

The compute manager then determines at the VM s logical network. As described above when a host joins a transport zone the host receives the information about the opaque LFEs assigned to the transport zone. The service on the compute manager that coordinates the VM migrations determines e.g. by looking up the VM s configuration information or receiving information from the network manager the opaque LFE that is connected to the VM s VNIC and the UUID of the transport zone connecting to the VNIC through the LFE. The service uses the UUID of the transport zone to identify the opaque network as described above when an opaque network is created the hosts and the compute manager receive information about the opaque network including the UUID of the associated transport zone . The compute manager then identifies at a potential destination host that has the same opaque logical network available. The compute manager then identifies at the corresponding opaque switch by correlating the transport zones of the opaque logical network and the opaque switch. The compute manager then determines at whether the status of the opaque switch indicates that the switch is unavailable or unhealthy . In some embodiments a status of unavailable indicates that the opaque switch is not ready to have VMs connected e.g. the opaque switch may not have an available port for a VM to connect . If yes the compute manager proceeds to which was described above.

If not the compute manager determines at whether the identified host satisfies other criteria for the placement of the VM. For instance whether the host has enough processing power and storage available for hosting the VM. If not the process proceeds to which was described above to identify a different potential destination host . Otherwise the compute manager sends at the VM s information to the identified destination host.

The destination host starts at the VM on the host. Live migration is orchestrated by compute manager source host and destination host . Although simplifies the process in some embodiments the data stored in the memory of the VM executing in source host is copied to destination host iteratively. That is it is initially copied then pages changed since the initial copy are sent then pages that were modified since the previous transmission is sent etc. until a sufficiently small working set of memory being changed remains at source host . The VM is then stunned and the remaining working set of memory and other state information i.e. of the processor etc. is transmitted to destination host whereupon the VM is immediately resumed on destination host . The VM may be stunned typically for just a fraction of a second and thus no downtime is noticeable. It should be noted that while this process describes a live migration of the VM it is also possible to perform steps with non live migration in which the VM is simply shut down on the source host and then started on the destination host. In this case downtime of the VM would be on the order of a number of minutes but would of course vary depending on the guest operating system and software disk storage latency and throughput and processor resources available to the VM on the source and destination.

The compute manager then identifies at a host that has the same opaque logical network available. The compute manager then determines at whether the host satisfies other criteria for hosting the VM. If not the compute manager proceeds to which was described above. Otherwise the compute manager sends at the VM s information to the identified destination host.

The destination host starts at a shadow VM. Different embodiments provide different fault tolerant VMs. In some embodiments the primary VM continuously transmits event data to the backup VM which runs in lock step a few milliseconds behind the primary. The backup VM is prevented from communicating with the outside world but events identified at the primary VM are injected at the exact right moment of execution of the destination VM i.e. when the instruction pointer hits the correct point in execution so that the backup VM does not deviate from the execution of the primary VM . If the primary VM fails it is halted and the backup VM continues execution but is now allowed to communicate with the outside world in place of the primary. Other embodiments use rapid checkpointing where the backup VM is not running but its state e.g. memory is constantly updated with state information from the primary VM. If the primary fails the backup VM is started from the last good state update without any noticeable downtime.

The compute manager determines at the VM s opaque logical network. The compute manager then identifies at a potential new host that has the same opaque logical network available. The compute manager then determines at whether the host satisfies other criteria for hosting the VM. If not the compute manager proceeds to which was described above to identify a different potential new host. Otherwise the compute manager sends the identity of the VM to the new host and instructs at the new host to start the VM. The new host then starts at the VM by reading the VM s configuration information from a configuration file in storage. Once the VM is started the VM begins booting the operating system from storage in the normal process.

Many of the above described features and applications are implemented as software processes that are specified as a set of instructions recorded on a computer readable storage medium also referred to as computer readable medium . When these instructions are executed by one or more processing unit s e.g. one or more processors cores of processors or other processing units they cause the processing unit s to perform the actions indicated in the instructions. Examples of computer readable media include but are not limited to CD ROMs flash drives RAM chips hard drives EPROMs etc. The computer readable media does not include carrier waves and electronic signals passing wirelessly or over wired connections.

In this specification the term software is meant to include firmware residing in read only memory or applications stored in magnetic storage which can be read into memory for processing by a processor. Also in some embodiments multiple software inventions can be implemented as sub parts of a larger program while remaining distinct software inventions. In some embodiments multiple software inventions can also be implemented as separate programs. Finally any combination of separate programs that together implement a software invention described here is within the scope of the invention. In some embodiments the software programs when installed to operate on one or more electronic systems define one or more specific machine implementations that execute and perform the operations of the software programs.

The bus collectively represents all system peripheral and chipset buses that communicatively connect the numerous internal devices of the electronic system . For instance the bus communicatively connects the processing unit s with the read only memory the system memory and the permanent storage device .

From these various memory units the processing unit s retrieve instructions to execute and data to process in order to execute the processes of the invention. The processing unit s may be a single processor or a multi core processor in different embodiments.

The read only memory stores static data and instructions that are needed by the processing unit s and other modules of the electronic system. The permanent storage device on the other hand is a read and write memory device. This device is a non volatile memory unit that stores instructions and data even when the electronic system is off. Some embodiments of the invention use a mass storage device such as a magnetic or optical disk and its corresponding disk drive as the permanent storage device .

Other embodiments use a removable storage device such as a floppy disk flash drive etc. as the permanent storage device. Like the permanent storage device the system memory is a read and write memory device. However unlike storage device the system memory is a volatile read and write memory such a random access memory. The system memory stores some of the instructions and data that the processor needs at runtime. In some embodiments the invention s processes are stored in the system memory the permanent storage device and or the read only memory . From these various memory units the processing unit s retrieve instructions to execute and data to process in order to execute the processes of some embodiments.

The bus also connects to the input and output devices and . The input devices enable the user to communicate information and select commands to the electronic system. The input devices include alphanumeric keyboards and pointing devices also called cursor control devices . The output devices display images generated by the electronic system. The output devices include printers and display devices such as cathode ray tubes CRT or liquid crystal displays LCD . Some embodiments include devices such as a touchscreen that function as both input and output devices.

Finally as shown in bus also couples electronic system to a network through a network adapter not shown . In this manner the computer can be a part of a network of computers such as a local area network LAN a wide area network WAN or an Intranet or a network of networks such as the Internet. Any or all components of electronic system may be used in conjunction with the invention.

Some embodiments include electronic components such as microprocessors storage and memory that store computer program instructions in a machine readable or computer readable medium alternatively referred to as computer readable storage media machine readable media or machine readable storage media . Some examples of such computer readable media include RAM ROM read only compact discs CD ROM recordable compact discs CD R rewritable compact discs CD RW read only digital versatile discs e.g. DVD ROM dual layer DVD ROM a variety of recordable rewritable DVDs e.g. DVD RAM DVD RW DVD RW etc. flash memory e.g. SD cards mini SD cards micro SD cards etc. magnetic and or solid state hard drives read only and recordable Blu Ray discs ultra density optical discs any other optical or magnetic media and floppy disks. The computer readable media may store a computer program that is executable by at least one processing unit and includes sets of instructions for performing various operations. Examples of computer programs or computer code include machine code such as is produced by a compiler and files including higher level code that are executed by a computer an electronic component or a microprocessor using an interpreter.

While the above discussion primarily refers to microprocessor or multi core processors that execute software some embodiments are performed by one or more integrated circuits such as application specific integrated circuits ASICs or field programmable gate arrays FPGAs . In some embodiments such integrated circuits execute instructions that are stored on the circuit itself

As used in this specification the terms computer server processor and memory all refer to electronic or other technological devices. These terms exclude people or groups of people. For the purposes of the specification the terms display or displaying means displaying on an electronic device. As used in this specification the terms computer readable medium computer readable media and machine readable medium are entirely restricted to tangible physical objects that store information in a form that is readable by a computer. These terms exclude any wireless signals wired download signals and any other ephemeral or transitory signals.

While the invention has been described with reference to numerous specific details one of ordinary skill in the art will recognize that the invention can be embodied in other specific forms without departing from the spirit of the invention. In addition a number of the figures including conceptually illustrate processes. The specific operations of these processes may not be performed in the exact order shown and described. The specific operations may not be performed in one continuous series of operations and different specific operations may be performed in different embodiments. Furthermore the process could be implemented using several sub processes or as part of a larger macro process.

This specification refers throughout to computational and network environments that include virtual machines VMs . However virtual machines are merely one example of data compute nodes DNCs or data compute end nodes also referred to as addressable nodes. DCNs may include non virtualized physical hosts virtual machines containers that run on top of a host operating system without the need for a hypervisor or separate operating system and hypervisor kernel network interface modules.

VMs in some embodiments operate with their own guest operating systems on a host using resources of the host virtualized by virtualization software e.g. a hypervisor virtual machine monitor etc. . The tenant i.e. the owner of the VM can choose which applications to operate on top of the guest operating system. Some containers on the other hand are constructs that run on top of a host operating system without the need for a hypervisor or separate guest operating system. In some embodiments the host operating system isolates the containers for different tenants and therefore provides operating system level segregation of the different groups of applications that operate within different containers. This segregation is akin to the VM segregation that is offered in hypervisor virtualized environments and thus can be viewed as a form of virtualization that isolates different groups of applications that operate in different containers. Such containers are more lightweight than VMs.

A hypervisor kernel network interface module in some embodiments is a non VM DCN that includes a network stack with a hypervisor kernel network interface and receive transmit threads. One example of a hypervisor kernel network interface module is the vmknic module that is part of the ESX hypervisor of VMware Inc.

One of ordinary skill in the art will recognize that while the specification refers to VMs the examples given could be any type of DCNs including physical hosts VMs non VM containers and hypervisor kernel network interface modules. In fact the example networks could include combinations of different types of DCNs in some embodiments.

In view of the foregoing one of ordinary skill in the art would understand that the invention is not to be limited by the foregoing illustrative details but rather is to be defined by the appended claims.

