---

title: System, method, and medium of optimizing load reallocation in an in-memory data management grid
abstract: An in-memory data management (IMDM) system is described that includes an IMDM cluster, a load balancer, and a reallocation processor controller. The IMDM cluster includes a plurality of nodes. The controller determines whether there is an actionable load imbalance of existing data elements, stored among the plurality of nodes, based on a predefined criteria or rule. The controller further identifies a source node from which at least one data element is to be deleted and a target node to which the at least one data element that is to be deleted is to be added. The source node and target node contribute to the actionable load imbalance. The controller copies the at least one data element that is to be deleted from the source node into the target node, and deletes the at least one data element that is to be deleted from the source node.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09652161&OS=09652161&RS=09652161
owner: Software AG USA Inc.
number: 09652161
owner_city: Reston
owner_country: US
publication_date: 20141216
---
The technical field relates in general to in memory data management IMDM systems and more particularly to load reallocation for IMDMs.

Conventionally IMDM systems employ a load balancing algorithm via a load balancer that allocates data across different nodes in a clustered environment. However since data is maintained in memory over a certain period of time there will eventually be an uneven distribution of data as data elements are deleted or expire on their own. The period of time over which the uneven distributions occur may be a few seconds or a few days.

In conventional IMDM systems the load balancer distributes data by identifying the least loaded node in a cluster. Succinctly put however the load balancer does not aid in redistributing total load by reallocating data on then existing nodes. As a result of this situation some nodes in a cluster run slower than others as the slower running nodes are loaded with more data compared to the other nodes in the cluster.

Another issue that arises in conventional IMDM systems is that the load balancer queues requests sent to the heavily loaded nodes. This clearly impacts the quality of service and may even result in financial penalties if service level contracts are violated. The embodiments disclosed herein resolve the issues in conventional IMDM systems using an optimized reallocation process which provides clustered nodes with approximately equal data loads.

Accordingly one or more embodiments provide a method computer device and or non transitory computer readable medium that optimizes load reallocation in an in memory data management IMDM system. The method is implemented in IMDM system that may include an IMDM cluster including a plurality of nodes a load balancer and a reallocation process controller cooperatively operable with the IMDM cluster and the load balancer. The method comprises performing a reallocation cycle including determining by the reallocation process controller whether there is an actionable load imbalance of existing data elements stored among the plurality of nodes included in the IMDM cluster based on a predefined criteria or rule.

When it is determined that there is an actionable load imbalance of existing data elements several other steps of the reallocation cycle are performed. One additional step includes identifying by the reallocation process controller in the plurality of nodes a source node from which at least one data element is to be deleted and a target node into which the at least one data element that is to be deleted is to be added. The source node and target node at least in part contribute to the actionable load imbalance of existing data elements.

The reallocation cycle further includes copying by the reallocation process controller the at least one data element that is to be deleted from the source node into the target node. The reallocation cycle further includes deleting by the reallocation process controller the at least one data element that is to be deleted from the source node.

In another embodiment the predefined criteria or rule used in determining whether there is an actionable load imbalance of existing data elements includes at least one of an amount of data used in the plurality of nodes an expiration of time from a starting point a number of data elements stored in the plurality of nodes an occurrence of a node being added or removed from the IMDM cluster an occurrence of a request from a client application that has been determined to be a trigger by a predictive learning function and an occurrence of a manual trigger.

In another embodiment the source node is identified according to the predefined criteria or rule used to determine whether there is an actionable load imbalance of existing data elements and the target node is identified based on its current load and total capacity obtained by the load balancer.

In another embodiment the at least one data element includes only data elements with an auto expiration date determined in part by the load balancer to be not imminent according to a predetermined standard for imminence.

In another embodiment the at least one data element includes only data elements that are a least recently used data element or a least frequently used data element in their respective nodes.

In another embodiment the method further comprises when after the source node and the target node are identified but before the at least one data element is copied the load balancer receives a write request from a client application pausing operation of the reallocation process controller executing the write request by the load balancer and restarting performing the reallocation cycle.

In another embodiment the method further comprises when after the source node and the target node are identified and after the at least one data element is copied the load balancer receives a request from a client application pausing operation of the reallocation process controller executing the request by the load balancer and restarting performing the reallocation cycle.

It should be noted that also disclosed herein are a system and non transitory computer readable storage medium featuring the functionality immediately above described.

One or a combination of more than one or all of the above embodiments can be combined and provided as a single embodiment.

It should also be noted that the purpose of the foregoing abstract is to enable the U.S. Patent and Trademark Office and the public generally and especially the scientists engineers and practitioners in the art who are not familiar with patent or legal terms or phraseology to determine quickly from a cursory inspection the nature and essence of the technical disclosure of the application. The abstract is neither intended to define the invention of the application which is measured by the claims nor is it intended to be limiting as to the scope of the invention in any way.

In overview the present disclosure concerns an improvement for an in memory data grid in which for example multiple nodes are triggered through a load balancer each node handles one or many unique data elements and then some time after operation the number of data elements belonging to one or more nodes are not distributed evenly. More particularly various inventive concepts and principles are embodied in systems devices and methods therein to identify and correct the imbalance situation in which the imbalance situation is recognized the appropriate nodes and data elements to reallocate the data elements nodes are selected and then a reallocation of the data elements which are re assigned to another node is performed thereby to gain back performance deficiencies.

As further discussed herein various inventive principles and combinations thereof are advantagenously employed to identify an imbalanced node set to identify which nodes and which elements shall be reallocated which can use certain special care and to perform the reallocation without affecting the general responsiveness of the whole system that uses the in memory data grid.

The embodiments disclosed herein are used in an in memory environment and aid in reallocating the load of different nodes based on a variety of factors. Reallocation as described herein may be contrasted with the load balancer conventionally used in an in memory environment. Specifically the conventional load balancer analyzes the load on each node prior to allocating additional load whereas reallocation involves moving existing load among nodes based on several criteria. Reallocation of load for in memory systems has become more important as in memory systems have become more complicated with data being transient and non persistent over longer time frames.

Optimized reallocation is required when an in memory data management IMDM system has been running for an extended period. Although the nodes may have started out with roughly equal loads eventually the nodes become unequal or varied. This results for a number of reasons for example from objects in memory expiring or being deleted. Additionally larger objects may be added.

The herein disclosed optimized reallocation process may be based on many different parameters such as load threshold data threshold change in number of nodes etc. Reallocation of data on the nodes allows each node to continue to maintain a desired latency. The conventional load balancer s allocation system will simply not work unless reallocation eventually occurs. Reallocation will allow IMDM systems to avoid related performance issues when the system has been running continuously for a period of time.

The prior art related to providing an equitable distribution of data does not specifically address reallocation of data in an in memory system which has unique problems. For example U.S. Pat. No. 8 769 238 to Sivasubramanian et al. Sivasubramanian discloses how data can be redistributed for any shared resource by restriping the data. However Sivasubramanian does not focus on the data management aspects for an in memory system. Rather it focuses on restriping data and on a shared resource. An in memory system is mostly deployed in a dedicated setup.

Sivasubramanian is additionally designed to support different storage systems that are being used as a shared resource. Memory could also be used as a shared resource e.g. the storage systems but the memory would not be rebalanced based on the needs of an in memory data management system. Moving data to balance out storage is a different problem than rebalancing a software system using memory during runtime.

Another prior art reference U.S. Patent Application Publication No. 2012 0136835 which was filed by Kosuru et al. Kosuru discloses a process for rebalancing data in a database. Again distributing data in a database presents different challenges than reallocating load in an in memory data management system. This is true whether the distributing of data is in a single database or over a network of interconnected databases.

There are several commercial in memory vendors. However these vendors have not focused on load reallocation or load balancing and in particular load reallocation or load balancing in IMDM systems. Oracle Coherence Hazelcast IBM eXtremeScale and Tibco ActiveSpaces are examples of in memory vendors which are conventional and do not include the features disclosed herein.

In GridGain the load balancing process moves jobs from nodes that are loaded to nodes with lighter loads. This is not the same as in memory data management. Succinctly put GridGain balances load by balancing where to run jobs versus optimized operation of a node itself by reallocating data across a cluster.

There are products available that implement load balancing for an in memory system. However as discussed herein in detail available load balancing focuses on balancing initial load distribution rather than reallocation after an unevenness in load distribution occurs. The prior art also includes solutions where network traffic is rebalanced. However reallocation of for example transient objects in memory such as disclosed herein is much different.

As further discussed herein below various inventive principles and combinations thereof are advantageously employed to optimize reallocation of node load in an IMDM system. As such a reasonable and desired latency in accessing data elements in the IMDM system may be achieved. Turning then to a diagram illustrating an initial state of load allocation in an IMDM and illustrating the state of load allocation of the IMDM of after a lapse of time are described.

In an initial state the nodes are pre loaded with data elements represented by the cylinders. An assumption made is that the nodes are evenly loaded prior to the load balancer beginning communication. This is because some data has to be bulk loaded. Thus in there are three data elements in each of the nodes prior to communication by the load balancer .

Several client applications communicate with the IMDM cluster and all requests for data are channeled through the load balancer . For the sake of simplicity assumes that all data in the cluster belongs to a single cache and that each key is unique with no data duplication. It should be noted however that in actuality the entire IMDM system may be implemented by one or more machines either virtual or real in one or more networks. That is to say the architecture illustrated in may be viewed generally as a logical architecture.

Turning now to a diagram illustrating load allocation in an IMDM system after some time has lapsed is described. The IMDM system is of the same basic structure as the IMDM system in . As mentioned above after a period of time has elapsed the load on the nodes in the IMDM cluster is not as evenly balanced as in as certain data elements cylinders are expired or deleted based on the client applications needs.

As seen in the first node and the fourth node each have six data elements. The fifth node has five data elements the third node has four data elements and the second node has two data elements. Once an uneven distribution of data elements occurs among nodes as seen in requests from the client applications that route to the first node and the fourth node will take longer to be processed than requests to the other four nodes because additional data elements must be searched. As well the load balancer might queue up the requests for the first node and the fourth node resulting in further degradation of the quality of service.

The inventors have observed that the load balancer may have information reflecting the lack of balance of load on the nodes but the load balancer is not designed to reallocate the data elements on the nodes as it is dealing with the incoming requests from the client applications . In this regard it should be noted as discussed below that reallocation should not impact incoming requests from client applications to the load balancer . Even so reallocation can be done in order to simply avoid the load balancer queuing up requests to read or write data from loaded nodes such as the first node and the fourth node .

The reallocation process can use one or more techniques to allocate the data elements across the nodes . The optimized reallocation process can be run as a background process on the IMDM cluster that wakes up based on one or more criteria. As described above when an unevenness in load occurs over time the reallocation process begins. It should be noted that the reallocation process does not begin at any unevenness but only when there is an actionable load imbalance.

The exact criteria for when the reallocation process is triggered are now described. One or more of the following may be used or variations on the following 

1. Data Amount Threshold Each node in the IMDM cluster will have a setting related to the amount of data used in node that triggers reallocation. For example when a node with 100 GB of total RAM is determined to have reached 80 GB of RAM in use reallocation will be triggered. The data amount threshold setting may either be a node level setting or a cluster level setting.

2. Time Threshold Each node in the IMDM cluster will have a setting related to the amount of elapsed time that triggers reallocation. That is to say the cluster or an individual node nodes can be setup to trigger reallocation after a certain period of time has expired. The time period could range from every few seconds or every few days depending on the need for reallocation in a specific environment. The time threshold may be a node level setting or a cluster level setting.

3. Number of Elements Threshold Each node in the IMDM cluster will have a setting related to the number of elements illustrated as cylinders in used in a node that triggers reallocation. For example reallocation may be triggered if the number of elements on a node exceeds a threshold such as 10 000. It should be clear that the number of elements threshold is different from the data amount threshold as the number of elements threshold does not take into account the size of the data elements.

This difference can be quite important as a node could have only 10 GB of data for example but tens of thousands of small elements. As a result many more client application requests may hit the low data amount node with respect to other nodes of larger data amounts. Such a scenario would certainly result in degradation of the quality of service. It should be noted that the number of elements threshold may be a node level setting or a cluster level setting.

4. Addition or Removal of a Node from the Cluster This cluster level setting automatically triggers the reallocation process when a new node requires a load. Similarly the reallocation process is automatically triggered when a node is removed or has become non functioning has crashed . In the case of removal the available capacity of the entire IMDM cluster is decreased. Therefore the reallocation process will review distribution of data in the nodes in the cluster in order to avoid impacting the quality of service.

5. Manual Trigger The reallocation process can be manually triggered from a management console used with the IMDM system . Typically a system administrator will have privileges to manually trigger the reallocation process .

6. Predictive Reallocation The reallocation process is triggered through the use of predictive analytics that correlate historical data with current real time data. Predictive reallocation involves designing a model once and optimizing the model over time depending on the data generated. The reallocation process thus learns by reviewing patterns over a period of time.

For example if after a certain request from a client application the load on the nodes increases the reallocation process anticipates that when the specific request is made again reallocation will be necessary. It should thus be understood that two exactly similar deployments of the IMDM system could have very different predictive models over a period of time as the users of the two systems act in different fashions with their requests. The benefit of predictive reallocation is that data elements can be reallocated without any thresholds being established. Succinctly put the thresholds that improve the overall reallocation process are learned over a period of time.

 1 As described above the initial step is first determining that there is an imbalanced set of in memory nodes based on the defined rules and or criteria above.

 3 In the third step the source nodes are identified based on the trigger criteria for the reallocation process and the target nodes are identified based on their current load and their total capacity. It is possible that a reallocation process is interrupted if the load balancer sends more data elements to a target node or deletes data elements that were being migrated from a source node. In such instances the reallocation process will again identify source and target nodes

 4 A fourth step in the reallocation process is identifying the actual data elements in the source node that are to be moved. This is certainly a non trivial action. Data elements are created with auto expiration and or may be deleted based on least recent use or some other algorithm used in managing their life cycle.

For example the reallocation process can be configured to ignore data elements that are expiring in the next ten seconds. The reallocation process would check the metadata of each data element to determine the expiry time of each data element. Time expiration of nodes can be managed in the reallocation process by a property called IgnoreElementsWithExpiryTime. The value of this property can be maintained in seconds for example and can be accessed at the time the reallocation process is starting.

Additionally the reallocation process may work with the load balancer to determine data elements that will be removed from memory using other cache algorithms. These algorithms determine which data elements may be discarded to effect a more even distribution of data elements in the nodes in a cluster. This may be particularly desirable where memory capacity is low. Two examples of such algorithms include the least recently used LRU algorithm mentioned above and the least frequently used LFU algorithm. It should certainly be understood that there are other cache algorithms known in the art that are not discussed herein and the examples above should not be viewed as limiting. In implementing cache algorithms the NodeInfo object may be obtained from the load balancer such that the Metadata associated with respective data elements can be retrieved.

At this stage it can be generally stated that identifying data elements to be moved in reallocation includes 

 5 The reallocation process completes these steps to review all of the attributes in order to identify correctly the data elements to be moved.

 6 Once the data elements that are to be reallocated are correctly identified actual movement and or change of ownership of a data element from a source node to a target node can be performed. Examples 1 4 are provided below.

The basic understanding of actual movement of data elements in the reallocation process is that client applications should not be impacted. Thus if a client application is impacted during reallocation the reallocation process can be rolled back preferably in its entirety. Thus the reallocation process may be fully completed with no impact on client applications or the reallocation process can be restarted.

Given that the load state of an IMDM cluster changes quite frequently the reallocation process can operate with the smallest data set possible at any given time. Moving small packages of data is a better approach as it is important to reallocate data elements without interruptions. Since an in memory system may be very dynamic it is preferred to completely reallocate each one of different small packages versus doing no reallocation because of continuous interruptions of larger packages. Normal processing of the in memory system will typically not be impacted because the reallocation process can be established as being lower priority than the load balancer. Four scenarios a to d are now described as example explanations of the reallocation process.

This is the ideal scenario. The reallocation process starts and identifies the source and the target node for the data elements that need to be reallocated. The reallocation process is completed without getting interrupted by any new requests from the load balancer. If new requests from the load balancer are received the reallocation process will give priority to the new request and stop current activity as shown in subsequent scenarios.

In this scenario the reallocation process begins and is working on identifying LFU data elements from a source node when the load balancer receives a request for an edit operation for example a write or a delete on the source node. It should be noted that a read request will not impact reallocation and the reallocation process can proceed as is. However when an edit operation is received the reallocation process pauses and waits for the load balancer request to finish. After the request is completed the reallocation process reassesses the load on the source node and the target node s before further processing. It is possible that upon reassessing no reallocation is required as the load balancer may have moved data around.

 c The Reallocation Process Runs with Interruption while Transferring Identified Source Node Data Elements

In this scenario the reallocation process is interrupted by the load balancer after it has identified the source data elements and is in the process of reallocating them to the target node. The reallocation process again pauses for the load balancer to finish and then reassesses the load on the source node and the target node s before proceeding ahead with processing the transferring.

In this scenario the reallocation process has copied the identified source node data elements to the target node but the data elements have not been deleted from the source node. Nevertheless the reallocation process can still proceed when the load balancer request is received. That is to say deletion of the source node elements can continue as no application is requesting these elements. Further deletions occurring in parallel with the load balancer request have minimal effect as the reallocation process runs at a lower priority than the load balancer.

Examples of implementation details for the various ways load reallocation works are now provided. It should again be noted that the implementation is intended to be conducted such that existing processing related to client applications are not impacted. The reallocation process in this example can start up when the IMDM cluster is initiated. It can run in the background and depending on the configurations it can be triggered to reallocate the data elements across the nodes. Reallocation may work closely with and or use the load balancer to get the current state of the nodes in the cluster and then can start the reallocation process. It can create a copy of data elements from one node to the other by copying the memory references and it can request the load balancer to route requests to the new node before unloading deleting the data elements that originally had the load. This process may be implemented so as to have negligible impact on client applications looking for data although for a short period of time it may result in multiple copies of the same data elements existing on the cluster.

The current state of the cluster is obtained for example from the load balancer which returns an array of node information for nodes within the cluster as shown in the example pseudo code below.

This can be used to identify the source and the target node s by running operations on the NodeInfo object.

For example to obtain total capacity for a specific node the following pseudo code can be implemented 

The source node can be identified if any of the defined criteria is met. A target node can be identified by comparing total capacity of the node with current capacity of the node. Each reallocation can be an atomic transaction. Thus if after reallocation a target node meets the criteria and requires another reallocation this would be a separate transaction. However at any given point target nodes can be identified by comparing current capacity with total capacity of the node. A reallocation process can call these application programming interfaces APIs multiple times as part of a single reallocation which may happen often as this might get interrupted during the reallocation. These APIs can be called multiple times to assure that the state of the nodes has not changed and processing can continue.

The copying of data elements is from a source node with a heavier load to a target node with a lighter load where the first parameter specifies the node with the heavier load and the second node is the lightly loaded node. Meta data for all data elements can be checked before copying them to skip unnecessary copying of data elements. For example if metadata for a data element indicates that it is due to expire shortly it does not make sense to copy the data element. The copying function may also notify the load balancer after the copy has been created. This functionality can be found in the following example pseudo code 

The copy operation can run while ensuring that there are no new requests for edit operations e.g. write or delete from the load balancer. If there are new requests for edit operations the reallocation process may use the logic defined above to pause processing. Pausing can include functionality so that when the copy operation restarts again it checks if it can still move forward with the original operation. If the change while paused on the source and the target nodes requires the reallocation to be canceled then the reallocation process can restart completely from scratch or otherwise can proceed as planned. The copy process implementation can work by getting free memory references from the target node to copy the source data elements.

Deleting can be performed after the requests coming from the client applications have been processed so as to avoid any impact. The delete function can be implemented as in the following pseudo code 

The reallocation process can be designed to look at internals of each data element by reviewing one or more of the expiry time most recently used time frequency of use and the like before deciding to move a specific data element. This intelligence can make the process more efficient as it is more aware of the characteristics of the data versus treating every data element the same way.

Turning back to what occurs before the reallocation process the implementation of the six criteria for beginning the reallocation process is now discussed.

The data threshold when at the cluster level can be stored for example in the clusterconfig.xml file. The data threshold conveniently can be represented in GB. The setting of the cluster threshold at 500 GB can be as follows 

The reallocation process is designed to look for this property at the cluster level. As with many allocation start thresholds the property is optional so may not exist all the time. The data threshold at the node level can have a similar setting stored in association with different nodes for example in the nodeconfig.xml. This can allow allows each node to have a different data threshold in order for reallocation to begin. This property at the node level can also be optional and all numbers conveniently can be represented in GB such as a node threshold at 100 GB 

The time threshold at the cluster level can be stored for example in the clusterconfig.xml file. The time threshold conveniently can be represented in seconds. The setting for 36 000 seconds can be as follows 

The reallocation process can be designed to look for this property at the cluster level. This property can be optional so may not exist all the time. The node level can have a similar setting stored in association with different nodes for example in the nodeconfig.xml. This can allow each node to have a different time threshold before reallocation starts. This property at the node level can be optional and all numbers conveniently can be represented in seconds such as 

The number of elements threshold at the cluster level can be stored for example in the clusterconfig.xml file. The number of elements threshold can be represented in an element count such as a threshold of 10 000 total elements within the cluster 

The reallocation process can be designed to look for this property at the cluster level. This property can be optional so may not exist all the time. The node level may have a similar setting which is stored for example in the nodeconfig.xml file. This allows each node to have a different threshold to start reallocating. This property also can be optional and all numbers conveniently can be represented as an element count such as a threshold of 1 000 total elements within the node 

For all the above criteria including data amount time and number of elements the reallocation process can be designed to check for the property. If the property exists both at the cluster level and the node level then the node level can overwrite the property set at the cluster level. When the property is only set at the cluster level then the reallocation process can continue to work closely with the load balancer and or to utilize load balancer information to get the current state of all the nodes in the cluster and then the reallocation process can trigger based on the criteria that is based on the current state obtained from the load balancer vs. the property ies . If the property is set only at the node level then the reallocation process can get the state of the node from that specific node directly and can the reallocation process can trigger based on defined criteria and the state of the node obtained from the specific node.

The reallocation process can maintain a map conveniently internal to the reallocation process which can keep track of criteria at the node level. Table 1 below is an example of such a map. This map can be continuously compared for example on a periodic basis against the current state of the nodes before the reallocation is triggered.

This criteria relates of course to two different events and can be very different from the other criteria. Addition of a node introduces more capacity into the cluster. The load balancer can start assigning capacity when a node is added but the reallocation process can actually move data elements from heavily loaded nodes to the newly added node. The intention behind reallocation is improving the overall latency for the cluster.

Removal of a node whether intentional or unintentional such as due to a crash results in reduced capacity. If the removal is intentional there should be no data loss so reallocating can be easier as it reshuffles the data onto existing nodes to avoid impacts to the quality of service. However if the removal is due to a crash then it can be important to determine the node s that should get the lost data. This can be done for example by either coordinating with the recovery process by reading from disk or by working with the client applications to distribute the data to the lightly loaded nodes. In these scenarios it is highly likely that the reallocation process will need to be extended to work with the client application s or the recovery process.

This criteria of course involves a manual triggering of the reallocation process for example by receiving a reallocation command from an administrative or management console for the IMDM system. As discussed above an appropriately authorized user such as an administrator for the system can initiate this reallocation of the data elements across the different nodes in the cluster.

In particular the EPL interface provides a path for uploading of the PMML file . After processing by the JAVA Plugin and the ADAPA Jar the APAMA correlator can maintain data as to the effects of particular events . On certain events occurring the APAMA correlator may provide instructions to begin reallocation based on these triggering events.

It should be noted that streaming events provided from the load balancer can be sent to using a language such as XML. The events may have the following structure which is provided by way of example to illustrate an appropriate event format which may be adapted as desired 

The reallocation process already knows the available capacity and it can use the above real time information in the Apama correlator along with historical data in the Hadoop cluster to determine when reallocation is be done. The request section can be used to define patterns within the predictive model. As an example if the historical data shows a sequence of events occurring before a large load is put in memory and the last call is usually a put for the user object then the predictive model can learn from that and predict the state to run a preemptive reallocation process. As the historical data grows the accuracy of the predictive model will increase and the timing of the reallocation process is improved.

Another example would be to build a model for an eCommerce environment where the predictive model looks at the put userKey request and knows that a large data load is imminent within the next few seconds as the user might place an order. This may cause running of a preemptive reallocation process. As the model becomes more intelligent it may identify characteristics of a particular user like the average order data size which will pre determine the space that needs to be freed up to keep that order in memory thus preemptively running the reallocation process.

It may be noted that the ADAPA APIs designed by Zementis can enable the functionality described above as they have built in capability to work with a predictive model however the techniques described herein may be adapted to other APIs or similar. ADAPA predictive analytics engine supports the REST standard so they can connect to any URL.

The predictive reallocation approach can be more expensive than other thresholds. Thus in operation it is thought that the predictive reallocation approach may be best suited for larger deployments dealing with at least hundreds of terabytes of data to justify the setup costs. Though expensive predictive reallocation can be most accurate and beneficial with its ability to preemptively reallocate the data elements in the system.

Reference is now made to which is a block diagram illustrating an IMDM apparatus in an IMDM system that implements a reallocation process. The IMDM apparatus may include a reallocation process controller an input output I O interface for communication such as over a network IMDM memory a display and a user input device such as a keyboard . Alternatively or in addition to the keyboard a user input device may comprise one or more of various known input devices such as a keypad a computer mouse a touchpad a touch screen a trackball and or a keyboard. The display may present information to the user by way of a conventional liquid crystal display LCD or other visual display and or by way of a conventional audible device for example a speaker for playing out audible messages. Portions of the IMDM apparatus are well understood to those of skill in this area and have been omitted to avoid obscuring the discussion.

The IMDM apparatus is connected through the network to various memory locations represented in this illustration by a first memory location a second memory location and an Nth memory location . The memory locations provide a plurality of nodes as discussed herein that form an IMDM grid cluster. That is to say the memory locations collectively form an IMDM cluster in which a plurality of data elements not shown can be stored. The memory locations are considered to provide in memory access to the reallocation processor controller . The memory locations are in part controlled by the load balancer which is connected with both the IMDM apparatus and the memory locations which provides an initial distribution of the data elements stored in the plurality of nodes formed by the memory locations according to known techniques.

Although the memory locations appear in to be disposed as physically separate this is not necessarily the case. The illustrated configuration is simply exemplary. The memory locations that form the plurality of nodes may be disposed virtually or physically on a single machine or on a plurality of machines. Also although illustrated as being connected over the network the memory locations may be disposed as completely within the IMDM apparatus itself. Succinctly put the memory locations may be provided in many different any configuration to provide in memory access to the reallocation process controller .

The reallocation process controller may comprise one or more microprocessors and or one or more digital signal processors. The IMDM memory may be coupled to the reallocation process controller and may comprise a read only memory ROM a random access memory RAM a programmable ROM PROM and or an electrically erasable read only memory EEPROM . The IMDM memory may include multiple memory locations for storing among other things an operating system data and variables for programs executed by the controller . The programs may cause the controller to operate in connection with various functions. It should be noted that the IMDM memory may be dedicated for use with the reallocation process controller rather than for data element storage as in memory access.

A first program may cause the reallocation process controller to determine whether there is an actionable load imbalance of existing data elements stored among the plurality of nodes included in the IMDM cluster based on a predefined criteria or rule. Thus for example when an amount of data used in each node exceeds a particular threshold or actionable amount the reallocation process begins. Other thresholds or actionable quantities include an expiration of time from a starting point and a number of data elements stored in each node. Other triggering or actionable events include an occurrence of a node being added or removed from the IMDM cluster an occurrence of a request from a client application that has been determined to be a trigger by a predictive learning function and an occurrence of a manual trigger.

When the result of the first program is a determination that there is an actionable load imbalance of existing data elements other programs cause the reallocation process controller to perform in a particular manner including performing a reallocation cycle. Specifically a second program may cause the reallocation process controller to identify in the plurality of nodes a source node from which at least one data element is to be deleted and a target node into which the at least one data element that is to be deleted is to be added. In the second program the source node and target node contribute at least in part to the actionable load imbalance of existing data elements.

In the second program the source node may be identified according to the predefined criteria or rule used to determine whether there is an actionable load imbalance of existing data elements. The target node may be identified based on its current load and total capacity as obtained by the load balancer .

A third program may cause the reallocation process controller to copy the at least one data element that is to be deleted from the source node into the target node. A fourth program may cause the reallocation process controller to delete the at least one data element that is to be deleted from the source node. The at least one data element that is to be deleted may include only data elements with an auto expiration date that is imminent. Of course whether something is imminent is decided according to a predetermined standard for imminence such as in IgnoreElementsWithExpiryTime described above .

Responsive to manual signaling from the user input device represented by the keyboard or in accordance with instructions stored in the IMDM memory and or automatically upon receipt of certain information via the I O interface the controller may direct the execution of the stored programs .

The IMDM apparatus can accommodate one or more disk drives or removable storage not illustrated . Typically these might be one or more of the following a flash memory a floppy disk drive a hard disk drive a CD ROM a digital video disk an optical disk and or a removable storage device such as a USB memory stick variations and evolutions thereof. The number and type of drives and removable storage may vary typically with different computer configurations. Disk drives may be options and for space considerations may be omitted from the IMDM apparatus used in conjunction with the processes described herein. The IMDM apparatus may also include a CD ROM reader and CD recorder which may be interconnected by a bus along with other peripheral devices which may be included and which may be supported by the bus structure and protocol not illustrated . The bus can serve as the main information highway interconnecting other components of the IMDM apparatus and can be connected via an interface to the IMDM apparatus . A disk controller not illustrated can interface disk drives to the system bus. These may be internal or external. The reallocation process controller IMDM memory a disk drive and or removable storage medium may be referred to as computer readable storage media and provide non transitory storage of computer programs and data.

As indicated above should be understood to describe components in connection with logical groupings of functions or resources. One or more of these logical groupings may be performed by different components from one or more embodiments. Likewise functions may be grouped differently combined or augmented without parting from the scope. Similarly the present description may describe various databases or collections of data and information. One or more groupings of the data or information may be omitted distributed combined or augmented or provided locally and or remotely without departing from the scope.

The first step is determining whether there is an imbalance of existing data elements stored among a plurality of nodes based on a predefined criteria or rule. The second step is identifying a source node in the plurality of nodes from which at least one data element should be deleted and a target node in the plurality of nodes to which the at least one data element should be added. The third step is copying the at least one data element from the source node into the target node. The fourth step is deleting the at least one data element from the source node. The process then ends or may be repeated not shown .

Terms as used herein are intended to be interpreted at a first level as understood to one of skill in the art of in memory data management and if not interpretable at the first level then at a second level as understood to one of skill in the art of computer science and then if not interpretable according to the first level and second level according to a more general dictionary.

The claims may use the following terms which are defined to have the following meanings for the purpose of the claims herein. Other definitions may be specified in this document.

The expressions in memory data grid in memory data management grid or IMDG as used herein refers to a data structure that resides entirely in random access memory RAM and is distributed on a single server or among multiple servers.

The expression in memory data management system or IMDM system or in memory data management cluster or IMDM cluster as used herein refers to a single apparatus or multiple apparatuses that physically support an IMDG and that contain components for controlling and or operating an IMDG.

The term cluster as used herein refers to a set of loosely connected or tightly connected computers that work together so that they may be viewed in many respects as a single system.

The term node as used herein refers to a single instance of a server that is used for an application. This can be setup on a virtualized server or a dedicated server.

The expression data element as user herein refers to a root level piece of information that is stored in a node.

The expression load balancer indicates a process of or an apparatus that implements a process of distributing new data elements to nodes that comprise an IMDG in a manner that seeks to achieve an equitable distribution of the new data elements. Various load balancers are known.

The above discussion has assumed that the reader has a sufficient background for appreciating the points made. This section provides some supplementary implementation and or technical notes which discuss some basic technical information that may be relevant to the above.

This discussion has involved particular examples. However the principles may be applied to other examples and or realizations. Naturally the relevant data may differ as appropriate. Further an embodiment has been discussed in certain examples as if it is made available by a provider to a single customer with a single site. An embodiment may be used by numerous users if preferred for example over distributed systems.

It is further understood that the use of relational terms such as first and second and the like if any are used solely to distinguish one from another entity item or action without necessarily requiring or implying any actual such relationship or order between such entities items or actions. It is noted that some embodiments may include a plurality of processes or steps which can be performed in any order unless expressly and necessarily limited to a particular order i.e. processes or steps that are not so limited may be performed in any order.

Much of the inventive functionality and many of the inventive principles when implemented are best supported with or in software or one or more integrated circuits ICs such as a central processing unit CPU which is the hardware that carries out instructions of a computer program and software therefore and or application specific ICs. It is expected that one of ordinary skill notwithstanding possibly significant effort and many design choices motivated by for example available time current technology and economic considerations when guided by the concepts and principles disclosed herein will be readily capable of generating such software instructions or ICs with minimal experimentation. Therefore in the interest of brevity and minimization of any risk of obscuring principles and concepts further discussion of such software and ICs if any will be limited to the essentials with respect to the principles and concepts used by the exemplary embodiments.

The various embodiments which demonstrate a method and or system for optimizing load reallocation in an in memory data management system have been discussed in detail above. It should be further noted that the above described processes can be stored as instructions in a computer readable storage medium. When the instructions are executed by a computer for example after being loaded from a computer readable storage medium the process es are performed. The detailed descriptions which appear herein may be presented in terms of program procedures executed on a computer or a network of computers. These procedural descriptions and representations herein are the means used by those skilled in the art to most effectively convey the substance of their work to others skilled in the art.

A procedure is generally conceived to be a self consistent sequence of steps leading to a desired result. These steps are those requiring physical manipulations of physical quantities. Usually though not necessarily these quantities take the form of electrical or magnetic signals capable of being stored on non transitory computer readable media transferred combined compared and otherwise manipulated. It proves convenient at times principally for reasons of common usage to refer to these signals as bits values elements symbols characters terms numbers or the like. It should be noted however that all of these and similar terms are to be associated with the appropriate physical quantities and are merely convenient labels applied to these quantities.

Further the manipulations performed are often referred to in terms such as adding or comparing which are commonly associated with mental operations performed by a human operator. While the discussion herein may contemplate the use of an operator a human operator is not necessary or desirable in most cases to perform the actual functions described herein the operations are machine operations.

Various computers or computer systems may be programmed with programs written in accordance with the teachings herein or it may prove more convenient to construct a more specialized apparatus to perform the required method steps. The required structure for a variety of these machines will be apparent from the description given herein.

A computer readable storage medium is tangible and non transitory a computer readable storage medium can be any of the memory or storage devices such as those examples described above or other removable or fixed storage medium provided such computer readable storage medium is tangible and non transitory.

Furthermore any communication network implicated in an embodiment can include by way of example but not limitation data and or packet communications networks which can provide wireless communications capability and or utilize wireline connections such as cable and or a connector or similar. Any appropriate communication protocol may be used.

The computer and or system embodied in connection herewith may or may not rely on the integration of various components including as appropriate and or if desired by way of example but not limitation hardware and software servers applications software database engines server area networks firewall and SSL security production back up systems and or applications interface software. An embodiment may be by way of example but not by way of limitation network based and may or may not utilize a network such as the Internet or other network as an exemplary interface with the user for any information delivery.

One or more databases implicated by the above discussion may be by way of example but not limitation in a relational database format but other standard data formats may also be used. Optionally the various databases may include a known conversion system capable of receiving data in various standard formats.

One or more displays for the system may be developed in connection with by way of example but not limitation HTML display format. Although HTML may be a preferred display format it is possible to utilize alternative display formats for interacting with a user and obtaining user instructions.

This disclosure is intended to explain how to fashion and use various embodiments in accordance with the invention rather than to limit the true intended and fair scope and spirit thereof. The invention is defined solely by the appended claims as they may be amended during the pendency of this application for patent and all equivalents thereof. The foregoing description is not intended to be exhaustive or to limit the invention to the precise form disclosed. Modifications or variations are possible in light of the above teachings. The embodiment s was chosen and described to provide the best illustration of the principles of the invention and its practical application and to enable one of ordinary skill in the art to utilize the invention in various embodiments and with various modifications as are suited to the particular use contemplated. All such modifications and variations are within the scope of the invention as determined by the appended claims as may be amended during the pendency of this application for patent and all equivalents thereof when interpreted in accordance with the breadth to which they are fairly legally and equitably entitled.

