---

title: Continuous voice sensing
abstract: Provided are methods and systems for continuous voice sensing. An example method allows for detecting and buffering, by a first module, a key phrase in an acoustic signal. Responsive to the detection, the method includes sending an interrupt to a second module and switching the first module to an omnidirectional microphone mode. Upon receiving the interrupt, the second module is operable to boot up from a low power mode to an operational mode. While the second module is booting up, the first module is operable to continue to buffer a clean speech output generated from an acoustic signal captured by at least one omnidirectional microphone. After the second module is booted, an indication may be sent to the first module that the second module is ready to exchange data through a fast connection. Upon receiving the indication, the buffered clean speech output may be sent to the second module.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09508345&OS=09508345&RS=09508345
owner: Knowles Electronics, LLC
number: 09508345
owner_city: Itasca
owner_country: US
publication_date: 20140924
---
The present application claims the benefit of U.S. Provisional Application No. 61 881 868 filed on Sep. 24 2013. The subject matter of the aforementioned application is incorporated herein by reference for all purposes.

The present application relates generally to audio processing and more specifically to systems and methods for continuous voice sensing.

In the majority of the existing mobile systems access to a mobile device or services may be obtained for example by touching the mobile device to wake it up from a low power state also referred to herein as a low power mode or sleep mode . In the low power mode it is desired that the power consumption be minimized to maximize the battery charge life span. Once the mobile device is awakened it may operate at an operational mode which consumes more power than the low state mode. An application processor AP may execute different voice sensing applications including a voice user interface to operate the mobile device. Some mobile devices can allow waking up the mobile device from the sleep mode using a voice command. The time to wake up the application processor and the time to startup a voice recognition application can take quite a long time e.g. between 500 ms to up to 1 or 2 seconds . Systems and methods for a smooth and fast transition from a low power sleep mode to a fully operational mode are desirable.

This summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used as an aid in determining the scope of the claimed subject matter.

Provided are systems and methods for continuous voice sensing. According to an example embodiment the method for continuous voice sensing may include detecting and buffering by a first module a key phrase in an acoustic signal. The acoustic signal can include at least one captured sound. In some embodiments the acoustic signal may be provided to the first module by at least one Pulse Density Modulation digital microphone.

Responsive to the detection the method may include sending an interrupt to a second module and switching the first module to an omnidirectional microphone mode. In some embodiments the interrupt can be sent via a General Purpose Input Output port. The method may further include booting up the second module from a low power mode to an operational mode. In certain embodiments the boot time can be 30 ms.

In various embodiments while booting the second module the method may further include continue buffering by the first module a clean speech output. The clean speech output can be generated by the first module from a further acoustic signal using an automatic speech recognition assist application. In some embodiments the further acoustic signal can be captured by at least one omnidirectional microphone.

In some embodiments after booting the second module the method may allow sending by the second module to the first module an indication that the second module is ready to exchange data using one of the following interfaces SLIMbus Universal Asynchronous Receiver Transmitter UART System Packet Interface SPI and I2C

In some embodiments the method further proceeds with sending the clean speech output buffered by the first module to the second module. In further embodiments the method further includes exchanging further data between the first module and the second module.

In various embodiments the method further comprises detecting and buffering another key phrase and response to the detection of both key phrases triggering at least two of different voice search engines different voice search engines or different applications.

In some embodiments the detecting and buffering by a first module a key phrase in an acoustic signal comprises detecting and buffering separately two key phrases such that clean speech output associated with the two key phrases are provided for respectively launching different voice search engines different voice recognition engines or different applications. For example each key phrase may be provided for triggering launch of a different one of two voice search engines different one of two voice recognition engines or different one of two applications.

According to another example embodiment of the present disclosure the steps of the method for continuous voice sensing are stored on a non transitory machine readable medium comprising instructions which when implemented by one or more processors perform the recited steps.

Other example embodiments of the disclosure and aspects will become apparent from the following description taken in conjunction with the following drawings.

The present disclosure provides example systems and methods for voice sensing and authentication. By way of example and not limitation embodiments of the present disclosure can be practiced on mobile devices.

According to various example embodiments a method for continuous voice sensing includes detecting and buffering by a first module a key phrase in an acoustic signal. The acoustic signal can include at least one captured sound. Upon detection of the key phrase or keyword the method may allow sending an interrupt to a second module and switching the first module to an omnidirectional microphone mode. After receiving the interrupt the second module may boot from a low power mode to an operational mode and send an indication that it is ready to exchange data with the first module via a fast connection. While the second module is booting the method may include continue buffering by the first module a clean speech output. The method may proceed with sending by the first module the clean speech output to the second module. The method may further include exchanging further data between the first module and second module.

Referring now to an example environment is shown in which a method for continuous voice sensing can be implemented. The example environment can include a mobile device . In various embodiments the mobile device may be operable to receive acoustic signal s . In some embodiments the mobile device can be operable to receive acoustic sound s from a user . In certain embodiments the mobile device may include one or more microphones . The acoustic signal s can be captured by the one or more microphones . In various embodiments the mobile device can be further operable to process the received acoustic input signal s in order to detect a voice one or more spoken keyword s and so forth.

The acoustic input signals may be contaminated by noise . Noise can include unwanted sound present in the environment which may be detected by for example sensors such as microphones . In stationary environments noise sources may include street noises ambient noises sounds from the mobile device such as audio speech from entities other than an intended speaker s and the like. Mobile environments can encounter certain kinds of noise which arise from their operations and the environments in which they operate.

In some embodiments the environment may include one or more cloud based computing resources also referred as a computing cloud s . The cloud based computing resource s can include computing resources hardware and software available at a remote location and accessible over a network for example the Internet . The cloud based computing resources can be shared by multiple users and can be dynamically reallocated based on demand. The cloud based computing resources may include one or more server farms clusters including a collection of computer servers which can be colocated with network switches and or routers. In various embodiments the mobile device can be connected to the computing cloud via one or more wired or wireless communications networks .

The processor may include hardware and or software which is operable to execute computer programs stored for example in a memory storage . The processor may use floating point operations complex operations and other operations including for example continuous voice sensing. In some embodiments the processor may include an Application Processor AP . In certain embodiments the AP may include a circuit e.g. a dedicated chipset running an operating system and software applications on the mobile device .

In various embodiments the software applications may include an Automatic Speech Recognition Assist ASR A . The ASR A may be a specialized Noise Suppression method or system minimizing the background noise when maximizing the correct automatic recognition of the device target user speech. Various embodiments of the ASR A may include a module executed as a front end module of an ASR server located either on the mobile device or in the computing cloud shown in .

The software applications may include a Voice Sense VS . The VS may include detection of voice commands while a mobile device is in a low power mode.

In some embodiments the software applications may include Continuous Voice Sense CVS . The CVS may include detection of voice commands while a mobile device is in a low power mode and seamless transition to an ASR A mode.

The graphic display system can provide a user graphic interface. In some embodiments a touch screen associated with the graphic display system can be utilized to receive an input from a user. The options can be provided to a user via an icon or text buttons once the user touches the screen.

The audio processing system can be operable to receive acoustic signals from an acoustic source via one or more microphones and process the acoustic signal components. The microphones can be spaced a distance apart such that the acoustic waves impinging on the device from certain directions exhibit different energy levels at the two or more microphones. After reception by the microphones the acoustic signals can be converted into electric signals. These electric signals can in turn be converted by an analog to digital converter not shown into digital signals for processing in accordance with some embodiments. In some embodiments the microphones may include pulse density modulation PDM microphones also referred to as digital microphones. The PDM may include a system for representing a sampled signal as a stream of single bits. In some embodiments the PDM may be a high sampling rate single bit digital system.

In various embodiments where the microphones are omni directional microphones that are closely spaced e.g. 1 2 cm apart a beamforming technique can be used to simulate a forward facing and backward facing directional microphone response. A level difference can be obtained using the simulated forward facing and a backward facing directional microphone. The level difference can be used to discriminate speech and noise in for example the time frequency domain which can be used in noise and or echo reduction. In certain embodiments some microphones can be used mainly to detect speech and other microphones can be used mainly to detect noise. In other embodiments some microphones can be used to detect both noise and speech.

In order to suppress the noise the audio processing system may include a noise reduction module . Noise suppression NS can be carried out by the audio processing system and noise reduction module of the mobile device based variously on an inter microphone level difference level salience pitch salience signal type classification speaker identification and so forth. By way of example and not limitation noise reduction methods are described in U.S. Utility patent application Ser. No. 12 215 980 entitled System and Method for Providing Noise Suppression Utilizing Null Processing Noise Subtraction filed Jun. 30 2008 and in U.S. Utility patent application Ser. No. 11 699 732 entitled System and Method for Utilizing Omni Directional Microphones for Speech Enhancement filed Jan. 29 2007 disclosures of which are incorporated herein by reference for the foregoing purposes.

The example systems in include many similar elements but have different configurations regarding microphones and the coupling thereto as is described below and shown in . In the system includes digital microphones three shown in this example coupled to a Digital Audio Processor DAP module . In the system includes analog microphones two shown in this example coupled to a codec and at least one digital microphone coupled to the DAP module operable to receive acoustic sounds from at least one digital microphone . Various elements and operations of the systems and are described further below.

Various embodiments of the DAP module may include any of an electronic circuit a combinational logic circuit an integrated circuit an application specific standard product ASSP an application specific integrated circuit ASIC a field programmable gate array FPGA a processor shared dedicated or group that executes one or more software or firmware programs and or other suitable components that provide the described functionality. The DAP module may include a Voice Sense application and Automatic Speech Recognition Assist ASR A application.

Voice Sense VS can include detection of voice commands while a mobile device is in a low power mode. In some embodiments access to device services is enabled for example by simply uttering a predefined command to wake up the system. In various embodiments the detection of the voice command is achieved while keeping the power consumption low e.g. below 2 mW . These systems and methods may be referred to as Voice Wake Up or Voice Sense.

Automatic Speech Recognition Assist ASR A may include a specialized noise suppression method or system minimizing the background noise when maximizing the correct automatic recognition of the device target user speech.

The system may further include a power management integrated circuit PMIC an application processor AP and a communication processor . The codec DAP module application processor and communication processor can be interconnected with SLIMbus . The DAP module can be operable to transmit data to application processor AP and or receive data from application processor via for example a General purpose input output GPIO port and or universal asynchronous receiver transmitter UART port . The codec may be operable to receive acoustic signal s from one or more analog microphones and provide an audio signal to speakers .

In some embodiments the DAP module and codec may be combined integrated as a single module unit. The voice sensing functionality may be performed within a digital subsystem of a codec in some embodiments.

Usage of voice to access the device services may further include any active power states of the mobile device allowing access to the device services through usage of the voice command. These systems and methods may be referred to as Continuous Voice Sense. The Continuous Voice Sense mode may provide seamless transition from a low power mode to an active power mode. For example a user may address a mobile device by enunciating the wake up command and without any unnatural pause continue speaking any other possible command s . By way of example and not limitation OK earSmart call Paul s cell phone. In some embodiments software and hardware architectures may be optimized to achieve a low power low latency and highly reliable solution.

The description of the following embodiments is provided by way of example and not limitation variously for system and and their different microphone configuration where PDM microphone refers to a digital microphone as noted above 

Step 1 When operating the mobile device in a low power mode the Voice Sense application on DAP module may run while the AP communication processor and codec may be put into a sleep mode. The PDM digital microphone may be operable to provide a stream representing an acoustic sound to the DAP module at a pre determined sampling rate. In some embodiments the pre determined sampling rate may be for example 8 or 16 kHz. The DAP module is operable to listen buffer and detect a voice. PMIC may be operable to provide a pulse to the DAP module at a 9.6 MHz rate. In other embodiments the DAP module includes an internal oscillator for generating for 1 mA the system clock.

Step 2 In some embodiments the DAP module may be operable to detect and buffer a voice signal and determine the voice signal includes a key phrase. Upon the determination the DAP module may set the GPIO line and send an interrupt to the AP . Upon receiving the interrupt the AP may start a bring up wake up sequence. A route definition may be preserved in a memory of the DAP module .

Step 3 At the same time upon detecting the key phrase the DAP module may be operable to be switch to a one microphone automatic speech recognition assistant ASR A omnidirectional mode.

Step 4 While booting the AP operating the DAP module may continue buffering clean speech with for example 16 bits sample at 16 kHz. The boot time of AP may be for example around 30 ms. The clean speech can be generated from an acoustic sound detected with the digital microphone using for example noise suppression or noise reduction techniques.

Step 5 Upon waking up the application processor may communicate and or indicate by an application programming interface API that it is ready to exchange data for example over at least one of SlimBus Universal Asynchronous Receiver Transmitter UART and System Packet Interface SPI .

Step 6 The buffer or clean speech output accumulated up to that point may be sent to the application processor for example using a fast interface such as UART or SPI. The UART or SPI interface may advantageously minimize response of the system during normal operation.

Step 7 In some embodiments the DAP module and AP may continue to exchange data using for example the fast interface for example UART or SPI from this point on. Alternatively in other embodiments the SLIMbus may be used. The AP may realign the samples.

If a two microphone ASR assist is desired then the two digital microphones e.g. PDM microphones disposed on top of the mobile device may be coupled directly to the DAP module e.g. two digital microphones coupled to the DAP module in the system in . The two microphone ASR assist may start processing without any involvement of the AP .

In some embodiments alternatively or in addition once a first ASR command is recognized using one microphone ASR assist the Application may reconfigure the DAP module to a two microphone ASR Assist Far Talk Omnidirectional mode and configure the Codec and for example the SLIMBus connection accordingly.

Step 1 Voice Sense application on the DAP module may commence. The AP may be put into a sleep mode. In case of PDM the Codec may also be in a sleep mode and the PDM digital microphone may be coupled to the DAP module . The sampling rate of an audio signal stream provided from digital microphone to the DAP module may be 8 or 16 kHz.

Step 2 The DAP module may detect the key phrase and send an interrupt to the AP . In response the AP may start the bring up sequence. The AP may first start the Codec and connect the DAP module and Codec .

Step 3 The DAP may switch to the two microphone ASR assist omnidirectional mode. The boot time for example may be around 30 ms. The route definition may be preserved in a memory of the DAP module . The DAP module may expect to receive samples for example at least at 16 kHz rate. In some embodiments a rate of 48 kHz may be used with the samples being downsampled to 16 kHz.

Step 4 The clean speech output of the DAP module may be buffered for example with 16 bits sample at 16 kHz. The clean speech can be generated from an acoustic sound detected with digital microphones by ASR A using for example noise suppression or noise reduction techniques.

Step 5 The host may communicate indicate that it is ready to exchange data by API over at least one of SlimBus UART and SPI.

Step 6 The buffer or clean speech output accumulated up to that point may be sent to the host using a fast interface such as UART or SPI which may advantageously minimize response of the system during a normal operation.

Step 7 From this point on the DAP module and AP may continue to exchange data using the fast interface for example UART or SPI . Alternatively the SLIMbus interface may be used but in this case the AP may at least realign the samples.

a. An Original Equipment Manufacturer OEM may have full control of the ASR application and the audio input to the application may be sourced from the operation system OS files system instead of the standard audio interfaces.

b. The Application may actually establish the connection from the Codec to the eS7xx module faster than completing the full wake up.

In step after the booting of the second module is complete the method may proceed with sending to the first module an indication that the second module is ready to exchange data with the first module. In various embodiments the data exchange can be carried by one of SlimBus Universal Asynchronous Receiver Transmitter UART and System Packet Interface SPI . In step the method can include sending by the first module the clean speech output to the second module. In step the method may further proceed with exchanging further data between the first module and the second module.

In some embodiments two keywords or groups of key words where each group comprises a key phrase e.g. OK Google Hi Galaxy or other key phrases are separately processed such that the two keywords or key phrases both trigger launching of two different voice search engines or voice recognition engines or simply two applications e.g. application such as S Voice Google Voice Search or other suitable applications . The keywords or key phrases and the associated clean speech output may be buffered and concatenated for providing to the two different voice search recognition engines or the two applications. In other embodiments each of the two keywords or key phrases may be processed by method and systems to each trigger a different application.

Various embodiments can be practiced on mobile devices. Mobile devices may be portable or stationary. Mobile devices can include radio frequency RF receivers transmitters and transceivers wired and or wireless telecommunications and or networking devices amplifiers audio and or video players encoders decoders speakers inputs outputs storage devices and user input devices. Mobile devices may include inputs such as buttons switches keys keyboards trackballs sliders touch screens one or more microphones gyroscopes accelerometers global positioning system GPS receivers and the like. Mobile devices may include outputs such as LED indicators video displays touchscreens speakers and the like. In some embodiments mobile devices may include hand held devices such as wired and or wireless remote controls notebook computers tablet computers phablets smart phones personal digital assistants media players mobile telephones and the like.

The mobile devices may be used in stationary and mobile environments. Stationary environments may include residencies and commercial buildings or structures. The stationary environments can also include living rooms bedrooms home theaters conference rooms auditoriums and the like. In the mobile environments a system may be moving with a vehicle carried by a user or be otherwise transportable.

The components shown in are depicted as being coupled via a single bus . The components may be coupled through one or more data transport means. Processor unit and main memory are coupled via a local microprocessor bus and the mass data storage peripheral device s portable storage device and graphics display system are coupled via one or more input output I O buses.

Mass data storage which can be implemented with a magnetic disk drive solid state drive or an optical disk drive is a non volatile storage device for storing data and instructions for use by processor unit . Mass data storage stores the system software for implementing embodiments of the present disclosure for purposes of loading that software into main memory .

Portable storage device operates in conjunction with a portable non volatile storage medium such as a flash drive floppy disk compact disk digital video disc or Universal Serial Bus USB storage device to input and output data and code to and from the computer system of . The system software for implementing embodiments of the present disclosure is stored on such a portable medium and input to the computer system via the portable storage device .

User input devices can provide a portion of a user interface. User input devices may include one or more microphones an alphanumeric keypad such as a keyboard for inputting alphanumeric and other information or a pointing device such as a mouse a trackball stylus or cursor direction keys. User input devices can also include a touchscreen. Additionally the computer system as shown in includes output devices . Suitable output devices include speakers printers network interfaces and monitors.

Graphics display system include a liquid crystal display LCD or other suitable display device. Graphics display system is configurable to receive textual and graphical information and processes the information for output to the display device.

Peripheral devices may include any type of computer support device to add additional functionality to the computer system.

The components provided in the computer system of are those typically found in computer systems that may be suitable for use with embodiments of the present disclosure and are intended to represent a broad category of such computer components that are well known in the art. Thus the computer system of can be a personal computer PC hand held computer system telephone mobile computer system workstation tablet phablet mobile phone server minicomputer mainframe computer wearable or any other computer system. The computer may also include different bus configurations networked platforms multi processor platforms and the like. Various operating systems may be used including UNIX LINUX WINDOWS MAC OS PALM OS QNX ANDROID IOS CHROME TIZEN and other suitable operating systems.

The processing for various embodiments may be implemented in software that is cloud based. In some embodiments the computer system is implemented as a cloud based computing environment such as a virtual machine operating within a computing cloud. In other embodiments the computer system may itself include a cloud based computing environment where the functionalities of the computer system are executed in a distributed fashion. Thus the computer system when configured as a computing cloud may include pluralities of computing devices in various forms as will be described in greater detail below.

In general a cloud based computing environment is a resource that typically combines the computational power of a large grouping of processors such as within web servers and or that combines the storage capacity of a large grouping of computer memories or storage devices. Systems that provide cloud based resources may be utilized exclusively by their owners or such systems may be accessible to outside users who deploy applications within the computing infrastructure to obtain the benefit of large computational or storage resources.

The cloud may be formed for example by a network of web servers that comprise a plurality of computing devices such as the computer system with each server or at least a plurality thereof providing processor and or storage resources. These servers may manage workloads provided by multiple users e.g. cloud resource customers or other users . Typically each user places workload demands upon the cloud that vary in real time sometimes dramatically. The nature and extent of these variations typically depends on the type of business associated with the user.

The present technology is described above with reference to example embodiments. Therefore other variations upon the example embodiments are intended to be covered by the present disclosure.

