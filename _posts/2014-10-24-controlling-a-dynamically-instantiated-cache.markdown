---

title: Controlling a dynamically instantiated cache
abstract: A change in workload characteristics detected at one tier of a multi-tiered cache is communicated to another tier of the multi-tiered cache. Multiple caching elements exist at different tiers, and at least one tier includes a cache element that is dynamically resizable. The communicated change in workload characteristics causes the receiving tier to adjust at least one aspect of cache performance in the multi-tiered cache. In one aspect, at least one dynamically resizable element in the multi-tiered cache is resized responsive to the change in workload characteristics.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09317430&OS=09317430&RS=09317430
owner: NETAPP, INC.
number: 09317430
owner_city: Sunnyvale
owner_country: US
publication_date: 20141024
---
This application is a Continuation of and claims the priority benefit of U.S. application Ser. No. 13 250 911 filed Sep. 30 2011.

Embodiments described are related generally to management of networked storage and embodiments described are more particularly related to managing a multi tiered caching system in a virtualized environment.

Portions of the disclosure of this patent document can contain material that is subject to copyright protection. The copyright owner has no objection to the reproduction by anyone of the patent document or the patent disclosure as it appears in the Patent and Trademark Office patent file or records but otherwise reserves all copyright rights whatsoever. The copyright notice applies to all data as described below and in the accompanying drawings hereto as well as to any software described below Copyright 2011 NetApp Inc. All Rights Reserved.

Data for companies or other organizations is commonly stored in networked storage. The networked storage center can be referred as a data center. The resources of a data center such as storage and access bandwidth are limited. Thus a common goal for a data center is to improve utilization of the resources of the storage center to improve storage utilization and access throughput. There may be applications within the company or organization that generate large workloads making many access requests to the data center. Data access to storage is typically slow relative to computer processing speeds. One way to improve resource use in a data center is to provide caching recognizing that data accessed in a workload has a greater likelihood of being accessed again relative to other data stored in the data center.

A caching system may be multi tiered where there are multiple layers of caching which are coordinated. Traditionally lower layers of cache or the layers closer to the processing resources have smaller size but are faster relative to the higher layers of cache. Thus traditional cache levels decrease in size and increase in speed the closer the cache level gets to the processing resources. From the other perspective the closer the cache level is to the client making the access request the larger and slower it traditionally is.

Cache coherency protocols or coordination communication among the levels of cache have an underlying assumption based on the speed and size of the cache levels. Traditional multi tier cache protocols or traditional inclusion exclusion approaches to cache coherency have underlying assumptions regarding cache size and speed. Deviations in cache structure would render such traditional protocols unsuitable for use with the different cache structure.

Returning to the concept of data center resource utilization the structuring of cache systems fixes the use of resources in the system. To the extent the data access of a data center is highly dynamic the traditional fixed structure of caches can hinder performance as often as it improves performance. The traditional fixed structure results in redundancy of data and redundancy of caching effort among cache layers. The redundancy of data consumes resources that could otherwise be preserved for other use.

In a traditional cache the different caching levels or tiers of the cache are oblivious of the caching decisions and workload changes experienced at each of these different tiers. The different tiers of the cache are traditionally not able to obtain first hand information about the changes at other tiers. The lack of first hand information leads to a communication gap among the cache levels. The communication gap in turn causes each caching tier to take extra effort to speculate about the workload changes and later adapt to them. Such duplicated effort increases the adaptation time of the overall system which decreases or nullifies the caching effort.

A change in workload characteristics detected at one tier of a multi tiered cache is communicated to another tier of the multi tiered cache. Multiple caching elements exist at different tiers and at least one tier includes a cache element that is dynamically resizable and can be dynamically instantiated or destroyed. The communicated change in workload characteristics causes the receiving tier to adjust at least one aspect of cache performance in the multi tiered cache. At least one dynamically resizable element in the multi tiered cache can be resized responsive to the change in workload characteristics.

Descriptions of certain details and embodiments follow including a description of the figures which can depict some or all of the embodiments described below as well as discussing other potential embodiments or implementations of the inventive concepts presented herein.

As described herein a multi tiered cache system detects changes in workload at each tier of the cache and each tier automatically responds to workload changes with dynamic changes in caching behavior. When one tier of the multi tiered cache system detects a change in workload characteristics it communicates the change to another tier of the multi tiered cache system. The communicated change in workload characteristics causes the cache tier receiving the communication to adjust at least one aspect of cache performance or behavior in the multi tiered cache. In one aspect the receiving tier resizes at least one dynamically resizable element in response to the change in workload characteristics. In certain embodiments the multi tiered cache system changes read ahead behavior or bypasses caching altogether in response to changes in workload characteristics. In other embodiments the multi tiered cache system can dynamically instantiate a cache such as a virtual storage appliance or VSA close to the client where the VSA can absorb most of the read traffic for the client. The cache system can also decommission the VSA if the characteristics of the workload are no longer VSA cache friendly.

System illustrates a multi tiered cache configuration with a dynamic cache VSA. Namely storage server includes cache which is a lowest tier or level of cache in the cache hierarchy. As used herein tier refers to the separation of the control logic within the multi tiered cache system. Each tier includes a caching device which includes storage or memory and a feedback sizing controller or logic 116 to determine how caching is to be performed at the specific tier. The controller determines what data is to be stored in the cache. It will be understood that alternative terms such as level or layer could also be used to refer to the separate tiers.

The storage server connects to dynamic cache VSA via network . Network can be any type or combination of wide area networks. Dynamic cache VSA is instantiated on hypervisor that is physically close or on host . Host hosts hypervisor . Physically close refers generally to the idea that a local area network or a local connection is used to connect the devices rather than connecting over a wide area network. As used herein instantiation refers to creating an instance or a copy of a source object or source code. The source code can be a class model or template and the instance is a copy that includes at least some overlap of a set of attributes which can have different configuration or settings than the source. Additionally modification of an instance can occur independent of modification of the source.

Dynamic cache VSA is typically populated as host reads data from the source storage server . On the first read of any data the cache fetches data from storage server stores it in dynamic cache VSA and forwards it to host . As the reads pass through dynamic cache VSA the cache fills up. Any subsequent access of the data that is stored in dynamic cache VSA can be immediately served from the dynamic cache which reduces the roundtrip time or the latency. In one embodiment dynamic cache VSA acts like a write through cache where all writes from host are passed directly to storage server . Only when storage server responds to a write request dynamic cache VSA acknowledges the result to host or other cache tiers e.g. RAM buffer cache and SSD or flash.

Similarly to dynamic cache VSA cache device within storage server caches data to serve to dynamic cache VSA avoiding access to storage resources for data that is cached within storage server . In one embodiment cache device is actually separated into two separate cache devices.

Storage server further includes controller which represents the control logic of storage server related to determining when a workload characteristic change occurs or when the working set size has changed whether there is overlap between working sets of two workloads and when to propagate these changes to the sizing controller of dynamic cache VSA . Working set refers to a set of data being cached due to an application accessing the data. Controller could be considered a cache device at a tier in a multi tiered cache. Additionally controller can determine what operations to perform in response to an indication from dynamic cache VSA . Controller can be implemented as part of other control logic of the storage server or it can be implemented as separate control logic whether virtually e.g. code or physically e.g. hardware separate .

The communication could be considered to occur between storage server and dynamic cache VSA or it could be considered to occur between controller of storage server and controller of dynamic cache VSA . Controller is similar to controller in that it performs the same functions at dynamic cache VSA that controller performs at storage server . Thus similarly controller could be considered a device of a multi tiered caching system. In one embodiment controller is implemented outside the virtual machine VM that contains dynamic cache VSA . For example controller could be a separate virtual entity of hypervisor . The controllers provide the intelligence of when to communicate to other cache tiers what to communicate and control how the receiving tier responds to an indication from another tier in terms of sizing its own cache in the tier. It will be understood that a virtual machine refers to a software environment instance or virtual environment that executes on hardware resources shared with other virtual environments. The allocation of hardware resources to virtual environments is typically performed by a virtual machine manager or hypervisor which maps resource requests from the virtual environments to physical hardware resources.

In one embodiment each cache tier includes such as in its controller a monitoring infrastructure that collects statistics at the cache tier. For example a monitor such as a monitoring daemon can collect statistics related to workload characteristics e.g. read write ratio random sequential ratio I O size cache statistics e.g. hit ratio utilization and or performance or SLO service level objectives statistics e.g. latency throughput . The monitored statistics are fed into the controller i.e. or to allow the controller to determine whether the workload or working set is changing. In one embodiment the controller can be considered to include the processing resources inter tier communication resources and monitoring resources by which determinations are made and communications passed between cache tiers.

Each controller e.g. or receives the statistics and communication from other cache tiers determines the size of the cache at its tier or level and the impact of its caching on other cache tiers. In one embodiment all determinations of impact are performed at a workload granularity. Thus the controller can use workload statistics to determine the changes experienced by its caching tier. The controller can determine from detected changes that another cache tier should be notified.

In one embodiment the controllers try to determine one or more of the following. The controller can determine whether a particular workload has changed such as if a read write ratio has increased. If the controller detects a workload change it will try to increase the size of the cache at the local tier and decrease the partition size on other cache tiers for the same workload. The controller can determine whether the working set size has changed or the working set has changed. The controller can determine whether there is enough free cache space available or determine other resource availability. The controller can determine whether the overlap between working sets of two workloads has increased or decreased. The controller can determine whether workloads have been added or removed.

System also illustrates different ways that storage server can experience multiple workloads. Workload and workload come to storage server through a channel other than dynamic cache VSA . More particularly clients and access storage server over a different network than network or via a different host than host . Clients and can be considered to access storage server directly and not via a dynamic cache VSA whereas the access of client is through dynamic cache VSA . Workload comes to storage server via dynamic cache VSA from client .

The workloads are separate or distinct from each other because they have different sources or they originate from different applications or different clients. Thus each workload associated with requests from a different application can be referred to as a distinct workload. The different workloads and could access either the same or different storage object such as a volume on the storage server. Depending on whether the different workloads are accessing the same or different volumes the storage server experiences a certain resultant workload characteristic at its end which is used by the storage server to make certain caching decisions as explained below.

There are many different possible protocols that could be used by the devices of system to communicate. In one embodiment the client can issue packets including file based access protocols such as the Common Internet File System CIFS protocol or Network File System NFS protocol over the Transmission Control Protocol Internet Protocol TCP IP when accessing information in the form of files and directories. Alternatively the client can issue packets including block based access protocols such as the Small Computer Systems Interface SCSI protocol encapsulated over TCP iSCSI and SCSI encapsulated over Fibre Channel FCP when accessing information in the form of blocks. Protocol represents the protocol used by a client to make data access requests such as NFS CIFS or others. Dynamic cache VSA communicates via protocol to the clients such as client of host .

In one embodiment dynamic cache VSA communicates with the origin storage server using a different protocol than protocol . The protocol used for communication with storage server is shown as protocol . There can be embodiments where protocol is the same as protocol . However they can be different. In one embodiment protocol is the NRV NetApp Remote Volume protocol. Protocol can be augmented to include cache communication requests for example augmented NFS or CIFS could be used similar to what is described herein . Examples of augmentations to the protocol are described in more detail below.

In one embodiment workloads and share cache e.g. a buffer cache on storage server along with workload which is purely a dynamic cache workload. Such sharing necessitates use of a partitioned cache within storage server in which case the workloads that are offloaded to the VSA have a small partition space of the buffer cache as compared to other workloads accessing data without any VSA. In one embodiment workload uses cache of storage server to cache data until the transfer to an N blade as described in is complete or to cache pre fetched data based on read ahead policies implemented at storage server .

Thus environment deviates from the traditional multi tier cache system size and speed conventions. In one embodiment the lowest tier of the cache system is storage server SSD solid state drive cache tier SSD tier referring to an SSD or flash device on storage server . The next tier is storage server buffer cache tier buffer cache then dynamic cache VSA . In one embodiment cache tier and cache tier are both included in cache of system as L3 and L2 respectively where tier is L1 . While both cache tier and cache tier are typically shared across clients and workloads in one embodiment each dynamic cache VSA instance is dedicated to a specific client. The size of dynamic cache VSA which is closest to client is much larger than buffer cache while SSD cache is also larger than buffer cache .

In terms of speed dynamic cache VSA is faster than buffer cache as the storage server sits across a WAN and buffer cache is faster than SSD cache . Dynamic cache VSA is much closer to client .

Origin includes storage hardware including storage volume which can be one or more logical groupings of data storage resources. In one embodiment origin is a blade enclosure with storage resources as well as storage server controller resources. Storage server manages the storage resources of origin . Requests related to data stored at origin are processed through storage server .

Switch represents one or more network resources to allow remote access to storage server . As illustrated switch connects to virtual environments and which are virtualized environments executing on hardware .

Virtual environment includes dynamic cache VSA executing on hypervisor which acts as a cache tier for storage server . Hypervisor as well as hypervisors and of virtual environments and respectively include a virtual switch Vswitch and a virtual environment filesystem VFS . Other virtual and or real environments could also be executed on hardware . The virtual switches provide access via the hypervisor to hardware switching resources used to connect to the physical resources of origin and the other virtual environments connected to dynamic cache VSA . In one embodiment hypervisor and are all the same hypervisor with dynamic cache VSA and virtual environments and being different VMs executing on it. As is understood by those skilled in the art the applications and operating systems of each virtual environment access the virtual switch of the respective virtual environment as though the switch were actual hardware. The virtualization controller hypervisor manages the mapping of virtual resources to hardware resources for the virtual switches as well as other virtualized physical resources.

In one embodiment virtual environment hosts the dynamic cache VSA and virtual environments and host access to clients. As illustrated environments and are configured identically with multiple operating system OS instances and application instances connecting to the corresponding hypervisor and . The configurations do not necessarily have to be identical. In one embodiment each operating system of virtual environments and represents a separate virtual machine VM and there can be one or more applications executing on each operating system. The applications could each represent one or more clients. The virtual switch of each virtual environment and presents an instance representation and respectively of storage volumes of origin .

In one embodiment the applications are multiple individual threads. In one embodiment each thread is considered a workload or one thread is considered an application. The applications are dynamic and can be opened and closed dynamically as well as dynamically changing what data and how much data they access.

In one embodiment dynamic cache VSA is implemented as an instance of an operating system the same or similar to the one executed on storage server . Thus storage server executes a storage server OS natively while the storage server OS executes virtually on hypervisor hosted remotely from origin . Storage server is local to storage volumes while dynamic cache VSA accesses storage volumes remotely via switch . Storage resources represent the physical storage resources for virtual environments and . In one embodiment storage resources could be considered part of hardware .

Dynamic cache VSA includes protocols and associated drivers and network stacks to communicate over the virtual switch of hypervisor . In one embodiment dynamic cache VSA includes at least NRV and NFS as supported protocols. In one embodiment origin can be a Fabric Attached Storage FAS and export storage volumes to dynamic cache VSA over the NRV protocol. Dynamic cache VSA can then serve the cached volumes to clients of virtual environments and over the NFS protocol.

Dynamic cache VSA also includes a filesystem as well as drivers and management resources for storage . A combination of storage and RAM of the hypervisor host part of hardware act as the caching device for dynamic cache VSA . Because the VSA cache tier is dynamic space from both DAS and RAM of the hypervisor can be carved out to implement the VSA tier as a dynamic resource. In one embodiment dynamic cache VSA controls all storage access for all VMs of virtual environments and . Data accessed from storage volumes is cached in storage resources and presented as instances and to virtual environments and respectively by the virtual switches of the respective environments. Each VM can store local data in addition to the data of storage volumes .

As mentioned above dynamic cache VSA can respond to dynamic behavior of different workloads which are represented either directed or indirectly by the applications of the VMs of virtual environments and . The dynamic behavior of the dynamic cache VSA with respect to the various workloads can include operations in accordance with the following examples.

Consider a scenario where the read write ratio of dynamic cache VSA is high. When the read write ratio is high the utility of the dynamic cache VSA is highest. Many reads generally means that cached data will be used and improve the overall data access throughput. As the ratio of reads to writes decreases the effectiveness of the dynamic cache VSA also decreases.

As described herein dynamic cache VSA can detect the read write ratio and indicate the changes in read write ratio to origin . Thus dynamic cache VSA can indicate one or more characteristics to the source. In addition to read write ratio characteristics such as workload parameters can be monitored and indicated to the source.

Additionally if a lower tier of the cache system e.g. a buffer cache or SSD on storage server detects a change in the working set based on a caching policy stored locally by each caching controller it can communicate the detected change to the upper level s or tier s of the cache e.g. to dynamic cache VSA . For example the buffer cache can indicate that the dynamic cache should not cache so much.

Assume for another example that storage server serves two different workloads that share a data set with Workload going through dynamic cache VSA and Workload 2 going directly to a buffer cache not shown of storage server . In such a scenario dynamic cache VSA is effective in a read and read scenario where both workloads are dominated by read access. However in a read and write scenario the effectiveness of dynamic cache VSA can go down even if Workload 1 has a high read write ratio. Observe that if Workload 1 and Workload 2 share the same data set and Workload 1 has a high read write ratio while Workload 2 has a much lower read write ratio then it will be changing the data accessed by Workload 1. Hence all the data blocks that were cached by Workload 1 and changed by Workload 2 will be invalidated in the cache.

Thus a low read write ratio on a workload not even known to dynamic cache VSA could have a negative impact on the effectiveness of the dynamic caching. However the buffer cache of storage server would be aware of Workload 2 and its access characteristics. Thus in one embodiment a lower cache tier i.e. buffer cache indicates to a higher cache tier VSA to decrease its cache size. Thus in one embodiment the buffer cache can direct dynamic cache VSA to cache less given it is indirectly less useful due to activity by other workloads.

Additionally in one embodiment the dynamic cache can detect changes in the working set of an application. If the dynamic cache determines that changes are occurring in the working set it can indicate more read ahead from a buffer cache or other lower cache tier. In one embodiment a predetermined rate of cache misses by dynamic cache VSA can indicate a working set change.

In one embodiment the system stores one or more preconfigured thresholds for cache performance which it loads and sets in the instantiated VSA process block . In an alternate embodiment the system configures thresholds based on performance of the VSA. In one embodiment the VSA can log and monitor performance to determine where breaks or changes in behavior occur. The VSA can execute one or more algorithms to perform calculations to determine where a threshold should be. The VSA can then set the thresholds to match expected or observed behavior of the system.

The VSA monitors performance characteristics of its performance Other tiers of the cache system likewise monitor their performance characteristics. In one embodiment different tiers of the cache monitor different parameters while in an alternative embodiment all cache tiers monitor the same parameters. As a whole the cache monitors performance characteristics at each tier process block . By the monitoring the cache determines at each tier if performance characteristics indicate a change in system behavior that should be communicated to another cache tier process block .

If a tier determines that a change should not be communicated decision block each tier continues to monitor performance process block and determine if there is anything to indicate to another tier process block . If a tier of the cache determines that a change should be communicated decision block the tier indicates the change to another or a different tier of the cache in accordance with a communication protocol for communicating to another tier process block . The receiving tier changes or alters its performance in response to the indicated change by the other tier process block .

In one embodiment as described herein a multi tiered cache system is dynamic in its structure implementing at least one tier of the cache system in a dynamic cache VSA. The multi tiered cache system enables communication between the levels or tiers of the cache system which reduces the likelihood of redundancy of effort among tiers of the cache system. The system allocates resources for caching as needed. When caching would result in a performance improvement the caching system allocates the resources for caching. When caching becomes less effective the system can de provision the resources. Thus rather than having each cache system tier make its own independent determination about how to perform caching based on its local circumstances lower and higher tiers of the multi tiered cache system communicate with each other to indicate what patterns and caching decisions are being made.

The improved communication and control of the virtualized dynamic caching can reduce the redundancy of effort. Communication among tiers about workload changes enables a balance between resource availability and workload need. In one embodiment the communication occurs between controllers or control logic of a dynamic cache VSA and a buffer cache on a storage server.

The caching structure described herein includes at least one tier of caching where control for the tier is implemented as a virtual instance. Virtualization of storage server functionality via a flexible cache VSA or dynamic cache VSA can improve resource utilization in a caching system. Not only can a cache system dynamically create instantiate or destroy flexible cache VSA instances having storage server functionality as needed the caching system can dynamically revise or change instances when needed. Thus the multi tiered cache caching system described herein dynamically obtains and releases resources in accordance with availability and need.

In one embodiment a storage server includes multiple caching tiers for example a flash or SSD tier and a buffer cache tier. A dynamic cache VSA provides another caching tier that sits outside the storage server and logically above the buffer cache tier. The multi tier cache communication described herein can be referred to as a multi tier cache communication protocol. In one embodiment the protocol is an end to end protocol from the dynamic cache VSA to the storage server s buffer cache and flash or SSD cache.

Communication via the protocol enables the dynamic cache VSA to communicate changes in workload characteristics to the storage server cache tiers. In one embodiment indication of the changes to the storage server triggers the storage server for example to alter read ahead policy and or alter the size of the buffer cache on the storage server. Thus if the sequential random ratio of data access requests increases the buffer cache tier can calculate a proportional increase in the read ahead size and issue I Os. If the buffer cache tier is under a resource crunch read ahead blocks can be cached in a flash SSD tier. The buffer cache tier can make a similar decision when the data access arrival rate of the workload increases.

In one embodiment the dynamic cache VSA communicates a size change of the dynamic cache VSA to the storage server to affect the caching policy of the flash SSD tier. For example if the dynamic cache VSA is completely destroyed de provisioned or de commissioned the storage server s various caching tiers such as buffer cache tier and flash SSD tier should be prepared to take the increased load of the de commissioned dynamic cache VSA. Before de commissioning the dynamic cache VSA the dynamic cache VSA should propagate all the workload characteristics to the storage server. The workload characteristics information can aid a feedback controller on the storage server to estimate the size of each of the caching tiers.

In one embodiment such a feedback controller is configured to take into consideration multiple workloads sharing the same set of caches buffer cache SSD . When multiple workloads share the same set of caches operation by one workload can affect cache performance with respect to the other workload. For example if one workload is write heavy and the other read heavy having a large cache at one tier may not make sense given a high likelihood of invalidating data stored in the cache. Thus even though a read heavy workload can normally be a good candidate for a large amount of cache at one tier in the case where caching resources are shared by different workloads it can make more sense to reduce the cache size in at least one tier under certain circumstances.

In one embodiment whenever the dynamic cache VSA detects change in the working set it communicates the change to the storage server. In one embodiment a change is only detected in the sense of communicating the change when the change is higher than a threshold. In one embodiment the threshold is based on a number of cache misses within a period of time. In one embodiment a system administrator sets a cache system with preconfigured thresholds. Alternatively in one embodiment the cache system dynamically determines thresholds by control logic in the cache system based on historical monitoring. Communicating the changes in working set to the storage server enables the buffer cache tier to appropriately adjust read ahead of data blocks. For example read ahead can be more aggressively applied even if a sequential random ratio is the same as before based on an indication of a change in working set.

In one embodiment the communication occurs only within the storage server and not from the dynamic cache VSA to the storage server. One scenario where the communication happens only within the storage server is as follows. If a read request to the storage server comes as a result of a cache miss at the dynamic cache VSA then there is no point for the storage server s buffer cache to retain that buffer after the data is transferred to the dynamic cache VSA. This is because all the future read accesses to that data block will be fulfilled by the dynamic cache VSA and buffer cache s copy will be redundant.

In one embodiment a tagging function is used with communication across the cache tiers within the storage server. In such an embodiment a dynamic cache VSA has a dynamic cache ID. With the tagging functionality all data access requests made to the storage server e.g. D blade on behalf of the protocol requests from the dynamic cache VSA can be tagged with the dynamic cache ID. For example if the dynamic cache VSA tags a read request with a dynamic cache ID the buffer cache tier can evict the buffer as soon as it is transferred. Evicting from the buffer cache tier could include either evicting it completely or requesting an SSD tier to save it. Similarly for a write request the buffer cache tier can evict the buffer soon after it is written to nonvolatile RAM Random Access Memory . Such a technique can be referred to as self eviction.

It will be understood that not all workloads accessing a particular storage are accessed through a dynamic cache VSA and a storage server can support many dynamic cache VSAs. One benefit of not storing redundant data on the buffer cache tier is that the cache system can allocate more space to other workloads that are coming directly to the storage server or indirectly through other dynamic cache VSAs. Thus memory resource utilization is improved.

It will be understood that the dynamic cache VSA as described herein is not a victim cache for the storage server s buffer cache. Thus self eviction as mentioned above is different from traditional multi tiered protocols. With self eviction as described above the buffer cache tier evicts data on its own will when the data is redundant.

In one embodiment the storage server implements NAS Network Attached Storage over a WAN. Traditional cache protocols that may work in a SAN Storage Area Network scenario do not necessarily work with NAS due to the differences in access of the data between SAN and NAS. The augmented cache protocol described herein allows cache communication between a dynamic cache VSA and storage server and can be used to augment either NAS or SAN implementations. The communication described herein is also distinct from dynamic partitioning across multiple cache tiers at least because there does not need to be any explicit partitioning at the different cache tiers. Dynamic partitioning explicitly partitions and then evaluates its impact using statistical regression. As described herein each tier can simply determine what workload characteristics are observed at the tier and then communicate those observations to other tiers.

As described above the controller monitors for changes in cache environment and in response to changes triggers cache re sizing or re partitioning at various cache tiers. The following scenarios illustrate embodiments of situations where the controller will trigger communication across multiple tiers of the cache system as well as triggering changes at the tier of the controller. It will be understood that not all scenarios apply the same to each cache tier or that different scenarios may be a little different for different tiers.

Assume a configuration with three cache tiers L1 L3 where L1 is implemented as a dynamic cache VSA and L2 and L3 reside on a storage server with L2 as buffer cache and L3 as SSD or flash on the storage server. The following cases show examples of communication between tiers of a multi tier cache. Each tier controller includes a local caching policy indicating how it performs caching such as how much to cache and what objects to cache. Each tier controller also follows protocol rules regarding what information to send when and to what other tiers or the controller of the other tiers . Thus the following cases provide examples of implementation of a multi tier caching communication protocol.

In one embodiment for Cases 1 4 L1 will trigger communication to L2 e.g. a buffer cache on the storage server . In one embodiment for Cases 5 7 L2 will communicate with L3 e.g. an SSD on the storage server . In one embodiment for Cases 8 10 L2 will communicate to L1 or L3 will communicate with L2.

Case 1 occurs when workload characteristic changes. If the read write ratio for a workload coming from a client goes below a threshold the dynamic cache VSA will no longer be useful given that the workload becomes write intensive. Similarly if the I O arrival rate declines a dynamic cache VSA might not be needed. With either of these workload characteristic changes the cache system may determine to decommission the dynamic cache VSA. Thus the L1 controller communicates to L2 to indicate the change. Once the L2 and or L3 controllers change their caching policies to absorb the workload change L2 can inform L1 of the changes and decommission the L1 cache. If arrival rates have declined then the controller of L1 should inform L2 that it does not need to increase its own cache size while L1 is decommissioned.

Another example of Case 1 is if the sequential random ratio has increased and the entire dataset belonging to the sequential access cannot be accommodated in L1. In such a circumstance L1 can inform L2 to increase its partition size to pre fetch and store the blocks from the main storage. If L2 cannot accommodate the entire dataset it can similarly send a communication requesting L3 to increase its partition size for the workload to cause L3 to pre fetch the data.

Case 2 occurs when a working set size increases or decreases. If the controller at L1 determines that a working set size has increased it can increase the cache size at its level assuming such resources are available at the hypervisor. If the resources are not available at L1 then Case 4 applies as described below. The controller can determine that the working set size has changed by observing an improvement in cache hit ratio with increase in cache size and the workload characteristics.

Case 3 occurs when a working set has changed. The controller of L1 can determine that the working set has changed if the miss rate at L1 starts increasing and or workload characteristics have changed. In such a scenario the L1 controller retrieves the working set size and workload characteristic and communicates it to L2. The communication to L2 prompts the L2 controller to perform read ahead and estimate the optimal partition size for the particular workload at its own level. Depending upon resource availability at L2 the controller will inform L3 whether it needs to re partition itself.

Case 4 occurs when resource availability does not support an increase in the cache size at L1. If the controller of L1 can no longer increase the L1 cache size due to shortage of HDD or SSD space on the hypervisor the controller communicates this information to L2. The L1 controller can determine the resource scarcity at its tier by looking at the estimated working set size. If the estimate is greater than the available L1 cache size then there is a resource scarcity and L1 cannot completely absorb the working set for that workload. In this case the controller of L1 informs L2 and or L3 or L2 can then inform L3 of the workload characteristic and how much more cache size is needed. Based on this information a controller of one of the other tiers can decide to re partition and warm up their respective caches.

Case 5 occurs when the L1 tier makes a sizing request. If the controller of L1 communicates to the controller of L2 and cannot accommodate the workload s dataset the controller of L2 can then communicate the workload information and cache size request to L3. Thus the controller of one cache tier can communicate to a tier below it on behalf of the tier above it. For example in Cases 1 and 3 if a heavy load of requests is expected to come to the storage server and the load cannot be stored completely in a buffer cache at the storage server the controller of L3 sizes L3 to pre fetch the data based on the workload pattern communicated by L1 to help improve performance.

Case 6 occurs when any of the above Cases result in workloads that run directly on L2 without an L1 cache tier. The L2 controller behaves in a similar fashion as the L1 controller as described above except that it cannot be decommissioned. Additionally in one embodiment L2 might be shared across multiple workloads and not be exclusive to a workload as an L1 cache tier can be. Thus depending on the availability of RAM and the demand from other workloads the controller of L2 can request L3 to increase its partition size for a workload. In one embodiment L2 performs better for write intensive loads than L1. Thus whereas the controller of L1 may only request a partition size increase in L2 for read intensive loads the controller of L2 can request an increase in partition size from L3 for either read or write caching.

Case 7 occurs when a controller detects the addition or deletion of workloads. If a new workload is provisioned directly on the storage server the controller of the L2 tier will have to resize or reduce the buffer cache of a different workload shared on L2 to meet the requirements of the new workload. In such a case the controller of L2 can indicate the change to L3 which may prompt the controller of L3 to increase caching for one or more workloads including the newly added one . Similarly if a workload is deleted or is no longer running on L2 directly other workloads can get a larger share of L2. The controller of L2 can indicate the deletion of a workload to L3 which can trigger the controller of L3 to decrease the L3 partition size for the workloads getting a bigger share of L2 cache.

Case 8 occurs where workload characteristic and resource availability cause competition among workloads on L2. If L2 has for example 10 workloads running on it directly all sharing the L2 cache the workloads will compete significantly for the resources of L2. If one or more workloads exhibit a high read write ratio and would thus benefit from a dedicated L2 cache the L2 controller can request dynamic instantiation of an L1 cache close to the client for the one or more workloads. In one embodiment the L2 controller will need to communicate with the hypervisor to instantiate the L1 cache. The L2 controller may only be successful in requesting the L1 cache if enough resources are available at the hypervisor to create the L1 tier. If an L1 tier is created the load for the workload at L2 is decreased and the controller of L2 will distribute resources to other competing workloads. In one embodiment if the L2 and L3 tiers have sufficient resources as determined by their controllers to absorb the I Os from a workload on L1 the L2 controller can decommission the L1 tier for that workload.

Case 9 occurs where overlap between datasets increases. Consider workload 1 and workload 2 sharing the same dataset on a storage server. Suppose workload 1 is a read intensive workload and hence accesses data through an L1 tier but workload 2 is a write intensive workload and writes data directly to the L2 tier. In this case the L1 cache of workload 1 is invalidated very frequently and hence there is little or no benefit in the cache system from L1. Thus the L2 controller will be able to determine that the L1 tier provides little or no benefit and can either decrease the size of the L1 cache for workload 1 or decommission it completely.

Case 10 occurs where overlap between datasets decreases. Case 10 is the opposite of Case 9. In this case if the resource availability at L2 is low it makes sense to create new L1 tiers or increase the size of existing L1 tiers to lower the contention at L2.

In one embodiment a multi tier cache communication protocol includes various APIs Application Programming Interfaces . The description herein can apply to either a protocol that is natively configured to have the APIs or their equivalent functionality or to a protocol that is modified to have the APIs or their equivalent functionality. The APIs can be used by any cache tier. Among other APIs that can exist in one embodiment the following are included.

An API call to notify a change in size of a dynamic cache VSA can be of a form SIZE CHANGE NOTIFY IN long current cache sz IN prev cache sz IN long wss .

Whenever the controller at a particular cache tier resizes its own cache size and if the resizing amount is above a threshold static or dynamic the controller can instruct a different cache tier of the information. Here the current cache sz and prev cache sz are the current and previous cache sizes of the present cache tier that is calling the API. The previous cache size is the one that was last communicated. The API also communicates the estimated wss by the present cache tier. This API can be used in Cases 2 4 5 and 7 referring to the Cases described above.

An API call to notify changes in workload can be of a form WORKLOAD CHANGE NOTIFY IN struct workload characteristic IN long wss . Whenever the controller detects a change in the workload characteristics or the working set size wss it calls this API to communicate the information to another cache tier. This API can be used in Cases 1 2 5 6 and 8.

An API call to notify a change in the working set can be of a form WORKINGSET CHANGE NOTIFY IN struct workload characteristic IN long wss . This API is used whenever the controller senses that working set has changed. It is used in Case 3 and can be used in Case 6.

An API call to set the cache size can be of a form SET CACHE SIZE IN cache size IN struct workload characteristic IN long wss . This API is used by the controller to set the size of a higher or lower cache tier directly. For example L2 can use this API to instantiate or decommission an L1 tier as in Cases 7 8 9 and 10.

It will be understood that any combination of the events described above can occur simultaneously. In such a case the different events could either be communicated independently or batched up into one notification.

The APIs described above are exemplary and similar and or additional APIs could be used in a system. The APIs described form one part of the communication in the system the communication between the dynamic cache VSA and the storage server. Another part of the multi tier cache communication requires support from the storage server to communicate the dynamic cache VSA information to the buffer cache tier and flash SSD tier. In one embodiment a storage server includes an N blade for network communication and a D blade for data access. At the storage server in one embodiment the N blade maintains a mapping of the dynamic cache VSA such as an Internet Protocol mapping dynamic cache ID and dynamic cache attributes.

In one embodiment each dynamic cache VSA is associated with a unique ID dynamic cache ID which is generated in conjunction with the dynamic cache VSA that is instantiated. Thus the ID is generated as part of generating the dynamic cache VSA instance or created after the instance is created. The mapping can include dynamic cache VSA attributes like workload characteristics dynamic cache size and working set change as indicated by the control calls mentioned above. In one embodiment whenever any I O data access request is made to the storage server e.g. D blade on behalf of a request arriving from a dynamic cache VSA it includes a tag with the corresponding dynamic cache ID. The buffer cache tier looks into the attributes corresponding to the dynamic cache ID to make appropriate caching and read ahead decisions.

As described herein the communication between tiers of the cache system can improve utilization of a system where the dynamic cache VSA is hosted. Additionally the system adapts dynamically to the dynamics of the applications accessing data through the dynamic cache VSA and storage server which in turn improves the overall performance of the application. The multi tier protocol expedites the response time of the backend storage server to changes on the external caching tier. Thus the system adapts much quicker to changes in behavior of the applications.

Data access in a data center is often highly dynamic. As described above the multi tiered cache system can respond to changes in a highly dynamic environment by quickly changing behavior of the caching. Thus better resource utilization is accomplished. In addition to having dynamic caching changes the ability to create dynamic caching in VMs allows another layer of adaptability in that virtual caching elements VSAs can be dynamically allocated and de allocated which makes better use of hardware resources than having a dedicated hardware resource to perform the caching.

Physical layer is depicted with various components that can be present in whole or in part and additional components or subcomponents can also be present. Physical layer includes one or more processors or processing resources which execute instructions and can perform various operations as described herein. Processor can include any type of microprocessor central processing unit CPU processing core including multi core devices or other processing devices.

Memory represents the main memory for system and provides temporary storage for code e.g. software routines or series of instructions commands operations programs data to be executed by processor . Memory can include read only memory ROM flash memory one or more varieties of random access memory RAM or the like or a combination of such devices.

The various components of physical layer can be coupled by one or more buses . Bus is an abstraction that represents any one or more separate physical buses communication lines and or point to point connections connected by appropriate bridges adapters and or controllers. Therefore bus can include for example one or more of a system bus a Peripheral Component Interconnect PCI bus a HyperTransport or industry standard architecture ISA bus a small computer system interface SCSI bus a universal serial bus USB or an Institute of Electrical and Electronics Engineers IEEE standard 1394 bus commonly referred to as Firewire .

Physical layer includes one or more network interfaces NIC which represent hardware and software e.g. drivers that enable physical layer to connect and communicate with remote devices over one or more networks. In one embodiment physical layer includes storage resources separated as local to a particular virtual environment and other shared data e.g. shared or cached data for a dynamic cache VSA . For example storage resources represent the cached data shared among multiple virtual environments while storage represents local storage.

Storage includes resources for implementing a write cache which is mapped by virtual filesystem to virtual machine to store the data written for various clients. Storage can be separated into multiple virtual disks VD through M. The virtualization of disks is merely for purposes of storage management and organization and can be performed in any way known in the art.

Storage includes storage resources for implementing a virtual cache layer with resources separated as virtual disks through N. Typically N will be an integer much larger than M. Controller provides physical tier management of the storage. The options for control or management of storage vary widely depending on the desired implementation. For example controller can be implemented as a JBOD Just a Bunch Of Disks controller a RAID Redundant Array of Independent Inexpensive Disks Drives controller or other controller.

Thus it will be understood that storage in addition to being a virtual resource can be managed with abstraction layers to allow a logical disk organization. In one embodiment the abstraction convention implemented in system is the same as the abstraction used by a backend storage server at the data origin e.g. storage server of origin in . However the abstraction convention at system could be different from a backend storage server that is the source of the cached data.

Virtual device layer represents the virtual device as mapped by hypervisor . In one embodiment virtual device includes network interface CPU RAM BIOS Basic Input Output System UART Universal Asynchronous Receiver Transmitter network storage and local storage . Network interface enables virtual device to access other devices across networks via network interface s . CPU represents the processing resources available to virtual machine which consists of dedicated and or shared processing resources .

RAM represents memory resources allocated to virtual machine and includes shared and or dedicated resources of memory . BIOS provides resources to initialize the software and virtual systems on the allocated hardware resources. UART represents direct connection resources rather than point to point or network connection resources. Network storage enables virtual machine to access storage via virtual filesystem and controller . Local storage can provide for example persistent write cache for storing data at system .

Each of the components described at virtual device layer has a physical complement at physical hardware layer . Hypervisor maps the resources of virtual device layer to its complement in physical hardware layer . Virtual device layer is illustrated as included in virtual machine but it will be understood that the resources are included virtually. Virtual machine includes virtual storage appliance VSA which could also be referred to as a virtual storage adapter which has access to the resources of virtual device layer as the available computing resources.

VSA includes software and drivers that manage and control the virtual resources. VSA presents the virtual resources to the applications or workloads that execute on virtual machine . In one embodiment VSA includes driver network stack protocol s OS RAID storage controller network storage driver and virtual nonvolatile RAM V NVRAM .

Driver provides driver resources to drive communication via the network interfaces. Network stack implements one or more communication stacks for protocol s . Protocol s include the one or more protocols used by virtual machine to communicate with networked devices. Operating system controls the flow of operation in virtual machine . RAID represents any type of storage abstraction used for managing storage with one of the various versions of RAID being common types. Many abstraction types are possible. Storage controller can include for example a storage stack and storage drivers used to access storage resources. Network storage driver provides one type of driver for access to storage area networks SANs network area storage NAS or other networked storage. Virtual nonvolatile RAM represents drivers for local storage of virtual machine .

Storage of data in storage units is managed by storage servers which receive and respond to various read and write requests from clients directed to data stored in or to be stored in storage units . Storage units constitute mass storage devices which can include for example flash memory magnetic or optical disks or tape drives illustrated as disks A B . Storage devices can further be organized into arrays not illustrated implementing a Redundant Array of Inexpensive Disks Devices RAID scheme whereby storage servers access storage units using one or more RAID protocols known in the art.

Storage servers can provide file level service such as used in a network attached storage NAS environment block level service such as used in a storage area network SAN environment a service which is capable of providing both file level and block level service or any other service capable of providing other data access services. Although storage servers are each illustrated as single units in a storage server can in other embodiments constitute a separate network element or module an N module and disk element or module a D module . In one embodiment the D module includes storage access components for servicing client requests. In contrast the N module includes functionality that enables client access to storage access components e.g. the D module and the N module can include protocol components such as Common Internet File System CIFS Network File System NFS or an Internet Protocol IP module for facilitating such connectivity. Details of a distributed architecture environment involving D modules and N modules are described further below with respect to and embodiments of a D module and an N module are described further below with respect to .

In one embodiment storage servers are referred to as network storage subsystems. A network storage subsystem provides networked storage services for a specific application or purpose and can be implemented with a collection of networked resources provided across multiple storage servers and or storage units.

In the embodiment of one of the storage servers e.g. storage server A functions as a primary provider of data storage services to client . Data storage requests from client are serviced using disks A organized as one or more storage objects. A secondary storage server e.g. storage server B takes a standby role in a mirror relationship with the primary storage server replicating storage objects from the primary storage server to storage objects organized on disks of the secondary storage server e.g. disks B . In operation the secondary storage server does not service requests from client until data in the primary storage object becomes inaccessible such as in a disaster with the primary storage server such event considered a failure at the primary storage server. Upon a failure at the primary storage server requests from client intended for the primary storage object are serviced using replicated data i.e. the secondary storage object at the secondary storage server.

It will be appreciated that in other embodiments network storage system can include more than two storage servers. In these cases protection relationships can be operative between various storage servers in system such that one or more primary storage objects from storage server A can be replicated to a storage server other than storage server B not shown in this figure . Secondary storage objects can further implement protection relationships with other storage objects such that the secondary storage objects are replicated e.g. to tertiary storage objects to protect against failures with secondary storage objects. Accordingly the description of a single tier protection relationship between primary and secondary storage objects of storage servers should be taken as illustrative only.

In one embodiment network storage system includes multi tiered cache MTC A B . The multi tiered cache is implemented with one or more cache elements at the storage servers and one or more elements across the network at the client side. Thus MTC A and MTC B are illustrated respectively as between storage server A and client and storage server B and client .

Nodes can be operative as multiple functional components that cooperate to provide a distributed architecture of system . To that end each node can be organized as a network element or module N module A B a disk element or module D module A B and a management element or module M host A B . In one embodiment each module includes a processor and memory for carrying out respective module operations. For example N module can include functionality that enables node to connect to client via network and can include protocol components such as a media access layer Internet Protocol IP layer Transport Control Protocol TCP layer User Datagram Protocol UDP layer and other protocols known in the art.

In contrast D module can connect to one or more storage devices via cluster switching fabric and can be operative to service access requests on devices . In one embodiment the D module includes storage access components such as a storage abstraction layer supporting multi protocol data access e.g. Common Internet File System protocol the Network File System protocol and the Hypertext Transfer Protocol a storage layer implementing storage protocols e.g. RAID protocol and a driver layer implementing storage device protocols e.g. Small Computer Systems Interface protocol for carrying out operations in support of storage access operations. In the embodiment shown in a storage abstraction layer e.g. file system of the D module divides the physical storage of devices into storage objects. Requests received by node e.g. via N module can thus include storage object identifiers to indicate a storage object on which to carry out the request.

Also operative in node is M host which provides cluster services for node by performing operations in support of a distributed storage system image for instance across system . M host provides cluster services by managing a data structure such as a RDB RDB A RDB B which contains information used by N module to determine which D module owns services each storage object. The various instances of RDB across respective nodes can be updated regularly by M host using conventional protocols operative between each of the M hosts e.g. across network to bring them into synchronization with each other. A client request received by N module can then be routed to the appropriate D module for servicing to provide a distributed storage system image.

In one embodiment node A includes MTC A and node B includes MTC B . MTC A and MTC B illustrate elements of a multi tiered cache which elements are included at nodes A and B. The elements are one or more tiers of the multi tiered cache. Each cache tier is managed by a corresponding or associated controller. MTC A is shown at the client side and is another tier of a multi tiered cache including MTC A . There can be one or more cache tiers at client that communicate with the tier s at the node to indicate changes of workload which in turn changes the operation of the cache. MTC B at client is illustrated with a dashed line to represent the fact that the tiers may be dynamically instantiated or allocated and dynamically de allocated.

It will be noted that while shows an equal number of N and D modules constituting a node in the illustrative system there can be different number of N and D modules constituting a node in accordance with various embodiments. For example there can be a number of N modules and D modules of node A that does not reflect a one to one correspondence between the N and D modules of node B. As such the description of a node comprising one N module and one D module for each node should be taken as illustrative only.

Memory includes storage locations addressable by processor network adapter and storage adapter for storing processor executable instructions and data structures associated with a multi tiered cache with a virtual storage appliance. A storage operating system portions of which are typically resident in memory and executed by processor functionally organizes the storage server by invoking operations in support of the storage services provided by the storage server. It will be apparent to those skilled in the art that other processing means can be used for executing instructions and other memory means including various computer readable media can be used for storing program instructions pertaining to the inventive techniques described herein. It will also be apparent that some or all of the functionality of the processor and executable software can be implemented by hardware such as integrated currents configured as programmable logic arrays ASICs and the like.

Network adapter comprises one or more ports to couple the storage server to one or more clients over point to point links or a network. Thus network adapter includes the mechanical electrical and signaling circuitry needed to couple the storage server to one or more client over a network. Each client can communicate with the storage server over the network by exchanging discrete frames or packets of data according to pre defined protocols such as TCP IP.

Storage adapter includes a plurality of ports having input output I O interface circuitry to couple the storage devices e.g. disks to bus over an I O interconnect arrangement such as a conventional high performance FC or SAS link topology. Storage adapter typically includes a device controller not illustrated comprising a processor and a memory for controlling the overall operation of the storage units in accordance with read and write commands received from storage operating system . As used herein data written by a device controller in response to a write command is referred to as write data whereas data read by device controller responsive to a read command is referred to as read data. 

User console enables an administrator to interface with the storage server to invoke operations and provide inputs to the storage server using a command line interface CLI or a graphical user interface GUI . In one embodiment user console is implemented using a monitor and keyboard.

In one embodiment computing device includes cache tier . Cache tier includes a cache element and an associated controller to manage the cache tier. There can be multiple cache tiers included in computing device implemented in either memory or a special purpose storage element on computing device . The controller of cache tier communicates with a controller on a cache tier at a client device as described above.

When implemented as a node of a cluster such as cluster of the storage server further includes a cluster access adapter shown in phantom having one or more ports to couple the node to other nodes in a cluster. In one embodiment Ethernet is used as the clustering protocol and interconnect media although it will apparent to one of skill in the art that other types of protocols and interconnects can by utilized within the cluster architecture.

Multi protocol engine includes a media access layer of network drivers e.g. gigabit Ethernet drivers that interface with network protocol layers such as the IP layer and its supporting transport mechanisms the TCP layer and the User Datagram Protocol UDP layer . A file system protocol layer provides multi protocol file access and to that end includes support for the Direct Access File System DAFS protocol the NFS protocol the CIFS protocol and the Hypertext Transfer Protocol HTTP protocol . A VI layer implements the VI architecture to provide direct access transport DAT capabilities such as RDMA as required by the DAFS protocol . An iSCSI driver layer provides block protocol access over the TCP IP network protocol layers while a FC driver layer receives and transmits block access requests and responses to and from the storage server. In certain cases a Fibre Channel over Ethernet FCoE layer not shown can also be operative in multi protocol engine to receive and transmit requests and responses to and from the storage server. The FC and iSCSI drivers provide respective FC and iSCSI specific access control to the blocks and thus manage exports of luns logical units to either iSCSI or FCP or alternatively to both iSCSI and FCP when accessing blocks on the storage server.

In one embodiment multi protocol engine includes a multi tiered cache MTC protocol used by tiers of a multi tiered cache to communicate with each other via their respective controllers . MTC protocol can be exposed as a set of APIs usable with any of a variety of protocols and is thus shown overlaying various protocols. MTC protocol includes rules or standards that manage when and what a cache tier controller will communicate to another cache tier controller. The communication between tiers affects the operation of the multi tiered cache as described above.

The storage operating system also includes a series of software layers organized to form a storage server that provides data paths for accessing information stored on storage devices. Information can include data received from a client in addition to data accessed by the storage operating system in support of storage server operations such as program application data or other system data. Preferably client data can be organized as one or more logical storage objects e.g. volumes that comprise a collection of storage devices cooperating to define an overall logical arrangement. In one embodiment the logical arrangement can involve logical volume block number vbn spaces wherein each volume is associated with a unique vbn.

File system implements a virtualization system of the storage operating system through the interaction with one or more virtualization modules illustrated as a SCSI target module . SCSI target module is generally disposed between drivers and file system to provide a translation layer between the block lun space and the file system space where luns are represented as blocks. In one embodiment file system implements a WAFL write anywhere file layout file system having an on disk format representation that is block based using e.g. 4 kilobyte KB blocks and using a data structure such as index nodes inodes to identify files and file attributes such as creation time access permissions size and block location . File system uses files to store metadata describing the layout of its file system including an Mode file which directly or indirectly references points to the underlying data blocks of a file.

Operationally a request from a client is forwarded as a packet over the network and onto the storage server where it is received at a network adapter. A network driver such as layer or layer processes the packet and if appropriate passes it on to a network protocol and file access layer for additional processing prior to forwarding to file system . There file system generates operations to load retrieve the requested data from the disks if it is not resident in core i.e. in memory . If the information is not in memory file system accesses the Mode file to retrieve a logical vbn and passes a message structure including the logical vbn to the RAID system . There the logical vbn is mapped to a disk identifier and device block number disk dbn and sent to an appropriate driver of disk drive system . The disk driver accesses the dbn from the specified disk and loads the requested data block s in memory for processing by the storage server. Upon completion of the request the node and operating system returns a reply to the client over the network.

It should be noted that the software path through the storage operating system layers described above needed to perform data storage access for the client request received at the storage server adaptable to the teachings of the invention can alternatively be implemented in hardware. That is in an alternate embodiment of the invention a storage access request data path can be implemented as logic circuitry embodied within a field programmable gate array FPGA or an application specific integrated circuit ASIC . This type of hardware embodiment increases the performance of the storage service provided by the storage server in response to a request issued by a client. Moreover in another alternate embodiment of the invention the processing elements of adapters can be configured to offload some or all of the packet processing and storage access operations respectively from processor to thereby increase the performance of the storage service provided by the storage server. It is expressly contemplated that the various processes architectures and procedures described herein can be implemented in hardware firmware or software.

When implemented in a cluster data access components of the storage operating system can be embodied as D module for accessing data stored on disk. In contrast multi protocol engine can be embodied as N module to perform protocol termination with respect to a client issuing incoming access over the network as well as to redirect the access requests to any other N module in the cluster. A cluster services system can further implement an M host e.g. M host to provide cluster services for generating information sharing operations to present a distributed file system image for the cluster. For instance media access layer can send and receive information packets between the various cluster services systems of the nodes to synchronize the replicated databases in each of the nodes.

In addition a cluster fabric CF interface module CF interface modules A B can facilitate intra cluster communication between N module and D module using a CF protocol . For instance D module can expose a CF application programming interface API to which N module or another D module not shown issues calls. To that end CF interface module can be organized as a CF encoder decoder using local procedure calls LPCs and remote procedure calls RPCs to communicate a file system command to between D modules residing on the same node and remote nodes respectively.

As used herein the term storage operating system generally refers to the computer executable code operable on a computer to perform a storage function that manages data access and can implement data access semantics of a general purpose operating system. The storage operating system can also be implemented as a microkernel an application program operating over a general purpose operating system or as a general purpose operating system with configurable functionality which is configured for storage applications as described herein.

Flow diagrams as illustrated herein provide examples of sequences of various process actions. Although shown in a particular sequence or order unless otherwise specified the order of the actions can be modified. Thus the illustrated embodiments should be understood only as an example and the process can be performed in a different order and some actions can be performed in parallel. Additionally one or more actions can be omitted in various embodiments thus not all actions are required in every embodiment. Other process flows are possible.

Various operations or functions are described herein which can be described or defined as software code instructions configuration and or data. The content can be directly executable object or executable form source code or difference code delta or patch code . The software content of the embodiments described herein can be provided via an article of manufacture with the content stored thereon or via a method of operating a communications interface to send data via the communications interface. A machine readable medium or computer readable medium can cause a machine to perform the functions or operations described and includes any mechanism that provides i.e. stores and or transmits information in a form accessible by a machine e.g. computing device electronic system or other device such as via recordable non recordable storage media e.g. read only memory ROM random access memory RAM magnetic disk storage media optical storage media flash memory devices or other storage media or via transmission media e.g. optical digital electrical acoustic signals or other propagated signal . A communication interface includes any mechanism that interfaces to any of a hardwired wireless optical or other medium to communicate to another device such as a memory bus interface a processor bus interface an Internet connection a disk controller. The communication interface can be configured by providing configuration parameters and or sending signals to prepare the communication interface to provide a data signal describing the software content.

Various components described herein can be a means for performing the operations or functions described. Each component described herein includes software hardware or a combination of these. The components can be implemented as software modules hardware modules special purpose hardware e.g. application specific hardware application specific integrated circuits ASICs digital signal processors DSPs etc. embedded controllers hardwired circuitry etc.

Besides what is described herein various modifications can be made to the disclosed embodiments and implementations without departing from their scope. Therefore the illustrations and examples herein should be construed in an illustrative and not a restrictive sense.

