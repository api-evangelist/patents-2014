---

title: Method and system for providing storage services
abstract: Method and system are provided for managing components of a storage operating environment having a plurality of virtual machines that can access a storage device managed by a storage system. The virtual machines are executed by a host platform that also executes a processor-executable host services module that interfaces with at least a processor-executable plug-in module for providing information regarding the virtual machines and assists in storage related services, for example, replicating the virtual machines.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09043791&OS=09043791&RS=09043791
owner: NETAPP, INC.
number: 09043791
owner_city: Sunnyvale
owner_country: US
publication_date: 20140124
---
This patent application claims priority of and is a continuation of U.S. patent application Ser. No. 13 096 959 filed on Apr. 28 2011 and now U.S. Pat. No. 8 671 406 the disclosure of which is incorporated herein by reference in its entirety.

Various forms of storage systems are used today. These forms include direct attached storage DAS network attached storage NAS systems storage area networks SANs and others. Network storage systems are commonly used for a variety of purposes such as providing multiple users with access to shared data backing up data and others.

A storage system typically includes at least one computing system executing a storage operating system for storing and retrieving data on behalf of one or more client processing systems clients . The storage operating system stores and manages shared data containers in a set of mass storage devices.

Storage systems are being used extensively with virtual machines in virtual environments which add to the complexity of a storage operating environment. As the complexity of a storage operating environment increases so does the need for efficiently managing components within the operating environment and providing storage related services. Continuous efforts are being made to efficiently manage a storage operating environment and provide storage related services.

In one embodiment a method and system is provided for managing components at a storage operating environment having a plurality of virtual machines that can access a storage device managed by a storage system. The virtual machines are executed by a host platform that also executes a processor executable host services module that interfaces with at least a processor executable plug in module for providing information regarding the virtual machines and storage related services for example replicating the virtual machines.

In another embodiment a machine implemented method for managing events in a storage operating environment is provided. The method includes providing a processor executable plug in application that interfaces with a processor executable host services module and an operating system of a computing system executing a plurality of virtual machines and the plug in application maintaining a queue for storing information related to any event related to any of the plurality of virtual machines.

The method further includes determining if a virtual machine was added or removed during a certain duration the plug in application collecting information regarding the plurality of virtual machines in the storage operating environment the plug in application notifying the host services module of a change in the storage operating environment and the plug in application providing information regarding the virtual machines to the host services module when requested.

In yet another embodiment a machine implemented method for a storage operating environment having a plurality of virtual machines and a storage system managing storage space at a storage device is provided. The method includes obtaining information regarding the plurality of virtual machines operating within the storage environment and generating a data structure for illustration of a hierarchical structure showing a virtual storage device associated with each of the plurality of virtual machines a replicated copy of the plurality of virtual machines with a location map showing where the replicated copy is stored and a replicated copy of a storage volume generated by a storage system executed within the storage operating environment. The hierarchical structure is generated by a processor executable plug in application that obtains information regarding the plurality of virtual machines.

The method further includes obtaining information regarding a virtual machine from among the plurality of virtual machines in response to a request for information using the data structure for the hierarchical structure and presenting the requested information on a display device.

In another embodiment a machine implemented method for a storage operating environment having a plurality of virtual machines and a storage system managing storage space at a storage device is provided. The method includes generating a data structure for representing the plurality of virtual machines in a searchable hierarchical structure showing a virtual storage device associated with each virtual machine a replicated copy of each virtual machine with a location map showing a location where the replicated copy is stored and a replicated copy of a storage volume at the storage device generated by the storage system executed within the storage operating environment. A processor executable plug in application obtains information regarding the plurality of virtual machines and generates the data structure for representing the virtual machines.

The method further includes filtering virtual machine representation based on a request for information and presenting the filtered representation on a display device.

This brief summary has been provided so that the nature of this disclosure may be understood quickly. A more complete understanding of the disclosure can be obtained by reference to the following detailed description of the various embodiments thereof in connection with the attached drawings.

As used in this disclosure the terms component module system and the like are intended to refer to a computer related entity either software executing general purpose processor hardware firmware and a combination thereof. For example a component may be but is not limited to being a process running on a processor a processor an object an executable a thread of execution a program and or a computer.

By way of illustration both an application running on a server and the server can be a component. One or more components may reside within a process and or thread of execution and a component may be localized on one computer and or distributed between two or more computers. Also these components can execute from various computer readable media having various data structures stored thereon. The components may communicate via local and or remote processes such as in accordance with a signal having one or more data packets e.g. data from one component interacting with another component in a local system distributed system and or across a network such as the Internet with other systems via the signal .

Computer executable components can be stored for example on computer readable media including but not limited to an ASIC application specific integrated circuit CD compact disc DVD digital video disk ROM read only memory floppy disk hard disk EEPROM electrically erasable programmable read only memory memory stick or any other storage device in accordance with the claimed subject matter.

In one embodiment a method and system is provided for managing components at a storage operating environment having a plurality of virtual machines that can access a storage device managed by a storage system. The virtual machines are executed by a host platform that also executes a processor executable host services module that interfaces with at least a processor executable plug in module for providing information regarding the virtual machines and assists in storage related services for example replicating the virtual machines

The guest software expects to operate as if it were running on a dedicated computer rather than in a VM. That is the guest software expects to control various events and have access to hardware resources on a physical computing system may also be referred to as a host platform which maybe referred to herein as host hardware resources . The host hardware resource may include one or more processors resources resident on the processors e.g. control registers caches and others memory instructions residing in memory e.g. descriptor tables and other resources e.g. input output devices host attached storage network attached storage or other like storage that reside in a physical machine or are coupled to the host platform.

In one embodiment system includes at least a computing system may also be referred to as a host platform and or server communicably coupled to a storage system executing a storage operating system via a connection system such as a local area network LAN wide area network WAN the Internet and others. As described herein the term communicably coupled may refer to a direct connection a network connection or other connections to enable communication between devices.

In another embodiment system may include a cluster of host platforms where computing tasks are distributed among different host platforms . More than one host platforms may also be used for redundancy because if one host platform fails another host platform can take over the responsibilities functionality of the failed host platform.

System may also include a management console that executes processor executable instructions for example a management application for managing and configuring various elements of system . One or more client systems may also be referred to as client system may also be provided for generating input output requests for reading and writing information or for any other reason.

Host platform management console and client system may be general purpose computers having a plurality of components. As described below in more detail these components may include a central processing unit CPU main memory I O devices and storage devices for example flash memory hard drives and others . The main memory may be coupled to the CPU via a system bus or a local memory bus. The main memory may be used to provide the CPU access to data and or program information that is stored in main memory at execution time. Typically the main memory is composed of random access memory RAM circuits.

In one embodiment the storage system has access to a set of mass storage devices may be referred to as storage devices within at least one storage subsystem . The mass storage devices may include writable storage device media such as magnetic disks video tape optical DVD magnetic tape non volatile memory devices for example self encrypting drives flash memory devices and any other similar media adapted to store information. The storage devices may be organized as one or more groups of Redundant Array of Independent or Inexpensive Disks RAID . The embodiments disclosed are not limited to any particular storage device or storage device configuration.

The storage system provides a set of storage volumes to the host platform via connection system . The storage operating system can present or export data stored at storage devices as a volume or one or more qtree sub volume units. Each volume may be configured to store data containers scripts word processing documents executable programs and any other type of structured or unstructured data. The term data container as used herein means a block a file a logical unit of data or any other information. From the perspective of one of the client systems each volume can appear to be a storage device. However each volume can represent the storage space in one storage device an aggregate of some or all of the storage space in multiple storage devices a RAID group or any other suitable set of storage space. The storage devices may be presented as a logical unit number LUN where a LUN may refer to a logical data container that looks like a storage device to a host client but which actually may be distributed across multiple storage devices by storage system .

The storage system may be used to store and manage information at storage devices based on a request generated by management console client system and or a VM. The request may be based on file based access protocols for example the Common Internet File System CIFS protocol or Network File System NFS protocol over the Transmission Control Protocol Internet Protocol TCP IP . Alternatively the request may use block based access protocols for example the Small Computer Systems Interface SCSI protocol encapsulated over TCP iSCSI and SCSI encapsulated over Fibre Channel FCP .

In a typical mode of operation the client system transmits one or more input output I O commands such as an NFS or CIFS request over connection system to the storage system . Storage system receives the request issues one or more I O commands to storage devices to read or write information on behalf of the client system and issues an NFS or CIFS response containing the requested information over the network to the respective client system.

Although storage system is shown as a stand alone system i.e. a non cluster based system in another embodiment storage system may have a distributed architecture that may include for example a separate N network blade and D disk blade. Briefly the N blade is used to communicate with host platform and clients while the D blade is used to communicate with the storage devices that are a part of a storage sub system. The N blade and D blade may communicate with each other using an internal protocol.

Alternatively storage system may have an integrated architecture where the network and data components are all contained in a single box. The storage system further may be coupled through a switching fabric to other similar storage systems not shown which have their own local storage subsystems. In this way all of the storage subsystems can form a single storage pool to which any client of any of the storage servers has access.

Information stored at storage devices is typically replicated or backed up at one or more storage locations not shown . A backup of a data container or a file system includes copying and storage of a directory and or tree structure of the file system. A processor executable backup process may use image taking technology e.g. the Snapshot technology provided by NetApp Inc. of Sunnyvale Calif. without derogation of trademark rights of NetApp Inc. to backup all or a portion of the file system. The image s can be used later during a restore process.

A snapshot is a persistent point in time PPT image of the active file system that enables quick recovery of information after information has been corrupted lost or altered. Snapshots can be created by copying the data at each predetermined point in time to form a consistent image. The terms snapshot and backup are used interchangeably throughout this specification. It is noteworthy that the adaptive embodiments described herein are not limited to using any particular imaging technology.

In some embodiments the backup process produces backup information that may include metadata that is stored by storage system . The backup information includes information describing the backup performed on the file system e.g. a time stamp data container names and or location information . The backup information and images are used later to restore the imaged file system or a portion thereof.

A restore operation using the metadata may be performed for a variety of reasons. For example a restore operation may be performed when an error occurs in the file system in an application in the server or storage operating system or in the other applications that causes a crash and reboot of the server system or the storage system. A restore may be performed when data in the file system have been undesirably altered corrupted and or deleted for example by a computer virus or other malignant code. If a data container has undesirably become altered corrupted and or deleted it is advantageously restored by copying and or replacing it by using a previously stored image thereof.

The restore process is configured to restore part of the file system. The restore process retrieves the backup information and or image s for the part of the file system. The restore process then restores the part of the file system by using the backup information and or images. The restore process may do so by deleting and replacing data containers of the file system with data containers from the backup images. Alternatively the set of original data containers may have been deleted and does not exist and thus are replaced by the retrieved backup information and images. Backup operations using the various modules of are described below in more detail.

Referring back to host platform may also include a processor executable virtual execution environment . The virtual execution environment may include a plurality of VMs executing a plurality of guest OS may also be referred to as guest OS that share hardware resources . As described above hardware resources may include CPU memory I O devices storage or any other hardware resource.

In one embodiment host platform may also include a virtual machine monitor VMM for example Hyper V layer provided by Microsoft Corporation without derogation of any trademark rights owned by Microsoft Corporation a processor executed hypervisor layer provided by VMWare Inc. without derogation of any trademark rights owned by VMWare Inc. or any other type of VMM . VMM presents and manages the plurality of VMs executed by the host platform. The VMM may include or interface with a virtualization layer VL shown as VIL in figures that provides one or more virtualized hardware resource to each guest OS 

In one embodiment VMM is executed by host platform . In another embodiment VMM may be executed by an independent stand alone computing system often referred to as a hypervisor server or a VMM server. It is noteworthy that various vendors provide virtualization environments for example VMware Inc. Microsoft Corporation and others. The generic virtualization environment described above with respect to may be customized depending on the virtual environment provider.

In one embodiment system also includes a centralized module that maintains a data structure for providing and assisting in various storage related services as described below in more detail. In one embodiment the centralized module may use an interface to interface with management console and client system via connection system . Interface is used to communicate with host services module that is described below in more detail. The structure and logic used by interface will depend on a protocol standard that is used to communicate with connection system . For example if connection system uses an Ethernet connection then interface will include the logic and circuitry for handling Ethernet based communication. Furthermore although separate interface and have been shown a single interface may be used to communicate with both connection system and host services module .

The host services module may include a plurality of processor executable host services application programming interface APIs according to one embodiment. The host services APIs are designed to interface with a plurality of processor executable plug in applications also referred to as plug ins . The APIs are used for certain operations functionality as described below. The plug in applications are used to assist or perform certain functions that are also described below in more detail. The plug in applications may be used to interface with various modules for example the VMM host services module storage system and others.

Plug in interfaces with host services module via a host services interface . The host services module interfaces with the centralized module via an interface . The host services module may include a plurality of processor executable APIs for example an event API a discovery API a backup API and others. The functionality of the various APIs is described below in more detail.

In one embodiment plug in interfaces with the operating system of host platform via an operating system interface . The operating system includes executable instructions for managing applications and other host platform operations. The operating system can be for example UNIX Windows NT Linux or any other operating system.

In one embodiment host platform executes a backup framework engine to help implement backup functions. In some embodiments the backup framework engine includes a VSS Volume Shadow Sevices layer a processor executable module provided by Microsoft Corporation. The VSS layer is used by the operating system to take snapshots of various VMs as described below in detail. The snapshots taken by VSS are different from the snapshots taken by the storage system via a back up request forwarded by plug in via the storage system interface . The storage operating system of storage system generates one or more images relating to the file system to backup all or a portion of the file system e.g. a single data container multiple data containers and or one or more volumes.

After the backup is performed the storage operating system notifies plug in module that the backup operation is completed. For each backup the storage system also generates backup information that is forwarded by plug in to centralized module via host services module and then stored at data structure . The backup information may be in the form of metadata and may include information about a backup for example identification for the backup a time stamp when the backup was performed and filenames directory locations on the storage device s and or the directory path where backups are stored. The backup information may be used later to restore the file system and or portions thereof for instance in case of a system crash data corruption virus or a similar occurrence.

Because all the backup information is stored by the centralized module at data structure the information is easy to access and secure. This allows one to efficiently perform a restore operation because centralized module maintains data structure with all the backup information and can easily access it when the backup information is needed for a restore operation.

The plug in interfaces with VMM for reporting events in system and other operations as described below in more detail. Plug in includes an event manager for managing events according to one embodiment. Event manager interfaces with an OS management interface described below in detail.

Event manager maintains a queue for storing event notifications that are received by OS management interface . In one embodiment queue stores a data structure that includes information regarding whether a VM is added removed or modified. As an example queue may store a table that identifies an event an affected VM and an indicator indicating if a VM is added removed or modified. The processes for managing event notifications are described below in more detail. Plug in may also include a cluster co coordinator that interfaces with a cluster to obtain cluster related information.

The storage system interface may be configured to act as an interface between various host platform components and the storage system . The storage system interface may communicate with the storage system by using for example a Zephyr Application and Programming Interface ZAPI protocol. In particular the storage system interface interacts with the plug in and backup frame work engine to receive and perform requests by interacting with other programs of host platform or the storage system .

In some embodiments the storage system interface includes SnapDrive without derogation of trademark rights of NetApp Inc. a program provided by NetApp Inc. of Sunnyvale Calif. It is noteworthy that the adaptive embodiments described herein are not limited to using SnapDrive any other module may be used for interfacing with the storage operating system.

The hyper V server executes an operating system referred to as a parent operating system that hosts a plurality of VMs similar to VMs . Storage operating system presents storage space to the parent operating system as LUNs for example and . The parent operating system generates a virtual hard drive VHD file for the presented LUNs for example VM VHD is the VHD file for LUN and VHD file is for LUN . The VHD files appear as a storage drive to a user using the VMs. For example VHD file appears as K to VM A and VHD B appears as M drive to VM B. LUNs and are typically replicated by the storage system while drives and may be replicated by backup framework engine using VSS.

The event information may be received by plug in from OS management interface that maintains a data structure not shown within the operating system . The data structure identifies each VM with a unique identifier and may store an indicator indicating when a VM is added removed or modified. As an example OS management interface may include a Windows Management Instrumentation WMI module in a Windows without derogation of any trademark rights of Microsoft Corporation Operating system environment. WMI provides a standard methodology for sharing management information. WMI maintains a data structure having objects with details regarding various VMs and other events. It is noteworthy that the embodiments disclosed herein are not limited to the WMI module that collects event information and then provides it to event manager of plug in

In block S the events are processed as described below with respect to the process flow diagram of and thereafter the process ends in block S.

If a VM is added or removed then the process moves to S where all VM details are obtained by event manager via storage system interface . In one embodiment storage system interface may first collect VM information from OS management interface . This may include VM identifier information and any other information that is maintained by OS management interface . Storage system interface may also request information from storage system . This may include information regarding where VM files may be stored by storage system . Storage system interface then provides the VM details to event manager .

Plug in engine then notifies host services module via host services interface that a change event has occurred in step S. The host services module notifies the centralized module of the changed event. If the centralized module requests details regarding the change event then in block S plug in engine provides the VM details collected in block S to the host services module and then to centralized module that stores the details at data structure .

Referring back to block S if a VM is not added or removed then the process moves to block S. During block S plug in engine determines if there is a cluster related event. An example of a cluster related event is when a cluster node is taken off line or is initialized and comes on line.

If a cluster event occurred then in block S plug in obtains cluster details via the cluster co coordinator that communicates with the various cluster nodes via host interface . The cluster information may include the identity of the various cluster nodes and information regarding the VMs that are operating within each cluster node. The process then moves to block S that has been described above.

If there is no cluster related event as determined in block S the process moves to block S where plug in obtains details regarding any VMs that may have changed. Plug in obtains the information regarding changed VMs from VMM that manages all the VMs and or OS management interface . Thereafter the changed VM details are sent to host services module via host services interface in block S. The information is provided to centralized module by the host services module and stored at data structure .

The embodiments disclosed herein have numerous advantages. For example event information and notification is consolidated so that the management console does not have to be notified after every change within system . This is efficient and saves computing resources while reducing overall interruption within system .

After the request is received in block S plug in engine determines if the request pertains to a cluster. Plug in engine is able to identify a cluster if the request specifies a unique identifier identifying the cluster. Plug in engine may access a data structure not shown maintained by cluster co coordinator to verify if a cluster is being used. If a cluster is involved then in block S plug in engine obtains details regarding the cluster. The details are obtained by cluster coordinator via host services interface . A resource data structure may also be referred to as resource graph is then generated by plug in engine . An example of the cluster resource data structure is shown in and is described below in more detail.

If a cluster is not being used as determined in block S the process moves to block S when a resource data structure for a non cluster based system is generated by plug in engine . The details regarding storage system and the VMs are obtained via storage system interface . An example of a resource data structure is also provided in and described below in more detail.

The resource data structures generated in blocks S and S are filtered in block S. The filtering process is based on parameters specified in the discovery request received in block S. For example a request may specify that a user wants the identity of the storage systems associated with a VM. Plug in obtains the identity of the storage systems from the resource data structure. The filtered results are returned to the requester in block S via the host services module and the centralized module .

In one embodiment resource data structures generated by plug in may be stored as part of data structure . In this case centralized module is able to provide resource information to a user efficiently because it maintains data structure . A user simply has to request information from centralized module and the information is easily provided using data structure . The underlying resource information is collected and generated by plug in as described above.

Host is identified as host in resource data structure . The virtual machine VM is shown as . The configuration information for VM is shown as Config stored in a data structure shown as file . The file is stored at a drive shown as MP H that is identified by a uniform global identifier shown as wvol associated with a LUN shown as LUn . LUN belongs to Volume I which is managed by storage system shown as Filer

VM is presented a virtual hard drive VHD as a data structure shown as File . File presents a drive MP F to VM identified by wvol associated with Lun . LUN is also a part of Volume that is managed by Filer for example storage system .

VM may be replicated by the operating system backup framework engine and the replicated copy is shown as . The replicated copy is represented by a data container shown as file stored at MP H identified by wvol which in turn is associated with LUN located in Volume and managed by Filer . The replicated copy of Volume that is taken by Filer is shown as Backup SS where SS stands for snapshot.

The resource data structure may be traversed to obtain any information that may be requested by a discovery request. For example if a request desires storage system identifier information for VM then plug in traverses through the resource data structure and obtains filer identifier and presents the requested information. By maintaining the resource data structure as a searchable data structure plug in can respond to different requests efficiently.

In one embodiment the resource data structure is for a cluster of host platforms identified as cluster . The virtual machine associated with cluster is shown as VM that is presented as a virtual hard disk shown as VHD . The file associated with VHD is shown as that is stored at MP H identified by wvol and LUN . LUN is a part of Volume which is managed by storage system shown as Filer. A pass through VHD is shown as that is also associated with LUN . A VHD typically includes a file system for example a NTFS file system. A pass through VHD PTVHD is presented to a VM by a parent operating system directly as a LUN without a file system. The VM may then add a file system.

The backup copy of VM as taken by operating system backup framework engine is shown as HVSS that is associated with a data container shown as File . File is stored at MP H identified by wvol which in turn is associated with LUN . LUN is a part of Volume managed by Filer . The replicated copy of Volume is shown as Backup SS

The resource data structures shown above may be used by a user for example an administrator to view virtual machine details including configuration information. The administrator is easily able to determine the location of the various data structures associated with the VMs. This allows the administrator to change the configuration information or restore a VM using a backup copy. The administrator does not have to determine any of the location information because it is conveniently presented in the searchable tree like graphical resource data structure.

In block S plug in engine determines if the request was forwarded by plug in module itself as explained below. The host services backup API makes this determination by evaluating the request and determining if an indicator for example a flag is set for a forwarded request. If the request is not forwarded by plug in module then the process moves to block S when plug in engine determines if the request involves a cluster of host platforms . This is determined by evaluating the request and determining if a cluster is identified in the request.

If a cluster is identified then in block S plug in engine determines the nodes i.e. host platform that are associated with the VMs. Plug in engine makes this determination using cluster coordinator that obtains cluster information via host services interface .

In block S plug in engine invokes the backup API for the backup operation and the process moves to block S. Plug in may also tag a request as being a forwarded request in block S. For example if there are a plurality of parent operating systems each managing one or more VMs after the request is received it may have to cycle through the parent operating systems to replicate all the VMs. Plug in in this case may insert a flag indicating that the request is being forwarded.

Referring back to blocks S and S if the request is forwarded or if does not involve a cluster then in block S plug in module sends a backup request to OS backup framework engine via the operating system interface . The request identifies which VMs are to be replicated.

In block S one or more VMs are backed up by the OS backup framework engine . For example in a Windows operating environment the VSS takes a snapshot of the VM. The snapshots are renamed in block S based on the backup request received in block S. The plug in engine collects meta data for the backups in block S. The meta data is received from the operating system backup framework engine that performs the backup. The meta data includes information for example identification for the backup a time stamp when the backup was performed and filenames directory locations on the storage device s and or the directory path where backups are stored.

In block S a resource data structure described above with respect to is generated. The backup results are consolidated in block S. For example if a cluster is involved plug in engine consolidates the results for multiple hot platforms .

In block S the plug in module returns the backup results the resource data structures and the meta data are sent to the requester via host services backup API . The backup information is forwarded to centralized module and stored as data structure .

The backup process of has various advantages. For example the resource data structure is made available to centralized module . This allows the centralized module to efficiently process any requests to restore a VM using the resource data structure because centralized module has access to all the relevant information regarding all the backups including location information.

System may include a plurality of processors A and B a memory a network adapter an access adapter used for a distributed storage environment a storage adapter and local storage interconnected by a system bus . The local storage comprises one or more storage devices such as disks utilized by the processors to locally store configuration and other information.

The access adapter comprises a plurality of ports adapted to couple system to other nodes of a distributed storage environment not shown . In the illustrative embodiment Ethernet may be used as the protocol and interconnect media although it will be apparent to those skilled in the art that other types of protocols and interconnects may be utilized within the distributed storage environment.

System is illustratively embodied as a dual processor storage system executing a storage operating system that preferably implements a high level module such as a file system to logically organize information as a hierarchical structure of named directories files and special types of files called virtual disks hereinafter generally blocks on storage devices . However it will be apparent to those of ordinary skill in the art that the system may alternatively comprise a single or more than two processor systems. Illustratively one processor executes the functions of an N module on a node while the other processor B executes the functions of a D module.

The memory illustratively comprises storage locations that are addressable by the processors and adapters for storing programmable instructions and data structures. The processor and adapters may in turn comprise processing elements and or logic circuitry configured to execute the programmable instructions and manipulate the data structures. It will be apparent to those skilled in the art that other processing and memory means including various computer readable media may be used for storing and executing program instructions pertaining to the invention described herein.

The storage operating system portions of which is typically resident in memory and executed by the processing elements functionally organizes the system by inter alia invoking storage operations in support of the storage service provided by storage system . An example of operating system is the DATA ONTAP Registered trademark of NetApp Inc. operating system available from NetApp Inc. that implements a Write Anywhere File Layout WAFLO Registered trademark of NetApp Inc. file system. However it is expressly contemplated that any appropriate storage operating system may be enhanced for use in accordance with the inventive principles described herein. As such where the term ONTAP is employed it should be taken broadly to refer to any storage operating system that is otherwise adaptable to the teachings of this invention.

The network adapter comprises a plurality of ports adapted to couple the system to one or more clients over point to point links wide area networks virtual private networks implemented over a public network Internet or a shared local area network. The network adapter thus may comprise the mechanical electrical and signaling circuitry needed to connect storage system to the network. Illustratively the computer network may be embodied as an Ethernet network or a FC network.

The storage adapter cooperates with the storage operating system executing on the system to access information requested by the clients and management application . The information may be stored on any type of attached array of writable storage device media such as video tape optical DVD magnetic tape bubble memory electronic random access memory flash memory devices micro electro mechanical and any other similar media adapted to store information including data and parity information.

The storage adapter comprises a plurality of ports having input output I O interface circuitry that couples to the disks over an I O interconnect arrangement such as a conventional high performance FC link topology. In another embodiment instead of using a separate network and storage adapter a converged adapter is used to process both network and storage traffic.

In one example operating system may include several modules or layers . These layers include a file system manager that keeps track of a directory structure hierarchy of the data stored in storage devices and manages read write operations i.e. executes read write operations on disks in response to client requests.

Operating system may also include a protocol layer and an associated network access layer to allow system to communicate over a network with other systems such as host platform clients and management application . Protocol layer may implement one or more of various higher level network protocols such as NFS CIFS Hypertext Transfer Protocol HTTP TCP IP and others as described below.

Network access layer may include one or more drivers which implement one or more lower level protocols to communicate over the network such as Ethernet. Interactions between clients and mass storage devices are illustrated schematically as a path which illustrates the flow of data through operating system .

The operating system may also include a storage access layer and an associated storage driver layer to allow system to communicate with a storage device. The storage access layer may implement a higher level disk storage protocol such as RAID redundant array of inexpensive disks while the storage driver layer may implement a lower level storage device access protocol such as FC or SCSI.

It should be noted that the software path through the operating system layers described above needed to perform data storage access for a client request received at node may alternatively be implemented in hardware. That is in an alternate embodiment of the disclosure the storage access request data path may be implemented as logic circuitry embodied within a field programmable gate array FPGA or an ASIC. This type of hardware implementation increases the performance of the file service provided by system in response to a file system request issued by client .

As used herein the term storage operating system generally refers to the computer executable code operable on a computer to perform a storage function that manages data access and may implement data access semantics of a general purpose operating system. The storage operating system can also be implemented as a microkernel an application program operating over a general purpose operating system such as UNIX or Windows XP or as a general purpose operating system with configurable functionality which is configured for storage applications as described herein.

In addition it will be understood to those skilled in the art that the invention described herein may apply to any type of special purpose e.g. file server filer or storage serving appliance or general purpose computer including a standalone computer or portion thereof embodied as or including a storage system. Moreover the teachings of this disclosure can be adapted to a variety of storage system architectures including but not limited to a network attached storage environment a storage area network and a disk assembly directly attached to a client or host computer. The term storage system should therefore be taken broadly to include such arrangements in addition to any subsystems configured to perform a storage function and associated with other equipment or systems. It should be noted that while this description is written in terms of a write any where file system the teachings of the present invention may be utilized with any suitable file system including a write in place file system.

The processing system includes one or more processors and memory coupled to a bus system . The bus system shown in is an abstraction that represents any one or more separate physical buses and or point to point connections connected by appropriate bridges adapters and or controllers. The bus system therefore may include for example a system bus a Peripheral Component Interconnect PCI bus a HyperTransport or industry standard architecture ISA bus a small computer system interface SCSI bus a universal serial bus USB or an Institute of Electrical and Electronics Engineers IEEE standard 1394 bus sometimes referred to as Firewire .

The processors are the central processing units CPUs of the processing system and thus control its overall operation. In certain embodiments the processors accomplish this by executing programmable instructions stored in memory . A processor may be or may include one or more programmable general purpose or special purpose microprocessors digital signal processors DSPs programmable controllers application specific integrated circuits ASICs programmable logic devices PLDs or the like or a combination of such devices.

Memory represents any form of random access memory RAM read only memory ROM flash memory or the like or a combination of such devices. Memory includes the main memory of the processing system . Instructions which implements techniques introduced above may reside in and may be executed by processors from memory . Instructions may include executable instructions for plug in host services APIs code for executing process steps of and H and other instructions.

Also connected to the processors through the bus system are one or more internal mass storage devices and a network adapter . Internal mass storage devices may be or may include any conventional medium for storing large volumes of data in a non volatile manner such as one or more magnetic or optical based disks. The network adapter provides the processing system with the ability to communicate with remote devices over a network and may be for example an Ethernet adapter a FC adapter or the like. The processing system also includes one or more input output I O devices coupled to the bus system . The I O devices may include for example a display device a keyboard a mouse etc.

The system and techniques described above are applicable and useful in the upcoming cloud computing environment. Cloud computing means computing capability that provides an abstraction between the computing resource and its underlying technical architecture e.g. servers storage networks enabling convenient on demand network access to a shared pool of configurable computing resources that can be rapidly provisioned and released with minimal management effort or service provider interaction. The term cloud is intended to refer to the Internet and cloud computing allows shared resources for example software and information to be available on demand like a public utility.

Typical cloud computing providers deliver common business applications online which are accessed from another web service or software like a web browser while the software and data are stored remotely on servers. The cloud computing architecture uses a layered approach for providing application services. A first layer is an application layer that is executed at client computers. In this example the application allows a client to access storage via a cloud.

After the application layer is a cloud platform and cloud infrastructure followed by a server layer that includes hardware and computer software designed for cloud specific services. The plug in module and the storage systems described above can be a part of the server layer for providing storage services. Details regarding these layers are not germane to the inventive embodiments.

Thus a method and apparatus for providing storage services using a plug in have been described. Note that references throughout this specification to one embodiment or an embodiment mean that a particular feature structure or characteristic described in connection with the embodiment is included in at least one embodiment of the present invention. Therefore it is emphasized and should be appreciated that two or more references to an embodiment or one embodiment or an alternative embodiment in various portions of this specification are not necessarily all referring to the same embodiment. Furthermore the particular features structures or characteristics being referred to may be combined as suitable in one or more embodiments of the invention as will be recognized by those of ordinary skill in the art.

While the present disclosure is described above with respect to what is currently considered its preferred embodiments it is to be understood that the disclosure is not limited to that described above. To the contrary the disclosure is intended to cover various modifications and equivalent arrangements within the spirit and scope of the appended claims.

