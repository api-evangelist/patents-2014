---

title: Method and system for transferring replicated information from source storage to destination storage
abstract: Machine implemented method and system for transferring replicated information from a first storage location managed by a storage operating system at a first storage system node and accessible to a client computing system to a second storage location managed by a second storage system node are provided. A resource pool having a plurality of tokens is maintained for authorizing a replication engine to transfer replicated information from the first storage location to the second storage location. The number of available tokens is increased when traffic due to client requests for accessing the first storage location is less than a first threshold level. The number of available tokens is decreased for reducing transfer of information via the replication engine, when latency in responding to the client requests reaches a second threshold value and the traffic due to client requests reaches the first threshold value.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08955087&OS=08955087&RS=08955087
owner: Netapp, Inc.
number: 08955087
owner_city: Sunnyvale
owner_country: US
publication_date: 20140123
---
This application is a continuation of co pending application Ser. No. 13 550 860 filed On Jul. 17 2012 the disclosure of which is incorporated herein by reference in its entirety.

The present disclosure relates to storage systems and more particularly to data replication of storage devices within the storage systems.

A storage system typically comprises one or more storage devices where information may be stored and from where information may be retrieved as desired. The storage system may be implemented in accordance with a variety of storage architectures including but not limited to a network attached storage NAS environment a storage area network SAN and a storage device assembly directly attached to a client or host computer.

The storage system typically includes a storage operating system that may implement a high level module such as a file system to logically organize information stored at storage volumes as a hierarchical structure of data containers such as files and logical units. For example stored files may be implemented as set of data structures i.e. storage device blocks configured to store information such as the actual data for the file. These data blocks are organized within a volume block number vbn space that is maintained by the file system. The file system typically organizes the data blocks within the vbn space as a logical volume each logical volume may be although is not necessarily associated with its own file system.

The storage system may be configured to operate according to a client server model of information delivery to thereby allow many clients to access data containers stored on the system. In this model the client may comprise an application such as a database application executing in a computer that communicates with the storage system. Each client may send input output I O requests to read and write data containers.

A storage volume is commonly replicated at a source storage array and then transferred to a destination storage array. Transferring the replicated information can negatively affect processing of client I O requests for reading and writing information at the storage arrays because during the transfer process the storage arrays have to be accessed and may not be fully available for processing client I O requests. Continuous efforts are being made to optimally provide redundancy where replicated information may be stored at more than one location without undesirably impacting the processing of read and write requests.

In one embodiment a machine implemented method and system for transferring replicated information from a first storage location managed by a storage operating system at a first storage system node and accessible to a client computing system to a second storage location managed by a second storage system node are provided. A resource pool having a plurality of tokens is maintained for authorizing a replication engine to transfer replicated information from the first storage location to the second storage location. The number of available tokens is increased by a first increment value when traffic due to client requests for accessing the first storage location is less than a first threshold level.

The number of available tokens may also be increased by a second increment value when traffic due to client requests reaches the first threshold value but the latency in processing the client requests is less than a second threshold value. The number of available tokens is decreased for reducing transfer of information via the replication engine when latency in responding to the client requests has reached the second threshold value and the traffic due to client requests has reached the first threshold value.

In another embodiment a machine implemented method for transferring replicated information from a first storage location managed by a storage operating system at a first storage system node and accessible to a client computing system to a second storage location managed by a second storage system node is provided. The method includes maintaining a resource pool having a plurality of tokens for authorizing a replication engine for transferring replicated information from the first storage location to the second storage location and increasing a number of available tokens for enabling the replication engine to increase transfer of information when traffic due to client requests for accessing the first storage location is less than a first threshold level.

The method further includes decreasing the number of available tokens for reducing transfer of information via the replication engine when latency in responding to the client requests has reached a second threshold value and the traffic due to client requests has reached the first threshold value.

In yet another embodiment a machine implemented method for transferring replicated information from a first storage location managed by a storage operating system at a storage system node and accessible to a client computing system to a second storage location is provided. The method includes maintaining a resource pool having a plurality of tokens for authorizing a replication engine for transferring replicated information from the first storage location to the second storage location and increasing a number of available tokens by a first increment value for enabling the replication engine to increase transfer of information when traffic due to client requests for accessing the first storage location is less than a first threshold level.

The method further includes increasing the number of available tokens by a second increment value for transfer of information via the replication engine when latency in responding to the client requests is less than a second threshold value and the traffic due to client requests has reached the first threshold value and decreasing the number of available tokens for reducing transfer of information via the replication engine when latency in responding to the client requests has reached a second threshold value and the traffic due to client requests has reached the first threshold value.

This brief summary has been provided so that the nature of this disclosure may be understood quickly. A more complete understanding of the disclosure can be obtained by reference to the following detailed description of the various embodiments thereof in connection with the attached drawings.

As preliminary note the terms component module system and the like as used herein are intended to refer to a computer related entity either software executing general purpose processor hardware firmware and a combination thereof. For example a component may be but is not limited to being a process running on a processor a processor an object an executable a thread of execution a program and or a computer.

By way of illustration both an application running on a server and the server can be a component. One or more components may reside within a process and or thread of execution and a component may be localized on one computer and or distributed between two or more computers. Also these components can execute from various computer readable media having various data structures stored thereon. The components may communicate via local and or remote processes such as in accordance with a signal having one or more data packets e.g. data from one component interacting with another component in a local system distributed system and or across a network such as the Internet with other systems via the signal .

Computer executable components can be stored for example on non transitory computer readable media including but not limited to an ASIC application specific integrated circuit CD compact disc DVD digital video disk ROM read only memory floppy disk hard disk EEPROM electrically erasable programmable read only memory memory stick or any other storage device in accordance with the claimed subject matter.

In one embodiment a machine implemented method and system for transferring replicated information from a first storage location managed by a storage operating system at a first storage system node and accessible to a client computing system to a second storage location managed by a second storage system node are provided. A resource pool having a plurality of tokens is maintained for authorizing a replication engine to transfer replicated information from the first storage location to the second storage location. The number of available tokens is increased by a first increment value when traffic due to client requests for accessing the first storage location is less than a first threshold level. The number of available tokens may also be increased by a second increment value when traffic due to client requests reaches the first threshold value but the latency in processing the client requests is less than a second threshold value. As an example the second increment value may be less than the first increment value. The number of available tokens is decreased for reducing transfer of information via the replication engine when latency in responding to the client requests reaches the second threshold value and the traffic due to client requests has reached the first threshold value.

To facilitate an understanding of the various embodiments of the present disclosure the general architecture and operation of a networked clustered storage system will now be described.

Clustered System is a schematic block diagram of a plurality of storage system nodes interconnected as a cluster and configured to provide storage services related to organization of information at a plurality of storage devices . Nodes interface with various client computing systems for reading and writing information stored at storage devices managed by the nodes .

Nodes comprise various functional components that cooperate to provide distributed storage system architecture of cluster . Each node is generally organized as a network element N module and a storage device element D module . N module includes functionality that enables node to connect to client computing systems over a network connection while each D module connects to one or more storage devices such as or a storage array having a plurality of storage devices . Illustratively network may be embodied as an Ethernet network a Fibre Channel FC network or any other network type. Nodes may be interconnected by a cluster switching fabric which in the illustrative embodiment may be embodied as a Gigabit Ethernet switch or any other interconnect type.

It should be noted that while there is shown an equal number of N and D modules in the illustrative cluster there may be differing numbers of N and or D modules in accordance with various embodiments of the present disclosure. For example there may be a plurality of N modules and or D modules interconnected in a cluster configuration that does not reflect a one to one correspondence between the N and D modules. As such the description of a node comprising one N module and one D module should be taken as illustrative only.

Clients may be general purpose computers having a plurality of components as described below in detail with respect to . These components may include a central processing unit CPU main memory I O devices and storage devices for example flash memory hard drives and others . The main memory may be coupled to the CPU via a system bus or a local memory bus. The main memory may be used to provide the CPU access to data and or program information that is stored in main memory at execution time. Typically the main memory is composed of random access memory RAM circuits. A computer system with the CPU and main memory is often referred to as a host system.

Clients may be configured to interact with a node in accordance with a client server model of information delivery. That is each client may request the services of the node and node may return the results of the services requested by the client over network . Clients may be configured to execute processor executable instructions shown as application for reading and writing information at storage devices . Such application may include a database application a financial management system an electronic mail application or any other application type.

Client may issue packets using application including file based access protocols such as the Common Internet File System CIFS protocol or the Network File System NFS protocol over the Transmission Control Protocol Internet Protocol TCP IP when accessing information in the form of certain data containers. Alternatively the client may issue packets using application including block based access protocols such as the Small Computer Systems Interface SCSI protocol encapsulated over TCP iSCSI and SCSI encapsulated over Fibre Channel FCP when accessing information in the form of other data containers such as blocks.

In one example one or both of N Module and D Module execute a plurality of layers of a storage operating system . These layers include a file system manager that keeps track of a directory structure hierarchy of the data stored in storage devices and manages read write operations i.e. executes read write operations on storage devices in response to client requests.

In cluster for storing and retrieving information it is sometimes advantageous to duplicate all or part of a file system. For example one purpose for duplicating a file system is to maintain a backup copy of the file system to protect against lost information. Another purpose for duplicating a file system is to provide replicas of the data in the file system at multiple servers to share load incurred in accessing that data.

One common way of replicating a file system is by taking snapshots without derogation of any trademark rights of NetApp Inc. which means a point in time copy of a storage file system. A snapshot is a persistent point in time PPT image of an active file system that enables quick recovery of data after data has been corrupted lost or altered. Snapshots can be created by copying data from a storage volume at each predetermined point in time to form a consistent image or virtually by using a pointer to form the image of the data. Often snapshot data is copied or mirrored from a source storage array to a destination storage array. Snapmirror technology from NetApp Inc. may be used to mirror snapshots from the source array to the destination array as described below with respect to .

In one embodiment a block replication engine BRE shown as A for node A and B for node B may be used to transfer a snapshot from source array A to destination array B via a network link which may be similar to the cluster switching fabric or the network link described above. BRE may use a generic file and or block based agnostic protocol having a collection of methods functions constituting an application programming interface API for transferring information from the source array A to the destination array B. Example of such an agnostic protocol is the SpinNP protocol available from NetApp Inc. The embodiments disclosed herein are not limited to any particular protocol standard for transferring the information from the source array A to the destination array B.

One challenge for transferring information from the source array A to the destination array B using BRE is that it may negatively impact processing of client requests for reading or writing information because BRE gets access to storage devices for transferring information. The negative impact may be due to lower data transfer to clients or delay i.e. latency in executing read and write operations. The embodiments disclosed herein provide an optimum system methodology for handling client related operations as well as performing transfer operations for transferring information from source array A to destination array B.

D Module executes a processor executable monitoring module using a data structure having a plurality of fields A H. The plurality of fields may be used to increase allocation of tokens A N to BRE or decrease allocation of tokens A N. The various fields are now described below in detail 

Field A is based on an amount of traffic that is being handled processed by node at any given time interval I that is specified by field G. Traffic in this context means requests from clients and or other nodes within the cluster . The storage operating system maintains this data since it processes all read and write requests for one or more clients . The monitoring module obtains traffic information by periodically requesting that information from storage operating system .

Field B is a measure of latency in processing client requested operations. Field B may be the queuing time a message from one of the layers of storage operating system has to wait to read or write information. This duration is indicative of the overall latency at a node to process client requests. In one embodiment monitoring module obtains this information from storage operating system that may store this information to track overall system efficiency in processing client requests.

Field C is a first threshold value T1 that may be programmed and is associated with field A. If a number of client operations is less than T1 then it indicates that client traffic is low and hence the tokens allocated to BRE from the BRE resource pool may be incremented by a certain value M1 that is indicated by field D.

Field E is a second threshold value T2 that is associated with overall latency as indicated by field B. T2 may be used as indicator of low latency which may be used to increase the number of allocated tokens by an amount M2 as provided by field F. In one embodiment M2 may be less than M1 because it assumes that client operations are in progress at any given time and by increasing M2 by a bigger value for example greater than M1 it may cause more latency or delay in processing active client requests. The values for M1 and M2 may be set based on the usage of storage system nodes by clients to store information for example based on the amount of data that clients store frequency of accessing storage system nodes frequency of replicating information from the source storage array to the destination storage array and others. Thus M1 and M2 may be fixed or configured by a storage administrator not shown managing the system of . The use of M2 and M1 values is described in more detail below.

Field H is a scaling S factor that is used to decrease the number of tokens allocated to BRE when latency is greater than T2. The scaling factor may be based on a current average latency within the interval I. As an example assume that a latency value for processing client requests is 100 millisecond then using a scaling factor of 50 the number of available tokens may be decreased by 50. In this example the scaling factor is multiplied by the latency to determine the amount by which the number of tokens is decreased.

The scaling factor may be a fixed or configurable value. The scaling factor value may be set based on the usage of storage system nodes by clients to store information for example based on the amount of data that clients store frequency of accessing storage system nodes frequency of replicating information from the source storage array to the destination storage array and others. This information is typically maintained by storage operating system .

The various fields A H or parameters are selected such that a decrease in available tokens due to high latencies is greater than an increase in available tokens when client traffic or latency is low. This allows a D Module to respond quickly to reduce latency if it suddenly becomes high. When the latency drops the increase in the number of tokens is gradual until the threshold value T2 is reached. This avoids unpredictable swings in overall performance of handling client requests.

In one embodiment two separate token pools A N may be maintained at BRE resource pool . A first token pool is maintained to read information from the source storage array A and a second token pool may be used for writing to the destination array B. The scaling mechanism described below with respect to may be the same for both read and write operations but the various parameters for example T1 T2 M1 M2 and S may be different for the read and write operations.

It is noteworthy that although the source and destination arrays in are shown as being managed by separate nodes A B the embodiments disclosed herein are equally applicable to an architecture where both the source and destination arrays are managed by the same node i.e. the source array A and the destination array B may be managed by a single node A and the data structures of and the process flow described below in detail are applicable to that architecture.

In block B the client traffic is compared to the first threshold T1. If the traffic is less than T1 then in block B the number of allocated tokens for BRE is increased by an amount M1.

If the traffic is greater than or equal to T1 or has reached T1 then in block B monitoring module determines if the latency is less than the second threshold T2. If the latency is less than T2 then in block B the number of tokens is increased by an amount M2. As described above in one embodiment M2 is less than M1. If the latency is greater than or equal to T2 i.e. has reached T2 then in block B the number of tokens is decreased by using the scaling factor S that may be fixed value or a dynamic value based on the overall storage environment as explained above.

The system and processes described herein have advantages because transfer from the source array A to the destination array B minimally affect client operations or delay client operations negatively. The frequent monitoring is self adjusting and hence automatic.

Storage System Node is a block diagram of a node that is illustratively embodied as a storage system comprising of a plurality of processors A B a memory a network adapter a cluster access adapter a storage adapter and local storage interconnected by an interconnect system referred to as bus . The local storage comprises one or more storage devices such as disks non volatile storage devices flash drives video tape optical DVD magnetic tape bubble memory electronic random access memory micro electro mechanical and any other similar media adapted to store information. The local storage may be utilized by the node to locally store configuration information e.g. in a configuration data structure .

Processors A B may be or may include one or more programmable general purpose or special purpose microprocessors digital signal processors DSPs programmable controllers application specific integrated circuits ASICs programmable logic devices PLDs or the like or a combination of such hardware based devices. The bus system may include for example a system bus a Peripheral Component Interconnect PCI bus a HyperTransport or industry standard architecture ISA bus a small computer system interface SCSI bus a universal serial bus USB or an Institute of Electrical and Electronics Engineers IEEE standard 1394 bus sometimes referred to as Firewire or any other interconnect type.

The cluster access adapter comprises a plurality of ports adapted to couple node to other nodes of cluster . In the illustrative embodiment Ethernet may be used as the clustering protocol and interconnect media although it will be apparent to those skilled in the art that other types of protocols and interconnects may be utilized within the cluster architecture described herein. In alternate embodiments where the N modules and D modules are implemented on separate storage systems or computers the cluster access adapter is utilized by the N D module for communicating with other N D modules in the cluster .

The network adapter comprises a plurality of ports adapted to couple the node to one or more clients over point to point links wide area networks virtual private networks implemented over a public network Internet or a shared local area network. The network adapter thus may comprise the mechanical electrical and signaling circuitry needed to connect the node to the network.

The storage adapter cooperates with a storage operating system executing on the node to access information requested by the clients. The information may be stored on any type of attached array of writable storage device media such as video tape optical DVD magnetic tape bubble memory electronic random access memory micro electro mechanical and any other similar media adapted to store information including data and parity information. However as illustratively described herein the information is preferably stored on the storage devices of array . The storage adapter comprises a plurality of ports having input output I O interface circuitry that couples to the storage devices over an I O interconnect arrangement such as a conventional high performance FC link topology.

It is noteworthy that although various adapters and have been shown as separate hardware based components the embodiments disclosed herein are not limited to separate components. The embodiments disclosed herein may be implemented using a converged network adapter CAN that is capable of handling both network and storage protocols for example a Fibre Channel over Ethernet FCoE adapter.

Each node is illustratively embodied as a multiple processor system executing the storage operating system that preferably implements a high level module such as a file system to logically organize the information as a hierarchical structure of named directories files and special types of files called virtual disks hereinafter generally blocks on storage devices . However it will be apparent to those of ordinary skill in the art that the node may alternatively comprise a single or more than two processor systems. Illustratively one processor A executes the functions of the N module on the node while the other processor B executes the functions of the D module .

The memory illustratively comprises storage locations that are addressable by the processors and adapters for storing programmable instructions and data structures. The processor and adapters may in turn comprise processing elements and or logic circuitry configured to execute the programmable instructions and manipulate the data structures. It will be apparent to those skilled in the art that other processing and memory means including various computer readable media may be used for storing and executing program instructions pertaining to the disclosure described herein.

The storage operating system portions of which is typically resident in memory and executed by the processing elements functionally organizes the node by inter alia invoking storage operations in support of the storage service implemented by the node and maintaining client traffic and latency information described above. An example of operating system is the DATA ONTAP Registered trademark of NetApp Inc. operating system available from NetApp Inc. that implements a Write Anywhere File Layout WAFL Registered trademark of NetApp Inc. file system. However it is expressly contemplated that any appropriate storage operating system may be enhanced for use in accordance with the inventive principles described herein. As such where the term ONTAP is employed it should be taken broadly to refer to any storage operating system that is otherwise adaptable to the teachings disclosed herein.

Storage of information on each storage array is preferably implemented as one or more storage volumes that comprise a collection of physical storage devices cooperating to define an overall logical arrangement of volume block number vbn space on the volume s . Each logical volume is generally although not necessarily associated with its own file system. The storage devices within a logical volume file system are typically organized as one or more groups wherein each group may be operated as a Redundant Array of Independent or Inexpensive Disks RAID . Details regarding storage operating system are provided below with respect to .

Operating System illustrates a generic example of operating system executed by node according to one embodiment of the present disclosure. Storage operating system interfaces with monitoring module and BRE stores client traffic information and latency information. The traffic and latency information is provided to monitoring module for executing the process steps of as described above. The BRE resource pool tokens are provided to BRE for transferring information as described above in detail.

In one example operating system may include several modules or layers executed by one or both of N Module and D Module . These layers include a file system manager that keeps track of a directory structure hierarchy of the data stored in storage devices and manages read write operations i.e. executes read write operations on storage devices in response to client requests. File system manager may also maintain information regarding client traffic and latency that are then used in the process flow of described above.

Operating system may also include a protocol layer and an associated network access layer to allow node to communicate over a network with other systems such as clients . Protocol layer may implement one or more of various higher level network protocols such as NFS CIFS Hypertext Transfer Protocol HTTP TCP IP and others.

Network access layer may include one or more drivers which implement one or more lower level protocols to communicate over the network such as Ethernet. Interactions between clients and mass storage devices are illustrated schematically as a path which illustrates the flow of data through operating system .

The operating system may also include a storage access layer and an associated storage driver layer to allow D module to communicate with a storage device. The storage access layer may implement a higher level disk storage protocol such as RAID while the storage driver layer may implement a lower level storage device access protocol such as FC or SCSI. In one embodiment the storage access layer may implement the RAID protocol such as RAID 4 or RAID DP RAID double parity for data protection provided by NetApp Inc. the assignee of the present disclosure .

The file system is illustratively a message based system that provides logical volume management capabilities for use in access to the information stored on the storage devices such as disks. The file system illustratively may implement the write anywhere file system having an on disk format representation that is block based using e.g. 4 kilobyte KB blocks and using index nodes inodes to identify data containers and data container attributes such as creation time access permissions size and block location . The file system uses data containers to store meta data describing the layout of its file system these meta data data containers include among others an inode data container. A data container handle i.e. an identifier that includes an inode number inum may be used to retrieve an inode from storage device.

Broadly stated all inodes of the write anywhere file system are organized into the inode data container. A file system fs info block specifies the layout of information in the file system and includes an inode of a data container that includes all other inodes of the file system. Each logical volume file system has an fsinfo block that is preferably stored at a fixed location within e.g. a RAID group. The inode of the inode data container may directly reference point to data blocks of the inode data container or may reference indirect blocks of the inode data container that in turn reference data blocks of the inode data container. Within each data block of the inode data container are embedded inodes each of which may reference indirect blocks that in turn reference data blocks of a data container.

Operationally a request from the client is forwarded as a packet over the computer network and onto the node where it is received at the network adapter . A network driver processes the packet and if appropriate passes it on to a network protocol and file access layer for additional processing prior to forwarding to the write anywhere file system . Here the file system generates operations to load retrieve the requested data from storage device if it is not resident in core i.e. in memory .

If the information is not in memory the file system indexes into the inode data container using the inode number inum to access an appropriate entry and retrieve a logical vbn. The file system then passes a message structure including the logical vbn to the RAID system the logical vbn is mapped to a storage device identifier and storage device block number storage device dbn and sent to an appropriate driver e.g. a SCSI driver not shown . The storage device driver accesses the dbn from the specified storage device and loads the requested data block s in memory for processing by the node. Upon completion of the request the node and operating system returns a reply to the client .

As described above processing client requests need access to storage devices . Access to storage devices may not be fully available when a replicated storage volume is transferred from a source array to a destination array. The embodiments described herein optimize the transfer process without negatively impacting processing of client requests.

It should be noted that the software path through the operating system layers described above needed to perform data storage access for a client request received at node may alternatively be implemented in hardware. That is in an alternate embodiment of the disclosure the storage access request data path may be implemented as logic circuitry embodied within a field programmable gate array FPGA or an ASIC. This type of hardware implementation increases the performance of the file service provided by node in response to a file system request issued by client .

As used herein the term storage operating system generally refers to the computer executable code operable on a computer to perform a storage function that manages data access and may in the case of a node implement data access semantics of a general purpose operating system. The storage operating system can also be implemented as a microkernel an application program operating over a general purpose operating system such as UNIX or Windows XP or as a general purpose operating system with configurable functionality which is configured for storage applications as described herein.

In addition it will be understood to those skilled in the art that the disclosure herein may apply to any type of special purpose e.g. file server filer or storage serving appliance or general purpose computer including a standalone computer or portion thereof embodied as or including a storage system. Moreover the teachings of this disclosure can be adapted to a variety of storage system architectures including but not limited to a network attached storage environment a storage area network and a disk assembly directly attached to a client or host computer. The term storage system should therefore be taken broadly to include such arrangements in addition to any subsystems configured to perform a storage function and associated with other equipment or systems. It should be noted that while this description is written in terms of a write any where file system the teachings of the present disclosure may be utilized with any suitable file system including a write in place file system.

Processing System is a high level block diagram showing an example of the architecture of a processing system at a high level in which executable instructions as described above can be implemented. The processing system can represent clients and other components. Note that certain standard and well known components which are not germane to the present disclosure are not shown in .

The processing system includes one or more processors and memory coupled to a bus system . The bus system shown in is an abstraction that represents any one or more separate physical buses and or point to point connections connected by appropriate bridges adapters and or controllers. The bus system therefore may include for example a system bus a Peripheral Component Interconnect PCI bus a HyperTransport or industry standard architecture ISA bus a small computer system interface SCSI bus a universal serial bus USB or an Institute of Electrical and Electronics Engineers IEEE standard 1394 bus sometimes referred to as Firewire .

The processors are the central processing units CPUs of the processing system and thus control its overall operation. In certain embodiments the processors accomplish this by executing programmable instructions stored in memory . A processor may be or may include one or more programmable general purpose or special purpose microprocessors digital signal processors DSPs programmable controllers application specific integrated circuits ASICs programmable logic devices PLDs or the like or a combination of such devices.

Memory represents any form of random access memory RAM read only memory ROM flash memory or the like or a combination of such devices. Memory includes the main memory of the processing system . Instructions which implements techniques introduced above may reside in and may be executed by processors from memory .

Also connected to the processors through the bus system are one or more internal mass storage devices and a network adapter . Internal mass storage devices may be or may include any conventional medium for storing large volumes of data in a non volatile manner such as one or more magnetic or optical based disks. The network adapter provides the processing system with the ability to communicate with remote devices e.g. storage servers over a network and may be for example an Ethernet adapter a FC adapter or the like. The processing system also includes one or more input output I O devices coupled to the bus system . The I O devices may include for example a display device a keyboard a mouse etc.

Cloud Computing The system and techniques described above are applicable and useful in the upcoming cloud computing environment. Cloud computing means computing capability that provides an abstraction between the computing resource and its underlying technical architecture e.g. servers storage networks enabling convenient on demand network access to a shared pool of configurable computing resources that can be rapidly provisioned and released with minimal management effort or service provider interaction. The term cloud is intended to refer to the Internet and cloud computing allows shared resources for example software and information to be available on demand like a public utility.

Typical cloud computing providers deliver common business applications online which are accessed from another web service or software like a web browser while the software and data are stored remotely on servers. The cloud computing architecture uses a layered approach for providing application services. A first layer is an application layer that is executed at client computers. In this example the application allows a client to access storage via a cloud.

After the application layer is a cloud platform and cloud infrastructure followed by a server layer that includes hardware and computer software designed for cloud specific services. Details regarding these layers are not germane to the inventive embodiments.

Thus a method and apparatus for optimizing information transfer from a source array to a destination array have been described. Note that references throughout this specification to one embodiment or an embodiment mean that a particular feature structure or characteristic described in connection with the embodiment is included in at least one embodiment of the present disclosure. Therefore it is emphasized and should be appreciated that two or more references to an embodiment or one embodiment or an alternative embodiment in various portions of this specification are not necessarily all referring to the same embodiment. Furthermore the particular features structures or characteristics being referred to may be combined as suitable in one or more embodiments of the disclosure as will be recognized by those of ordinary skill in the art.

While the present disclosure is described above with respect to what is currently considered its preferred embodiments it is to be understood that the disclosure is not limited to that described above. To the contrary the disclosure is intended to cover various modifications and equivalent arrangements within the spirit and scope of the appended claims.

