---

title: Interaction with a computing device via movement of a portion of a user interface
abstract: Computing devices, computer-readable storage media, and methods associated with human computer interaction. In embodiments, a computing device may include a display, a processor coupled with the display, a user interface engine and one or more applications to be operated on the processor. In embodiments, the user interface engine or the one or more applications may be configured to detect movement of the portable computing device indicating a direction a user of the portable computing device would like a portion of the user interface to move and cause the portion of the user interface to be moved, from a current location on the display to another location on the display, in accordance with the indicated direction. Such movement may facilitate the user to interact with the portion of the user interface via the interaction zone of the display. Other embodiments may be described and/or claimed.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09477316&OS=09477316&RS=09477316
owner: Intel Corporation
number: 09477316
owner_city: Santa Clara
owner_country: US
publication_date: 20140324
---
Embodiments of the present disclosure are related to the field of human computer interaction and in particular to interaction with a computing device via a display of the computing device.

The background description provided herein is for the purpose of generally presenting the context of the disclosure. Unless otherwise indicated herein the materials described in this section are not prior art to the claims in this application and are not admitted to be prior art by inclusion in this section.

Touch screens have revolutionized the market for portable computing devices. At the same time there is a constant struggle between increasing screen sizes of the portable computing devices and the usability of the computing devices. Screen sizes of portable computing devices are getting bigger and bigger but this additional screen size may lead to cumbersome interaction with the entire screen making it hard for consumers to control the portable computing device while maintaining a consistent grip with one or more hands. This cumbersome interaction is magnified when trying to control a portable computing device with a single hand as many consumers do. These interaction issues may provide usability drawbacks that consumers may feel they have to suffer for additional screen size of the portable computing device.

Methods storage media and computing devices for human computer interaction are described. In embodiments a computing device may include a display a processor coupled with the display one or more applications to be operated on the processor and a user interface engine to be operated on the processor. The user interface engine or the one or more applications may be operated on the processor to detect movement of the portable computing device indicating a direction the user would like a portion of the user interface to move. In embodiments the user interface engine or the one or more applications may cause the portion of the user interface to be moved from a current location on the display to another location on the display in accordance with the indicated direction. Such movement may facilitate the user of the portable computing device to interact with the portion of the user interface via the display. For example the portable computing device may be a smart phone. The portion of the user interface may be an icon of the user interface that a user may wish to activate. In some embodiments the user may move or rotate the smart phone in a direction to indicate the direction the user would like the user interface to move and in response the user interface engine may move the icon along with the rest of the user interface in some embodiments in the indicated direction to facilitate the user in activating the icon.

In the following detailed description reference is made to the accompanying drawings which form a part hereof wherein like numerals designate like parts throughout and in which is shown by way of illustration embodiments that may be practiced. It is to be understood that other embodiments may be utilized and structural or logical changes may be made without departing from the scope of the present disclosure. Therefore the following detailed description is not to be taken in a limiting sense and the scope of embodiments is defined by the appended claims and their equivalents.

Various operations may be described as multiple discrete actions or operations in turn in a manner that is most helpful in understanding the claimed subject matter. However the order of description should not be construed as to imply that these operations are necessarily order dependent. In particular these operations may not be performed in the order of presentation. Operations described may be performed in a different order than the described embodiment. Various additional operations may be performed and or described operations may be omitted in additional embodiments.

For the purposes of the present disclosure the phrase A and or B means A B or A and B . For the purposes of the present disclosure the phrase A B and or C means A B C A and B A and C B and C or A B and C . The description may use the phrases in an embodiment or in embodiments which may each refer to one or more of the same or different embodiments. Furthermore the terms comprising including having and the like as used with respect to embodiments of the present disclosure are synonymous.

In embodiments smart phone may be configured such that a user may be able to interact with the entirety of the user interface utilizing one or more hands without the need to move the one or more hands to perform the interaction. This may be accomplished for example by configuring smart phone to detect user input indicating a direction the user would like the user interface to move or input indicating a portion of the user interface with which the user would like to interact. As depicted here at one example of input indicating a direction the user would like the user interface to move may be tilting smart phone down and to the left. Such input may be detected by for example an accelerometer gyroscope camera or any other mechanism capable of detecting movement of smart phone . Once such an input is detected smart phone may be configured to utilize the input to determine a direction indicated by arrow in which the user would like the user interface to move for interaction with the portion of the user interface. As depicted here tilting smart phone down and to the left may be utilized by the user to effectively indicate that the user would like the user interface to move down and to the left. Such an effect may be described as a waterfall effect where the user may move smart phone such that a portion of the user interface with which the user would like to interact may fall in the direction indicated by the input at to effectively bring that portion of the user interface closer to an area of the screen with which the user may more easily interact. This may facilitate the user in interacting with the entirety of the user interface via an interactive display even where the user may not be able to reach the entirety of the display. Once the portion of the user interface the user would like to interact with is within the user s reach e.g. by the user s thumb when operating smart phone with a single hand the user may interact with that portion. Once the portion of the user interface has moved far enough in the indicated direction for the user to interact with the portion of the user interface the user may activate icon to initiate a calendar application associated with icon at . The user may then utilize the same process described above to interact with the user interface of the calendar application depicted.

While interactions are depicted and described above as being movement of smart phone to indicate a direction the user would like a portion of the user interface to move the particular interactions of are used for illustrative purposes. In other embodiments the user may be able to interact with the display of smart phone to indicate a direction the user would like a portion of the user interface to move. For example the user may utilize the user s thumb on an area of the display to drag a portion of the user interface into an area of the display with which the user may be able to interact more easily. Furthermore the movement of smart phone may vary from that described above. For example the user may be able to shake rotate tilt or otherwise move smart phone to indicate a direction the user would like a portion of the user interface to move.

As depicted smart phone may be tilted in a direction such as downward to indicate that the user wishes to move a portion of the user interface down into interaction zone to interact with portion via the display. In response smart phone may be configured to move portion of the user interface into interaction zone as indicated by arrow . This may be accomplished in some embodiments by moving the entirety of the user interface down such that the portion of the user interface with which the user would like to interact moves into interaction zone . In such embodiments as the entire interface moves a section of the interface may leave the display. In some embodiments such as that depicted in above as the section of the user interface leaves the display the section of the user interface disappears until the user has completed the interaction with the portion of the user interface through the interaction zone after which the user interface may return to an original or home position. In other embodiments such as that depicted in by section of the user interface as a section of the user interface passes off of an edge of the display that section may reemerge on an opposite edge of the display effectively looping the user interface on the display. While depicted in as being utilized in an embodiment with interaction zone such looping may be utilized in embodiments without a defined interaction zone such as that described above in reference to . In embodiments where the screen may loop rather than moving the user interface back to the original location smart phone may be configured to merely continue the loop to achieve the same effect. While the user interface movement of is depicted as moving the user interface of an application without moving drop down notification bar this is merely for illustrative purposes only. In some embodiments not depicted movement of the user interface may include the entirety of the user interface including but not limited to drop down notification bar scroll bars not depicted or anything else rendered on the display.

In some embodiments the user may not need to move smart phone or interact with the display to achieve movement of the portion of the user interface. For instance smart phone may be configured to detect an audible command given by the user e.g. via a microphone not depicted of smart phone . The audible command may describe a portion of the user interface such as a section of the user interface or specific element of the user interface with which the user would like to interact. In response to receiving such a command smart phone may be configured to move the described portion of the user interface to interaction zone to facilitate the user s interaction with the user interface. In other instances smart phone may be configured to detect a portion of the user interface with which the user would like to interact by tracking eye movement of the user utilizing a camera not depicted integrated with smart phone . In such embodiments the user may be able to look at a portion of the user interface with which the user would like to interact and that portion of the user interface may be moved into interaction zone by smart phone to facilitate the user s interaction with that portion of the user interface.

In block input indicating a direction the user would like a portion of the user interface to move may be detected e.g. by a user interface engine of the portable computing device. Such input may in some embodiments include moving e.g. tilting and or rotating the portable computing device. For example the user may position the portable computing device such that a desired portion of the user interface may fall into an area of the display with which the user may more easily interact. In other embodiments a user may utilize a finger on the display to drag the user interface in a desired direction until the portion of the user interface the user wishes to interact with is within reach. The above described inputs are merely meant to be illustrative any input by a user that may indicate a direction the user would like a portion of the user interface to move to facilitate the user s interaction with that portion may be utilized without departing from the scope of this disclosure.

Once the direction the user would like a portion of the user interface to move has been determined the portion of the user interface may be moved in the indicated direction at block e.g. by a user interface engine of the portable computing device. This may be accomplished for example by moving the entirety of the user interface such that the portion of the user interface moves closer to an area the user is able to more easily reach on the display. In some embodiments this may appear to the user as a jump from one location of the screen to another. In other embodiments it may appear to the user to be a sliding of the user interface. In such embodiments a speed of the sliding may be determined based upon a degree of tilt and or rotation of the portable computing device for example. In such embodiments as the entire interface moves a section of the interface may leave the display. In some embodiments such as that depicted in above as the section of the user interface leaves the display the section of the display may disappear until the user has completed the interaction with the portion of the user interface after which the user interface may return to an original or home position. In other embodiments such as that depicted in above as a section of the user interface passes off of an edge of the display that section may reemerge on an opposite edge of the display essentially looping the user interface on the display. In such embodiments rather than moving the user interface back to the original location the user interface may merely continue the loop to achieve the same effect.

In block the portion of the user interface may be returned to a home location e.g. by a user interface engine of the portable computing device. As used herein the home location may be the original location of the portion of the user interface prior to the movement of block . In some embodiments the procedure of block may occur once the user has completed interaction with the portion of the user interface. The completion of the interaction may in some embodiments be determined by the user s activation of an element of the user interface e.g. selection of an icon to initiate an associated application. In other embodiments the completion of the interaction may be based upon a time period of inactivity or any other suitable action or inaction.

In some circumstances a user may change the user s mind about the direction the user would like a portion of the user interface to move. In such circumstances the user may perform an action such as adjusting the tilt or rotation to move the user interface in another direction or shaking the portable computing device for example to indicate a desire to discontinue the current action.

In block input indicating a portion of the user interface with which the user wishes to interact may be detected e.g. by a user interface engine of the portable computing device. Such input may in some embodiments include tilting and or rotating the portable computing device. In some embodiments portions of the user interface may be associated with different degrees of tilt and or rotation of the portable computing device such that a change in the degree of the tilt and or rotation may indicate a different portion of the user interface. For example a display may be divided into a number of segments and successive rotations from a reference point e.g. rotations of 10 degrees may cause the portion of the user interface with which the user wishes to interact to change from one segment to the next. In other embodiments a user may utilize a finger on the display or an interaction zone of the display to drag the user interface until the portion of the user interface the user wishes to interact with is within the interaction zone. In still another embodiment sections of the interaction zone may correlate with sections of the user interface and a touch of a specific section of the interaction zone may indicate that the user wishes to interact with the correlated section of the user interface. For example if a user wishes to interact with a top right portion of the user interface then the user may touch the top right portion of the interaction zone to indicate the portion of the user interface with which the user would like to interact. The above described inputs are merely meant to be illustrative inputs any input by a user that may indicate a portion of the user interface with which the user would like to interact may be utilized without departing from the scope of this disclosure. Furthermore the portion of the user interface may be an area of the user interface or even a specific element or icon of the user interface.

In block an interaction zone may be determined e.g. by a user interface engine of the portable computing device. In some embodiments the interaction zone may be determined based at least in part upon an orientation of the portable computing device. For example if the portable computing device is in a portrait orientation then one interaction zone may be determined while in a landscape orientation another interaction zone may be determined. In other embodiments the interaction zone may be determined based upon a position of one or more of the user s hands on the portable computing device. For example if a user is holding the portable computing device in the lower right hand corner then the interaction zone may be determined to also be located in the lower right hand corner. In still other embodiments the interaction zone may be determined based upon a combination of the orientation of the portable computing device and a position of one or more of the user s hands on the portable computing device. For example if the portable computing device is being held in a landscape position and one of the user s hands is located in the lower right corner of the portable computing device then a corresponding interaction zone may be determined. In still other embodiments the user may tap an area of the screen to identify the location that the user desires to be the interaction zone.

In further embodiments more than one interaction zone may be determined. For example if the portable computing device is operated with one hand on either side of the portable computing device then an interaction zone may be determined based upon the location of each hand. In such embodiments another determination may be made based upon the portion of the user interface that has been determined above and or the manner of the selection. For example in an embodiment where the user positions the computing device for a portion of the user interface to fall into an interaction zone then an interaction zone in the path of the fall may be selected over another interaction zone where more than one interaction zone has been identified. In another example where a user may utilize a finger on the display or the interaction zone to drag the user interface until the portion of the user interface the user wishes to interact appears in the interaction zone the direction of the dragging may determine the interaction zone the user wishes to utilize in interacting with the portion of the user interface.

In some of these embodiments the interaction zone may be selected from a number of possible interaction zones based upon the above described determination processes or any other suitable determination process. These possible interaction zones may be predefined in the device by the manufacturer of the device or software firmware of the device by the software firmware vendor. In other embodiments the user of the device may define one or more interaction zones from which to select along with criteria for the defined interaction zone s selection where more than one interaction zone has been defined. For example a user may define an interaction zone and may associate that defined interaction zone with an orientation and or hand position for when that interaction zone may be selected. In some embodiments a single interaction zone may be predefined and in such embodiments the determination of the interaction zone may be omitted. While depicted here as occurring after block in some embodiments the procedure described above for block may occur before that described for block above.

Once the portion of the user interface the user wishes to interact with and the interaction zone have been determined the portion of the user interface may be moved to the interaction zone at block e.g. by a user interface engine of the portable computing device. This may be accomplished as described above in reference to by moving the entirety of the user interface such that the portion of the user interface may align with or be encompassed by the determined interaction zone. In some embodiments this may appear to the user as a jump from one location of the screen to another. In other embodiments it may appear to the user to be a sliding of the user interface. In such embodiments a speed of the sliding may be determined based upon a degree of tilt and or rotation of the portable computing device for example. In some embodiments the portion of the user interface may separate from a reminder of the user interface and move to the interaction zone. In such embodiments only the portion of the user interface would move rather than the entirety of the user interface leaving the remainder of the user interface in the original location. In further embodiments the entirety of the user interface may be moved such that the portion of the user interface with which the user would like to interact moves into the interaction zone. In such embodiments as the entire interface moves a section of the interface may leave the display. In some embodiments such as that depicted in above as the section of the user interface leaves the display the section of the display may disappear until the user has completed the interaction with the portion of the screen through the interaction zone after which the user interface may return to an original or home position. In other embodiments such as that depicted in above as a section of the user interface passes off of an edge of the display that section may reemerge on an opposite edge of the display essentially looping the user interface on the display. In such embodiments rather than moving the user interface back to the original location the user interface may merely continue the loop to achieve the same effect.

In block the portion of the user interface may be returned to a home location e.g. by a user interface engine of the portable computing device. As used herein the home location may be the original location of the portion of the user interface prior to the movement of block . In some embodiments the procedure of block may occur once the user has completed interaction with the portion of the screen through the interaction zone. The completion of the interaction may in some embodiments be determined by the user s activation of an element of the user interface e.g. selection of an icon to initiate an associated application. In other embodiments the completion of the interaction may be based upon a time period of inactivity or any other suitable action or inaction.

In some circumstances a user may change the user s mind about the portion of the user interface the user wishes to interact with or the portion may be incorrectly determined. In such circumstances the user may perform an action such as shaking the portable computing device to indicate a desire to discontinue the current action. Once such an action is detected the portion of the user interface may return to its home location if the movement described in reference to block has begun.

Portable computing device may comprise processor s display Sensors Storage containing user interface engine and other input output I O devices . Processor s display sensors storage and other input output I O devices may all be coupled together utilizing system bus .

Processor s may be comprised of a single processor or multiple processors. In multiple processor embodiments the multiple processors may be of the same type i.e. homogeneous or may be of differing types i.e. heterogeneous and may include any type of single or multi core processors. This disclosure is equally applicable regardless of type and or number of processors.

Display may be any type of display including a liquid crystal diode LCD an organic light emitting diode OLED a plasma display or any other similar display. In embodiments display may be an interactive display such as a touch sensitive display. Display may be incorporated into computing device or may be peripherally connected to computing device through any type of wired and or wireless connection. This disclosure is equally applicable regardless of the type of display.

Sensors may include in some embodiments sensors such as a camera accelerometer gyroscope pressure sensors etc. These sensors may enable computing device to carry out one or more of the processes described above in reference to . It will be appreciated that the sensors mentioned above are merely examples that are meant to be illustrative.

In embodiments storage may be any type of computer readable storage medium or any combination of differing types of computer readable storage media. Storage may include volatile and non volatile persistent storage. Volatile storage may include e.g. dynamic random access memory DRAM . Non volatile persistent storage may include but is not limited to a solid state drive SSD a magnetic or optical disk hard drive flash memory or any multiple or combination thereof.

In embodiments user interface engine may be implemented as software firmware or any combination thereof. In some embodiments user interface engine may comprise one or more instructions that when executed by processor s cause computing device to perform one or more operations of any process described herein. In embodiments user interface engine may be configured to receive data from sensors . In some embodiments user interface engine may be configured to monitor a stream of data produced by sensors . In other embodiments user interface engine may be configured to periodically receive portions of data from sensors for analysis. In some embodiments user interface engine may be configured to determine a direction the user would like a portion of the user interface to move or determine a portion of the user interface with which the user would like to interact as described above but may output these determinations to another application or an operating system OS of the portable computing device to have the actual movement of the user interface accomplished. This may be accomplished in some embodiments through an application programming interface API integrated into user interface engine .

In embodiments user interface engine may be configured to analyze the data received from sensors . In some embodiments the data may be analyzed to determine a position of a user s hand s and or finger s on computing device . In some embodiments the data may be analyzed to determine if a pre determined action occurs within the sensor data such as those actions described in reference to . This may be accomplished for example by comparing data captured by one or more of the sensors with a database of predefined actions.

For the purposes of this description a computer usable or computer readable medium can be any medium that can contain store communicate propagate or transport the program for use by or in connection with the instruction execution system apparatus or device. The medium can be an electronic magnetic optical electromagnetic infrared or semiconductor system or apparatus or device or a propagation medium. Examples of a computer readable storage medium include a semiconductor or solid state memory magnetic tape a removable computer diskette a random access memory RAM a read only memory ROM a rigid magnetic disk and an optical disk. Current examples of optical disks include compact disk read only memory CD ROM compact disk read write CD R W and DVD.

Embodiments of the disclosure can take the form of an entirely hardware embodiment an entirely software embodiment or an embodiment containing both hardware and software elements. In various embodiments software may include but is not limited to firmware resident software microcode and the like. Furthermore the disclosure can take the form of a computer program product accessible from a computer usable or computer readable medium providing program code for use by or in connection with a computer or any instruction execution system.

Although specific embodiments have been illustrated and described herein it will be appreciated by those of ordinary skill in the art that a wide variety of alternate and or equivalent implementations may be substituted for the specific embodiments shown and described without departing from the scope of the embodiments of the disclosure. This application is intended to cover any adaptations or variations of the embodiments discussed herein. Therefore it is manifestly intended that the embodiments of the disclosure be limited only by the claims and the equivalents thereof.

Example 1 is a portable computing device to facilitate user interaction with a user interface comprising a display a processor coupled with the display one or more applications operated on the processor and a user interface engine. The user interface engine is to be operated on the processor to render a user interface of the one or more applications on the display. The user interface engine or the one or more applications are further operated by the processor to detect movement of the portable computing device indicating a direction a user of the portable computing device would like a portion of the user interface to move and cause the portion of the user interface to be moved from a current location on the display to another location on the display in accordance with the indicated direction to facilitate the user to interact with the portion of the user interface.

Example 2 may include the subject matter of Example 1 wherein the user interface engine or the one or more applications are further to be operated on the processor to cause the portion of the user interface to return to the current location from the another location upon completion of the user s interaction with the portion of the user interface.

Example 3 may include the subject matter of Example 1 wherein movement of the portable computing device comprises at least one of tilt or rotation of the portable computing device.

Example 4 may include the subject matter of Example 3 wherein the portable computing device further comprises a movement sensor configured to detect and output data concerning movement of the portable computing device and wherein detection of movement of the portable computing device is based at least in part on the data output by the movement sensor.

Example 5 may include the subject matter of Example 4 wherein the movement sensor comprises at least one of an accelerometer or a gyroscope.

Example 6 may include the subject matter of any one of Examples 1 5 wherein the another location comprises an interaction zone of the display.

Example 7 may include the subject matter of Example 6 wherein the interaction zone comprises a fixed area of the display.

Example 8 may include the subject matter of Example 6 wherein the user interface engine is further operated on the processor to determine the interaction zone.

Example 9 may include the subject matter of Example 8 wherein determination of the interaction zone comprises selection of the interaction zone from one or more areas of the display based upon an orientation of the portable computing device.

Example 10 may include the subject matter of Example 9 wherein the one or more areas of the display are predefined by the user of the portable computing device.

Example 11 may include the subject matter of Example 10 wherein the one or more areas of the display predefined by the user enable the user to interact with the entire user interface via the interaction zone with one hand while holding the portable computing device with the one hand.

Example 12 may include the subject matter of Example 8 wherein to determine the interaction zone comprises to determine the interaction zone based on movement of the portable computing device.

Example 13 may include the subject matter of Example 8 wherein to determine the interaction zone comprises determination of an area of the display of the portable computing device with which the user may interact given a current position of one or more of the user s hands on the portable computing device.

Example 14 may include the subject matter of Example 13 wherein to determine the interaction zone comprises to determine a position of one or more of the user s hands on the portable computing device and to determine the interaction zone based on the determined position.

Example 15 may include the subject matter of any one of Examples 1 5 wherein the portion of the user interface is an interactive element of the user interface.

Example 16 may include the subject matter of any one of Examples 1 5 wherein to detect movement of the portable computing device is further to only detect movement of the portable computing device on detection of a predefined scroll activation action.

Example 17 may include the subject matter of any one of Examples 1 5 wherein to cause the portion of the user interface to move from the current location to the another location is to scroll the entire user interface to move the portion of the user interface from the current location to the another location.

Example 18 may include the subject matter of any one of Examples 1 5 wherein the portable computing device is one of a smart phone tablet personal digital assistant PDA tablet laptop hybrid media player or gaming device.

Example 19 is a computer implemented method for facilitating user interaction with a user interface comprising detecting by a user interface engine of a portable computing device movement of the portable computing device indicating a direction a user of the portable computing device would like a portion of the user interface to move and causing by the user interface engine the portion of the user interface to be moved from a current location on the display to another location on the display in accordance with the indicated direction to facilitate the user of the portable computing device in interacting with the portion of the user interface.

Example 20 may include the subject matter of Example 19 wherein causing the portion of the user interface to be moved in accordance with the indicated direction comprises outputting by the user interface engine of an indicator of the indicated direction to an operating system OS or an application of the computing device for use by the OS or the application to move the portion of the user interface in accordance with the indicated direction.

Example 21 may include the subject matter of Example 20 further comprising returning by the user interface engine the portion of the user interface to the current location from the another location upon completion of the user s interaction with the portion of the user interface.

Example 22 may include the subject matter of Example 19 wherein movement of the portable computing device comprises at least one of tilt or rotation of the portable computing device.

Example 23 may include the subject matter of Example 22 wherein detecting movement of the portable computing device is based at least in part on data output by an accelerometer or a gyroscope of the portable computing device.

Example 24 may include the subject matter of any one of Examples 19 23 wherein the another location comprises an interaction zone of the display.

Example 25 may include the subject matter of Example 24 wherein the interaction zone comprises a fixed area of the display.

Example 26 may include the subject matter of Example 24 further comprising determining by the user interface engine the interaction zone.

Example 27 may include the subject matter of Example 26 wherein determining the interaction zone comprises selecting the interaction zone from one or more areas of the display based upon an orientation of the portable computing device.

Example 28 may include the subject matter of Example 27 wherein the one or more areas of the display are predefined by the user of the portable computing device.

Example 29 may include the subject matter of Example 27 wherein the one or more areas of the display predefined by the user enable the user to interact with the entire user interface via the interaction zone with one hand while holding the portable computing device with the one hand.

Example 30 may include the subject matter of Example 26 wherein determining the interaction zone is based at least in part on movement or rotation of the portable computing device.

Example 31 may include the subject matter of Example 26 wherein determining the interaction zone comprises determining an area of the display of the portable computing device with which the user may interact given a current position of one or more of the user s hands on the portable computing device.

Example 32 may include the subject matter of Example 31 wherein determining the interaction zone comprises determining a position of one or more of the user s hands on the portable computing device and determining the interaction zone based on the determined position.

Example 33 may include the subject matter of any one of Examples 19 23 wherein the portion of the user interface is an interactive element of the user interface.

Example 34 may include the subject matter of any one of Examples 19 23 wherein detecting movement of the portable computing device comprises only detecting movement of the portable computing device on detecting a predefined scroll activation action.

Example 35 may include the subject matter of any one of Examples 19 23 wherein causing the portion of the user interface to move from the current location to the another location comprises scrolling the entire user interface to move the portion of the user interface from the current location to the another location.

Example 36 is at least one computer readable storage medium having instructions to facilitate user interaction with a portion of a user interface stored thereon which when executed by a portable computing device cause the portable computing device to carry out the method of any one of Examples 19 35.

Example 37 is an apparatus to facilitate user interaction with a user interface comprising means for detecting movement of the computing apparatus indicating a direction the user of the computing apparatus would like a portion of the user interface to move and means for causing the portion of the user interface to be moved from a current location on a display to another location on the display in accordance with the indicated direction to facilitate the user of the apparatus in interacting with the portion of the user interface.

Example 38 may include the subject matter of Example 37 wherein means for causing the portion of the user interface to be moved in accordance with the indicated direction further comprises means for outputting of an indicator of the indicated direction to an operating system OS or an application of the computing device for use by the OS or the application to move the portion of the user interface in accordance with the indicated direction.

Example 39 may include the subject matter of Example 38 further comprising means for returning by the user interface engine the portion of the user interface to the current location from the another location upon completion of the user s interaction with the portion of the user interface.

Example 40 may include the subject matter of Example 37 wherein movement of the apparatus comprises at least one of tilt or rotation of the apparatus.

Example 41 may include the subject matter of Example 40 wherein detecting movement of the apparatus is based at least in part on data output by an accelerometer or a gyroscope of the apparatus.

Example 42 may include the subject matter of any one of Examples 37 41 wherein the another location comprises an interaction zone of the display.

Example 43 may include the subject matter of Example 42 wherein the interaction zone comprises a fixed area of the display.

Example 44 may include the subject matter of Example 42 further comprising means for determining the interaction zone.

Example 45 may include the subject matter of Example 44 wherein means for determining the interaction zone comprises means for selecting the interaction zone from one or more areas of the display based upon an orientation of the apparatus.

Example 46 may include the subject matter of Example 45 wherein the one or more areas of the display are predefined by the user of the apparatus.

Example 47 may include the subject matter of Example 45 wherein the one or more areas of the display predefined by the user enable the user to interact with the entire user interface via the interaction zone with one hand while holding the apparatus with the one hand.

Example 48 may include the subject matter of Example 44 wherein determining the interaction zone is based at least in part on movement or rotation of the apparatus.

Example 49 may include the subject matter of Example 44 wherein determining the interaction zone comprises means for determining an area of the display of the apparatus with which the user may interact given a current position of one or more of the user s hands on the apparatus.

Example 50 may include the subject matter of Example 49 wherein determining the interaction zone comprises means for determining a position of one or more of the user s hands on the apparatus and means for determining the interaction zone based on the determined position.

Example 51 may include the subject matter of any one of Examples 37 41 wherein the portion of the user interface is an interactive element of the user interface.

Example 52 may include the subject matter of any one of Examples 37 41 further comprising means for detecting a predefined scroll activation action and wherein detecting movement of the apparatus comprises only detecting movement of the apparatus on detecting the predefined scroll activation action.

Example 53 may include the subject matter of any one of Examples 37 41 wherein means for causing the portion of the user interface to move from the current location to the another location comprises means for scrolling the entire user interface to move the portion of the user interface from the current location to the another location.

Example 54 is at least one computer readable storage medium having instructions stored thereon which when executed by a portable computing device cause the portable computing device to detect movement of the portable computing device indicating a direction a user of the portable computing device would like a portion of the user interface to move and cause the portion of the user interface to be moved from a current location on the display to another location on the display in accordance with the indicated direction to facilitate the user of the portable computing device in interacting with the portion of the user interface.

Example 55 may include the subject matter of Example 54 wherein to cause the portion of the user interface to be moved in accordance with the indicated direction comprises output of an indicator of the indicated direction to an operating system OS or an application of the computing device for use by the OS or the application to move the portion of the user interface in accordance with the indicated direction.

Example 56 may include the subject matter of Example 55 wherein the instructions when executed by the portable computing device further cause the portable computing device to return the portion of the user interface to the current location from the another location upon completion of the user s interaction with the portion of the user interface.

Example 57 may include the subject matter of Example 54 wherein movement of the portable computing device comprises at least one of tilt or rotation of the portable computing device.

Example 58 may include the subject matter of Example 57 wherein to detect movement of the portable computing device is based at least in part on data output by an accelerometer or a gyroscope of the portable computing device.

Example 59 may include the subject matter of any one of Examples 54 58 wherein the another location comprises an interaction zone of the display.

Example 60 may include the subject matter of Example 59 wherein the interaction zone comprises a fixed area of the display.

Example 61 may include the subject matter of Example 59 further comprising determining by the user interface engine the interaction zone.

Example 62 may include the subject matter of Example 61 wherein to determine the interaction zone comprises selecting the interaction zone from one or more areas of the display based upon an orientation of the portable computing device.

Example 63 may include the subject matter of Example 62 wherein the one or more areas of the display are predefined by the user of the portable computing device.

Example 64 may include the subject matter of Example 62 wherein the one or more areas of the display predefined by the user enable the user to interact with the entire user interface via the interaction zone with one hand while holding the portable computing device with the one hand.

Example 65 may include the subject matter of Example 61 wherein to determine the interaction zone is based at least in part on movement or rotation of the portable computing device.

Example 66 may include the subject matter of Example 61 wherein to determine the interaction zone is based at least in part on a current position of one or more of the user s hands on the portable computing device.

Example 67 may include the subject matter of Example 66 wherein to determine the interaction zone comprises determination of a position of one or more of the user s hands on the portable computing device and determination of the interaction zone based on the determined position.

Example 68 may include the subject matter of any one of Examples 54 58 wherein the portion of the user interface is an interactive element of the user interface.

Example 69 may include the subject matter of any one of Examples 54 58 wherein the instructions when executed by the portable computing device further cause the portable computing device to detect a predefined scroll activation action and wherein to detect movement of the portable computing device is based on the detection of the predefined scroll activation action.

Example 70 may include the subject matter of any one of Examples 54 58 wherein to move the portion of the user interface is to scroll the entire user interface to move the portion of the user interface from the current location to the another location.

