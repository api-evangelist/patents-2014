---

title: Overlapping write detection and processing for sync replication
abstract: A primary write request that is to modify a primary portion of primary data stored in a primary storage node is received. The primary write request is to be replicated to create a current secondary write request. The current secondary write request is to modify a current secondary portion of secondary data that is stored in a secondary storage node. A current data range of the current secondary portion is determined. A determination is made of whether a previous secondary write request is in process of modifying a previous data range that at least partially overlaps with a current data range of the current secondary portion. Execution of the primary write request is suspended, until the previous secondary write request has completed updating the secondary storage node.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09645753&OS=09645753&RS=09645753
owner: NetApp, Inc.
number: 09645753
owner_city: Sunnyvale
owner_country: US
publication_date: 20140829
---
Aspects of this disclosure generally relate to the field of distributed storage and more particularly to syncing data that is replicated across at least two storage nodes in a distributed storage system.

Whether maintaining customer data or their own data businesses demand always available or highly available data and protection of that data. To support these demands data often resides across multiple storage systems in multiple sites that are often great distances apart. One of the reasons these sites are great distances apart is to avoid a single catastrophe impacting data availability. Metrics used to define the availability requirements include recovery point objective RPO and recovery time objective RTO . A business specifies an RTO as the maximum amount of time that the business tolerates lack of access to the business data. A business specifies an RPO as the amount of data in terms of time that can be lost due to an interruption. For instance a business can specify an RTO as 15 seconds. In other words the business will accept at most 15 seconds from the time of a service interruption or failure to the time their data is again available. For an RPO a business can specify 5 seconds. That means that the business will not accept losing any more than the data written e.g. new writes updates etc. in the 5 seconds that precede a failure or interruption.

Storage features to support the availability and protection demands of businesses across storage systems have been given various names such as snapshotting mirroring cloning and replicating. Each of these storage features can also vary by the provider of the storage feature and or storage product. Despite the variations each storage feature provides a consistent view of a business data.

The description that follows includes exemplary systems methods techniques instruction sequences and computer program products that embody techniques of various aspects. However it is understood that the described aspects may be practiced without these specific details. For instance although examples refer to a primary storage node and a secondary storage node being located in two different storage clusters in accordance with some aspects the primary storage node and the secondary storage node can be in a same cluster. In other instances well known instruction instances protocols structures and techniques have not been shown in detail in order not to obfuscate the description.

Synchronization sync replication includes replicating data stored in a primary storage node in a separate copy stored at a secondary storage node. This allows the primary data stored in the primary storage node to remain in sync with the secondary data stored in the secondary storage node. Therefore when a write request primary write request is received to update data stored in the primary storage node the write request is replicated to create a separate write request secondary write request that is transmitted to the secondary storage node to perform an update to the corresponding data therein. This allows the secondary data to remain in sync with the primary data. Executing the write requests in the order received at the primary storage node maintains an accurate replication of the data. In conventional systems a file system serializes the write operations to preclude multiple write operations from updating a same portion of data in the data storage at a same time. Accordingly if a first write operation is currently updating a portion of data the file system can preclude a second write operation from performing an update to the portion of data until the first write operation has completed. As further described below according to some aspects the sync replication to create the secondary write request can occur external to the file system. Because the sync replication can occur external to the file system the file system does not serialize and maintain an order of the write operations that are part of the sync replication at the secondary storage node. Some aspects therefore include overlapping write detection and processing to maintain an order of the write operations that are part of the sync replication at the secondary storage node. Thus the sync replication to create the secondary write request can occur after overlapping write detection and processing.

Accordingly some aspects provide sync replication at a primary storage node using overlapping write detection and processing based on write requests that are created and processed external to a file system. As further described below the overlapping detection and processing of the write requests ensures that the order that the write requests are received by the secondary storage node is the order in write requests are executed to update the secondary write data.

The example illustrations depicted in depict different degrees of example details as an attempt to avoid presenting an overwhelming amount of information about the systems. Every possible data structure and every possible modularization of functionality is not presented since they are numerous and not necessary to understanding aspects of the disclosure. For instance data structures presented as multiple data structures can be organized differently with a variety of possible indexing accessing schemes and arrangement of data. Similarly the functionality presented as individual modules engines units in the example illustrations can also be organized differently in accordance with any one of platform operating system and or hardware application ecosystem interfaces programmer preferences programming language etc. In addition some functionality is described later in the description also as an attempt to avoid presenting an overwhelming amount of information.

The secondary node can include all of the same modules engines as the primary node . In some of the modules are not depicted to reduce repetition. The secondary node is depicted as including a filesystem request generator a storage cluster synchronization engine a change propagation engine and a filesystem . The change propagation engine includes an interceptor and a secondary writer . The secondary writer of the secondary node accesses data depicted in as sync tracking data . The sync tracking data indicates progress or state of requests from the perspective of the secondary writer . The sync tracking data is not necessarily contained within the change propagation engine . The sync tracking data is merely depicted near the secondary writer for this description. The secondary node is communicatively coupled with hosting storage elements that host a group of logical storage objects. The group includes a secondary storage object.

Although there may be some variation in functionality across different nodes the functionality of modules having a same name will be generally the same in this illustration. The filesystem request generators generate filesystem requests based on storage protocol input output I O requests passed to the filesystem generators . The filesystem generators can receive storage protocol I O requests from a network stack a small computer system interface SCSI stack Internet SCSI iSCSI module etc. Examples of storage protocol I O requests include storage area network SAN requests and network attached storage NAS requests. The filesystem generators generate the filesystem requests based on the filesystem implemented on their node. The interceptors intercept requests from the filesystem request generators . Intercepting can be implemented differently. An application programming interface can be modified so that the underlying functionality changes without changing the interface presented to callers. As another example a monitoring process can monitor an execution queue and redirect a call when a specified address occurs in the execution queue. The filesystems access the underlying hosting storage element in accordance with filesystem requests. The storage cluster synchronization engines process communications in accordance with a protocol implemented via the network . As examples the protocols implemented by the engines can be any one or more of Fibre Channel FC Fibre Chanel over Ethernet FCoE Internet Fibre Channel protocol iFCP and a tunneling protocol. Regardless of the specific protocol the engines implement a protocol that supports an active connection that can be perceived as a direct connection between machines despite distance and hops between the machines.

At stage C the interceptor accesses sync mappings to determine any sync relationships relevant to the filesystem request. The filesystem request indicates a logical storage object in group in terms of the filesystem location information that is a target of the filesystem request. The interceptor accesses the sync mappings to determine any sync relationships defined for the filesystem request target. The target may have a single sync relationship multiple sync relationships or no sync relationships. If the target has no sync relationships then the filesystem request would be passed off to the filesystem . For this illustration the sync mappings indicate that the target has a full sync relationship with a logical storage object in the group . Since the target of the filesystem request has a sync relationship the target of the filesystem request can be considered the primary logical storage object. As stated previously the logical storage objects are identified by immutable identifiers that are exclusive at least across clusters that are associated with each other. The sync mappings which may be indicated in one or more data structures map the sync relationships across the levels or layers of the logical object or filesystem request target depending upon the logical object e.g. file LUN etc. and underlying filesystem. For example the logical object may be a file. The logical object identifier will initially be the file identifier or file handle. The filesystem resolves a write request targeting the file handle to impacted data blocks. The filesystem may resolve through any number of Mode levels for example. When there is a sync relationship the sync mappings not only map the higher level identifier i.e. the logical object identifier at the primary node to the higher level identifier at the secondary node but the sync mappings also map the lower level identifiers i.e. filesystem location information . In this example case the lower level identifiers would be the Mode identifiers. The primary node Mode identifiers for the part of the file being targeted would map to Mode identifiers on the secondary node for the part of the file being targeted.

At stage D the interceptor passes the filesystem request and an indication of the sync relationship for the target to the change propagator A. If the primary node has not yet received a change request that targets the same primary logical storage object as indicated in the change request then the interceptor may invoke code that instantiates the change propagator A. Although not necessary a change propagator is instantiated per primary logical storage object in this illustration. The interceptor can indicate the sync relationship for the primary logical storage object to the change propagator in various manners. For example the interceptor can call a function that instantiates change propagators with the primary logical storage object identifier as a parameter value and the secondary logical storage object identifier as a parameter value. As another example the interceptor can send an inter process communication to an already instantiated change propagator A along with a reference to the filesystem request stored in a local memory. To illustrate the per primary logical storage object instantiations of change propagators the change propagator N is depicted with a dashed line to the in flight tracking data . The dashed line is used to indicate that the change propagator N may be accessing the in flight tracking data for a different filesystem request.

At stage E the change propagator A creates a filesystem request targeting the secondary logical storage object of the sync relationship and updates the in flight tracking data . If the change propagator A has just been instantiated then there may not yet be a structure for tracking data or there may be an empty structure. The change propagator A updates the in flight tracking data to indicate that a filesystem request targeting the primary logical storage object is in flight i.e. will be sent or is being sent . The change propagator A updates the in flight tracking data to also indicate that a filesystem request targeting the secondary logical storage object is in flight. The change propagator A then or concurrently creates the request with an identifier of the secondary logical storage object that has a full sync relationship with the primary logical storage object. The change propagator A creates this filesystem request with a different requestor as well. The change propagator A indicates the change propagator A as the requestor. The change propagator A can be identified with various data that exclusively identifies the change propagator A within any associated clusters such as a combination of a process thread identifier of the change propagator A and a network address of the primary node . The change propagator A can also incorporate the primary logical storage object identifier into the indication of the requestor. The filesystem request targeting the primary logical storage object sent from the change propagator A will be referred to as the primary change request. The filesystem request targeting the secondary logical storage object sent from the change propagator A will be referred to as the secondary change request.

At stage F the change propagator A sends the filesystem requests for servicing. Because the primary logical storage object has a full sync relationship with the secondary logical storage object the primary node will not respond to the change request until the change has been made at both the primary and secondary logical storage objects. Therefore the change propagator A can send the primary and secondary change requests in any order. The change propagator A sends the primary change request to the filesystem . The change propagator A sends the secondary change request to the storage cluster sync engine . After the change requests are passed from the change propagator A timing of the operations can vary depending on network conditions differences in node capabilities etc.

At stage H the storage cluster sync engine processes the secondary change request in accordance with a protocol of a connection between the storage cluster sync engine and the storage cluster sync engine that traverses the network . The storage cluster sync engine can construct a new request in accordance with the connection protocol and populate the new request with the relevant information from the secondary change request e.g. secondary logical storage object identifier data to be written etc. . The storage cluster sync engine may encapsulate the secondary change request with a header compliant with the connection protocol. For this illustration the sync mappings at the primary node map logical object identifiers e.g. file handles between the primary node and the secondary node as well as map the filesystem location information e.g. Mode identifiers . The secondary change request is constructed with the secondary node filesystem location information of the data blocks impacted by the change request. In some cases the filesystem location information sync mappings will be separate from the logical object identifier sync mappings. And the filesystem location information sync mappings may be maintained at the secondary node. In those cases the secondary change request is constructed with indications of the targeted logical object and the filesystem location information of the primary node. When received the secondary node will access the sync mappings and resolve the primary node filesystem location information to the secondary node filesystem location information.

At stage I the storage cluster sync engine processes the received request in accordance with the connection protocol and passes the secondary change request to the secondary writer . The storage cluster sync engine may reconstruct the secondary change request from the received request or extract the secondary change request from the received request. If no secondary change requests have been received yet the storage cluster sync engine may invoke code to instantiate the secondary writer . The storage cluster sync engine can instantiate a secondary writer to handle all secondary change requests received by the storage cluster sync engine or instantiate them per primary logical storage object and secondary logical storage object pair.

At stage J the secondary writer updates sync tracking data . The secondary writer records indications of the secondary change request that at least include the targeted secondary logical storage object the requestor i.e. the change propagator A and state of the secondary change request. At this point the secondary writer records state as in flight since the secondary change request is being or will be sent. At stage K the secondary writer sends the secondary change request to the filesystem .

At stage L the filesystem accesses a hosting storage element in accordance with the secondary change request.

Stages A C illustrate a response traveling from the hosting storage element of the primary logical storage object to the change propagator A and a corresponding update of the in flight tracking data . At stage A a hosting storage element that hosts the primary logical storage object supplies a response to the filesystem . The filesystem forwards the response to the change propagator A at stage B. At stage C the change propagator A updates the in flight tracking data to indicate that the primary change request has been performed in the primary logical storage object.

Stages D J illustrate a response traveling from the hosting storage element of the secondary logical storage object to the change propagator A and a corresponding update of the in flight tracking data . At stage D a hosting storage element that hosts the secondary logical storage object supplies a response to the filesystem . The filesystem forwards the response to the secondary writer at stage E. At stage F the secondary writer updates the sync tracking data to reflect the update to the secondary logical storage object. For example the secondary writer uses a combination of the secondary logical storage object identifier and the requestor of the forwarded response to look up an entry in a structure that hosts the sync tracking data . The secondary writer sets a value or flag in the entry to indicate that the change has been completed to the secondary logical storage object. The secondary writer then forwards the response to the storage cluster synchronization engine . The storage cluster synchronization engine determines that the response to the secondary change request secondary response is to be sent to the primary node . The storage cluster synchronization engine processes the secondary response in accordance with the connection protocol and sends the secondary response over the connection via the network at stage H. At stage I the storage cluster synchronization engine processes the secondary response in accordance with the connection protocol and forwards the secondary response to the change propagator A. As part of processing the secondary response the storage cluster synchronization engine can determine that the secondary response should be sent to the change propagator A based on the requestor identifier that incorporates a process thread identifier of the change propagator A. At stage J the change propagator A updates the in flight tracking data to indicate that the secondary change request has been performed in the secondary logical storage object.

After determining that all outstanding change requests corresponding to the initial change request have been completed the change propagator A supplies a response to the filesystem request generator . Each time the change propagator A updates the in flight tracking data the change propagator A can read the entry to determine whether all requests indicated in the entry have been completed or are still in flight for example. For this illustration the filesystem request generator maintains data that indicates the requestor that corresponds to the change request . When a request is initially received by the filesystem request generator the request can be tagged with a request identifier that corresponds to the requestor. This request identifier can travel with the request and corresponding response. The request identifier indicates an identity of the requestor and the request to distinguish it from other requests from the same requestor. The change propagation engine can be programmed to also or instead of maintain data that indicates the requestor of the change request and that indicates the change request itself. At stage L the filesystem request generator forms a change response and supplies the change response to the corresponding requestor.

The primary storage node and the secondary storage node can be in two different clusters. According to some other aspects the primary storage node and the secondary storage node can be in a same cluster. The primary storage node is coupled to a primary storage which can store data e.g. files that can be referenced herein as primary data . With reference to the primary storage can be part of the group of logical storage objects. The secondary storage node is coupled to a secondary storage which can store data e.g. files that can be referenced herein as secondary data . With reference to the primary storage can be part of the group of logical storage objects. As further described below the secondary data can be a replication of the primary data .

In this example the operations and messages depicted in keep the secondary data in sync with the primary data sync replication in response to a write request to update a portion of the primary data . The primary storage node receives a primary write request . With reference to one of the clients can transmit the primary write request to be received by the primary storage node . The primary write request can be a request to update a portion of the primary data . The protocol translator can translate or convert a protocol of the primary write request into a uniform protocol to create a filesystem request. Specifically different primary write requests received by the primary storage node can be based on different protocols. The filesystem request can then be used to update the primary data in the primary storage at a file level e.g. updates to particular files directories etc. . For example the primary storage can be a Network Attached Storage NAS device.

After the protocol translation the change propagator replicates the primary write request to create a secondary write request for sync replication . The secondary write request is created and transmitted to the secondary storage node where a copy of the primary data has been replicated as secondary data that is stored in the secondary storage . The secondary write request is a request to update the same portion in the secondary data that is being updated in the primary data .

Also prior to replicating the primary write request to create the secondary write request the change propagator can determine whether the write request overlaps with another write request . As part of the sync replication the change propagator can perform overlapping detection and processing of the primary write request. As further described below the overlapping detection and processing of the primary write requests ensures that the order that the secondary write requests are received by the secondary storage node is the order in secondary write requests are executed to update the secondary write data. For example assume that the primary storage node transmits to the secondary storage node a secondary write request A at a first point in time and transmits a secondary write request B at a second later point in time. Because the secondary write requests A and B are created and processed external to the file system the file system cannot guarantee that the secondary write request A is executed before the secondary write request B. This can be especially problematic if the secondary write requests A and B are updating at least partially a same portion of the secondary data . For example if the secondary write request A is executed after the secondary write request B for a same portion of the secondary data the secondary write request A could overwrite some part of the update by the secondary write request B which should have been the latest update . Therefore the secondary data may not be an accurate replication of the primary data. Some aspects for overlapping detection and processing of primary write requests are described in more detail below in reference to .

Also the secondary write request created by the replication is transmitted to the secondary storage node . After sync replication is complete the secondary writer transmits a confirmation of sync replication back to the primary storage node . In response the primary storage node can also send a confirmation to the client that initially transmitted the primary write request that the update is complete.

At block a network adapter of the primary storage node receives a primary write request to modify a portion of primary data that is stored in the primary storage. With reference to the example depicted in a client that is coupled to the primary storage node can transmit the primary write request to the primary storage node . The primary write request can be a request to update an existing file create a new file update directory structure in the file system etc. Operations of the flowchart continue at block .

At block the protocol translator translates a protocol of the primary write request into a unified protocol. With reference to the example depicted in different primary write requests received by the primary storage node can be based on different protocols. According to some aspects protocol translation may not be required. For example the protocol of the primary write request may already be the uniform protocol. Operations of the flowchart continue at block .

At block the change propagator performs overlapping detection and processing in response to receiving the primary write request. With reference to the example in the change propagator can perform overlapping detection and processing in response to receiving the primary write request . The operations of the overlapping detection and processing according to some aspects are described in more detail below in reference to . Operations of the flowchart continue at block .

At block the change propagator replicates the primary write request to create a secondary write request to modify the portion replicated in secondary data stored in a secondary storage. With reference to the example depicted in the change propagator can perform this replication after a determination is made by the operations of the overlapping detection and processing that the primary write request can be replicated to create the secondary write request. The secondary write request is created and transmitted to the secondary storage node where a copy of the primary data has been replicated as secondary data that is stored in the secondary storage . The secondary write request is a request to update the same portion in the secondary data that is being updated in the primary data . This replication operation therefore enables the secondary data to remain in sync with the primary data . Operations of the flowchart continue at block .

At block the portion of the primary data is updated based on the filesystem request. With reference to the example depicted in the portion of the primary data in the primary storage is to be updated as defined by the filesystem request. For example a particular file or directory can be modified renamed moved or deleted. Operations of the flowchart continue at block .

At block a network adapter of the primary storage node transmits to the secondary storage node the secondary write request for sync replication of the primary write request. With reference to the example depicted in a network adapter not shown of the primary storage node transmits to the secondary storage node the secondary write request for sync replication of the primary write request . As further described below in response the secondary writer performs sync replication such that the update to the primary data is also replicated by an update to the secondary data thereby enabling the primary data and the secondary data to remain in sync . As part of the sync replication the change propagator performs overlapping detection and processing of the primary write request. The overlapping detection and processing of the primary write requests ensures that the order that the secondary write requests are received by the secondary storage node is the order in secondary write requests are executed to update the secondary write data. Operations of the flowchart continue at block .

At block after sync replication is complete the secondary writer transmits a confirmation of sync replication back to the primary storage node. With reference to the example depicted in the secondary writer transmits a confirmation back to the primary storage node . In response the primary storage node can also send a confirmation to the client that initially transmitted the primary write request that the update is complete. The operations of the flowchart are complete.

While the flowchart described operations that are sequentially order at least some of the operations can be performed at least partially in parallel. For example the operations for updating the portion of the primary data based on the filesystem request can be performed at least partially in parallel with transmitting the secondary write request performing the sync replication and or the receiving back of confirmation of the sync replication.

Example operations of the primary storage node for sync replication are now described. depict flowcharts of operations executing in a primary storage node to enable overlapping write detection and processing for sync replication according to some aspects. Operations of a flowchart of continues in a flowchart of at transition point A. The operations of the flowcharts can be performed by software firmware hardware or a combination thereof. The operations of the flowcharts are described as being performed by a component executing within the primary storage node of e.g. the change propagator . According to some other aspects some or all of the operations of the flowcharts can be performed by other modules in other nodes or devices communicatively coupled to the primary storage node . The operations of the flowchart start at block .

At block the change propagator receives a current primary write request that is to be replicated to perform a sync replication of the current primary write request . With reference to the example depicted in the change propagator can receive the primary write request after any protocol translation from the protocol translator . Operations of the flowchart continue at block .

At block the change propagator determines a data range of the portion that is replicated and to be synced based on the current primary write request. With reference to the example depicted in the change propagator can determine the data range of the portion to be updated in the secondary data based on the beginning address and its offset that defines the ending address of the data range to be updated in the secondary data . According to some aspects one or more data ranges can be updated for a current primary write request. If multiple data ranges are updated the change propagator can determine each of the multiple data ranges. Operations of the flowchart continue at block .

At block the change propagator determines whether there is a previous secondary write request that is not yet completed execution that is still in process of an update to at least a part of the data range defined by the current primary write request. With reference to the example depicted in the change propagator can make this determination. To help illustrate depicts range locking for primary write requests for overlapping write detection and processing according to some aspects. depicts a data range of data to be modified in the secondary data by execution of a first primary write request . The data range is defined to be between an offset 1 and an offset 2. A second primary write request that follows in time the first primary write request defines data to be modified in the secondary data in a data range . The data range is defined to be between an offset 3 and an offset 4. The data range is within and thus overlaps with the data range .

A third primary write request that follows in time the second primary write request defines data to be modified in the secondary data in a data range . The data range is defined to be between an offset 5 and an offset 6. The data range does not overlap with the data range . A fourth primary write request that follows in time the third primary write request defines data to be modified in the secondary data in a data range . The data range is defined to be between an offset 7 and an offset 8. The data range is within and thus overlaps with the data range .

In the example depicted in the first primary write request can be considered a previous primary write request that has not yet completed. Also if the current primary write request being processed is the second primary write request or the fourth primary write request the data range of the current primary write request would overlap with the data range of the previous primary write request that has not yet completed. However if the current primary write request being processed is the third primary write request the data range of the current primary write request would not overlap with the data range of the previous primary write request that has not yet completed.

Different data structures can be used to store the data ranges of the primary write requests that are being executed or have been suspended from execution. To illustrate depicts a range lock tree and suspended primary write request queue for overlapping write detection and processing according to some aspects. In particular depicts a range lock tree and a suspended primary write request queue . The range lock tree is a hierarchical tree structure with a root node and subtrees of children that include a set of linked nodes. Each node can comprise a data structure that stores a data range that is currently locked along with references to its children nodes. The range lock tree includes a root node that has two child nodes a child node and a child node . The child node has two child nodes a child node and a child node . The child node has a child node . Each node in the range lock tree stores a data range that is currently locked because an associated secondary write request is being processed to update the data range but has not yet completed. Because two secondary write requests cannot update at a same time data ranges that overlap the data ranges stored in the range lock tree are non overlapping relative to each other.

Returning to the operation at block of the flowchart in the change propagator can traverse the range lock tree to determine whether there is a previous secondary write request that is not yet completed execution that is updating a data range that overlaps with the data range defined for the current primary write request. In other words if the change propagator cannot locate any node in the range lock tree that has a data range that overlaps with the data range defined for the current primary write request the change propagator can determine that there are no previous secondary write requests not yet completed execution that are updating a data range that overlaps with the data range defined for the current primary write request.

Otherwise if the change propagator does find a node in the range lock tree that has a data range that overlaps with the data range defined for the current primary write request the change propagator determines that there is a previous secondary write request not yet completed execution that is updating a data range that overlaps with the data range defined for the current primary write request. As shown in by the arrow linking the range lock tree to the suspended primary write request queue if the current data range is found at one of the nodes of the range lock tree the current primary write request is stored in the suspended primary write request queue . An example use of the suspended primary write request queue is further described below. Also an alternative example to the range lock tree to make this determination regarding data range overlapping for primary write requests is depicted in which is further described below .

With reference to the flowchart of if there is a previous secondary write request that is not yet completed execution that is updating a data range that overlaps with the data range defined for the current primary write request operations at block continue at block . Otherwise operations at block continue at block .

At block the change propagator suspends execution of the current primary write request. With reference to the example depicted in the change propagator suspends execution of the primary write request. With reference to as part of suspending execution the change propagator can store the current primary write request at the bottom of the suspended primary write request queue . In this example the suspended primary write request queue is storing a number of primary write requests that have been suspended from execution a primary write request a primary write request a primary write request a primary write request a primary write request and a primary write request . The suspended primary write request queue can be configured to store the primary write requests in a descending order based on length of time of suspension. Therefore the primary write request that has been suspended the longest is stored on top while the primary write request that has been suspended the shortest is stored on bottom. Thus the current primary write request can be stored at the bottom below the primary write request in the suspended primary write request queue . Further use of the suspended primary write request queue is described below in the operations of the flowchart of . Operations of the flowchart continue at block .

At block the change propagator determines whether the previous secondary write request has completed its update of the secondary data. With reference to the example depicted in the change propagator makes this determination. This determination can be made periodically. Alternatively the change propagator can monitor the execution of the previous secondary write request or can receive notification that the previous secondary write request has completed execution. If the previous secondary write request has not completed operations of the flowchart remain at block . Otherwise operations of the flowchart continue at the transition point A which continues at transition point A in the flowchart .

Returning to the determination at block if there is not a previous secondary write request that is not yet completed execution that is updating a data range that overlaps with the data range defined for the current primary write request operations continue at block . At block after detecting that there is no overlap the change propagator can create a current secondary write request from the primary write request and transmit the current secondary write request to the secondary writer for execution. With reference to the example depicted in the change propagator can create the current secondary write request by replicating the primary write request . The change propagator can then transmit the current secondary write request to the secondary writer for execution. Accordingly the change propagator can defer replicating the primary write request to create the current secondary write request until the current secondary write request is to be transmitted. Also in response to creating and transmitting the current secondary write request to the secondary writer the change propagator can update data within the data range defined by the current primary write request . Operations of the flowchart continue at block . Operations of the flowchart continue at transition point A which continues at block from transition point A in the flowchart .

Operations of the flowchart of are now described. The operations of the flowchart can start at transition point A and continue at block .

At block the secondary writer transmits back to the primary storage node confirmation that the sync replication for the previous secondary write request is complete. With reference to the example depicted in the secondary writer can transmit confirmation of sync replication back to the primary storage node . Operations of the flowchart continue at block .

At block the change propagator determines whether there are any suspended write requests having a data range that at least partially overlaps with the data range associated with the secondary write request that is complete. With reference to the example depicted in the change propagator can make this determination. For example the change propagator can start at the top of the suspended primary write request queue depicted in and process the primary write requests until a primary write request is located that has a data range that overlaps with the data range associated with the secondary write request that has completed execution. As described above the primary write requests can be ordered in the suspended primary write request queue from top to bottom according to length of suspensions. Therefore starting at the top and traversing down the suspended primary write request queue the change propagator can locate the primary write request whose data range overlaps with the previous data range that has been suspended the longest. As previously described the current primary write request is stored at the bottom of the suspended primary write request queue . Thus if there is at least one other primary write request that is suspended that is configured to modify at least part of the current data range the change propagator can locate the at least one other primary write request prior to locating the current primary write request while traversing the suspended primary write request queue from top to bottom. The change propagator can therefore determine if there is at least one other primary write request that is suspended that is also configured to modify at least a part of the current data range to be modified by the current primary write request based on traversal of the suspended primary write request queue .

Also after locating a first primary write request that has been suspended the longest the change propagator can continue to traverse the suspended primary write request queue to locate any other primary write requests that had data ranges that at least partially overlaps with the data range associated with the secondary write request that is complete. If a second primary write request is located below the first primary write request in the suspended primary write request queue and if the data range for this second primary write request does not overlap with the data range for the first primary write request this second primary write request can also be selected for execution. The change propagator can continue to traverse the suspended primary write request queue to locate any other primary write requests that can be selected for execution. For example if a third primary write request is located below the second primary write request in the suspended primary write request queue and if the data range for this third primary write request does not overlap with the data range for the first primary write request and the second primary write request this third primary write request can also be selected for execution. Therefore the change propagator can select for execution any number of primary write requests that satisfy the data range overlapping described above . If there are any suspended write requests having a data range that at least partially overlaps with the data range associated with the secondary write request that is complete operations of the flowchart continue at block . Otherwise operations of the flowchart are complete.

At block the change propagator selects among the suspended primary write request that have a data range that at least partially overlaps with the data range of the secondary write request that just complete one or more primary write requests having non overlapping data ranges. With reference to the example depicted in the change propagator can make this selection. As described above the change propagator can traverse the suspended primary write request queue to locate one or more primary write requests that have a data range that at least partially overlaps with the data range of the secondary write request that just complete and that have a data range that is non overlapping relative to each other. Also as described above the primary write requests that have been suspended the longest can be given priority for selection and execution. Also during operation two or more consecutive write requests in the suspended primary write request queue can update a same data range. If this situation occurs the change propagator can execute the one consecutive write request that has been in the suspended primary write request queue the shortest time while discarding the remaining consecutive write requests that have been in the suspended primary write request queue . Operations of the flowchart continue at block .

At block the change propagator creates secondary write request s based on the primary write request s and then transmits the secondary write request s to the secondary storage node for execution. For example the change propagator can create secondary write request s by replicating the primary write request s . With reference to the example depicted in the change propagator can transmit the secondary write request s to the secondary writer in the secondary storage node for execution. In response to receiving the secondary write request s the secondary writer can update data within the data range defined by the secondary write request s in the secondary data . Operations of the flowchart continue at block .

At block the secondary writer transmits back to the primary storage node confirmation that the sync replication for the secondary write request s is complete. With reference to the example depicted in the secondary writer can transmit confirmation of sync replication back to the primary storage node . Operations of the flowchart return to block to determine if there are any suspended write requests having a data range that at least partially overlaps with the data range associated with the secondary write request s that is complete.

While the operations depicted in are described using a queue the suspended primary write request queue of that is shared for different data ranges in the secondary data according to some other aspects multiple queues can be used such that each queue is associated with a different data range in the secondary data. For example depicts a number of suspended primary write request queues wherein each suspended primary write request queue is associated with a data block to enable overlapping write detection and processing according to some aspects.

The suspended primary write queue is associated with data block . The suspended primary write queue is associated with data block . The suspended primary write queue is associated with data block X. The suspended primary write queue is storing two suspended primary write requests a primary write request and a primary write request . The suspended primary write queue is storing one suspended primary write request a primary write request . The suspended primary write queue is storing three suspended primary write requests a primary write request a primary write request and a primary write request . Therefore if a given data block is currently being updated by a secondary write request any primary write requests that are received are stored in the suspended primary write queue associated with that data block.

Operations of the flowcharts can execute similar to the description above with a shared queue. However with the use of multiple suspended primary write queues the change propagator can traverse a suspended primary write queue that is associated with a data block for which a secondary write request has just completed updating and is now available for updating by a primary write request.

According to some aspects one primary write request can update multiple data blocks. In such a situation the primary write request is suspended from execution until all of the multiple data blocks to be updated are available. For example assume that primary write request A is to update data block and data block . Also assume that data block is currently being updated by secondary write request B and data block is currently being updated by secondary write request C. Therefore primary write request A is stored at the bottom of the suspended primary write queue and is stored at the bottom of the suspended primary write queue . Assume that data block becomes available because secondary write request C has completed its update to data block but that data block is still being updated by secondary write request B. In this example the primary write request A would suspend any other primary write requests from updating data block while continuing to wait for data block to become available. Once data block become available a secondary write request can be created from the primary write request A and can then be executed to update data block and data block .

According to some aspects the change propagator might replicate I O commands including secondary write requests in groups or batches to the secondary storage node . These batches can be referred to as changesets . As the secondary storage node receives the changeset the I O commands can be written to a log file. The primary storage node can initiate a log change after the operations associated with the changeset have been sent. In response the secondary storage node can switch to a different log file. Any I O commands including additional secondary write requests received after the log switch are inserted into the different log file while the I O commands in the original log file are committed.

Although this description refers to individual logical storage objects being paired for synchronization relationships the endpoints of a synchronization relationship can be groups of logical storage objects. A group of files or group of LUNs for example can be in a synchronization relationship with another group of logical storage objects. The nodes can maintain additional data to resolve group identifiers to the logical storage objects that are members of the group.

As will be appreciated by one skilled in the art some aspects may be embodied as a system method or computer program product. Accordingly some aspects may take the form of entirely hardware entirely software including firmware resident software micro code etc. or a combination of software and hardware aspects that may all generally be referred to herein as a circuit module or system. Furthermore some aspects may take the form of a computer program product embodied in one or more computer readable medium s having computer readable program code embodied thereon.

Any combination of one or more computer readable medium s may be utilized. The computer readable medium may be a computer readable signal medium or a computer readable storage medium. A computer readable storage medium may be for example but not limited to an electronic magnetic optical electromagnetic infrared or semiconductor system apparatus or device or any suitable combination of the foregoing. More specific examples a non exhaustive list of the computer readable storage medium would include the following an electrical connection having one or more wires a portable computer diskette a hard disk a random access memory RAM a read only memory ROM an erasable programmable read only memory EPROM or Flash memory an optical fiber a portable compact disc read only memory CD ROM an optical storage device a magnetic storage device or any suitable combination of the foregoing. In the context of this document a computer readable storage medium may be any tangible medium that can contain or store a program for use by or in connection with an instruction execution system apparatus or device.

A computer readable signal medium may include a propagated data signal with computer readable program code embodied therein for example in baseband or as part of a carrier wave. Such a propagated signal may take any of a variety of forms including but not limited to electro magnetic optical or any suitable combination thereof. A computer readable signal medium may be any computer readable medium that is not a computer readable storage medium and that can communicate propagate or transport a program for use by or in connection with an instruction execution system apparatus or device.

Program code embodied on a computer readable medium may be transmitted using any appropriate medium including but not limited to wireless wireline optical fiber cable RF etc. or any suitable combination of the foregoing.

Computer program code for carrying out operations of various aspects may be written in any combination of one or more programming languages including an object oriented programming language such as Java Smalltalk C or the like and conventional procedural programming languages such as the C programming language or similar programming languages. The program code may execute entirely on the user s computer partly on the user s computer as a stand alone software package partly on the user s computer and partly on a remote computer or entirely on the remote computer or server. In the latter scenario the remote computer may be connected to the user s computer through any type of network including a local area network LAN or a wide area network WAN or the connection may be made to an external computer for example through the Internet using an Internet Service Provider .

Some aspects are described with reference to flowchart illustrations and or block diagrams of methods apparatus systems and computer program products. It will be understood that each block of the flowchart illustrations and or block diagrams and combinations of blocks in the flowchart illustrations and or block diagrams can be implemented by computer program instructions. These computer program instructions may be provided to a processor of a general purpose computer special purpose computer or other programmable data processing apparatus to produce a machine such that the instructions which execute via the processor of the computer or other programmable data processing apparatus create means for implementing the functions acts specified in the flowchart and or block diagram block or blocks.

These computer program instructions may also be stored in a computer readable medium that can direct a computer other programmable data processing apparatus or other devices to function in a particular manner such that the instructions stored in the computer readable medium produce an article of manufacture including instructions which implement the function act specified in the flowchart and or block diagram block or blocks.

The computer program instructions may also be loaded onto a computer other programmable data processing apparatus or other devices to cause a series of operational steps to be performed on the computer other programmable apparatus or other devices to produce a computer implemented process such that the instructions which execute on the computer or other programmable apparatus provide processes for implementing the functions acts specified in the flowchart and or block diagram block or blocks.

While various aspects are described with reference to various implementations and exploitations it will be understood that these aspects are illustrative and that the scope of these aspects is not limited to them. In general techniques for overlapping write detection and processing for sync replication as described herein may be implemented with facilities consistent with any hardware system or hardware systems. Many variations modifications additions and improvements are possible.

Plural instances may be provided for components operations or structures described herein as a single instance. Finally boundaries between various components operations and data stores are somewhat arbitrary and particular operations are illustrated in the context of specific illustrative configurations. Other allocations of functionality are envisioned and may fall within the scope of the various aspects. In general structures and functionality presented as separate components in the exemplary configurations may be implemented as a combined structure or component. Similarly structures and functionality presented as a single component may be implemented as separate components. These and other variations modifications additions and improvements may fall within the scope of the various aspects.

