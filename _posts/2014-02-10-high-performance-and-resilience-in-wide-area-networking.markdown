---

title: High performance and resilience in wide area networking
abstract: Aspects and implementations of the present disclosure generally relate to use of a multi-chassis link aggregation for high performance and resilience in wide-area networking. In one aspect, the disclosure relates to a system that includes a switch fabric. The fabric includes at least a plurality of edge network devices, a set of internal switch devices, and a plurality of internal network links coupling each edge network device to at least a subset of the set of internal switch devices. The system includes a network controller coupled to the switch fabric, configured to maintain at least one link aggregation comprising a logical grouping of externally facing network interfaces of at least two of the plurality of edge devices. The network controller is configured to monitor internal link performance characteristics and determine throughput characteristics for each link aggregation over time based at least in part on current internal link performance characteristics.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09369408&OS=09369408&RS=09369408
owner: Google Inc.
number: 09369408
owner_city: Mountain View
owner_country: US
publication_date: 20140210
---
This application claims priority to U.S. Provisional Application No. 61 934 539 filed Jan. 31 2014 and titled High Performance and Resilience in Wide Area Networking the entirety of which is incorporated herein by reference.

Communication and data networks generally include a variety of network devices or nodes connected by various links. Each link connects a pair of network devices and enables exchange of information between them. In some networks multiple links between a pair of nodes are treated as a single logical link referred to as a trunk or link aggregation LAG .

A software defined network SDN is a set of network devices in a data network that includes at least one network device that relies on a separate controller for configuration information such as updates to tables for routing network traffic.

In one aspect the disclosure relates to a system. The system includes a switch fabric that includes at least a plurality of edge network devices each having a plurality of externally facing network interfaces and a plurality of internally facing network interfaces a set of internal switch devices and a plurality of internal network links coupling each edge network device to at least a subset of the set of internal switch devices. The system also includes a network controller coupled to switch fabric the network controller configured to maintain at least one link aggregation LAG comprising a logical grouping of externally facing network interfaces of at least two of the plurality of edge devices. The network controller is configured to monitor internal link performance characteristics and determine throughput characteristics for each LAG over time based at least in part on current internal link performance characteristics. In some implementations the network controller is further configured to publish the determined throughput characteristics for each LAG.

In one aspect the disclosure relates to a method. The method includes maintaining by a network controller coupled to a switch fabric at least one link aggregation LAG comprising a logical grouping of externally facing network interfaces of at least two of a plurality of edge devices in the switch fabric. The method includes monitoring by the network controller internal link performance characteristics for internal network links coupling the plurality of edge devices to internal switch devices and determining by the network controller throughput characteristics for each LAG over time based at least in part on current internal link performance characteristics. In some implementations the method includes publishing by the network controller the determined throughput characteristics for each LAG.

In one aspect the disclosure relates to tangible computer readable storage media storing non transient processor executable instructions that when executed by a computing device comprising the storage media and one or more processors cause the one or more processors to perform the operations of maintaining at least one link aggregation LAG comprising a logical grouping of externally facing network interfaces of at least two of a plurality of edge devices in a switch fabric. The instructions cause the one or more processors to perform the operations of monitoring internal link performance characteristics for internal network links coupling the plurality of edge devices to internal switch devices and determining throughput characteristics for each LAG over time based at least in part on current internal link performance characteristics. In some implementations the instructions cause the one or more processors to publish the determined throughput characteristics for each LAG.

Aspects and implementations of the present disclosure generally relate to use of a multi chassis link aggregation a LAG for high performance and resilience in wide area networking. A LAG is an aggregation of parallel data links between two end nodes. That is a LAG is a logical grouping of links or of the network interfaces to those links such that the logical grouping acts as a single higher capacity link. In a multi chassis LAG between nodes at least one end node is a multi chassis node that includes multiple edge switches with externally facing network interfaces that each serve as physical termination points for respective parallel links of the LAG. The multi chassis node may also include internal network switches that participate in data connections between edge switches. When there are failures at edge switches or internal switches or failures in the links there between the overall capacity of the multi chassis LAG is impaired.

The multi chassis node is part of a software defined network SDN . Generally an SDN is a data network in which some or all of the network devices in the network are controlled by a separate SDN controller. In the SDN controller controls the internal switches and edge switches via a control plane . In some implementations the SDN controller operates collaboratively with a traffic engineering module to determine flow routes flow rates and flow limits through the multi chassis node and any larger network to which the multi chassis node may be connected. described below illustrates an example SDN controller linked to an example network device. described below illustrates an example computing system or network device. Referring still to each internal switch and each edge switch is a network device with multiple network interfaces in accordance with the computing device illustrated in .

Referring to in more detail the multi chassis node is one end of a LAG linking the multi chassis node to an external node . The LAG is an aggregation of data links between the external node and one or more edge switches of the multi chassis node . The multi chassis node may participate in multiple LAGs . Each data link may be any form of data link including but not limited to links employing copper wire coaxial cable optical fiber radio waves or satellites. Each data link has characteristics. Link characteristics may include for example bandwidth capacity latency data loss rate signal stability and or operational state operational as compared to non operational or failed . The data links may be aggregated together into a LAG according to any link aggregation protocol e.g. the Link Aggregation Control Protocol LACP . In some implementations the link aggregation protocol is maintained by the SDN controller . The data links of a LAG may connect an external node to multiple edge switches of the multi chassis node . For example the LAG from external node is a logical grouping of data links to three edge switches and . In some implementations there are multiple links between an external node and a single edge switch .

As shown in the multi chassis node can participate in multiple LAGs . For example an external node may have a first LAG to the multi chassis node and another external node may have a second LAG to the multi chassis node . The two external nodes and exchange information via their respective LAGs and the multi chassis node . The information exchanged may traverse multiple data links and multiple edge switches . The information exchanged is passed from one edge switch to another edge switch via an internal data plane of the multi chassis node .

The internal data plane connects ach of the edge switches to one or more internal switches by one or more data links. In some implementations the data plane is a switched network fabric. Data packets are propagated through the data plane between the edge switches and the internal switches . Generally the data packets may participate in a flow of data packets between any two of the external nodes . For example a data packet may arrive from an external node at a first edge switch be forwarded to an internal switch and subsequently forwarded to a second edge switch for egress to another external node . In some examples a route could have multiple hops e.g. from a first edge switch to a first internal switch to a second edge switch to another internal switch eventually reaching an edge switch for egress. This may occur for example if there are no shorter routes if there is a failure or if it helps in distributing load. In some other examples a data packet may arrive from an external node at a first edge switch and may be directly forwarded to a second external node by the first edge switch without traversing the internal data plane . Data packets flow through the data plane according to routes. In some implementations the routes are determined by the SDN controller .

The SDN controller manages the multiple switches of the multi chassis node . In some implementations the SDN controller is linked to the internal switches and or the edge switches by a control plane . In some implementations the control plane and the data plane overlap share links or are the same physical links. In some implementations the control plane is wireless. The control plane carries configuration data and control information between the SDN controller and the switches in the multi chassis node . In some implementations the SDN controller manages each LAG as well as the interface groupings of the edge switches comprising each LAG . For example in some implementations the SDN controller implements the link aggregation control protocol LACP .

In some implementations a traffic engineering module manages network policy. For example in some implementations the traffic engineering module controls traffic policies including for example policies for capacity management resource management regulation of long tail traffic flows and bandwidth allocation. In some implementations the traffic engineering module determines a maximum bandwidth that may be used by any one flow. In some implementations the traffic engineering module reserves a minimum bandwidth level for short burst traffic e.g. UDP packets. In some implementations the traffic engineering module manages quality of service QoS policies. In some implementations the traffic engineering module determines TCP window sizes queue sizes buffer sizes and or other resource allocations. In some implementations the traffic engineering module is part of the SDN controller in other implementations the traffic engineering module is distinct from the SDN controller . In some implementations the SDN controller publishes information to the traffic engineering module . In some implementations the information is transmitted using a standard network protocol e.g. BGP or SNMP. In some implementations the information is transmitted using a custom protocol.

In some implementations the SDN controller determines characteristics for each LAG . For example in some implementations the SDN controller determines the throughput levels for a LAG and in the event of a failure in one or more links or switches participating in the LAG adjusts network configurations to reflect and adjust for a resulting impairment. In some implementations the SDN controller calculates a performance level based on internal link characteristics. For example in some implementations the SDN controller publishes a capacity metric value for each LAG and the SDN controller is able to reduce the published capacity value of an impaired LAG to accurately reflect its capacity to avoid overloading the LAG. In some implementations the SDN controller modifies internal network routes to maintain communication over portions of an impaired LAG that are still functioning. In some implementations the SDN can be configured to modify the routes of the impaired LAG to mitigate the impacts of the impairment.

Generally a failure manifests as a reduction or loss of communication across an internal link in the data plane . The problem may be caused by a failure along a particular link a failure at an internal switch or a failure at an edge switch . In some instances multiple failures may occur.

In some implementations each internal switch is configured to report status information to an SDN controller . For example in some implementations an internal switch reports to the SDN controller if a network interface fails. Likewise in some implementations each edge switch is similarly configured to report status information to the SDN controller . In some implementations a failure of a single network interface at a switch either an internal switch or an edge switch is treated as a failure of the entire switch. In some such implementations the entire switch is removed from routes and can be replaced without further interruption of service.

Similarly referring to an edge switch failure may occur when an edge switch becomes partially or completely non operational. For example in some implementations a failure of a network interface internally facing or externally facing at an edge switch is treated as a failure of the entire edge switch . In some implementations the SDN controller receives a status message from the failed edge switch . In some implementations the SDN controller determines that an edge switch has failed without receiving such a message. For example in some implementations the edge switch sends a periodic status message to the SDN controller . If the SDN controller does not receive the periodic status message from a particular edge switch within a predetermined length of time the SDN controller determines that the particular edge switch has failed. In some implementations the SDN controller sends periodic status request messages to the edge switches .

An internal switch failure or an edge switch failure may resemble multiple link failures. For example an internal switch failure results in a loss of every link between the failed internal switch and its respective linked edge switches . The lost links are illustrated in as dashed lines. A switch failure proportionally reduces the capacity of the multi chassis node .

In some implementations every internal switch in a block of internal switches is linked to every edge switch in a block of edge switches all linked to the same LAG. If each internal switch was configured to provide equal throughput for the LAG a loss of one internal switch in a block of n internal switches reduces the capacity of that LAG by 1 n. For example if there are eight internal switches in a block then losing one diminishes capacity of the LAG by 12.5 . In general capacity is reduced as a function of the number of internal network interfaces participating in the LAG that are lost.

In more detail in the method a network controller maintains a link aggregation stage . In some implementations the network controller e.g. the SDN controller illustrated in implements a link aggregation protocol e.g. the Link Aggregation Control Protocol LACP See for example the IEEE 802.1AX 2008 Standard . In some implementations the network controller manages a multi chassis node for a link aggregation LAG . In some implementations the node participates in multiple LAGs. In some implementations the network controller maintains information for each flow carried by each LAG connected to the node. In some other implementations the network controller maintains aggregate information about data flows carried by each LAG.

The network controller monitors internal link performance characteristics stage . The multi chassis node includes a network fabric made up of internal links connecting edge switches and internal switches. The network controller monitors the performance characteristics of these links. In some implementations the network controller receives status messages from the switches to monitor performance characteristics of the internal links. In some implementations the network controller executes network tests e.g. by sending test packets through the network fabric and monitors results of the executed network tests. In some implementations the performance characteristics include whether each particular link is functioning or not functioning that is whether each link has failed. In some implementations the performance characteristics include throughput characteristics such as maximum capacity current bandwidth usage explicit congestion indications average bit rate and or latency. In some implementations the performance characteristics include physical or artificial limitations on traffic flows. For example in some implementations internal links are constrained by packet size limitations windowing or rate limiters throttling traffic flow artificially.

The network controller determines throughput characteristics for the link aggregation based at least in part on current internal link performance characteristics stage . The throughput characteristics of a LAG include the ability of the multi chassis node to convey data packets to or from the links of the LAG. If the current internal link performance characteristics as monitored in stage impair data flow to or from the LAG then the LAG has reduced throughput characteristics even if all external links are operating at full capacity. In some implementations the network controller calculates a value for a throughput metric related to the LAG. For example if each internal link contributes to the throughput of a LAG equally an impairment metric can be calculated as a percentage of the internal links that are considered to be operational. In some implementations the network controller reduces a published bandwidth of a LAG by an amount proportional to a single internal link when the determined throughput characteristics indicate that a single internal link used by the LAG has failed. In some implementations if an internally facing network interface fails at a switch the switch is treated as having failed. As a result a single internal link failure may be treated as multiple internal link failures.

In some implementations the network controller publishes the determined throughput characteristics stage . In some implementations the network controller interacts with a traffic engine e.g. the traffic engineering module illustrated in . In some such implementations the network controller provides the traffic engine with information related to the availability of each LAG. In some implementations the network controller provides the traffic engine with the throughput characteristics determined at stage . In some implementations the network controller publishes the throughput characteristics determined at stage via an interface e.g. an Application Programming Interface API . In some implementations the network controller publishes the throughput characteristics determined at stage via a custom protocol. In some implementations the network controller publishes the throughput characteristics determined at stage by setting status flags. In some implementations the network controller publishes the throughput characteristics using a flexible standard networking protocol e.g. the Border Gateway Protocol BGP or the Simple Network Management Protocol SNMP . In some implementations the network controller publishes the throughput characteristics generally to any network recipient.

In more detail in the method begins with a network controller determining an internal route for a data flow through a link aggregation stage . In some implementations the network controller is an SDN controller e.g. the SDN controller illustrated in . The link aggregation LAG is a logical grouping of links connected to externally facing network interfaces of one or more edge switches. The network controller determines a route for a data flow through the one or more edge switches. That is data packets flowing through the LAG are forwarded to or from multiple edge switches according to route information determined by the network controller. The routes may include one or more internal switches and in some instances one or more other edge switches. For example referring to a data flow from a first external node to another external node includes packets arriving at the multi chassis node via a first LAG and leaving via a second LAG . The packets may arrive at different edge switches e.g. a first edge switch or a second edge switch . The packets may leave at different edge switches e.g. a third edge switch or a fourth edge switch . In some instances the network controller may distribute a route for the flow across multiple internal switches e.g. internal switch and internal switch. In some implementations the network controller determines the internal route for a data flow based on current throughput characteristics for the link aggregation.

Referring to the method continues with the network controller identifying a change in throughput characteristics for the link aggregation stage . For example the network controller may detect a link failure e.g. as illustrated in or receive a published notification of network impairment. In some implementations the network controller detects a particular failure. In some implementations the network controller is configured to receive specific status notifications regarding links of the data plane . For example referring to an internal switch may be configured to periodically report status information to the network controller. When there is an internal failure the failed switch may report the failure or fail to report at all to the network controller.

As shown in the network controller then modifies the internal route for the data flow responsive to the identified change in throughput characteristics for the link aggregation stage . In some implementations the change in throughput characteristics for the LAG result from an internal switch failure e.g. as shown in . The network controller may respond by altering a flow route to avoid the failed internal switch. For example referring to if a network controller is maintaining a route that distributes traffic over internal switch and internal switch and if one of the two internal switches fails the network controller may modify the distributed route to only use the surviving switch. In the network shown in internal switch has a link to an edge switch and internal switch does not. Thus if internal switch fails there is still a route to edge switch but if internal switch fails there is not. That is if internal switch fails the data flow will be more severely impacted because the edge switch will also be lost. In some implementations the network controller accounts for each edge switch and each internal switch that is available for a flow. For example returning to where every edge switch has an internal link to every internal switch an internal switch failure reduces the network s capacity leaving an impaired but functional network fabric. Similarly referring to an edge switch failure reduces the number of links available to a multi switch multi chassis LAG. In some implementations if the throughput characteristics of the surviving switches cannot sustain a flow the network controller can introduce a rate limiter to throttle the flow to within the capacity of the surviving switches. In some implementations if the throughput characteristics of the surviving switches cannot sustain a flow the network controller can terminate the flow.

Referring to in more detail the SDN controller includes a control module and memory storing configuration and routing data. The control module uses the configuration and routing data stored in memory to configure the network device . In some implementations the control module periodically sends a status or availability message to the network device . In some implementations the SDN controller includes additional application modules not shown. For example in some implementations the SDN controller includes a traffic engineering module.

The network device includes a control module and memory storing configuration and routing data. The network device control module receives configuration and routing information from the SDN controller control module via the control plane and updates the configuration and routing data stored in memory .

The network device includes a set of network interfaces . Each network interface may be connected to a data plane as shown in . External devices send data packets to the network device via the data plane and a first network interface e.g. network interface . The network device forwards received data packets to an appropriate next hop via another interface e.g. network interface . In some implementations the forwarding engine determines which network interface to use for each data packet received.

The forwarding engine uses the configuration and routing data in memory to manage the data traffic at network interface ports . The configuration and routing data in memory are controlled by the SDN controller via the control module .

The memory and the memory may each be any device suitable for storing computer readable data. The memory may be similar to the memory illustrated in and described below. Examples include but are not limited to semiconductor memory devices such as EPROM EEPROM SRAM and flash memory devices. In some implementations the memory of a network device includes one or more ternary content addressable memory TCAM devices. A network device may have any number of memory devices . An SDN controller may have any number of memory devices .

In more detail the processor may be any logic circuitry that processes instructions e.g. instructions fetched from the memory or cache . In many embodiments the processor is a microprocessor unit or special purpose processor. The computing device may be based on any processor or set of processors capable of operating as described herein. The processor may be a single core or multi core processor. The processor may be multiple processors.

The memory may be any device suitable for storing computer readable data. The memory may be a device with fixed storage or a device for reading removable storage media. Examples include all forms of non volatile memory media and memory devices semiconductor memory devices e.g. EPROM EEPROM SRAM and flash memory devices magnetic disks magneto optical disks and optical discs e.g. CD ROM DVD ROM and Blu Ray discs . A computing system may have any number of memory devices .

The cache memory is generally a form of computer memory placed in close proximity to the processor for fast read times. In some implementations the cache memory is part of or on the same chip as the processor . In some implementations there are multiple levels of cache e.g. L2 and L3 cache layers.

The network interface controller manages data exchanges via the network interfaces . The network interface controller handles the physical and data link layers of the OSI model for network communication. In some implementations some of the network interface controller s tasks are handled by the processor . In some implementations the network interface controller is part of the processor . In some implementations a computing system has multiple network interface controllers . The network interfaces are connection points for physical network links. In some implementations the network interface controller supports wireless network connections and an interface port is a wireless receiver transmitter. Generally a computing device exchanges data with other computing devices via physical or wireless links to a network interface . In some implementations the network interface controller implements a network protocol such as Ethernet.

The other computing devices are connected to the computing device via a network interface port . The other computing devices may be peer computing devices network devices or any other computing device with network functionality. For example a first computing device may be a network device such as a hub a bridge a switch or a router connecting the computing device to a data network such as the Internet.

The other devices may include an I O interface external serial device ports and any additional co processors. For example a computing system may include an interface e.g. a universal serial bus USB interface for connecting input devices e.g. a keyboard microphone mouse or other pointing device output devices e.g. video display speaker or printer or additional memory devices e.g. portable flash drive or external media drive . In some implementations a computing device includes an additional device such as a co processor e.g. a math co processor can assist the processor with high precision or complex calculations.

Implementations of the subject matter and the operations described in this specification can be implemented in digital electronic circuitry or in computer software embodied on a tangible medium firmware or hardware including the structures disclosed in this specification and their structural equivalents or in combinations of one or more of them. Implementations of the subject matter described in this specification can be implemented as one or more computer programs embodied on a tangible medium i.e. one or more modules of computer program instructions encoded on one or more computer storage media for execution by or to control the operation of a data processing apparatus. A computer storage medium can be or be included in a computer readable storage device a computer readable storage substrate a random or serial access memory array or device or a combination of one or more of them. The computer storage medium can also be or be included in one or more separate components or media e.g. multiple CDs disks or other storage devices . The computer storage medium may be tangible and non transitory.

The operations described in this specification can be implemented as operations performed by a data processing apparatus on data stored on one or more computer readable storage devices or received from other sources.

A computer program also known as a program software software application script or code can be written in any form of programming language including compiled or interpreted languages declarative or procedural languages and it can be deployed in any form including as a stand alone program or as a module component subroutine object or other unit suitable for use in a computing environment. A computer program may but need not correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data e.g. one or more scripts stored in a markup language document in a single file dedicated to the program in question or in multiple coordinated files e.g. files that store one or more modules sub programs or portions of code . A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network. Examples of communication networks include a local area network LAN and a wide area network WAN an inter network e.g. the Internet and peer to peer networks e.g. ad hoc peer to peer networks .

The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform actions by operating on input data and generating output. The processes and logic flows can also be performed by and apparatus can also be implemented as special purpose logic circuitry e.g. an FPGA field programmable gate array or an ASIC application specific integrated circuit .

While this specification contains many specific implementation details these should not be construed as limitations on the scope of any inventions or of what may be claimed but rather as descriptions of features specific to particular implementations of particular inventions. Certain features that are described in this specification in the context of separate implementations can also be implemented in combination in a single implementation. Conversely various features that are described in the context of a single implementation can also be implemented in multiple implementations separately or in any suitable sub combination. Moreover although features may be described above as acting in certain combinations and even initially claimed as such one or more features from a claimed combination can in some cases be excised from the combination and the claimed combination may be directed to a sub combination or variation of a sub combination.

Similarly while operations are depicted in the drawings in a particular order this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order or that all illustrated operations be performed to achieve desirable results. In certain circumstances multitasking and parallel processing may be advantageous. Moreover the separation of various system components in the implementations described above should not be understood as requiring such separation in all implementations and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.

References to or may be construed as inclusive so that any terms described using or may indicate any of a single more than one and all of the described terms. The labels first second third and so forth are not necessarily meant to indicate an ordering and are generally used merely to distinguish between like or similar items or elements.

Thus particular implementations of the subject matter have been described. Other implementations are within the scope of the following claims. In some cases the actions recited in the claims can be performed in a different order and still achieve desirable results. In addition the processes depicted in the accompanying figures do not necessarily require the particular order shown or sequential order to achieve desirable results. In certain implementations multitasking or parallel processing may be utilized.

