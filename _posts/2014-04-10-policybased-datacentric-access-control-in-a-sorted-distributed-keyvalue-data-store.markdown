---

title: Policy-based data-centric access control in a sorted, distributed key-value data store
abstract: A method, apparatus and computer program product for policy-based access control in association with a sorted, distributed key-value data store in which keys comprise a cell-level access control. In this approach, an information security policy is used to create a set of pluggable policies. A pluggable policy may be used during data ingest time, when data is being ingested into the data store, and a pluggable policy may be used during query time, when a query to the data store is received for processing against data stored therein. Generally, a pluggable policy associates one or more user-centric attributes (or some function thereof), to a particular set of data-centric attributes. By using pluggable policies, preferably at both ingest time and query time, the data store is enhanced to provide a seamless and secure policy-based access control mechanism in association with the cell-level access control enabled by the data store.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08914323&OS=08914323&RS=08914323
owner: Sqrrl Data, Inc.
number: 08914323
owner_city: Cambridge
owner_country: US
publication_date: 20140410
---
This application relates generally to secure large scale data storage and in particular to database systems providing fine grained access control.

 Big Data is the term used for a collection of data sets so large and complex that it becomes difficult to process e.g. capture store search transfer analyze visualize etc. using on hand database management tools or traditional data processing applications. Such data sets typically on the order of terabytes and petabytes are generated by many different types of processes.

Big Data has received a great amount of attention over the last few years. Much of the promise of Big Data can be summarized by what is often referred to as the five V s volume variety velocity value and veracity. Volume refers to processing petabytes of data with low administrative overhead and complexity. Variety refers to leveraging flexible schemas to handle unstructured and semi structured data in addition to structured data. Velocity refers to conducting real time analytics and ingesting streaming data feeds in addition to batch processing. Value refers to using commodity hardware instead of expensive specialized appliances. Veracity refers to leveraging data from a variety of domains some of which may have unknown provenance. Apache Hadoop is a widely adopted Big Data solution that enables users to take advantage of these characteristics. The Apache Hadoop framework allows for the distributed processing of Big Data across clusters of computers using simple programming models. It is designed to scale up from single servers to thousands of machines each offering local computation and storage. The Hadoop Distributed File System HDFS is a module within the larger Hadoop project and provides high throughput access to application data. HDFS has become a mainstream solution for thousands of organizations that use it as a warehouse for very large amounts of unstructured and semi structured data.

In 2008 when the National Security Agency NSA began searching for an operational data store that could meet its growing data challenges it designed and built a database solution on top of HDFS that could address these needs. That solution known as Accumulo is a sorted distributed key value store largely based on Google s Bigtable design. In 2011 NSA open sourced Accumulo and it became an Apache Foundation project in 2012. Apache Accumulo is within a category of databases referred to as NoSQL databases which are distinguished by their flexible schemas that accommodate semi structured and unstructured data. They are distributed to scale well horizontally and they are not constrained by the data organization implicit in the SQL query language. Compared to other NoSQL databases Apache Accumulo has several advantages. It provides fine grained security controls or the ability to tag data with security labels at an atomic cell level. This feature enables users to ingest data with diverse security requirements into a single platform. It also simplifies application development by pushing security down to the data level. Accumulo has a proven ability to scale in a stable manner to tens of petabytes and thousands of nodes on a single instance of the software. It also provides a server side mechanism Iterators that provide flexibility to conduct a wide variety of different types of analytical functions. Accumulo can easily adapt to a wide variety of different data types use cases and query types. While organizations are storing Big Data in HDFS and while great strides have been made to make that data searchable many of these organizations are still struggling to build secure real time applications on top of Big Data. Today numerous Federal agencies and companies use Accumulo.

While technologies such as Accumulo provide scalable and reliable mechanisms for storing and querying Big Data there remains a need to provide enhanced enterprise based solutions that seamlessly but securely integrate with existing enterprise authentication and authorization systems and that enable the enforcement of internal information security policies during database access.

This disclosure describes a method for policy based access control in association with a sorted distributed key value data store in which keys comprise an n tuple structure that includes a key value access control. A representative data store is Accumulo. In this approach an information security policy is used to create a set of pluggable policies each of which may include one or more policy rules. A pluggable policy may be used during data ingest time when data is being ingested into the data store and a pluggable policy may be used during query time when a query to the data store is received for processing against data stored therein. Generally a pluggable policy associates one or more user centric attributes or some function thereof to a particular set of data centric attributes that may be compared to a data centric label. By using pluggable policies preferably at both ingest time and query time the data store is enhanced to provide a seamless and secure policy based access control mechanism in association with the cell level access control enabled by the data store.

In one embodiment a method of access control operates in association with the data store. As data is ingested into the data store one or more key value pairs in the data are tagged with a data centric label as determined by an information policy to generate a tagged representation of the data. The tagged data is then stored in the data store. Then at query time and in response to receipt of a query from a querier typically a set of operations is carried out assuming the query is to be evaluated . First the query is processed according to an information policy to identify a set of one or more data centric attributes to allow the query to use. Preferably the processing evaluates values of one or more user centric attributes associated with the querier against at least one rule in the information policy to identify the set of one or more data centric attributes. Preferably the values of the one or more user centric attributes are retrieved from one or more user attribute data sources as defined in the rule. Based on the processing step the query is then forwarded to the data store with the set of one more identified data centric attributes. A response to the query is then received. The response is generated in the data store in a known manner by evaluating the set of one or more data centric attributes in the query with at least one data centric access label in the data store. The response to the query is then returned to the querier to complete the query time processing.

The foregoing has outlined some of the more pertinent features of the subject matter. These features should be construed to be merely illustrative. Many other beneficial results can be attained by applying the disclosed subject matter in a different manner or by modifying the subject matter as will be described.

Referring back to the system components comprise a data loader component a security component and an analytics component . Generally the data loader component provides integration with a data ingest service such as Apache Flume to enable the system to ingest streaming data feeds such as log files. The data loader can also bulk load JSON CSV and other file formats. The security component provides data centric security at the cell level i.e. each individual key value pair is tagged with a security level . As will be described in more detail below the security component provides a labeling engine that automates the tagging of key value pairs with security labels preferably using policy based heuristics that are derived from an organization s existing information security policies and that are loaded into the labeling engine to apply security labels at ingest time. The security component also provides a policy engine that enables both role based and attribute based access controls. As will also be described the policy engine in the security component allows the organization to transform identity and environmental attributes into policy rules that dictate who can access certain types of data. The security component also integrates with enterprise authentication and authorization systems such as Active Directory LDAP and the like. The analytics component enables the organization to build a variety of analytical applications and to plug existing applications and tools into the system. The analytics component preferably supports a variety of query languages e.g. Lucene custom SQL and the like as well as a variety of data models that enable the storage of data as key value pairs native Accumulo data format as graph data and as JavaScript Object Notation JSON data. The analytics component also provides an application programming interface API e.g. through Apache Thrift. The component also provides real time processing capabilities powered by iterators Accumulo s native server side mechanism and an extensible indexing framework that indexes data upon.

The Accumulo database provides a sorted distributed key value data store in which keys comprises a five 5 tuple structure row controls atomicity column family controls locality column qualifier controls uniqueness visibility label controls access and timestamp controls versioning . Values associated with the keys can be text numbers images video or audio files. Visibility labels are generated by translating an organization s existing data security and information sharing policies into Boolean expressions over data centric attributes. In Accumulo a key value pair may have its own security label that is stored under the column visibility element of the key and that when present is used to determine whether a given user meets security requirements to read the value. This cell level security approach enables data of various security levels to be stored within the same row and users of varying degrees of access to query the same table while preserving data confidentiality. Typically these labels consist of a set of user defined attributes that are required to read the value the label is associated with. The set of attributes required can be specified using syntax that supports logical combinations and nesting. When clients attempt to read data any security labels present in a cell are examined against a set of attributes passed by the client code and vetted by the security framework. Interaction with Accumulo may take place through a query layer that is implemented via a Java API. A typical query layer is provided as a web service e.g. using Apache Tomcat .

Referring back to and according to this disclosure the labeling engine automates the tagging of key value pairs with security labels e.g. using policy based heuristics. As will be described in more detail below these labeling heuristics preferably are derived from an organization s existing information security policies and they are loaded into the labeling engine to apply security labels preferably at the time of ingest of the data . For example a labeling heuristic could require that any piece of data in the format of xxx xx xxxx receive a specific type of security label e.g. ssn or ssn sensitive . The policy engine as will be described in more detail below as well provides both role based and attribute based access controls. The policy engine enables the enterprise to transform identity and environmental attributes into policy rules that dictate who can access certain types of data. For example the policy engine could support a rule that data tagged with a certain data centric label can only be accessed by current employees during the hours of 9 5 and who are located within the United States. Another rule could support a rule that only employees who work for HR and who have passed a sensitivity training class can access certain data. Of course the nature and details of the rule s are not a limitation.

The process for applying these security labels to the data and connecting the labels to a user s designated authorizations is now described. The first step is gathering the organization s information security policies and dissecting them into data centric and user centric components. As data is ingested the labeling engine tags individual key value pairs with data centric visibility labels that are preferably based on these policies. Data is then stored in the database where it is available for real time queries by the operational application s . End users are authenticated and authorized to access underlying data based on their defined attributes. For example as an end user performs an operation e.g. performs a search via the application the security label on each candidate key value pair is checked against the set of one or more data centric attributes derived from the user centric attributes and only the data that he or she is authorized to see is returned.

At query time and in response to receipt of a query from a querier the query processing engine calls out to the security policy engine to determine an appropriate set of data centric attributes to allow the query to use if the query is to be passed onto the Accumulo database for further evaluation. The query received by the query processing engine may include a set of one or more data centric attributes specified by the querier or the query may not have specified data centric attributes associated therewith. Typically the query originates from a human at a shell command prompt or it may represent one or more actions of a human conveyed by an application on the human s behalf. Thus as used herein a querier is a user an application associated with a user or some program or process. According to this disclosure the security policy engine supports one or more pluggable policies that are generated from information security policies in the organization. When the query processing engine receives the query with or without the data centric labels it calls out to the security policy engine to obtain an appropriate set of data centric attributes to include with the query assuming it will be passed based on these one or more policies . As further illustrated in during this call out process the security policy engine in turn may consult with any number of sources for values of user centric attributes about the user based on the one or more pluggable policies supported by the security policy engine. If the query is permitted by the query processing engine to proceed the query together with the one or more data centric attributes then is provided by the query processing engine to the scanning and enforcement engine in the NoSQL database. The scanning and enforcement engine then evaluates the set of one or more data centric attributes in the query against one or more data centric access controls the visibility labels to determine whether read access to a particular piece of information in the database is permitted. This key value access mechanism provided by the scanning and enforcement engine is a conventional operation.

The query processing engine typically operates in one of two use modes. In one use case shown in the query received by the query processing engine includes one or more specified data centric attributes that the querier would like to use in this example L1 L3 . Based on the configured policy or policies the query processing engine determines that the query may proceed with this set or perhaps some narrower set of data centric attributes and thus the query is passed to the scanning and processing engine as shown. In the alternative and as indicated by the dotted portion the query processing engine may simply reject the query operation entirely e.g. if the querier is requesting more access than they would otherwise properly be granted by the configured policy or policy. illustrates a second use case wherein the query does not included any specified data centric attributes. In this example once again the query processing engine calls out to the security policy engine which in turn evaluates the one or more configured policies to return the appropriate set of data centric attributes. In this scenario in effect the querier is stating it wants all of his or her entitled data centric attributes e.g. labels L1 L6 to be applied to the query if this is permitted the query includes these labels and is once again passed to the scanning and processing engine.

In the pluggable policy enforces a rule that grants access to the data centric label PII if two conditions are met for a given user 1 the user s Active Directory AD group is specified as HR Human Resources and 2 the user s completed courses in an education database EDU indicate that he or she has passed a sensitivity training class. Of course this is just a representative policy for descriptive purposes. During the query processing the policy engine queries those attribute sources which may be local or external and makes in this example the positive determination for this user that he or she meets those qualifications in other words that the policy rule evaluates true . As a result the security policy engine grants the PII label. The data centric label is then included in the query which is now modified from the original query . If the user does not meet this particular policy rule the query would not include this particular data centric label.

The security policy engine may implement one or more pluggable policies and each such policy may include one or more policy rules. The particular manner in which the policy rules are evaluated within a particular policy and or the particular order or sequence of evaluating multiple policies may be varied and is not a limitation. Typically these considerations are based on the enterprise s information security policies. Within a particular rule there may be a one to one or one to many correspondence between a user centric attribute on the one hand and a data centric attribute or set of attributes on the other. The particular translation from user centric realm to data centric realm provided by the policy rule in a policy will depend on implementation.

The query time policy and the ingest time policy typically include different user centric attribute s to data centric attribute s associations.

The system preferably includes one or more pluggable policies in each of the data labeling engine and the security policy engine although this is not a requirement.

The word pluggable is not intended to be limiting and may extend to fixed or static policies that are hard coded into the engine or policies that are generated dynamically or based on other criteria.

The individual engines identified in the figures need not be standalone module or code components these functions may be integrated in whole or in part.

The key value transform and indexing engine interprets hierarchical document labels and propagates those labels through a document hierarchy. That operation is described in Ser. No. 61 832 454 filed Jun. 7 2013 and assigned to the assignee of this application. That disclosure is incorporated herein by reference. The transform and indexing engine interprets fields in hierarchical documents as field name and visibility authorization label. In JSON these two elements may be parsed out of a single string representing the field. The visibility authorization label detected is translated into the protection mechanism supported by the database using a simple data model. The engine preserves the labels through the field hierarchy such that a field is releasable for a given query only when all of its labeled ancestors are releasable. It transforms hierarchical documents into indexed forms such as forward indices and numerical range indexes such that the index is represented in the database using the data model and the information contained in any given field is protected in the index of the database at the same level as the field.

The above described technique provides many advantages. The approach takes Accumulo s native cell level security capabilities and integrates with commonly used identity credentialing and access management systems such as Active Directory and LDAP. The enterprise based architecture described is useful to securely integrate vast amounts of multi structured data e.g. tens of petabytes onto a single Big Data platform onto which real time discovery search and predictive analytic applications may then be built. The security framework described herein provides an organization with entirely new Big Data capabilities including secure information sharing and multi tenancy. Using the described approach an organization can integrate disparate data sets and user communities within a single data store while being assured that only authorized users can access appropriate data. This feature set allows for improved sharing of information within and across organizations.

The above described architecture may be applied in many different types of use cases. General non industry specific use cases include making Hadoop real time and supporting interactive Big Data applications. Other types of real time applications that may use this architecture include without limitation cybersecurity applications healthcare applications smart grid applications and many others.

The approach herein is not limited to use with Accumulo the security extensions role based and attribute based access controls derived from information policy may be integrated with other NoSQL database platforms. NoSQL databases store information that is keyed potentially hierarchically. The techniques herein are useful with any NoSQL databases that also store labels with the data and provide access controls that check those labels.

Each above described process preferably is implemented in computer software as a set of program instructions executable in one or more processors as a special purpose machine.

Representative machines on which the subject matter herein is provided may be Intel Pentium based computers running a Linux or Linux variant operating system and one or more applications to carry out the described functionality. One or more of the processes described above are implemented as computer programs namely as a set of computer instructions for performing the functionality described.

While the above describes a particular order of operations performed by certain embodiments of the invention it should be understood that such order is exemplary as alternative embodiments may perform the operations in a different order combine certain operations overlap certain operations or the like. References in the specification to a given embodiment indicate that the embodiment described may include a particular feature structure or characteristic but every embodiment may not necessarily include the particular feature structure or characteristic.

While the disclosed subject matter has been described in the context of a method or process the subject matter also relates to apparatus for performing the operations herein. This apparatus may be a particular machine that is specially constructed for the required purposes or it may comprise a computer otherwise selectively activated or reconfigured by a computer program stored in the computer. Such a computer program may be stored in a computer readable storage medium such as but is not limited to any type of disk including an optical disk a CD ROM and a magnetic optical disk a read only memory ROM a random access memory RAM a magnetic or optical card or any type of media suitable for storing electronic instructions and each coupled to a computer system bus. The functionality may be built into the name server code or it may be executed as an adjunct to that code. A machine implementing the techniques herein comprises a processor computer memory holding instructions that are executed by the processor to perform the above described methods.

While given components of the system have been described separately one of ordinary skill will appreciate that some of the functions may be combined or shared in given instructions program sequences code portions and the like.

Preferably the functionality is implemented in an application layer solution although this is not a limitation as portions of the identified functions may be built into an operating system or the like.

The functionality may be implemented with any application layer protocols or any other protocol having similar operating characteristics.

There is no limitation on the type of computing entity that may implement the client side or server side of the connection. Any computing entity system machine device program process utility or the like may act as the client or the server.

While given components of the system have been described separately one of ordinary skill will appreciate that some of the functions may be combined or shared in given instructions program sequences code portions and the like. Any application or functionality described herein may be implemented as native code by providing hooks into another application by facilitating use of the mechanism as a plug in by linking to the mechanism and the like.

More generally the techniques described herein are provided using a set of one or more computing related entities systems machines processes programs libraries functions or the like that together facilitate or provide the described functionality described above. In a typical implementation a representative machine on which the software executes comprises commodity hardware an operating system an application runtime environment and a set of applications or processes and associated data that provide the functionality of a given system or subsystem. As described the functionality may be implemented in a standalone machine or across a distributed set of machines.

The platform functionality may be co located or various parts components may be separately and run as distinct functions in one or more locations over a distributed network .

