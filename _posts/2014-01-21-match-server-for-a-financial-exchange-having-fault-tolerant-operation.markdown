---

title: Match server for a financial exchange having fault tolerant operation
abstract: Fault tolerant operation is disclosed for a primary match server of a financial exchange using an active copy-cat instance, a.k.a. backup match server, that mirrors operations in the primary match server, but only after those operations have successfully completed in the primary match server. Fault tolerant logic monitors inputs and outputs of the primary match server and gates those inputs to the backup match server once a given input has been processed. The outputs of the backup match server are then compared with the outputs of the primary match server to ensure correct operation. The disclosed embodiments further relate to fault tolerant failover mechanism allowing the backup match server to take over for the primary match server in a fault situation wherein the primary and backup match servers are loosely coupled, i.e. they need not be aware that they are operating in a fault tolerant environment. As such, the primary match server need not be specifically designed or programmed to interact with the fault tolerant mechanisms. Instead, the primary match server need only be designed to adhere to specific basic operating guidelines and shut itself down when it cannot do so. By externally controlling the ability of the primary match server to successfully adhere to its operating guidelines, the fault tolerant mechanisms of the disclosed embodiments can recognize error conditions and easily failover from the primary match server to the backup match server.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09336087&OS=09336087&RS=09336087
owner: Chicago Mercantile Exchange Inc.
number: 09336087
owner_city: Chicago
owner_country: US
publication_date: 20140121
---
This application is a continuation under 37 C.F.R. 1.53 b of U.S. patent application Ser. No. 13 852 679 filed Mar. 28 2013 now U.S. Pat. No. 8 656 210 which is a continuation under 37 C.F.R. 1.53 b of U.S. patent application Ser. No. 13 227 774 filed Sep. 8 2011 now U.S. Pat. No. 8 433 945 which is a continuation under 37 C.F.R. 1.53 b of U.S. patent application Ser. No. 12 560 029 filed Sep. 15 2009 now U.S. Pat. No. 8 041 985 which is a continuation in part under 37 C.F.R. 1.53 b of U.S. patent application Ser. No. 12 188 474 filed Aug. 8 2008 now U.S. Pat. No. 7 694 170 which is a continuation under 37 C.F.R. 1.53 b of U.S. patent application Ser. No. 11 502 851 filed Aug. 11 2006 now U.S. Pat. No. 7 434 096 the entire disclosures of which are hereby incorporated by reference.

Fault Tolerance is generally regarded as the ability to mask or recover from erroneous conditions in a system once an error has been detected. Fault tolerance is typically required for mission critical systems applications. Mission critical typically refers to any indispensable operation that cannot tolerate intervention compromise or shutdown during the performance of its critical function e.g. any computer process that cannot fail during normal business hours. Exemplary mission critical environments include business essential process control finance health safety and security. These environments typically monitor store support and communicate data that cannot be lost or corrupted without compromising their core function.

One exemplary environment where fault tolerance is essential is in financial markets and in particular electronic financial exchanges. The systems that implement an electronic exchange receive and match orders and otherwise consummate trades so as to implement the marketplace and facilitate the exchanges therein. Consistent reliable operation is critical to ensuring market stability reliability and acceptance.

Fault tolerant typically describes a computer system or component designed so that in the event that a component fails a backup component or procedure can take its place with substantially little or no loss of service. Fault tolerance may be provided with software or embedded in hardware or provided by some combination. For example in a software implementation the operating system may provide an interface that allows a programmer to checkpoint critical data at pre determined points within a transaction. In a hardware implementation the programmer need not be aware of the fault tolerant capabilities of the machine. For example at a hardware level fault tolerance may be achieved by duplexing each hardware component e.g. disks are mirrored multiple processors are lock stepped together and their outputs are compared for correctness etc. When an anomaly occurs the faulty component is determined and taken out of service but the machine continues to function as usual.

The level of fault tolerance that is required is typically defined by the needs of the system requirements i.e. specifications that clearly state acceptable behavior upon error e.g. do errors need to be detected and corrected or merely detected and how quickly must such actions be taken 

One method of providing fault tolerance to a system is to add redundancy to one or more of the critical components of the system. Redundant describes computer or network system components such as fans hard disk drives servers operating systems switches and or telecommunication links that are installed to back up primary resources in case they fail. Three types of redundancy schemes are commonly used for this purpose 

One plus one 1 1 This is similar to the one for one scheme except that in the case of one plus one traffic is transmitted simultaneously on both active and standby components. Traffic is generally ignored on the standby. An example of one plus one redundancy is the 1 1 SONET SDH APS scheme that avoids loss of data traffic caused by link failure.

When providing redundant operation for processing components voting logic may be used to compare the results of the redundant logic and choose which component is correct. For example in Triple Mode Redundancy three redundant components may be provided wherein if the result of one component fails to match the other two which match each other the ultimate result will be that of the two components that matched.

A well known example of a redundant system is the redundant array of independent disks RAID . RAID originally redundant array of inexpensive disks is a way of storing the same data in different places thus redundantly on multiple hard disks. By placing data on multiple disks I O input output operations can overlap in a balanced way improving performance. Since multiple disks increases the mean time between failures MTBF storing data redundantly also increases fault tolerance. A RAID appears to the operating system to be a single logical hard disk. RAID employs the technique of disk striping which involves partitioning each drive s storage space into units ranging from a sector 512 bytes up to several megabytes. The stripes of all the disks are interleaved and addressed in order. In a single user system where large records such as medical or other scientific images are stored the stripes are typically set up to be small perhaps 512 bytes so that a single record spans all disks and can be accessed quickly by reading all disks at the same time. In a multi user system better performance requires establishing a stripe wide enough to hold the typical or maximum size record. This allows overlapped disk I O across drives.

Similar to RAID RAIN also called channel bonding redundant array of independent nodes reliable array of independent nodes or random array of independent nodes is a cluster of nodes connected in a network topology with multiple interfaces and redundant storage. RAIN is used to increase fault tolerance. It is an implementation of RAID across nodes instead of across disk arrays. RAIN can provide fully automated data recovery in a local area network LAN or wide area network WAN even if multiple nodes fail. A browser based centralized secure management interface facilitates monitoring and configuration from a single location. There is no limit to the number of nodes that can exist in a RAIN cluster. New nodes can be added and maintenance conducted without incurring network downtime. RAIN originated in a research project for computing in outer space at the California Institute of Technology Caltech the Jet Propulsion Laboratory JPL and the Defense Advanced Research Projects Agency DARPA in the United States. The researchers were looking at distributed computing models for data storage that could be built using off the shelf components.

The idea for RAIN came from RAID redundant array of independent disks technology. RAID partitions data among a set of hard drives in a single system. RAIN partitions storage space across multiple nodes in a network. Partitioning of storage is called disk striping. Several patents have been granted for various proprietary versions of RAIN.

In databases and processing systems especially stateful processing systems which store or accumulate state as they continue to process or transact redundancy presents additional complications of ensuring that the redundant component is synchronized with the primary component so as to be ready to take over should the primary component fail.

A Hot Standby HS is a mechanism which supports non disruptive failover of database server system maintaining system availability i.e. its ability to provide desired service when required by a second server system ready to take over when the main system is unavailable. In the hot standby replication scheme servers usually have two different roles the first of which is a primary server and the second a secondary backup slave server. The hot standby configuration provides a way for a secondary database to automatically maintain a mirror image of the primary database. The secondary database on the secondary server is usually of read only type and it is logically identical to the primary database on the primary server. In case a failure occurs in the primary server the secondary server can take over and assume the role of a new primary server.

There are several methods for achieving high availability in computer systems that contain databases. One known way to carry out continuous hot standby is to mirror the entire system i.e. databases and the applications that use the database. All operations of the system are performed on both applications of the system. The applications write each transaction to their respective databases so both systems are completely synchronized at all times. To ensure that the applications and their databases are mutually in synchronization typically a mechanism called application checkpointing is used. After each executed operation the application ensures by some means that the other application has executed the same operation. In other words the secondary database in association with the secondary application precisely mirrors the primary database and application. The application level mirroring is a good choice for real time applications where everything including the application processes need to be fault tolerant.

The primary process actually performs the work and periodically synchronizes a backup process with the primary process using checkpointing techniques. With prior known checkpointing techniques the primary sends messages that contain information about changes in the state of the primary process to the backup process. Immediately after each checkpoint the primary and backup processes are in the same state.

In other prior known checkpointing methods distinctions between operations that change state such as write operations and operations that do not change the state such as read operations are not made and all operations are checkpointed to the backup process. Such a system is shown in U.S. Pat. No. 4 590 554 Glazer Parallel Computer Systems where all inputs to the primary are provided via messages and all messages sent to the primary are made available to the secondary or backup essentially allowing the backup to listen in on the primary s messages. Another such system is described in and U.S. Pat. No. 5 363 503 Gleeson Unisys Corporation where checkpointing is provided as described in U.S. Pat. No. 4 590 554.

Other prior art such as that shown in U.S. Pat. No. 4 228 496 Katzman Tandem Computers describe that the primary receives a message processes the message and produces data. The produced data is stored in the primary s data space thereby changing the primary s data space. The change in the primary s data space causes a checkpointing operation of the data space to be made available to the backup. Thus there is frequent copying of the primary s data space to the backup s data space which uses a significant amount of time and memory for transferring the state of the primary to the backup. It may also result in the interruption of service upon failure of the primary. The overhead for such checkpointing methods can have considerable performance penalties.

Other prior art examples attempt to update only portions of the state of the primary that has changed since the previous update but use complex memory and data management schemes. In others as shown in U.S. Pat. No. 5 621 885 Del Vigna Tandem Computers the primary and backup which run on top of a fault tolerant runtime support layer that is an interface between the application program and operating system are resident in memory and accessible by both the primary and backup CPUs used in the described fault tolerance model. The primary and backup processes perform the same calculations because they include the same code.

U.S. Pat. No. 6 954 877 discloses a system and method for checkpointing a primary computer process to a backup computer process such that if there is a failure of a primary process the backup process can takeover without interruption. In addition upgrades to different version of software or equipment can take place without interruption. A lightweight checkpointing method is disclosed that allows checkpointing of only external requests or messages that change the state of the service instance thereby reducing the overhead and performance penalties.

In particular a computing system provides a mechanism for checkpointing in a fault tolerant service. The service is made fault tolerant by using a process pair the primary process performs the work officially while one or more backup processes provide a logical equivalent that can be used in the event of failure. The primary and backup are allowed to be logically equivalent at any given point in time but may be internally different physically or in their implementation.

Implementation of checkpointing mechanisms requires lots of work from the application programmers as the application checkpointing mechanism is a difficult task to implement. Another method for processing hot standby replication operations is to create a transaction log of the operations of a transaction run in the primary server transfer the log to the secondary server and run serially the transferred transaction log on the secondary server. This log is a record of all data items that have been inserted deleted or updated as a result of processing and manipulation of the data within the transaction. The data needs to be written to both databases before it can be committed in either of the databases. This ensures that data is safely stored in the secondary server before the primary server sends acknowledgement of successful commit to the client application. An example of this kind of data mirroring system is described in the U.S. Pat. No. 6 324 654 where A primary mirror daemon on a local computer system monitors the writelog device redundant data storage or memory device for data updates and feeds the data over a network in the same order in which it is stored to a receiving remote mirror daemon on a remote computer system which in turns commits the data updates to a mirror device. In a situation of a failure recovery these primary and secondary mirror daemons transfer the log to the secondary node where the log is run just as it was in the primary node. The replicated operations are run serially in the secondary node which slows down processing speed and hence reduces overall performance.

Still another mechanism for achieving database fault tolerance is to have an application connect to two databases. Whenever the application executes an application function it commits the related data changes to both servers. To ensure that the transaction is committed in both databases the application typically needs to use so called two phase commit protocol to ensure the success of the transaction in both databases. If the transaction fails in either of the databases it needs to fail also in the other databases. Using two phase commit protocol needs to be done in the application which makes the application code more complex. Moreover distributed transactions are quite common cause to performance problems as the transaction cannot be completed before both databases acknowledge the transaction commit. In this scenario recovery from error situations can also be very difficult.

Still another way for processing hot standby replication operations is to copy the transaction rows to the secondary node after they have been committed on the primary node. This method is a mere copying procedure where transactions are run serially in the secondary node. This method is known as asynchronous data replication. This method is not always suitable for real time database mirroring because all transactions of the primary database may not yet be executed in the secondary database when the fail over from primary to secondary happens.

Many database servers are able to execute concurrent transactions in parallel in an efficient manner. For example the server may execute different transactions on different processors of a multi processor computer. This way the processing power of the database server can be scaled up by adding processors to the computer. Moreover parallel execution of transactions avoid blocking effect of serially executed long running transactions such as creating an index to a large table. To ensure integrity of the database some concurrency control method such as locking or data versioning needs to be used to manage access to data that is shared between transactions. If two transactions try to have write access to the same data item simultaneously and versioning concurrency control is in use the server either returns a concurrency conflict error to one of the transactions and the application needs to re attempt executing the transaction later. If locking concurrency control is in use the server makes one of the transactions wait until the locked resources are released. However in this scenario it is possible that a deadlock condition where two transactions lock resources from each other occurs and one of the transactions must be killed to clear the deadlock condition. The application that tried to execute the killed transaction must handle the error e.g. by re attempting execution of the transaction.

These concurrency control methods known in the prior art are suitable for use in the primary server of the Hot Standby database configuration to manage concurrent online transactions of client applications but they cannot be applied in the secondary server of the system. This is because the concurrency conflict errors cannot be allowed in the secondary server as there is no way to properly handle these error conditions. Because of the absence of a proper Hot Standby concurrency control method in the prior art replicated hot standby operations are run substantially in a serial form in the secondary node. Because operations cannot be executed in parallel it is difficult to improve secondary server s performance without raising problems in data integrity and transaction consistency. Essentially a mechanism is needed that allows transactions to run parallel but that ensures that transactions are not started too early and they are committed before dependent transactions are started.

U.S. Pat. No. 6 978 396 discloses a mechanism to run transaction operations originating from a primary server used to replicate data in parallel in a secondary server and relates to running concurrent or parallel operations in a secondary server for redundancy recovery and propagated transactions. According to the disclosure executing parallel operations in a secondary server improves performance and availability and how it maintains transaction order and output congruent with the primary server where transaction operations are originated. A set of specific rules is determined. The specific rules are defined on basis of a first timestamp and second timestamp attached to each transaction in the primary server and the rules form a timestamp criteria . When a transaction meets this timestamp criteria it can be run in parallel with other transactions met the same criteria in the secondary server in accordance with the instructions set in the specific rules to maintain the transaction order and output correct.

As can be seen implementation of fault tolerance in complex processing systems requires complex logic to ensure that the redundant components are synchronized with the primary component so that the backup component is ready to take over should the primary component fail.

Accordingly there is a need for a simplified mechanism for providing fault tolerance which reduces the complexities related to ensuring that the redundant component is ready to take over for a filed primary component.

The disclosed embodiments relate to providing fault tolerant operation for a primary instance such as a process thread application processor etc. using an active copy cat instance a.k.a. backup instance that mirrors operations in the primary instance but only after those operations have successfully completed in the primary instance. The disclosed fault tolerant logic monitors inputs and outputs of the primary instance and gates those inputs to the backup instance once a given input has been processed. The outputs of the backup instance are then compared with the outputs of the primary instance to ensure correct operation. The disclosed embodiments further relate to fault tolerant failover mechanism allowing the backup instance to take over for the primary instance in a fault situation wherein the primary and backup instances are loosely coupled i.e. they need not be aware of each other or that they are operating in a fault tolerant environment. As such the primary instance need not be specifically designed or programmed to interact with the fault tolerant mechanisms. Instead the primary instance need only be designed to adhere to specific basic operating guidelines and shut itself down when it cannot do so. By externally controlling the ability of the primary instance to successfully adhere to its operating guidelines the fault tolerant mechanisms of the disclosed embodiments can recognize error conditions and easily failover from the primary instance to the backup instance.

To clarify the use in the pending claims and to hereby provide notice to the public the phrases at least one of . . . and or at least one of . . . or combinations thereof are defined by the Applicant in the broadest sense superseding any other implied definitions herebefore or hereinafter unless expressly asserted by the Applicant to the contrary to mean one or more elements selected from the group comprising A B . . . and N that is to say any combination of one or more of the elements A B . . . or N including any one element alone or in combination with one or more of the other elements which may also include in combination additional elements not listed.

The primary instance is coupled with a database for the purpose of storing transaction data related to the function s performed by the primary instance . Herein the phrase coupled with is defined to mean directly connected to or indirectly connected through one or more intermediate components. Such intermediate components may include both hardware and software based components. As will be described the primary instance is programmed to complete a transaction with the database for each input it receives for processing according to its function s and before it can generate and transmit an output such as a response or acknowledgement in response to the input. Should the primary instance be unable to complete the database transaction it will internally fail and shut itself down. In one exemplary embodiment the primary instance must log data such as regulatory or audit related data regarding each received input to a table in the database .

A transaction typically refers to a sequence of information exchange and related work such as database updating that is treated as a unit for the purposes of satisfying a request and for ensuring database integrity. For a transaction to be completed and database changes to made permanent a transaction has to be completed in its entirety. A typical transaction is a catalog merchandise order phoned in by a customer and entered into a computer by a customer representative. The order transaction involves checking an inventory database confirming that the item is available placing the order and confirming that the order has been placed and the expected time of shipment. If we view this as a single transaction then all of the steps must be completed before the transaction is successful and the database is actually changed to reflect the new order. If something happens before the transaction is successfully completed any changes to the database must be kept track of so that they can be undone.

A program that manages or oversees the sequence of events that are part of a transaction is sometimes called a transaction manager or transaction monitor. In one embodiment transactions are supported by Structured Query Language a standard database user and programming interface. When a transaction completes successfully database changes are said to be committed when a transaction does not complete changes are rolled back i.e. partly completed database changes are undone when a database transaction is determined to have failed. In IBM s Customer Information Control System product a transaction is a unit of application data processing that results from a particular type of transaction request. In CICS an instance of a particular transaction request by a computer operator or user is called a task. A commit is the final step in the successful completion of a previously started database change as part of handling a transaction in a computing system.

In one embodiment the database is an ACID compliant database. ACID atomicity consistency isolation and durability is an acronym and mnemonic device for learning and remembering the four primary attributes ensured to any transaction by a transaction manager. These attributes are 

As was discussed above the primary instance is further operative to generate one or more responses outputs and or acknowledgements for each input it receives. As will be described below the primary instance s dependence on completing an external transaction with the database and its operation to generate at least one response or output for each input received confirming the successful processing thereof is utilized by the disclosed fault tolerant mechanisms described herein to detect faults and failover to the backup instance when necessary. It will be appreciated that the disclosed embodiments may utilize any operationally dependent external transaction and any indicator which confirms the processing of a given input of the primary instance to accomplish the disclosed functionality.

The system further includes fault tolerant logic . The components of the fault tolerant logic will be described with reference to their functionality as shown in which depict flow charts showing the operation of a fault tolerant system of according to one embodiment. The fault tolerant logic may be implemented in hardware software or a combination thereof and further may include computer program logic processes threads or combinations thereof which interact with the primary and backup instances as well as the database and implement the functionality described herein. The fault tolerant logic may execute on the same logical partitions servers or processors as the primary and or backup instances or on a separate server or processor and interconnected with the primary and backup instances via suitable means such as a network or other interconnect.

In particular the fault tolerant logic includes an input receiver coupled between the network and the input to the backup instance primary failure detection logic coupled with the outputs of the primary instance the input receiver and the database and backup failure detection logic coupled with the network not shown and output matching logic . The input receiver receives copies of the input s which should have also been received by the primary instance from the network . Of course if there is a communication or network failure with the primary instance the receipt of the input s by the fault tolerant logic will detect the fault as will be described. The input receiver buffers the input s and gates them to the backup instance as will be described. The primary failure detection logic monitors the output s of the primary instance and as will be described determines if the primary instance has failed. If the primary failure detection logic determines that the primary instance has failed the primary failure detection logic also acts to shut down the primary instance and fail over to the backup instance determining whether there are unprocessed input s that the primary instance failed process and then causing the backup instance to take over normal operations from the primary instance . The backup failure detection logic monitors the output s of both the primary and backup instances for a given input s and determines whether they match or not. In one embodiment a mismatch triggers a fault in the backup instance . In an alternate embodiment a mismatch triggers a fail over from the primary instance to the backup instance as described herein. The backup failure detection logic also checks the network connectivity of the backup instance and determines a fault in the backup instance when the network connectivity of the backup instance has been determined to have failed.

In operation of the system input s are received by or at least transmitted to the primary instance such as via a network . The input s may be received from or transmitted by other entities also coupled with the network and or they may be generated by the primary instance itself. As will be described the fault tolerant logic may also generate input s to the primary instance to determine whether or not the primary instance is operating correctly. The network may include one or more input busses public or private wired or wireless networks or combinations thereof and may further feature security or authentication protocols as well as error detection and correction protocols. In one embodiment the network implements the TCP IP protocol suite. It will be appreciated that any network protocol and communications technology may be used with the disclosed embodiments. A copy of the input s is also received by an input receiver of the fault tolerant logic the receipt of which may occur substantially simultaneously with the presumed receipt thereof by the primary instance or within a acceptable margin thereof depending upon the implementation. In one embodiment inputs are multicast on the network to both the primary instance and the fault tolerant logic . Multicast is communication between a single sender and multiple receivers on a network. It will be appreciated that multiple inputs may be multicast to the primary instance and the fault tolerant logic and that due to the implementation of the network the input s may be received by the fault tolerant logic in a different order and or at a different time than they are received by the primary instance . In one embodiment the network includes an order entry bus of a match server of a trading engine used by a financial exchange. It is a feature of the disclosed embodiments that the order of receipt of the input s does not matter. As will be described the input s received by the fault tolerant logic are buffered by the input receiver and gated to the backup instance under the control of the fault tolerant logic . In this way as will be described in more detail below the fault tolerant logic surrounds the backup instance to ensure synchronization with the primary instance without requiring that the backup instance be aware of the fault tolerant logic external thereto.

As shown in under normal operating conditions as input s block are received by the primary instance over the network block the primary instance processes the input s according to its programmed function e.g. matches trader order inputs to consummate trades in a financial exchange. At the completion of or during processing the primary instance attempts to transact with the database e.g. to store input related data such as audit data or transactional data related to the receipt and or processing of the input s shown in more detail in blocks which depicts a flow chart showing the operation of a database for use with the fault tolerant system of according to one embodiment. If the transaction with the database fails for reasons other than a constraint violation not shown in the primary instance may retry the transaction until it is successful or until a threshold of successive failures has occurred. If the database transaction is successful blocks of the primary instance generates and transmits one or more responses outputs and or acknowledgements as dictated by its program function such as to the originator of the input s . For example where the primary instance is a match server the input s may include trade orders and the response s thereto may include acknowledgements of the receipt of the orders and confirmation of trade execution. In addition to being transmitted to its intended recipient the output s of the primary instance are also copied to the matching logic of the fault tolerant logic . The matching logic includes a buffer and a comparator . The buffer holds the output s of the primary instance until the corresponding output s of the backup instance are transmitted by the backup instance for a given input s . The output s of the primary and backup instances for a given input s are then compared by the comparator and the results thereof are reported to the backup failure detection logic . It will be appreciated that the comparator may determine an exact match between the compared outputs or a substantial match there between. Further the comparator may only compare a portion of the output s a hash value or checksum or combinations thereof.

In one embodiment the primary instance implements temporal functionality i.e. performs functions on the inputs based on time parameters such as the current clock time. For example the primary instance may compare the time that an input was generated with the time that the input reaches the primary instance . Where a given input represents an order this functionality may be utilized to determine if the order has expired. As shown in the primary instance includes a clock input which is coupled with a clock such as a clock circuit or other device operative to provide an indication of time. The backup instance also features a clock input which similarly receives a time input for performing the particular temporal functions on the copies of the inputs. However as noted elsewhere the backup instance necessarily processes the inputs later in time than the primary instance . If the backup instance also used the current time as an input it may not end up with the same result as the primary instance . Accordingly once the primary instance has completed its processing and generated an output result based thereon it includes data representative of the particular time used in its processing in the output. This data is then extracted by the input receiver and provided to the clock input of the backup instance along with the corresponding input thereby providing the backup instance with the equivalent time data as used by the primary instance even if the real time is much later. It will be appreciated that this time data may be specified as an absolute time or a relative or elapsed time period. This ensures that the backup instance accurately produces the same results with respect to temporal based functions.

Should the database transaction fail due to a constraint violation blocks of the primary instance will enter a failure state block . As will be described in more detail below the fault tolerant logic is capable of forcing a constraint violation to be returned by the database to the primary instance so as to force the primary instance into a failure state. This is done for the purpose of disabling the primary instance so that the backup instance can take over in fault situations in effect taking advantage of the primary instance s own internal fault handling mechanisms. It will be appreciated that causing a constraint violation in response to a database transaction by the primary instance is one example of a mechanism for interrupting or inhibiting the primary instance from completing an operationally dependent external transaction and that other such mechanisms may be available and are dependent upon the implementation of the primary instance . Further while primary instance could also be externally terminated such as by killing the process removing power from the executing server etc. forcing the primary instance to self terminate or fail soft allows for a cleaner exit i.e. resources in use by the primary instance such as allocated memory registers stack space etc. can be returned to the operating system and any ambiguous states such as uncommitted or unresolved transactions can be resolved. Further by forcing the primary instance to fail rather than simply cutting off the primary instance from communicating or interacting the continued consumption of resources by the primary instance and the subsequent effects on other processes etc. can be mitigated. In addition by utilizing the return of a constraint violation to the primary instance the primary instance is guaranteed to be halted at a known point and or in a known at least external state and that the sequenced set of inputs that have been processed by the primary instance prior to failure can be known or discovered by the fault tolerant logic even if the primary instance is unreachable or otherwise in an inconsistent state.

In an alternative embodiment inputs or transactions may be differentiated based on whether their completion is critical or not. In particular functionality may be provided which allows differentiation between database operations the completion of which should be waited on and operations that could still be in flight or in process while other processing continued e.g. while messages were returned to the customer. For example persisting a rejected order involves three calls one to the fault tolerance table message sequence one per message one to the order history table and one to the orders table. Only the first call need be waited on. If the engine were to fail after sending the response to the message but prior to successfully persisting the orders and order history call the impact would be that one rejected order was not logged. This is considered not significant and by determining which persistence operations had business significance in the face of our policy on failures we could take many database transactions out of the round trip time. This is most obvious when looking at orders. Good Till Cancel and Good Till Date orders GTC GTD are orders that are guaranteed to be working regardless of exchange or engine failures. As a result we must guarantee they are persisted prior to replying to the message. By CME policy Day orders and GTC orders for the current day but not other GTC orders or GTD orders are eliminated on exchange failure and we need not wait on them. Accordingly the primary instance backup instance may be implemented to distinguish between two different types of inputs those for which the database operation must complete before generating an output and those for which the database operation need not complete and therefore the output can be generated prior to completion. As shown in in parallel with attempting store the transaction Blocks for those inputs which do not require waiting the processing may continue to transmit the Acknowledgments Responses Blocks without waiting for the database to report on the completion of the store Blocks . It will be appreciated that the primary and backup instances may be suitably programmed to identify those transactions which depend upon completion of the database operation and those that do not.

It will be appreciated that the fault tolerant logic does not actually need to shut down the primary instance but simply needs to guarantee that the primary instance will not send any more operations to the outside world thereby conflicting with the backup instance that is taking over. In particular in the disclosed embodiments the fault tolerant logic assumes the worst case scenario wherein the primary instance cannot be contacted killed or otherwise directly impacted. By blocking the primary instance from completing an operationally dependent external operation such as a database transaction it is guaranteed that the backup instance can take over for a primary instance that has totally disconnected from the network without having to contact that primary instance or attack it in any way directly. Once the database block out is completed it does not matter to the backup instance if the primary instance remains in an unresponsive state or if the primary instance self terminates but when the primary instance discovers the block out via a constraint violation it may as well shut down as it is now a useless process and logging information and orderly shutting down allows those monitoring the process to note the failure and take appropriate restart steps.

As was described above the input s i.e. copies thereof are also received by the input receiver of the fault tolerant logic block . The input receiver buffers the received input s such as in the order of receipt and gates those input s to the backup instance for processing. In particular for a given input e.g. n n 1 n y etc. the input receiver monitors the output s of the primary instance to determine when an output s corresponding to a subsequently received input s e.g. n 1 2 x is transmitted by the primary instance block . When this occurs the input receiver sends one or more of the given prior input s e.g. n x y to the backup instance for processing block . In this way the backup instance is always processing behind but in step with the primary instance . Further the receipt of an output for a subsequently received input in one embodiment ensures that the input waiting for processing by the backup instance is currently processing or has already been successfully processed by the primary instance . The backup instance then processes the input s in the same manner as the primary instance described above blocks . However in circumstances where the primary instance is operating normally and no faults have been detected the backup instance is prevented from interacting with the database and instead interacts with database mimic logic which mimics and returns a successful database transaction result back to the backup instance . In an alternative embodiment the backup instance may be programmed so as not to attempt interaction with the database thereby eliminating the need for the database mimic logic . Further while the output s of the backup instance are provided to the matching logic to determine if they match with the corresponding output s of the primary instance the backup instance is prevented from otherwise communicating those output s to other entities so as not to interfere with the normal operation of the primary instance . Gating logic under control of the fault tolerant logic controls whether the primary or backup instance is permitted to transmit its output s to external entities such as via the network based on whether there has been a failure detected etc.

As long as the primary and backup instances continue to operate normally processing input s and generating the requisite output s the system operates as described.

As long as no faults have been detected as will be described the system remains in a normal operating state with the primary instance operating and the backup instance lagging behind in step with the primary instance block . As shown in under normal operating conditions the backup instance output s are suppressed from being communicated while the primary instance output s are allowed by the gating logic to external entities such as over the network . Further the backup instance s access to the database is blocked and successful completion of the backup instance s database transactions are mimicked by the database mimic logic or alternatively the backup instance is programmed to not interact with the database .

On each given iteration of the process flow which may be determined by a clock counter event or other trigger a determination is made as to whether or not the primary instance has transmitted an output block . If one or more outputs have been received it is determined as was described above whether the outputs correspond to one or more inputs received subsequent to other prior received inputs block such that the prior received inputs should have been processed by the primary instance and therefore can then be sent to the backup instance for processing block . Further the received output s are buffered pending receiving the corresponding output s from the backup instance blocks . Once the corresponding output s are received from the backup instance they are compared with the output s of the primary instance block . If they match processing continues. If they do not match in one embodiment a fault in the backup instance is determined and the backup instance is placed into a fail state block . Alternatively a mismatch between the corresponding output s of the primary and backup instances may be determined to reflect a failure of the primary instance triggering the fail over mechanisms described herein.

If there has been no output from the primary instance it is next determined if a threshold time out has elapsed or been exceeded since an output was last detected from the primary instance. In one embodiment a global time out threshold is defined for use by all of the event process flows described herein which may then utilize the threshold or multiples thereof for their comparison operations. Alternatively separate thresholds may be defined for each particular process. In the present embodiment if no output s have been received from the primary instance for a multiple of the time out threshold e.g. 10 times the threshold block the fault tolerant logic generates a heartbeat input to the primary instance which as described above is also copied to the backup instance and triggers the mechanisms herein to force some form of output from the primary instance . This allows the fault tolerant logic to discover whether the lack of activity from the primary instance is due to a failure or is legitimate e.g. it doesn t have any inputs to process. The heartbeat input will also be received by the fault tolerant logic just like any other input that is received and will trigger the other fault detection processes described herein to determine whether the primary instance has actually failed.

Additionally when it has been determined that no output from the primary instance has been received a set of conditions are checked serially as shown in parallel or some other logical arrangement to conclude that the primary instance has failed. In alternative embodiments the conclusion of failure of the primary instance may be reached by other logical means and based on other conditions and or events. In the exemplary embodiment the conditions that are checked include determining that there is an input that was supposedly received by the primary instance as determined by its receipt by the fault tolerant logic that is waiting for an output to be generated block . Again if there is no input to the primary instance there legitimately will be no output therefrom. In addition if there is an input waiting for an output the age of the input is determined and compared with a defined threshold age block . If the input is older than the threshold age then no fault will be determined. This prevents occasionally dropped inputs such as inputs received during startup of the system which may be expected to occur depending on the implementation and the load on the primary instance from being processed by the backup instance and from causing a failover condition. This check may be tailored to accommodate the level of tolerance for faults in the system that are deemed acceptable according to the implementation e.g. the acceptable threshold age may be adjusted and or the frequency of such faults may be measured and compared against an acceptable threshold. In systems which can tolerate absolutely no errors or faults this check may not be implemented.

Further it is determined whether a prior output has been received from the primary instance which indicates that the primary instance was functioning in the past and prevents a fault condition from being determined when the system is first started block . Again in systems which cannot tolerate faults this check may not be implemented. If these conditions are met but only one half of the threshold time out has elapsed or some other portion thereof blocks the network connectivity of the fault tolerant logic and or backup instance is checked. In one embodiment a ping signal is transmitted from the fault tolerant logic or the server it is executing on to a recipient such as the primary instance or the server it is executing on e.g. a ping utility process may be used transmit a test communication designed to determine the state of network connectivity. While processing continues a separate process block awaits a response to the ping which would indicate that network connectivity is okay. If no response is received the backup instance is placed into a fail state due to presumed loss of network connectivity. If the threshold timeout has been exceeded and all of the other conditions have been met a failure of the primary instance is determined block .

As shown in if a failure of the primary instance is determined block the primary instance is failed over to the backup instance . In particular the outputs of the primary instance are suppressed or otherwise inhibited by the gating logic from being communicated while the outputs of the backup instance are permitted to be communicated by the gating logic . In addition the backup instance is permitted to transact with the database . The primary failure detection logic then stores blocking data into the database . The blocking data is configured so as to occupy storage that the primary instance would attempt to also store into. In one embodiment wherein the primary instance is multithreaded the primary failure detection logic is able to handle blocking out N number of asynchronously writing threads that may not be writing in sequence and may be writing continuously attempting to write a block jump ahead on failure . . . etc . The blocking data will cause the database to return a constraint violation to the primary instance should the primary instance still be active and trying to process inputs. As was described the constraint violation should force the primary instance to self fail. Once the primary instance has been blocked the fault tolerant logic determines which inputs need to be processed by analyzing the inputs received by the input receiver that remain unprocessed and also analyzing the database to determine the final actions of the primary instance prior to failure block . These inputs are then sent to the backup instance to be processed block . Normal processing by the backup instance is then started block .

In one embodiment if the backup instance fails it may be prevented from taking over for a failed primary instance . Further another backup instance may be started to take over for the failed backup instance . In the case of failure of either the primary or backup instances alerts may be transmitted to other monitoring programs or processes or to monitoring staff alerting them to the need for intervention.

By following behind during normal operations of the primary instance the backup instance can take care of any unfinished processing by a failed primary instance without having to worry about staying in sync with the primary instance . In operation the disclosed fault tolerant logic detects failures when the primary instance stops operating completely continues processing but fails to send the proper output s or when the corresponding output s of the primary and backup instances for a given input s fail to match.

In one embodiment the primary and backup instances are instances of a match engine for a trading engine of a financial exchange such as the Falcon Trading Engine utilized by the Chicago Mercantile Exchange as shown in . The Match Engine and Database are run on redundant pairs. The system is designed so that any single Match Engine A B or database server can fail without an interruption to trading activity. Falcon Match Engine fault tolerance is handled at the application level by a custom architecture described herein.

The Match Engine has been designed to run in a paired primary backup configuration as described above. Each individual Falcon Match Engine A B is one process with a set of loosely coupled threads. The functionality of a complete match engine process is subdivided among these threads. This allows an easy parallelization of work over separate physical processors and use of lower cost hardware.

Each Falcon instance A. B will go through specific engine states before it is ready to accept new incoming orders. The Match Component is the ultimate authority on engine state and controls all other components in each Falcon Match Engine server. Market schedules are stored in an Admin database and communicated to the Match Server at start up and through Trading calendar Updates during run time . State changes are then scheduled in the Match Component and initiated by the Match Component. After a state change occurs this information is communicated to the rest of the components via an AdminOperation message. The Admin server can also change the match servers current state by sending an AdminOperation. All administrative AdminOperations calls are received by the Gateway Component and sent to the Match Component. Once the Match Component has acknowledged the AdminOperation all other components respond in the same manner.

The main execution path of the match component is a single thread. This thread pulls messages off the internal message bus processes them and sends out responses. As a result only one operation is going on in the match thread at any given time. Regarding persisting in the database many threads may be used asynchronously and order is restored afterwards. As the size of a block is equal to of database threads database batch size the primary failure detection logic as described above is able to handle blocking out N number of asynchronously writing threads that may not be writing in sequence and may be writing continuously attempting to write a block jump ahead on failure . . . etc .

For a given order book in the match thread there is a single allocation algorithm and a configurable amount of overlays. Incoming orders are passed in sequence first to each overlay and then to the allocation algorithm. Each overlay and allocation algorithm then generates appropriate fills. Once the order has passed through each of the above the remainder if any is placed on the book and fill messages are sent out. There are two overlays in Falcon 1.5 which control the order allocation for incoming orders Lead Market Maker LMM and TOP with Min Max . With LMM orders from certain users get preferential allocation say x of every incoming order. In return for preferential allocation those users agree to quote many markets and provide liquidity. With TOP orders that turn the market first order at a better price get preferential allocation over other orders that join it at the same price. Min and Max are modifiers that determine what MIN size an order must be and what MAX allocation it can get before it loses top status. A book can be configured with all overlays some overlays or no overlays at all.

The match thread does not fire timed events such as group open group close . Instead these events are fired by a dedicated thread. Once the event fires it is converted into a message and placed on the match thread s queue like any other message. As a result they are processed one at a time like any other message.

There are only three reasons to persist data in Falcon Recovery Regulatory and Surveillance by the operations staff . Some of this data needs to be transactionally written to disk in line. This means the Match Server must delay sending out responses while the transaction completes.

Additionally one or more logs of business specific data and system specific data are maintained. These logs are not transactional and reside on the local disk. The business specific Log contains all business information that Falcon generates while the System Log contains Falcon technical information which is more suited for system administrators.

As described the Falcon architecture will consist of a primary server A instance and an actively running backup server B instance . The backup B will have the ability to replicate the primary s state exactly and failover with no apparent interruption of service in a reasonably quick amount of time.

The maximum delay it should take for the backup to failover is 7 10 seconds. Quicker times may be attainable but this likely is a decision based on finding the optimal setting that prevents false positive failure detection. The failover time will be a configurable parameter.

The system should never send duplicate messages without marking them as possible duplicates. The system should limit the number of outgoing messages that are marked possible duplicate. The system will only send possible duplicates when caused by failover during normal operation none will be generated.

Falcon Fault Tolerance is based on a concept called Active Copycat as has been described above. In this system both the Backup B and Primary A listen for INPUT messages from the Order Entry bus . In one embodiment the order entry bus is a logical component and there may be multiple order entry busses . In addition the Backup B listens for RESPONSE messages from the primary A. When a RESPONSE for a given INPUT and a response for a subsequently received input are received the Backup B then copies the Primary A by processing the same INPUT and compares the Primary RESPONSE to its own OUTPUT. The Backup server B performs the same actions as the Primary server with the exception of publishing OUTPUT messages and writing to the database . In order for Active CopyCat to work the following requirements on the messaging infrastructure do exist 

In primary mode no fault tolerance specific classes are used. The primary A is unaware whether a backup B exists or not there is no requirement that a backup B need to be run at all and the backup is a passive listener on traffic that would be sent in any case. As a result the described implementation of fault tolerance has no performance impact on the primary running instance.

In the backup B the actual CopyCat logic checking is done by Fault Tolerant FT Message Managers. These objects allow the business logic components of the system Gateway Market Data and Match Server to be mostly ignorant of their primary backup status. Gateway and Market data need to be aware of their status only at startup so that they can instantiate the correct FT Message Manager. The Match Server needs to be aware of status to enable disable writing to the database. Regardless of status each component sends and receives messages as normal it is the responsibility that component s FT Message Manager to sequence inbound messages and suppress output messages in backup mode.

The FT Message Managers are controlled by FT Policy objects one for Order Entry and one for Market Data.

The order entry fault tolerant policy object is responsible for reordering input messages to ensure that they are processed in the same order as the primary A. It also does verification that primary output matches backup output and initiates failover if necessary. All sources of input for the backup falcon B send their messages to the order entry FT policy. This includes the admin server other order entry gateways and internal Falcon timer events. As a result all input events can be reordered to the same sequence processed by the primary.

The backup B compares output it produces with responses received by the primary A. If the comparison fails the backup B will enter a state where it is unable to take over for the primary A going forward. All FIX message fields except for timestamp and checksum fields are hashed by both the primary A and backup B and those hashes are compared. If there is any difference in the messages the backup B will note it and not attempt to take over for the primary A going forward. All important events are logged at an appropriate log level.

In order to detect failures during periods of low market activity if the backup B has not heard from the primary A for a configurable period such as 10 the failure timeout it will generate an input message. The input message is sent to both the primary A and backup B in the same manner as any input from a user. The input message has no effect on the engine but generates an output message from the primary A which is read and processed by the backup B. The net effect is that the backup B will discover any failures with the primary in a reasonable timeframe without having to wait for a user input to trigger failover.

The disclosed match engine allows for order book migration the process of copying an order book from a running primary server A to a newly started backup B without halting the primary. Book migration is used whenever a backup server B starts up whether there is a working book to migrate or not. If there is no primary server A up the backup B will wait until one exists to fully start.

It will be appreciated that the disclosed embodiments support any number of backup instances running at the same time any one of which may used in case a failure occurs. In addition additional backup instances may be utilized for other purposes such as to provide auditing e.g. real time auditing of trader behavior or testing environments or to enable surveillance without impacting the primary instance . For these instances which are not relied upon for the purpose of backing up the primary instance i.e. non takeover backups the requirement of always being one message behind may be relaxed. While the main backup instance must remain a message behind since it has to be able to re process a failed message that may have resulted in partial output the other backup instances used for alternative purposes don t necessarily need to reprocess that failed message they only need to process messages in the same order received by the primary i.e. they do not need to know that the primary completed processing the failed message only that it started to process it.

When a backup B starts up it gets its configuration from the Falcon Admin Server. It then sends a message to the primary server A to check if it is alive and waits for a response. There are two possible situations if the primary A is up it will respond right away. If not as part of the startup behavior the primary A sends a message on startup. This message will be treated as a response and the backup B will know that the primary A is up.

Once the primary A confirmation message has been received the backup B checks to see how much time has elapsed since it received its configuration file from the FAS. If more than three seconds has elapsed the backup reacquires the configuration from the FAS. This helps minimize the risk that the FAS makes a change to engine configuration after the backup B downloaded its configuration file but before the primary A downloaded its configuration file. Should this happen the backup B will detect this at startup and immediately fail. Thus reacquiring the configuration file minimizes the cases of intentional fast fail. 

Following this process the backup sends a state request message to the primary A. The primary A responds with a state aggregate response which contains all the mutable state database sequence numbers outstanding orders host order numbers HON s host trade numbers HTN s etc. The backup B receives and applies this information and is then ready to perform as a backup. In order to fully eliminate the risk of an admin server change not being applied during this process the backup B compares the last sequence numbers received by the primary from the FAS to those it received from the FAS in the configuration object. There is an extremely small risk of the sequence numbers differing but when they do it indicates that there was a user initiated configuration change from the admin server in the few seconds between receipt of the configuration and receipt of the aggregate state. If this condition is detected the backup B shuts down and must be restarted to attempt book migration again it does not indicate a persistent error condition referred to as a fast fail above .

Once up the backup B will queue input from all sources other order entry gateways Admin server internal timer events but will not attempt to detect primary A failure until one failure timeout has elapsed after receipt of the first primary response message. No message received before that time can cause failover.

Primary A startup is fairly simple. The primary A downloads its configuration from the Falcon Admin Server and sends a message on the shared engine bus to determine if there are any other primary engines out there. If it receives a response it shuts down. If not it starts up normally and sends out a message indicating that it has started.

The market data fault tolerant policy has no role other than to suppress the output of market data messages from the backup B. It does not do validation or checking nor does it subscribe to the market data feed from the primary A. All important events are logged at an appropriate log level.

In another embodiment mechanisms are provided to handle a failed database and to prevent such a failure from causing the fault tolerant mechanisms described herein from failing over from a fully functional primary instance . The database itself may represent a single point of failure or delay e.g. in one embodiment the database is implemented using an Oracle RAC which occasionally may delay transactions for many seconds while it reconfigures after an internal failure . As shown in a database switch mechanism is provided. In operation if the primary instance determines that it has not had a response from the database in X seconds the exact value of which is implementation dependent it realizes that the backup instance may be about to take over even though the primary instance is perfectly healthy. The primary instance may then initiate a conversation with the backup instance and coordinate a switch to a standby database . This switch is done without market interruption and must be coordinated with the failover process since during a database switch failover must be disabled since the database is the arbiter of which instance is the primary instance. Once database switch is completed failover is re enabled.

The backup B locks out the primary A and thereby prevents a run away primary by writing blocking data to the message sequence table. Because the primary A cannot send out a message until the input associated with that message is persisted the backup B knows that only messages written prior to the blocking data can be sent by the primary A.

After the backup B which is taking over marks that it is now the primary A in the database it then asks the database for any Input Message Identifiers that the backup may have missed the primary processing by stating the last Input ID that it saw . The backup B will then run those Inputs in the same sequence as the primary A processed them. The database procedure will move those entries from the Order History table to another Failover Order History table and from the Trades table to another Failover Trades table since the database knows that all orders entered after the Input ID that the backup asks about are invalid and will be replayed by the backup that is taking over . In this way the Order History and Trades tables are kept consistent for inquiries while still having a complete record of what occurred.

It is therefore intended that the foregoing detailed description be regarded as illustrative rather than limiting and that it be understood that it is the following claims including all equivalents that are intended to define the spirit and scope of this invention.

