---

title: Input/output trace sampling
abstract: Exemplary methods, apparatuses, and systems include a host computer selecting a first workload of a plurality of workloads running on the host computer to be subjected to an input/output (I/O) trace. The host computer determines whether to generate the I/O trace for the first workload for a first length of time or for a second length of time. The first length of time is shorter than the second length of time. The determination is based upon runtime history for the first workload, I/O trace history for the first workload, and/or workload type of the first workload. The host computer generates the I/O trace of the first workload for the selected length of time.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09471482&OS=09471482&RS=09471482
owner: VMware, Inc.
number: 09471482
owner_city: Palo Alto
owner_country: US
publication_date: 20141126
---
The various embodiments described herein relate to configuring a virtual storage area network. In particular embodiments relate to selecting workloads to run within a virtual storage area network and analyzing selected workloads to determine an amount of memory to allocate to the virtual storage area network.

A virtual storage area network VSAN aggregates storage directly attached to servers e.g. host computers to create a distributed shared storage system for a number of workloads e.g. virtual machines running on the servers. For example each server may utilize an attached solid state drive as a cache and one or more hard disk drives as underlying storage. The cache may improve input output I O performance for some workloads e.g. as compared to operating outside of a VSAN. The I O performance of other workloads however may not benefit from the cache. As a result it is worthwhile to be selective in which workloads are run within a VSAN and or the caching policies for the selected workloads. Additionally the amount by which I O performance is improved is dependent upon the amount of memory allocated for caching each workload. Collecting and analyzing I O requests for a large number of workloads to determine an amount of memory to allocate however is costly in terms of time storage and processing resources.

Embodiments described herein receive input output I O characteristics and based upon the characteristics determine which workloads that are suitable for operation within a virtual storage area network VSAN . As a result an administrator is presented with workloads that may experience an improvement in performance when run within a VSAN. Additionally embodiments may be selective in which workloads are analyzed for determining VSAN cache allocation. It is more efficient to analyze only those workloads that are selected suitable for operation within the VSAN especially when hundreds of workloads running within a virtual datacenter may be considered for VSAN suitability.

In analyzing workloads to determine VSAN cache allocation embodiments collect I O trace data for workloads over a period of time e.g. an I O trace representing seven days of workload I O requests. To minimize the amount of memory required to store this amount of I O trace data embodiments simulate cache performance using increments of the I O traces as the I O trace data is collected. Upon simulation of each increment of the I O trace a simulation state is stored as a checkpoint and the corresponding increment of the I O trace is deleted.

Additionally embodiments may be limited by the number of concurrent I O traces a host computer is able to generate. For example a host computer may run workloads while only being able to concurrently generate workload traces. Accordingly partial I O traces e.g. each representing a separate continuous period of time of a workload are taken at random intervals from varying subsets of workloads. For example incremental I O traces for a given workload may have intervening periods of time that are not subject to an I O trace. Embodiments determine a miss ratio curve MRC of each incremental I O trace and determine cache sizes corresponding to a target miss rate using the determined MRCs. Simply summing the cache sizes for each incremental I O trace may overestimate the cache size needed for the corresponding workload. Instead some embodiments determine if the incremental I O traces represent different or common phases of operation of a workload. A maximum value is selected from the cache sizes that correspond to incremental I O traces from a common phase. A sum or other combination of cache sizes is selected for incremental I O traces from different phases.

Furthermore while some workloads may run for longer periods of time or continuously other workloads may power on and only run for a relatively short period of time before powering off. Long I O trace sample periods may provide a more accurate representation of data reuse distances. With the number of concurrent I O traces a host computer can generate being less than the number of workloads running at a given time however short I O trace sample periods increase the likelihood a host computer will capture I O trace data even for those workloads that have a short run time. Accordingly embodiments determine whether to generate a long I O trace or a short I O trace for a given workload based upon runtime history for the workload I O trace history for the workload and or workload type of the first workload.

Embodiments set forth herein are described with reference to VSAN suitability and cache allocation. Other embodiments however may be directed to a different host computer configuration or virtual datacenter configuration in which workloads are similarly evaluated for suitability to work with a non VSAN caching architecture and the workloads are similarly analyzed to determine a corresponding cache allocation.

The illustrated server based computing allows client devices to access centrally managed user virtual desktops such as those implemented by VMs via network s e.g. a local area network or other private or publically accessible wide area network such as the Internet . For example client device and VM may use a desktop remoting protocol such as Remote Desktop Protocol RDP Virtual Network Computing VNC or Personal Computer over Internet Protocol PCoIP to remotely access provide remote access to a virtual desktop generated by VM . Additionally one or more VMs may implement another virtualized compute networking storage or security service e.g. a firewall webserver database server etc. .

Remote access to virtual desktops is generally provided to client devices through virtual infrastructure management VIM server . In one embodiment the VIM server provides virtual desktop access to the client devices . Additionally or alternatively VIM server orchestrates the provisioning of software defined datacenter services implemented by one or more VMs . VIM server may be a physical computer system or a virtual machine that runs infrastructure management software to e.g. provide a remotely accessible user interface to administrator device to manage the configuration of VMs virtualization software and hardware .

VMs are complete computation environments containing virtual equivalents of the hardware and system software components of a physical system and are typically implemented by an extensive virtualization infrastructure which includes a variety of software and hardware components. A virtualization software layer e.g. a hypervisor running on hardware of host computer manages one or more VMs . Virtualization software layer manages physical resources e.g. hardware as well as maintains virtual to physical hardware mappings. For example virtualization software layer maps each VM to a portion of memory and storage allocated to the VM . Memory may be flash memory or another high speed memory used to implement a cache between VMs and storage . As a result data for a given VM may be quickly fetched from the host computer s memory rather than by accessing underlying storage which may be a disk drive or other non volatile data stores directly attached to a host computer .

In one embodiment memory and storage may be configured to be a part of a VSAN. The VSAN aggregates storage of multiple host computers to create a distributed shared storage system for a number of workloads e.g. VMs running on the corresponding host computers . For example each host computer may utilize an attached solid state drive as a cache and one or more hard disk drives as underlying storage.

Analysis and planning VM runs within one or more host computers and works cooperatively with workload data collector within virtualization software of each host computer to determine VMs suitable for implementation of a VSAN and perform a cache analysis and allocation recommendation for suitable VMs . While analysis and planning VM is illustrated and described as a virtual machine running on a host computer analysis and planning VM may alternatively be implemented in a non virtual environment e.g. as software running on a physical computer rather than as a virtual machine .

In one embodiment workload data collector is a command line interface CLI application programming interface API or other interface that enables analysis and planning VM running on the same or on another host computer to initiate the collection of I O characteristics and I O trace data. Workload data collector gathers and or computes statistics on I O requests from each workload and transmits the I O characteristics and trace data to analysis and planning VM e.g. directly via network s or via VIM server . In one embodiment workload data collector includes a number of loggers that are capable of collecting I O characteristics and trace data in parallel. For example workload data collector may include 19 loggers that concurrently collect I O characteristics and trace data from 19 workloads. The collection and analysis of I O characteristics and trace data and corresponding cache allocation planning is described further with reference to .

At block analysis and planning VM determines a representative length value and access pattern for each workload. For example a representative length value may represent a maximum average mean median or other common value for representing a set of I O request data lengths. In one embodiment the representative length is included within and selected from the received I O characteristics. Alternatively the representative length is calculated by analysis and planning VM from a set of I O request lengths within the received I O characteristics.

Similar to the representative length an access pattern is included within the received I O characteristics or determined by analysis and planning VM from the received I O characteristics. The access pattern falls within a spectrum from random access to sequential access based upon the distance between consecutive I O requests. In one embodiment the access pattern is determined using the distance between logical addresses of consecutive I O requests and corresponding lengths of data subject to the I O requests e.g. the logical address distance between the end of one request and the beginning of the next request . An I O request for data at an address that immediately follows the end of a previous I O request e.g. the first logical address following the logical address of the previous I O request plus the corresponding data length is representative of a sequential access. Non zero distances between I O requests are indicative of a random access pattern. In one embodiment the access pattern for a given workload is based upon the number of sequential I O requests as compared to the number non sequential I O requests. A greater the number of sequential I O requests results in access pattern that is close to the sequential end of the spectrum and a greater number of non sequential I O requests results in an access pattern that is close to the random end of the spectrum.

At block analysis and planning VM determines whether the received I O characteristics are within a threshold of VSAN suitability. For example analysis and planning VM may compare the representative data length value to a length threshold and or the access pattern to an access pattern threshold. As the representative data length value increases and or the access pattern becomes more sequential the suitability of the workload may decrease. In one embodiment analysis and planning VM compares a combination of the representative data length value and access pattern to a combined threshold. For example analysis and planning VM calculates a suitability score based upon a combination of the characteristics and wherein determines if the calculated suitability score is less than a combined suitability threshold value. In one embodiment the access pattern is assigned a numerical value that can be added to multiplied by or otherwise combined with the data length value to generate the calculated suitability score.

In one embodiment one threshold value is dependent upon the value of another characteristic. For example the length threshold may increase as the access pattern becomes more random.

In one embodiment the access pattern is compared against multiple thresholds. For example analysis and planning VM may determine that the workload is suitable for VSAN when the access pattern falls between first and second thresholds on the spectrum from random access to sequential access.

Other characteristics may also be used alone or in combination with the representative data length value and access pattern. Exemplary other characteristics include the rate of I O requests the bandwidth of I O requests whether the I O requests are reads or writes I O latency etc. For example as the rate of I O requests increases the suitability of a workload may decrease. Additionally as the bandwidth of the I O requests e.g. the rate of I O requests multiplied by the representative data length increases the suitability of the workload may decrease. In yet another embodiment analysis and planning VM further determines suitability based upon whether the I O requests are read or write commands. For example a workload including long data length values for sequential random reads may be suitable for VSAN while a workload including long data length values for sequential random writes may not. As a result a data length threshold may be applied to writes but not to reads.

If the received I O characteristics are within a threshold of VSAN suitability at block analysis and planning VM generates a notification that the corresponding workload is suitable for VSAN. In one embodiment generating the notification includes transmitting a notification to administrator device including a list of workloads suitable for VSAN. In one embodiment generating the notification includes notifying workload data collector s to collect I O trace data from workloads determined to be suitable for VSAN e.g. as described further with reference to .

If the received I O characteristics are not within a threshold of VSAN suitability at block analysis and planning VM optionally determines if the workload is permitted to operate with a modified caching policy that would make the workload suitable for VSAN. For example the workload may be eligible for the use of a different cache replacement algorithm or bypassing the cache altogether. In one embodiment one or more thresholds e.g. the data length threshold or the access pattern threshold are dependent upon a caching policy or algorithm. As a result of modifying the caching policy or algorithm the one or more thresholds are also modified. Analysis and planning VM may reevaluate whether or not the received I O characteristics are within the modified threshold s of VSAN suitability.

If the workload is permitted to operate with a modified caching policy that would make the workload suitable for VSAN at block analysis and planning VM generates a notification that the corresponding workload is suitable for VSAN with the modified caching policy. If the workload is not permitted to operate with a modified caching policy or if a modified caching policy is not considered at block analysis and planning VM generates a notification that the corresponding workload is not suitable for VSAN.

In one embodiment analysis and planning VM receives multiple incremental I O traces from a single workload that were observed during the same period of time. For example a workload may be associated with multiple virtual disks. I O requests from the workload directed to one virtual disk may result in a first incremental I O trace while I O requests from the workload directed to another virtual disk may result in a second incremental I O trace. In such an embodiment analysis and planning VM modifies the logical storage addresses of one or more of the incremental I O traces to avoid collision between virtual disk addresses. For example analysis and planning VM may increment the logical storage addresses of a second incremental I O trace by a first predetermined amount the logical storage addresses of a third incremental I O trace by a second predetermined amount and so on. Once analysis and planning VM has modified the logical storage addresses analysis and planning VM merges the incremental I O traces from the workload that were observed during the same period of time into a single incremental I O trace.

At block analysis and planning VM runs the incremental I O trace through a cache simulation. For example analysis and planning VM may simulate a caching algorithm for VSAN to determine the MRC for the incremental I O trace based upon the logical address reuse distance and a plurality of possible cache sizes. The tracking of reuse distances and determination of the resulting MRC are described with reference to .

At block analysis and planning VM stores the state of the simulation to create a checkpoint for the cumulative simulation of the cumulative I O trace. For example analysis and planning VM stores the current hit count of various reuse distances based upon the current incremental trace. In one embodiment analysis and planning VM stores the MRC for the incremental I O trace and or for the cumulative I O trace.

At block once the checkpoint has been created by storing the state of the simulation analysis and planning VM deletes the current incremental I O trace to free the corresponding storage space for additional incremental I O trace data from the same or a different workload. The stored checkpoint data consumes less storage space than the corresponding incremental I O trace. As a result the analysis of the cumulative I O traces from e.g. hundreds of workloads over multiday periods does not create a burden on storage resources.

At block analysis and planning VM determines if there is another incremental I O trace to analyze. Additional incremental I O traces may be received from the same workload or from one or more other workloads. In one embodiment analysis and planning VM analyzes incremental I O traces from multiple workloads in parallel. In another embodiment analysis and planning VM incremental I O traces from multiple workloads sequentially and interleaved with one another e.g. analysis of an incremental I O trace from a first workload may be followed by the analysis of incremental I O trace s from one or more other workloads before analyzing another incremental I O trace from the first workload .

If there is an additional incremental I O trace at block analysis and planning VM determines if a state for this workload has previously been saved as a checkpoint. If so at block analysis and planning VM loads the stored state of the simulation to continue analysis of the cumulative I O trace for the corresponding workload. Once the checkpoint data has been loaded or if a state for this workload has not previously been saved as a checkpoint method returns to block to simulate the current incremental I O trace.

In one embodiment analysis and planning VM simulates each incremental I O trace alone e.g. independent of previous incremental I O traces as well as a continuation of any previous incremental I O traces from the same workload. As a result analysis and planning VM determines an MRC for a workload based upon independent incremental I O traces as well as a cumulative I O trace. Alternatively analysis and planning VM determines an MRC for a workload using the stored checkpoint data and utilizes that MRC determined at the next checkpoint as a representation of the corresponding incremental I O trace.

If there are no additional incremental I O traces to be analyzed at block analysis and planning VM optionally receives selection of a target miss rate for each workload. For example an administrator may select a target miss rate for one or more workloads for analysis and planning VM to use in selecting recommended cache allocations for workload s . Alternatively a default target miss rate may be utilized by analysis and planning VM .

At block analysis and planning VM generates a recommended cache size for the one or more workloads based upon the target miss rate s and corresponding MRC s . In one embodiment the recommended cache size is based upon a simple summation of cache allocation of each workload determined using the MRC of the cumulative I O trace for the corresponding workload and the workload s target miss rate. In another embodiment the recommended cache size is based upon maximum amounts of cache that would be used be each workload during an incremental period of time. For example analysis and planning VM may determine potential cache allocations based upon the target miss rate and MRC for each incremental I O trace for each workload and select the largest potential cache allocation for each workload and sum or otherwise combine them. In yet another embodiment the recommended cache size is based upon a maximum amount of cache that would be used by a combination of workloads during a single incremental period of time. For example analysis and planning VM may determine potential cache allocations based upon the target miss rate and MRC for each incremental I O trace for each workload sum or otherwise combine the allocations of the various workloads within each incremental period of time e.g. the sum of the allocation for a first workload within an incremental period of time and the allocation for a second workload within the same incremental period of time and select the maximum of the sums combined allocations. The determination of the recommended cache size is described further with reference to .

As a result an administrator is presented with a recommendation as to how much cache to allocate e.g. for the configuration of a VSAN with the analyzed workloads.

in which Hit i refers to the hit count for a given reuse distance i and Miss represents the number of misses that occur even if the maximum of n blocks is allocated. The MRC can be constructed by computing the miss ratios for various block allocations.

As described herein analysis and planning VM utilizes one or more MRC s to determine an amount of cache memory allocate for a given workload based upon a target miss rate for the workload. For example a target miss rate at high target value corresponds to small cache allocation . Similarly a target miss rate at low target value corresponds to large cache allocation .

In another embodiment the number of concurrent I O traces a host computer is able to generate is limited. As a result two or more workloads may be analyzed during different incremental periods of time. Additionally a single workload may be analyzed during non contiguous incremental periods of time. For example a first incremental I O trace may include a list of storage addresses and sizes that were subject to a plurality of I O requests from a workload during first period of time and a second I O trace may include a list of storage addresses and sizes that were subject to a plurality of I O requests from the workload during second period of time . Intervening period of time exists between first period of time and second period of time and I O trace data for the workload is lacking during intervening time period .

In one embodiment the periods of time corresponding to incremental I O traces represent random samples during an operation of a workload. Continuing the example above two incremental periods fall between first incremental I O trace period and second incremental I O trace period . Due to the random sampling the number of incremental periods between I O trace periods may vary. For example a third incremental I O trace may be created during time period with only one incremental period falling between second incremental I O trace period and third incremental I O trace time period . A fourth incremental I O trace time period may be randomly selected 0 1 2 3 or more incremental time periods following third incremental I O trace time period and so on. The analysis of non contiguous random incremental I O trace time periods is described further with reference to .

Using the maximum of the sums of cache sizes as an example the first and second workloads have cache sizes respectively of 250 and 50 in time period . The combination of these cache sizes results in a sum of 300 for time period . Similarly the first and second workloads have cache sizes respectively of 100 and 50 in time period 50 and 200 in time period 200 and 0 in time period etc. The combination of these cache sizes results in a sum of 150 for time period 250 for time period 200 for time period etc. The maximum of the illustrated exemplary sums is the cache size 300 in time period .

At block analysis and planning VM receives an incremental I O trace of a workload e.g. as described with reference to block of . At block analysis and planning VM runs the received incremental I O trace through a cache simulation e.g. as described with reference to block of and generates an MRC e.g. as described with reference to .

At block analysis and planning VM generates a fingerprint of the incremental I O trace stores the fingerprint along with the MRC and deletes the incremental I O trace. For example analysis and planning VM may generate a hash of the incremental I O trace or use another fingerprint algorithm that maps the incremental I O trace to a shorter representation of the incremental I O trace. In one embodiment the fingerprint is generated using one or more of an ordered list of logical storage addresses data lengths of the requests the access pattern and or I O latency.

In one embodiment the fingerprint is mapped to the MRC. For example analysis and planning VM creates or updates a data structure mapping the stored fingerprint to the stored MRC. In one embodiment the fingerprint and or MRC are also mapped to the corresponding workload. As a result fingerprints of incremental I O traces from the same workload can be compared to one another as described below.

At block analysis and planning VM determines if another incremental I O trace is to be processed. If so method returns to block . If no other incremental I O trace is to be processed at block analysis and planning VM determines the cache size s for each incremental I O trace based upon corresponding target miss rate s e.g. as described with reference to .

At block analysis and planning VM determines if any fingerprints of the incremental I O traces of the same workload are within a threshold level of similarity. Incremental I O traces within a threshold level of similarity are determined to represent similar phases of operation of the workload. For example if the fingerprint of a first incremental I O trace from a workload differs from the fingerprint of a second incremental I O trace from the workload by 5 or less analysis and planning VM determines that the first and second incremental I O traces represent a similar phase of operation. Following the same example if the fingerprint of a first incremental I O trace from a workload differs from the fingerprint of a second incremental I O trace from the workload by more than 5 analysis and planning VM determines that the first and second incremental I O traces represent different phases of operation.

If two or more fingerprints are within the threshold level of similarity at block analysis and planning VM determines a cache size to represent each group of incremental I O traces with similar fingerprints. In one embodiment analysis and planning VM selects the largest determined cache size within the group to represent the group. In an alternate embodiment analysis and planning VM determines an average mean median or other combination of the determined cache sizes to represent the group.

At block analysis and planning VM determines if the comparison of fingerprints yielded an incremental I O trace that represents a different phase of operation than another incremental I O trace from the same workload. For example individual non grouped incremental I O traces may have dissimilar fingerprints an individual incremental I O trace may have a fingerprint that differs from a group of incremental I O traces or two groups of incremental I O traces may have dissimilar fingerprints.

If the fingerprints of individual or groups of I O traces are not within the threshold level of similarity at block analysis and planning VM combines the cache sizes representing the dissimilar incremental I O traces groups for each workload. For example analysis and planning VM may sum the determined cache sizes of dissimilar individual incremental I O traces. Additionally analysis and planning VM may sum the determined cache sizes of one or more dissimilar individual incremental I O traces with a determined representative cache size for a group of similar incremental I O traces. Furthermore analysis and planning VM may sum the determined representative cache sizes of multiple different groups of incremental I O traces.

Upon combining the cache sizes of dissimilar incremental I O traces groups for each workload or if there are no dissimilar incremental I O traces groups at block analysis and planning VM generates a recommend cache size for each workload. For example the recommended cache size for a given workload may be the corresponding combined cache sizes of dissimilar and or selected representative cache sizes or if the incremental I O traces are all within a single group the selected representative cache size.

At block analysis and planning VM generates an aggregate recommended cache size based upon the recommend cache sizes for each workload. For example analysis and planning VM may sum the workload cache sizes to generate the aggregate recommended cache size. As a result an administrator is presented with a recommendation as to how much cache to allocate e.g. for the configuration of a VSAN with the analyzed workloads.

At block workload collector selects a workload to be subjected to an I O trace. In one embodiment workload collector utilizes random sampling with replacement to select a workload among the plurality of workloads running on host computer . Alternatively workload collector utilizes random sampling without replacement round robin or another selection algorithm to select a workload.

At block workload collector determines the runtime history I O trace history and or workload type of one or more of the plurality of workloads running on host computer . For example workload collector may determine a history of how long the selected workload has run. This runtime history may be expressed as a collection of runtimes from start up to shut down a minimum maximum average median or mean runtime from start up to shut down and or an amount of time the workload has been running since its latest start up. The runtime history may also be expressed as a zero value to represent that the workload has not previously run. The I O trace history may be expressed as a number of I O traces previously collected for a given workload. Exemplary workload types include virtual desktops and various virtualized compute networking storage or security services e.g. a firewall webserver database server etc. .

At block workload collector optionally determines whether or not to adjust a fast pool threshold. For example workload collector may divide the limited number of concurrent I O traces a host computer is able to generate into two pools a fast pool and a slow pool. I O traces collected in the fast pool are collected for a short length of time e.g. one minute. I O traces collected in the slow pool are collected for a longer length of time e.g. five to ten minutes. In order to balance I O traces collected in each pool workload collector may utilize a static or dynamic threshold to limit the number of loggers collecting I O traces in the fast pool and or a static or dynamic threshold to limit the number of loggers collecting I O traces in the slow pool. For example a default fast pool threshold may be set to limit workload collector to a maximum of five loggers collecting I O traces in the fast pool.

In one embodiment as described in further detail below workload collector utilizes threshold values for runtime history and or I O trace history to differentiate whether each workload is a candidate for the fast pool or the slow pool. Additionally workload collector may associate particular types of workloads with the fast pool or the slow pool. For example virtual desktops may initially default to the fast pool while webservers default to the slow pool. In one embodiment workload collector maintains a table or other data structure mapping the plurality of workloads running on host computer to runtime history I O trace history workload type and or an I O trace pool. Additionally workload collector may determine e.g. based upon an indication provided by virtualization software or VIM server when workloads are being booted up to determine runtime history.

If workload collector determines to adjust or otherwise set the fast pool threshold at block workload collector updates the fast pool threshold. For example using the data structure described above workload collector determines a ratio of the workloads that are going to be assigned to the fast pool as compared to the workloads that are going to be assigned to the slow pool. When a large number of VMs is initially booted up or otherwise associated with the fast pool workload collector may set or otherwise adjust the fast pool threshold to accommodate a larger number of loggers to collect I O traces in the fast pool. When a large number of the VMs is associated with the slow pool workload collector may set or otherwise adjust the fast pool threshold to accommodate a smaller number of loggers to collect I O traces in the fast pool.

If workload collector determines not to adjust the fast pool threshold or bypasses this optional determination at block workload collector determines whether the selected workload is to be subjected to a short I O trace or a long I O trace e.g. whether the selected workload is to be added to the fast pool or the slow pool . As discussed above workload collector may utilize a table or other data structure to map the selected workload to runtime history I O trace history workload type and or an I O trace pool. In one embodiment a workload that has not yet to be subjected to a threshold number of I O traces is added to the fast pool. For example a workload that has yet to be subjected to any I O trace collections is added to the fast pool while a workload that has been subjected to one or more I O trace collections is added to the slow pool. In one embodiment a workload that has yet to run for a threshold period of time is added to the fast pool. For example a workload that has run for less than five minutes may be added to the fast pool while a workload that has run for five or more minutes may be added to the slow pool. In one embodiment workload collector selects the fast pool or the slow pool based upon workload type. For example a virtual desktop lacking a threshold number of I O traces or threshold amount of runtime may be added to the fast pool while a database server is added to the slow pool. In one embodiment workload collector determines a collective runtime history for workloads of each workload type to determine whether workloads of that type should by default be added to the fast pool or to the slow pool.

If the workload is to be added to the slow pool at block workload collector adds the workload to the slow pool or otherwise subjects the workload to a long I O trace. In one embodiment workloads added to the slow pool are subjected to an I O trace for five to ten minutes.

If the workload is to be added to the fast pool at block workload collector optionally determines if all loggers are currently in use by the slow pool. For example if a new workload has booted up when all loggers are in use by the slow pool workload collector may be exposed to the risk of the new workload shutting down before at least one logger completes the current slow pool I O trace thereby causing workload collector to miss the opportunity to collect an I O trace for the new workload.

If all loggers are currently in use by the slow pool at block workload collector terminates an I O trace in the slow pool. For example workload collector may select the I O trace that has been running the longest and terminate it. In an alternate embodiment workload collector terminates an I O trace in the slow pool when the longest running I O trace has more than a threshold period of time remaining in the trace. For example if the longest running I O trace has more time remaining than the corresponding period of time for a fast trace the I O trace is terminated.

If less than all of the loggers are currently in use by the slow pool or if this determination is bypassed at block workload collector adds the selected workload to the fast pool or otherwise subjects the workload to a short I O trace. In one embodiment workloads added to the fast pool are subjected to an I O trace for one minute.

Data processing system includes memory which is coupled to microprocessor s . Memory may be used for storing data metadata and programs for execution by the microprocessor s . Memory may include one or more of volatile and non volatile memories such as Random Access Memory RAM Read Only Memory ROM a solid state disk SSD Flash Phase Change Memory PCM or other types of data storage. Memory may be internal or distributed memory.

Data processing system includes network and port interfaces such as a port connector for a dock or a connector for a USB interface FireWire Thunderbolt Ethernet Fibre Channel etc. to connect the system with another device external component or a network. Exemplary network and port interfaces also include wireless transceivers such as an I7 802.11 transceiver an infrared transceiver a Bluetooth transceiver a wireless cellular telephony transceiver e.g. 2G 3G 4G etc. or another wireless protocol to connect data processing system with another device external component or a network and receive stored instructions data tokens etc.

Data processing system also includes display controller and display device and one or more input or output I O devices and interfaces . Display controller and display device provides a visual user interface for the user. I O devices allow a user to provide input to receive output from and otherwise transfer data to and from the system. I O devices may include a mouse keypad or a keyboard a touch panel or a multi touch input panel camera optical scanner audio input output e.g. microphone and or a speaker other known I O devices or a combination of such I O devices.

It will be appreciated that one or more buses may be used to interconnect the various components shown in .

Data processing system is an exemplary representation of one or more of client device s administrator device VIM server and host computer s described above. Data processing system may be a personal computer tablet style device a personal digital assistant PDA a cellular telephone with PDA like functionality a Wi Fi based telephone a handheld computer which includes a cellular telephone a media player an entertainment system or devices which combine aspects or functions of these devices such as a media player combined with a PDA and a cellular telephone in one device. In other embodiments data processing system may be a network computer server or an embedded processing device within another device or consumer electronic product. As used herein the terms computer device system processing system processing device and apparatus comprising a processing device may be used interchangeably with data processing system and include the above listed exemplary embodiments.

It will be appreciated that additional components not shown may also be part of data processing system and in certain embodiments fewer components than that shown in may also be used in data processing system . It will be apparent from this description that aspects of the inventions may be embodied at least in part in software. That is the computer implemented method s and may be carried out in a computer system or other data processing system in response to its processor or processing system executing sequences of instructions contained in a memory such as memory or other non transitory machine readable storage medium. The software may further be transmitted or received over a network not shown via network interface device . In various embodiments hardwired circuitry may be used in combination with the software instructions to implement the present embodiments. Thus the techniques are not limited to any specific combination of hardware circuitry and software or to any particular source for the instructions executed by data processing system .

An article of manufacture may be used to store program code providing at least some of the functionality of the embodiments described above. Additionally an article of manufacture may be used to store program code created using at least some of the functionality of the embodiments described above. An article of manufacture that stores program code may be embodied as but is not limited to one or more memories e.g. one or more flash memories random access memories static dynamic or other optical disks CD ROMs DVD ROMs EPROMs EEPROMs magnetic or optical cards or other type of non transitory machine readable media suitable for storing electronic instructions. Additionally embodiments of the invention may be implemented in but not limited to hardware or firmware utilizing an FPGA ASIC a processor a computer or a computer system including a network. Modules and components of hardware or software implementations can be divided or combined without significantly altering embodiments of the invention.

This specification refers throughout to workloads implemented by computational and network environments that include virtual machines VMs . However VMs are merely one example of data compute nodes DCNs or data compute end nodes also referred to as addressable nodes. DCNs may include non virtualized physical hosts virtual machines containers that run on top of a host operating system without the need for a hypervisor or separate operating system and hypervisor kernel network interface modules.

VMs in some embodiments operate with their own guest operating systems on a host using resources of the host virtualized by virtualization software e.g. a hypervisor virtual machine monitor etc. . The tenant i.e. the owner of the VM can choose which applications to operate on top of the guest operating system. Some containers on the other hand are constructs that run on top of a host operating system without the need for a hypervisor or separate guest operating system. In some embodiments the host operating system uses distinct name spaces to isolate the containers from each other and therefore provides operating system level segregation of the different groups of applications that operate within different containers. This segregation is akin to the VM segregation that is offered in hypervisor virtualized environments and thus can be viewed as a form of virtualization that isolates different groups of applications that operate in different containers. Such containers are more lightweight than VMs.

It should be recognized that while the specification refers to VMs the examples given could be any type of DCNs including physical hosts VMs non VM containers and hypervisor kernel network interface modules. In fact the example networks could include combinations of different types of DCNs in some embodiments.

In the foregoing specification the invention has been described with reference to specific exemplary embodiments thereof. Various embodiments and aspects of the invention s are described with reference to details discussed herein and the accompanying drawings illustrate the various embodiments. The description above and drawings are illustrative of the invention and are not to be construed as limiting the invention. References in the specification to one embodiment an embodiment an exemplary embodiment etc. indicate that the embodiment described may include a particular feature structure or characteristic but not every embodiment may necessarily include the particular feature structure or characteristic. Moreover such phrases are not necessarily referring to the same embodiment. Furthermore when a particular feature structure or characteristic is described in connection with an embodiment such feature structure or characteristic may be implemented in connection with other embodiments whether or not explicitly described. Additionally as used herein the term exemplary refers to embodiments that serve as simply an example or illustration. The use of exemplary should not be construed as an indication of preferred examples. Blocks with dashed borders e.g. large dashes small dashes dot dash dots are used herein to illustrate optional operations that add additional features to embodiments of the invention. However such notation should not be taken to mean that these are the only options or optional operations and or that blocks with solid borders are not optional in certain embodiments of the invention. Numerous specific details are described to provide a thorough understanding of various embodiments of the present invention. However in certain instances well known or conventional details are not described in order to provide a concise discussion of embodiments of the present inventions.

It will be evident that various modifications may be made thereto without departing from the broader spirit and scope of the invention as set forth in the following claims. For example the methods described herein may be performed with fewer or more features blocks or the features blocks may be performed in differing orders. Additionally the methods described herein may be repeated or performed in parallel with one another or in parallel with different instances of the same or similar methods.

