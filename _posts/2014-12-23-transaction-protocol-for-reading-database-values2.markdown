---

title: Transaction protocol for reading database values
abstract: Techniques are provided for more efficient multi-row atomic, consistent, isolated and durable (ACID)-compliant transactions with snapshot isolation semantics (or just “multi-row transactions” for short). In some embodiments, the techniques are implemented in a computing system that includes a client application, a lightweight in-memory lease-based lock service, a multi-row transaction orchestrator, and an underlying database system. The transaction orchestrator implements a read protocol and a write protocol that provides support to the client application for carrying out multi-row transactions against the underlying database system irrespective of whether the database system itself supports multi-row transactions. The transaction orchestrator explicitly maintains transaction-level locks obtained from the lease-based lock service as part of the transaction protocol. Further, the transaction orchestrator is tolerant to lock service failure and unavailability without compromising ACID-compliance and snapshot isolation guarantees to the client application.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09619507&OS=09619507&RS=09619507
owner: Palantir Technologies, Inc.
number: 09619507
owner_city: Palo Alto
owner_country: US
publication_date: 20141223
---
This application claims benefit under 35 U.S.C. 120 as a Continuation of application Ser. No. 13 958 817 filed Aug. 5 2013 which is a Continuation of application Ser. No. 13 224 500 filed Sep. 2 2011 now U.S. Pat. No. 8 504 542 issued Aug. 6 2013 the entire contents of each of which is hereby incorporated by reference as if fully set forth herein. The applicant s hereby rescind any disclaimer of claim scope in the parent applications or the prosecution history thereof and advise the USPTO that the claims in this application may be broader than any claim in the parent applications.

Embodiments relate generally to database systems and more specifically to techniques for more efficient multi row transactions.

Computers are very powerful tools for accessing and storing vast amounts of information. Computer databases are a common mechanism for storing information on computer systems. A typical database is a collection of tables having rows with columns of information. For example a database table of employees may have a row for each employee where each row contains columns designating specifics about the employee such as the employee s name address salary etc.

A database management system DBMS is typically provided as a software layer on top of the database itself i.e. the data actually stored on a non volatile storage device s . The DBMS controls and coordinates access to the database by other client software applications. Typically all requests from clients to retrieve and store data in the database are processed by the DBMS. Thus the client software applications may be viewed as a software layer on top of the DBMS with the DBMS being an intermediary software layer between the client applications and the database. A DBMS and the database it manages are often referred to collectively as just a database system .

In recent years the need for client applications to be able operate on very large database datasets has spurred the development of large scale distributed database systems. A large scale distributed database system typically is a database system in which the DBMS and or the database is are distributed among multiple computer systems. Large scale distributed database systems often support highly parallel database data processing computation. Today some large scale distributed database systems manage between hundreds of gigabytes up to multiple petabytes of database data and are distributed over tens hundreds even thousands of computer systems.

Large scale distributed database systems typically support only basic database functionality and may not support a full relational database model as a trade off of being able to scale up to support highly parallel client applications such as those that can be found in a some cloud computing environments. For example some large scale distributed database systems support only simple query syntax and do not provide full Structured Query Language SQL or join support. In addition some of these systems provide only single atomic writes based on row locks and provide only limited transactional support as a trade off for reduced overhead in supporting strongly consistent distributed transactions. Many of these systems include a distributed column oriented database. One example of a distributed column oriented database is Google s Bigtable. See F. Chang J. Dean S. Ghemawat W. C. Hsieh D. A. Wallach M. Burrows T. Chandra A. Fikes and R. Bruger Bigtable A Distributed Storage System for Structured Data OSDI 205 218 USENIX Association 2006. An open source example of a large scale distributed database system is Apache HBase currently available from the Apache Software Foundation at the Internet domain hbase.apache.org.

Recently in an effort to make it easier for developers of client applications to reason about the state of the large scale distributed databases that the client applications read from and write to solutions have been developed to provide support for multi row ACID Atomic Consistent Isolated and Durable compliant transactions with snapshot isolation semantics or just multi row transactions for short . With snapshot isolation typically all row reads from the database within a transaction see a consistent snapshot of the database that remains unaffected by any other concurrent transactions. Further any row writes to the database within the transaction typically are committed to the database only if none of the row writes conflict with any concurrent write committed to the database since that snapshot. To provide snapshot isolation some of these solutions store in the database multiple time stamped versions of each data item a technique known as Multi Version Concurrency Control MVCC . A potential benefit of MVCC is more efficient row reads because reading a data item from a row typically does not require acquiring a lock on the row. Further MVCC may protect against write write conflicts. For example if multiple transactions running concurrently write to the same cell e.g. row column pair at most one of the transactions will be allowed to commit its write to the cell. Google s Percolator system built on top of its Bigtable distributed database is one example of a large scale distributed database system that provides support for multi row transactions. See Large scale Incremental Processing Using Distributed Transactions and Notifications Daniel Peng Frank Dabek Proceedings of the 9th USENIX Symposium on Operating Systems Design and Implementation 2010 a PDF copy of which is currently available via HTTP at research pubs archive 36726.pdf in the www.google.com Internet domain.

Some current solutions implement multi row transactions with an additional software layer transaction service that executes on top of an existing large scale distributed database system e.g. HBase Bigtable etc. . In some cases this is a design goal of such solutions to avoid requiring modifications to the existing systems. As a result these solutions generally do not integrate locking functionality for implementing multi row transactions into the underlying database system. Nor do these solutions typically employ a centralized global deadlock detection process as that may hinder horizontal scaling of the system. As a result locks for implementing multi row transactions may be explicitly maintained by the transaction service itself.

Current multi row transaction services for large scale distributed databases may implement multi row transactions with a two phase commit transaction protocol. During a transaction initiated by a client application row writes within the transaction may be buffered until the client commits the transaction at which point the transaction service initiates the two phase commit process. In the first commit phase of the transaction the buffered row writes and associated lock metadata are atomically written to the database using row level transactions provided by the underlying database system e.g. HBase Bigtable etc. . The lock metadata is generated and used by the transaction service for detecting conflicts e.g. write write conflicts between different transactions. In the second phase assuming no other transactions conflict with the current transaction the transaction service commits the current transaction by atomically modifying the lock metadata in the database for the current transaction using a row level transaction provided by the underlying database system.

Lock metadata of current transaction services may be stored in the database in non volatile memories where it can persist in the case of a system failure e.g. power outage . If lock metadata were to disappear between the two phases of commit the transaction service might mistakenly commit two transactions that should have conflicted. In current systems row writes during the first commit phase typically require a volatile memory to non volatile memory synchronization operation to ensure that associated lock metadata is actually persisted i.e. stored in non volatile memory rather than just being stored in a volatile memory based write cache or other volatile memory where the metadata could be lost in the event of a failure. Volatile memory to non volatile memory synchronization operations often require physical movement of mechanical components e.g. disk platters read write heads etc. of non volatile storage device s making these synchronization operations much slower than volatile memory only synchronization operations. As a result the requirement of current transaction services that lock metadata be persisted in the database as well as adding to the size of the database can increase the latency of transaction commit operations perhaps to the point that is intolerable for some types of database tasks such as for example some online transaction processing tasks. This increased latency can be mitigated by increasing parallelism of the system at the expense of additional computer systems and associated management overhead. However some users of large scale distributed database systems may want support for multi row transactions without having to incur additional expenses for scaling current systems to provide lower latency commits.

The approaches described in this section are approaches that could be pursued but not necessarily approaches that have been previously conceived or pursued. Therefore unless otherwise indicated it should not be assumed that any of the approaches described in this section qualify as prior art merely by virtue of their inclusion in this section.

In the following description for the purposes of explanation numerous specific details are set forth in order to provide a thorough understanding of the present invention. It will be apparent however that the present invention may be practiced without these specific details. In other instances well known structures and devices are shown in block diagram form in order to avoid unnecessarily obscuring the present invention.

Techniques are provided for more efficient multi row atomic consistent isolated and durable ACID compliant transactions with snapshot isolation semantics or just multi row transactions for short . For example techniques are provided for using a lightweight in memory lease based lock service that does not require persisting lock metadata to a non volatile data storage medium.

In some embodiments of the present invention the techniques are implemented in a computing system that includes a client application the lease based lock service a multi row transaction orchestrator and an underlying database system. The transaction orchestrator implements a read protocol and a write protocol that provides support to the client application for carrying out multi row transactions against the underlying database system irrespective of whether the database system itself supports multi row transactions.

In some embodiments of the present invention the transaction orchestrator explicitly maintains transaction level locks obtained from the lease based lock service as part of the transaction protocol. Further the transaction orchestrator and in particular the read and write transaction protocol is tolerant to lock service failure and unavailability without compromising ACID compliance and snapshot isolation guarantees to the client application.

While embodiments of the present invention are directed to techniques for more efficient multi row transactions the embodiments may apply to single row transactions as well. However as the underlying database system may provide support for single row transactions it is expected that in most cases the greatest benefit of implementing the disclosed techniques will inure to client applications that conduct multi row transactions.

The components of the system may all embodied within a single computing system such as the computer system of . Alternatively one or more of the components may be embodied within multiple computing systems in a distributed arrangement. Distributed components may be operatively connected to one another using any suitable data communication bus such as for example a data network. Further distributed components may communicate with one another using any suitable data communication protocol such as an inter process communication IPC mechanism or a remote procedure call RPC mechanism as examples.

In some embodiments the client application the transaction orchestrator the lock service the timestamp service and the database management system are each implemented in software. However one or more of these components may be implemented in hardware or a combination of hardware and software.

The transaction orchestrator provides to the client application the ability to randomly access database . In addition the transaction orchestrator provides transaction ality to the client application so that multiple threads of the client application can access and transform the database concurrently in an orderly manner. More specifically the transaction orchestrator provides ACID compliant transactions with snapshot isolation semantics to client applications to make it easier for programmers of client applications to reason about the state of the database as they are developing coding the client applications.

In some embodiments the transaction orchestrator is a software library such as for example a static library or dynamic link library DLL that executes in the same process space as the client application . In other embodiments the transaction orchestrator executes as a computing process separate from the client application computing process. Communication between the two processes may be facilitated using an inter process communication IPC mechanism or a remote procedure call RPC mechanism for example.

The transaction orchestrator uses two additional services to provide multi row transaction ality to the client application a timestamp service and a lightweight in memory leased based lock service . Both of these services and may be network services that are communicatively and operatively coupled to the transaction orchestrator via a data network such as for example a Local Area Network LAN .

According to some embodiments multiple executing client applications concurrently access and transform the same database each using a separate transaction orchestrator instances. Each executing instance of the transaction orchestrator may use the same timestamp service and the same leased based lock service for coordinating and orchestrating multi row transactions among the multiple client applications . However a one to one correspondence between client applications and transaction orchestrator instances is not required and a single transaction orchestrator instance can serve multiple client applications or a single client application may use multiple transaction orchestrator instances.

The timestamp service provides strictly increasing timestamps to the transaction orchestrator . As described in greater detail below the transaction orchestrator uses the timestamps received from the timestamp service to implement the transaction protocol.

The lightweight in memory leased based lock service provides the ability for the transaction orchestrator to obtain and release locks on database rows. The lock service may maintain lock metadata for row locks in a volatile memory operatively coupled to the lock service such as for example in a random access memory RAM . In this description volatile memory includes any non volatile memory that is used to implement virtual memory.

At the same time the transaction orchestrator provides ACID compliant multi row transaction ality to the client application even in cases where the lock service fails and lock metadata is lost from volatile memory. The transaction orchestrator does this in part by implementing a particular read and write transaction protocol that is tolerant to lock service failures and does not require lock metadata to be durable i.e. persisted to a non volatile data storage medium . The write and read transaction protocol are described in greater detail below.

In some embodiments the database system is a key value database system and the transaction orchestrator is implemented as a computing layer on top of the key value database system . The term key value is not meant to imply any particular database system or imply any particular type of database system. Rather key value refers broadly to the general manner in which the database management system presents the underlying database to the transaction orchestrator . More specifically the database management system may present a key value abstraction of the underlying database to the transaction orchestrator through an Application Programming Interface API . The transaction orchestrator uses the API provided by database management system to add the ability to run multi row transactions on top of this key value layer.

The database management system may be implemented by any one of a variety of different database management systems and embodiments of the invention are not limited to any particular database management system. For example the database management system may be implemented by a conventional relational database management system RDBMS . Alternatively as another example the database management system may be implemented using a NoSQL database management system or other database management system that differs from a traditional RDBMS in one or more respects. In one particular non limiting embodiment the database management system is implemented using a version of the Apache HBase database management system.

The database may be embodied as a collection of one or more file system files. For example the database may be embodied as a collection of one or more files of an operating system. Alternatively the database may be a file system itself. For example the database may be a distributed file system such as for example the Apache Hadoop Distributed File System HDFS .

A goal of the transaction orchestrator is to provide to the client application the ability to conduct more efficient multi row transactions against the underlying database irrespective of whether the underlying database management system also provides support for multi row transactions. In many cases the underlying database management system will provide support for only single row transactions. Thus the transaction orchestrator can be used to provide support for multi row transactions on top of a database system that supports only single row transactions. While a variety of different database systems and different types of database systems may be used as the underlying database system the transaction orchestrator makes some assumptions about the operation of the underlying database system when providing support for multi row transactions to the client application . These assumptions will now be described.

The transaction orchestrator assumes that the database system is durable. That is the database system ensures that data that the database management system successfully writes to the underlying database permanently changes the state of the database . From the perspective of the transaction orchestrator durability means that data that the database management system indicates to the transaction orchestrator as successfully written to the database can be immediately read from the database assuming no intervening writes to the data absent catastrophic database system failure e.g. act of nature . The database system may ensure durability through use of one or more write ahead logs for example. Other techniques for ensuring durability are possible and embodiments are not limited to any particular technique.

The transaction orchestrator assumes that writes performed by the database management system to the database are atomic at least within a row of the database . In other words when the transaction orchestrator commands the database management system to write a row of the database the write of the row either completely succeeds or completely fails but does not partially succeed or partially fail. However there is no requirement of the database management system that multi row writes be atomic across the multiple rows. Indeed a purpose of the transaction orchestrator is to provide support to client the application for atomic multi row writes irrespective of whether the database management system itself supports multi row transactions.

In general however a row of the database is viewed from the perspective of the client application as presented to it by the transaction orchestrator . In particular the transaction orchestrator presents a row of the database to the client application generally as a collection of one or more values one for each of one or more columns of the row. This row as viewed by the client application may or may not correspond directly to the notion of a row as presented by the database management system to the transaction orchestrator . For example where the database is a relational database a single row as viewed by the client application may correspond to multiple rows of a relational table of the relational database. For the purposes of the following discussion unless otherwise apparent in context the term row refers to a row of the database viewed from the perspective of the client application which may or may not correspond one to one to a row in the database as presented by the underlying database management system to the transaction orchestrator .

The underlying database system may be distributed in the sense that the database is stored on multiple storage devices that are not all operatively coupled to a single computing node that executes the database management system . In this context computing node refers to a processor or set of processors that executes an instance of the database management system as part of a cluster of multiple database management system instances. Thus in this case the database management system as a whole may viewed as executing on a cluster of multiple computing nodes.

In the case where the database system is distributed the transaction orchestrator assumes that a write to a row of the database is strongly consistent. That is after the underlying database management system has indicated to the transaction orchestrator that a write to a row of the database was successful an immediately subsequent read of that value from the row will return the value successfully written assuming no intervening writes to the row regardless of which computing node in the cluster the transaction orchestrator submits the subsequent read request to. In some embodiments the database management system ensures strongly consistent row writes using concurrency control mechanism that ensures that the subsequent read does not accidentally return the value in the row that existed prior to the write. However other techniques for ensuring strongly consistent row writes may be possible and embodiments are not limited to any particular technique for implementing strongly consistent row writes.

In some embodiments the database system is configured in a shared nothing architecture to facilitate horizontal scaling of the database system . Very generally scaling the database system horizontally refers to adding more computing nodes to the system such that the database management system and or the database operates on more computing nodes. Generally in these embodiments the database system is implemented on a cluster of multiple computing nodes each with their own processor or set of processors and their own data storage device for storing a portion shard of the database . For example each node may be a computer system in which the hardware components of the computer system are implemented with relatively inexpensive commodity hardware. Each computing node in the cluster executes at least one instance of the database management system and each instance may execute and operate independently or semi independently of other instances.

The database may be partitioned into shards that are distributed among the nodes each node storing a portion shard of the database .

The database may be replicated such that some or the entire database is duplicated among multiple shards.

The database management system instances distributed across the nodes may use an instance to instance co ordination protocol for co coordinating database activities e.g. database read and write requests among the instances.

As mentioned the transaction orchestrator provides to the client application cross row ACID compliant transactions with snapshot isolation semantics. The client application may be written in an imperative programming language such as for example C C Java Python Ruby VB.NET C etc. Transaction code of the client application may be mixed with calls to the transaction orchestrator application programming interface API .

According to some embodiments the transaction orchestrator provides through its API to the client application a set of transaction primitives by which the client application can carry out a multi row transaction. is a flowchart of a multi row transaction that may be carried out by the client application according to some embodiments of the invention. Each block of the transaction corresponds to one of four transaction primitives supported by the transaction orchestrator . In some embodiments the transaction is executed in the context of a single thread of the client application . Parallelism within the client application may be achieved by executing many transactions simultaneously in multiple separate threads of execution.

As shown in the transaction orchestrator supports at least four transaction primitives START TX GET SET and COMMIT TX. Discussion of the implementation details of each of the four primitives by the transaction orchestrator is provided below. For now a multi row transaction will be described from the perspective of the client application .

Initially to start a multi row transaction the client application invokes the START TX primitive . The START TX primitive returns or outputs to the client application a programmatic handle by which the client application can encapsulate zero or more GET primitives i.e. row reads and or zero or more SET primitives i.e. row writes in an atomic multi row transaction .

A GET primitive i.e. a row read takes as input from the client application an identifier of a row in the database from which a value is to be read by the transaction orchestrator . For example the identifier may include an identifier of a table in the database and the key of the row in the table. As output the GET primitive provides the requested value in that row that existed at the time the encapsulating transaction was started. This time is set by when the START TX primitive is invoked for the encapsulating transaction . The GET primitive may also accept as additional input an identifier of a column of the identified row. The output that is provided in this case is the value in the given column of the row cell at the time the encapsulating transaction was started. Again this time is set by when the START TX primitive is invoked for the encapsulating transaction .

In some embodiments the GET primitive can operate in a batched mode in which multiple keys are input by the client application to a single GET primitive invocation. In the batched mode of operation the transaction orchestrator obtains values for the multiple keys from the database system in one or a small number of calls to the database management system . Batched mode operation provides improved read performance to the client application when compared to invoking the GET primitive once for each of the multiple keys because fewer network round trips between the transaction orchestrator and the database system are needed to obtain values for the multiple keys.

In some embodiments the GET primitive is a blocking call from the perspective of the client application . That is after the client application invokes the GET primitive execution control does not return to the client application until after the transaction orchestrator has determined whether the requested value exists in the database or an error occurs. If the requested value was not available in the database or an error occurred the GET primitive may indicate so by for example returning false or other value to the client application that indicates that the read request was not successful.

A SET primitive i.e. a row write takes as input from the client application an identifier of a row in the database to which a value is to be written. For example the identifier may include an identifier of a table in the database and the key of the row in the table. Additionally the SET primitive accepts as input the value to be written. The client application may also specify as input to the SET primitive a particular column of the row cell to which the provided value is to be written.

In some embodiments invocations of the SET primitive within the context of a transaction are buffered by the transaction orchestrator until commit time. In general the approach for committing buffered writes is two phase commit which is coordinated by the client application . Implementation details of the two phase commit process by the transaction orchestrator are described in greater detail below.

In some embodiments in response to a GET primitive invocation the transaction orchestrator returns to the client application a value buffered by the transaction orchestrator for a previous SET primitive invocation instead of a value obtained from the database system . For example if the client application invokes within the context of a transaction a SET primitive to write the value 1234 in row abc a subsequent GET primitive invocation within the context of the transaction to read the value in row abc may return the value 1234 buffered by the transaction orchestrator instead of whatever value for row abc currently exists in the database .

A number of GET primitives and or SET primitives may be encapsulated within the transaction by the client application . When the client application is ready to commit the client application invokes the COMMIT TX primitive . As output the COMMIT TX primitive indicates whether the commit operation was successful. In particular the output indicates whether all the values to be written to the database and passed as input to the encapsulated SET primitives were committed to the database in which case the transaction was successful or whether none of the values to be written to the database and were committed to the database in which case the transaction was unsuccessful. The transaction orchestrator ensures that all SET primitives encapsulated by the transaction are atomic with respect to committing the writes of the encapsulated SET primitives to the database . In other words the transaction orchestrator ensures that either all the writes are committed to the database or that none of them are committed.

In some embodiments to abort a started transaction the client application simply does not invoke the COMMIT TX primitive for the transaction. Aborting a transaction in this way does not affect the ACID compliance and snapshot isolation guarantees provided by the transaction orchestrator for the aborted transaction or for other transactions.

In some embodiments the transaction orchestrator supports a GET RANGE primitive. The GET RANGE primitive operates similar to the GET primitive expect that instead of accepting as input a single key or multiple keys the GET RANGE primitive accepts a range of keys as input. The GET RANGE primitive returns a programmatic iterator object that allows the client application to iterate over the one or more values within the specified range.

The following is example pseudo programming language code of a hypothetical client application that uses a transaction orchestrator API to conduct a multi row transaction in accordance with some embodiments of the invention. In this example a transaction that spans multiple rows of the database is required rather than just a single row transaction that the underlying database system might already provide. At line 2 the START TX primitive is invoked and the client application obtains a programmatic handle to the transaction in the form of a reference to a Transaction object. At line 3 the SET primitive is invoked in the context of the current transaction represented by programmatic handle tx to set the value of the column1 column of row key1 in table table1 in the database to value1 . This SET primitive at line 3 is buffered by the transaction orchestrator until the COMMIT TX primitive is invoked at line 8. At line 5 the GET primitive is invoked to obtain the value of the column2 column of row key2 in table table2 in the database and assign it to the local String type variable var2 . The GET primitive at line 5 returns true if the requested value was successfully obtained by the transaction orchestrator from the database false otherwise. At line 6 the SET primitive is invoked to set the value of the column2 column of row key2 in table table2 to key1 . Again this SET primitive at line 6 is buffered by the transaction orchestrator until the COMMIT TX primitive is invoked at line 8. If at line 8 the COMMIT TX primitive returns false then the transaction has conflicted with another transaction or was otherwise unsuccessful in which case neither of the SET primitives at lines and will have been committed to the database .

According to some embodiments the transaction orchestrator stores in the database multiple versions of each database value using a timestamp dimension in order to provide snapshot isolation to client applications . This time stamping scheme is represented in by an example.

In two rows of the database are shown one having a key of Alice and the other having a key of Bob . Both rows have two columns Primary Contact and Bank Balance . Each cell i.e. row column pair of the two rows has one or more time stamped database values. For example at timestamp 3 the values 555 552 7789 and 3 were written to the Primary Contact and Bank Balance columns of the Bob row respectively. Sometime later at timestamp 5 the values 555 233 1277 and 12 were written to the Primary Contact and Bank Balance columns of the Alice row. Sometime later still at timestamp 7 the value 2 was written to the Bank Balance column of the Alice row. Also at timestamp 7 the value 13 was written to the Bank Balance column of the Bob row. Thus the most recently written versions of the values of the Primary Contact and Bank Balance columns of the Alice row are 555 233 1277 and 2 respectively. And the most recently written versions of the values the Primary Contact and Bank Balance columns of the Bob row are 555 552 7789 and 13 respectively.

As explained in greater detail below when a client application invokes the GET primitive in the context of a transaction the value that is returned by the transaction orchestrator depends on when the transaction was started by the client application . In other words the time at which the transaction is started determines the snapshot of the database that the client application sees when it invokes the GET primitives in the context of the transaction . In particular when START TX primitive is invoked by the client application the transaction orchestrator contacts the timestamp service to obtain a start transaction timestamp. When a GET primitive is invoked in the context of the transaction the start transaction timestamp determines the version of the requested database value that is fetched from the database by the transaction orchestrator . In particular the transaction orchestrator returns the version of the requested value that has been committed and that has the latest timestamp not later than the start transaction timestamp. For example if a client application accessing the database rows of started a transaction at timestamp 9 then a GET of the value in the Primary Contact column of the Alice row would return the value at timestamp 5 or 555 233 1277 assuming that value has been committed to the database . Similarly a GET of the value in the Primary Contact column of the Bob row would return the value timestamp 3 or 555 552 7789 assuming that value has been committed to the database .

As will be explained in greater detail below in conjunction with discussion of the write and read transaction protocol a value can be written to the database but not committed. Thus the timestamp associated with a particular database value in the database e.g. timestamp 7 with the value 2 in the Bank Balance column of the Alice row indicates when the associated database value was written to the database by a multi row transaction but does not indicate by itself whether the value was actually committed to the database . The database value may not have been committed to the database if the transaction is still pending failed was aborted or conflicted with another transaction.

According to some embodiments the timestamp service is a server computing process or set of processes that provides timestamps to the transaction orchestrator in strictly increasing order. The timestamp service may maintain a numerical counter. Each time the transaction orchestrator requests a timestamp the timestamp service increments the counter by a fixed amount e.g. one and returns the value of the counter to the transaction orchestrator after the increment.

As a scalability optimization the timestamp service may periodically allocate a range of timestamps by writing the highest allocated timestamp of the range to a non volatile storage. Timestamps in the allocated range can then be handed out to the transaction orchestrator more efficiently from volatile memory. If the timestamp service fails or restarts the timestamp service jumps forward to the previously stored highest allocated timestamp but will not go backwards.

In one embodiment as another scalability optimization requests for timestamps from clients of the timestamp service are batched together at the client such that any given client does not send more than a certain number of requests per period of time to the timestamp service so as to avoid overloading the timestamp service with requests.

According to some embodiments the lock service is a server computing process or set of processes that provides lock services to the transaction orchestrator . In particular the lock service provides a network interface by which the transaction orchestrator can request at least four lock operations acquire lock release lock refresh lock and validate lock. The transaction orchestrator may use the lock service to acquire release refresh and validate locks on database rows.

In operation the lock service creates and maintains lease records in volatile memory where the lease records can be more quickly accessed and manipulated. Volatile memory may include any non volatile memory used to implement virtual memory. A lease record represents either a currently held lock or an expired lock on a row of the database .

The lessee identifier identifies the lessee to which the lock represented by the record is or was leased to. In some embodiments the lessees are transactions conducted by the transaction orchestrator and the lessee identifier of a lease record includes the start transaction timestamp of the transaction to which the lock represented by the record is or was leased to.

The lock identifier identifies the entity or thing that is or was locked. The lock identifier may be a character byte sequence that uniquely identifies the entity or thing that is or was locked. In some embodiments the lock identifier of a lease record uniquely identifies a row of the database that is or was locked by a transaction. In some embodiments the lock identifier is a character byte sequence of the form where is the name of the database table that contains the row and is the key of the row in that table. The lock identifier may be formatted otherwise and embodiments are not limited to any particular format so long as the row is uniquely identified within the database .

The end lease time indicates the time that the lease on the lock expires or expired. The end lease time may be updated by the lessee by a refresh lock operation while the lease is active.

The lock type information may contain other record keeping information. For example the lock type information may indicate whether the lessee acquire a read lock or a write lock.

In some embodiments the lock service indexes the lease records stored in volatile memory by their lock identifiers for efficient access and retrieval. A hash table associative array or other suitable indexing data structure may be used for this purpose.

For the acquire lock operation the transaction orchestrator provides a row identifier that uniquely identifies the particular row of the database to be locked and a transaction identifier that identifies the transaction for which the lock is being requested.

In response to receiving the acquire lock request from the transaction orchestrator another transaction may currently hold a lock on the requested row. This may be indicated by an existing lease record for the row that has a lessee identifier different from the one provided in the acquire lock request and that has not yet expired. The lock service can determine whether a lease record has expired by obtaining a current time and determining whether the current time is before the end lease time of the record .

In some embodiments the current time is obtained by the lock service from a clock of the lock service such as for example a clock maintained by the operating system on which the lock service executes. If the current time is before the end lease time then the lock service informs the transaction orchestrator that the acquire lock operation failed.

If another transaction does not currently hold a lock on the requested row then the lock service may create a new lease record for the row or reuse the expired lease record for the row. Where a new lease record is created the lock identifier of the lease record is set to the row identifier provided by the transaction orchestrator . In both cases where a new lease record is created or an existing record is re used the lessee identifier of the lease record for the particular row may be set to include the transaction identifier provided by the transaction orchestrator .

In addition the start lease time of the record is set to a current time obtained by the lock service . The end lease time of the record is determined by adding a fixed time amount to the start lease time . The fixed time amount represents the length of the lease e.g. thirty seconds .

In some embodiments the fixed time amount is a configuration parameter of the lock service . The lock service then indicates to the transaction orchestrator that the lock on the row was successfully obtained.

In some embodiments one of two different types of locks can be acquired on a row through the acquire lock operation. A first type is referred to herein as a read lock and a second type is referred to herein as a write lock. The semantics of read lock and the write lock are enforced by the lock service . In particular a transaction can acquire a read lock on a row if no other transaction currently holds a write lock on the row. A transaction can acquire a write lock on a row if no other transaction currently holds a read lock or a write lock on the row. Thus multiple transactions may concurrently hold a read lock on a row. But only one transaction can hold a write lock on a row at a time and while that transaction holds the write lock on the row no other transactions can acquire a read lock or a write lock on that row. A transaction no longer holds a lock on a row when the lease on the lock expires or the lock is expressly released through a release lock operation.

For the release lock operation the transaction orchestrator provides a row identifier that identifies the locked row to be released and a transaction identifier identifying the transaction that currently holds the lock on the row.

In response to receiving the release lock request from the transaction orchestrator the lock service may invoke the validate lock operation discussed below to ensure that the transaction requesting to release the lock on the row acquired the lock on the row and currently holds the lock on the row. If the transaction acquired and currently holds the lock on the row the lock service updates the lease record for the row to indicate that the current lease has expired. For example the lock service may set to the end lease time to a current time or a time in the past.

For the refresh lock operation the transaction orchestrator provides a row identifier that identifies the locked row to be refreshed and a transaction identifier identifying the transaction that currently holds the lock on the row.

In response to receiving the refresh lock request from the transaction orchestrator the lock service may invoke the validate lock operation discussed below to ensure that the transaction requesting to refresh the lock on the row acquired the lock on the row and currently holds the lock on the row. If the transaction acquired and currently holds the lock on the row the lock service may update the end lease time field of the lease record for the row to extend the time of the lease. For example the lock service may add a fixed time amount to the current end lease time to produce a new end lease time that is then written to the lease record .

The validate lock operation may be used to determine whether a specified transaction acquired a lock on a specified row and still currently holds that lock on the specified row. For the validate lock operation the transaction orchestrator provides a row identifier that identifies the locked row to be validated and a transaction identifier identifying the transaction that purportedly still holds the lock on the row. The lock service may also invoke the validate lock operation internally as part of handling a release lock or refresh lock request.

In response to receiving a validate lock request the lock service obtains the lease record for the specified row. If a lease record for the specified row does not exist then the lock is not validated. If a lease record for the row does exist then the lessee identifier of the lease record is compared to the specified transaction. A current time obtained by the lock service is compared to the end lease time . If the lessee identifier matches or includes the specified transaction and the current time is less than the end lease time then the lock is validated. Otherwise the lock is not validated.

According to some embodiments the transaction orchestrator maintains in the database a transaction table for tracking multi row transactions and for providing ACID compliant multi row transactions with proper snapshot isolation semantics to the client application . In particular the transaction table contains at most one row per transaction. A transaction s row in the transaction table if one exists is keyed by the start transaction timestamp of the transaction. The row has at least one column whose value is a commit timestamp for the transaction if the transaction was successfully committed. If the transaction has been explicitly failed then the value in the one column is an invalid commit timestamp e.g. 1 .

When the client application invokes the START TX primitive the transaction orchestrator in response obtains a timestamp from the timestamp service that is the start transaction timestamp for the transaction. Since timestamp service provides timestamps in a strictly increasing order no two transactions should be associated with the same start transaction timestamp.

When the client application invokes the COMMIT TX primitive for the transaction the transaction orchestrator in response and assuming the transaction can be successfully committed obtains another timestamp from the timestamp service that is the commit timestamp for the transaction. By the strictly increasing order property of the timestamps provided by the timestamp service the commit timestamp of the transaction should be later than the start transaction timestamp of the transaction.

In response to receiving the commit timestamp from the timestamp service the transaction orchestrator may attempt to write a row to the transaction table with a key equal to the start transaction timestamp and a value containing the commit timestamp. In some embodiments this write attempt is a put if absent operation. That is the write succeeds only if a row keyed by start transaction timestamp does not already exist in the transaction table. If the row already exists then the write fails. In some embodiments the put if absent operation is an atomic operation provided and carried out by the database management system .

As described in greater detail below with respect to the read and write transaction protocol the transaction table may be used to coordinate multi row transactions and ensure proper snapshot isolation semantics. For example before a particular time stamped value is returned in response to a GET primitive or other read primitive from the client application the transaction orchestrator consults the transaction table to make sure the transaction that wrote the time stamped value was actually committed.

In embodiments where the database system is configured in a shared nothing architecture the transaction table can be distributed sharded and replicated.

The write transaction protocol will be explained in part by continuing the example of . In particular assume a multi row transaction of the client application for transferring 10 from Bob to Alice is started at timestamp 9. In this example reflects the state of the Alice and Bob rows in the database before the rows have been written to reflect the transfer of the 10. illustrates the state of the Alice and Bob rows after 10 has been subtracted from Bob s account and added to Alice s account.

Turning now to the write transaction protocol at the client application invokes the START TX primitive of the transaction orchestrator to start a transaction. This causes the transaction orchestrator at to message the timestamp service to obtain a start transaction timestamp from the timestamp service for the current transaction. As mentioned previously the start transaction timestamp determines the consistent snapshot of the database seen by any GET primitives invoked by the client application in the context of the current transaction. A start transaction timestamp is returned to the transaction orchestrator at .

A handle to the transaction by which the client application can encapsulate GET primitives and SET primitives and other read and write primitives in the transaction and commit the transaction is returned to the client application at .

At one or more SET primitives are invoked. Each SET primitive is buffered by the transaction orchestrator until commit time.

Assume for the purposes of the current example in which 10 is be transferred from Bob to Alice that the client application reads from the database through two GET primitives in the context of the current transaction the current bank balances of Alice and Bob which prior to the start timestamp for the current transaction in this example 9 is 13 for Bob timestamp 7 and 2 for Alice timestamp 7 . The read protocol is explained in greater detail below. Further assume the client application then invokes a SET primitive to write the value 3 in the Bank Balance column of the Bob row reflecting that 10 is to be withdrawn from Bob s account and invokes another SET primitive to write the value 12 in the Bank Balance column of the Alice row reflecting that 10 is to be added to Alice s account.

At this point the row writes of the SET primitives buffered by the transaction orchestrator have not yet been committed to the database . To attempt to do so the client application at may invoke the COMMIT TX primitive to commit the buffered writes to the database . As mentioned previously the transaction orchestrator uses a two phased approach for committing buffered writes.

In the first commit phase the transaction orchestrator initially attempts to obtain from the lock service a write lock on the current transaction s row of the transaction table. This row may not actually exist in the transaction table before the current transaction is successfully committed. The transaction orchestrator may still acquire a write lock on the row by providing a row identifier to the lock service in an acquire lock operation.

In some embodiments the row identifier provided to the lock service contains the start transaction timestamp of the current transaction. If and when the current transaction is successfully committed the current transaction s row will be written to the transaction table and the write lock on the row released.

As described in greater detail below with respect to the read transaction protocol the write lock on the current transaction s row of the transaction table ensures that other transactions do not read values written to the transaction table by the current transaction before the current transaction has been committed.

The transaction orchestrator does not proceed with the remainder of the first commit phase until the write lock on the current transaction s row in the transaction table can be acquired.

In some embodiments of the first commit phase the transaction orchestrator optionally checks for write write conflicts. As explained in greater detail below this check involves the transaction orchestrator attempting to obtain write locks on all of the database data rows to be written by the current transaction. These are the rows to be written by the SET primitives buffered by the transaction orchestrator for the current transaction.

In one embodiment if any one of these rows cannot be write locked because another transaction currently holds a lock on one of the rows the transaction orchestrator does not proceed with the remainder of the write write conflict check until all of the these rows can be write locked.

Once write locks on all of these rows are obtained the transaction orchestrator then reads from the database the time stamps of the latest time stamped values in the database data cells to be written by the current transaction. For each such time stamp the transaction orchestrator consults the transaction table in the database to determine whether the transaction that wrote the time stamped value committed that time stamped value after the start transaction timestamp of the current transaction. If so then a write write conflict is detected and the transaction orchestrator aborts the current transaction and notifies the client application .

After acquiring a write lock on the current transaction s row of the transaction table if there are no write write conflicts detected or if a write write conflict check was not performed then the transaction orchestrator proceeds to write the buffered row writes to the database .

After the buffered row writes are written to the database the transaction orchestrator queries the lock service to validate the row locks obtained by the current transaction at the start of the first commit phase of the current transaction including the write lock acquired on the current transaction s row of the transaction table and any write locks acquired on written data rows for the write write conflict check. This validation involves querying the lock service to determine whether the acquired row locks have been held by the current transaction since they were obtained including while the write write conflict check was performed if it was performed and while the buffered row writes were written to the database . If the locks cannot be validated the transaction orchestrator aborts the current transaction and notifies the client application . If the locks are validated then the transaction orchestrator proceeds to the second phase of the commit process. The second commit phase is described in greater detail below. Steps of the first commit phase will now be described in greater detail.

At the transaction orchestrator attempts to acquire from the lock service a write lock on the current transaction s row of the transaction table.

In one embodiment the transaction orchestrator sends an acquire lock request to the lock service requesting a write lock on the current transaction s row of the transaction table. For example the row identifier in the acquire lock request may be of the form where is the identifier of the transaction table and is the start transaction timestamp of the current transaction obtained at step .

As mentioned this row many not actually exist in the transaction table of the database when the transaction orchestrator makes this request.

Optionally if write write conflict detection is being performed for the current transaction the transaction orchestrator also attempts to acquire from the lock service a write lock on each data row to be written by the SET primitives encapsulated by the current transaction.

In the current example if a write write conflict check is being performed the transaction orchestrator would attempt to acquire a write lock from the lock service on the Alice row and a write lock on the Bob row. If any one of the write locks on the row or rows to be written cannot be acquired for example because another transaction currently holds one of the locks then the transaction orchestrator waits until the locks can be acquired before proceeding with the remainder of the write write conflict check.

In some embodiments the transaction orchestrator avoids deadlocks by acquiring write locks on the rows to be written according to a total ordering of the locks to be acquired. The total ordering is defined over identifiers of the locks to be acquired according to a lexical ordering. In particular before the transaction orchestrator attempts to acquire write locks on the rows to be written from the lock service the transaction orchestrator generates a lock identifier for each of the rows to be write locked.

This lock identifier may be a character byte sequence. For example the lock identifier may be of the form where is character byte sequence identifying the table in the database that contains the row with the key .

Once lock identifiers for all of the rows to be written by the current transaction have been generated the transaction orchestrator sorts the lock identifiers lexicographically from smallest to largest. The transaction orchestrator then attempts to acquire the write locks on the rows from the lock service in their lexicographically sorted order. If a particular write lock on a row to be written cannot be acquired because another transaction currently holds a lock on the row the transaction orchestrator waits until the write lock on the row can be acquired. This wait may involve the transaction orchestrator repeatedly attempting to acquire the write lock on a periodic basis. Deadlocks are avoided so long as write locks on the rows to be written by a transaction are acquired according to a total ordering. Further this total ordering solution does not require a centralized global deadlock detection process that could hinder horizontal scaling of the system.

In some embodiments the transaction orchestrator at attempts to acquire the write lock on the current transaction s row of the transaction table and all of the write locks on the rows to be written by the current transaction in a single acquire lock request to the lock service . In other embodiments multiple acquire lock requests are made for example an acquire lock request per row lock.

At the transaction orchestrator may optionally check for write write conflicts with the current transaction. A write write conflict exists if another transaction committed a database value to a cell i.e. row column pair of the database that is to be written by the current transaction after the start transaction timestamp of the current transaction.

Two write write conflict scenarios are depicted in . In Scenario 1 Transaction 2 starts before Transaction 1 but commits during Transaction 1. If both Transaction 1 and Transaction 2 write to the same database cell then Transaction 1 will be aborted by the transaction orchestrator . In Scenario 2 Transaction 4 starts after Transaction 3 and commits during Transaction 3. If both Transaction 3 and Transaction 4 write the same database cell then Transaction 3 will be aborted by the transaction orchestrator .

Returning to at to detect any write write conflicts the transaction orchestrator reads from the database the time stamp of the latest value in each database cell to be written by the current transaction. The latest value in a cell is the value with the latest time stamp.

After obtaining all such time stamps the transaction orchestrator reads the transaction table to obtain for each unique one of the obtained time stamps the value in the commit timestamp column of the row keyed by the time stamp. If the row for the time stamp does not exist in the transaction table then the transaction that started at the time stamp has not yet committed. If the row exists and the value in the commit timestamp column of the row is an invalid commit timestamp e.g. 1 then the transaction has been explicitly failed by another transaction. If the row exists and the value in the commit timestamp column of the row is a valid commit timestamp then the transaction orchestrator compares that commit timestamp to the start transaction timestamp for the current transaction. If the commit timestamp is later than the start transaction timestamp for the current transaction a write write conflict has been detected and the transaction orchestrator will abort the current transaction.

For example assume reflects the state of the Alice and Bob rows after write locks are acquired on those rows at . To detect any write write conflicts the transaction orchestrator would read the latest timestamp in each of the database cells to be written by the current transaction. This would result in reading the time stamp 7 from both the Bank Balance column of the Alice row and the Bank Balance column of the Bob row.

Next the transaction orchestrator would read the value in the commit timestamp column of the row in the transaction table having a key equal to 7 . For example assume reflects the transaction table when the transaction orchestrator consults the transaction table at the value 8 would be read as the commit timestamp for the transaction that started at timestamp 7 . Since this commit timestamp 8 is earlier that the start transaction timestamp of the current transaction 9 the transaction orchestrator would determine that there is no write write conflict between the transaction that started at timestamp 7 and the current transaction.

As mentioned the write write conflict check at is optional and need not be performed for every transaction. When the transaction orchestrator performs the write write conflict check at then the transaction orchestrator also acquires at write locks on all the database rows to be written by the current transaction. These write locks are acquired in addition to the write lock on the current transaction s row of the transaction table. If the transaction orchestrator does not perform the write write conflict check at then the write locks on the rows to be written need not be acquired by the transaction orchestrator at . In both cases when a write write conflict check is performed and when a write write conflict check is not performed by the transaction orchestrator the transaction orchestrator acquires at a write lock on the current transaction row s of the transaction table.

In one embodiment whether the transaction orchestrator performs a write write conflict check for the current transaction is based on configuration provided to transaction orchestrator . In one embodiment the configuration is provided to the transaction orchestrator by the client application for example through the programmatic handle returned at for example as an argument to the COMMIT TX primitive invocation. In one embodiment the transaction orchestrator is provided or reads from the database a list of database tables. The transaction orchestrator will perform a write write conflict check for any transaction that writes to one of these tables and will not perform a write write conflict check for a transaction that does not write to any of the listed tables. Alternatively the transaction orchestrator is configured by default to perform a write write conflict check for all transactions except those that write to one the tables in the list. The transaction orchestrator may not perform write write conflict detection for each and every transaction as a performance optimization.

As discussed above to detect write write conflicts with the current transaction the transaction orchestrator reads from the database the time stamp of the latest value in each database cell to be written by the current transaction. Next after obtaining all such time stamps the transaction orchestrator consults the transaction table to obtain for each unique one of the obtained time stamps the value in the commit timestamp column of the row of the transaction table keyed by the time stamp. If the row keyed by the time stamp does not exist in the transaction table this indicates that the transaction that started at the time stamp has not yet committed and may have failed or been aborted. In this case the transaction orchestrator may attempt to explicitly fail the transaction pending commit by attempting to write a row to the transaction table keyed by the time stamp and with an invalid commit timestamp value e.g. 1 .

This write attempt is a put if absent operation such that one of but not both of the following will occur 1 the put if absent operation succeeds and the transaction pending commit is explicitly failed 2 the put if absent operation fails because the transaction pending commit committed in the interim or 3 the put if absent operation fails because another transaction explicitly failed the transaction pending commit in the interim.

In the case of 1 the transaction orchestrator repeats the write write conflict detection check to determine if an earlier committed transaction conflicts with the current transaction. When repeating the write write conflict check the transaction orchestrator ignores those time stamped values read from the cells during the previous write write conflict check iteration that were written by the transaction that was explicitly failed during the previous write write conflict check iteration. Instead for the subsequent iteration of the write write conflict check the transaction orchestrator reads the next latest time stamped values in those cells. The write write conflict check may be repeated by the transaction orchestrator for the next latest time stamped values in those cells and so on if the previous write check conflict iteration successfully explicitly failed a transaction pending commit.

In the case of 2 the current transaction is aborted by the transaction orchestrator as it conflicts with the now committed transaction that was previously pending commit.

In the case of 3 the put if absent operation failure by the current transaction is ignored as the other transaction successfully explicitly failed the transaction pending commit.

Optionally in the case of 1 or 3 if the transaction pending commit is successfully explicitly failed the transaction orchestrator may also delete or remove the time stamped values from the database written to the database cells by the now failed transaction pending commit.

Next assuming there are no write write conflicts and the current transaction has not been aborted the transaction orchestrator at writes the buffered SET primitives to the database .

Each value written to the database is time stamped with the start transaction timestamp for the current transaction. For example illustrates the state of the Alice and Bob rows after 10 has been subtracted from Bob s account and added to Alice s account. In particular the value 3 is written with time stamp 9 to the Bank Balance column of the Bob row reflecting that 10 has been subtracted from Bob s account. The value 12 is written with time stamp 9 to the Bank Balance column of the Alice row reflecting that 10 has been added to Alice s account.

After writing the buffered values to the database rows and before actually committing the current transaction the transaction orchestrator queries the lock service at to ensure that the locks that were acquired from the lock service at were actually held by the current transaction since they were acquired including during when the write write conflict check was performed at if it was performed and during when the buffered writes were written to the database at .

In the current example the transaction orchestrator at would query the lock service to validate that the write lock on the current transaction s row of the transaction table acquired at has been locked by the current transaction since it was acquired at . In addition if the write write conflict check was performed at the transaction at would also query the lock service to validate that the write lock on the Alice row and the write lock on the Bob row acquired at have been locked by the current transaction since they were acquired at . The lock service may validate these locks by checking that lease records for the locked rows exist identify the current transaction according the lessee identifiers and have not expired according to the end lease times .

If the locks acquired at cannot be validated at then the transaction orchestrator aborts the current transaction and notifies the client application . Otherwise the transaction orchestrator proceeds to the second phase of the two phase commit process.

At the beginning of the second phase the transaction orchestrator at obtains a commit timestamp for the current transaction from the timestamp service . Assume for the current example that the commit timestamp obtained from the timestamp service is 10 .

Next the transaction orchestrator performs at the put if absent operation discussed above in an attempt to atomically write a row to the transaction table keyed by the start timestamp for the current transaction and having the commit timestamp obtained as value.

In the current example the transaction orchestrator would attempt to put if absent a row keyed by the start transaction timestamp of 9 with a commit timestamp value of 10 . This is shown in .

At this point assuming the put if absent operation at is successful the current transaction has been committed to the database . At the transaction orchestrator performs any cleanup operations include messaging the lock service to release the locks acquired during the first commit phase. If the put if absent operation at failed then the transaction coordinator aborts the current transaction.

At a success indication may be provided to the client application to indicate that the multi row transaction was successfully committed.

Turning now to the read transaction protocol interaction depicted in at the client application invokes the START TX primitive of the transaction orchestrator . This causes the transaction orchestrator at to message the timestamp service to obtain a start transaction timestamp from the timestamp service . As mentioned previously the start transaction timestamp determines the consistent snapshot of the database seen by any GET primitives invoked by the client application in the context of the current transaction.

A handle to the transaction by which the client application can encapsulate GET primitives and other read primitives in the transaction is returned to the client application at .

At a GET primitive is invoked by the client application in the context of the current transaction. The read primitive specifies a database cell i.e. row column pair to read.

In response to the GET primitive invocation the transaction orchestrator at steps and successively obtains time stamped database values from the requested database cells starting with the latest time stamped database values prior to the start transaction timestamp of the current transaction and proceeding if necessary to the next latest time stamped databases value prior to the start transaction timestamp of the current transaction and so on as necessary until valid time stamped values are obtained for each of the requested cells or it is determined that no valid value exists in a cell. A valid time stamped database value is one that is committed before the start transaction timestamp of the current transaction. If there is no valid time stamped database value in a cell then it is determined that no valid value exists in the cell in the consistent snapshot of the database corresponding to the start transaction timestamp of the current transaction.

In response to the read primitive invocation the transaction orchestrator initially associates the database cell to be read with a snapshot timestamp . Initially the snapshot timestamp associated with each database cell to be read is the start transaction timestamp of the current transaction.

At the transaction orchestrator reads the latest time stamped database value from the requested database cell. In particular the transaction orchestrator reads the latest time stamped value in the cell that is time stamped no later than the snapshot timestamp associated with the cell. Step involves not just reading the latest time stamp database value but also reading the timestamp associated with the value in the cell. This timestamp may be referred to as the value s write timestamp. This write timestamp is the same as the start transaction timestamp of the transaction that wrote the value associated with the write timestamp to the cell. If there is no database value in the cell having a write timestamp equal to or earlier than the snapshot timestamp associated with the cell then the cell has no value for the snapshot of the database that the current transaction sees as dictated by the current transaction s start timestamp. In this case the transaction orchestrator at may return an empty value for the cell.

At the transaction orchestrator attempts to acquire a read lock on the row in the transaction table corresponding to the write timestamp read from the cell at . By attempting to acquire a read lock on the row in the transaction table corresponding to the write timestamp read from the cell at if the transaction that wrote the value associated with the write timestamp read from a cell at is currently in the process of committing and currently holds a write lock on the row in the transaction table for the transaction i.e. the row keyed by the write timestamp the current transaction will block as necessary until that transaction has either successfully committed or the write lock on the row has timed out. Once the read lock has been successfully acquired by the current transaction the transaction orchestrator may immediately release the read lock.

At the transaction orchestrator attempts to read the row in the transaction table corresponding to the write timestamp read from the cell at . The row may not exist may exist with an invalid commit timestamp 1 may exist with a commit timestamp greater than the start transaction timestamp of the current transaction or may exist with a commit timestamp less than the start transaction timestamp of the current transaction.

If the row does not exist in the transaction table then the transaction that started at the write timestamp has either aborted or timed out. Where the row does not exist the transaction orchestrator may attempt to explicitly fail the corresponding transaction by performing a put if absent operation against the transaction table that attempts to add a row to the transaction table keyed by the write timestamp and having an invalid commit timestamp value e.g. 1 .

If the row does exist in the transaction table then the commit timestamp value is obtained from the row.

If the commit timestamp value is an invalid commit timestamp e.g. 1 then the transaction that started at the write timestamp has been explicitly failed.

If the commit timestamp value is valid then the transaction orchestrator determines whether the valid commit timestamp value is ordered before or after the current transaction start timestamp. If ordered before then the value associated with the write timestamp is the latest value in the corresponding cell that committed before the current transaction was started. This value is returned for the cell by the transaction orchestrator at .

After a first performance of steps and by the transaction orchestrator the latest time stamped database value read from the cell may have been written by a transaction that has been explicitly failed not yet committed or that committed after the current transaction was started. When this occurs the transaction orchestrator repeats steps and as often as necessary until a valid value can be read from the cell or until it is determined that no valid value exists in the cell. When repeating steps and for the cell the snapshot timestamp associated with a cell during the previous performance of steps and is set to the write timestamp read from the cell during the previous performance. In a dashed box is drawn around interactions and to indicate that these interactions may be successively repeated multiple times by the transaction orchestrator when attempting to obtain a valid time stamped database value from a requested cell.

According to one embodiment the techniques described herein are implemented by one or more special purpose computing devices. The special purpose computing devices may be hard wired to perform the techniques or may include digital electronic devices such as one or more application specific integrated circuits ASICs or field programmable gate arrays FPGAs that are persistently programmed to perform the techniques or may include one or more general purpose hardware processors programmed to perform the techniques pursuant to program instructions in firmware memory other storage or a combination. Such special purpose computing devices may also combine custom hard wired logic ASICs or FPGAs with custom programming to accomplish the techniques. The special purpose computing devices may be desktop computer systems portable computer systems handheld devices networking devices or any other device that incorporates hard wired and or program logic to implement the techniques.

For example is a block diagram that illustrates a computer system upon which an embodiment of the invention may be implemented. Computer system includes a bus or other communication mechanism for communicating information and a hardware processor coupled with bus for processing information. Hardware processor may be for example a general purpose microprocessor.

Computer system also includes a main memory such as a random access memory RAM or other dynamic storage device coupled to bus for storing information and instructions to be executed by processor . Main memory also may be used for storing temporary variables or other intermediate information during execution of instructions to be executed by processor . Such instructions when stored in storage media accessible to processor render computer system into a special purpose machine that is customized to perform the operations specified in the instructions.

Computer system further includes a read only memory ROM or other static storage device coupled to bus for storing static information and instructions for processor . A storage device such as a magnetic disk or optical disk is provided and coupled to bus for storing information and instructions.

Computer system may be coupled via bus to a display such as a cathode ray tube CRT for displaying information to a computer user. An input device including alphanumeric and other keys is coupled to bus for communicating information and command selections to processor . Another type of user input device is cursor control such as a mouse a trackball or cursor direction keys for communicating direction information and command selections to processor and for controlling cursor movement on display . This input device typically has two degrees of freedom in two axes a first axis e.g. x and a second axis e.g. y that allows the device to specify positions in a plane.

Computer system may implement the techniques described herein using customized hard wired logic one or more ASICs or FPGAs firmware and or program logic which in combination with the computer system causes or programs computer system to be a special purpose machine. According to one embodiment the techniques herein are performed by computer system in response to processor executing one or more sequences of one or more instructions contained in main memory . Such instructions may be read into main memory from another storage medium such as storage device . Execution of the sequences of instructions contained in main memory causes processor to perform the process steps described herein. In alternative embodiments hard wired circuitry may be used in place of or in combination with software instructions.

The term non transitory media as used herein refers to any media that store data and or instructions that cause a machine to operation in a specific fashion. Such non transitory media may comprise non volatile media and or volatile media. Non volatile media includes for example optical or magnetic disks such as storage device . Volatile media includes dynamic memory such as main memory . Common forms of non transitory media include for example a floppy disk a flexible disk hard disk solid state drive magnetic tape or any other magnetic data storage medium a CD ROM any other optical data storage medium any physical medium with patterns of holes a RAM a PROM and EPROM a FLASH EPROM NVRAM any other memory chip or cartridge.

Non transitory media is distinct from but may be used in conjunction with transmission media. Transmission media participates in transferring information between non transitory media. For example transmission media includes coaxial cables copper wire and fiber optics including the wires that comprise bus . Transmission media can also take the form of acoustic or light waves such as those generated during radio wave and infra red data communications.

Various forms of media may be involved in carrying one or more sequences of one or more instructions to processor for execution. For example the instructions may initially be carried on a magnetic disk or solid state drive of a remote computer. The remote computer can load the instructions into its dynamic memory and send the instructions over a telephone line using a modem. A modem local to computer system can receive the data on the telephone line and use an infra red transmitter to convert the data to an infra red signal. An infra red detector can receive the data carried in the infra red signal and appropriate circuitry can place the data on bus . Bus carries the data to main memory from which processor retrieves and executes the instructions. The instructions received by main memory may optionally be stored on storage device either before or after execution by processor .

Computer system also includes a communication interface coupled to bus . Communication interface provides a two way data communication coupling to a network link that is connected to a local network . For example communication interface may be an integrated services digital network ISDN card cable modem satellite modem or a modem to provide a data communication connection to a corresponding type of telephone line. As another example communication interface may be a local area network LAN card to provide a data communication connection to a compatible LAN. Wireless links may also be implemented. In any such implementation communication interface sends and receives electrical electromagnetic or optical signals that carry digital data streams representing various types of information.

Network link typically provides data communication through one or more networks to other data devices. For example network link may provide a connection through local network to a host computer or to data equipment operated by an Internet Service Provider ISP . ISP in turn provides data communication services through the world wide packet data communication network now commonly referred to as the Internet . Local network and Internet both use electrical electromagnetic or optical signals that carry digital data streams. The signals through the various networks and the signals on network link and through communication interface which carry the digital data to and from computer system are example forms of transmission media.

Computer system can send messages and receive data including program code through the network s network link and communication interface . In the Internet example a server might transmit a requested code for an application program through Internet ISP local network and communication interface .

The received code may be executed by processor as it is received and or stored in storage device or other non volatile storage for later execution.

In the foregoing specification embodiments have been described with reference to numerous specific details that may vary from implementation to implementation. Thus the sole and exclusive indicator of what is the invention and is intended by the applicants to be the invention is the set of claims that issue from this application in the specific form in which such claims issue including any subsequent correction. Any definitions expressly set forth herein for terms contained in such claims shall govern the meaning of such terms as used in the claims. Hence no limitation element property feature advantage or attribute that is not expressly recited in a claim should limit the scope of such claim in any way. The specification and drawings are accordingly to be regarded in an illustrative rather than a restrictive sense.

