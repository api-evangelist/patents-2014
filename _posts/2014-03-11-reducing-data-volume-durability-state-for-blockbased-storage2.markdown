---

title: Reducing data volume durability state for block-based storage
abstract: A block-based storage system may implement reducing durability state for a data volume. A determination may be made that storage node replicating write requests for a data volume is unavailable. In response, subsequent write requests may be processed according to a reduced durability state for the data volume such that replication for the data volume may be disabled for the storage node. Write requests may then be completed at a fewer number of storage nodes prior to acknowledging the write request as complete. Durability state for the data volume may be increase in various embodiments. A storage node may be identified and replication operations may be performed to synchronize the current data volume at the storage node with a replica of the data volume maintained at the identified storage node.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09600203&OS=09600203&RS=09600203
owner: Amazon Technologies, Inc.
number: 09600203
owner_city: Reno
owner_country: US
publication_date: 20140311
---
The recent revolution in technologies for dynamically sharing virtualizations of hardware resources software and information storage across networks has increased the reliability scalability and cost efficiency of computing. More specifically the ability to provide on demand virtual computing resources and storage through the advent of virtualization has enabled consumers of processing resources and storage to flexibly structure their computing and storage costs in response to immediately perceived computing and storage needs. Virtualization allows customers to purchase processor cycles and storage at the time of demand rather than buying or leasing fixed hardware in provisioning cycles that are dictated by the delays and costs of manufacture and deployment of hardware. Rather than depending on the accuracy of predictions of future demand to determine the availability of computing and storage users are able to purchase the use of computing and storage resources on a relatively instantaneous as needed basis.

Virtualized computing environments are frequently supported by block based storage. Such block based storage provides a storage system that is able to interact with various computing virtualizations through a series of standardized storage calls that render the block based storage functionally agnostic to the structural and functional details of the volumes that it supports and the operating systems executing on the virtualizations to which it provides storage availability. However as network transmission capabilities increase along with greater processing capacity for virtualized resources I O demands upon block based storage may grow. If I O demands exceed the capacity of block based storage to service requests then latency and or durability of block based storage performance for virtualized computing resources suffer resulting in a loss of virtualized computing performance.

While embodiments are described herein by way of example for several embodiments and illustrative drawings those skilled in the art will recognize that the embodiments are not limited to the embodiments or drawings described. It should be understood that the drawings and detailed description thereto are not intended to limit embodiments to the particular form disclosed but on the contrary the intention is to cover all modifications equivalents and alternatives falling within the spirit and scope as defined by the appended claims. The headings used herein are for organizational purposes only and are not meant to be used to limit the scope of the description or the claims. As used throughout this application the word may is used in a permissive sense i.e. meaning having the potential to rather than the mandatory sense i.e. meaning must . Similarly the words include including and includes mean including but not limited to.

The systems and methods described herein may implement reducing data volume durability state for block based storage. Block based storage systems may establish a durability state for data volumes maintained by the block based storage systems by increasing a number of replicas maintaining a current version of a data volume in various embodiments. The durability state for a data volume may be enforced for the data volume as part of a rule requirement or other guarantee of the block based storage system maintaining the data volume. Replication operations among storage nodes maintaining replicas of a particular data volume may ensure that changes to a data volume may be made across the data volume replicas in order to maintain the current version of the data volume in accordance with the durability state for the data volume. However replication operations among storage nodes may become disrupted. Network partitions high network utilization or system failures are some of the many different events that may disrupt replication operations for a data volume. These disruptions in replication operations may result in a storage node maintaining a replica of data volume becoming unavailable for replication with other storage nodes violating the durability state for the data volume. Write requests and other input output operations directed toward the data volume may be blocked as the durability state for the data volume is not satisfied. Reducing durability state for data volumes in block based storage systems may allow operations directed toward the data volume to continue with little interruption in the event that durability state for the data volume cannot be maintained. In some embodiments a reduction in durability state for a data volume may not significantly risk the durability of a data volume e.g. if only done so for a limited time .

The system and methods described herein may implement efficient data volume replication. As noted above multiple replicas of data volumes may be maintained at different storage nodes to establish durability for a data volume in some embodiments. If a data volume is no longer maintained at multiple storage nodes such as may occur when the data volume is in a reduced durability state modifications to portions of the data volume may be maintained in order to indicate changes made to a data volume upon entering a reduced durability state. For example if a data volume is maintained at a master storage node and a slave storage node and the slave storage node becomes unavailable for replication the master storage node may continue to process input output I O for the data volume and record which portions of the data volume have been changed. At a later time the master storage node may be able increase the durability state of the data volume by replicating the data volume at another storage node maintaining a stale replica of the data volume such as the prior slave storage node or another storage node maintaining a replica of the data volume . Modified portions of the data volume may be sent to update the stale replica without sending portions of the data volume that are not out of data in the state replica reducing the amount of time to perform the replication and or lower the amount network traffic between the master storage node and the new slave storage node. Once the stale replica is up to date replication operations for the data volume between the master storage node and the new slave storage node may be enabled for future changes to the data volume.

As illustrated in scene I O requests e.g. write requests are received at the master storage node which may complete the I O requests . For example write requests to modify portions of data volume received at master storage node may be performed. The I O requests may then in some embodiments be forwarded on to slave storage node for replication of any changes made to the data at the master storage node . In this way the current durability state for the data volume may be satisfied.

As illustrated at scene slave storage node has become unavailable . This may be due to network conditions such as a network partition high amounts of network traffic system failure or under the direction of a control system e.g. which may direct slave storage node to cease maintaining volume replica . I O requests may however still be received at master storage node . Instead of blocking failing or otherwise not completing write requests the durability state for data volume may be reduced. For instance in some embodiments I O requests may be completed at master storage node and acknowledged back to a client or other system component or device that requested I O . Therefore the I O requests as received in scene may be completed at master storage node without being completed at any other storage node such as slave storage node or another storage node in block based storage service . In another example with more than two Storage nodes e.g. 3 storage nodes reducing the durability state for a data volume maintained at the 3 storage nodes such as reducing the durability state to 2 storage nodes may allow for I O requests to be completed at a fewer number of storage nodes prior to acknowledging the I O requests as complete.

In various embodiments master storage node may track the changes made to data volume while operating in a reduced durability state. For example data chunks that are modified may be marked indexed listed or otherwise identified e.g. in data volume metadata . At some time a determination may be made to enable replication for a data volume in order to process I O requests according to an increased the durability state for data volume . This determination may be made based on several factors such as the availability of storage nodes to store another replica of a data volume current network utilization traffic or other network events or the state of durability for the data volume e.g. replicated on 1 storage node 2 storage nodes etc. . In order to increase the durability state another storage node to maintain the data volume replica may be identified. For example a control system may send a list of storage node identifiers to master storage node or master storage node itself may retain a list of previous storage nodes that maintained replicas of the data volume. Once a slave storage node is identified in the illustrated example the identified storage node is the most recent storage node that maintained a replica of the data volume however other storage nodes that also maintain a stale replica of the data volume or no replica of the data volume at all may be selected the modified data chunks may be replicated to the slave storage node to be stored in volume replica as illustrated in scene . Unmodified data chunks need not be replicated. As illustrated at scene replication may again be enabled for the data volume as I O requests are again sent to slave storage node for replication increasing the durability state for data volume to be maintained at both master storage node and slave storage node .

Please note that previous descriptions are not intended to be limiting but are merely provided as a reducing state and efficient replication for block based storage systems. For example the number of storage nodes or the number of data volumes may be different than illustrated in . Different replication schemes e.g. no master or slave roles may be implemented and different durability states may be established for a data volume. In some embodiments a reduction in durability state may occur when more than one storage node becomes unavailable e.g. 3 storage nodes to 1 available storage node .

This specification begins with a general description block based storage services provider which may implement reducing data volume durability state and efficient data volume replication. Then various examples of a block based storage services provider are discussed including different components modules or arrangements of components module that may be employed as part of implementing a block based storage services provider. A number of different methods and techniques to implement reducing data volume durability state and efficient data volume replication are then discussed some of which are illustrated in accompanying flowcharts. Finally a description of an example computing system upon which the various components modules systems devices and or nodes may be implemented is provided. Various examples are provided throughout the specification.

As noted above virtual compute service may offer various compute instances to clients . A virtual compute instance may for example comprise one or more servers with a specified computational capacity which may be specified by indicating the type and number of CPUs the main memory size and so on and a specified software stack e.g. a particular version of an operating system which may in turn run on top of a hypervisor . A number of different types of computing devices may be used singly or in combination to implement the compute instances of virtual compute service in different embodiments including general purpose or special purpose computer servers storage devices network devices and the like. In some embodiments instance clients or other any other user may be configured and or authorized to direct network traffic to a compute instance. In various embodiments compute instances may attach or map to one or more data volumes provided by block based storage service in order to obtain persistent block based storage for performing various operations.

Compute instances may operate or implement a variety of different platforms such as application server instances Java virtual machines JVMs general purpose or special purpose operating systems platforms that support various interpreted or compiled programming languages such as Ruby Perl Python C C and the like or high performance computing platforms suitable for performing client applications without for example requiring the client to access an instance. In some embodiments compute instances have different types or configurations based on expected uptime ratios. The uptime ratio of a particular compute instance may be defined as the ratio of the amount of time the instance is activated to the total amount of time for which the instance is reserved. Uptime ratios may also be referred to as utilizations in some implementations. If a client expects to use a compute instance for a relatively small fraction of the time for which the instance is reserved e.g. 30 35 of a year long reservation the client may decide to reserve the instance as a Low Uptime Ratio instance and pay a discounted hourly usage fee in accordance with the associated pricing policy. If the client expects to have a steady state workload that requires an instance to be up most of the time the client may reserve a High Uptime Ratio instance and potentially pay an even lower hourly usage fee although in some embodiments the hourly fee may be charged for the entire duration of the reservation regardless of the actual number of hours of use in accordance with pricing policy. An option for Medium Uptime Ratio instances with a corresponding pricing policy may be supported in some embodiments as well where the upfront costs and the per hour costs fall between the corresponding High Uptime Ratio and Low Uptime Ratio costs.

Compute instance configurations may also include compute instances with a general or specific purpose such as computational workloads for compute intensive applications e.g. high traffic web applications ad serving batch processing video encoding distributed analytics high energy physics genome analysis and computational fluid dynamics graphics intensive workloads e.g. game streaming 3D application streaming server side graphics workloads rendering financial modeling and engineering design memory intensive workloads e.g. high performance databases distributed memory caches in memory analytics genome assembly and analysis and storage optimized workloads e.g. data warehousing and cluster file systems . Size of compute instances such as a particular number of virtual CPU cores memory cache storage as well as any other performance characteristic. Configurations of compute instances may also include their location in a particular data center availability zone geographic location etc. . . . and in the case of reserved compute instances reservation term length.

In various embodiments provider network may also implement block based storage service for performing storage operations. Block based storage service is a storage system composed of a pool of multiple independent storage nodes through e.g. server block data storage systems which provide block level storage for storing one or more sets of data volumes data volume s through . Data volumes may be mapped to particular clients providing virtual block based storage e.g. hard disk storage or other persistent storage as a contiguous set of logical blocks. In some embodiments a data volume may be divided up into multiple data chunks including one or more data blocks for performing other block storage operations such as snapshot operations or replication operations.

A volume snapshot of a data volume may be a fixed point in time representation of the state of the data volume . In some embodiments volume snapshots may be stored remotely from a storage node maintaining a data volume such as in another storage service . Snapshot operations may be performed to send copy and or otherwise preserve the snapshot of a given data volume in another storage location such as a remote snapshot data store in other storage service .

Block based storage service may implement block based storage service control plane to assist in the operation of block based storage service . In various embodiments block based storage service control plane assists in managing the availability of block data storage to clients such as programs executing on compute instances provided by virtual compute service and or other network based services located within provider network and or optionally computing systems not shown located within one or more other data centers or other computing systems external to provider network available over a network . Access to data volumes may be provided over an internal network within provider network or externally via network in response to block data transaction instructions.

Block based storage service control plane may provide a variety of services related to providing block level storage functionality including the management of user accounts e.g. creation deletion billing collection of payment etc. . Block based storage service control plane may further provide services related to the creation usage and deletion of data volumes in response to configuration requests. Block based storage service control plane may also provide services related to the creation usage and deletion of volume snapshots on other storage service . Block based storage service control plane may also provide services related to the collection and processing of performance and auditing data related to the use of data volumes and snapshots of those volumes.

Provider network may also implement another storage service as noted above. Other storage service may provide a same or different type of storage as provided by block based storage service . For example in some embodiments other storage service may provide an object based storage service which may store and manage data as data objects. For example volume snapshots of various data volumes may be stored as snapshot objects for a particular data volume . In addition to other storage service provider network may implement other network based services which may include various different types of analytical computational storage or other network based system allowing clients as well as other services of provider network e.g. block based storage service virtual compute service and or other storage service to perform or request various tasks.

Clients may encompass any type of client configurable to submit requests to network provider . For example a given client may include a suitable version of a web browser or may include a plug in module or other type of code module configured to execute as an extension to or within an execution environment provided by a web browser. Alternatively a client may encompass an application such as a database application or user interface thereof a media application an office application or any other application that may make use of compute instances a data volume or other network based service in provider network to perform various operations. In some embodiments such an application may include sufficient protocol support e.g. for a suitable version of Hypertext Transfer Protocol HTTP for generating and processing network based services requests without necessarily implementing full browser support for all types of network based data. In some embodiments clients may be configured to generate network based services requests according to a Representational State Transfer REST style network based services architecture a document or message based network based services architecture or another suitable network based services architecture. In some embodiments a client e.g. a computational client may be configured to provide access to a compute instance or data volume in a manner that is transparent to applications implement on the client utilizing computational resources provided by the compute instance or block storage provided by the data volume .

Clients may convey network based services requests to provider network via external network . In various embodiments external network may encompass any suitable combination of networking hardware and protocols necessary to establish network based communications between clients and provider network . For example a network may generally encompass the various telecommunications networks and service providers that collectively implement the Internet. A network may also include private networks such as local area networks LANs or wide area networks WANs as well as public or private wireless networks. For example both a given client and provider network may be respectively provisioned within enterprises having their own internal networks. In such an embodiment a network may include the hardware e.g. modems routers switches load balancers proxy servers etc. and software e.g. protocol stacks accounting software firewall security software etc. necessary to establish a networking link between given client and the Internet as well as between the Internet and provider network . It is noted that in some embodiments clients may communicate with provider network using a private network rather than the public Internet.

Block based storage service may manage and maintain data volumes in a variety of different ways. Different durability schemes may be implemented for some data volumes among two or more storage nodes maintaining a replica of a data volume. For example different types of mirroring and or replication techniques may be implemented e.g. RAID 1 to increase the durability of a data volume such as by eliminating a single point of failure for a data volume. In order to provide access to a data volume storage nodes may then coordinate I O requests such as write requests among the two or more storage nodes maintaining a replica of a data volume. For storage nodes providing write optimization during a snapshot operation additional coordination may be implemented.

Block based storage service may manage and maintain data volumes in a variety of different ways. Different durability schemes may be implemented for some data volumes among two or more storage nodes maintaining a same replica of a data volume establishing a durability state for a data volume. For example different types of mirroring and or replication techniques may be implemented e.g. RAID 1 to increase the durability of a data volume such as by eliminating a single point of failure for a data volume. In order to provide access to a data volume storage nodes may then coordinate I O requests such as write requests among the two or more storage nodes maintaining a replica of a data volume. For example for a given data volume storage node may serve as a master storage node. A master storage node may in various embodiments receive and process requests e.g. I O requests from clients of the data volume. Thus storage node may then coordinate replication of I O requests such as write requests or any other changes or modifications to data volume to one or more other storage nodes serving as slave storage nodes. For instance storage node may maintain data volume which is a replica of data volume . Thus when a write request is received for data volume at storage node storage node may forward the write request to storage node and wait until storage node acknowledges the write request as complete before completing the write request at storage node . Master storage nodes may direct other operations for data volumes like snapshot operations or other I O operations e.g. serving a read request .

Please note that in some embodiments the role of master and slave storage nodes may be assigned per data volume. For example for data volume maintained at storage node storage node may serve as a master storage node. While for another data volume such as data volume maintained at storage node storage node may serve as a slave storage node.

In various embodiments storage nodes may each implement a respective page cache. A page cache may be a portion of system memory or other memory device that stores pages or other groupings of data from one of the data volumes maintained a respective storage node. Instead of directly writing to or reading from a block based storage device maintaining the portion of requested data of a data volume the page cache may be updated. For example if a read request is received for a portion of a data volume it may first be determined whether the data resides in the page cache. If yes then the data may be read from the page cache. If no then the data may be retrieved from the respective block based storage device maintaining the portion of the requested data of the data volume and written into the page cache for future use. Similarly a write request may be directed toward the page cache. For a received write request a page cache entry corresponding to the data to be written e.g. a portion of a data volume already maintained in the page cache may be updated according to the write request. Similarly if the data to be modified by the write request is not in the page cache the data may be first obtained from the block based storage device that persists the data written into a new page cache entry in the page cache and then updated according to the write request. Page cache techniques are well known to those of ordinary skill in the art and thus the previous examples are not intended to be limiting as to other page cache techniques.

In various embodiments storage nodes may implement respective page cache logs such as page cache logs through . Page cache logs may store log records describing updates to the respective page cache such as write requests that modify data maintained in the page cache . Thus in the event of a system or other failure that causes a loss of data in the page cache log records in the page cache log may be used to restore the page cache to a state prior to the failure. Log records may be stored sequentially according to the order in which updates are made to page cache in some embodiments. For example write request A is received performed and a corresponding log record A is stored. Then write request B is received performed and a corresponding log record B is stored and so on. By storing log records sequentially log records may be replayed or re applied in the order in which they are stored to generate a state of the page cache at a particular point in time.

Moreover sequential storage of log records may decrease write latency for storing log records. Page cache logs may be implemented as a persistent storage device e.g. a hard disk drive solid state drive SSD or other block based storage device . Sequential writes to such types of persistent storage devices are often faster than random access writes. In various embodiments page cache logs may be implemented on a dedicated block storage device for the storage node. The persistent storage device persisting the page cache log may be separate from block based storage devices persisting data volumes at a storage node in some embodiments.

Storage nodes may implement respective I O managers such as I O managers through . I O managers may handle I O request directed toward data volumes maintained at a particular storage node. Thus I O manager may process and handle a write request to volume at storage node for example. I O manager may be configured to process I O requests according to block based storage service application programming interface API and or other communication protocols such as such as internet small computer system interface iSCSI . In some embodiments I O managers may implement respective log management components such as log management through . Log management components may perform various log management functions such as trimming log records and or performing page cache recovery based at least in part on log records.

Please note that provides an example of storage nodes that may be implemented in a block based storage service. Other arrangements or configurations of storage nodes may also be implemented in some embodiments such as storage nodes that do not implement a page cache write log

At some point master storage node or control plane may wish to resume a greater durability state for the data volume. For instance master storage node may wait until a certain amount of time has elapsed or a certain number of modifications to the data volume are received before attempting to increase the durability state for the data volume. A request from master storage node may be sent to storage service control plane to authorize the enabling of replication for the data volume . Based on factors similar to those discussed above the storage service control plane may send a list of candidate slave storage nodes to the master storage node . The list of candidate storage nodes may include storage nodes that maintain stale replicas of the data volume that are not current and or storage nodes that do not maintain a replica of the data volume but have capacity to store a replica of the data volume. In some embodiments master storage node may not need to obtain a list of candidate storage nodes but may maintain a list of candidate slave storage nodes locally e.g. storage nodes the master storage node previously replicated with for the data volume . Master storage node may select a slave node from the list of candidate storage nodes such as new slave storage node to replicate a data volume with 450. Master storage node may send a request to create a new volume replica of the data volume on slave storage node . Slave storage node may acknowledge the crated volume replica . Master storage node may then commence one or more replication operations to replicate the volume as maintained at the master storage node . Each replication operation may send a portion of the data volume to new slave storage node to be stored. Once replication is complete master storage node may enable replication again for the data volume processing I O requests in accordance with the increased durability state of the data volume. For example I O request received at master storage node is now sent to new slave storage node .

In some embodiments various efficient data volume replication techniques may be implemented when transitioning from a reduced durability state to an increase durability state for a data volume. is a sequence diagram illustrating interactions between master storage nodes and a slave storage node performing efficient data volume replication operations from a reduced durability state to an increased durability state according to some embodiments. Client may send an I O request to master storage node . As discussed above with regard to the I O request may fail because the slave storage node is unavailable. Master storage node may request authorization to disable replication and enter a reduced durability state for the data volume. Based on various factors e.g. is there a network partition mass failure large amount of network traffic the storage service control plane may authorize disabling of replication allowing master storage node to again complete and acknowledge I O .

At some point in time master storage node may wish to increase the durability state for the data volume and request authorization to enable replication from storage service control plane . Again as above based on various factors storage service control plane may authorize replication and send candidate slave nodes to master storage node. Candidate slave nodes may include storage nodes that maintain stale i.e. out of date replicas of the data volume. Stale replicas may result from previous pairings between the candidate storage node and the master storage node replicating changes to the data volume. Using the listing of candidate storage nodes master storage node may be configured to select a slave to storage node such as by identifying the slave storage node with the least amount of data to replicate. For example in some embodiments master storage node may send volume metadata indicating version numbers for data chunks in the volume to a prospective slave storage node. The slave storage node may evaluate the volume metadata by comparing it to its own volume metadata for the stale replica and identify data chunks in its own replica the need to be replicated.

The identified data chunks may be indicated to the master storage node . If there are more than one slave storage nodes with stale replicas the slave storage node with the least number of data chunks needed or the greatest number of non stale data chunks may selected as the new slave storage node . In some embodiments this selection may be made in combination with other factors such as the current amount of network traffic being directed toward a candidate slave storage node and or the workload of the slave storage node. In slave storage node is selected as the most recent slave storage node it had the least differences with the data volume on the master storage node . Identified data chunks are then replicated to the slave storage node to synchronize the replicas of the data volume maintained at the master storage node and the slave storage node . In this way data chunks in the stale replica of the data volume need not be updated in various embodiments. Replication may then be enabled 571 for the data volume in order to process requests according to an increased durability state for the data volume. Thus I O request sent to master storage node may be sent on to slave storage node as part of replicating the data volume restoring the durability state of the data volume.

Please note that in some embodiments the role of master and slave storage nodes may be assigned per data volume. For example for one data volume maintained at a storage node the storage node may serve as a master storage node. While for another data volume maintained at the same storage node the storage node may serve as a slave storage node.

The examples of reducing data volume durability state for block based storage and efficient data volume replication for block based storage discussed above with regard to have been given in regard to a block based storage service. Various other types or configurations of block based storage may implement these techniques. For example different configurations of storage nodes may also implement various numbers of replicas mirroring or other durability techniques that may establish a durability state for the data volume different than a master and slave s model discussed above. is a high level flowchart illustrating various methods and techniques for reducing data volume durability for block based storage according to some embodiments. These techniques may be implemented using one or more storage nodes or other system component that maintains a data volume in block based storage as described above with regard to .

A group of storage nodes may in some embodiments maintain replicas of a data volume in block based storage. The group of storage nodes may establish a durability state for the data volume by completing write requests received for the data volume at each of the storage nodes in the group before acknowledging a write request as complete in order to perform replication for the data volume. In some embodiments these storage nodes may be referred to as a peer storage node. As indicated at a determination may be made that a storage node of a group of storage nodes maintaining a replica of a data volume is unavailable for replication. For example if 3 storage nodes are maintaining a replica of a data volume and 1 of the 3 storage nodes becomes unavailable. A storage node may become unavailable for replication for many reasons including but not limited to network partitions high network utilization system failures or any other type of event. In some embodiments the determination may be made at a another storage node of the group of storage nodes that a peer storage node maintaining a replica of data volume maintained at the storage node is unavailable for replicating write requests. For example a write request sent to the peer storage node may never be acknowledged various heartbeat or other gossip protocols indicating health or status to peer storage nodes may indicated the storage node is unavailable. In some embodiments a control system such as block based storage service control plane in may send an indication to the other storage node that the peer storage node is unavailable.

However determined in response processing of subsequently received write requests for the data volume may be performed according to a reduced durability state for the data volume such that replication for the data volume is disable for the unavailable storage node as indicated at . The change in durability state may be recorded or indicated in information maintained about the data volume such as may be maintained at storage nodes in the group of storage nodes maintaining the data volume. In some embodiments a control system such as block based storage service control plane in may update block based storage system information indicating that the particular data volume is operating according to a reduced durability state.

When a subsequent write request is received for a data volume with a reduced durability state the write request is completed at storage nodes that are available for replication as indicated at . Thus the write request may be completed at a fewer number of storage nodes than prior to the reduction in durability state. For instance if 1 storage node of 5 storage nodes maintaining a replica of a data volume is unavailable for replication and the durability state for the data volume is reduced then 4 of the 5 storage nodes which are available for replication may complete the write request which is fewer than the 5 storage nodes which would have completed the write request . In some embodiments such as those described above with regard to which include only a master storage node and slave storage node write requests may be completed for a data volume at a single storage node without being completed at any other storage node in a block based storage system. In effect the write request is only completed at the storage node that received the write request in various embodiments. No other storage node may replicate or complete the write request in such a scenario. The write request completed according to the reduced durability state may then be acknowledged to a client as indicated at .

Various different factors may determine when to increase a durability state for the data volume by enabling replication. In some embodiments the number of available storage nodes to serve as a peer storage node master or slave may change. For instance if the number of available storage nodes increases then replication may be enabled for the data volume as the capacity to store another replica of the data volume may increase. Another factor the status of a block based storage system as a whole whether or not the physical infrastructure in particular location such as a data center may be considered. The number of replication operations in a block based storage system occurring among storage nodes in the block based storage system may also be considered. In some embodiments the durability state of the data volume may also be considered. For example if the data volume is archived or backed up to another data store besides the block based storage system or whether or not particular hardware optimizations such as page cache write logging discussed above with regard to is implemented. In some embodiments another factor may be the availability of a desired or preferred peer storage node to perform replication with. For instance a preferred peer may currently be performing multiple replication operations and unable able to begin replication for the data volume at that time. In various embodiments one some or all of these various factors may be used to evaluate when to enable replication. In some embodiments a prioritization scheme or ordering may be used to weight each factor break ties between when replication should be enabled for one data volume and not another.

As indicated at another storage node may be identified to maintain a replica for the data volume in some embodiments. This storage node may be a previous peer maintaining a stale replica of the data volume in some embodiments. In some embodiments the identified storage node may be a new storage node that does not maintain a replica of the data volume. Once identified one or more replication operations may be performed to update a replica of the data volume stored in the other storage node. Replication operations may include sending some or all of the data chunks of the data volume maintained at the storage node to the other storage node to be written into the replica of the data volume maintained there. describes efficient replication techniques in more detail below such as techniques to send data chunks that have been modified subsequent to a data volume entering a reduced durability state. Once the replication operations are complete and the data volume maintained at the storage node and the other storage node are synchronized replication may be enabled at the storage node such that write requests are not acknowledged as complete until the write is complete at both the storage node and the other storage node as indicated at . Write requests may then be acknowledged when completed at the storage node and the other storage node which now maintains a current version of the data volume. As discussed above in some embodiments multiple storage nodes may maintain a replica of a data volume such that a reduced durability state of storage nodes e.g. 4 storage nodes is increased to add an additional storage node as indicated at and e.g. increased to 5 storage nodes . When write requests are received for the data volume with replication enabled for the other storage node then write requests may be completed at the multiple storage nodes and the other storage node prior to acknowledging the write request as complete e.g. completed at all 5 storage nodes prior to acknowledgement .

A peer storage node may be identified that maintains a stale replica of the data volume to update in accordance with the data volume maintained at the storage node as indicated at . In some embodiments the identified storage node may be the most recent storage node maintaining a stale replica of the data volume to perform replication operations for the data volume. is a high level flowchart illustrating various methods and techniques for identifying a storage node with a stale replicate to update according to some embodiments. As indicated at identifiers of candidate storage nodes maintaining respective stale replicas of a data volume may be received at the storage node in various embodiments. A control system such as control plane in may track previous storage nodes which have maintained replicas of a data volume. In some embodiments the control plane may create the list of candidate storage nodes from a subset of the number of storage nodes maintaining a stale replica of the data volume according to various factors e.g. whether a storage node is available healthy has a history of prior failures or current workload . In some embodiments the storage node itself may track or retain a history of storage nodes that the storage node performed replication operations with for the data volume. Storage node identifiers such as network addresses of these storage nodes may be sent to or retained at the storage node.

In some embodiments a determination may be made for each of the candidate storage nodes as to a respective number of stale data chunks to be updated in the respective stale replicas of the data volume as indicated at . For example in some embodiments when write requests are received at a storage node with replication disabled metadata for the data volume may be updated to indicate which data chunks were changed e.g. a monotonically increasing version number may be maintained for each data chunk . The candidate storage nodes also may maintain volume metadata with respective version numbers for each data chunk of the stale replica. This volume metadata for the stale replicas however may be stale as well. Therefore differences in data chunks between stale replicas of the data volume and the current version of the data volume may be identified. For example in some embodiments the storage node may query each of the candidate storage nodes to compare their volume metadata with the current volume metadata for the data volume. The candidate storage nodes may send identifiers of the data chunks that need to be updated in order to synchronize the replicas of the data volume. Based on these respective numbers of stale data chunks a peer storage node may be selected to update. For example this selected storage node may be one of the candidate storage nodes with a least number of stale data chunks as indicated at . In some embodiments other factors may also be used in combination with the amount of stale data chunks to be updated to select the peer storage node to update. For example the current workload network traffic directed to or other information about the performance or capability of a particular candidate storage node to perform replication may also be used. If for instance one candidate storage node may have more data chunks to be updated than another candidate storage node but has a greater capacity to perform replication operations e.g. the other candidate storage node is performing replication operations for another data volume or other work then the candidate storage node with more data chunks to be update may be selected.

Turning back to once selected replication operations to update data chunks in the replica of the data volume at the peer storage node to match the corresponding data chunks in the data volume at the storage node may be performed as indicated . Thus in some embodiments only those data chunks that are not synchronized i.e. do not match between the storage nodes need be updated reducing the network traffic required to send data chunks between the storage nodes. For those data chunks that match between the stale replica of the data volume and the current version of the data volume no replication operations may be performed. Once the replication operations are complete replication for the data volume may be enabled at both the storage node and the peer storage node. Write requests are not acknowledged as complete until performed at both the storage node and the peer storage node.

For storage nodes maintaining stale replicas for which an amount of time such as described with regard to data or other consideration renders efficient replication techniques inefficient the stale replicas may themselves be identified as indicated at . A control system such as control plane in may request that the storage nodes delete the respective stale replicas as indicated at . In some embodiments a storage node itself may determine that a particular stale replica maintained at the storage node may no longer need to be maintained and delete the stale replica.

The methods described herein may in various embodiments be implemented by any combination of hardware and software. For example in one embodiment the methods may be implemented by a computer system e.g. a computer system as in that includes one or more processors executing program instructions stored on a computer readable storage medium coupled to the processors. The program instructions may be configured to implement the functionality described herein e.g. the functionality of various servers storage nodes control planes managers and or other components such as those that implement the block based storage service described herein . The various methods as illustrated in the figures and described herein represent example embodiments of methods. The order of any method may be changed and various elements may be added reordered combined omitted modified etc.

Embodiments of reducing durability state for block based storage and efficient data volume replication as described herein may be executed on one or more computer systems which may interact with various other devices. is a block diagram illustrating an example computer system according to various embodiments. For example computer system may be configured to implement storage and or compute nodes of a compute cluster a data stores and or a client in different embodiments. Computer system may be any of various types of devices including but not limited to a personal computer system desktop computer laptop or notebook computer mainframe computer system handheld computer workstation network computer a consumer device application server storage device telephone mobile telephone or in general any type of computing device.

Computer system includes one or more processors any of which may include multiple cores which may be single or multi threaded coupled to a system memory via an input output I O interface . Computer system further includes a network interface coupled to I O interface . In various embodiments computer system may be a uniprocessor system including one processor or a multiprocessor system including several processors e.g. two four eight or another suitable number . Processors may be any suitable processors capable of executing instructions. For example in various embodiments processors may be general purpose or embedded processors implementing any of a variety of instruction set architectures ISAs such as the x86 PowerPC SPARC or MIPS ISAs or any other suitable ISA. In multiprocessor systems each of processors may commonly but not necessarily implement the same ISA. The computer system also includes one or more network communication devices e.g. network interface for communicating with other systems and or components over a communications network e.g. Internet LAN etc. .

In the illustrated embodiment computer system also includes one or more persistent storage devices and or one or more I O devices . In various embodiments persistent storage devices may correspond to disk drives tape drives solid state memory other mass storage devices block based storage devices or any other persistent storage device. Computer system or a distributed application or operating system operating thereon may store instructions and or data in persistent storage devices as desired and may retrieve the stored instruction and or data as needed. For example in some embodiments computer system may host a storage system server node and persistent storage may include the SSDs attached to that server node.

Computer system includes one or more system memories that are configured to store instructions and data accessible by processor s . In various embodiments system memories may be implemented using any suitable memory technology e.g. one or more of cache static random access memory SRAM DRAM RDRAM EDO RAM DDR 10 RAM synchronous dynamic RAM SDRAM Rambus RAM EEPROM non volatile Flash type memory or any other type of memory . System memory may contain program instructions that are executable by processor s to implement the methods and techniques described herein. In various embodiments program instructions may be encoded in platform native binary any interpreted language such as Java byte code or in any other language such as C C Java etc. or in any combination thereof. For example in the illustrated embodiment program instructions include program instructions executable to implement the functionality of a storage node in different embodiments. In some embodiments program instructions may implement multiple separate clients nodes and or other components.

In some embodiments program instructions may include instructions executable to implement an operating system not shown which may be any of various operating systems such as UNIX LINUX Solaris MacOS Windows etc. Any or all of program instructions may be provided as a computer program product or software that may include a non transitory computer readable storage medium having stored thereon instructions which may be used to program a computer system or other electronic devices to perform a process according to various embodiments. A non transitory computer readable storage medium may include any mechanism for storing information in a form e.g. software processing application readable by a machine e.g. a computer . Generally speaking a non transitory computer accessible medium may include computer readable storage media or memory media such as magnetic or optical media e.g. disk or DVD CD ROM coupled to computer system via I O interface . A non transitory computer readable storage medium may also include any volatile or non volatile media such as RAM e.g. SDRAM DDR SDRAM RDRAM SRAM etc. ROM etc. that may be included in some embodiments of computer system as system memory or another type of memory. In other embodiments program instructions may be communicated using optical acoustical or other form of propagated signal e.g. carrier waves infrared signals digital signals etc. conveyed via a communication medium such as a network and or a wireless link such as may be implemented via network interface .

In some embodiments system memory may include data store which may be configured as described herein. In general system memory e.g. data store within system memory persistent storage and or remote storage may store data blocks replicas of data blocks metadata associated with data blocks and or their state configuration information and or any other information usable in implementing the methods and techniques described herein.

In one embodiment I O interface may be configured to coordinate I O traffic between processor system memory and any peripheral devices in the system including through network interface or other peripheral interfaces. In some embodiments I O interface may perform any necessary protocol timing or other data transformations to convert data signals from one component e.g. system memory into a format suitable for use by another component e.g. processor . In some embodiments I O interface may include support for devices attached through various types of peripheral buses such as a variant of the Peripheral Component Interconnect PCI bus standard or the Universal Serial Bus USB standard for example. In some embodiments the function of I O interface may be split into two or more separate components such as a north bridge and a south bridge for example. Also in some embodiments some or all of the functionality of I O interface such as an interface to system memory may be incorporated directly into processor .

Network interface may be configured to allow data to be exchanged between computer system and other devices attached to a network such as other computer systems for example. In addition network interface may be configured to allow communication between computer system and various I O devices and or remote storage . Input output devices may in some embodiments include one or more display terminals keyboards keypads touchpads scanning devices voice or optical recognition devices or any other devices suitable for entering or retrieving data by one or more computer systems . Multiple input output devices may be present in computer system or may be distributed on various nodes of a distributed system that includes computer system . In some embodiments similar input output devices may be separate from computer system and may interact with one or more nodes of a distributed system that includes computer system through a wired or wireless connection such as over network interface . Network interface may commonly support one or more wireless networking protocols e.g. Wi Fi IEEE 802.11 or another wireless networking standard . However in various embodiments network interface may support communication via any suitable wired or wireless general data networks such as other types of Ethernet networks for example. Additionally network interface may support communication via telecommunications telephony networks such as analog voice networks or digital fiber communications networks via storage area networks such as Fibre Channel SANs or via any other suitable type of network and or protocol. In various embodiments computer system may include more fewer or different components than those illustrated in e.g. displays video cards audio cards peripheral devices other network interfaces such as an ATM interface an Ethernet interface a Frame Relay interface etc. 

It is noted that any of the distributed system embodiments described herein or any of their components may be implemented as one or more network based services. For example a compute cluster within a computing service may present computing and or storage services and or other types of services that employ the distributed computing systems described herein to clients as network based services. In some embodiments a network based service may be implemented by a software and or hardware system designed to support interoperable machine to machine interaction over a network. A network based service may have an interface described in a machine processable format such as the Web Services Description Language WSDL . Other systems may interact with the network based service in a manner prescribed by the description of the network based service s interface. For example the network based service may define various operations that other systems may invoke and may define a particular application programming interface API to which other systems may be expected to conform when requesting the various operations. though

In various embodiments a network based service may be requested or invoked through the use of a message that includes parameters and or data associated with the network based services request. Such a message may be formatted according to a particular markup language such as Extensible Markup Language XML and or may be encapsulated using a protocol such as Simple Object Access Protocol SOAP . To perform a network based services request a network based services client may assemble a message including the request and convey the message to an addressable endpoint e.g. a Uniform Resource Locator URL corresponding to the network based service using an Internet based application layer transfer protocol such as Hypertext Transfer Protocol HTTP .

In some embodiments network based services may be implemented using Representational State Transfer RESTful techniques rather than message based techniques. For example a network based service implemented according to a RESTful technique may be invoked through parameters included within an HTTP method such as PUT GET or DELETE rather than encapsulated within a SOAP message.

Although the embodiments above have been described in considerable detail numerous variations and modifications may be made as would become apparent to those skilled in the art once the above disclosure is fully appreciated. It is intended that the following claims be interpreted to embrace all such modifications and changes and accordingly the above description to be regarded in an illustrative rather than a restrictive sense.

