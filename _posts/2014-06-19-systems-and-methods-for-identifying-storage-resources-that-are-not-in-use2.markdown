---

title: Systems and methods for identifying storage resources that are not in use
abstract: An apparatus, system, and method are disclosed for managing a non-volatile storage medium. A storage controller receives a message that identifies data that no longer needs to be retained on the non-volatile storage medium. The data may be identified using a logical identifier. The message may comprise a hint, directive, or other indication that the data has been erased and/or deleted. In response to the message, the storage controller records an indication that the contents of a physical storage location and/or physical address associated with the logical identifier do not need to be preserved on the non-volatile storage medium.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09632727&OS=09632727&RS=09632727
owner: Longitude Enterprise Flash S.A.R.L.
number: 09632727
owner_city: Luxembourg
owner_country: LU
publication_date: 20140619
---
This application is a continuation of and claims priority to U.S. patent application Ser. No. 13 566 471 entitled Systems and Methods for Persistent Deallocation filed on Aug. 3 2012 for David Flynn et al. and which claims priority to U.S. patent application Ser. No. 11 952 113 entitled Apparatus System and Method for Managing Data in a Storage Device with an Empty Data Token Directive filed on Dec. 6 2007 and issued as U.S. Pat. No. 8 261 005 on Sep. 4 2012 and which claims priority to U.S. Provisional Patent Application Ser. No. 60 873 111 entitled Elemental Blade System filed on Dec. 6 2006 for David Flynn et al. and to U.S. Provisional Patent Application No. 60 974 470 entitled Apparatus System and Method for Object Oriented Solid State Storage filed on Sep. 22 2007 for David Flynn et al. Each of the above referenced applications are incorporated herein by reference in their entirety.

This invention relates to managing data in a data storage device and more particularly relates to managing data in a storage device using an empty data segment directive.

Typically when data is no longer useful it may be erased. In many file systems an erase command deletes a directory entry in the file system while leaving the data in place in the storage device containing the data. Typically a data storage device is not involved in this type of erase operation. Another method of erasing data is to write zeros ones or some other null data character to the data storage device to actually replace the erased file. However this is inefficient because valuable bandwidth is used while transmitting the data is being overwritten. In addition space in the storage device is taken up by the data used to overwrite invalid data.

In some storage devices like the solid state storage device described herein updating previously stored data does not overwrite existing data. Attempting to overwrite data with a string of ones or a string of zeros on such a device takes up valuable space without fulfilling the desired intent of overwriting the existing data. For these devices such as solid state storage devices a client typically does not have an ability to overwrite data to erase it.

When receiving a string of repeated characters or character strings the received data is highly compressible but typically compression is done by a file system prior to transmission to a storage device. A typical storage device cannot distinguish between compressed data and uncompressed data. The storage device may also receive a command to read the erased file so the storage device may transmit a stream of zeros ones or a null character to the requesting device. Again bandwidth is required to transmit data representing the erased file.

From the foregoing discussion it should be apparent that a need exists for an apparatus system and method for a storage device to receive a directive that data is to be erased so that the storage device can store a data segment token that represents an empty data segment or data with repeated characters or character strings. The apparatus system and method may also erase the existing data so that the resulting used storage space comprises the small data segment token. An apparatus system and method are presented that overcome some or all of the shortcomings of the prior art.

The present invention has been developed in response to the present state of the art and in particular in response to the problems and needs in the art that have not yet been fully solved by currently available data management systems. Accordingly the present invention has been developed to provide an apparatus system and method for managing data that overcome many or all of the above discussed shortcomings in the art.

Embodiments of methods for managing a non volatile storage media are disclosed. The methods may comprise receiving a message comprising a logical identifier corresponding to data that does not need to be preserved on a non volatile storage medium and indicating that that contents of a physical storage location on the non volatile storage medium corresponding to the logical identifier do not need to be preserved on the non volatile storage medium in response to the message.

Indicating that the contents of the physical storage location do not need to be preserved may comprise deleting an index entry that associates the logical identifier with the physical storage location deleting a mapping between the logical identifier and the physical storage location invalidating an association between the logical identifier and an address of the physical storage location and or marking the contents of the physical storage location invalid.

The methods may further comprise maintaining an index of mappings between logical identifiers and physical storage locations and that the contents of the physical storage location do not need to be preserved may comprise removing a mapping between the logical identifier and the physical storage location from the index.

Methods for managing a non volatile storage medium may further comprise receiving a hint or other indication that data of a specified logical identifier does not need to be preserved on a non volatile storage device and removing a mapping between the specified logical identifier and a physical address of the data of the specified logical identifier on the non volatile storage device in response to the hint wherein removal of the mapping indicates that the data does not need to be preserved on the non volatile storage device. The methods may further comprise invalidating a data packet in response to the hint.

In some embodiments the methods disclosed herein may further comprise erasing the contents of the physical storage location in a storage recovery process such as garbage collection or the like. The storage recovery process may comprise erasing a storage division comprising the physical storage location e.g. erase block logical erase block or the like .

In some embodiments the methods disclosed herein may further comprise receiving the message from a block storage client such as a file system operating system application client or the like. The message may be received through a block storage application programming interface block storage protocol or the like.

Disclosed herein are embodiments of apparatuses and or systems for managing a non volatile storage medium comprising a request receiver module configured to receive an indication identifying data that can be erased from a non volatile storage medium wherein the indication identifies the data using a logical identifier associated with the data and a marking module configured to record that data stored at a physical address on the non volatile storage medium can be erased from the non volatile storage medium in response to receiving the indication.

The marking module may be configured to invalidate an association between the logical identifier and the physical address delete a mapping between the logical identifier and the physical address and or mark a data packet at the physical address invalid.

The apparatuses and systems disclosed herein may further comprise an index comprising mappings between logical identifiers and physical addresses on the non volatile storage medium wherein the marking module is configured to remove a mapping between the logical identifier and the physical addresses of the data from the index. The marking module may be further configured to delete a reference to the physical address from an index entry of the logical identifier. Removal of the mapping may indicate that data stored at the physical address can be erased from the non volatile storage medium.

The apparatuses and systems disclosed herein may further comprise a request receiver module configured to receive the hint. The request receiver module may comprise a driver. The request receiver module may be configured to receive the hint from a file system the file system configured to send the hint in response to deletion of file system data associated with the logical identifier and or removal of a file associated with the logical identifier.

In some embodiments the request receiver module is configured to receive the hint from an application wherein the application is configured to send the hint in response to determining that data associated with the logical identifier no longer needs to be retained on the non volatile storage medium.

The apparatuses and systems disclosed herein may further comprise a storage recovery module configured to recover the physical storage location at the physical address and a storage module configured to store data associated with another logical identifier on the physical storage location in response to recovering the physical storage location.

The apparatuses and systems disclosed herein may comprise an index configured to contain a plurality of entries wherein each entry associates a logical identifier with a physical storage location of a non volatile storage medium that comprises valid data of the logical identifier a request receiver module configured to receive an indication that a specified logical identifier is empty and an index module configured to remove an entry from the index that associates the specified logical identifier with a physical storage location of the non volatile storage medium in response to the indication. The request receiver module may be configured to receive the indication through one of a block device interface and via a block storage protocol.

The apparatuses and systems disclosed herein may further comprise a storage recovery module configured to erase a storage division comprising the physical storage location in response to removing the entry.

The apparatuses and systems disclosed herein may further comprise a read request response module configured to return an indication that the logical identifier is empty in response to a request to read data of the logical identifier while the data remains on the non volatile storage medium.

Reference throughout this specification to features advantages or similar language does not imply that all of the features and advantages that may be realized with the present invention should be or are in any single embodiment of the invention. Rather language referring to the features and advantages is understood to mean that a specific feature advantage or characteristic described in connection with an embodiment is included in at least one embodiment of the present invention. Thus discussion of the features and advantages and similar language throughout this specification may but do not necessarily refer to the same embodiment.

Furthermore the described features advantages and characteristics of the invention may be combined in any suitable manner in one or more embodiments. One skilled in the relevant art will recognize that the invention may be practiced without one or more of the specific features or advantages of a particular embodiment. In other instances additional features and advantages may be recognized in certain embodiments that may not be present in all embodiments of the invention.

These features and advantages of the present invention will become more fully apparent from the following description and appended claims or may be learned by the practice of the invention as set forth hereinafter.

Many of the functional units described in this specification have been labeled as modules in order to more particularly emphasize their implementation independence. For example a module may be implemented as a hardware circuit comprising custom VLSI circuits or gate arrays off the shelf semiconductors such as logic chips transistors or other discrete components. A module may also be implemented in programmable hardware devices such as field programmable gate arrays programmable array logic programmable logic devices or the like.

Modules may also be implemented in software for execution by various types of processors. An identified module of executable code may for instance comprise one or more physical or logical blocks of computer instructions which may for instance be organized as an object procedure or function. Nevertheless the executables of an identified module need not be physically located together but may comprise disparate instructions stored in different locations which when joined logically together comprise the module and achieve the stated purpose for the module.

Indeed a module of executable code may be a single instruction or many instructions and may even be distributed over several different code segments among different programs and across several memory devices. Similarly operational data may be identified and illustrated herein within modules and may be embodied in any suitable form and organized within any suitable type of data structure. The operational data may be collected as a single data set or may be distributed over different locations including over different storage devices and may exist at least partially merely as electronic signals on a system or network. Where a module or portions of a module are implemented in software the software portions are stored on one or more computer readable media.

Reference throughout this specification to one embodiment an embodiment or similar language means that a particular feature structure or characteristic described in connection with the embodiment is included in at least one embodiment of the present invention. Thus appearances of the phrases in one embodiment in an embodiment and similar language throughout this specification may but do not necessarily all refer to the same embodiment.

Reference to a signal bearing medium may take any form capable of generating a signal causing a signal to be generated or causing execution of a program of machine readable instructions on a digital processing apparatus. A signal bearing medium may be embodied by a transmission line a compact disk digital video disk a magnetic tape a Bernoulli drive a magnetic disk a punch card flash memory integrated circuits or other digital processing apparatus memory device.

Furthermore the described features structures or characteristics of the invention may be combined in any suitable manner in one or more embodiments. In the following description numerous specific details are provided such as examples of programming software modules user selections network transactions database queries database structures hardware modules hardware circuits hardware chips etc. to provide a thorough understanding of embodiments of the invention. One skilled in the relevant art will recognize however that the invention may be practiced without one or more of the specific details or with other methods components materials and so forth. In other instances well known structures materials or operations are not shown or described in detail to avoid obscuring aspects of the invention.

The schematic flow chart diagrams included herein are generally set forth as logical flow chart diagrams. As such the depicted order and labeled steps are indicative of one embodiment of the presented method. Other steps and methods may be conceived that are equivalent in function logic or effect to one or more steps or portions thereof of the illustrated method. Additionally the format and symbols employed are provided to explain the logical steps of the method and are understood not to limit the scope of the method. Although various arrow types and line types may be employed in the flow chart diagrams they are understood not to limit the scope of the corresponding method. Indeed some arrows or other connectors may be used to indicate only the logical flow of the method. For instance an arrow may indicate a waiting or monitoring period of unspecified duration between enumerated steps of the depicted method. Additionally the order in which a particular method occurs may or may not strictly adhere to the order of the corresponding steps shown.

The system includes at least one solid state storage device . In another embodiment the system includes two or more solid state storage devices . Each solid state storage device may include non volatile solid state storage such as flash memory nano random access memory nano RAM or NRAM magneto resistive RAM MRAM dynamic RAM DRAM phase change RAM PRAM etc. The solid state storage device is described in more detail with respect to . The solid state storage device is depicted in a computer connected to a client through a computer network . In one embodiment the solid state storage device is internal to the computer and is connected using a system bus such as a peripheral component interconnect express PCI e bus a Serial Advanced Technology Attachment serial ATA bus or the like. In another embodiment the solid state storage device is external to the computer and is connected a universal serial bus USB connection an Institute of Electrical and Electronics Engineers IEEE 1394 bus FireWire or the like. In other embodiments the solid state storage device is connected to the computer using a peripheral component interconnect PCI express bus using external electrical or optical bus extension or bus networking solution such as Infiniband or PCI Express Advanced Switching PCIe AS or the like.

In various embodiments the solid state storage device may be in the form of a dual inline memory module DIMM a daughter card or a micro module. In another embodiment the solid state storage device is an element within a rack mounted blade. In another embodiment the solid state storage device is contained within a package that is integrated directly onto a higher level assembly e.g. mother board lap top graphics processor . In another embodiment individual components comprising the solid state storage device are integrated directly onto a higher level assembly without intermediate packaging.

The solid state storage device includes one or more solid state storage controllers each may include a write data pipeline and a read data pipeline and each includes a solid state storage which are described in more detail below with respect to .

The system includes one or more computers connected to the solid state storage device . A computer may be a host a server a storage controller of a storage area network SAN a workstation a personal computer a laptop computer a handheld computer a supercomputer a computer cluster a network switch router or appliance a database or storage appliance a data acquisition or data capture system a diagnostic system a test system a robot a portable electronic device a wireless device or the like. In another embodiment a computer may be a client and the solid state storage device operates autonomously to service data requests sent from the computer . In this embodiment the computer and solid state storage device may be connected using a computer network system bus or other communication means suitable for connection between a computer and an autonomous solid state storage device .

In one embodiment the system includes one or more clients connected to one or more computer through one or more computer networks . A client may be a host a server a storage controller of a SAN a workstation a personal computer a laptop computer a handheld computer a supercomputer a computer cluster a network switch router or appliance a database or storage appliance a data acquisition or data capture system a diagnostic system a test system a robot a portable electronic device a wireless device or the like. The computer network may include the Internet a wide area network WAN a metropolitan area network MAN a local area network LAN a token ring a wireless network a fiber channel network a SAN network attached storage NAS ESCON or the like or any combination of networks. The computer network may also include a network from the IEEE 802 family of network technologies such Ethernet token ring WiFi WiMax and the like.

The computer network may include servers switches routers cabling radios and other equipment used to facilitate networking computers and clients . In one embodiment the system includes multiple computers that communicate as peers over a computer network . In another embodiment the system includes multiple solid state storage devices that communicate as peers over a computer network . One of skill in the art will recognize other computer networks comprising one or more computer networks and related equipment with single or redundant connection between one or more clients or other computer with one or more solid state storage devices or one or more solid state storage devices connected to one or more computers . In one embodiment the system includes two or more solid state storage devices connected through the computer network to a client without a computer .

In one embodiment the storage controller and data storage device are separate devices. In another embodiment the storage controller and data storage device are integrated into one storage device . In another embodiment a data storage device is a solid state storage and the storage controller is a solid state storage device controller . In other embodiments a data storage device may be a hard disk drive an optical drive tape storage or the like. In another embodiment a storage device may include two or more data storage devices of different types.

In one embodiment the data storage device is a solid state storage and is arranged as an array of solid state storage elements . In another embodiment the solid state storage is arranged in two or more banks . Solid state storage is described in more detail below with respect to .

The storage devices may be networked together and act as a distributed storage device. The storage device coupled to the requesting device controls object requests to the distributed storage device. In one embodiment the storage devices and associated storage controllers manage objects and appear to the requesting device s as a distributed object file system. In this context a parallel object file system is an example of a type of distributed object file system. In another embodiment the storage devices and associated storage controllers manage objects and appear to the requesting device as distributed object file servers. In this context a parallel object file server is an example of a type of distributed object file server. In these and other embodiments the requesting device may exclusively manage objects or participate in managing objects in conjunction with storage devices this typically does not limit the ability of storage devices to fully manage objects for other clients . In the degenerate case each distributed storage device distributed object file system and distributed object file server can operate independently as a single device. The networked storage devices may operate as distributed storage devices distributed object file systems distributed object file servers and any combination thereof having images of one or more of these capabilities configured for one or more requesting devices . Fore example the storage devices may be configured to operate as distributed storage devices for a first requesting device while operating as distributed storage devices and distributed object file systems for requesting devices . Where the system includes one storage device the storage controller of the storage device manages objects may appear to the requesting device s as an object file system or an object file server.

In one embodiment where the storage devices are networked together as a distributed storage device the storage devices serve as a redundant array of independent drives RAID managed by one or more distributed storage controllers . For example a request to write a data segment of an object results in the data segment being stripped across the data storage devices with a parity stripe depending upon the RAID level. One benefit of such an arrangement is that such an object management system may continue to be available when a single storage device has a failure whether of the storage controller the data storage device or other components of storage device .

When redundant networks are used to interconnect the storage devices and requesting devices the object management system may continue to be available in the presence of network failures as long as one of the networks remains operational. A system with a single storage device may also include multiple data storage devices and the storage controller of the storage device may act as a RAID controller and stripe the data segment across the data storage devices of the storage device and may include a parity stripe depending upon the RAID level.

In one embodiment where the one or more storage devices are solid state storage devices with a solid state storage device controller and solid state storage the solid state storage device s may be configured in a DIMM configuration daughter card micro module etc. and reside in a computer . The computer may be a server or similar device with the solid state storage devices networked together and acting as distributed RAID controllers. Beneficially the storage devices may be connected using PCI e PCIe AS Infiniband or other high performance bus switched bus networked bus or network and may provide a very compact high performance RAID storage system with single or distributed solid state storage controllers autonomously striping a data segment across solid state storage 

In one embodiment the same network used by the requesting device to communicate with storage devices may be used by the peer storage device to communicate with peer storage devices to accomplish RAID functionality. In another embodiment a separate network may be used between the storage devices for the purpose of RAIDing. In another embodiment the requesting devices may participate in the RAIDing process by sending redundant requests to the storage devices . For example requesting device may send a first object write request to a first storage device and a second object write request with the same data segment to a second storage device to achieve simple mirroring.

With the ability for object handling within the storage device s the storage controller s uniquely have the ability to store one data segment or object using one RAID level while another data segment or object is stored using a different RAID level or without RAID striping. These multiple RAID groupings may be associated with multiple partitions within the storage devices . RAID 0 RAID 1 RAID5 RAID6 and composite RAID types can be supported simultaneously across a variety of RAID groups comprising data storage devices . One skilled in the art will recognize other RAID types and configurations that may also be simultaneously supported.

Also because the storage controller s operate autonomously as RAID controllers the RAID controllers can perform progressive RAIDing and can transform objects or portions of objects striped across data storage devices with one RAID level to another RAID level without the requesting device being affected participating or even detecting the change in RAID levels. In the preferred embodiment progressing the RAID configuration from one level to another level may be accomplished autonomously on an object or even a packet bases and is initiated by a distributed RAID control module operating in one of the storage devices or the storage controllers . Typically RAID progression will be from a higher performance and lower efficiency storage configuration such as RAID 1 to a lower performance and higher storage efficiency configuration such as RAID5 where the transformation is dynamically initiated based on the frequency of access. But one can see that progressing the configuration from RAID5 to RAID 1 is also possible. Other processes for initiating RAID progression may be configured or requested from clients or external agents such a storage system management server request. One of skill in the art will recognize other features and benefits of a storage device with a storage controller that autonomously manages objects.

The storage controller is substantially similar to the storage controller described in relation to the system of and may be a solid state storage device controller described in relation to . The apparatus includes an object request receiver module that receives an object request from one or more requesting devices . For example for a store object data request the storage controller stores the data segment as a data packet in a data storage device coupled to the storage controller . The object request is typically directed at a data segment stored or to be stored in one or more object data packets for an object managed by the storage controller . The object request may request that the storage controller create an object to be later filled with data through later object request which may utilize a local or remote direct memory access DMA RDMA transfer.

In one embodiment the object request is a write request to write all or part of an object to a previously created object. In one example the write request is for a data segment of an object. The other data segments of the object may be written to the storage device or to other storage devices. In another example the write request is for an entire object. In another example the object request is to read data from a data segment managed by the storage controller . In yet another embodiment the object request is a delete request to delete a data segment or object.

Advantageously the storage controller can accept write requests that do more than write a new object or append data to an existing object. For example a write request received by the object request receiver module may include a request to add data ahead of data stored by the storage controller to insert data into the stored data or to replace a segment of data. The object index maintained by the storage controller provides the flexibility required for these complex write operations that is not available in other storage controllers but is currently available only outside of storage controllers in file systems of servers and other computers.

The apparatus includes a parsing module that parses the object request into one or more commands. Typically the parsing module parses the object request into one or more buffers. For example one or more commands in the object request may be parsed into a command buffer. Typically the parsing module prepares an object request so that the information in the object request can be understood and executed by the storage controller . One of skill in the art will recognize other functions of a parsing module that parses an object request into one or more commands.

The apparatus includes a command execution module that executes the command s parsed from the object request. In one embodiment the command execution module executes one command. In another embodiment the command execution module executes multiple commands. Typically the command execution module interprets a command parsed from the object request such as a write command and then creates queues and executes subcommands. For example a write command parsed from an object request may direct the storage controller to store multiple data segments. The object request may also include required attributes such as encryption compression etc. The command execution module may direct the storage controller to compress the data segments encrypt the data segments create one or more data packets and associated headers for each data packet encrypt the data packets with a media encryption key add error correcting code and store the data packets a specific location. Storing the data packets at a specific location and other subcommands may also be broken down into other lower level subcommands. One of skill in the art will recognize other ways that the command execution module can execute one or more commands parsed from an object request.

The apparatus includes an object index module that creates an object entry in an object index in response to the storage controller creating an object or storing the data segment of the object. Typically the storage controller creates a data packet from the data segment and the location of where the data packet is stored is assigned at the time the data segment is stored. Object metadata received with a data segment or as part of an object request may be stored in a similar way.

The object index module creates an object entry into an object index at the time the data packet is stored and the physical address of the data packet is assigned. The object entry includes a mapping between a logical identifier of the object and one or more physical addresses corresponding to where the storage controller stored one or more data packets and any object metadata packets. In another embodiment the entry in the object index is created before the data packets of the object are stored. For example if the storage controller determines a physical address of where the data packets are to be stored earlier the object index module may create the entry in the object index earlier.

Typically when an object request or group of object requests results in an object or data segment being modified possibly during a read modify write operation the object index module updates an entry in the object index corresponding the modified object. In one embodiment the object index creates a new object and a new entry in the object index for the modified object. Typically where only a portion of an object is modified the object includes modified data packets and some data packets that remain unchanged. In this case the new entry includes a mapping to the unchanged data packets as where they were originally written and to the modified objects written to a new location.

In another embodiment where the object request receiver module receives an object request that includes a command that erases a data block or other object elements the storage controller may store at least one packet such as an erase packet that includes information including a reference to the object relationship to the object and the size of the data block erased. Additionally it may further indicate that the erased object elements are filled with zeros. Thus the erase object request can be used to emulate actual memory or storage that is erased and actually has a portion of the appropriate memory storage actually stored with zeros in the cells of the memory storage.

Beneficially creating an object index with entries indicating mapping between data segments and metadata of an object allows the storage controller to autonomously handle and manage objects. This capability allows a great amount of flexibility for storing data in the storage device . Once the index entry for the object is created subsequent object requests regarding the object can be serviced efficiently by the storage controller .

In one embodiment the storage controller includes an object request queuing module that queues one or more object requests received by the object request receiver module prior to parsing by the parsing module . The object request queuing module allows flexibility between when an object request is received and when it is executed.

In another embodiment the storage controller includes a packetizer that creates one or more data packets from the one or more data segments where the data packets are sized for storage in the data storage device . The packetizer is described below in more detail with respect to . The packetizer includes in one embodiment a messages module that creates a header for each packet. The header includes a packet identifier and a packet length. The packet identifier relates the packet to the object for which the packet was formed.

In one embodiment each packet includes a packet identifier that is self contained in that the packet identifier contains adequate information to identify the object and relationship within the object of the object elements contained within the packet. However a more efficient preferred embodiment is to store packets in containers.

A container is a data construct that facilitates more efficient storage of packets and helps establish relationships between an object and data packets metadata packets and other packets related to the object that are stored within the container. Note that the storage controller typically treats object metadata received as part of an object and data segments in a similar manner. Typically packet may refer to a data packet comprising data a metadata packet comprising metadata or another packet of another packet type. An object may be stored in one or more containers and a container typically includes packets for no more than one unique object. An object may be distributed between multiple containers. Typically a container is stored within a single logical erase block storage division and is typically never split between logical erase blocks.

A container in one example may be split between two or more logical virtual pages. A container is identified by a container label that associates that container with an object. A container may contain zero to many packets and the packets within a container are typically from one object. A packet may be of many object element types including object attribute elements object data elements object index elements and the like. Hybrid packets may be created that include more than one object element type. Each packet may contain zero to many elements of the same element type. Each packet within a container typically contains a unique identifier that identifies the relationship to the object.

Each packet is associated with one container. In a preferred embodiment containers are limited to an erase block so that at or near the beginning of each erase block a container packet can be found. This helps limit data loss to an erase block with a corrupted packet header. In this embodiment if the object index is unavailable and a packet header within the erase block is corrupted the contents from the corrupted packet header to the end of the erase block may be lost because there is possibly no reliable mechanism to determine the location of subsequent packets. In another embodiment a more reliable approach is to have a container limited to a page boundary. This embodiment requires more header overhead. In another embodiment containers can flow across page and erase block boundaries. This requires less header overhead but a larger portion of data may be lost if a packet header is corrupted. For these several embodiments it is expected that some type of RAID is used to further ensure data integrity.

In one embodiment the apparatus includes an object index reconstruction module that that reconstructs the entries in the object index using information from packet headers stored in the data storage device . In one embodiment the object index reconstruction module reconstructs the entries of the object index by reading headers to determine the object to which each packet belongs and sequence information to determine where in the object the data or metadata belongs. The object index reconstruction module uses physical address information for each packet and timestamp or sequence information to create a mapping between the physical locations of the packets and the object identifier and data segment sequence. Timestamp or sequence information is used by the object index reconstruction module to replay the sequence of changes made to the index and thereby typically reestablish the most recent state.

In another embodiment the object index reconstruction module locates packets using packet header information along with container packet information to identify physical locations of the packets object identifier and sequence number of each packet to reconstruct entries in the object index. In one embodiment erase blocks are time stamped or given a sequence number as packets are written and the timestamp or sequence information of an erase block is used along with information gathered from container headers and packet headers to reconstruct the object index. In another embodiment timestamp or sequence information is written to an erase block when the erase block is recovered.

Where the object index is stored in volatile memory an error loss of power or other problem causing the storage controller to shut down without saving the object index could be a problem if the object index cannot be reconstructed. The object index reconstruction module allows the object index to be stored in volatile memory allowing the advantages of volatile memory such as fast access. The object index reconstruction module allows quick reconstruction of the object index autonomously without dependence on a device external to the storage device .

In one embodiment the object index in volatile memory is stored periodically in a data storage device . In a particular example the object index or index metadata is stored periodically in a solid state storage . In another embodiment the index metadata is stored in a solid state storage separate from solid state storage 1 storing packets. The index metadata is managed independently from data and object metadata transmitted from a requesting device and managed by the storage controller solid state storage device controller . Managing and storing index metadata separate from other data and metadata from an object allows efficient data flow without the storage controller solid state storage device controller unnecessarily processing object metadata.

In one embodiment where an object request received by the object request receiver module includes a write request the storage controller receives one or more data segments of an object from memory of a requesting device as a local or remote direct memory access DMA RDMA operation. In a preferred example the storage controller pulls data from the memory of the requesting device in one or more DMA or RDMA operations. In another example the requesting device pushes the data segment s to the storage controller in one or more DMA or RDMA operations. In another embodiment where the object request includes a read request the storage controller transmits one or more data segments of an object to the memory of the requesting device in one or more DMA or RDMA operations. In a preferred example the storage controller pushes data to the memory of the requesting device in one or more DMA or RDMA operations. In another example the requesting device pulls data from the storage controller in one or more DMA or RDMA operations. In another example the storage controller pulls object command request sets from the memory of the requesting device in one or more DMA or RDMA operations. In another example the requesting device pushes object command request sets to the storage controller in one or more DMA or RDMA operations.

In one embodiment the storage controller emulates block storage and an object communicated between the requesting device and the storage controller comprises one or more data blocks. In one embodiment the requesting device includes a driver so that the storage device appears as a block storage device. For example the requesting device may send a block of data of a certain size along with a physical address of where the requesting device wants the data block stored. The storage controller receives the data block and uses the physical block address transmitted with the data block or a transformation of the physical block address as an object identifier. The storage controller then stores the data block as an object or data segment of an object by packetizing the data block and storing the data block at will. The object index module then creates an entry in the object index using the physical block based object identifier and the actual physical location where the storage controller stored the data packets comprising the data from the data block.

In another embodiment the storage controller emulates block storage by accepting block objects. A block object may include one or more data blocks in a block structure. In one embodiment the storage controller treats the block object as any other object. In another embodiment an object may represent an entire block device partition of a block device or some other logical or physical sub element of a block device including a track sector channel and the like. Of particular note is the ability to remap a block device RAID group to an object supporting a different RAID construction such as progressive RAID. One skilled in the art will recognize other mappings of traditional or future block devices to objects.

In one embodiment at least one solid state controller is field programmable gate array FPGA and controller functions are programmed into the FPGA. In a particular embodiment the FPGA is a Xilinx FPGA. In another embodiment the solid state storage controller comprises components specifically designed as a solid state storage controller such as an application specific integrated circuit ASIC or custom logic solution. Each solid state storage controller typically includes a write data pipeline and a read data pipeline which are describe further in relation to . In another embodiment at least one solid state storage controller is made up of a combination FPGA ASIC and custom logic components.

The solid state storage is an array of non volatile solid state storage elements arranged in banks and accessed in parallel through a bi directional storage input output I O bus . The storage I O bus in one embodiment is capable of unidirectional communication at any one time. For example when data is being written to the solid state storage data cannot be read from the solid state storage . In another embodiment data can flow both directions simultaneously. However bi directional as used herein with respect to a data bus refers to a data pathway that can have data flowing in only one direction at a time but when data flowing one direction on the bi directional data bus is stopped data can flow in the opposite direction on the bi directional data bus.

A solid state storage element e.g. SSS 0.0 is typically configured as a chip a package of one or more dies or a die on a circuit board. As depicted a solid state storage element e.g. operates independently or semi independently of other solid state storage elements e.g. even if these several elements are packaged together in a chip package a stack of chip packages or some other package element. As depicted a column of solid state storage elements is designated as a bank . As depicted there may be n banks and m solid state storage elements per bank in an array of n m solid state storage elements in a solid state storage . In one embodiment a solid state storage includes twenty solid state storage elements per bank with eight banks and a solid state storage includes 2 solid state storage elements per bank with one bank . In one embodiment each solid state storage element is comprised of a single level cell SLC devices. In another embodiment each solid state storage element is comprised of multi level cell MLC devices.

In one embodiment solid state storage elements for multiple banks that share a common storage I O bus row e.g. are packaged together. In one embodiment a solid state storage element may have one or more dies per chip with one or more chips stacked vertically and each die may be accessed independently. In another embodiment a solid state storage element e.g. SSS 0.0 may have one or more virtual dies per die and one or more dies per chip and one or more chips stacked vertically and each virtual die may be accessed independently. In another embodiment a solid state storage element SSS 0.0 may have one or more virtual dies per die and one or more dies per chip with some or all of the one or more dies stacked vertically and each virtual die may be accessed independently.

In one embodiment two dies are stacked vertically with four stacks per group to form eight storage elements e.g. SSS 0.0 SSS 0.8 each in a separate bank . In another embodiment 20 storage elements e.g. SSS 0.0 SSS 20.0 form a virtual bank so that each of the eight virtual banks has 20 storage elements e.g. SSS0.0 SSS 20.8 . Data is sent to the solid state storage over the storage I O bus to all storage elements of a particular group of storage elements SSS 0.0 SSS 0.8 . The storage control bus is used to select a particular bank e.g. Bank 0 so that the data received over the storage I O bus connected to all banks is written just to the selected bank

In a preferred embodiment the storage I O bus is comprised of one or more independent I O buses IIOBa m comprising wherein the solid state storage elements within each row share one of the independent I O buses accesses each solid state storage element in parallel so that all banks are accessed simultaneously. For example one channel of the storage I O bus may access a first solid state storage element of each bank simultaneously. A second channel of the storage I O bus may access a second solid state storage element of each bank simultaneously. Each row of solid state storage element is accessed simultaneously. In one embodiment where solid state storage elements are multi level physically stacked all physical levels of the solid state storage elements are accessed simultaneously. As used herein simultaneously also includes near simultaneous access where devices are accessed at slightly different intervals to avoid switching noise. Simultaneously is used in this context to be distinguished from a sequential or serial access wherein commands and or data are sent individually one after the other.

Typically banks are independently selected using the storage control bus . In one embodiment a bank is selected using a chip enable or chip select. Where both chip select and chip enable are available the storage control bus may select one level of a multi level solid state storage element . In other embodiments other commands are used by the storage control bus to individually select one level of a multi level solid state storage element . Solid state storage elements may also be selected through a combination of control and of address information transmitted on storage I O bus and the storage control bus .

In one embodiment each solid state storage element is partitioned into erase blocks and each erase block is partitioned into pages. A typical page is 2000 bytes 2 kB . In one example a solid state storage element e.g. SSS0.0 includes two registers and can program two pages so that a two register solid state storage element has a capacity of 4 kB. A bank of 20 solid state storage elements would then have an 80 kB capacity of pages accessed with the same address going out the channels of the storage I O bus .

This group of pages in a bank of solid state storage elements of 80 kB may be called a virtual page. Similarly an erase block of each storage element of a bank may be grouped to form a virtual erase block. In a preferred embodiment an erase block of pages within a solid state storage element is erased when an erase command is received within a solid state storage element . Whereas the size and number of erase blocks pages planes or other logical and physical divisions within a solid state storage element are expected to change over time with advancements in technology it is to be expected that many embodiments consistent with new configurations are possible and are consistent with the general description herein.

Typically when a packet is written to a particular location within a solid state storage element wherein the packet is intended to be written to a location within a particular page which is specific to a of a particular erase block of a particular element of a particular bank a physical address is sent on the storage I O bus and followed by the packet. The physical address contains enough information for the solid state storage element to direct the packet to the designated location within the page. Since all storage elements in a row of storage elements e.g. SSS 0.0 SSS 0.N are accessed simultaneously by the appropriate bus within the storage I O bus to reach the proper page and to avoid writing the data packet to similarly addressed pages in the row of storage elements SSS 0.0 SSS 0.N the bank that includes the solid state storage element SSS 0.0 with the correct page where the data packet is to be written is simultaneously selected by the storage control bus .

Similarly a read command traveling on the storage I O bus requires a simultaneous command on the storage control bus to select a single bank and the appropriate page within that bank . In a preferred embodiment a read command reads an entire page and because there are multiple solid state storage elements in parallel in a bank an entire virtual page is read with a read command. However the read command may be broken into subcommands as will be explained below with respect to bank interleave. A virtual page may also be accessed in a write operation.

An erase block erase command may be sent out to erase an erase block over the storage I O bus with a particular erase block address to erase a particular erase block. Typically an erase block erase command may be sent over the parallel paths of the storage I O bus to erase a virtual erase block each with a particular erase block address to erase a particular erase block. Simultaneously a particular bank e.g. bank 0 is selected over the storage control bus to prevent erasure of similarly addressed erase blocks in all of the banks banks 1 N . Other commands may also be sent to a particular location using a combination of the storage I O bus and the storage control bus . One of skill in the art will recognize other ways to select a particular storage location using the bi directional storage I O bus and the storage control bus .

In one embodiment packets are written sequentially to the solid state storage . For example packets are streamed to the storage write buffers of a bank of storage elements and when the buffers are full the packets are programmed to a designated virtual page. Packets then refill the storage write buffers and when full the packets are written to the next virtual page. The next virtual page may be in the same bank or another bank e.g. . This process continues virtual page after virtual page typically until a virtual erase block is filled. In another embodiment the streaming may continue across virtual erase block boundaries with the process continuing virtual erase block after virtual erase block.

In a read modify write operation data packets associated with the object are located and read in a read operation. Data segments of the modified object that have been modified are not written to the location from which they are read. Instead the modified data segments are again converted to data packets and then written to the next available location in the virtual page currently being written. The object index entries for the respective data packets are modified to point to the packets that contain the modified data segments. The entry or entries in the object index for data packets associated with the same object that have not been modified will include pointers to original location of the unmodified data packets. Thus if the original object is maintained for example to maintain a previous version of the object the original object will have pointers in the object index to all data packets as originally written. The new object will have pointers in the object index to some of the original data packets and pointers to the modified data packets in the virtual page that is currently being written.

In a copy operation the object index includes an entry for the original object mapped to a number of packets stored in the solid state storage . When a copy is made a new object is created and a new entry is created in the object index mapping the new object to the original packets. The new object is also written to the solid state storage with its location mapped to the new entry in the object index. The new object packets may be used to identify the packets within the original object that are referenced in case changes have been made in the original object that have not been propagated to the copy and the object index is lost or corrupted.

Beneficially sequentially writing packets facilitates a more even use of the solid state storage and allows the solid storage device controller to monitor storage hot spots and level usage of the various virtual pages in the solid state storage . Sequentially writing packets also facilitates a powerful efficient garbage collection system which is described in detail below. One of skill in the art will recognize other benefits of sequential storage of data packets.

In various embodiments the solid state storage device controller also includes a data bus a local bus a buffer controller buffers 0 N a master controller a direct memory access DMA controller a memory controller a dynamic memory array a static random memory array a management controller a management bus a bridge to a system bus and miscellaneous logic which are described below. In other embodiments the system bus is coupled to one or more network interface cards NICs some of which may include remote DMA RDMA controllers one or more central processing unit CPU one or more external memory controllers and associated external memory arrays one or more storage controllers peer controllers and application specific processors which are described below. The components connected to the system bus may be located in the computer or may be other devices.

Typically the solid state storage controller s communicate data to the solid state storage over a storage I O bus . In a typical embodiment where the solid state storage is arranged in banks and each bank includes multiple storage elements accessed in parallel the storage I O bus is an array of busses one for each row of storage elements spanning the banks . As used herein the term storage I O bus may refer to one storage I O bus or an array of data independent busses . In a preferred embodiment each storage I O bus accessing a row of storage elements e.g. may include a logical to physical mapping for storage divisions e.g. erase blocks accessed in a row of storage elements . This mapping allows a logical address mapped to a physical address of a storage division to be remapped to a different storage division if the first storage division fails partially fails is inaccessible or has some other problem. Remapping is explained further in relation to the remapping module of .

Data may also be communicated to the solid state storage controller s from a requesting device through the system bus bridge local bus buffer s and finally over a data bus . The data bus typically is connected to one or more buffers controlled with a buffer controller . The buffer controller typically controls transfer of data from the local bus to the buffers and through the data bus to the pipeline input buffer and output buffer . The buffer controller typically controls how data arriving from a requesting device can be temporarily stored in a buffer and then transferred onto a data bus or vice versa to account for different clock domains to prevent data collisions etc. The buffer controller typically works in conjunction with the master controller to coordinate data flow. As data arrives the data will arrive on the system bus be transferred to the local bus through a bridge .

Typically the data is transferred from the local bus to one or more data buffers as directed by the master controller and the buffer controller . The data then flows out of the buffer s to the data bus through a solid state controller and on to the solid state storage such as NAND flash or other storage media. In a preferred embodiment data and associated out of band metadata object metadata arriving with the data is communicated using one or more data channels comprising one or more solid state storage controllers 1 and associated solid state storage 1 while at least one channel solid state storage controller solid state storage is dedicated to in band metadata such as index information and other metadata generated internally to the solid state storage device .

The local bus is typically a bidirectional bus or set of busses that allows for communication of data and commands between devices internal to the solid state storage device controller and between devices internal to the solid state storage device and devices connected to the system bus . The bridge facilitates communication between the local bus and system bus . One of skill in the art will recognize other embodiments such as ring structures or switched star configurations and functions of buses and bridges .

The system bus is typically a bus of a computer or other device in which the solid state storage device is installed or connected. In one embodiment the system bus may be a PCI e bus a Serial Advanced Technology Attachment serial ATA bus parallel ATA or the like. In another embodiment the system bus is an external bus such as small computer system interface SCSI FireWire Fiber Channel USB PCIe AS or the like. The solid state storage device may be packaged to fit internally to a device or as an externally connected device.

The solid state storage device controller includes a master controller that controls higher level functions within the solid state storage device . The master controller in various embodiments controls data flow by interpreting object requests and other requests directs creation of indexes to map object identifiers associated with data to physical locations of associated data coordinating DMA requests etc. Many of the functions described herein are controlled wholly or in part by the master controller .

In one embodiment the master controller uses embedded controller s . In another embodiment the master controller uses local memory such as a dynamic memory array dynamic random access memory DRAM a static memory array static random access memory SRAM etc. In one embodiment the local memory is controlled using the master controller . In another embodiment the master controller accesses the local memory via a memory controller . In another embodiment the master controller runs a Linux server and may support various common server interfaces such as the World Wide Web hyper text markup language HTML etc. In another embodiment the master controller uses a nano processor. The master controller may be constructed using programmable or standard logic or any combination of controller types listed above. One skilled in the art will recognize many embodiments for the master controller .

In one embodiment where the storage controller solid state storage device controller manages multiple data storage devices solid state storage the master controller divides the work load among internal controllers such as the solid state storage controllers . For example the master controller may divide an object to be written to the data storage devices e.g. solid state storage so that a portion of the object is stored on each of the attached data storage devices. This feature is a performance enhancement allowing quicker storage and access to an object. In one embodiment the master controller is implemented using an FPGA. In another embodiment the firmware within the master controller may be updated through the management bus the system bus over a network connected to a NIC or other device connected to the system bus .

In one embodiment the master controller which manages objects emulates block storage such that a computer or other device connected to the storage device solid state storage device views the storage device solid state storage device as a block storage device and sends data to specific physical addresses in the storage device solid state storage device . The master controller then divides up the blocks and stores the data blocks as it would objects. The master controller then maps the blocks and physical address sent with the block to the actual locations determined by the master controller . The mapping is stored in the object index. Typically for block emulation a block device application program interface API is provided in a driver in the computer client or other device wishing to use the storage device solid state storage device as a block storage device.

In another embodiment the master controller coordinates with NIC controllers and embedded RDMA controllers to deliver just in time RDMA transfers of data and command sets. NIC controller may be hidden behind a non transparent port to enable the use of custom drivers. Also a driver on a client may have access to the computer network through an I O memory driver using a standard stack API and operating in conjunction with NICs .

In one embodiment the master controller is also a redundant array of independent drive RAID controller. Where the data storage device solid state storage device is networked with one or more other data storage devices solid state storage devices the master controller may be a RAID controller for single tier RAID multi tier RAID progressive RAID etc. The master controller also allows some objects to be stored in a RAID array and other objects to be stored without RAID. In another embodiment the master controller may be a distributed RAID controller element. In another embodiment the master controller may comprise many RAID distributed RAID and other functions as described elsewhere.

In one embodiment the master controller coordinates with single or redundant network managers e.g. switches to establish routing to balance bandwidth utilization failover etc. In another embodiment the master controller coordinates with integrated application specific logic via local bus and associated driver software. In another embodiment the master controller coordinates with attached application specific processors or logic via the external system bus and associated driver software. In another embodiment the master controller coordinates with remote application specific logic via the computer network and associated driver software. In another embodiment the master controller coordinates with the local bus or external bus attached hard disk drive HDD storage controller.

In one embodiment the master controller communicates with one or more storage controllers where the storage device solid state storage device may appear as a storage device connected through a SCSI bus Internet SCSI iSCSI fiber channel etc. Meanwhile the storage device solid state storage device may autonomously manage objects and may appear as an object file system or distributed object file system. The master controller may also be accessed by peer controllers and or application specific processors .

In another embodiment the master controller coordinates with an autonomous integrated management controller to periodically validate FPGA code and or controller software validate FPGA code while running reset and or validate controller software during power on reset support external reset requests support reset requests due to watchdog timeouts and support voltage current power temperature and other environmental measurements and setting of threshold interrupts. In another embodiment the master controller manages garbage collection to free erase blocks for reuse. In another embodiment the master controller manages wear leveling. In another embodiment the master controller allows the data storage device solid state storage device to be partitioned into multiple virtual devices and allows partition based media encryption. In yet another embodiment the master controller supports a solid state storage controller with advanced multi bit ECC correction. One of skill in the art will recognize other features and functions of a master controller in a storage controller or more specifically in a solid state storage device .

In one embodiment the solid state storage device controller includes a memory controller which controls a dynamic random memory array and or a static random memory array . As stated above the memory controller may be independent or integrated with the master controller . The memory controller typically controls volatile memory of some type such as DRAM dynamic random memory array and SRAM static random memory array . In other examples the memory controller also controls other memory types such as electrically erasable programmable read only memory EEPROM etc. In other embodiments the memory controller controls two or more memory types and the memory controller may include more than one controller. Typically the memory controller controls as much SRAM as is feasible and by DRAM to supplement the SRAM .

In one embodiment the object index is stored in memory and then periodically off loaded to a channel of the solid state storage or other non volatile memory. One of skill in the art will recognize other uses and configurations of the memory controller dynamic memory array and static memory array .

In one embodiment the solid state storage device controller includes a DMA controller that controls DMA operations between the storage device solid state storage device and one or more external memory controllers and associated external memory arrays and CPUs . Note that the external memory controllers and external memory arrays are called external because they are external to the storage device solid state storage device . In addition the DMA controller may also control RDMA operations with requesting devices through a NIC and associated RDMA controller . DMA and RDMA are explained in more detail below.

In one embodiment the solid state storage device controller includes a management controller connected to a management bus . Typically the management controller manages environmental metrics and status of the storage device solid state storage device . The management controller may monitor device temperature fan speed power supply settings etc. over the management bus . The management controller may support the reading and programming of erasable programmable read only memory EEPROM for storage of FPGA code and controller software. Typically the management bus is connected to the various components within the storage device solid state storage device . The management controller may communicate alerts interrupts etc. over the local bus or may include a separate connection to a system bus or other bus. In one embodiment the management bus is an Inter Integrated Circuit IC bus. One of skill in the art will recognize other related functions and uses of a management controller connected to components of the storage device solid state storage device by a management bus .

In one embodiment the solid state storage device controller includes miscellaneous logic that may be customized for a specific application. Typically where the solid state device controller or master controller is are configured using a FPGA or other configurable controller custom logic may be included based on a particular application customer requirement storage requirement etc.

The write data pipeline includes a packetizer that receives a data or metadata segment to be written to the solid state storage either directly or indirectly through another write data pipeline stage and creates one or more packets sized for the solid state storage . The data or metadata segment is typically part of an object but may also include an entire object. In another embodiment the data segment is part of a block of data but may also include an entire block of data. Typically an object is received from a computer client or other computer or device and is transmitted to the solid state storage device in data segments streamed to the solid state storage device or computer . A data segment may also be known by another name such as data parcel but as referenced herein includes all or a portion of an object or data block.

Each object is stored as one or more packets. Each object may have one or more container packets. Each packet contains a header. The header may include a header type field. Type fields may include data object attribute metadata data segment delimiters multi packet object structures object linkages and the like. The header may also include information regarding the size of the packet such as the number of bytes of data included in the packet. The length of the packet may be established by the packet type. The header may include information that establishes the relationship of the packet to the object. An example might be the use of an offset in a data packet header to identify the location of the data segment within the object. One of skill in the art will recognize other information that may be included in a header added to data by a packetizer and other information that may be added to a data packet.

Each packet includes a header and possibly data from the data or metadata segment. The header of each packet includes pertinent information to relate the packet to the object to which the packet belongs. For example the header may include an object identifier and offset that indicates the data segment object or data block from which the data packet was formed. The header may also include a logical address used by the storage bus controller to store the packet. The header may also include information regarding the size of the packet such as the number of bytes included in the packet. The header may also include a sequence number that identifies where the data segment belongs with respect to other packets within the object when reconstructing the data segment or object. The header may include a header type field. Type fields may include data object attributes metadata data segment delimiters multi packet object structures object linkages and the like. One of skill in the art will recognize other information that may be included in a header added to data or metadata by a packetizer and other information that may be added to a packet.

The write data pipeline includes an ECC generator that generates one or more error correcting codes ECC for the one or more packets received from the packetizer . The ECC generator typically uses an error correcting algorithm to generate ECC which is stored with the packet. The ECC stored with the packet is typically used to detect and correct errors introduced into the data through transmission and storage. In one embodiment packets are streamed into the ECC generator as un encoded blocks of length N. A syndrome of length S is calculated appended and output as an encoded block of length N S. The value of N and S are dependent upon the characteristics of the algorithm which is selected to achieve specific performance efficiency and robustness metrics. In the preferred embodiment there is no fixed relationship between the ECC blocks and the packets the packet may comprise more than one ECC block the ECC block may comprise more than one packet and a first packet may end anywhere within the ECC block and a second packet may begin after the end of the first packet within the same ECC block. In the preferred embodiment ECC algorithms are not dynamically modified. In a preferred embodiment the ECC stored with the data packets is robust enough to correct errors in more than two bits.

Beneficially using a robust ECC algorithm allowing more than single bit correction or even double bit correction allows the life of the solid state storage to be extended. For example if flash memory is used as the storage medium in the solid state storage the flash memory may be written approximately 100 000 times without error per erase cycle. This usage limit may be extended using a robust ECC algorithm. Having the ECC generator and corresponding ECC correction module onboard the solid state storage device the solid state storage device can internally correct errors and has a longer useful life than if a less robust ECC algorithm is used such as single bit correction. However in other embodiments the ECC generator may use a less robust algorithm and may correct single bit or double bit errors. In another embodiment the solid state storage device may comprise less reliable storage such as multi level cell MLC flash in order to increase capacity which storage may not be sufficiently reliable without more robust ECC algorithms.

In one embodiment the write pipeline includes an input buffer that receives a data segment to be written to the solid state storage and stores the incoming data segments until the next stage of the write data pipeline such as the packetizer or other stage for a more complex write data pipeline is ready to process the next data segment. The input buffer typically allows for discrepancies between the rate data segments are received and processed by the write data pipeline using an appropriately sized data buffer. The input buffer also allows the data bus to transfer data to the write data pipeline at rates greater than can be sustained by the write data pipeline in order to improve efficiency of operation of the data bus . Typically when the write data pipeline does not include an input buffer a buffering function is performed elsewhere such as in the solid state storage device but outside the write data pipeline in the computer such as within a network interface card NIC or at another device for example when using remote direct memory access RDMA .

In another embodiment the write data pipeline also includes a write synchronization buffer that buffers packets received from the ECC generator prior to writing the packets to the solid state storage . The write synch buffer is located at a boundary between a local clock domain and a solid state storage clock domain and provides buffering to account for the clock domain differences. In other embodiments synchronous solid state storage may be used and synchronization buffers may be eliminated.

In one embodiment the write data pipeline also includes a media encryption module that receives the one or more packets from the packetizer either directly or indirectly and encrypts the one or more packets using an encryption key unique to the solid state storage device prior to sending the packets to the ECC generator . Typically the entire packet is encrypted including the headers. In another embodiment headers are not encrypted. In this document encryption key is understood to mean a secret encryption key that is managed externally from an embodiment that integrates the solid state storage and where the embodiment requires encryption protection. The media encryption module and corresponding media decryption module provide a level of security for data stored in the solid state storage . For example where data is encrypted with the media encryption module if the solid state storage is connected to a different solid state storage controller solid state storage device or computer the contents of the solid state storage typically could not be read without use of the same encryption key used during the write of the data to the solid state storage without significant effort.

In a typical embodiment the solid state storage device does not store the encryption key in non volatile storage and allows no external access to the encryption key. The encryption key is provided to the solid state storage controller during initialization. The solid sate storage device may use and store a non secret cryptographic nonce that is used in conjunction with an encryption key. A different nonce may be stored with every packet. Data segments may be split between multiple packets with unique nonces for the purpose of improving protection by the encryption algorithm. The encryption key may be received from a client a computer key manager or other device that manages the encryption key to be used by the solid state storage controller . In another embodiment the solid state storage may have two or more partitions and the solid state storage controller behaves as though it were two or more solid state storage controllers each operating on a single partition within the solid state storage . In this embodiment a unique media encryption key may be used with each partition.

In another embodiment the write data pipeline also includes an encryption module that encrypts a data or metadata segment received from the input buffer either directly or indirectly prior sending the data segment to the packetizer the data segment encrypted using an encryption key received in conjunction with the data segment. The encryption module differs from the media encryption module in that the encryption keys used by the encryption module to encrypt data may not be common to all data stored within the solid state storage device but may vary on an object basis and received in conjunction with receiving data segments as described below. For example an encryption key for a data segment to be encrypted by the encryption module may be received with the data segment or may be received as part of a command to write an object to which the data segment belongs. The solid sate storage device may use and store a non secret cryptographic nonce in each object packet that is used in conjunction with the encryption key. A different nonce may be stored with every packet. Data segments may be split between multiple packets with unique nonces for the purpose of improving protection by the encryption algorithm. In one embodiment the nonce used by the media encryption module is the same as that used by the encryption module .

The encryption key may be received from a client a computer key manager or other device that holds the encryption key to be used to encrypt the data segment. In one embodiment encryption keys are transferred to the solid state storage controller from one of a solid state storage device computer client or other external agent which has the ability to execute industry standard methods to securely transfer and protect private and public keys.

In one embodiment the encryption module encrypts a first packet with a first encryption key received in conjunction with the packet and encrypts a second packet with a second encryption key received in conjunction with the second packet. In another embodiment the encryption module encrypts a first packet with a first encryption key received in conjunction with the packet and passes a second data packet on to the next stage without encryption. Beneficially the encryption module included in the write data pipeline of the solid state storage device allows object by object or segment by segment data encryption without a single file system or other external system to keep track of the different encryption keys used to store corresponding objects or data segments. Each requesting device or related key manager independently manages encryption keys used to encrypt only the objects or data segments sent by the requesting device .

In another embodiment the write data pipeline includes a compression module that compresses the data for metadata segment prior to sending the data segment to the packetizer . The compression module typically compresses a data or metadata segment using a compression routine known to those of skill in the art to reduce the storage size of the segment. For example if a data segment includes a string of 512 zeros the compression module may replace the 512 zeros with code or token indicating the 512 zeros where the code is much more compact than the space taken by the 512 zeros.

In one embodiment the compression module compresses a first segment with a first compression routine and passes along a second segment without compression. In another embodiment the compression module compresses a first segment with a first compression routine and compresses the second segment with a second compression routine. Having this flexibility within the solid state storage device is beneficial so that clients or other devices writing data to the solid state storage device may each specify a compression routine or so that one can specify a compression routine while another specifies no compression. Selection of compression routines may also be selected according to default settings on a per object type or object class basis. For example a first object of a specific object may be able to override default compression routine settings and a second object of the same object class and object type may use the default compression routine and a third object of the same object class and object type may use no compression.

In one embodiment the write data pipeline includes a garbage collector bypass that receives data segments from the read data pipeline as part of a data bypass in a garbage collection system. A garbage collection system typically marks packets that are no longer valid typically because the packet is marked for deletion or has been modified and the modified data is stored in a different location. At some point the garbage collection system determines that a particular section of storage may be recovered. This determination may be due to a lack of available storage capacity the percentage of data marked as invalid reaching a threshold a consolidation of valid data an error detection rate for that section of storage reaching a threshold or improving performance based on data distribution etc. Numerous factors may be considered by a garbage collection algorithm to determine when a section of storage is to be recovered.

Once a section of storage has been marked for recovery valid packets in the section typically must be relocated. The garbage collector bypass allows packets to be read into the read data pipeline and then transferred directly to the write data pipeline without being routed out of the solid state storage controller . In a preferred embodiment the garbage collector bypass is part of an autonomous garbage collector system that operates within the solid state storage device . This allows the solid state storage device to manage data so that data is systematically spread throughout the solid state storage to improve performance data reliability and to avoid overuse and underuse of any one location or area of the solid state storage and to lengthen the useful life of the solid state storage .

The garbage collector bypass coordinates insertion of segments into the write data pipeline with other segments being written by clients or other devices. In the depicted embodiment the garbage collector bypass is before the packetizer in the write data pipeline and after the depacketizer in the read data pipeline but may also be located elsewhere in the read and write data pipelines . The garbage collector bypass may be used during a flush of the write pipeline to fill the remainder of the virtual page in order to improve the efficiency of storage within the Solid State Storage and thereby reduce the frequency of garbage collection.

In one embodiment the write data pipeline includes a write buffer that buffers data for efficient write operations. Typically the write buffer includes enough capacity for packets to fill at least one virtual page in the solid state storage . This allows a write operation to send an entire page of data to the solid state storage without interruption. By sizing the write buffer of the write data pipeline and buffers within the read data pipeline to be the same capacity or larger than a storage write buffer within the solid state storage writing and reading data is more efficient since a single write command may be crafted to send a full virtual page of data to the solid state storage instead of multiple commands.

While the write buffer is being filled the solid state storage may be used for other read operations. This is advantageous because other solid state devices with a smaller write buffer or no write buffer may tie up the solid state storage when data is written to a storage write buffer and data flowing into the storage write buffer stalls. Read operations will be blocked until the entire storage write buffer is filled and programmed. Another approach for systems without a write buffer or a small write buffer is to flush the storage write buffer that is not full in order to enable reads. Again this is inefficient because multiple write program cycles are required to fill a page.

For depicted embodiment with a write buffer sized larger than a virtual page a single write command which includes numerous subcommands can then be followed by a single program command to transfer the page of data from the storage write buffer in each solid state storage element to the designated page within each solid state storage element . This technique has the benefits of eliminating partial page programming which is known to reduce data reliability and durability and freeing up the destination bank for reads and other commands while the buffer fills.

In one embodiment the write buffer is a ping pong buffer where one side of the buffer is filled and then designated for transfer at an appropriate time while the other side of the ping pong buffer is being filled. In another embodiment the write buffer includes a first in first out FIFO register with a capacity of more than a virtual page of data segments. One of skill in the art will recognize other write buffer configurations that allow a virtual page of data to be stored prior to writing the data to the solid state storage .

In another embodiment the write buffer is sized smaller than a virtual page so that less than a page of information could be written to a storage write buffer in the solid state storage . In the embodiment to prevent a stall in the write data pipeline from holding up read operations data is queued using the garbage collection system that needs to be moved from one location to another as part of the garbage collection process. In case of a data stall in the write data pipeline the data can be fed through the garbage collector bypass to the write buffer and then on to the storage write buffer in the solid state storage to fill the pages of a virtual page prior to programming the data. In this way a data stall in the write data pipeline would not stall reading from the solid state storage device .

In another embodiment the write data pipeline includes a write program module with one or more user definable functions within the write data pipeline . The write program module allows a user to customize the write data pipeline . A user may customize the write data pipeline based on a particular data requirement or application. Where the solid state storage controller is an FPGA the user may program the write data pipeline with custom commands and functions relatively easily. A user may also use the write program module to include custom functions with an ASIC however customizing an ASIC may be more difficult than with an FPGA. The write program module may include buffers and bypass mechanisms to allow a first data segment to execute in the write program module while a second data segment may continue through the write data pipeline . In another embodiment the write program module may include a processor core that can be programmed through software.

Note that the write program module is shown between the input buffer and the compression module however the write program module could be anywhere in the write data pipeline and may be distributed among the various stages . In addition there may be multiple write program modules distributed among the various states that are programmed and operate independently. In addition the order of the stages may be altered. One of skill in the art will recognize workable alterations to the order of the stages based on particular user requirements.

The read data pipeline includes an ECC correction module that determines if a data error exists in ECC blocks a requested packet received from the solid state storage by using ECC stored with each ECC block of the requested packet. The ECC correction module then corrects any errors in the requested packet if any error exists and the errors are correctable using the ECC. For example if the ECC can detect an error in six bits but can only correct three bit errors the ECC correction module corrects ECC blocks of the requested packet with up to three bits in error. The ECC correction module corrects the bits in error by changing the bits in error to the correct one or zero state so that the requested data packet is identical to when it was written to the solid state storage and the ECC was generated for the packet.

If the ECC correction module determines that the requested packets contains more bits in error than the ECC can correct the ECC correction module cannot correct the errors in the corrupted ECC blocks of the requested packet and sends an interrupt. In one embodiment the ECC correction module sends an interrupt with a message indicating that the requested packet is in error. The message may include information that the ECC correction module cannot correct the errors or the inability of the ECC correction module to correct the errors may be implied. In another embodiment the ECC correction module sends the corrupted ECC blocks of the requested packet with the interrupt and or the message.

In the preferred embodiment a corrupted ECC block or portion of a corrupted ECC block of the requested packet that cannot be corrected by the ECC correction module is read by the master controller corrected and returned to the ECC correction module for further processing by the read data pipeline . In one embodiment a corrupted ECC block or portion of a corrupted ECC block of the requested packet is sent to the device requesting the data. The requesting device may correct the ECC block or replace the data using another copy such as a backup or mirror copy and then may use the replacement data of the requested data packet or return it to the read data pipeline . The requesting device may use header information in the requested packet in error to identify data required to replace the corrupted requested packet or to replace the object to which the packet belongs. In another preferred embodiment the solid state storage controller stores data using some type of RAID and is able to recover the corrupted data. In another embodiment the ECC correction module sends and interrupt and or message and the receiving device fails the read operation associated with the requested data packet. One of skill in the art will recognize other options and actions to be taken as a result of the ECC correction module determining that one or more ECC blocks of the requested packet are corrupted and that the ECC correction module cannot correct the errors.

The read data pipeline includes a depacketizer that receives ECC blocks of the requested packet from the ECC correction module directly or indirectly and checks and removes one or more packet headers. The depacketizer may validate the packet headers by checking packet identifiers data length data location etc. within the headers. In one embodiment the header includes a hash code that can be used to validate that the packet delivered to the read data pipeline is the requested packet. The depacketizer also removes the headers from the requested packet added by the packetizer . The depacketizer may directed to not operate on certain packets but pass these forward without modification. An example might be a container label that is requested during the course of a rebuild process where the header information is required by the object index reconstruction module . Further examples include the transfer of packets of various types destined for use within the solid state storage device . In another embodiment the depacketizer operation may be packet type dependent.

The read data pipeline includes an alignment module that receives data from the depacketizer and removes unwanted data. In one embodiment a read command sent to the solid state storage retrieves a packet of data. A device requesting the data may not require all data within the retrieved packet and the alignment module removes the unwanted data. If all data within a retrieved page is requested data the alignment module does not remove any data.

The alignment module re formats the data as data segments of an object in a form compatible with a device requesting the data segment prior to forwarding the data segment to the next stage. Typically as data is processed by the read data pipeline the size of data segments or packets changes at various stages. The alignment module uses received data to format the data into data segments suitable to be sent to the requesting device and joined to form a response. For example data from a portion of a first data packet may be combined with data from a portion of a second data packet. If a data segment is larger than a data requested by the requesting device the alignment module may discard the unwanted data.

In one embodiment the read data pipeline includes a read synchronization buffer that buffers one or more requested packets read from the solid state storage prior to processing by the read data pipeline . The read synchronization buffer is at the boundary between the solid state storage clock domain and the local bus clock domain and provides buffering to account for the clock domain differences.

In another embodiment the read data pipeline includes an output buffer that receives requested packets from the alignment module and stores the packets prior to transmission to the requesting device . The output buffer accounts for differences between when data segments are received from stages of the read data pipeline and when the data segments are transmitted to other parts of the solid state storage controller or to the requesting device. The output buffer also allows the data bus to receive data from the read data pipeline at rates greater than can be sustained by the read data pipeline in order to improve efficiency of operation of the data bus .

In one embodiment the read data pipeline includes a media decryption module that receives one or more encrypted requested packets from the ECC correction module and decrypts the one or more requested packets using the encryption key unique to the solid state storage device prior to sending the one or more requested packets to the depacketizer . Typically the encryption key used to decrypt data by the media decryption module is identical to the encryption key used by the media encryption module . In another embodiment the solid state storage may have two or more partitions and the solid state storage controller behaves as though it were two or more solid state storage controllers each operating on a single partition within the solid state storage . In this embodiment a unique media encryption key may be used with each partition.

In another embodiment the read data pipeline includes a decryption module that decrypts a data segment formatted by the depacketizer prior to sending the data segment to the output buffer . The data segment decrypted using an encryption key received in conjunction with the read request that initiates retrieval of the requested packet received by the read synchronization buffer . The decryption module may decrypt a first packet with an encryption key received in conjunction with the read request for the first packet and then may decrypt a second packet with a different encryption key or may pass the second packet on to the next stage of the read data pipeline without decryption. Typically the decryption module uses a different encryption key to decrypt a data segment than the media decryption module uses to decrypt requested packets. When the packet was stored with a non secret cryptographic nonce the nonce is used in conjunction with an encryption key to decrypt the data packet. The encryption key may be received from a client a computer key manager or other device that manages the encryption key to be used by the solid state storage controller .

In another embodiment the read data pipeline includes a decompression module that decompresses a data segment formatted by the depacketizer . In the preferred embodiment the decompression module uses compression information stored in one or both of the packet header and the container label to select a complementary routine to that used to compress the data by the compression module . In another embodiment the decompression routine used by the decompression module is dictated by the device requesting the data segment being decompressed. In another embodiment the decompression module selects a decompression routine according to default settings on a per object type or object class basis. A first packet of a first object may be able to override a default decompression routine and a second packet of a second object of the same object class and object type may use the default decompression routine and a third packet of a third object of the same object class and object type may use no decompression.

In another embodiment the read data pipeline includes a read program module that includes one or more user definable functions within the read data pipeline . The read program module has similar characteristics to the write program module and allows a user to provide custom functions to the read data pipeline . The read program module may be located as shown in may be located in another position within the read data pipeline or may include multiple parts in multiple locations within the read data pipeline . Additionally there may be multiple read program modules within multiple locations within the read data pipeline that operate independently. One of skill in the art will recognize other forms of a read program module within a read data pipeline . As with the write data pipeline the stages of the read data pipeline may be rearranged and one of skill in the art will recognize other orders of stages within the read data pipeline .

The solid state storage controller includes control and status registers and corresponding control queues . The control and status registers and control queues facilitate control and sequencing commands and subcommands associated with data processed in the write and read data pipelines . For example a data segment in the packetizer may have one or more corresponding control commands or instructions in a control queue associated with the ECC generator . As the data segment is packetized some of the instructions or commands may be executed within the packetizer . Other commands or instructions may be passed to the next control queue through the control and status registers as the newly formed data packet created from the data segment is passed to the next stage.

Commands or instructions may be simultaneously loaded into the control queues for a packet being forwarded to the write data pipeline with each pipeline stage pulling the appropriate command or instruction as the respective packet is executed by that stage. Similarly commands or instructions may be simultaneously loaded into the control queues for a packet being requested from the read data pipeline with each pipeline stage pulling the appropriate command or instruction as the respective packet is executed by that stage. One of skill in the art will recognize other features and functions of control and status registers and control queues .

The solid state storage controller and or solid state storage device may also include a bank interleave controller a synchronization buffer a storage bus controller and a multiplexer MUX which are described in relation to .

The bank interleave controller directs one or more commands to two or more queues in the bank interleave controller and coordinates among the banks of the solid state storage execution of the commands stored in the queues such that a command of a first type executes on one bank while a command of a second type executes on a second bank . The one or more commands are separated by command type into the queues. Each bank of the solid state storage has a corresponding set of queues within the bank interleave controller and each set of queues includes a queue for each command type.

The bank interleave controller coordinates among the banks of the solid state storage execution of the commands stored in the queues. For example a command of a first type executes on one bank while a command of a second type executes on a second bank . Typically the command types and queue types include read and write commands and queues but may also include other commands and queues that are storage media specific. For example in the embodiment depicted in erase and management queues are included and would be appropriate for flash memory NRAM MRAM DRAM PRAM etc.

For other types of solid state storage other types of commands and corresponding queues may be included without straying from the scope of the invention. The flexible nature of an FPGA solid state storage controller allows flexibility in storage media. If flash memory were changed to another solid state storage type the bank interleave controller storage bus controller and MUX could be altered to accommodate the media type without significantly affecting the data pipelines and other solid state storage controller functions.

In the embodiment depicted in the bank interleave controller includes for each bank a read queue for reading data from the solid state storage a write queue for write commands to the solid state storage an erase queue for erasing an erase block in the solid state storage an a management queue for management commands. The bank interleave controller also includes corresponding read write erase and management agents . In another embodiment the control and status registers and control queues or similar components queue commands for data sent to the banks of the solid state storage without a bank interleave controller .

The agents in one embodiment direct commands of the appropriate type destined for a particular bank to the correct queue for the bank . For example the read agent may receive a read command for bank 1 and directs the read command to the bank 1 read queue . The write agent may receive a write command to write data to a location in bank 0 of the solid state storage and will then send the write command to the bank 0 write queue . Similarly the erase agent may receive an erase command to erase an erase block in bank 1 and will then pass the erase command to the bank 1 erase queue . The management agent typically receives management commands status requests and the like such as a reset command or a request to read a configuration register of a bank such as bank 0 . The management agent sends the management command to the bank 0 management queue

The agents typically also monitor status of the queues and send status interrupt or other messages when the queues are full nearly full non functional etc. In one embodiment the agents receive commands and generate corresponding sub commands. In one embodiment the agents receive commands through the control status registers and generate corresponding sub commands which are forwarded to the queues . One of skill in the art will recognize other functions of the agents .

The queues typically receive commands and store the commands until required to be sent to the solid state storage banks . In a typical embodiment the queues are first in first out FIFO registers or a similar component that operates as a FIFO. In another embodiment the queues store commands in an order that matches data order of importance or other criteria.

The bank controllers typically receive commands from the queues and generate appropriate subcommands. For example the bank 0 write queue may receive a command to write a page of data packets to bank 0 . The bank 0 controller may receive the write command at an appropriate time and may generate one or more write subcommands for each data packet stored in the write buffer to be written to the page in bank 0 . For example bank 0 controller may generate commands to validate the status of bank 0 and the solid state storage array select the appropriate location for writing one or more data packets clear the input buffers within the solid state storage memory array transfer the one or more data packets to the input buffers program the input buffers into the selected location verify that the data was correctly programmed and if program failures occur do one or more of interrupting the master controller retrying the write to the same physical location and retrying the write to a different physical location. Additionally in conjunction with example write command the storage bus controller will cause the one or more commands to multiplied to each of the each of the storage I O buses with the logical address of the command mapped to a first physical addresses for storage I O bus and mapped to a second physical address for storage I O bus and so forth as further described below.

Typically bus arbiter selects from among the bank controllers and pulls subcommands from output queues within the bank controllers and forwards these to the Storage Bus Controller in a sequence that optimizes the performance of the banks . In another embodiment the bus arbiter may respond to a high level interrupt and modify the normal selection criteria. In another embodiment the master controller can control the bus arbiter through the control and status registers . One of skill in the art will recognize other means by which the bus arbiter may control and interleave the sequence of commands from the bank controllers to the solid state storage .

The bus arbiter typically coordinates selection of appropriate commands and corresponding data when required for the command type from the bank controllers and sends the commands and data to the storage bus controller . The bus arbiter typically also sends commands to the storage control bus to select the appropriate bank . For the case of flash memory or other solid state storage with an asynchronous bi directional serial storage I O bus only one command control information or set of data can be transmitted at a time. For example when write commands or data are being transmitted to the solid state storage on the storage I O bus read commands data being read erase commands management commands or other status commands cannot be transmitted on the storage I O bus . For example when data is being read from the storage I O bus data cannot be written to the solid state storage .

For example during a write operation on bank 0 the bus arbiter selects the bank 0 controller which may have a write command or a series of write sub commands on the top of its queue which cause the storage bus controller to execute the following sequence. The bus arbiter forwards the write command to the storage bus controller which sets up a write command by selecting bank 0 through the storage control bus sending a command to clear the input buffers of the solid state storage elements associated with the bank 0 and sending a command to validate the status of the solid state storage elements associated with the bank 0 . The storage bus controller then transmits a write subcommand on the storage I O bus which contains the physical addresses including the address of the logical erase block for each individual physical erase solid stage storage element as mapped from the logical erase block address. The storage bus controller then muxes the write buffer through the write sync buffer to the storage I O bus through the MUX and streams write data to the appropriate page. When the page is full then storage bus controller causes the solid state storage elements associated with the bank 0 to program the input buffer to the memory cells within the solid state storage elements . Finally the storage bus controller validates the status to ensure that page was correctly programmed.

A read operation is similar to the write example above. During a read operation typically the bus arbiter or other component of the bank interleave controller receives data and corresponding status information and sends the data to the read data pipeline while sending the status information on to the control and status registers . Typically a read data command forwarded from bus arbiter to the storage bus controller will cause the MUX to gate the read data on storage I O bus to the read data pipeline and send status information to the appropriate control and status registers through the status MUX .

The bus arbiter coordinates the various command types and data access modes so that only an appropriate command type or corresponding data is on the bus at any given time. If the bus arbiter has selected a write command and write subcommands and corresponding data are being written to the solid state storage the bus arbiter will not allow other command types on the storage I O bus . Beneficially the bus arbiter uses timing information such as predicted command execution times along with status information received concerning bank status to coordinate execution of the various commands on the bus with the goal of minimizing or eliminating idle time of the busses.

The master controller through the bus arbiter typically uses expected completion times of the commands stored in the queues along with status information so that when the subcommands associated with a command are executing on one bank other subcommands of other commands are executing on other banks . When one command is fully executed on a bank the bus arbiter directs another command to the bank . The bus arbiter may also coordinate commands stored in the queues with other commands that are not stored in the queues .

For example an erase command may be sent out to erase a group of erase blocks within the solid state storage . An erase command may take 10 to 1000 times more time to execute than a write or a read command or 10 to 100 times more time to execute than a program command. For N banks the bank interleave controller may split the erase command into N commands each to erase a virtual erase block of a bank . While bank 0 is executing an erase command the bus arbiter may select other commands for execution on the other banks . The bus arbiter may also work with other components such as the storage bus controller the master controller etc. to coordinate command execution among the buses. Coordinating execution of commands using the bus arbiter bank controllers queues and agents of the bank interleave controller can dramatically increase performance over other solid state storage systems without a bank interleave function.

In one embodiment the solid state controller includes one bank interleave controller that serves all of the storage elements of the solid state storage . In another embodiment the solid state controller includes a bank interleave controller for each row of storage elements . For example one bank interleave controller serves one row of storage elements SSS 0.0 SSS 0.N a second bank interleave controller serves a second row of storage elements SSS 1.0 SSS 1.N etc.

In another alternate embodiment not shown commands are stored in a single queue where the commands may be pulled from the queue in an order other than how they are stored so that the bank interleave controller can execute a command on one bank while other commands are executing on the remaining banks . One of skill in the art will easily recognize other queue configurations and types to enable execution of a command on one bank while other commands are executing on other banks 

The solid state storage controller includes a synchronization buffer that buffers commands and status messages sent and received from the solid state storage . The synchronization buffer is located at the boundary between the solid state storage clock domain and the local bus clock domain and provides buffering to account for the clock domain differences. The synchronization buffer write synchronization buffer and read synchronization buffer may be independent or may act together to buffer data commands status messages etc. In the preferred embodiment the synchronization buffer is located where there are the fewest number of signals crossing the clock domains. One skilled in the art will recognize that synchronization between clock domains may be arbitrarily moved to other locations within the solid state storage device in order to optimize some aspect of design implementation.

The solid state storage controller includes a storage bus controller that interprets and translates commands for data sent to and read from the solid state storage and status messages received from the solid state storage based on the type of solid state storage . For example the storage bus controller may have different timing requirements for different types of storage storage with different performance characteristics storage from different manufacturers etc. The storage bus controller also sends control commands to the storage control bus .

In the preferred embodiment the solid state storage controller includes a MUX that comprises an array of multiplexers where each multiplexer is dedicated to a row in the solid state storage array . For example multiplexer is associated with solid state storage elements . MUX routes the data from the write data pipeline and commands from the storage bus controller to the solid state storage via the storage I O bus and routes data and status messages from the solid state storage via the storage I O bus to the read data pipeline and the control and status registers through the storage bus controller synchronization buffer and bank interleave controller .

In the preferred embodiment the solid state storage controller includes a MUX for each row of solid state storage elements e.g. SSS 0.1 SSS 0.2 SSS 0.N . A MUX combines data from the write data pipeline and commands sent to the solid state storage via the storage I O bus and separates data to be processed by the read data pipeline from commands. Packets stored in the write buffer are directed on busses out of the write buffer through a write synchronization buffer for each row of solid state storage elements SSS x.0 to SSS x.N to the MUX for each row of solid state storage elements SSS x.0 to SSS x.N . The commands and read data are received by the MUXes from the storage I O bus . The MUXes also direct status messages to the storage bus controller .

The storage bus controller includes a mapping module . The mapping module maps a logical address of an erase block to one or more physical addresses of an erase block. For example a solid state storage with an array of twenty storage elements e.g. SSS 0.0 to SSS M.0 per block may have a logical address for a particular erase block mapped to twenty physical addresses of the erase block one physical address per storage element. Because the storage elements are accessed in parallel erase blocks at the same position in each storage element in a row of storage elements will share a physical address. To select one erase block e.g. in storage element SSS 0.0 instead of all erase blocks in the row e.g. in storage elements SSS 0.0 0.1 . . . 0.N one bank in this case bank 0 is selected.

This logical to physical mapping for erase blocks is beneficial because if one erase block becomes damaged or inaccessible the mapping can be changed to map to another erase block. This mitigates the loss of losing an entire virtual erase block when one element s erase block is faulty. The remapping module changes a mapping of a logical address of an erase block to one or more physical addresses of a virtual erase block spread over the array of storage elements . For example virtual erase block 1 may be mapped to erase block 1 of storage element SSS 0.0 to erase block 1 of storage element SSS 1.0 . . . and to storage element M.0 virtual erase block 2 may be mapped to erase block 2 of storage element SSS 0.1 to erase block 2 of storage element SSS 1.1 . . . and to storage element M.1 etc.

If erase block 1 of a storage element SSS0.0 is damaged experiencing errors due to wear etc. or cannot be used for some reason the remapping module could change the logical to physical mapping for the logical address that pointed to erase block 1 of virtual erase block 1. If a spare erase block call it erase block of storage element SSS 0.0 is available and currently not mapped the remapping module could change the mapping of virtual erase block 1 to point to erase block of storage element SSS 0.0 while continuing to point to erase block 1 of storage element SSS 1.0 erase block 1 of storage element SSS 2.0 not shown . . . and to storage element M.0 . The mapping module or remapping module could map erase blocks in a prescribed order virtual erase block 1 to erase block 1 of the storage elements virtual erase block 2 to erase block 2 of the storage elements etc. or may map erase blocks of the storage elements in another order based on some other criteria.

In one embodiment the erase blocks could be grouped by access time. Grouping by access time meaning time to execute a command such as programming writing data into pages of specific erase blocks can level command completion so that a command executed across the erase blocks of a virtual erase block is not limited by the slowest erase block. In other embodiments the erase blocks may be grouped by wear level health etc. One of skill in the art will recognize other factors to consider when mapping or remapping erase blocks.

In one embodiment the storage bus controller includes a status capture module that receives status messages from the solid state storage and sends the status messages to the status MUX . In another embodiment when the solid state storage is flash memory the storage bus controller includes a NAND bus controller . The NAND bus controller directs commands from the read and write data pipelines to the correct location in the solid state storage coordinates timing of command execution based on characteristics of the flash memory etc. If the solid state storage is another solid state storage type the NAND bus controller would be replaced by a bus controller specific to the storage type. One of skill in the art will recognize other functions of a NAND bus controller .

Typically a first packet includes an object identifier that identifies the object for which the packet was created. A second packet may include a header with information used by the solid state storage device to associate the second packet to the object identified in the first packet and offset information locating the second packet within the object and data. The solid state storage device controller manages the bank and physical area to which the packets are streamed.

The ECC generator receives a packet from the packetizer and generates ECC for the data packets. Typically there is no fixed relationship between packets and ECC blocks. An ECC block may comprise one or more packets. A packet may comprise one or more ECC blocks. A packet may start and end anywhere within an ECC block. A packet may start anywhere in a first ECC block and end anywhere in a subsequent ECC block.

The write synchronization buffer buffers the packets as distributed within the corresponding ECC blocks prior to writing ECC blocks to the solid state storage and then the solid state storage controller writes the data at an appropriate time considering clock domain differences and the method ends . The write synch buffer is located at the boundary between a local clock domain and a solid state storage clock domain. Note that the method describes receiving one or more data segments and writing one or more data packets for convenience but typically a stream of data segments is received and a group. Typically a number of ECC blocks comprising a complete virtual page of solid state storage are written to the solid state storage . Typically the packetizer receives data segments of one size and generates packets of another size. This necessarily requires data or metadata segments or parts of data or metadata segments to be combined to form data packets to capture all of the data of the segments into packets.

The ECC generator receives a packet from the packetizer and generates one or more ECC blocks for the packets. The write synchronization buffer buffers the packets as distributed within the corresponding ECC blocks prior to writing ECC blocks to the solid state storage and then the solid state storage controller writes the data at an appropriate time considering clock domain differences. When data is requested from the solid state storage ECC blocks comprising one or more data packets are read into the read synchronization buffer and buffered . The ECC blocks of the packet are received over the storage I O bus . Since the storage I O bus is bi directional when data is read write operations command operations etc. are halted.

The ECC correction module receives the ECC blocks of the requested packets held in the read synchronization buffer and corrects errors within each ECC block as necessary. If the ECC correction module determines that one or more errors exist in an ECC block and the errors are correctable using the ECC syndrom the ECC correction module corrects the error in the ECC block. If the ECC correction module determines that a detected error is not correctable using the ECC the ECC correction module sends an interrupt.

The depacketizer receives the requested packet after the ECC correction module corrects any errors and depacketizes the packets by checking and removing the packet header of each packet. The alignment module receives packets after depacketizing removes unwanted data and re formats the data packets as data or metadata segments of an object in a form compatible with the device requesting the segment or object. The output buffer receives requested packets after depacketizing and buffers the packets prior to transmission to the requesting device and the method ends .

The apparatus includes a sequential storage module that sequentially writes data packets in a page within a storage division. The packets are sequentially stored whether they are new packets or modified packets. Modified packets are in this embodiment are typically not written back to a location where they were previously stored. In one embodiment the sequential storage module writes a packet to a first location in a page of a storage division then to the next location in the page and to the next and the next until the page is filled. The sequential storage module then starts to fill the next page in the storage division. This continues until the storage division is filled.

In a preferred embodiment the sequential storage module starts writing packets to storage write buffers in the storage elements e.g. SSS 0.0 to SSS M.0 of a bank bank 0 . When the storage write buffers are full the solid state storage controller causes the data in the storage write buffers to be programmed into designated pages within the storage elements of the bank . Then another bank e.g. bank 1 is selected and the sequential storage module starts writing packets to storage write buffers of the storage elements of the bank while the first bank 0 is programming the designated pages. When the storage write buffers of this bank are full the contents of the storage write buffers are programmed into another designated page in each storage element . This process is efficient because while one bank is programming a page storage write buffers of another bank can be filling.

The storage division includes a portion of a solid state storage in a solid state storage device . Typically the storage division is an erase block. For flash memory an erase operation on an erase block writes ones to every bit in the erase block by charging each cell. This is a lengthy process compared to a program operation which starts with a location being all ones and as data is written some bits are changed to zero by discharging the cells written with a zero. However where the solid state storage is not flash memory or has flash memory where an erase cycle takes a similar amount of time as other operations such as a read or a program the storage division may not be required to be erased.

As used herein a storage division is equivalent in area to an erase block but may or may not be erased. Where erase block is used herein an erase block may refer to a particular area of a designated size within a storage element e.g. SSS 0.0 and typically includes a certain quantity of pages. Where erase block is used in conjunction with flash memory it is typically a storage division that is erased prior to being written. Where erase block is used with solid state storage it may or may not be erased. As used herein an erase block may include one erase block or a group of erase blocks with one erase block in each of a row of storage elements e.g. SSS 0.0 to SSS M.0 which may also be referred to herein as a virtual erase block. When referring to the logical construct associated with the virtual erase block the erase blocks may be referred to herein as a logical erase block LEB .

Typically the packets are sequentially stored by order of processing. In one embodiment where a write data pipeline is used the sequential storage module stores packets in the order that they come out of the write data pipeline . This order may be a result of data segments arriving from a requesting device mixed with packets of valid data that are being read from another storage division as valid data is being recovered from a storage division during a recovery operation as explained below. Re routing recovered valid data packets to the write data pipeline may include the garbage collector bypass as described above in relation to the solid state storage controller of .

The apparatus includes a storage division selection module that selects a storage division for recovery. Selecting a storage division for recovery may be to reuse the storage division by the sequential storage module for writing data thus adding the recovered storage division to the storage pool or to recover valid data from the storage division after determining that the storage division is failing unreliable should be refreshed or other reason to take the storage division temporarily or permanently out of the storage pool. In another embodiment the storage division selection module selects a storage division for recovery by identifying a storage division or erase block with a high amount of invalid data.

In another embodiment the storage division selection module selects a storage division for recovery by identifying a storage division or erase block with a low amount of wear. For example identifying a storage division or erase block with a low amount of wear may include identifying a storage division with a low amount of invalid data a low number of erase cycles low bit error rate or low program count low number of times a page of data in a buffer is written to a page in the storage division program count may be measured from when the device was manufactured from when the storage division was last erased from other arbitrary events and from combinations of these . The storage division selection module may also use any combination of the above or other parameters to determine a storage division with a low amount of wear. Selecting a storage division for recovery by determining a storage division with a low amount of wear may be desirable to find storage divisions that are under used may be recovered for wear leveling etc.

In another embodiment the storage division selection module selects a storage division for recovery by identifying a storage division or erase block with a high amount of wear. For example identifying a storage division or erase block with a high amount of wear may include identifying a storage division with a high number of erase cycles high bit error rate a storage division with a non recoverable ECC block or high program count. The storage division selection module may also use any combination of the above or other parameters to determine a storage division with a high amount of wear. Selecting a storage division for recovery by determining a storage division with a high amount of wear may be desirable to find storage divisions that are over used may be recovered by refreshing the storage division using an erase cycle etc. or to retire the storage division from service as being unusable.

The apparatus includes a data recovery module that reads valid data packets from the storage division selected for recovery queues the valid data packets with other data packets to be written sequentially by the sequential storage module and updates an index with a new physical address of the valid data written by the sequential storage module . Typically the index is the object index mapping data object identifiers of objects to physical addresses of where packets derived from the data object are stored in the solid state storage .

In one embodiment the apparatus includes a storage division recovery module that prepares the storage division for use or reuse and marks the storage division as available to the sequential storage module for sequentially writing data packets after the data recovery module has completed copying valid data from the storage division. In another embodiment the apparatus includes a storage division recovery module that marks the storage division selected for recovery as unavailable for storing data. Typically this is due to the storage division selection module identifying a storage division or erase block with a high amount of wear such that the storage division or erase block is not in condition to be used for reliable data storage.

In one embodiment the apparatus is in a solid state storage device controller of a solid state storage device . In another embodiment the apparatus controls a solid state storage device controller . In another embodiment a portion of the apparatus is in a solid state storage device controller . In another embodiment the object index updated by the data recovery module is also located in the solid state storage device controller

In one embodiment the storage division is an erase block and the apparatus includes an erase module that erases an erase block selected for recovery after the data recovery module has copied valid data packets from the selected erase block and before the storage division recovery module marks the erase block as available. For flash memory and other solid state storage with an erase operation taking much longer than read or write operations erasing a data block prior to making it available for writing new data is desirable for efficient operation. Where the solid state storage is arranged in banks the erase operation by the erase module may be executed on one bank while other banks are executing reads writes or other operations.

In one embodiment the apparatus includes a garbage marking module that identifies a data packet in a storage division as invalid in response to an operation indicating that the data packet is no longer valid. For example if a data packet is deleted the garbage marking module may identify the data packet as invalid. A read modify write operation is another way for a data packet to be identified as invalid. In one embodiment the garbage marking module may identify the data packet as invalid by updating an index. In another embodiment the garbage marking module may identify the data packet as invalid by storing another data packet that indicates that the invalid data packet has been deleted. This is advantageous because storing in the solid state storage information that the data packet has been deleted allows the object index reconstruction module or similar module to reconstruct the object index with an entry indicating that the invalid data packet has been deleted.

In one embodiment the apparatus may be utilized to fill the remainder of a virtual page of data following a flush command in order to improve overall performance where the flush command halts data flowing into the write pipeline until the write pipeline empties and all packets have been permanently written into non volatile solid state storage . This has the benefit of reducing the amount of garbage collection required the amount of time used to erase storage divisions and the amount of time required to program virtual pages. For example a flush command may be received when only one small packet is prepared for writing into the virtual page of the solid state storage . Programming this nearly empty virtual page might result in a need to immediately recover the wasted space causing the valid data within the storage division to be unnecessarily garbage collected and the storage division erased recovered and returned to the pool of available space for writing by the sequential storage module .

Marking the data packet as invalid rather than actually erasing an invalid data packet is efficient because as mentioned above for flash memory and other similar storage an erase operation takes a significant amount of time. Allowing a garbage collection system as described in the apparatus to operate autonomously within the solid state storage provides a way to separate erase operations from reads writes and other faster operations so that the solid state storage device can operate much faster than many other solid state storage systems or data storage devices.

The storage division selection module selects a storage division for recovery and the data recovery module reads valid data packets from the storage division selected for recovery. Typically valid data packets are data packets that have not been marked for erasure or deletion or some other invalid data marking and are considered valid or good data. The data recovery module queues the valid data packets with other data packets scheduled to be written sequentially by the sequential storage module . The data recovery module updates an index with a new physical address of the valid data written by the sequential storage module . The index includes a mapping of physical addresses of data packets to object identifiers. The data packets are those stored in stored in the solid state storage and the object identifiers correspond to the data packets.

After the data recovery module completes copying valid data from the storage division the storage division recovery module marks the storage division selected for recovery as available to the sequential storage module for sequentially writing data packets and the method ends .

Typically when data is no longer useful it may be erased. In many file systems an erase command deletes a directory entry in the file system while leaving the data in place in the storage device containing the data. Typically a data storage device is not involved in this type of erase operation. Another method of erasing data is to write zeros ones or some other null data character to the data storage device to actually replace the erased file. However this is inefficient because valuable bandwidth is used while transmitting the data is being overwritten. In addition space in the storage device is taken up by the data used to overwrite invalid data.

In some storage devices like the solid state storage device described herein updating previously stored data does not overwrite existing data. Attempting to overwrite data with a string of ones or a string of zeros on such a device takes up valuable space without fulfilling the desired intent of overwriting the existing data. For these devices such as solid state storage devices a client typically does not have an ability to overwrite data to erase it.

When receiving a string of repeated characters or character strings the received data is highly compressible but typically compression is done by a file system prior to transmission to a storage device. A typical storage device cannot distinguish between compressed data and uncompressed data. The storage device may also receive a command to read the erased file so the storage device may transmit a stream of zeros ones or a null character to the requesting device. Again bandwidth is required to transmit data representing the erased file.

From the foregoing discussion it should be apparent that a need exists for an apparatus system and method for a storage device to receive a directive that data is to be erased so that the storage device can store a data segment token that represents an empty data segment or data with repeated characters or character strings. The apparatus system and method may also erase the existing data so that the resulting used storage space comprises the small data segment token. An apparatus system and method are presented that overcome some or all of the shortcomings of the prior art.

In one embodiment the apparatus includes a token directive generation module that generates a storage request with a token directive. The token directive includes a request to store a data segment token on the storage device . The token directive is intended to be substituted for a series of repeated identical characters or a series of repeated identical character strings that would be sent to the storage device and stored as a data segment if the data segment token was not sent in its place. In one embodiment the series of repeated identical characters indicate that the data segment is empty. For example the series of repeated identical characters may be zeros or ones and a data segment filled with zeros or ones may be interpreted as empty.

The token directive includes at least a data segment identifier and a data segment length. The data segment identifier is typically an object ID file name or other identifier known to a file system application server etc. seeking to store the repeated identical characters or character string on the storage device. The data segment length is typically the storage space required by the series of repeated identical characters or character strings. The data segment token and the token directive typically do not include data of the data segment such as the series of repeated identical characters.

The token directive may however include other pertinent information for forming the data segment token such as at least one instance of the repeated identical character or character string. The token directive may also include metadata such as a data segment location an address from a file system location on a data storage device corresponding to the data segment etc. One of skill in the art will recognize other information that may be included with a token directive. In one embodiment the token directive generation module generates a data segment token along with the token directive.

In one embodiment the token directive generation module generates both a token directive and a secure erase command in response to a request to overwrite existing data on the storage device . The existing data includes data identified on the storage device with the same data segment identifier as the data segment identifier in the token directive. Typically a request to overwrite data is sent where it is not sufficient to merely mark data as invalid or garbage delete a pointer to the data or other typical delete operation but where the data is required to be overwritten in such a way that that the data is not recoverable. For example the request to overwrite the data may be required where data is considered sensitive information and must be destroyed for security reasons.

The secure erase command directs the storage device to overwrite existing data so that the existing data is non recoverable. The storage device then creates the data segment token as well as overwrites recovers erases etc. the existing data. As a result the existing data is non recoverable and the data segment token is stored on the storage device which typically takes much less storage space than the existing data.

In a further embodiment the apparatus includes an erase confirmation module that receives a confirmation that the existing data on the storage device has been overwritten with characters so that the existing data is non recoverable. The confirmation may be forwarded to a requesting device or client and may be used to verify that the existing data has been put in a condition where it is non recoverable. In other embodiments the secure erase command may direct the storage device to overwrite the existing data with a specific character or character string or the execute command may be executed multiple times. One of skill in the art will recognize other ways to configure one or more secure erase commands to ensure the existing data is non recoverable.

Data may be encrypted and then stored on a storage device where encryption is accomplished using an encryption key received by the storage device in conjunction with storing the data. Where the existing data is encrypted with this received encryption key before being stored in another embodiment the token directive generation module generates an encryption erase command along with generating a token directive in response to receiving a request to overwrite the existing data. The encryption erase command erases the encryption key used to store the existing data so that the encryption key is non recoverable.

In one embodiment erasing the encryption key includes erasing the encryption key from the requesting device. In another embodiment erasing the encryption key includes erasing the encryption key from a server key vault or other location where the encryption key is stored. Erasing the encryption key may include replacing the encryption key with other data or with a series of characters so that the encryption key cannot be recovered in any way. Erasing the encryption key typically makes the existing data on the storage device non recoverable where an encryption routine was used to encrypt the existing data that is robust enough to thwart attempts to decrypt the existing data. The request to overwrite the existing data could be a secure erase directive where the data is overwritten for security reasons a request to overwrite data to erase the data a request that seeks to replace the existing data with the repeated identical characters or character strings or the like. In one embodiment a secure erase directive causes devices to both securely erase the encryption key and securely erase the existing data. In one embodiment erasure of the encryption key may allow secure erasure of the data on the storage device to be postponed until a garbage collection process erases the data as part of a storage space recovery process. One of skill in the art will recognize other ways to erase an encryption key and other ways to receive a request to overwrite existing data.

In one embodiment a token directive includes the data segment token and the token directive transmission module transmits the data segment token with the token directive. In another embodiment a token directive does not include the data segment token and includes a command for the storage device to generate the data segment token. In this embodiment the token directive transmission module transmits the token directive with the command to generate a data segment token but does not transmit a data segment token.

The apparatus includes a token directive transmission module that transmits the token directive to the storage device . Typically the token directive transmission module transmits the token directive as part of a storage request. The storage request may be in the form of an object request a data request or other form known to those of skill in the art. Where the token directive generation module generates a secure erase directive the token directive transmission module transmits the secure erase directive to the storage device . Where the token directive generation module generates an erase encryption key command the erase encryption key command is transmitted where necessary to another device to execute the command.

In one embodiment the token directive transmission module transmits a token directive without a data segment token. In this embodiment the token directive includes instructions and information for the storage device to create a data segment token. In another embodiment the token directive transmission module transmits a token directive that includes a data segment token. In this embodiment the storage device is capable of recognizing that the data segment token received with the token directive represents a data segment and takes appropriate action to store the data segment token such that the data segment token represents the data segment rather than merely storing the data segment token as ordinary data.

The apparatus in a particular embodiment includes a read receiver module that receives a storage request to read the data segment from the storage device and a read request transmission module that transmits the storage request to the storage device . Typically the storage request is received from a requesting client such as an external client a client internal to the server such as an application or file server running on the server etc. One of skill in the art will recognize other devices and software functioning as a requesting client from which the read receiver module can receive a storage request.

The storage request includes a request to read the data segment corresponding to the data segment token requested to be stored in the token directive transmitted to the storage device by the token directive transmission module . The requesting client in one embodiment is unaware that the data segment has been stored in the form of a data segment token. In another embodiment the requesting device is aware that the data segment has been stored as a data segment token but is unaware of the information stored in the data segment token.

The apparatus in the particular embodiment also includes a read token directive receiver module that receives a message corresponding to the requested data segment token from the storage device where the message includes at least the data segment identifier and the data segment length. The message typically does not include data of the data segment. The message may also include other information stored in the data segment token such as a data segment location or the repeated identical character or character string. In the particular embodiment the apparatus includes a requesting client response module that transmits a response to the requesting client formulated from the message received from the storage device .

In one embodiment read token directive receiver module also receives in the message a confirmation that existing data has been overwritten with characters such that the existing data is non recoverable where the existing data has been previously stored on the storage device and referenced with the same data segment identifier from the data segment token received in the message. The confirmation may also be received from the storage device independently from any storage request to read the data segment.

In another embodiment where the requesting client requires the data segment the apparatus includes a data segment regeneration module that reconstitutes data of the data segment using information contained in the message. In this case the response sent to the requesting client includes the reconstituted data segment. In another embodiment the response sent to the requesting client includes information contained in the message received from the storage device . The requesting client may then reconstitute the data segment or use the information in some other way. In another embodiment the message includes the data segment token. The data segment token may be used by the data segment regeneration module to reconstitute the data segment before forwarding to the requesting client or the requesting client response module may simply forward the data segment token.

In one embodiment the storage request with the token directive also includes a request to reserve storage space on the storage device where the requested reserved storage space is an amount of storage space that is about the same to the data segment length. In another embodiment the request to reserve storage space is for an amount of storage space different than the data segment length. For example if the storage device is a solid state storage device the solid state storage device may be connected to a hard drive or other long term inexpensive storage while the solid state storage is configured as cache for the long term storage. The request to reserve storage may cause the solid state storage device to flush a portion of the cache to the long term storage in preparation for writing data to the solid state storage device . One of skill in the art will recognize other situations were a request to reserve storage space is desired.

In one embodiment an apparatus may be provided with a read receiver module a read request transmission module a read token directive receiver module and a requesting client response module which are substantially similar to those described above. In the embodiment the modules may be independent from an apparatus including a token directive generation module or a token directive transmission module . In one embodiment the apparatus includes a data segment regeneration module which is substantially similar to the data segment regeneration module described above.

The read token directive receiver module receives a message corresponding to the requested data segment token from the storage device where the message includes at least the data segment identifier and the data segment length. The message is substantially free from data of the data segment. The requesting client response module transmits a response to the requesting client formulated from the message received from the storage device and the method ends .

In the depicted embodiment the modules are included in the storage device or storage controller . In another embodiment at least a portion of one or more of the modules is located outside the storage device . In a further embodiment the requesting device includes a portion of the modules in the form of drivers software or other function of one or more of the modules . For example a token generation module and a reconstitute data segment module are shown in the requesting device . One of skill in the art will recognize other ways to distribute and implement the functionality of the modules .

The apparatus includes a write request receiver module that receives a storage request from the requesting device where the storage request includes a request to store a data segment in the storage device . The data segment includes a series of repeated identical characters or character strings. Typically the series of repeated identical characters signify that the data segment is empty. This is especially true when the series of repeated identical characters are ones or zeros. The apparatus includes a data segment token storage module that stores a data segment token in the storage device . The data segment token includes at least a data segment identifier and a data segment length. The data segment token is substantially free of the actual data from the data segment.

The data segment token may be stored in many forms. In one embodiment includes an entry in an index where the index corresponds to information and data stored on the storage device . For example the index may be an object index as described above in relation to the apparatus depicted in . The index may also be file system index a block storage index or other index known to those of skill in the art. In another embodiment the data segment token includes or is in the form of metadata stored on the storage device . In another embodiment the data segment token is stored as metadata on the storage device and includes an entry in an index. One of skill in the art will recognize other ways to storage a data segment token.

In one embodiment the storage request includes a token directive to store the data segment token where the storage request is essentially free from data of the data segment. The token directive may include the data segment token or a command to generate a data segment token. Where the token directive does not include the data segment token the data segment token storage module generates the data segment token from information in the token directive. If the token directive includes the data segment token the data segment token storage module recognizes the data segment token as a data structure representing the data segment identified by the data segment identifier in the token directive and stores the data segment token appropriately.

Typically where the data segment token storage module recognizes a data segment token the data segment token is differentiated in some way from other data stored on the storage device . For example a requesting device may merely compress data and send a compressed object file or data segment so that the storage device does not distinguish the compressed data segment from other data received by way of other storage requests.

Where the data segment token storage module recognizes that a received data segment token is a data segment token the data segment token storage module may store the data segment token in such a way that when read it may appear as a data segment rather than the data segment token. One of skill in the art will recognize other ways that the data segment token storage module may store a data segment token after recognizing that a received data segment token is a data segment token rather than a data segment.

In another embodiment storage request includes data from the data segment. In this embodiment the apparatus includes a token generation module that generates a data segment token from the data segment where the data segment token is created in response to the storage request to store the data segment. In a further embodiment the token generation module resides in the requesting device possibly in the form of a driver.

In one embodiment the apparatus includes a secure erase module that overwrites existing data with characters such that the existing data is non recoverable where the existing data includes data of a data segment previously stored on the storage device identified with the same data segment identifier as the data segment identified in the storage request. In this embodiment a data segment token is stored with the data segment identifier and data segment length and existing data identified by the same data segment identifier stored in the data segment token is erased by overwriting the existing data. Typically the existing characters are overwritten by zeros ones or some other character string so that the existing data is destroyed and is non recoverable.

In a further embodiment the secure erase module also includes an erase confirmation module that transmits a message indicating that the existing data has been overwritten. Typically the message is sent to the requesting device . The erase confirmation message is transmitted after the secure erase module has overwritten the existing data. The message may be transmitted in the same transaction as the storage request or in a different transaction.

In another embodiment the secure erase module overwrites the existing data during a storage space recovery operation. For example if the storage device is a solid state storage device as described above the storage space recovery operation may be related to the garbage collection describe in relation to the apparatus depicted in . However a storage space recovery operation involving a request to overwrite the existing data would typically be expedited so that the storage location where the existing data is stored is necessarily recovered prior to any confirmation message is sent by the erase confirmation module . In one embodiment the existing data is marked or otherwise identified to indicate that a secure erase has been requested. The confirmation message would typically not be sent until the existing data marked for erasure has been overwritten and made non recoverable. In another embodiment the secure erase module merely marks the existing data as invalid for later storage space recovery. In another embodiment the secure erase updates an index to indicate that the existing data is invalid and prevents access to this data until the data is overwritten during later storage space recovery.

In one embodiment the secure erase module overwrites the existing data each time a data segment token is stored. In another embodiment the storage request specifically includes a request to overwrite the existing data and the secure erase module overwrites the existing data in response to the request to overwrite the existing data. In another embodiment the secure erase module stores metadata information regarding confirmation that the existing data has been erased so that subsequent reads can indicate the erasure.

In other embodiments where a secure erase is not received the existing data is deleted. In one embodiment deleting the data includes deleting an index entry address etc. In a preferred embodiment the when a data segment token is stored the corresponding existing data is marked as invalid or otherwise ready for storage recovery. The data may be later recovered in a storage recovery operation garbage collection operation etc.

In a particular embodiment the apparatus includes a read request receiver module that receives a storage request to read the data segment a read data segment token module that reads the data segment token corresponding to the data segment requested by the storage request and a read request response module that transmits a response to the requesting device . The response is generated using the data segment token corresponding to the requested data segment.

The storage request to read the data segment in one embodiment is associated with the storage request and serves to confirm that the storage request has been successful. In another embodiment the request to read the data segment is independent from the storage request and may be initiated by the requesting device that made the storage request or another separate requesting device .

In one embodiment where the requesting device is capable of receiving information from the data segment token in place of actual data the read request response module includes a transmit data segment token module that transmits in the response a message to the requesting device . The message includes at least the data segment identifier and the data segment length but may also include a data segment location at least one instance of the repeated identical character or character string or other pertinent information. Typically the message does not include the actual data of the data segment other than what is included in the data segment token.

In another embodiment where the requesting device is expecting to receive a data segment the apparatus includes a reconstitute data segment module that reconstitutes data of the data segment using the data segment token. The read request response module also includes a transmit data segment module that transmits the reconstituted requested data segment to the requesting device . In another embodiment the reconstitute data segment module resides at the requesting device possibly as in the form of a driver and the transmit data segment token module transmits the message with the data segment token information to the requesting device . The reconstitute data segment module at the requesting device reconstitutes the requested data segment from the message.

In one embodiment the system includes a separate apparatus that includes a read request receiver module a read data segment token module a read request response module which are substantially similar to those described above. The apparatus may be independent from the apparatus including the write request receiver module and the data segment token storage module . In one embodiment the read request response module includes a transmit data segment token module and or a transmit data segment module and the apparatus may include a reconstitute data segment module where the modules are substantially similar to those described above.

The present invention may be embodied in other specific forms without departing from its spirit or essential characteristics. The described embodiments are to be considered in all respects only as illustrative and not restrictive. The scope of the invention is therefore indicated by the appended claims rather than by the foregoing description. All changes which come within the meaning and range of equivalency of the claims are to be embraced within their scope.

