---

title: Method and apparatus for efficient and transparent network management and application coordination for software defined optical switched data center networks
abstract: Application actions are optimized by receiving actions from one or more distributed applications, translating the received actions into one or more network operation primitives, processing the network operation primitives to determine an optimized set of network operation primitives, determining an optimal execution sequence for the optimized set of network operation primitives, and executing the optimized set of network operation primitives based on the optimal execution sequence.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09332324&OS=09332324&RS=09332324
owner: 
number: 09332324
owner_city: Chino Hills
owner_country: US
publication_date: 20140310
---
This application claims priority to U.S. Provisional Patent Application No. 61 777 675 filed Mar. 12 2013 which is incorporated herein by reference.

This patent application is related to U.S. Provisional Patent Application No. 61 719 026 filed Oct. 26 2012 now U.S. application Ser. No. 14 057 133 filed Oct. 18 2013 published as U.S. Patent Application Publication No. 2014 0119728. Substantive portions of U.S. Provisional Patent Application No. 61 719 026 are attached hereto in an Appendix to the present application. U.S. Provisional Patent Application No. 61 719 026 is incorporated herein by reference.

The present invention relates generally to network management systems and more particularly to self optimizing systems and methods for managing and coordinating a plurality of applications deployed on an optical switched data center network.

In the past several years designing the next generation data center network switching fabric and server interconnect has received much attention from both academia and industry. One approach seeks to design a completely flat network architecture in which all to all non blocking communication is achieved by deploying highly redundant switches and cables throughout the network. However such designs are limited by high deployment cost and cabling complexity. In contrast a second approach proposes constructing an over subscribed network with on demand high throughput paths to resolve network congestion and hotspots. Examples of this approach are the c Through and Helios design hybrid electrical and optical network architectures. In these designs the electrical portion is responsible for maintaining connectivity between all servers and delivering traffic for low bandwidth flows and the optical portion provides on demand high bandwidth links for server pairs with heavy network traffic. Compared to these architectures the newly proposed systems such as the system presented in the referenced U.S. Provisional Patent Application No. 61 719 026 pursue an all optical design and employ optical switching and optical wavelength division multiplexing technologies. This on demand bandwidth flexibility is achieved using reconfigurable optical switching devices which are further controlled by a central network manager. This resulting network architecture is called a software defined optical switched network platform. However managing such an optical switched data center network involves interacting with the low level optical devices monitoring network traffic statistics and optimizing network performance all of which are daunting tasks for most application developers and network operators.

Accordingly it is desirable to solve the management challenges and complexities of software defined optical switched data center networks by providing a transparent and self optimizing network management platform for application developers and network operators. It is further preferable to provide an intermediate management platform that abstracts functionalities of lower layer physical optical network equipment communicates with the higher layer applications via application programming interfaces APIs and automatically coordinates schedules and optimizes the actions of multiple applications.

In one embodiment application actions are optimized by receiving actions from one or more distributed applications translating the received actions into one or more network operation primitives processing the network operation primitives to determine an optimized set of network operation primitives determining an optimal execution sequence for the optimized set of network operation primitives and executing the optimized set of network operation primitives based on the optimal execution sequence.

In another embodiment a method of optimizing application actions performed in an optical switching fabric is described. A processor system receives actions from one or more distributed applications. The actions are associated with an optical switching fabric. The processor system translates the received actions into one or more network operation primitives. The processor system processes the network operation primitives to determine an optimized set of network operation primitives to be actuated on the optical switching fabric and determines an optimal execution sequence for the optimized set of network operation primitives. The processor system executes the optimized set of network operation primitives based on the optimal execution sequence on the optical switching fabric.

In another embodiment a system for optimizing distributed application actions in an optical switching fabric is described. The optical switching fabric includes a plurality of interconnected optical switching units. A network manager is operably coupled to the optical switching fabric. The network manager is configured to reconfigure the interconnected optical switching units. A managing platform is configured to receive actions from one or more distributed applications. The actions are associated with the optical switching fabric. The received actions are translated into one or more network operation primitives. The network operation primitives are processed to determine an optimized set of network operation primitives to be actuated on the optical switching fabric. An optimal execution sequence is determined for the optimized set of network operation primitives. The optimized set of network operation primitives is executed based on the optimal execution sequence on the optical switching fabric.

The present invention will now be described in detail with reference to the drawings which are provided as illustrative examples of the invention as to enable those skilled in the art to practice the invention. The figures and examples below are not meant to limit the scope of the present invention to a single embodiment but other embodiments are possible by way of interchange of some or all of the described or illustrated elements. Moreover where some of the elements of the present invention can be partially or fully implemented using known components only portions of such known components that are necessary for an understanding of the present invention will be described and a detailed description of other portions of such known components will be omitted so as not to obscure the invention. Embodiments described as being implemented in software should not be limited thereto but can include embodiments implemented in hardware or a combination of hardware and software and vice versa as will be apparent to those skilled in the art unless otherwise specified therein.

In the present specification an embodiment showing a singular component should not be considered limiting rather the invention is intended to encompass other embodiments including a plurality of the same embodiments and vice versa unless explicitly stated otherwise herein. Moreover applicants do not intend for any term in the specification or claims to be ascribed an uncommon or special meaning unless explicitly set forth as such. Further the present invention encompasses present and future known equivalents to the known components referred to herein by way of illustration.

Preferred embodiments of the present invention relate to network management systems and more particularly to self optimizing systems and methods for managing and coordinating a plurality of applications deployed on an optical switched data center network. The network management platform abstracts the functionalities of the lower layer optical switched network into a set of operation primitives and defines a set of intuitive application programming interfaces APIs that translate the actions of the higher layer applications into the lower layer network operation primitives. Additional aspects also allow the described network management platform to manage the actions of higher layer applications such that network performance is optimized and conflicts between application actions are resolved. In accordance with these and other aspects a method of managing the actions of higher layer applications includes translating application actions into network operation primitives resolving conflicts between operation primitives and violations of external e.g. quality of service or QoS constraints and mapping these operation primitives into device implementable commands.

Certain terminology is used in the following description for convenience only and is not limiting. The words lower bottom top and upper designate directions in the drawings to which reference is made. The terminology includes the above listed words derivatives thereof and words of similar import. Additionally the words a and an as used in the claims and in the corresponding portions of the specification mean at least one. 

Referring to the drawings in detail wherein like reference numerals indicate like elements throughout is a system diagram of an exemplary prior art architecture of a software defined optical data center network defined by optical switched server cluster interconnects in data centers or other facilities . In the optical data center network a plurality of servers are interconnected with an optical switching fabric . The optical switching fabric includes a plurality of optical switching units such that every server in the data center can reach every other server through the optical switching fabric . All of the optical switching units are controlled by a network manager and can be reconfigured at run time. Preferably the network manager is centralized but in other embodiments the network manager may be distributed. A plurality of distributed applications are deployed on the servers . Preferably each application is configured to initiate network related actions such as demanding bandwidth of a particular network path imposing latency requirement s between a certain pair of origin s and destination s and requesting physical isolation from certain other applications . All these actions involve low level interactions between the physical network equipment such as the optical switching units and detailed knowledge of network operation. These tasks are typically difficult to perform for the application owner and or the network service provider.

Referring to there is shown a management platform for managing the software defined optical data center network in which a software network controller monitors and reconfigures the set of optical switching units such that certain functionality is achieved such as performance optimization traffic engineering and network virtualization . The management platform is preferably deployed to reside between the applications and the optical switching fabric . The management platform is implemented by one or more physical or virtual servers. Based on the practical application scenarios one can choose to implement the management platform in a centralized server with failover mechanisms or distribute the functionality onto different servers using decentralized management mechanisms such as a distributed hashing table DHT . Various such failover mechanisms and decentralized management mechanisms are known to those skilled in the art. The management platform preferably includes a management portal a fusion engine an optimization engine and an actuator . The management portal is an interface between the applications and the network management system .

Referring to a flowchart showing the operating process of the management system is shown. The process starts at step and in step the management portal translates application actions such as querying network status and changing network equipment configurations into network operation primitives. Network operation primitives as defined below are a set of high level abstractions of basic functionalities of the underlying optical switching fabric . In step the translated network operation primitives of the multiple applications actions are passed from the management portal to the fusion engine . The fusion engine processes the received operation primitives to for example remove duplicate primitives consolidate related primitives into a single primitive resolve any conflicts between primitives and the like. In step the processed operation primitives are output by the fusion engine to the optimization engine . The optimization engine processes the received primitives output by the fusion engine to determine a primitive execution plan such that network performance is optimized and all the primitives objectives and deadlines are met. In step the actuator translates the optimized primitive execution plan into a sequence of implementable network operations e.g. device commands and configuration files and sends them to the underlying optical switching fabric .

The management portal is implemented by one or more servers and is configured to abstract the functionalities of the optical switching fabric using a set of operation primitives. Referring to an exemplary definition of a network operation primitive is shown as a set of arguments. In the preferred embodiment the set of arguments is a six tuple defined as . However those skilled in the art will understand that other forms arguments and definitions of the network operation primitives can be defined without departing from the scope of this invention to achieve the described functionalities. Under the preferred definition of the network operation primitive the arguments ID1 and ID2 are globally unique identifiers and can refer to both the end hosts i.e. physical servers or virtual machines and network nodes i.e. switches and routers . The argument t designates the deadline for this action. The Type argument denotes whether the action is GET or SET The Parameter argument specifies whether this action is related to BANDWIDTH LATENCY PATHLENGTH CONNECTIVITY ISOLATION or the like. The Metric argument carries the value of the Parameter argument.

Applying the above network operation primitive definitions as shown by entry Ex1 of the action Are Server 10.1.10.1 and Server 10.1.10.2 connected can be denoted by 

Similarly as shown by entry Ex2 in the action Allocate 500 Mbps between Server 01 23 45 67 89 AB and Server 00 14 22 01 23 45 in 100 ms can be denoted by 

Furthermore as shown by entry Ex3 in the action the network traffic between Server 10.1.10.1 and Server 10.1.10.2 cannot share a link with any other applications can be denoted by 

More complicated actions can be decomposed into multiple atomic actions using the network operation primitives.

The network operation primitives also serve as the interface between the applications and the management platform . In this way without knowing detailed network configurations and complicated device control commands the applications can query the network statistics e.g. bandwidth latency path length connectivity etc. specify their own desired values and compose more complicated actions. However actions of different applications are initiated from the individual applications own perspectives and therefore may contain redundancy inconsistency and even conflicts. The fusion engine is designed to process the native application actions and resolve these issues.

The fusion engine is implemented by one or more servers and is configured to perform a fusion process as shown by the steps of the flowchart of . The fusion process begins at step and proceeds to step where the fusion engine first enumerates all ID pairs. All identical actions are consolidated by the fusion engine in step . At step for each ID pair the fusion engine examines each action to determine whether it is subsumed by other actions. All such actions are reduced to the actions that subsume them. For instance if one action queries a network bandwidth of a given link in 100 ms and another action does the same thing but in 150 ms then the second action is subsumed and can be replaced by the first action.

At step for each ID pair the fusion engine checks whether there is any conflict between actions. As is understood by those skilled in the art a conflict arises when two or more irreconcilable actions access the same system variables such as latency of a network route configuration file or bandwidth of a link. For instance a conflict arises if one application sets the latency of a certain network segment to 50 ms and another application sets it to 100 ms. Another example of conflict is the total bandwidth demand of a certain network link from all applications exceeding the link capacity. After identifying all conflicts the fusion engine in step passes the conflicts to the network manager of the underlying optical switching fabric to determine whether the network can be reconfigured to resolve the identified conflicts. If not the network manager returns the irresolvable conflicts to the fusion engine in step . The fusion engine reports the errors to the applications that initiated these conflicted actions. If the conflict can be resolved by the network manager at step the fusion engine passes all the remaining conflict free actions to the optimization engine for further processing.

Referring to one exemplary approach to identify subsumed or conflicted operations is shown. Under this approach an operation forest is constructed. The operation forest is composed of one or multiple operation trees which is further composed of a set of nodes and directed links connecting many pairs of nodes. Within each operation tree a node represents an operation. A link pointing from node A to node B implies that operation A subsumes operation B.

An algorithm that identifies subsumed and conflicted operations is illustrated in the flowchart of . The procedure starts at step takes as input a set of operations and empties the operation forest the conflict operation set and non conflict operation set. In step the system enumerates all the input actions. For each operation the procedure in step first checks whether the operation is conflicted with an operation that already exists in the operation forest . If yes in step the pair of conflicted operations is added to the conflict operation set. If not in step the system checks whether the input operation is subsumed by an operation that is already in the operation forest . If yes in step the system adds a link from the subsuming node i.e. operation to the input node i.e. operation . If not in step the system adds to the operation forest the input operation as the root of a new operation tree. After all the input operations are enumerated in step the system adds the root node of all operation trees in the operation forest to the non conflict operation set. The system returns the conflict operation set and non conflict operation set in step and ends in step .

Others approaches to achieve the above functionality are known to those skilled in the art and are within the scope of our disclosure.

The optimization engine is implemented by one or more servers and is configured to determine an optimal execution sequence for the conflict free actions. The optimization engine receives the conflict free actions from the fusion engine and determines an optimal execution sequence of these actions such that the overhead incurred to the network operation is minimized.

Referring to the flowchart of the optimization process performed by the optimization engine is composed of the following steps. The process begins at step and in step all the actions received from the fusion engine are sorted in the order of their deadlines t. At step time is divided into epochs each of which contains a number of deadlines. The division can follow a uniform even division or a probabilistic distribution. In step within each epoch actions are grouped into action sets. Actions belonging to the same action set requires access to the same configuration file device or other identities. In step all action sets are examined and any conflicts inconsistencies are resolved using similar techniques as those used in the fusion engine as illustrated in . After the optimization process is completed the process ends at step and the action sets are fed into the actuator from the optimization engine for further processing.

The actuator implemented by one or more servers is a communication module of the management platform and is configured to send the action sets to the network manager following a predefined communication protocol. Then based on the received commands the network manager either responds to queries of network status i.e. latency routing bandwidth or the like or issues reconfiguration commands i.e. redistributing wavelength assignment changing optical circuits turning on off mechanical switches or the like to the optical switching units . In case of status queries the actuator propagates the response generated by the network manager up to the querying application. In case of reconfiguration commands the optical switching units reconfigures based on the received commands. The network manager is responsible for ensuring minimum service disruption during the reconfiguration.

According to the preferred embodiments the intermediate network management platform for software defined optical networks resides between the high level applications and the low level network manager. From the perspectives of the applications the management platform is a unified interface for managing the underlying optical switching fabric . This way the application developers do not need to understand the detailed hardware configurations and operations thereby significantly lowering the development barrier. On the other hand from the perspective of the network manager the management platform is a single application with consolidated and optimized queries and action sequences. Thus the network manager does not need to coordinates actions issued by different applications which significantly reduces the complexity and workload of the network manager.

Embodiments of the present invention relate generally to computer network switch design and network management. More particularly the present invention relates to scalable and self optimizing optical circuit switching networks and methods for managing such networks.

Inside traditional data centers network load has evolved from local traffic i.e. intra rack or intra subnet communications into global traffic i.e. all to all communications . Global traffic requires high network throughput between any pair of servers. The conventional over subscribed tree like architectures of data center networks provide abundant network bandwidth to the local areas of the hierarchical tree but provide scarce bandwidth to the remote areas. For this reason such conventional architectures are unsuitable for the characteristics of today s global data center network traffic.

Various next generation data center network switching fabric and server interconnect architectures have been proposed to address the issue of global traffic. One such proposed architecture is a completely flat network architecture in which all to all non blocking communication is achieved. That is all servers can communicate with all the other servers at the line speed at the same time. Representatives of this design paradigm are the Clos network based architectures such as FatTree and VL2. These systems use highly redundant switches and cables to achieve high network throughput. However these designs have several key limitations. First the redundant switches and cables significantly increase the cost for building the network architecture. Second the complicated interconnections lead to high cabling complexity making such designs infeasible in practice. Third the achieved all time all to all non blocking network communication is not necessary in practical settings where high throughput communications are required only during certain periods of time and are constrained to a subset of servers which may change over time.

A second such proposed architecture attempts to address these limitations by constructing an over subscribed network with on demand high throughput paths to resolve network congestion and hotspots. Specifically c Through and Helios design hybrid electrical and optical network architectures where the electrical part is responsible for maintaining connectivity between all servers and delivering traffic for low bandwidth flows and the optical part provides on demand high bandwidth links for server pairs with heavy network traffic. Another proposal called Flyways is very similar to c Through and Helios except that it replaces the optical links with wireless connections. These proposals suffer from similar drawbacks.

Compared to these architectures a newly proposed system called OSA pursues an all optical design and employs optical switching and optical wavelength division multiplexing technologies. However the optical switching matrix or Microelectromechanical systems MEMS component in OSA significantly increases the cost of the proposed architecture and more importantly limits the applicability of OSA to only small or medium sized data centers.

Accordingly it is desirable to provide a high dimensional optical circuit switching fabric with wavelength division multiplexing and wavelength switching and routing technologies that is suitable for all sizes of data centers and that reduces the cost and improves the scalability and reliability of the system. It is further desirable to control the optical circuit switching fabric to support high performance interconnection of a large number of network nodes or servers.

In one embodiment an optical switching system is described. The system includes a plurality of interconnected wavelength selective switching units. Each of the wavelength selective switching units is associated with one or more server racks. The interconnected wavelength selective switching units are arranged into a fixed structure high dimensional interconnect architecture comprising a plurality of fixed and structured optical links. The optical links are arranged in a k ary n cube ring mesh torus direct binary n cube indirect binary n cube Omega network or hypercube architecture.

In another embodiment a broadcast select optical switching unit is described. The optical switching unit includes a multiplexer an optical power splitter a wavelength selective switch and a demultiplexer. The multiplexer has a plurality of first input ports. The multiplexer is configured to combine a plurality of signals in different wavelengths from the plurality of first input ports into a first signal output on a first optical link. The optical power splitter has a plurality of first output ports. The optical power splitter is configured to receive the first signal from the first optical link and to duplicate the first signal into a plurality of duplicate first signals on the plurality of first output ports. The duplicated first signal is transmitted to one or more second optical switching units. The wavelength selective switch has a plurality of second input ports. The wavelength selective switch is configured to receive one or more duplicated second signals from one or more third optical switching units and to output a third signal on a second optical link. The one or more duplicated second signals are generated by second optical power splitters of the one or more third optical switching units. The demultiplexer has a plurality of second output ports. Each second output port has a distinct wavelength. The demultiplexer is configured to receive the third signal from the second optical link and to separate the third signal into the plurality of second output ports.

An optical switching fabric comprising a plurality of optical switching units. The plurality of optical switching units are arranged into a fixed structure high dimensional interconnect architecture. Each optical switching unit includes a multiplexer a wavelength selective switch an optical power combiner and a demultiplexer. The multiplexer has a plurality of first input ports. The multiplexer is configured to combine a plurality of signals in different wavelengths from the plurality of first input ports into a first signal output on a first optical link. The wavelength selective switch has a plurality of first output ports. The wavelength selective switch is configured to receive the first signal from the first optical link and to divide the first signal into a plurality of second signals. Each second signal has a distinct wavelength. The plurality of second signals are output on the plurality of first output ports. The plurality of second signals are transmitted to one or more second optical switching units. The optical power combiner has a plurality of second input ports. The optical power combiner is configured to receive one or more third signals having distinct wavelengths from one or more third optical switching units and to output a fourth signal on a second optical link. The fourth signal is a combination of the received one or more third signals. The demultiplexer has a plurality of second output ports. Each second output port has a distinct wavelength. The demultiplexer is configured to receive the fourth signal from the second optical link and to separate the fourth signal into the plurality of second output ports based on their distinct wavelengths.

Certain terminology is used in the following description for convenience only and is not limiting. The words right left lower and upper designate directions in the drawings to which reference is made. The terminology includes the above listed words derivatives thereof and words of similar import. Additionally the words a and an as used in the claims and in the corresponding portions of the specification mean at least one. 

The present invention will be described in detail with reference to the drawings. The figures and examples below are not meant to limit the scope of the present invention to a single embodiment but other embodiments are possible by way of interchange of some or all of the described or illustrated elements. Moreover where some of the elements of the present invention can be partially or fully implemented using known components only portions of such known components that are necessary for an understanding of the present invention will be described and a detailed description of other portions of such known components will be omitted so as not to obscure the invention.

Referring to the drawings in detail wherein like reference numerals indicate like elements throughout is a system diagram which illustrates the typical components of a data center in accordance with the present invention. The most basic elements of a data center are servers a plurality of which may be arranged into server racks . Each server rack is equipped with a top of rack switch ToR . All of the ToRs are further interconnected with one or multiple layers of cluster e.g. aggregation and core switches such that every server in the data center can communicate with any one of the other servers . The present invention is directed to the network switching fabric interconnecting all ToRs in the data center .

Referring to a high dimensional optical switching fabric for use with the data center of is shown. The switching fabric includes a plurality of wavelength selective switching units interconnected using a high dimensional data center architecture . The high dimensional data center architecture is achieved by coupling multiple wavelength selective switching units with fixed and structured fiber links to form a high dimensional interconnection architecture. Each wavelength selective switching unit is associated with and communicatively coupled to a server rack through a ToR . The high dimensional data center architecture preferably employs a generalized k ary n cube architecture where k is the radix and n is the dimension of the graph. The design of the wavelength selective switching units and the associated procedures of the network manager are not limited to k ary n cube architectures. Other architectures that are isomorphic to k ary n cubes including rings meshes tori direct or indirect binary n cubes Omega network hypercubes etc may also be implemented in the high dimensional data center architecture and are within the scope of this disclosure.

The k ary n cube architecture is denoted by Cnk where n is the dimension and vector k denotes the number of elements in each dimension. Referring to examples of a 4 ary 2 cube i.e. k and n 2 and 3 4 2 ary 3 cube i.e. k and n 3 respectively are shown. Each node in represents a server rack including a ToR and its corresponding wavelength selective switching unit . Other examples of architectures are not shown for sake of brevity but those skilled in the art will understand that such alternative architectures are within the scope of this disclosure.

Two designs of the wavelength selective switching unit of are described with reference to and prior art . The designs of vary based on whether the underlying communication mechanism is broadcast and select or point to point. Furthermore a broadcast and select based wavelength selective switching unit may be symmetric or asymmetric depending on the requirements and constraints of practical settings.

A symmetric architecture of a broadcast and select based wavelength selective switching unit connected to ToR and servers is shown in . Each electrical ToR has 2m downstream ports. Downstream ports usually have lower line speed and are conventionally used to connect to the servers . The higher speed upstream ports are described with respect to the asymmetric architecture below.

In the symmetric wavelength selective switching unit of half of the 2m downstream ports of electrical ToR are connected to rack servers and the other half are connected to m optical transceivers at different wavelengths 1 2 . . . m. In typical applications the optical transceivers have small form factors such as the SFP Small Form Factor Pluggable type optical transceivers at different wavelengths following typical wavelength division multiplexing WDM grids. Each optical transceiver typically consisting of a SFP type optical module sitting on a media converter not shown has one electrical signal connecting port such as an electrical Ethernet port one optical transmitting port and one optical receiving port. The bit rate of the optical transceivers at least matches or is higher than that of the Ethernet port . For instance if the Ethernet port supports 1 Gb s signal transmission the bit rate of each optical transceiver can be 1 Gb s or 2.5 Gb s if the Ethernet port is 10 Gb s the bit rate of each optical transceiver is preferably 10 Gb s as well. This configuration assures non blocking communication between the servers residing in the same server rack and the servers residing in all other server racks .

Logically above the ToR is a broadcast and select type design for the wavelength selective switching units . The wavelength selective switching units are further interconnected via fixed and structured fiber links to support a larger number of server inter communications. Each wavelength selective switching unit includes an optical signal multiplexing unit MUX an optical signal demultiplexing unit DEMUX each with m ports a 1 2n optical wavelength selective switch WSS a 1 2n optical power splitter PS and 2n optical circulators c . The optical MUX combines the optical signals at different wavelengths for transmission in a single fiber. Typically two types of optical MUX devices can be used. In a first type of optical MUX each of the input ports does not correspond to any specific wavelength while in the second type of optical MUX each of the input ports corresponds to a specific wavelength. The optical DEMUX splits the multiple optical signals in different wavelengths in the same fiber into different output ports. Preferably each of the output ports corresponds to a specific wavelength. The optical PS splits the optical signals in a single fiber into multiple fibers. The output ports of the optical PS do not have optical wavelength selectivity. The WSS can be dynamically configured to decide the wavelength selectivity of each of the multiple input ports. As for the optical circulators the optical signals arriving via port a come out at port b and optical signals arriving via port b come out at port c . The optical circulators are used to support bidirectional optical communications in a single fiber. However in other embodiments optical circulators are not required and may be replaced with two fibers instead of a single fiber.

In the wavelength selective switching unit of the optical transmitting port of the transceiver is connected to the input port of the optical MUX . The optical MUX combines m optical signals from m optical transceivers into a single fiber forming WDM optical signals. The output of optical MUX is connected to the optical PS . The optical PS splits the optical signals into 2n output ports. Each of the output ports of the optical PS has the same type of optical signals as the input to the optical PS . Therefore the m transmitting signals are broadcast to all of the output ports of the optical PS . Each of the output ports of optical PS is connected to port a of an optical circulator and the transmitting signal passes port a and exits at port b of optical circulator .

In the receiving part of the wavelength selective switching unit optical signals are received from other wavelength selective switching units . The optical signals arrive at port b of optical circulators and leave at port c . Port c of each optical circulator is coupled with one of the 2n ports of WSS . Through dynamic configuration of the WSS with the algorithms described below selected channels at different wavelengths from different server racks can pass the WSS and be further demultiplexed by the optical DEMUX . Preferably each of the output ports of optical DEMUX corresponds to a specific wavelength that is different from other ports. Each of the m output ports of the optical DEMUX is preferably connected with the receiving port of the optical transceiver at the corresponding wavelength.

Inter rack communication is conducted using broadcast and select communication wherein each of the outgoing fibers from the optical PS carries all the m wavelengths i.e. all outgoing traffic of the rack . At the receiving end the WSS decides what wavelengths of which port are to be admitted and then forwards them to the output port of the WSS and the output of the WSS that is connected to the optical DEMUX . The optical DEMUX separates the WDM optical signals into the individual output port which is connected to the receiving port of the optical transceivers . Each ToR combined with one wavelength selective switching unit described above constitutes a node in . All of the nodes are interconnected following a high dimensional architecture . All the wavelength selective switching units are further controlled by a centralized or distributed network manager . The network manager continuously monitors the network situation of the data center determines bandwidth demand of each flow and adaptively reconfigures the network to improve the network throughput and resolve hot spots. These functionalities are realized through a plurality of procedures described in further detail below.

The asymmetric architecture broadcast select architecture achieves 100 switch port utilization but at the expense of lower bisection bandwidth. The asymmetric architecture is therefore more suitable than the symmetric architecture for scenarios where server density is of major concern. In an asymmetric architecture the inter rack connection topology is the same as that of the symmetric counterpart. The key difference is that the number of the ports of a ToR that are connected to servers is greater than the number of the ports of the same ToR that are connected to the wavelength selective switching unit . More specifically each electrical ToR has m downstream ports all of which are connected to servers in a server rack . Each ToR also has u upstream ports which are equipped with u small form factor optical transceivers at different wavelength 1 2 . . . u. In a typical 48 port GigE switch with four 10 GigE upstream ports for instance we have 2 m 48 and u 4.

Logically above the ToR is the wavelength selective switching unit which consists of a multiplexer and a demultipexer each with u ports a 1 2n WSS and a 1 2n power splitter PS . The transmitting ports and receiving ports of the optical transceivers are connected to the corresponding port of optical multiplexer and demultiplexer respectively. The output of optical multiplexer is connected to the input of optical PS and the input of the optical demultiplexer is connected to the output of the WSS . Each input port of the WSS is connected directly or through an optical circulator to an output port of PS of the wavelength selective switching unit in another rack via an optical fiber. Again the optical circulator may be replaced by two fibers.

In practice it is possible that the ports which are originally dedicated for downstream communications connected with servers can be connected to the wavelength selective switching unit together with the upstream ports. In this case the optical transceivers may carry a different bit rate depending on the link capacity of the ports they are connected to. Consequently the corresponding control software will also need to consider the bit rate heterogeneity while provisioning network bandwidth as discussed further below.

In both the symmetric and asymmetric architectures a network manager optimizes network traffic flows using a plurality of procedures. These procedures will now be described in further detail.

The first procedure estimates the network bandwidth demand of each flow. Multiple options exist for performing this estimation. One option is to run on each server a software agent that monitors the sending rates of all flows originated from the local server . Such information from all servers in a data center can be further aggregated and the server to server traffic demand can be inferred by the network manager . A second option for estimating network demand is to mirror the network traffic at the ToRs using switched port analyzer SPAN ports. After collecting the traffic data network traffic demand can be similarly inferred as in the first option. The third option is to estimate the network demand by emulating the additive increase and multiplicative decrease AIMD behavior of TCP and dynamically inferring the traffic demand without actually capturing the network packets. Based on the deployment scenario a network administrator can choose the most efficient mechanism from these or other known options.

In the second procedure routing is allocated in a greedy fashion based on the following steps as shown in the flow chart of . The process begins at step and proceeds to step where the network manager identifies the source and destination of all flows and estimates the network bandwidth demand of all flows. At step all flows are sorted in a descending order of the network bandwidth demand of each flow. At step it is checked whether all of the flows have been allocated a path. If all flows have been allocated a path the procedure terminates in step . Otherwise the network manager identifies the flow with the highest bandwidth demand in step and allocates the most direct path to the flow in step . If multiple equivalent direct paths of a given flow exist in step the network manager chooses the path that balances the network load. The network manager then checks whether the capacities of all links in the selected path are exceeded in step . Link capacity is preferably decided by the receivers instead of the senders which broadcast all the m wavelengths to all the 2n direct neighbors.

If the capacity of at least one of the links in the selected path is exceeded the network manager goes back to step and picks the next most direct path and repeats steps and . Otherwise the network manager goes to step to pick the flow with the second highest bandwidth demand and repeats steps through .

In a physical network each server rack is connected to another server rack by a single optical fiber. But logically the link is directed. From the perspective of each server all the optical links connecting other optical switching modules in both the ingress and egress directions carry all the m wavelengths. But since these m wavelengths will be selected by the WSS at the receiving end these links can logically be represented by the set of wavelengths to be admitted.

The logical graph of a 4 ary 2 cube cluster is illustrated in . Each directed link in the graph represents the unidirectional transmission of the optical signal. For ease of illustration the nodes are indexed from 1 to k in each dimension. For instance the i th element in column j is denoted by i j . All nodes in i j i 1 3 . . . k 1 j 2 4 . . . k and all nodes in i j i 2 4 . . . k j 1 3 . . . k 1 are shown in WHITE and all the remaining nodes are shaded. As long as k is even such a perfect shading always exists.

Next all the WHITE nodes are placed on top and all GREY nodes are placed on the bottom and a bipartite graph is obtained as shown in . In the graph of all directed communications are between WHITE and GREY colored nodes and no communications occur within nodes of the same color. This graph property forms the foundation of the key mechanisms of the present system including routing and bandwidth provisioning.

In this procedure the network manager provisions the network bandwidth based on the traffic demand obtained from Procedure 1 and or Procedure 2 and then allocates wavelengths to be admitted at different receiving WSSs based on the following steps as shown in the flowchart of . The process begins at step and proceeds to step where the network manager estimates the bandwidth demand of each optical link based on the bandwidth demand of each flow. In step the network manager determines for each link the number of wavelengths necessary to satisfy the bandwidth demand for that link. In step the network manager allocates a corresponding number of wavelengths to each link such that there is no overlap between the sets of wavelengths allocated to all the input optical links connected to the same wavelength selective switch .

In step since at the WSS the same wavelength carried by multiple optical links cannot be admitted simultaneously i.e. the wavelength contention problem the network manager needs to ensure that for each receiving node there is no overlap of wavelength assignment across the 2n input ports. Thereafter the process ends at step .

Procedure 3 does not consider the impact of changes of wavelength assignment which may disrupt network connectivity and lead to application performance degradation. Thus in practice it is desirable that only a minimum number of wavelength changes are performed to satisfy the bandwidth demands. Therefore it is desirable to maximize the overlap between the old wavelength assignment old and the new assignment new. The classic Hungarian method can be adopted as a heuristic to achieve this goal. The Hungarian method is a combinatorial optimization algorithm to solve assignment problems in polynomial time. This procedure is described with reference to the flow chart of . The process begins at step and proceeds to step at which the network manager first identifies the old wavelength assignment A1 A2 . . . A2n where Ai denotes the set of wavelengths assigned to link i and wavelength distribution i.e. the number of wavelength required for each link under the new traffic matrix. At step the network manager finds a new wavelength assignment A 1 A 2 . . . A 2n that satisfies the wavelength distribution and has as much overlap with old as possible. In step the network manager constructs a cost matrix M whose each element mij is equal to the number of common wavelengths between sets Ai and A j. Finally in step the network manager generates a new wavelength assignment matrix R where r 0 1 

The fifth procedure achieves highly fault tolerant routing. Given the n dimensional architecture there are 2n node disjoint parallel paths between any two ToRs . Upon detecting a failure event the associated ToRs notifies the network manager immediately and the network manager informs all the remaining ToRs . Each ToR receiving the failure message can easily check which paths and corresponding destinations are affected and detour the packets via the rest of the paths to the appropriate destinations. Applying this procedure allows the performance of the whole system to degrade very gracefully even in the presence of a large percentage of failed network nodes and or links.

In the broadcast and select based design each of the 2n egress links of a ToR carries all the m wavelengths. It is left up to the receiving WSS to decide what wavelengths to admit. Thus multicast anycast or broadcast can be efficiently realized by configuring the WSSs in a way that the same wavelength of the same ToR is simultaneously admitted by multiple ToRs . The network manager needs to employ methods similar to the IP based counterparts to maintain the group membership for the multicast anycast or broadcast.

In the symmetric architecture described so far the number of the ports of a ToR switch that are connected to servers equals the number of the ports of the same ToR that are connected to the wavelength selective switching unit . This architecture achieves high bisection bandwidth between servers residing in the same server rack with the rest of the network at the expense of only 50 switch port utilization.

The architecture of the wavelength selective switching unit used for point to point communication is described in U.S. Patent Application Publication Nos. 2012 0008944 to Ankit Singla and 2012 0099863 to Lei Xu the entire disclosures of both of which are incorporated by reference herein. In the present invention these point to point based wavelength selective switching units are arranged into the high dimensional interconnect architecture in a fixed structure. In the wavelength selective switching unit as illustrated with reference to each electrical ToR has 2m ports half of which are connected to rack servers and the other half are connected with m wavelength division multiplexing small form factor pluggable WDM SFP transceivers .

Logically above the ToR are the wavelength selective switching units which are further interconnected to support a larger number of inter communications between servers . Each wavelength selective switching unit includes optical MUX and DEMUX each with m ports a 1 2n optical wavelength selective switch WSS a 1 2n optical power combiner PC and 2n optical circulators . In operation the optical PC combines optical signals from multiple fibers into a single fiber. The WSS can be dynamically configured to decide how to allocate the optical signals at different wavelengths in the single input port into one of the different output ports. The optical circulators are used to support bi directional optical communications using a single fiber. Again the optical circulators are not required as two fibers can be used to achieve the same function.

Similar to the broadcast and select based system described earlier all the wavelength selective switching units are interconnected using a high dimensional architecture and are controlled by the network manager . The network manager dynamically controls the optical switch fabric following the procedures below.

Procedures 1 2 5 and 6 are the same as the corresponding procedures discussed above with respect to the broadcast and select based system.

The third procedure of the point to point architecture is described with reference to wherein N G is the maximum node degree of a bipartite graph G. Each node of G represents a wavelength selective switching unit . The procedure begins at step and proceeds to step where the network manager first constructs a N G regular i.e. each node in the graph G has exactly degree of N G multi graph where multiple links connecting two nodes is allowed by adding wavelength links each representing a distinct wavelength to each node of G. Next in step the network manager identifies all sets of links such that within each set there are no two links sharing a common node and the links in the same set covers all nodes in the graph G. In step the network manager assigns a distinct wavelength to all links in the same set by configuring the wavelength selective switch . The process then ends at step .

This procedure is similar to Procedure 4 in the broadcast and select based system finding a minimum set of wavelengths while satisfying the bandwidth demands. This procedure first finds a new wavelength assignment new which has a large wavelength overlap with the old assignment old. Then uses new as the initial state and uses an adapted Hungarian method to fine tune new to further increase the overlap between new and old.

In the present invention all of the wavelength selective switching units are interconnected using a fixed specially designed high dimensional architecture. Ideal scalability intelligent network control high routing flexibility and excellent fault tolerance are all embedded and efficiently realized in the disclosed fixed high dimensional architecture. Thus network downtime and application performance degradation due to the long switching delay of an optical switching matrix are overcome in the present invention.

It will be appreciated by those skilled in the art that changes could be made to the embodiments described above without departing from the broad inventive concept thereof. It is understood therefore that this invention is not limited to the particular embodiments disclosed but it is intended to cover modifications within the spirit and scope of the present invention as defined by the appended claims.

