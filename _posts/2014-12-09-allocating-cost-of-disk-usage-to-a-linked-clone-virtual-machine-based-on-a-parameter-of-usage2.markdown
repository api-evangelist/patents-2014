---

title: Allocating cost of disk usage to a linked clone virtual machine based on a parameter of usage
abstract: The present disclosure is related to methods, systems, and machine-readable media for allocating cost of disk usage to a linked clone virtual machine (VM) based on a parameter of usage. A determination can be made as to a number of disks used by a linked clone VM among a plurality of disks in a software defined data center over a time period and as to a respective portion of a parameter of usage for each of the number of disks used by the linked clone VM over the time period that is attributable to the linked clone VM. A portion of a cost for usage of each of the number of disks over the time period can be allocated to the linked clone VM in proportion to the respective portion of the parameter of usage attributable to the linked clone VM and/or on a relative latency.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09582309&OS=09582309&RS=09582309
owner: VMware, Inc.
number: 09582309
owner_city: Palo Alto
owner_country: US
publication_date: 20141209
---
A data center is a facility that houses servers data storage devices and or other associated components such as backup power supplies redundant data communications connections environmental controls such as air conditioning and or fire suppression and or various security systems. A data center may be maintained by an information technology IT service provider. An enterprise may purchase data storage and or data processing services from the provider in order to run applications that handle the enterprises core business and operational data. The applications may be proprietary and used exclusively by the enterprise or made available through a network for anyone to access and use.

Virtual machines VMs have been introduced to lower data center capital investment in facilities and operational expenses and reduce energy consumption. A VM is a software implementation of a computer that executes application software analogously to a physical computer. VMs have the advantage of not being bound to physical resources which allows VMs to be moved around and scaled to meet changing demands of an enterprise without affecting the use of the enterprise s applications.

In a software defined data center storage resources may be allocated to VMs in various ways such as through network attached storage NAS a storage area network SAN such as fiber channel and or Internet small computer system interface iSCSI and or raw device mappings among others. VMs may run a finite set of operating systems along with some applications. Installing operating systems and applications may be time consuming. Accordingly a virtualization technology called clones may be used to reduce the time in setup and to reuse already setup images.

The term virtual machine VM refers generally to an isolated user space instance which can be executed within a virtualized environment. Other technologies aside from hardware virtualization can provide isolated user space instances also referred to as data compute nodes. Data compute nodes may include non virtualized physical hosts VMs containers that run on top of a host operating system without a hypervisor or separate operating system and or hypervisor kernel network interface modules among others. Hypervisor kernel network interface modules are non VM data compute nodes that include a network stack with a hypervisor kernel network interface and receive transmit threads.

VMs in some embodiments operate with their own guest operating systems on a host using resources of the host virtualized by virtualization software e.g. a hypervisor virtual machine monitor etc. . The tenant i.e. the owner of the VM can choose which applications to operate on top of the guest operating system. Some containers on the other hand are constructs that run on top of a host operating system without the need for a hypervisor or separate guest operating system. The host operating system can use name spaces to isolate the containers from each other and therefore can provide operating system level segregation of the different groups of applications that operate within different containers. This segregation is akin to the VM segregation that may be offered in hypervisor virtualized environments that virtualize system hardware and thus can be viewed as a form of virtualization that isolates different groups of applications that operate in different containers. Such containers may be more lightweight than VMs.

While the specification refers generally to VMs the examples given could be any type of data compute node including physical hosts VMs non VM containers and hypervisor kernel network interface modules. Embodiments of the present disclosure can include combinations of different types of data compute nodes.

Examples of cloned VMs include full clones and linked clones. A full clone is an independent copy of a VM that shares nothing from the parent VM. Thus for example any changes to the parent VM made after the full clone is created will not be propagated to the full clone. A linked clone is a copy of a VM that shares disks with the parent VM. Thus any changes made to the shared disks for the parent VM will be propagated to the linked clone VM. In some instances linked clone VMs may share some but not all disks. For example a parent VM may have a shared disk with a linked clone VM and the parent VM may also have its own delta disk that is not shared with the linked clone VM so that changes for the parent VM can be made in the delta disk and not propagated to the linked clone VM while changes that are intended to affect both the parent VM and the linked clone VM can be made in the shared disk. As used herein with respect to VMs a disk is a representation of memory resources e.g. memory resources illustrated in that are used by a VM. As used herein memory resource includes primary storage e.g. cache memory registers and or main memory such as random access memory RAM and secondary or other storage e.g. mass storage such as hard drives solid state drives removable media etc. which may include non volatile memory . The term disk does not imply a single physical memory device. Rather disk implies a portion of memory resources that are being used by a VM regardless of how many physical devices provide the memory resources. Because operating systems and general applications may take up most of the space on the VM disks which can be duplicated across many VMs linked clones can help to improve this situation. In addition to saving disk space linked clones can also help in fast provisioning.

Linked clones can be created using the concept of VM snapshots. A VM snapshot can preserve the state of a VM so that it can be reverted to at a later point in time. The snapshot can include memory as well. In some embodiments a snapshot includes secondary storage while primary storage is optionally included with the snapshot. A linked clone VM can be constructed from a snapshot. A snapshot hierarchy and or linked clone hierarchy can be represented as a tree of disks. The leaves of the tree can represent currently active delta disks. A delta disk can be a disk that stores changes from a parent disk e.g. without storing an entire copy of the parent disk . Some embodiments of the present disclosure can accurately allocate storage costs for a VM that is using linked clones and or snapshots a linked clone VM using the linked clone hierarchy.

The tree of disks representing the linked clone hierarchy can become complex as the linked clone levels increase. At a given point in time there could be multiple VMs sharing the same linked clone in multiple levels. This can complicate the costing of storage consumption per VM in a software defined data center at a given point in time. The complexity of the environment can multiply further when the cost of storage is to be allocated over a time period e.g. where changes to the hierarchy may be made over the time period . In order to allocate costs to a VM accurately a determination can be made as to the amount of resources consumed by the VM.

According to a number of embodiments of the present disclosure costs can be allocated to a linked clone VM based on dynamic parameters. A dynamic parameter is a parameter that changes during runtime whereas a static parameter is a parameter that does not usually change during runtime. Some examples of static parameters include a size of a disk being used a number of VMs using the disk and or a clone count for the disk among others. A parameter being static does not mean that the parameter is fixed or cannot change as for example the number of VMs using a disk can change and a use of the disk a size of the disk can change. Again the term disk represents the memory resources used by a VM and is more indicative of an amount of memory used rather than an indication of one or more physical memory devices. However such changes may occur outside of runtime that is outside of a time where operations are being performed on the disk by a VM. Some examples of dynamic parameters include a number of operations performed on a disk over time and or a latency for operations performed on the disk. For example a proportionate cost of the usage of a disk can be attributed to a linked clone VM that uses the disk based on operations performed on the disk by the linked clone VM and or a proportionate cost of the usage of a disk can be attributed to a linked clone VM that uses the disk based on operations performed on the disk by the linked clone VM in consideration of the operational latency. Some advantages of using dynamic parameters can include higher accuracy than using static parameters because allocation can be based on actual runtime data about the a VM s disk usage. Some hypervisors support storage input output I O control such that performance can be controlled balanced and or optimized across multiple hosts that host multiple VMs. For example if two VMs VM 1 and VM 2 are sharing a disk and the disk is accessed more by VM 1 than VM 2 over a time period then more of the cost of the disk can be allocated to VM 1 than VM 2.

The present disclosure is not limited to particular devices or methods which may vary. The terminology used herein is for the purpose of describing particular embodiments and is not intended to be limiting. As used herein the singular forms a an and the include singular and plural referents unless the content clearly dictates otherwise. Furthermore the words can and may are used throughout this application in a permissive sense i.e. having the potential to being able to not in a mandatory sense i.e. must . The term include and derivations thereof mean including but not limited to. 

The figures herein follow a numbering convention in which the first digit or digits correspond to the drawing figure number and the remaining digits identify an element or component in the drawing. Similar elements or components between different figures may be identified by the use of similar digits. For example may reference element 14 in and a similar element may be referenced as in . As will be appreciated elements shown in the various embodiments herein can be added exchanged and or eliminated so as to provide a number of additional embodiments of the present disclosure. In addition as will be appreciated the proportion and the relative scale of the elements provided in the figures are intended to illustrate certain embodiments of the present invention and should not be taken in a limiting sense.

The host can incorporate a hypervisor that can execute a number of virtual machines . . . N referred to generally herein as VMs . The VMs can be provisioned with processing resources and or memory resources and can communicate via the network interface . The processing resources and the memory resources provisioned to the VMs can be local and or remote to the host . For example in a software defined data center the VMs can be provisioned with resources that are generally available to the software defined data center and not tied to any particular hardware device. By way of example the memory resources can include volatile and or non volatile memory available to the VMs . The VMs can be moved to different hosts not specifically illustrated such that a different hypervisor manages the VMs . The host can be in communication with a VM cost allocation system . An example of the VM cost allocation system is illustrated and described in more detail with respect to . In some embodiments the cost allocation system can be a server such as a web server.

The number of engines can include a combination of hardware and program instructions that is configured to perform a number of functions described herein. The program instructions e.g. software firmware etc. can be stored in a memory resource e.g. machine readable medium as well as hard wired program e.g. logic . Hard wired program instructions e.g. logic can be considered as both program instructions and hardware.

The disk usage engine can include a combination of hardware and program instructions that is configured to determine for a linked clone VM a number of disks used by the linked clone VM among a plurality of disks in a software defined data center over a period of time. The disk usage engine can be configured to determine a respective portion of a parameter of usage for each of the number of disks used by the linked clone VM over the time period that is attributable to the linked clone VM. In some embodiments the disk usage engine can be configured to poll a hypervisor of the linked clone VM to determine the portion of the parameter of usage e.g. input output operations IOPs that is attributable to the linked clone VM. The disk usage engine can be configured to determine for each of the plurality of disks in the software defined data center a respective total plurality of IOPs over the time period. For example the IOPs can be read IOPS.

In some embodiments the disk usage engine can be configured to build a linked clone tree for a number of VMs that use the plurality of disks in the software defined data center during the time period. Each node in the tree represents one of the plurality of disks in the software defined data center that is used by at least one of the number of VMs. Specifically while disks can be physical or virtual a node represents one virtual disk which can be a portion of a physical disk a whole physical disk multiple physical disks or combinations thereof. Examples of linked clone trees are illustrated and described in more detail with respect to . The disk usage engine can be configured for a leaf in the tree corresponding to the linked clone VM to determine a path from a root of the tree. The path can include a number of nodes representing the number of virtual disks used by the linked clone VM.

The cost engine can include a combination of hardware and program instructions that is configured to allocate to the linked clone VM a portion of a cost for usage of each of the number of disks over the time period in proportion to the respective portion of the parameter of usage attributable to the linked clone VM. In some embodiments the cost engine can be configured to allocate the portion of the cost based on a unit rate per unit time. The unit rate per unit time can be common to two or more of the disks and or specific to a particular disk. The size of a particular disk is an indication of how much memory the disk uses. In some embodiments the cost engine can be configured to allocate to the linked clone VM the portion of the cost regardless of relative latency of the linked clone VM to latency of other linked clone VMs that use the disks. In some embodiments relative latency can be a factor in the allocation of costs as described in more detail herein.

Memory resources can be non transitory and can include volatile and or non volatile memory. Volatile memory can include memory that depends upon power to store information such as various types of dynamic random access memory DRAM among others. Non volatile memory can include memory that does not depend upon power to store information. Examples of non volatile memory can include solid state media such as flash memory electrically erasable programmable read only memory EEPROM phase change random access memory PCRAM magnetic memory optical memory and or a solid state drive SSD etc. as well as other types of machine readable media.

The processing resources can be coupled to the memory resources via a communication path . The communication path can be local or remote to the machine . Examples of a local communication path can include an electronic bus internal to a machine where the memory resources are in communication with the processing resources via the electronic bus. Examples of such electronic buses can include Industry Standard Architecture ISA Peripheral Component Interconnect PCI Advanced Technology Attachment ATA Small Computer System Interface SCSI Universal Serial Bus USB among other types of electronic buses and variants thereof. The communication path can be such that the memory resources are remote from the processing resources such as in a network connection between the memory resources and the processing resources . That is the communication path can be a network connection. Examples of such a network connection can include a local area network LAN wide area network WAN personal area network PAN and the Internet among others.

As shown in the MRI stored in the memory resources can be segmented into a number of modules that when executed by the processing resources can perform a number of functions. As used herein a module includes a set of instructions included to perform a particular task or action. The number of modules can be sub modules of other modules. For example the cost module can be a sub module of the disk usage module and or can be contained within a single module. Furthermore the number of modules can comprise individual modules separate and distinct from one another. Examples are not limited to the specific modules illustrated in .

Each of the number of modules can include program instructions and or a combination of hardware and program instructions that when executed by a processing resource can function as a corresponding engine as described with respect to . For example the disk usage module can include program instructions and or a combination of hardware and program instructions that when executed by a processing resource can function as the disk usage engine and or the cost module can include program instructions and or a combination of hardware and program instructions that when executed by a processing resource can function as the cost engine .

The machine can include a disk usage module which can include instructions to determine for each of a number of disks used by a linked clone VM among a plurality of disks in a software defined data center a relative usage attributable to the linked clone VM over a time period in consideration of a relative latency for the linked clone VM over the time period versus other linked clone VMs that use the number of disks. The instructions to determine the relative usage in consideration of the relative latency can include instructions to determine a latency factor for the linked clone VM. A latency factor is described in more detail with respect to . The latency factor can be based on a latency for the linked clone VM and average latency for the linked clone VM and the other linked clone VMs and a weight of latency on cost. The latency for the linked clone VM and or the average latency for the linked clone VM and the other linked clone VMs can be measured during runtime. The weight of the latency on cost can be received as an input.

In some embodiments the disk usage module can include instructions to determine the disks used by the linked cone VM over the time period. For example such instructions can include instructions to determine a path from a root of a linked clone tree for the linked clone VM. The linked clone tree as described herein can represent linked clone VMs that use the disks during the time period. Each node in the tree can represent one virtual disk and the path can include nodes representing the virtual disks used by the linked clone VM.

The machine can include a cost module which can include instructions to allocate to the linked clone VM a cost for usage of each of the number of disks over the time period based on the relative usage and relative latency. The disk usage module can include instructions to determine the relative usage and the relative latency with instructions to determine a first value that is a number of IOPs attributable to the linked clone VM over a time period in consideration of a latency factor for the linked clone VM over the time period. The disk usage module can further include instructions to determine a second value that is a sum of a number of IOPs attributable to each other linked clone VM that uses the number of disks over the time period in consideration of a latency factor for each other linked clone VM. In some embodiments the cost module can include instructions to allocate to the linked clone VM the cost for the usage of each of the number of disks over the time period in proportion to a ratio of the first value to the second value.

From the snapshot of VM two linked clone VMs VM and VM can be created with delta disks D VM and D VM respectively. Thus the linked clone VMs VM and VM are linked clones of VM in its state as recorded on disk D. This is illustrated in the linked clone hierarchy by the lines connecting disk D to disks D and D respectively. Thus any changes to VM made after disk D was locked and delta disk D was created would not be reflected in the linked clones VM and VM because such changes would be stored in delta disk D and the linked clones VM and VM were snapshots of VM according to disk D rather than delta disk D.

An application can be installed in VM in delta disk D. This is a change to VM e.g. installing a new application . As described above such a change to VM would not reach the linked clones VM and VM because they were cloned from VM in its state recorded in disk D. After the application is installed a snapshot can be taken on VM which locked disk D where the application was installed and created a new delta disk D e.g. where any further changes for VM can be stored .

From the snapshot of VM at disk D a linked clone VM can be created with delta disk D. VM can write some data into delta disk D. Subsequently a snapshot can be taken on VM which can lock delta disk D to create delta disk D. Thus any further changes for VM can be reflected in delta disk D. A linked clone VM VM can be created from VM at disk D with delta disk D for VM. A VM can make changes to its disk at any time and may not lock the disk unless a snapshot is going to be taken for example to preserve a particular state of the VM and or to create a linked clone VM. The snapshot allows the VM to have a new delta disk for further changes and allows the new linked clone VM to have its own delta disk based on the previous state of the VM from which it was cloned. Thus for example a snapshot can be taken on VM which can lock disk D and create delta disk D.

In the linked clone hierarchy A illustrated in various states of VM are represented by disks D D and D as illustrated by dotted line . Various states of VM are represented by disks D and D as illustrated by dotted line . A state of VM is represented by disk D as illustrated by dotted line . Various states of VM are represented by disks D and D as illustrated by dotted line . A state of VM is represented by disk D as illustrated by dotted line .

The linked clone hierarchy A is presented as a linked clone tree. Each virtual disk is presented as a node. A node is a parent node if it has a child node. A child node is a node that depends from a parent node as represented by a line coming from a first node with an arrow toward a second node. Child nodes are generally presented below parent nodes. A parent node that has no parent node of its own is a root node e.g. disk D . A child node with no child node of its own is a leaf node e.g. disks D D D D and D . A leaf node in the linked clone tree represents a current state of a particular VM. Thus for example leaf node disk D represents a current state of VM leaf node disk D represents a current state of VM leaf node disk D represents a current state of VM leaf node disk D represents a current state of VM and leaf node disk D represents a current state of VM. The linked clone tree illustrated in represents the linked clone hierarchy A at a first time t .

Each VM can also have a diskchain which is a path from the root node to that VM s leaf node representing its current state. A diskchain for VM is D D D. A diskchain for VM is D D D. A diskchain for VMS is D D D D. A diskchain for VM is D D D D. A diskchain for VM is D D. An example of executable instructions to determine a diskchain for VM is Path root VM . The diskchains for each VM over a particular time period can be superimposed to create a linked clone hierarchy such as linked clone hierarchy A.

In the linked clone hierarchy B illustrated in various states of VM are represented by disks D D and D as illustrated by dotted line . Various states of VM are represented by disks D and D as illustrated by dotted line . A state of VM is represented by disk D as illustrated by dotted line . Leaf node disk D represents a current state of VM leaf node disk D represents a current state of VM and leaf node disk D represents a current state of VM.

In the linked clone hierarchy C illustrated in various states of VM are represented by disks D D and D as illustrated by dotted line . Various states of VM are represented by disks D and D as illustrated by dotted line . A state of VM is represented by disk D as illustrated by dotted line . A state of VM is represented by disk D as illustrated by dotted line . Leaf node disk D represents a current state of VM leaf node disk D represents a current state of VM leaf node disk D represents a current state of VM and leaf node disk D represents a current state of VM.

In the linked clone hierarchy D illustrated in various states of VM are represented by disks D D and D as illustrated by dotted line . Various states of VM are represented by disks D and D as illustrated by dotted line . A state of VM is represented by disk D as illustrated by dotted line . Various states of VM are represented by disks D and D as illustrated by dotted line . A state of VM is represented by disk D as illustrated by dotted line . A state of VM is represented by disk D as illustrated by dotted line .

An example of executable instructions to provide diskchains which can be used in creating a linked clone hierarchy is 

An example of executable instructions to provide a linked clone hierarchy such as is illustrated in is 

Allocating costs based on IOPs can be a good metric because it can allocate more cost to a VM that is using a disk more than another VM. As described herein runtime details about the IOPs can be provided by a hypervisor for the VM e.g. hypervisor illustrated in .

The graph provides an example of usage of a disk in terms of IOPs for VM as indicated by the solid line at and a trend of the usage by VM as indicated by the dashed line at . The usage is illustrated over time from t t . Examples of the specific data indicating usage in terms of IOPs for VM as well as VM and VM for comparison purposes is illustrated in .

According to a number of embodiments of the present disclosure a portion of a cost for usage of the disk over a time period can be allocated in proportion to the respective portion of the parameter of usage attributable to the linked clone VM for example as opposed to allocating the total cost to each of the VMs. As an example assume that a cost for usage of the disk according to the parameter of usage e.g. IOPs is 1. For time period t the total IOPs are 130 as illustrated and as indicated in the chart by the sum of 35 10 85 . Thus the total cost of the disk for time period t is 130. An approach that allocated a total cost to each VM using the disk would allocate the full 130 to each of VMS VM and VM. However according to some embodiments of the present disclosure the cost can be allocated in proportion to the usage by each VM e.g. VM can have a cost of 35 VM can have a cost of 10 and VMS can have a cost of 85 allocated thereto . As is also illustrated by the graph the total usage and or cost of a disk can vary with time. For example the total usage of the disk changes from 130 over time period t to over time period t.

In order to allocate costs according to the present disclosure a total parameter of usage e.g. IOPs can be calculated for a time period for the VMs that use a disk. The cost can be allocated as a ratio of the parameter of usage for a particular VM to the total parameter of usage of the disk. For example the cost for a VM can be calculated according to 

At a method can include determining a number of disks used by a linked clone VM over a time period. From this point a number of options are available. For example at a method can include determining a respective portion of a parameter of usage for each of the disks used by the linked clone VM over the time period that is attributable to the linked clone VM. Then at a method can include allocating a portion of a cost for usage of the disks over the time period in proportion to the respective portion of the parameter of usage attributable to the linked clone VM.

In contrast at a method can include determining for each of the disks a relative usage attributable to the linked clone VM over the time period in consideration of a relative latency for the linked clone VM over the time period versus other linked clone VMs that use the disks. Then at a method can include allocating a cost for usage of the disks over the time period to the linked clone VM based on the relative usage and the relative latency.

In some embodiments a method for allocating costs to linked clone VMs can include determining a number of disks among a plurality of disks in a software defined data center from which snapshots were taken to form a linked clone virtual machine VM . In some embodiments determining the number of disks includes adding a delta disk for the linked clone VM to the number of disks. A method can include determining a respective portion of a plurality of IOPs for the number of disks over a time period that are attributable to the linked clone VM. A method can include allocating to the linked clone VM a portion of a cost for usage of each of the number of disks over the time period in proportion to the respective portion of the plurality of IOPs attributable to the linked clone VM. In some embodiments a method can include determining the respective portion of the plurality of IOPs over the time period in consideration of a relative latency for the linked clone VM over the time period versus other linked clone VMs that use the number of disks and allocating the portion of the cost in proportion to the plurality of IOPs attributable to the linked clone VM in consideration of the relative latency for the linked clone VM.

In some embodiments a proportionate cost of usage of a disk can be allocated to a linked clone VM based on a parameter of usage e.g. IOPs on the disk in consideration of the relative latency of the parameter of usage for the linked clone VM. Consider for example two linked clone VMs that make the same number of IOPs on a disk over a time period. If the latency of the IOPs is different for the different VMs then the different VMs are not receiving the same quality of service. Examples of cause for such a difference can include different storage I O control parameters e.g. different tiers of service different network latencies etc. In some embodiments a greater cost of usage can be allocated for a lower latency for a same parameter of usage between two different VMs over a same time period. For example the cost for a VM can be calculated according to 

Although specific embodiments have been described above these embodiments are not intended to limit the scope of the present disclosure even where only a single embodiment is described with respect to a particular feature. Examples of features provided in the disclosure are intended to be illustrative rather than restrictive unless stated otherwise. The above description is intended to cover such alternatives modifications and equivalents as would be apparent to a person skilled in the art having the benefit of this disclosure.

The scope of the present disclosure includes any feature or combination of features disclosed herein either explicitly or implicitly or any generalization thereof whether or not it mitigates any or all of the problems addressed herein. Various advantages of the present disclosure have been described herein but embodiments may provide some all or none of such advantages or may provide other advantages.

In the foregoing Detailed Description some features are grouped together in a single embodiment for the purpose of streamlining the disclosure. This method of disclosure is not to be interpreted as reflecting an intention that the disclosed embodiments of the present disclosure have to use more features than are expressly recited in each claim. Rather as the following claims reflect inventive subject matter lies in less than all features of a single disclosed embodiment. Thus the following claims are hereby incorporated into the Detailed Description with each claim standing on its own as a separate embodiment.

