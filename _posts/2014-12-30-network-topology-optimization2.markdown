---

title: Network topology optimization
abstract: In some examples, a controller for a multi-layer network comprising a network layer and an underlying transport layer is configured to obtain abstract link data describing a plurality of candidate links; determine, based at least on the abstract link data, a first solution comprising a network topology for the network layer that includes a first selected subset of the candidate links; determine, after generating a modified network topology based at least on the network topology and the abstract link data, a second solution comprising the modified network topology for the network layer that includes a second selected subset of the candidate links; and output, for configuring the multi-layer network, topology data for one of the first solution or the second solution having a lowest total cost, the lowest total cost including a total resource cost to the network for the one of the first solution or the second solution.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09602387&OS=09602387&RS=09602387
owner: Juniper Networks, Inc.
number: 09602387
owner_city: Sunnyvale
owner_country: US
publication_date: 20141230
---
This application is a continuation in part of application Ser. No. 14 585 170 filed Dec. 29 2014 which is incorporated herein by reference in its entirety.

The invention relates to computer networks and more specifically to determining a computer network topology.

Routing devices within a network often referred to as routers maintain tables of routing information that describe available routes through the network. Network routers maintain routing information that describes available routes through the network. Upon receiving a packet a router examines information within the packet and forwards the packet in accordance with the routing information. In order to maintain an accurate representation of the network routers exchange routing information in accordance with one or more routing protocols such as an interior gateway protocol IGP or Border Gateway Protocol BGP .

The term link is often used to refer to the connection between two devices on a network. The link may be a physical connection such as a copper wire a coaxial cable any of a host of different fiber optic lines or a wireless connection. In addition network devices may define virtual or logical links and map the virtual links to the physical links. In other words the use of virtual links provides a degree of abstraction. As networks grow in size and complexity the traffic on any given link may approach a maximum bandwidth capacity for the link thereby leading to congestion and loss.

In general techniques are described for dynamically determining a logical network topology for efficiently transporting network traffic over a physical topology based on end to end network traffic demands. The techniques may be applicable in a multi layer network having a base transport layer and a logical or overlay Internet Protocol IP layer routed on the transport layer to meet network traffic demands placed upon the multi layer network.

In one example a controller for a multi layer network determines a logical network topology for transporting a traffic demand matrix the logical network topology is determined to ensure sufficient capacity in the event of a failure of any base layer component and to facilitate an optimized total resource cost to the network for transporting the traffic. The controller obtains abstract link data describing a set of candidate links available for use as links in the network topology. The controller may also obtain abstract link data describing the shared risks encountered by these candidate links on their physical transport paths as well as information relevant to path optimization such as the physical length or delay of the link in some cases. The controller iteratively analyzes candidate links and abstract link data in view of the traffic demand matrix to select a subset of the candidate links to efficiently and robustly carry the demands. As part of its design output the controller may signal to the network or the network operator the information required to configure and activate any of these selected subset of candidate links that are not already activated and configured.

The techniques may provide one or more advantages. For example a controller that applies the above described techniques may facilitate with each iteration movement toward global optimization along the total cost of solutions gradient for a traffic demand matrix with respect to a total cost to the network. In some examples the controller applying the techniques described herein may further facilitate that the selected subset of candidate links is able to carry the demands under any single element failure while also satisfying other constraints applied to the network such as delays and the number of next hops for a given path through the network. While the globally optimal solution may not be reached in all cases the techniques may avoid at least some local minima on the total cost of solutions gradient which may result in robust yet lower resource cost solutions.

In one example a method comprises obtaining by a controller of a multi layer network comprising a network layer and an underlying transport layer abstract link data describing a plurality of candidate links wherein each candidate link of the plurality of candidate links is routed in the transport layer and usable in network topologies for the network layer determining by the controller based at least on the abstract link data a first solution comprising a network topology for the network layer that includes a first selected subset of the candidate links determining by the controller after generating a modified network topology based at least on the network topology and the abstract link data a second solution comprising the modified network topology for the network layer that includes a second selected subset of the candidate links and outputting by the controller for configuring the multi layer network topology data for one of the first solution or the second solution having a lowest total cost the lowest total cost including a total resource cost to the network for the one of the first solution or the second solution.

In another example a controller for a multi layer network comprising a network layer and an underlying transport layer the controller comprising one or more processors coupled to a memory a topology computation module configured for execution by the one or more processors to obtain abstract link data describing a plurality of candidate links wherein each candidate link of the plurality of candidate links is routed in the transport layer and usable in network topologies for the network layer determine based at least on the abstract link data a first solution comprising a network topology for the network layer that includes a first selected subset of the candidate links and determine after generating a modified network topology based at least on the network topology and the abstract link data a second solution comprising the modified network topology for the network layer that includes a second selected subset of the candidate links and a topology provisioning module configured for execution by the one or more processors to output for configuring the multi layer network topology data for one of the first solution or the second solution having a lowest total cost the lowest total cost including a total resource cost to the network for the one of the first solution or the second solution.

In another example a non transitory computer readable medium contains instructions for causing one or more programmable processors of a controller of a multi layer network comprising a network layer and an underlying transport layer obtain abstract link data describing a plurality of candidate links wherein each candidate link of the plurality of candidate links is routed in the transport layer and usable in network topologies for the network layer determine based at least on the abstract link data a first solution comprising a network topology for the network layer that includes a first selected subset of the candidate links determine after generating a modified network topology based at least on the network topology and the abstract link data a second solution comprising the modified network topology for the network layer that includes a second selected subset of the candidate links and output for configuring the multi layer network topology data for one of the first solution or the second solution having a lowest total cost the lowest total cost including a total resource cost to the network for the one of the first solution or the second solution.

The details of one or more embodiments of the invention are set forth in the accompanying drawings and the description below. Other features objects and advantages of the invention will be apparent from the description and drawings and from the claims.

Underlying transport network transports multiplexes and switches packet based communications through high speed optical fiber links. Transport network may include multiple optical communication devices e.g. packet optical transport devices interconnected via optical links and controlling transmission of optical signals carrying packet data along the optical links. In this way transport network provides a physical layer that physically interconnects routers of network .

Although not shown in for simplicity packet optical transport devices may be for example ROADMs PCXs wavelength division multiplexing WDM dense WDM DWDM and time division multiplexing TDM based devices optical cross connects OXCs optical add drop multiplexers OADMs multiplexing devices or other types of devices or other devices that transmit switch and or multiplex optical signals. As one example routers may be layer three L3 routers optically connected by intermediate OXCs of transport network such as OXCs to which the routers have access links.

Transport network typically includes a number of other components such as amplifiers transponders OTTs repeaters and other equipment for controlling transmission of optical packet data along optical links also not shown . Large optical transport systems may have significant numbers of such devices that influence optical transmissions. Although described with respect to only optical links transport system may include other types of physical links as well such as Ethernet PHY Synchronous Optical Networking SONET Synchronous Digital Hierarchy SDH Lambda or other Layer 2 data links that include packet transport capability.

Routers are members of a path computation domain served by controller . The path computation domain may include for example an Interior Gateway Protocol e.g. Open Shortest Path First OSPF or Intermediate System to Intermediate System IS IS area an Autonomous System AS multiple ASes within a service provider network multiple ASes that span multiple service provider networks. In various examples different combinations of routers may include member routers of multiple ASes. Network links connecting routers may thus be interior links inter AS transport links another type of network link or some combination thereof.

Logical network is in effect an overlay network built on top of underlying transport network . Routers are connected by virtual or logical links an example topology for which is illustrated in with links A I collectively links each of which corresponds to a path in the underlying transport network . Each path may include one or more physical links of the transport network again such physical links not shown in .

In some example implementations controller provides integrated control over both routers and packet optical transport devices underlying transport network with respect to transport of packet data through the optical links and other equipment. For example controller may not only control routing and traffic engineering operations of network but may also provide integrated control over allocation or utilization of the optical spectrum and wavelengths utilized by each packet optical transport device within transport network that underlies the elements of network or controller may use the path or abstract link information from the transport layer to select candidate links for routing on the transport network .

Controller may represent a high level controller for configuring and managing network . Controller may represent one or more general purpose servers an appliance controller or other special purpose device for computing paths an application executed by a computing device a distributed control plane of routers that computes paths for LSPs managed by the routers and so forth. In some cases aspects of controller may be distributed among one or more real or virtual computing devices. Any such devices listed above may be in network or out of network with regard to network . Example details of a software defined networking SDN controller for a software defined network which may perform operations described herein to compute paths and route LSPs are described in PCT International Patent Application PCT US2013 044378 filed Jun. 5 2013 and entitled PHYSICAL PATH DETERMINATION FOR VIRTUAL NETWORK PACKET FLOWS which is incorporated by reference herein in its entirety. Additional examples details of an SDN controller for a software defined network to obtain topology information for and to provision a network are described in U.S. patent application Ser. No. 14 042 614 filed Sep. 30 2013 and entitled SOFTWARE DEFINED NETWORK CONTROLLER and U.S. patent application Ser. No. 14 500 736 filed Sep. 29 2014 and entitled BATCHED PATH COMPUTATION IN RESOURCE CONSTRAINED NETWORKS which are both incorporated by reference herein in their entireties.

Controller may obtain traffic engineering information for network by executing one or more network routing protocols extended to carry traffic engineering information to listen for routing protocol advertisements that carry such traffic engineering information. Traffic engineering information may include node and interface identifiers for routers administrative weights and available bandwidth per priority level for links LSP identifier and state information for virtual links and other information for computing paths for traffic engineered LSPs. Controller may store traffic engineering information to a traffic engineering database TED .

Controller in this example presents northbound interface that may be invoked by other controllers in a hierarchical arrangement of controllers or by an orchestrator administrator application or other entity to present traffic demands for network . Interface may be usable for integration with an orchestration system such as OpenStack interface may also or alternatively usable by other applications or the operator s Operations Support Systems OSS Business Support Systems BSS . Interface may in some cases present a RESTful Application Programming Interface API .

A traffic demand corresponds to an end to end traffic flow traversing network from one of routers at the network edge to another of routers at the network edge. In the illustrated example routers A D E and F are logically located at the network edge and thus ingress and or egress traffic flows for transport across network .

The traffic demand may be defined according to an expected traffic bandwidth that is to be routed or re routed by the network from a source node to a destination node. In some cases the traffic demand may be associated with timing calendaring information that defines an interval during the expected traffic bandwidth will be received by network for transport. A traffic flow corresponds to one or more network packets that each matches a set of one or more properties. Different packet flows may be classified using different properties and property values. For example some packet flows may be identified as matching a standard 5 tuple or subset thereof consisting of transport layer protocol e.g. User Datagram Protocol UDP or Transmission Control Protocol TCP source IP address destination IP address source port destination port. Packet flows may also be distinguishable from one another by application protocol e.g. LDP ARP OSPF BGP etc. and or MPLS labels for example.

Controller may in some cases determine traffic demands based on current traffic demands being experienced by network in which case controller may apply the techniques described herein in near real time to modify a network topology to potentially improve the traffic routing. In some cases controller may receive via interface or estimate projected demands based on patterns of demands previously experienced by network upcoming application activation or other known and or expected changes to the traffic demand patterns such as changing the peering point for the entry to the network of data from a major customer adding a new node or point of presence merging two or more networks and so forth. For example controller or another controller may analyze traffic LSP statistics and trends to project future traffic demands on network . These may be useful for long term planning for network .

The various traffic demands form a traffic demand matrix of traffic demands from the various possible source ingress routers to the various possible destination egress routers . In accordance with techniques described herein controller includes topology computation module that determines a topology of network links for network by which routers may switch network traffic flows in order to meet the traffic demands corresponding to the traffic flows .

Topology computation module may determine the logical network topology for network to ensure sufficient capacity in the event of a failure of components of transport network and to facilitate an optimized total resource cost to the network for transporting the traffic. Topology computation module obtains a set of candidate links available for use as network links in network . Topology computation module additionally in some instances obtains abstract link data describing the shared risks encountered by these candidate links on their physical transport paths. In some cases abstract link data also defines the available candidate links and may define additional abstract links already configured and activated in network . Abstract link data is in other words and in such cases the mechanism by which topology computation module obtains the set of candidate links. Abstract link data may further include information relevant to path optimization such as the physical length or delay of the link in some cases.

Abstract link data in this way represents data leaked in some manner from the transport network to controller to enable the application of further constraints by topology computation module to the determination of paths and corresponding candidate links on which to route traffic. Such constraints may correspond to the types of abstract link data which may include available candidate links link lengths link metrics which may be based on link lengths link costs which may also be based on link lengths and a list of Shared Risk Link Groups SRLGs for links.

Topology computation module may obtain abstract link data in some cases by building additional candidate links for the controller to use if required and if the use of such links would result in a lower cost overall solution based on user defined or application defined rules set in data files and configured in controller for controlling transport network . In other words controller may build candidate links from candidate link definitions obtained by controller . For example a user or application may define groups of packet optical transport devices as types of nodes within transport network e.g. access nodes core nodes and supercore nodes and may indicate the circumstances in which the packet optical transport devices allow connections within a group or between groups. For instance the rules may specify 

The above rules are merely examples. The defined rules may also define the administrative weighting scheme usable by the software to transport the traffic if a candidate link is used to transport traffic. With regard to the above the defined rules determine only candidate links and do not specify that such links must be used to transporting traffic. After applying techniques described herein to determine paths for links for a network topology controller may configure only the selected subset of the available links indicated in the candidate links for use in transporting traffic. In addition controller may be unable to add links to a given solution if such links are not in the collection generated in the candidate link sets. By contrast topology computation module may use links already defined for network even if such links are not in the candidate link set. In other words topology computation module may be unable to use links in determining a solution unless the links are defined either in the candidate link set or already exist in the network .

Controller may route the available candidate links in transport network to determine their actual physical lengths and the shared risks SRLGs for instance in the link paths in the transport network . Paths for such links may be pre computed prior to choosing the candidate links as described in further detail below. Because the shortest path for a transport route may be excessively restrictive for purposes of failure resiliency e.g. to protect against a failure of an SRLG from a transport link topology computation module may determine multiple paths for a given candidate link from which topology computation module may choose. In some examples the multiple paths for a given link may include the shortest path the shorter of two diverse paths and the longer of two diverse paths. While the shortest path and the shorter of two diverse paths may be identical this need not necessarily be the case. Controller may determine the diverse paths using a strong diverse path algorithm suitable for finding shortest diverse cycle paths and taking account of SRLGs if available. In some cases a logical link such as any of links may already be configured in network i.e. exist and the path for the logical link in transport layer may be obtained by controller . In such cases the known path can be fixed and the diverse paths describe above may not be determined or utilized by topology computation module . Additional details regarding example techniques for computing diverse paths are described in David Wood Ping Wang U.S. patent application Ser. No. 14 585 170 filed Dec. 29 2014 and entitled POINT TO MULTIPOINT PATH COMPUTATION FOR WIDE AREA NETWORK OPTIMIZATION which is incorporated by reference herein in its entirety.

Because in some cases paths for links chosen from the candidate set or pre computed prior to iteratively determining solutions for traffic demand matrix topology computation module may in some embodiments avoid attempting to design the paths for the links in such a manner as to account for available wavelengths in the transport network elements. This in effect allows topology computation module to assume the optical e.g. WDM capacity does not limit the determination of solutions for the demands.

Upon selecting a candidate link controller may map the routed path for the candidate link to SRLG information for transport link sections or nodes having the SRLG information and underlying the path i.e. transporting traffic in the transport network to effectuate the path . Controller may in some cases prune the set of candidate links based on the number of routers the links bypass in the transport network which may allow the candidate link set to be reduced on the basis of the transport link topology and the equipment rather than merely on the basis of length. This may further enable realistic modelling of core IP networks made up of high speed routers having only direct lambda optical connections or a restricted set of connections that are limited to bypass only a small number of nodes. Thus all IP Layer links whose routes bypass these high speed routers may be pruned from the candidate set of links.

As noted above topology computation module may obtain abstract link data from an abstract link file or other data structure obtained from e.g. a third party network management system for transport network or built by a user. Topology computation module may in this way obtain an abstract picture of the transport layer represented here by transport network but remain unaware of the details of the transport network topology. In other words topology computation module may have a restricted or merely abstract picture of the transport network taken via abstract link data from a transport network controller or derived from an export of a transport network network management system NMS for instance. Obtaining abstract link data in this way may be advantageous for the rules defining whether candidate links or are not available depend upon the particularities of the various packet optical transport devices employed in transport network . Obtaining candidate links directly as a set of abstract links from an abstract link file may enable more complex constraints on the connections than are possible using relatively simple formulae for candidate link generation as described above.

As noted above abstract link data may include link information for available candidate links such as link lengths link metrics link costs and or a list of Shared Risk Link Groups SRLGs for links. In order to perform designs that take into account potential failure modes in the transport network of fiber cuts or WDM optical component failures as well as failures of devices and interfaces in the IP layer the topology computation module may account for the transport layer equipment used by the IP links by applying penalties to links according SRLGs. Controller may for instance obtain the SRLG information for transport layer elements corresponding to candidate links identified in abstract link data . Such SRLG information could be for fiber sections conduit ducts carrying the fibers transport layer switching elements and so forth. Controller may obtain such SRLG information for any existing links in network . Controller may obtain such SRLG information for existing links to understand the failure modes of the network modified to include candidate links described in abstract link data and selected by topology computation module during an iteration for determining solutions for the traffic demand matrix according to techniques described herein.

With regard to the above resiliency mechanisms need to rely on predicting which resources have a high likelihood to fail contemporaneously in order to correctly assign redundant routes. In a simple network a node or a link between nodes may fail due to a local failure. However in a packet optical network a single fiber cut of a DWDM link would affect all wavelengths transported. Moreover each individual wavelength may connect different pairs of routers such that a single fiber cut in the optical network appears to be a triple or quadruple failure in the network topology as may occur when there are more than two layers e.g. IP over SDH over WDM SDH over WDM representing transport network in this example .

To cope with such situations an SRLG or a set of SRLGs is assigned as a link attribute. An SRLG may be for instance represented by a 32 bit number unique within an IGP OSPFv2 and IS IS domain such as network or an IGP within network where network encompasses multiple domains. A link may be assigned multiple SRLGs. The SRLG of a path in a label switched path LSP is the set of SRLGs for all the links in the path. Topology computation module may use SRLG information provided in abstract link data when determining paths for candidate links. In general when computing diverse paths for a candidate link it is preferable to find paths such that the primary and secondary paths do not have any links in common in case the SRLGs for the primary and secondary paths are disjoint. This ensures that a single point of failure on a particular link does not bring down both the primary and secondary paths. By comparing the SRLG attributes of links a topology computation module can apply penalties during an iteration to facilitate disjoint SRLG attributes between the sets of links for the primary path and the secondary path and in this way arrive at diverse failure routes.

As a prerequisite SRLGs of the optical domain represented by transport network must be leaked into the packet domain represented by network . SRLGs may thus enable synchronizing routing decisions between layers of multi layer network . Moreover the nature of SRLG information is layer independent and can therefore be used as common reference information for routing at any layer.

In addition to or alternatively to representing shared risks for the abstract links abstract link data may indicate resource constraints for a given set of the abstract links or an SRLG that contains a set of the abstract links A resource constraint for the set of abstract links may specify that only a specified number of abstract links from the set may be selected for use in a network topology for network . For example a particular fiber section in transport network may have a limit of 40 wavelengths for carrying network links activated on the fiber section. By specifying a resource constraint of 40 e.g. on a particular set of candidate links that traverse the fiber section the abstract link data may ensure that only a maximum of 40 of the particular set of candidate links may be selected by topology computation module for activation in the network .

Topology computation module iteratively analyzes candidate links and abstract link data in view of the traffic demand matrix to select a subset of the candidate links to efficiently and robustly carry the demands. In response to topology computation module determining a logical network topology made up of the selected subset of candidate links the topology provisioning module may signal to transport network or to the network operator determined network topology information required to route the selected subset of candidate links as demands in the transport layer represented by transport network . Network topology information may include the selected subset of the candidate links. The selected subset of the candidate links may be expressed in network topology information as a set of demands for the transport network .

By applying techniques described in this disclosure controller may facilitate global optimization with respect to a total cost to the network of the network topology for the satisfaction of a traffic demand matrix. In some examples controller may further facilitate a configuration of the network able to carry the traffic demand matrix under any single element failure while also satisfying other constraints applied to the network such as delays the number of next hops the total resources defined as available in specific places on the network etc. for a given path through the network or applying penalties to the overall cost if the intermediate solution does not meet these requested constraints. While the globally optimal solution may not be reached in all cases the techniques may avoid at least some local minima on the total cost of solutions gradient which may result in robust yet lower resource cost solutions.

Controller includes a control unit coupled to a network interface to exchange packets with other network devices by one or more inbound links and one or more outbound links . Main memory of control unit represents one or more computer readable storage media which may include random access memory RAM such as various forms of dynamic RAM DRAM e.g. DDR2 SDRAM or static RAM SRAM Flash memory or any other form of fixed or removable storage medium that can be used to carry or store desired program code and program data in the form of instructions or data structures and that can be accessed by a computer. Main memory provides a physical address space composed of addressable memory locations accessible by modules .

Main memory is coupled to disk which may comprise computer readable storage media that includes volatile and or non volatile removable and or non removable media implemented in any method or technology for storage of information such as processor readable instructions data structures program modules or other data. Computer readable storage media includes but is not limited to random access memory RAM read only memory ROM EEPROM flash memory CD ROM digital versatile discs DVD or other optical storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium that can be used to store data and instructions.

Control unit in this example includes multi core computing platform to execute modules . Multi core computing platform includes multiple processing cores that each includes an independent execution unit to perform instructions that conform to an instruction set architecture for the core. Cores of multi core computing platform may each be implemented as separate integrated circuits ICs or may be combined within one or more multi core processors or many core processors that are each implemented using a single IC i.e. a chip multiprocessor .

Multi core computing platform executes software instructions such as those used to define a software or computer program stored to main memory . Alternatively or additionally control unit may comprise dedicated hardware such as one or more integrated circuits one or more Application Specific Integrated Circuits ASICs one or more Application Specific Special Processors ASSPs one or more Field Programmable Gate Arrays FPGAs or any combination of one or more of the foregoing examples of dedicated hardware for performing the techniques described herein.

Control unit provides an operating environment for network services applications and topology element . In some examples these modules may be implemented as one or more processes executing on one or more virtual machines of one or more servers. That is while generally illustrated and described as executing on a single controller aspects of modules may be executed on other computing devices or on different virtual machines of one or more computing devices.

Network services applications represent one or more processes that provide services to clients of a service provider network that includes network and controller to manage connectivity in the path computation domain. Network services applications may provide for instance include movie television or other media content distribution Voice over IP VoIP Video on Demand VOD bulk transport walled open garden IP Mobility Subsystem IMS and other mobility services and Internet services to clients of a service provider network controlled at least in part by controller . Networks services applications may issue demands to topology element to request transport services of network . One or more of network services applications may include or otherwise make use of a client interface by which one or more client applications request transport services. Client interface may represent a command line interface CLI or graphical user interface GUI for instance. Client may also or alternatively provide an application programming interface API such as a web service to client applications.

Network services applications may issue demands to topology element to request respective paths in a path computation domain controlled by controller from sources to destinations. For example a demand may include a required bandwidth or other constraint and two endpoints representing a source and a destination that communicate over the path computation domain managed by controller . Control unit stores demands as a list of demands in the demands data structure demands . In some cases the service provider or other administrator of network may configure via an administrative interface one or more demands . In some cases topology element may additionally or alternatively derive projected demands based on patterns of demands previously experienced by network .

Topology element accepts demands to route traffic between the endpoints for the demands over the path computation domain. Demands may be requested for different times and dates and with disparate bandwidth requirements. Topology element may reconcile demands from network services applications to multiplex requested paths for the corresponding traffic onto the network path computation domain based on demand parameters and network resource availability.

To intelligently compute a topology for network layer 6 topology element may in some cases include topology module to receive traffic engineering information such as traffic engineering information of describing available resources of network including routers and interfaces thereof and interconnecting network links. Topology module may execute one or more southbound protocols such as Open Shortest Path First with Traffic Engineering extensions OSPF TE Intermediate System to Intermediate System with Traffic Engineering extensions ISIS TE BGP Link State BGP LS to learn traffic engineering information for network .

Traffic engineering database TED stores traffic engineering information received by topology module for network that constitutes a path computation domain for controller . TED may include one or more link state databases LSDBs where link and node data is received in routing protocol advertisements received from a topology server and or discovered by link layer entities such as an overlay controller and then provided to topology module . In some instances the service provider or other administrative entity may configure traffic engineering or other topology information within TED via an administrative interface.

In accordance with techniques described in this disclosure and to satisfy demands for network topology computation module of topology element determines and optimizes paths for demands . Topology computation module applies techniques described in this disclosure to iteratively determine solutions for demands to facilitate a globally optimized total resource cost to the network and underlying transport network for transporting the traffic. Topology computation module may represent an example instance of topology computation module of .

To that end topology computation module obtains abstract link data describing candidate links for network and the shared risks encountered by these candidate links on their physical transport paths as well as information relevant to path optimization such as the physical length or delay of the link in some cases. Topology computation module iteratively analyzes candidate links and other abstract link data in view of demands to select a subset of the candidate links to efficiently and robustly carry traffic corresponding to demands .

Topology computation module having selected and routed a subset of the candidate links for network topology provisioning module attempts to set the routed paths for the candidate links onto network . Topology provisioning module of controller may program the paths into network to cause the state of network to match the state of network as determined by topology computation module . Topology provisioning module may represent an example of topology provisioning module of . Provisioning a path may require path validation prior to committing the path to provide for packet transport. Topology provisioning module executes one or more southbound protocols for path provisioning to inject state into elements of network such as any one or more of routers . A southbound protocol refers to a protocol by which components of controller may communicate with network elements such as routers to obtain or inject topology information forwarding and other network information that determines the operation of the network . For example southbound protocols may include Path Computation Element PCE Communication Protocol PCEP Open Shortest Path First with Traffic Engineering extensions OSPF TE Intermediate System to Intermediate System with Traffic Engineering extensions ISIS TE BGP Link State BGP LS NETCONF Yang Interface to the Routing System I2RS protocols CLIs for the network elements Simple Network Management Protocol SNMP and OpenFlow or other SDN configuration protocol .

Topology module may receive updated traffic engineering information from network which may trigger dynamic re computation by topology computation module of a topology for network . For example TED upon receiving a new or updated TE link or receiving information that a TE link has failed may trigger topology computation module to re compute the paths which may include respective diverse paths for candidate links in order to account for the TED change.

Topology computation module may additionally alternatively dynamically re compute an updated network topology on receipt on new or updated abstract link data . For example updated abstract link data may indicate a new SRLG for a link which may indicate previously diverse paths candidate now have a common SRLG and are thus no longer diverse with respect to SRLG. Topology computation module may as a result re compute diverse paths for the candidate link in order to again obtain diversity for the candidate link.

Topology computation module obtains live topology information for network including routers that in this example cooperatively implement an IP routing plane . Topology computation module also obtains candidate links for network the candidate links available for use in optimizing the logical network topology . The determined solution typically does not use all candidate links obtained and controller applies operation to determine the subset of candidate links to use to build the lowest cost network topology. Topology computation module may obtain candidate links by building the candidate links based on user defined rules configured in or otherwise received by controller . Topology computation module may route these newly built links in transport network to determine their actual physical lengths and the shared risks SRLGs that the newly built links encounter in their paths in the transport layer. In some cases these paths are pre computed when the calculation starts. To compute the paths topology computation module may calculate three paths and the optimisation algorithm is free to choose between these 

Often the first two paths of these paths are identical but this is not necessarily the case. Topology computation module may apply a strong diverse path algorithm that works well to find shortest diverse cycle paths in complicated networks taking account of SRLG information if available. If a logical link already exists and its path in the transport network is known then this can be read into topology computation module and the known route can be fixed and the diverse path set described above is not determined. In some cases because these paths are all pre calculated before the applying operation topology computation module may not attempt to design the paths taking into account available wavelengths in the transport network . Topology computation module instead assumes in such cases that the WDM capacity does not limit the design. Alternatively the computation module may have information on the WDM resource constraints e.g. obtained from abstract link data and apply penalties to a solution if the solution does not meet these constraints. Once the path is selected topology computation module maps these paths to SRLG information for the IP links carried over each transport network link section or node. Related to the transport paths topology computation module may in some instances have a user configurable parameter for pruning the set of candidate links based on the number of IP nodes the links bypass in the transport layer. This allows the candidate link set to be reduced on the basis of the transport link topology and the equipment passed rather than on the basis of distance for example.

Topology computation module may alternatively receive abstract link data that includes information describing candidate links. In some cases abstract link data is a file built as a data extraction from a third party network management system for the transport layer. In some cases a network operator for network may build such a file by applying a script to or otherwise manipulating available transport layer data. Obtaining candidate link information directly in this way from abstract link data e.g. provides only an abstract or restricted description of transport network that does not include details of the transport layer topology. As a result topology computation module may apply more complicated constraints for determining selected candidate links. For example a network operator may specify maximum counts maximum delay maximum capacity on any group of links or SRLGs or any combination thereof . Topology computation module may apply such constraints to topology determination for network where such constraints are soft constraints in that solutions that violate the requirements of the constraints are not forbidden but rather receive a penalty cost that is added to the total network cost topology computation module iteratively applies steps of operation to determine solutions that reduce or bring to zero these penalties .

Information describing the candidate links may include available links and associated link metrics link costs and or SRLGs on the link. The combination of live topology information for network and the obtained candidate links define a network topology model for network . Topology computation module routes the traffic demands for network on the network topology model . Example detailed operations for routing traffic demands are included below with respect to .

Topology computation module then performs failure simulations with respect to the solution represented by the current network topology model including the current subset of candidate links over which topology computation module has routed any of the traffic demands . The failure simulations determine penalties to be applied to the solution if for instance traffic cannot be protected certain failure resistance constraints are not met or fixed equipment is required to exceed its constrained capacity in order to carry the traffic. Example details of a failure simulation are provided below with respect to .

Topology computation module determines a resource cost to the network for the solution and the penalties for the solution in addition to those determined during the failure simulation . To determine the resource costs for the purpose of optimization topology computation module determines a total resource cost of all the equipment in multi layer network . Such costs may be based at least in part on link capacities or dimensions needed to carry the traffic. The total resource cost formulas are operator configurable such that an operator may focus attention on a single measure of the network costs such as total link mileage or total interface count or may use the formulas to gain a realistic measure of actual costs in order to form a fair comparison between different potential solutions. In some cases the operation may add at least some component to the costs to reflect e.g. that all else being equal shorter links are better to use than longer links etc. For instance this may be reflected in the cost formula by adding a small component to the link costs that is proportional to distance. Such small changes to the cost formulas often make it very much easier for topology computation module to identify an advantageous solution for large problems as topology computation module can find some indication of the direction to steer the solution along the cost gradient. Topology computation module in some cases may also attempt to do simple allocation of the node equipment based on the number of links on the node and the traffic through it. As can be deduced from the above description the optimum solution for a network where the costs are dominated by the interface link count will look very different to the optimum solution for a network where the costs are dominated by the link mileage.

Topology computation module additionally determines penalties for the solution. For example the solution may violate one or more constraints having associated penalties. Such constraints may include as noted above maximum counts or maximum capacity on any group of links or SRLGs or combination of the two . Topology computation module may therefore determine which constraints are violated by the solution and apply the associated penalty. The failure simulations of step may also accrue penalties for e.g. traffic that cannot be routed in the solution under either normal or failure conditions. Topology computation module accumulates the penalties applied to a solution and adds the accumulated total penalty cost to the total resource cost to determine a total cost for the solution .

For the initial run iteration of the optimization algorithm YES branch of topology computation module does not perform a comparison with a previous solution but instead modifies the network topology . To modify the network topology of network topology computation module may either 1 select one of the candidate links to block by adding a high but not infinite penalty to the routing metric on the candidate link 2 select a candidate link that had been previously blocked to unblock by removing a penalty previously applied on the routing metric for the selected candidate link or 3 in some cases routing the link on a different path in the transport layer such that the new path changes the shared risk groups encountered by the link in the logical network layer and the capacity requirements in the transport network . Topology computation module may choose between blocking or unblocking and select a link according to a random function. Topology computation module in some cases however may apply simple heuristics such as biasing more expensive links toward blocking and less expensive links toward unblocking by biasing more toward blocking links that have very low traffic on them e.g. a very low ratio traffic carried link cost and towards unblocking shorter links on busy node or by biasing the selection such that active links that are on shared resource constraints at or above their constrained capacity may be preferentially selected for blocking.

Having modified the network topology for purposes of the algorithm topology computation module applies steps and to determine a new solution having newly routed traffic and to determine a total cost for the new solution. This is a subsequent iteration NO branch of . Topology computation module compares the total cost for the new solution with the total cost for the previous solution and if the total cost has been reduced with the new solution YES branch of topology computation module accepts the modified network topology and proceeds to step . If however the total cost has not been reduced with the new solution NO branch of topology computation module applies a simulated annealing function to determine whether to accept the modified network topology despite the modified network topology leading to a larger total cost . In this way topology computation module may facilitate avoiding local minima of the total cost gradient to progress the solutions to a more globally optimal solution. The simulated annealing function is a function that returns a positive result according to probability dependent on the magnitude of the cost increase and the iteration progress of the operation e.g. the number of iterations . As one example the probability for the function may be defined as 

At step topology computation module modifies the network topology by blocking or unblocking one or more candidate links as described above . If the number of iterations to be performed has not been reached NO branch of topology computation module modifies the temperature parameter for the simulated annealing function applied in step . This reduction may be proportional to the number of iterations based on configurable thresholds for the number of iterations or some other scheme. Parameter T may be user configurable or dependent on some aspect of the computation such as the number of candidate links or other aspect. To facilitate a global optimum algorithm topology computation module should spend as much time as possible in the temperature region where a reasonable percentage of the changes will increase the cost and then gradually reduce this percentage. As one example for determining T at the start of the operation topology computation module sets a target percentage to 10 such that 10 of network topology modifications result in a cost increase. At the end the target percentage is set to 0 . During the iteration the target percentage is reduced linearly as the iteration progresses. For instance every N iterations topology computation module will check the actual percentage of changes that increase the cost and check this against the target value. If the actual percentage is too high then topology computation module will decrease the parameter T. If this actual percentage is too low then topology computation module will increase the parameter T. Example intermediate and final results of this process are depicted in below.

Once the iteration loop limit has been reached and the number of iterations to be performed are performed YES of topology computation module exits the operation. In some cases the iteration complete check of step is based on other acceptance criteria such as iterating for a fixed elapsed time until the total resource costs are less than some acceptable value or until some other acceptance criteria is met. During the run of operation topology computation module stores the solution for the lowest cost solution identified during any of the iterations. While the lowest cost solution identified during operation may not be globally optimal the solution may nevertheless be optimized versus the initial determination or at least in some instances versus a solution that can be obtained in practice by alternative methods. Topology provisioning module outputs the topology data determined for the solution which may include the selected candidate links to the transport layer to set up the selected candidate links to establish the determined network topology . In some cases topology provisioning module configures the selected candidate links.

Then for each source node topology computation module computes at least one shortest path from the source node to each destination in the list of destinations in the source node . Topology computation module may leverage the characteristic of an applied open shortest path first open SPF algorithm that takes a similar amount of time to find the paths from a node to a single node as it does to find the paths from the node to all of the nodes. This takes advantage of the pre processing steps . Topology computation module may in some cases use multiple cores e.g. of multi core computing platform of to execute multiple parallel threads to perform step . Computing shortest paths for each source node to its associated destination list is an independent operation and thus each of the multiple threads may independently compute shortest paths for different subsets of the set of various source nodes. For example the pre processing steps may result in a queue of source nodes each source node requiring the computation of shortest paths and the multiple threads may dequeue source nodes from the queue for processing to collectively perform step . To ensure synchronization each source node may be associated with a mutex or other synchronization primitive. Additional details regarding computing constrained shortest paths and multi threaded computation of shortest paths generally are found in David Wood U.S. patent application Ser. No. 14 500 736 filed Sep. 29 2014 and entitled Batched Path Computation in Resource Constrained Networks which is incorporated by reference herein in its entirety.

In some cases the at least one shortest path may include multiple equal cost multipath ECMP paths. To find multiple ECMP paths taking different routes through the network the topology computation module may invoke the shortest path algorithm several times and achieves different paths by swapping the link and path search order additional example details described below with respect to . Having computed shortest paths topology computation module then routes the flows representing the traffic demands onto the shortest paths . Topology computation module adds the traffic to the links of the shortest paths and to the intermediate nodes for the shortest paths. Where ECMP paths are available topology computation module may recursively split the traffic flows over the available ECMP paths allocate traffic to the various paths by this recursive algorithm and route the split traffic flows onto the network.

The raw shortest path algorithm applied by topology computation module finds the paths in the order of the links sorted on the nodes. Accordingly if the links are re sorted such that links with identical metrics appear in a different order then the paths found will be different. The first time a re sort is invoked the order of links with the same metric is simply reversed. After the first invocation the order is randomly shuffled.

The other ordering in the shortest path algorithm comes from the order in which the list of nodes already connected is searched. As applied by topology computation module this list of nodes is randomly shuffled and when a new connected node is found it is placed in a random position in that list. Hence the shortest path algorithm will find different paths each time the algorithm is invoked if there are ECMP paths associated.

Thus topology computation module iteratively computes shortest paths for the source nodes and re sorts the various lists. The first step in the iteration is to compute at least one shortest path from the source node to each destination in the list of destinations in the source node . If any new shortest paths are identified topology computation module saves these are new ECMP paths for the source node . Topology computation module may then re sort the order of links sorted on the nodes and the ordering of the list of nodes already connected . Topology computation module iterates around the loop until the list of ECMP paths for the source node stops growing YES branch of or a maximum number of ECMP paths is identified YES branch of . The maximum number may be 5 10 12 or 15 for instance.

At this point traffic demands have a list of ECMP paths associated with them. The ECMP load balancing algorithm in the routers of network will split the traffic evenly across each branching path. Topology computation module applies a recursive model of the above to allocate the traffic for a demand on each branching path to mimic this behavior.

An example shortest path first algorithm for step above is provided in pseudocode as follows. The algorithm acts on arrays local to each node 

The above local arrays could in some cases be pointers to the links or nodes themselves or iterator pointers in the lists for example.

Note that this algorithm appears to have 3 nested levels. However the innermost loop only iterates from the last link used on that node to the first neighbor not yet connected. Hence it typically only loops 1 or 2 times and the algorithm complexity is thereby reduced.

This algorithm using sorted links may provide a large speed up for the situation in which a node will have at least in principle a large number of possible links to use. For straightforward routing calculations on normal networks in which a node will typically only have a few neighbors there is some speed up but the advantages may not be as substantial. Even so the use of batch processing to find the routes to a list of destinations from a single source node will speed up the open shortest path calculation in all circumstances.

As noted above topology computation module in above examples perform open SPF interior gateway protocol IGP routing computations and then set the capacity requirements on the network components based on this open routing. In some cases path determination may instead or additionally use constrained SPF CSPF to design the network capacity requirements for a set of LSPs requiring with CSPF based routing. In such cases topology computation module may modify the network topology model by modifying maximum capacities of the candidate links or nodes for network rather than by simply blocking or unblocking. Topology computation module may apply batched path computation techniques described in Wood U.S. patent application Ser. No. 14 500 736 incorporated above to perform CSPF computations in some cases.

Topology computation module fails in turn multiple components for multi layer network . For example topology computation module may fail each candidate link used to route traffic for the solution each of routers through which traffic has been routed each transport link or SRLG if candidate links obtained as abstract links each transport node if known and each site. The types of components failed may by be configurable by the network operator for network . For each failed component topology computation module identifies the affected traffic arranges the affected traffic by source node and generates destination lists for each source node . Topology computation module generates lists of demands affected by the failure . For traffic originating or terminating at a failed node topology computation module may remove the traffic from the routed network model. For traffic passing through a failed component topology computation module attempts to re route the traffic.

To re route topology computation module determines shortest paths to destination for each affected source node . Step may be similar to step of . Topology computation module re routes the affected flows onto the shortest paths . Such re routing may include recursively splitting re routed traffic flows over available ECMP paths routing the split traffic flows onto the network model adding the traffic to the links and to intermediate nodes. In addition topology computation module may store data describing the worst case traffic levels through the network model on the links and through the nodes these may define the dimensions for these elements . Topology computation module may also add a penalty for traffic that cannot be re routed because of the failure . Operation may incorporate all such penalties into the total cost for the solution.

The techniques described herein may be implemented in hardware software firmware or any combination thereof. Various features described as modules units or components may be implemented together in an integrated logic device or separately as discrete but interoperable logic devices or other hardware devices. In some cases various features of electronic circuitry may be implemented as one or more integrated circuit devices such as an integrated circuit chip or chipset.

If implemented in hardware this disclosure may be directed to an apparatus such a processor or an integrated circuit device such as an integrated circuit chip or chipset. Alternatively or additionally if implemented in software or firmware the techniques may be realized at least in part by a computer readable data storage medium comprising instructions that when executed cause a processor to perform one or more of the methods described above. For example the computer readable data storage medium may store such instructions for execution by a processor.

A computer readable medium may form part of a computer program product which may include packaging materials. A computer readable medium may comprise a computer data storage medium such as random access memory RAM read only memory ROM non volatile random access memory NVRAM electrically erasable programmable read only memory EEPROM Flash memory magnetic or optical data storage media and the like. In some examples an article of manufacture may comprise one or more computer readable storage media.

In some examples the computer readable storage media may comprise non transitory media. The term non transitory may indicate that the storage medium is not embodied in a carrier wave or a propagated signal. In certain examples a non transitory storage medium may store data that can over time change e.g. in RAM or cache .

The code or instructions may be software and or firmware executed by processing circuitry including one or more processors such as one or more digital signal processors DSPs general purpose microprocessors application specific integrated circuits ASICs field programmable gate arrays FPGAs or other equivalent integrated or discrete logic circuitry. Accordingly the term processor as used herein may refer to any of the foregoing structure or any other structure suitable for implementation of the techniques described herein. In addition in some aspects functionality described in this disclosure may be provided within software modules or hardware modules.

Various embodiments have been described. These and other embodiments are within the scope of the following examples.

