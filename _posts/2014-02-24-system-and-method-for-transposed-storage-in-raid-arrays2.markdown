---

title: System and method for transposed storage in raid arrays
abstract: A system and method of transposed storage in RAID arrays includes a storage manager with an interface for receiving storage requests associated with multi-segment blocks stored in a storage array, a controller for processing the received storage requests and controlling the storage manager, and a write request handler. The storage array includes a plurality of storage devices for which consecutive logical addresses are assigned to different ones of the storage devices. The write request handler is configured to process block write requests and send segment write requests to the storage array based on the write requests so that each segment of a given multi-segment block is written to a single respective one of the storage devices in the storage array. In some embodiments, the storage manager further includes a read request handler configured to determine logical addresses of requested segments and send segment read requests to the storage array.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09547448&OS=09547448&RS=09547448
owner: NetApp, Inc.
number: 09547448
owner_city: Sunnyvale
owner_country: US
publication_date: 20140224
---
The present disclosure relates generally to computing systems and more particularly to transposed storage in RAID arrays.

Modern storage systems are being increasingly called upon to store and manage ever increasing amounts of data. As the amount of data increases more and more big data applications are being developed that would like to concurrently access significant amounts of this data for analysis and processing. This puts an ever increasing burden on the storage system to not only be able to store the data but to provide access to larger and larger amounts of the data with minimum amounts of delay. The storage system is also expected to provide backup and redundancy so that failures in portions of the storage system do not result in the loss of data. The storage system may also provide high availability to the data so that periods of high demand do not unduly impact the responsiveness of the storage system.

As more and more big data applications are developed ever improving big data storage systems are desired. Accordingly it would be desirable to provide improved storage systems capable of supporting big data and big data applications.

In the following description specific details are set forth describing some embodiments consistent with the present disclosure. It will be apparent however to one skilled in the art that some embodiments may be practiced without some or all of these specific details. The specific embodiments disclosed herein are meant to be illustrative but not limiting. One skilled in the art may realize other elements that although not specifically described here are within the scope and the spirit of this disclosure. In addition to avoid unnecessary repetition one or more features shown and described in association with one embodiment may be incorporated into other embodiments unless specifically described otherwise or if the one or more features would make an embodiment non functional.

Various storage systems and storage paradigms have been developed to address one or more aspects of big data. For example the Hadoop Distributed File System HDFS which is used in many big data applications uses a distributed storage approach. HDFS is designed around an array of distributed processors that provide access to large amounts of data in distributed storage devices which are typically individual disk drives or their equivalents. To provide for backup redundancy and high availability HDFS stores replica copies of each block of data in multiple storage devices so that failures in one of the storage devices does not result in data loss with the multiple copies also supporting high availability. This arrangement however typically multiplies the amount of storage used by HDFS by a factor equal to the number of replica copies of each block and may greatly reduce the storage efficiency of HDFS.

HDFS also includes support for big data processing using the Hadoop framework which includes map reduce capabilities for partitioning processing or mapping of the data and then reducing the mapped data into results that may be aggregated. The map reduce capabilities of the Hadoop framework provide good support for searching sorting and similar big data activities. To operate effectively the Hadoop framework leverages the distributed storage and processing model of HDFS during the map reduce operations that typically involve parallel access to and analysis of several large blocks of data.

Other storage device arrangements have been used outside of HDFS with varying amounts of success with big data. One such arrangement is a redundant array of independent disks RAID array . In a RAID array a series of independent disk drives or other storage devices are operated in parallel. As data is received for storage it is divided into segments and the segments are typically stored across multiple storage devices using a technique called striping. By dividing each block of data into segments and storing the segments in stripes across the separate storage devices RAID arrays often provide good support for high availability as multiple storage devices may work in parallel to satisfy a read request or a series of read requests.

RAID arrays also store segments that contain redundancy information that varies depending on the version of RAID being implemented in the RAID array. In some versions of RAID the redundancy information includes duplicates of the segments and in other versions the redundancy information includes parity segments or other forms of error detection and or correction. This redundancy information also provides a good compromise between backup and redundancy goals and total storage used with the total amount of storage used being at most twice as large as the amount of data stored.

RAID arrays using striping however are not always the best choice for the storage devices in HDFS or other file systems that use large data blocks. For example the block sizes used by HDFS are typically much larger than the segment sizes used by RAID arrays typically as much as a thousand times larger. This means that the striping that supports high availability in the RAID array becomes somewhat of a detriment when used with large block file systems such as HDFS. This is because the data for just a single large block may be stored on 100 or more stripes in a RAID array. This means that the segments of two different large blocks and the stripes they are stored in are often a long distance away from each other on the storage devices in the RAID array. Thus for a file system to concurrently access multiple large blocks in parallel a common occurrence with map reduce operations the storage device often has to cycle back and forth or thrash great distances across each storage device. This often greatly reduces the responsiveness of the RAID array during storage read requests.

Much of this thrashing may be eliminated by carefully controlling how large multi segment blocks are stored in the storage devices of the RAID array. Rather than writing each large multi segment block in stripes on the RAID array several large multi segment blocks are stored at the same time with consecutive segments being taken from different large multi segment blocks. Thus rather than each large multi segment block being striped across the storage devices in consecutive locations from the perspective of the RAID array the segments in each large multi segment block are transposed so that they may be stored in consecutive locations in a single storage device. In this way each storage device in a stripe stores a segment from a different large multi segment block. Thus when a large multi segment block is read back from the RAID array each of the segments containing the multi segment block is sequentially located on just a single storage device. This supports read operations where little or no thrashing occurs as long as each storage device is responding to block read requests for just one large multi segment block at a time. This may be accomplished by separating read requests for large multi segment blocks into separate queues corresponding to each of the storage devices in the RAID array.

Transposing the storage of the segments of large multi segment blocks in a RAID array involves careful control of the locations or addresses where the segments are stored. From the perspective of the RAID array logical addresses are organized consecutively with the first logical address being assigned to the first segment location in the first storage device with consecutive logical addresses being assigned across the stripes with the address for the second logical segment being in the second storage device and so forth until the end of the stripe is reached. At this point the logical addresses are then assigned beginning with the second segment on the first drive then the second segment of the second drive and so forth. This is further complicated by segments containing the redundancy information which are skipped over during the assignment of the logical addresses. However by transposing the storage of large multi segment blocks onto a single storage device consecutive segments from the large multi segment block are no longer found at consecutive logical addresses in the RAID array. This requires careful mapping of storage requests specifying a large multi segment block and segments within the large multi segment block to the logical addresses where the segments are located within the RAID array so that the large multi segment blocks may be retrieved later.

Memory may be used to store software executed by the storage server one or more data structures used during operation of storage server as well as data being cached by storage server . Memory may include one or more types of machine readable media. Some common forms of machine readable media may include floppy disk flexible disk hard disk magnetic tape any other magnetic medium CD ROM any other optical medium punch cards paper tape any other physical medium with patterns of holes RAM PROM EPROM FLASH EPROM any other memory chip or cartridge and or any other medium from which a processor or computer is adapted to read.

Storage server is further coupled to persistent storage in the form of RAID array through a network . Network may be any kind of network including one or more connectors or cables such as small computer serial interface SCSI cables local area networks LANs such as Ethernets and or wide area networks like the internet. Storage manager may communicate with a RAID controller in RAID array using any suitable storage protocol such as the SCSI protocol the Internet SCSI iSCSI protocol and or the like. RAID controller may receive storage requests from storage manager and forward them to one or more storage devices included in RAID array . Each of the storage requests typically includes one or more logical addresses or logical block addresses LBAs that identify corresponding segments of data stored in storage devices . Each of the segments may be located in different storage devices and at different locations within the storage devices as is discussed in greater detail with respect to . The storage efficiency and utilization of a RAID array such as RAID array is generally better than the storage efficiency and utilization of HDFS and its system of replicating blocks. This allows a RAID array with storage devices having the same capacity as corresponding storage devices in a HDFS to store more data. Depending upon the version of RAID used the storage efficiency and utilization is 50 percent or greater whereas a HDFS with a commonly used replication factor of 3 has a storage efficiency and utilization of only 33 percent.

Each of the storage devices may include persistent storage. Some common forms of persistent storage include for example floppy disk flexible disk hard disk magnetic tape any other magnetic medium CD ROM DVD ROM any other optical medium battery backed RAM PROM EPROM FLASH EPROM any other memory chip or cartridge and or any other medium from which a processor or computer is adapted to read from and write data to.

Storage server and storage manager typically facilitate access to the data stored in RAID array to one or more applications such as representative application . Application may be coupled to storage server using a network . Network may include one or more LANs and or WANs. In some embodiments network may be the same as network and or partially overlap network . In some embodiments application may be running on any computing device including storage server . Application may access the services of storage manager by using an interface of storage manager for supporting application programming interface API calls remote procedure calls web services and or like that may be used to make one or more storage requests. Each of the storage requests may include sufficient information for storage manager to determine the logical addresses used by RAID controller to store the requested data. In some embodiments application may use a map reduce interface such as the Hadoop API of the Hadoop framework. The map reduce interface may receive processing requests from application to support searching sorting and or analyzing of the large amounts of data RAID array and may further access the interface of storage manager .

Each row represents a stripe of data in the RAID array . In RAID 6 two segments in each stripe are used for the dual parity information as represented by the P and Q segments. When data is written in consecutive order to a RAID 6 array the last two segments written include the P and Q parity segments. To evenly distribute the P and Q segments across each of the storage devices the P and Q segments are rotated through each of the storage devices. Segments in each of the stripes are generally assigned consecutive logical addresses in ascending order from left to right in a circular fashion with the complication that the position of the P and Q segments dictates which of the storage devices in each stripe receives the first segment in the stripe. The assignment of logical addresses further omits assigning logical addresses to the P and Q segments.

In the 10 storage device RAID 6 array each stripe includes 8 10 2 segments with user data which is assigned a logical address along with two segments for P and Q. In the first stripe row or stripe 0 the first eight storage devices storage devices 0 through 7 are assigned logical addresses 0 through 7 respectively. Storage devices 8 and 9 are then used for the P and Q respectively for that row stripe. Once the first stripe is assigned logical addresses 0 through 7 the assignment process continues for the second stripe row or stripe 1 but with P and Q rotated one column storage device to the left so that the next logical address 8 is assigned to the segment in column storage device 9 with logical addresses 9 through 15 being assigned to column storage devices 0 through 6 respectively. The assignment of logical addresses continues in similar fashion for rows stripes 2 through 9 until P and Q are rotated around back to the last two columns storage devices. The pattern of logical address assignment then repeats for rows stripes 10 through 19 and so on. Thus for a RAID 6 array with D storage devices D D 2 segments 80 10 10 2 in the examples of are assigned logical addresses in each rotation of D rows stripes 10 in the examples of .

The assignment of logical addresses for each segment in the RAID array follows a logical pattern. The logical addresses are assigned in circular left to right order across each row stripe subject to the locations of the P and Q segments in the row stripe. The pattern of logical addresses also advances regularly in each column storage device. For D 3 of the rows stripes the logical addresses increase by D 1 between rows. And when the P and Q segments are rotated through the column storage device two rows stripes are skipped and the logical addresses increase by 2 D 3 17 2 10 3 in the examples of over the last 3 rows stripes.

The scope of embodiments for RAID arrays is not limited to the structure and arrangement shown in . According to some embodiments different RAID versions may be used in the RAID array . In some examples other than 10 columns drives storage devices may be used in the RAID array . In some examples different numbers than the two parity segments may be used in each row stripe including zero one and or three or more parity segments. In some embodiments logical addresses may be assigned with a different granularity. The granularity of logical addresses is based on segments so that at most a single logical address is assigned for each column storage device in each row stripe. In some embodiments the logical addresses may be scaled to account for smaller storage units. In some examples the logical addresses may be assigned to individual blocks and or to other storage units that form each segment. In some examples each segment may be used to store 64 kilobytes of data. When logical addresses are assigned at the byte level each of the segments includes 65 536 logical addresses with the first segment segment 0 including logical addresses 0 through 65 535 the second segment segment 1 including logical addresses 65 536 through 131 071 and so forth. The segment numbers in may be used to identify the starting logical address in each segment by multiplying the segment number by the number of logical addresses in each segment. When logical addresses are assigned using other storage unit sizes such as the 512 byte storage unit used by SCSI each 64 kilobyte segment includes 128 SCSI storage units and the starting logical address in each segment may be determined by multiplying the segment number by 128.

When a large amount of data is being written to the RAID array the P and Q values for each row stripe may be computed as each segment in the row stripe is written with the P and Q values being written as the last two segments in the row stripe. When only single segments are being written to the RAID array such as during an update to and or replacement of a segment the segment is read followed by the P and Q segments from the same row stripe. The data read from the segment is then removed from the parity values of P and Q and then the new data for the segment is added to the parity values of P and Q. The segment is then overwritten with the new data and the new P and Q parity values are written over the old P and Q parity values in the corresponding segments.

Without modification RAID arrays such as the RAID array are generally not ideal for big data storage such as that used by HDFS. This is due to the use of striping by the RAID array and the general desire to consecutively access multiple large multi segment data blocks during operations like map reduce. Consider the scenario where a RAID array uses 64 kilobyte segments and the large multi segment blocks used by HDFS include 64 megabytes of user data and 512 kilobytes of metadata. Each multi segment HDFS block stored in the RAID array uses 1032 segments 1024 for the user data and 8 for the metadata . When this multi segment HDFS block is stored in the RAID array it occupies 129 stripes. When the RAID array is asked to retrieve a single multi segment block it may do so efficiently by moving in order from stripe to stripe. However as soon as the RAID array is asked to concurrently retrieve two or more large multi segment blocks it may end up alternating accesses to segments that are at least 129 stripes apart. When the storage devices in the RAID array are disk drives this may result in a significant drop in efficiency as the disk drives end up thrashing back and forth between the very distant stripes with the corresponding long seek delays with each thrash between stripes from the different large multi segment blocks.

This thrashing may be significantly reduced and or eliminated by more intelligently controlling the segments to which each large multi segment block is written to. If segments from multiple large multi segment blocks are carefully interleaved in the RAID array it is possible to store each of the large multi segment blocks so that it is stored on a single one of the storage devices in the RAID array. This in effect transposes the storage of individual large multi segment blocks so that instead of being stored across the columns storage devices in rows stripes the large multi segment block is stored in a single column storage device. By transposing the storage for large multi segment blocks in a RAID array the RAID array becomes an effective substitute for the distributed storage devices and replication of blocks typically used with HDFS. Thus the application using the Hadoop framework and or Hadoop API may use RAID based storage devices with their lower storage overhead without suffering from reduced performance caused by thrashing when a RAID array with un transposed storage is asked to concurrently retrieve data from two or more large multi segment HDFS blocks.

For example when the RAID array includes D storage devices by waiting until D multi segment blocks are ready to be written it is possible to store each of the D multi segment blocks on its own storage device in the RAID array by controlling the order in which segments from the D multi segment blocks are sent to the RAID array for writing. This approach allows the RAID array to lay down the segments in the stripes in the normal allocation order consistent with a burst style write operation but helps ensure that each multi segment block is written to a single storage device. In the examples of this may be accomplished by selecting segments from ten large blocks based on the order in which the logical addresses are assigned to the various columns storage devices. Thus segments may be selected in order from the first through eighth large blocks for the first stripe with the RAID array preparing and writing the P and Q segments on the ninth and tenth storage devices based on the other segments on the first stripe. The ninth large block is skipped and then a segment from the tenth large block is followed by segments from the first through seventh large blocks for the second stripe with the RAID array again preparing and writing the P and Q segments on the eighth and ninth storage devices. For the third stripe segments are selected from the ninth and tenth large blocks and then the first through sixth large blocks with the RAID array handling the P and Q segments on the seventh and eighth storage devices. This pattern continues until after every tenth stripe the pattern repeats. This approach may also be adapted to write fewer than D large blocks by skipping over some of the segments during the writing process by providing either dummy segments or using partial writes that also update the P and Q segments on the respective stripes. The storage system then records at least the starting or base logical address and or the row and column of the first segment in the large block in a data structure so that the large block may be located later during subsequent storage operations.

Reading back the multi segment blocks from the RAID array presents more of a challenge because each of the multi segment blocks is no longer stored at consecutive logical addresses. When a multi segment block is read from the RAID array a corresponding logical address for that segment in the RAID array is determined for each of the segments in the multi segment block. This translation from a segment number to a logical address involves determining the starting or base logical address for the multi segment block in the RAID array and then stepping through each of the logical addresses based on the segment number or offset within the multi segment block. The starting or base logical address may be determined by consulting the data structure maintained during write operations that associates multi segment blocks with starting or base logical addresses and or a starting row and column in the RAID array.

Thrashing in the RAID array may be reduced by exercising additional controls on how the multi segment blocks are read. When multiple multi segment blocks are being concurrently read from the RAID array thrashing is greatly reduced as long as each of the multi segment blocks is stored in a different storage device. This is because the RAID array may operate the seek operations on each of the storage devices independent of the others. To further reduce thrashing the storage system may control the ordering of read operations so when two requested multi segment blocks are stored in the same storage device all of the read requests for one of the multi segment blocks should be handled before any read requests for the other of the multi segment blocks. This may be accomplished by using individual first in first out FIFO queues for each of the storage devices in the RAID array.

Controller provides the general control and or management for storage manager . As controller processes storage requests it may access a block metadata data structure that keeps track of where each of the blocks are stored in the RAID array such as RAID array and or being managed by storage manager . In some examples the block metadata data structure may include a record for each block being managed. Each record includes sufficient information that allows storage manager to determine the logical addresses associated with each block identifier provided in a storage request. For example the record may include a starting or base logical address for the corresponding block and or a row and column location of the first segment of the block in the RAID array. In some examples the records may be indexed by the block identifiers. Any suitable collection based data structure may be used for block metadata data structure such as arrays vectors maps database tables and or the like.

Controller further examines the received storage requests to determine whether they are block read or write requests. When the storage requests are block read requests they are passed to a read request handler . The read request handler uses the starting or base address information or the row and column information for the block in each block read request to determine the corresponding storage device on which the block is stored. Once the storage device is determined the block read request is placed in the FIFO queue for that storage device. When the block read request reaches the head of its respective FIFO queue it is extracted from the respective FIFO queue . By using one FIFO queue for each storage device the read request handler may wait for the segment read requests for a first block on a storage device to complete before sending segment read requests to the RAID array for a second block on the same storage device thus reducing the possibility that the storage device will be asked to handle segment read requests for the second block at very different logical addresses in between segment read requests for the first block. The block read request is further examined by the read request handler and the logical addresses of the segments storing the requested portions of the requested block are determined. The logical addresses are then used to generate one or more segment read requests that are sent to the RAID array for further handling. The likelihood of thrashing may also be reduced by sending the segment read requests to the RAID array in order of ascending logical addresses.

When the RAID array returns the requested segments they are passed back to controller and then returned to the application that made the corresponding block read request. Read request handler may further use read caching to improve its read operation. This may include checking a read cache to see whether the requested block is cached before placing the block read request in one of the FIFO queues and or sending segment read requests to the FIFO array. When responses from the segment read requests are returned from the RAID array the data in them may be cached in read cache . Read cache may include one or more types of machine readable media suitable for use in a cache including floppy disk flexible disk hard disk any other magnetic medium optical media RAM EEPROM FLASH EPROM and or any other rewritable memory chip cartridge or medium from which a processor or computer is adapted to read.

When the storage requests are block write requests they are passed to a write request handler . The write request handler uses a write cache to collect write data associated with multiple blocks before sending the write data to the RAID array via a series of segment write requests thus implementing a write back strategy. In some examples write cache may include at least enough storage capacity to cache as many blocks as there are storage devices in the RAID array. This allows write request handler to write segments to consecutive logical addresses of the RAID array by interleaving segments from the cached blocks as described previously. As each of the blocks are written to the RAID array the block metadata data structure is updated to associate the starting or base logical address and or the row and column of the starting segment with the corresponding block identifier. This update allows the read request handler to appropriately handle requests for the blocks later. Write cache may include one or more types of machine readable media suitable for use in a cache including floppy disk flexible disk hard disk any other magnetic medium optical media RAM EEPROM FLASH EPROM and or any other rewritable memory chip cartridge or medium from which a processor or computer is adapted to read.

In some embodiments the write request handler may not be able to wait until a sufficient number of blocks are cached in write cache before sending segment write requests to the RAID array. In some examples this may occur when segment write requests are being processed that update a portion of a previously written block and or to overwrite and reclaim a deleted block. In some examples this may also occur when a caching policy for write cache includes a maximum delay between when the block is written to storage manager and when the block is stored in the RAID array. In some examples write request handler may use one or more timers to implement these delays. In some examples this may additionally occur as part of the write back operation that occurs when a block or page in write cache is invalidated prior to replacement.

In some embodiments read request handler may additionally consult write cache to determine whether a requested block is located in write cache such as when a block is written to storage manager but is not yet written to the RAID array.

Algorithm describes one possible way to iteratively determine the logical addresses of the segments in a block within the RAID array beginning with a Base logical address. Each of the logical addresses is recorded into the array Addr where the index of array Addr corresponds to the segment offset within the block. For example the first segment of the block has an offset of zero the second segment of the block has an offset of 1 and so on. Size indicates the number of segments in the block and corresponds to the size of the block

Algorithm begins by noting that the logical address of the first segment is stored at the Base logical address. A variable Next in then used to track how the logical addresses increase with each segment of the block. Algorithm then iterates through each of the remaining segments of the block. As the logical addresses of each of the segments is determined in turn algorithm determines whether the next segment is located on the next row stripe of the RAID array or whether two rows stripes are skipped to account for P and Q parity segments. The test for whether the next segment is one or three stripes away is determined by testing when the remainder found when the logical address is divided by D 2 is D 3 . In the examples of this corresponds to whenever the logical address divided by 8 10 2 is equal to 7 10 3. This corresponds to the logical addresses 7 15 . . . 63 71 79 . . . . From these logical addresses correspond to each segment located in the same column storage device but in the row stripe before a P segment in that column storage device. When one of these logical addresses is encountered the logical address or the next segment is located three stripes away at a logical address that is 2 D 3 17 2 10 3 in larger. In the other cases the next segment is located in the next row stripe at a logical address that is D 1 9 10 1 in larger. Once the logical address of the next segment is determined it is recorded in the Addr array. The storage manager may then use the Addr array to determine the logical address of any segment within the block based on the offset of the segment within the block.

Algorithm shows one possible way to determine the logical address of any segment based on the Base address of the block and the Offset of the segment within the block without having to iterate through each of the segment logical addresses. In some examples algorithm may be more suitable for random access to a desired segment within a block.

Algorithm begins by determining how may full rotations of P and Q stripe patterns and the number of steps within the last rotation exist between the Base logical address and the logical address of the segment with the Offset. Each full rotation of P and Q includes D 2 8 10 2 in segments so the number of full rotations and the number of steps may be determined using the integer division and modular remainder functions using the Offset. The Addr of the segment may then be advanced D D 2 80 10 10 2 in logical addresses for each full rotation.

Once the full rotations are accounted for algorithm accounts for whether stripes with P and Q segments are included in the last rotation. This may be determined by observing how far into the rotation the Base address occurs by finding the remainder when the Base address is divided by D 2 8 10 2 in and then adding in the number of steps the logical address of the Offset is within the last rotation. This combination crosses the P and Q stripes when the sum is greater than or equal to D 2 8 10 2 in . When the P and Q stripes are skipped this accounts for 2 D 3 17 2 10 3 in logical addresses plus D 1 9 10 1 in logical addresses for each of the remaining steps in the last rotation. When the P and Q stripes are not skipped D 1 9 10 1 in logical addresses are skipped for each step in the last rotation.

Algorithm shows one possible way to determine the Row and column storage device Col of the Base address for a block or any other logical address in a RAID array. Because each row stripe in the RAID array includes D 2 8 10 2 in segments the Row may be determined by the number of full rows in the RAID array before the Base address by finding the integer quotient when the Base address is divided by D 2 . The Partial position of the Base address within the row stripe may be determined by finding the remainder when the Base address is divided by D 2 . However because logical address assignment on each row stripe begins to the right of the Q segment with the exception of the rows stripes that are evenly divisible by D the logical addresses of the Partial segments are assigned beginning with the starting column storage device StartCol found by subtracting D from the remainder when the Row is divided by D. When the Row reaches the end i.e. where the column storage device reaches D the logical addresses roll over to zero which may be accounted for by finding the remainder when the sum of the starting column storage device and the number of Partial segments is divided by D.

Algorithm shows one possible way to determine a logical address for a segment such as the Base segment of a block from the row stripe Row and the column storage device Col in which the segment is stored. Determination of the logical address depends on whether the segment is to the right or the left of the P and Q segments on the same row stripe. This may be determined by comparing the column storage device for the segment to the remainder when the row stripe is divided by D. Looking at each row stripe from left to right shows that the logical address in the first zero column is evenly divisible by D 1 9 10 1 in and has the value Row D 1 . When the segment is to the left of the P and Q segments the logical addresses of the segment is larger than the logical address of the segment in the first column by the column storage device number. When the segment is to the right of the P and Q segments the logical addresses of the segment is smaller than the logical address of the segment in the first column by the column storage device subtracted from D.

At a process a storage request is received. As each application such as application desires to read and write data to a storage system they make requests to the storage system. In some embodiments these requests may be received by a storage manager such as storage manager and or . The storage requests may be received at an interface of the storage manager such as interface via API calls remote procedure calls web services requests as part of one or more messages and or similar mechanisms. The storage requests may each include a block identifier that may be used to determine the block or multi segment block that is to be accessed to satisfy the respective storage request.

At a process it is determined whether the storage request is a block read request or a block write request. The storage request received during process is examined to determine whether it is a block read request or a block write request. In some examples the storage request may include one or more fields that designate the storage request as a block read request or a block write request. In some examples different API calls remote procedure calls web services and or the like may be used for block read requests and block write requests. When the storage request is a block write request it is handled beginning with a process . When the storage request is a block read request it is handled beginning with a process .

At the process write data is cached. When the storage request is a block write request the block write request may include data that is to be written to a storage system. Rather than send the write data directly to the storage devices used by the storage system the write data is temporarily stored in a write cache. The use of a write cache allows the storage server to accumulate write data for at least as many different blocks as there are storage devices in the RAID array that is used as storage for the storage system. The use of a write cache may also make the storage system more responsive to block write requests as the write data may typically be cached more quickly than it can be written to the storage devices in the RAID array.

At a process the write data is sent to the RAID array. The storage manager for the storage system periodically sends the write data in the write cache to the RAID array for storage in the storage devices of the RAID array. In some examples the storage devices may be the storage devices in RAID array . The write data may be sent to the RAID array by making one or more segment write requests to a RAID controller for the RAID array. The storage manager may decide to send the write data to the RAID array based on one or more policies. In some examples one of the policies may include writing the write data to the RAID array when write data has been received for as many blocks as there are storage devices in the RAID array. The storage manager may send segment write requests with segments interleaved from each of the blocks in the write cache so that the segment write requests sent to the RAID array occur in an order so that each of the blocks is written to in a single respective storage device. In some examples the order in which the segments are selected from the blocks is consistent with the logical address numbering of .

In some embodiments other policies may include writing the write data to the RAID array before write data from a complete set of blocks is stored in the write cache. In some examples the write data may be sent to the RAID array when the block write request is updating a previously written block a maximum delay has passed since the write data was received by the storage manager and or when write back occurs during write cache page invalidation and or page replacement. In some embodiments when the storage manager sends less than a complete set of blocks the segment write requests sent to the RAID array may include the logical addresses to be used for the write data. In some examples when less than a complete set of blocks is being sent to the RAID array the address calculations of algorithms like algorithms and or may be used to specify the logical address of each of the segments of write data being written to the RAID array. In some examples when the set of blocks are new blocks the storage manager may send dummy segments to the RAID array for the missing blocks so that RAID array may assign the logical addresses to the segments in consecutive order.

Each of the segment write requests sent to the RAID controller of the RAID array may generate a response from the RAID controller indicating the logical addresses or the rows stripes and columns storage devices of the segments into which the write data is being written to.

At a process block metadata is saved. After the write data for each block is sent to the RAID array metadata associated with the block is saved in a block metadata data structure such as block metadata data structure . The storage manager may use the logical addresses returned in response to the segment write requests of process to determine where each of the blocks is being written in the RAID array. In some examples the logical address of the first segment of the first segment in each block may be recorded as a starting or base logical address for the block in the corresponding record of the block metadata data structure. In some examples the row and column location of each block may be recorded in the corresponding record of the block metadata data structure. The storage manager may use algorithms like algorithms and or to convert between starting or base logical address values and row stripe and column storage device values based on the information returned during process and the metadata to be recorded in the block metadata data structure.

Responses from the handling of the block write request received during process may be returned using a process . For example the response may be returned as soon as the write data is cached during process .

At the process block metadata is read. Using a block identifier in the block read request received during process the storage manager may read or retrieve a record from the block metadata data structure corresponding to the block identifier using a lookup and or search operation. For example the block metadata data structure may be indexed by block identifiers to facilitate this lookup and or search operation.

At a process the RAID drive number is determined. When the block metadata data structure records include the column storage device for the first segment of the block the RAID drive number may be determined from this value. When the block metadata data structure records include the starting or base logical address for the block an algorithm like algorithm may be used to determine the RAID drive number.

At a process the RAID logical addresses are determined. Because the segments in the block are not stored in consecutive logical addresses in the RAID array the offsets of the desired segments from the block that is being read are converted to the logical addresses where they are stored in the RAID array. Algorithms like algorithms and or may be used to convert the segment offsets to their corresponding logical addresses.

At a process the block read request is inserted into a read queue. To help prevent thrashing that may be caused by sending segment read requests for two or more blocks that are stored on the same storage device in the RAID array the block read requests are inserted into the read queue that corresponds to the RAID drive number determined during process . Use of separate read queues for each of the storage devices helps reduce the likelihood that none of the storage devices will receive segment read requests for a first block in between segment read requests for a second block with the large change in logical addresses that likely exists between the first and second blocks. In some examples the read queue may be one of the FIFO queues .

At a process the block read request is extracted from the read queue. When the block read request reaches the head of the read queue it is extracted from the read queue for further processing.

At a process segment read requests are sent to the RAID array. Using the RAID logical addresses determined during process one or more segment read requests are sent to the RAID array. In some examples each of the segment read requests includes a corresponding logical address for one of the segments from the block. To help prevent thrashing the segment read requests may be sent to the RAID array in order of ascending logical address so that as the corresponding storage device of the RAID array responds to the segment read requests it seeks in a consistent direction. Thrashing may further be prevented by ensuring that the segment read requests for the block are not sent to the RAID array until each of the segment read requests for the previous block i.e. the segment read requests corresponding to the previous block read request extracted from the read queue are handled by the RAID array.

In some embodiments the data returned by the RAID array as responses from the segment read requests may be stored in a read cache such as read cache . The data may also be returned as responses to the read request received during process by using process .

At the process a response to the storage request is returned. Based on the responses received for the segment write requests send to the RAID array during process and or the segment read requests sent to the RAID array during process a response to the storage request received during process is returned to the application that made the storage request. In some examples when the storage request was received due to an API call the response may be returned as a return value for the API call. In some examples the response may be returned using one or more messages. Method may then repeat with the receipt of another storage request during process .

The scope of embodiments for storage request handling in a storage system is not limited to the structures and arrangements shown in . According to some embodiments the storage manager performing method may also include further support for caching. In some examples the storage manager may determine whether segments associated with a block are stored in the write cache or the read cache before performing much of the processing of processes . According to some embodiments the storage manager may further provide support for load balancing filling of deleted blocks and or other storage system operations. In some examples this may include moving blocks from one storage device to another storage device within the RAID array to better balance the utilization among the storage devices. As the blocks are redistributed the records for the corresponding blocks in the block metadata data structure are updated to reflect the change in block location.

Some examples of storage server storage manager and or storage manager may include non transient tangible machine readable media that include executable code that when run by one or more processors may cause the one or more processors e.g. the processor in storage server to perform the processes of method and or algorithms as described above. Some common forms of machine readable media that may include the processes of method and or algorithms are for example floppy disk flexible disk hard disk magnetic tape any other magnetic medium CD ROM any other optical medium punch cards paper tape any other physical medium with patterns of holes RAM PROM EPROM FLASH EPROM any other memory chip or cartridge and or any other medium from which a processor or computer is adapted to read.

Although illustrative embodiments have been shown and described a wide range of modification change and substitution is contemplated in the foregoing disclosure and in some instances some features of the embodiments may be employed without a corresponding use of other features. One of ordinary skill in the art would recognize many variations alternatives and modifications. Thus the scope of the invention should be limited only by the following claims and it is appropriate that the claims be construed broadly and in a manner consistent with the scope of the embodiments disclosed herein.

