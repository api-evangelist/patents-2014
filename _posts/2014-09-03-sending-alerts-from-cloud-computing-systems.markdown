---

title: Sending alerts from cloud computing systems
abstract: A system is described for sending alert messages collected for cloud computing system to external systems. The alert messages may be sent according to determined protocols, such as the syslog protocol and/or SNMP trap, among other appropriate protocols. Partitioning and/or fragmenting of the alert message may be provided based on the use of various types of message identifications and/or other information in which, by virtue of the partitioning and fragmenting, an alert message, regardless of its length, may be sent and reconstructed at the external system. The system advantageously provides for transmission of alert messages using mechanisms other than the syslog protocol and with additional included alert attribute information.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09516112&OS=09516112&RS=09516112
owner: EMC IP Holding Company LLC
number: 09516112
owner_city: Hopkinton
owner_country: US
publication_date: 20140903
---
This application is a continuation of U.S. application Ser. No. 13 537 889 filed Jun. 29 2012 pending which is hereby incorporated by reference.

This application relates to management of a distributed system and more particularly to the managing and transmission of cloud computing system alert messages.

Host processor systems may store and retrieve data using storage devices also referred to as storage arrays containing a plurality of host interface units host adapters disk drives and disk interface units disk adapters . Such storage devices are provided for example by EMC Corporation of Hopkinton Mass. The host systems access the storage device through a plurality of channels provided therewith. Host systems provide data and access control information through the channels of the storage device and the storage device provides data to the host systems also through the channels. The host systems do not address the disk drives of the storage device directly but rather access what appears to the host systems as a plurality of logical volumes. Different sections of the logical volumes may or may not correspond to the actual disk drives. The hosts storage devices and or other elements such as switches and or array components may be provided as part of a storage area network SAN .

Performance characteristics of the storage devices and or other elements of the SAN may be monitored according to different performance statistics and measures. Performance characteristics may include for example performance data capacity data and or discovery data including configuration data and or topology data among other characteristics. As an example performance characteristics of input output I O data paths among storage devices and components may be measured and may include I O operations e.g. measured in I Os per second and Mbs per second initiated by a host that will result in corresponding activity in SAN fabric links storage array ports and adapters and storage volumes. Other characteristics may similarly be measured. Such characteristics may be significant factors in managing storage system performance for example in analyzing use of lowering access performance versus more expensive higher performance disk drives in a SAN or by expanding number of SAN channels or channel capacity. Users may balance performance capacity and costs when considering how and whether to replace and or modify one or more storage devices or components.

Root cause and impact analysis of events occurring in a system may provide automated processes for correlating the events with their root causes. Reference is made for example to U.S. Pat. No. 7 529 181 to Yardeni et al. entitled Method and Apparatus for Adaptive Monitoring and Management of Distributed Systems that discloses a system for providing adaptive monitoring of detected events in a distributed system U.S. Pat. No. 7 003 433 to Yemini et al. entitled Apparatus and Method for Event Correlation and Problem Reporting that discloses a system for determining the source of a problem in a complex system of managed components based upon symptoms and U.S. Pat. No. 6 965 845 to Ohsie et al. entitled Method and Apparatus for System Management Using Codebook Correlation with Symptom Exclusion that discloses a system for correlating events in a system and provides a mapping between each of a plurality of groups of possible symptoms and one of a plurality of likely problems in the system all of which are assigned to EMC Corporation and are incorporated herein by reference.

Under various circumstances it is desirable to send alerts concerning events causes and impacts collected for cloud computing system components to external systems e.g. remote systems. Known techniques provide for the use of the syslog protocol to send alert messages to external systems. Use of the syslog protocol however may result in various disadvantages and or inefficiencies in the transmission of alert messages including for example considerations of permitted syslog message size. Accordingly it would be desirable to transmit cloud computing system alert messages to external systems using mechanisms in addition to or other than the syslog protocol and or with enhancements to the use of the syslog protocol for alert transmissions.

According to the system described herein a method for sending an alert message from a cloud computing system to an external system includes collecting at least one alert for the could computing system. A time for sending the at least one alert as an alert message to the external system is determined. A transmission type of the alert message is determined. The alert message is prepared according to the transmission type. It is determined whether the alert message is transmissible in unaltered form according to the determined transmission type. After determining that the alert message is not transmissible in unaltered form according to the determined transmission type message form processing is performed on the alert message to yield a plurality of processed form messages. The message form processing includes partitioning the alert message into a plurality of partitioned messages and fragmenting at least one of the plurality of partitioned messages into a plurality of fragmented messages. The plurality of processed form messages are then sent to the external system. The transmission type may include at least one of syslog protocol or simple network management protocol SNMP trap. The time for sending the at least one alert may be determined according to at least one of i based on content of the alert message ii manually set by a user iii automatically at periodic intervals or iv in real time after the at least one alert is processed. The alert message may include alert data that includes at least one alert attribute. Each of the plurality of fragmented messages may include a portion of the alert data. Each of the plurality of partitioned messages and each of the plurality of fragmented messages may include information that permits reconstructing of the alert message. The cloud computing system may include at least one Vblock and the at least one alert may include a Vblock alert. Determining whether the alert message is transmissible in unaltered form according to the determined transmission type may include determining that the alert message is not transmissible in unaltered from based on a size of the alert message exceeding a permitted size according to the determined transmission type.

According further to the system described herein a non transitory computer readable medium stores software for sending an alert message from a cloud computing system to an external system. The software includes executable code that collects at least one alert for the cloud computing system. Executable code is provided that determines a time for sending the at least one alert as an alert message to the external system. Executable code is provided that determines a transmission type of the alert message. Executable code is provided that prepares the alert message according to the transmission type. Executable code is provided that determines whether the alert message is transmissible in unaltered form according to the determined transmission type. Executable code is provided that after determining that the alert message is not transmissible in unaltered form according to the determined transmission type performs message form processing on the alert message to yield a plurality of processed form messages. The message form processing may include partitioning the alert message into a plurality of partitioned messages and fragmenting at least one of the plurality of partitioned messages into a plurality of fragmented messages. Executable code is provided that sends the plurality of processed form messages to the external system. The transmission type may include at least one of syslog protocol or simple network management protocol SNMP trap. The time for sending the at least one alert may be determined according to at least one of i based on content of the alert message ii manually set by a user iii automatically at periodic intervals or iv in real time after the at least one alert is processed. The alert message may include alert data that includes at least one alert attribute. Each of the plurality of fragmented messages may include a portion of the alert data. Each of the plurality of partitioned messages and each of the plurality of fragmented messages may include information that permits reconstructing of the alert message. The cloud computing system may include at least one Vblock and the at least one alert may include a Vblock alert. Determining whether the alert message is transmissible in unaltered form according to the determined transmission type may include determining that the alert message is not transmissible in unaltered from based on a size of the alert message exceeding a permitted size according to the determined transmission type.

According further to the system described herein a cloud computing system that sends an alert message to an external system includes at least one cloud processing component wherein the cloud processing component is at least one of a compute component a network component or a storage component. A manager component is included that includes at least one processor and a non transitory computer readable medium storing software executable by the at least one processor. The software includes executable code that collects at least one alert for the cloud computing system. Executable code is provided that determines a time for sending the at least one alert as an alert message to the external system. Executable code is provided that determines a transmission type of the alert message. Executable code is provided that prepares the alert message according to the transmission type. Executable code is provided that determines whether the alert message is transmissible in unaltered form according to the determined transmission type. Executable code is provided that after determining that the alert message is not transmissible in unaltered form according to the determined transmission type performs message form processing on the alert message to yield a plurality of processed form messages. The message form processing may include partitioning the alert message into a plurality of partitioned messages and fragmenting at least one of the plurality of partitioned messages into a plurality of fragmented messages. Executable code is provided that sends the plurality of processed form messages to the external system. The transmission type includes at least one of syslog protocol or simple network management protocol SNMP trap. The at least one cloud processing component may include a Vblock and the at least one alert may include a Vblock alert. The alert message may include alert data that includes at least one alert attribute and in which each of the plurality of fragmented messages may include a portion of the alert data.

Each of the hosts may be communicably coupled to one or more of directors over one or more network connections . It is noted that host devices may be operatively coupled with directors over any of a number of connection schemes as required for the specific application and geographical location relative to each of the directors including for example a direct wired or wireless connection an Internet connection a local area network LAN type connection a wide area network WAN type connection a VLAN a proprietary network connection a Fibre channel FC network etc. Furthermore hosts may also be coupled to one another via the networks and or operationally via a different network and several of the hosts may be clustered together at one or more sites in which the sites are geographically distant from one another.

Each of the directors may also include or be communicably coupled with one or more file systems such as a virtual machine file system VMFS a new technology file system NTFS and or other appropriate file system and may be communicably coupled with one or multiple storage resources each including one or more disk drives and or other storage volumes over one or more storage area networks SAN and or other appropriate network such as a LAN WAN etc.

The directors may be located in close physical proximity to each other and or one or more may be remotely located e.g. geographically remote from other directors as further discussed elsewhere herein. It is possible for the SANs to be coupled together and or for embodiments of the system described herein to operate on the same SAN as illustrated by a dashed line between the SAN and the SAN . Each of the directors may also be able to intercommunicate with other directors over a network such as a public or private network a peripheral component interconnected PCI bus a Fibre Channel FC network an Ethernet network and or an InfiniBand network among other appropriate networks. In other embodiments the directors may also be able to communicate over the SANs and or over the networks and it is noted that the networks may be combined with and or incorporated with one or more of the SANs . Generally a SAN may be used to couple one or directors and or host devices with one or more storage devices in a manner that allows reconfiguring connections without having to physically disconnect and reconnect cables from and to ports of the devices. A SAN may be implemented using one or more switches to which the storage devices directors and or host devices are coupled. The switches may be programmed to allow connections between specific ports of devices coupled to the switches. A port that can initiate a data path connection may be called an initiator port while the other port may be deemed a target port.

Several of the directors may be clustered together at one or more sites and in which the sites are geographically distant from one another. The system described herein may be used in connection with a vSphere produced by VMware Inc. of Palo Alto Calif. and or a VPLEX product produced EMC Corporation of Hopkinton Mass. respectively. The system described herein may also be used in connection with a storage array product produced by EMC Corporation such as a Symmetrix product and or with a Vblock platform product produced by VCE Company LLC. Although generally discussed and illustrated in connection with embodiment for a distributed storage system the system described herein may generally be used in connection with any appropriate distributed processing or cloud computing system as further discussed elsewhere herein.

Each distributed cache manager may be responsible for providing coherence mechanisms for shared data across a distributed set of directors. In general the distributed cache manager may include a module with software executing on a processor or other intelligence module e.g. ASIC in a director. The distributed cache manager may be implemented in a single director or distributed across multiple intercommunicating directors. In certain aspects each of the directors may be embodied as a controller device or blade communicably coupled to one or more of the SANs that allows access to data stored on the storage networks. However it may be appreciated that a director may also be embodied as an intelligent fabric switch a hub adapter and or other appropriate network device and may also be implemented as a virtual machine as further discussed elsewhere herein. Because Locality Conscious Directory Migration LCDM is applicable to databases any suitable networked director may be configured to operate as an access node with distributed cache manager functionality. For example a distributed cache manager may be run on one or more desktop computers and or virtual machines with a network connection.

A distributed storage system may enable a storage device to be exported from multiple distributed directors which may be either appliances or arrays for example. In an active active storage system if there are multiple interfaces to a storage device each of the interfaces may provide equal access to the storage device. With an active active storage system hosts in different locations may have simultaneous write access to mirrored exported storage device s through a local front end thereof i.e. a director . The distributed storage system may be responsible for providing globally consistent and coherent data access. The system described herein may be used in connection with enabling the distributed storage system to meet consistency guarantees and maximize data access even in response to failures that may cause inconsistent data within the distributed storage system.

Using virtualization software one or more physical servers may be subdivided into a plurality of virtual machines. As further discussed elsewhere herein a virtual machine VM is a software implementation of a machine that executes programs like a physical machine. Virtualization software allows multiple VMs with separate operating systems to run in isolation on the same physical server. Each VM may have its own set of virtual hardware e.g. RAM CPU NIC etc. upon which an operating system and applications are loaded. The operating system may see a consistent normalized set of hardware regardless of the actual physical hardware components. The term virtualization software is used herein to generally refer to any and all software that supports the operation of one or more VMs. A number of virtualization software products exist including the VMware product family provided by VMware Inc. of Palo Alto Calif. A benefit of providing VMs is the ability to host multiple unrelated clients in a single physical server. The virtualization software may maintain separation of each of the clients and in which each of the clients separately access their own virtual server s . Other virtualization products that may be used in connection with the system described herein include Hyper V by Microsoft Corporation of Redmond Wash. public license virtualization products and or other appropriate virtualization software.

Configuring and deploying VMs is known in the field of computer science. For example U.S. Pat. No. 7 577 722 to Khandekar et al. entitled Provisioning of Computer Systems Using Virtual Machines which is incorporated herein by reference discloses techniques for configuring and deploying a VM according to user specifications. VMs may be provisioned with respect to any appropriate resource including for example storage resources CPU processing resources and or memory. Operations of VMs may include using virtual machine images. A VM image is the state of the virtual machine as it resides in the host s memory. The VM image may be obtained for an operating VM and transferred to another location where the VM continues execution from the state defined by the virtual machine image. In this way the VM image may be a snapshot of an execution state of a program by a VM that may be moved between different locations and processing thereafter continued without interruption.

As discussed in detail elsewhere herein in a virtualized environment a virtual center an example of which may be a vCenter product produced by VMware Inc. of Palo Alto Calif. may provide a central point of control for managing monitoring provisioning and migrating virtual machines. Data storage and management may be provided in connection with one or more data centers coupled by a network. Virtual centers may operate to control virtual machines in the data centers and for example in connection with cloud computing. A virtual center may further include a virtual data center VDC that provides logical control and management of data storage in a data center. A virtual center may be used in connection with an infrastructure platform that provides an integrated package of components to provide network compute and or storage services for use in a virtualized environment. One example of an infrastructure platform is a Vblock product produced by VCE Company LLC. It is noted that where the term Vblock is used herein it may also be generally understood as including and referring to any other appropriate software and or component packages that provide network compute and or storage services for use in a virtualized computing environment. Management of a Vblock may be provided by an appropriate software element. For example EMC s Ionix Unified Infrastructure Manager UIM may be integrated with Vblock and provide a management console for management of the Vblock package in accordance with operations and functionality further discussed in detail elsewhere herein.

The data centers may contain any number of processors and storage devices that are configured to provide the functionality described herein. The data centers may be configured similarly to each other or may be configured differently. The network may be any network or similar mechanism allowing data communication between the data centers . In an embodiment herein the network may be the Internet and or any other appropriate network and each of the data centers may be coupled thereto using any appropriate mechanism. In other embodiments the network may represent a direct connection e.g. a physical connection between the data centers .

In various embodiments VMs may be migrated from a source one of the data centers to a destination one of the data centers . VMs may be transferred from one data site to another including VM mobility over geographical distances for example for reasons of disaster avoidance load balancing and testing among other reasons. For a discussion of migrating VMs reference is made to U.S. patent application Ser. No. 12 932 080 to Meiri et al. filed Feb. 17 2011 entitled VM Mobility Over Distance and U.S. patent application Ser. No. 13 136 359 to Van Der Goot filed Jul. 29 2011 entitled Active Active Storage and Virtual Machine Mobility Over Asynchronous Distances which are incorporated herein by reference and are assigned to the assignee of the present application. A product such as EMC s VPLEX Metro and or VPLEX Geo may be used to manage VMs and other storage resources. VPLEX allows for logical storage units e.g. logical unit numbers LUNs provisioned from various storage arrays to be managed through a centralized management interface. Products like VPLEX Metro or Geo provide for data mobility availability and collaboration through active active data over synchronous and asynchronous distances with provide for the ability to non disruptively move many VMs. It is noted that where the term VPLEX is used herein it may also generally be understood as including and referring any other appropriate software and or component packages that provide for coordinating and or federating resources of disparate systems as a single pool of virtual resources in particular for example a single pool of virtual storage.

In an embodiment each host cluster may include ESX hosts in a vSphere cluster and director cluster may include directors in a VPLEX cluster. Front end networks may connect through host links to the host clusters and through front end links to the director clusters . One or more back end networks may connect through back end links to the director clusters and through array links to the disk arrays . In an embodiment the front and back end networks may be Fibre Channel networks. The front end networks allow the hosts or VMs running therein to perform I O operations with the host clusters while the back end networks allow the directors of the director clusters to perform I O on the disk arrays . One or more host networks such as vSphere Ethernet networks connect the ESX hosts in host clusters . One or more director networks connect the directors of the director clusters

Various types of failures including network failures within a cluster may result in behaviors that are further discussed elsewhere herein. It should be noted that the host cluster e.g. vSphere cluster may be connected in such a way that VMs can keep their network e.g. IP FC IB addresses when migrating between clusters for example by means of a vLan or an open vSwitch . In an embodiment VPLEX may be used and configured to expose one or more distributed volumes from both VPLEX director clusters. A virtual machine file system VMFS may be created on top of these distributed volumes allowing VMs that migrate between the sites to see the same file system in either site. It is also noted that as illustrated and according to various embodiments each site may include redundancies in hosts directors and links therebetween.

In some embodiments the system described herein may be used in connection with a first set of one or more data centers that are relatively active primary data centers and a second set of one or more data centers that are relatively inactive failover data centers . The first set of data centers and second set of data centers may both be used for application reading and writing but the first set of data centers may be more active and or include more response time sensitive applications than the second set of data centers. Each of the relatively active data centers in the first set of data centers may use at least one corresponding data center in the second set of data centers for failover operations. It should also be noted that in addition to the active active system described herein the system described herein may also be used in active passive functioning as appropriate or desired.

I O access may be provided to distributed volumes in an active active system with two sites separated by an asynchronous distance. For asynchronous operation a write operation to cluster at a remote site may be acknowledged as soon as a protection copy is made within the cluster. Sometime later the write data is synchronized to the remote site. Similarly writes to the remote site are later synchronized to a cluster at the local site. Software or other controllers at the director clusters such as VPLEX may present the same image of the data on either cluster to provide a cache coherent view of the data. In an embodiment this may be achieved by fetching data that has not yet been replicated between a source and destination site i.e. dirty data as compared with clean data which has been copied and is protected on multiple sites over the inter cluster link on an as needed basis. In the background the controller VPLEX may synchronize the oldest dirty data between the clusters.

The above operations may work as long as the inter cluster network is available. If the inter cluster link fails both clusters may contain dirty data that is unknown by the respective remote clusters. As a consequence of this failure the director cluster may rollback the image of the data to a write order consistent point. In other words the director cluster may rollback the image of the data to a point where it knows the data that is available on both clusters or to a time where the write data was exchanged between both sites. The director cluster may also guarantee rollback to an image of the disk or volume that is write order consistent which means that if the data of a specific write is available on the volume all data of writes that were acknowledged before preceded that write should be present too. Write order consistency is a feature that allows databases to recover by inspecting the volume image. As noted elsewhere herein known techniques may provide write order consistency by grouping writes in what are called deltas and providing the consistency on a delta boundary basis see e.g. U.S. Pat. No. 7 475 207 to Bromling et al. .

Suspend resume migration processing may involve suspending a VM in the source site and resuming that VM in the destination site. Before the suspended VM is resumed all dirty data for the affected VMFS may be synchronized from the source VPLEX cluster to the destination VPLEX cluster and the preference i.e. winner site for the distributed volume may be changed from the source cluster to the destination cluster. The preference attribute may be related to a VPLEX consistency group that contains one or more VMs. Hence the VM may be in a consistency group of its own or all VMs in a consistency group may be migrated together. To know when the synchronization of VPLEX s dirty cache is finished the customer may map the VMFS to a distributed volume.

Failures may also occur when a VM is migrated while performing I O operations. In an example the migration of a VM during I O operations may be referred to herein as vMotion and may be facilitated by a VMware product called vMotion. In a director network failure situation during VM migration both the source cluster directors and the destination cluster directors may contain dirty data. A similar problem may occur when multiple VMs have to be migrated together because they all access one VMFS volume. In an embodiment this problem could be alleviated by suspending the restart of the VM on the destination cluster until the director cluster e.g. VPLEX cluster cache has been synchronized however such operation may cause undesirable delays. For further detailed discussion of specific system behaviors in connection with different types of failure scenarios reference is made to U.S. patent application Ser. No. 13 136 359 to Van Der Goot as cited elsewhere herein.

In various embodiments the system described herein may be used in connection with performance data collection for data migration and or data mirroring techniques using a SAN. Data transfer among storage devices including transfers for data migration and or mirroring functions may involve various data synchronization processing and techniques to provide reliable protection copies of data among a source site and a destination site. In synchronous transfers data may be transmitted to a remote site and an acknowledgement of a successful write is transmitted synchronously with the completion thereof. In asynchronous transfers a data transfer process may be initiated and a data write may be acknowledged before the data is actually transferred to directors at the remote site. Asynchronous transfers may occur in connection with sites located geographically distant from each other. Asynchronous distances may be distances in which asynchronous transfers are used because synchronous transfers would take more time than is preferable or desired. Examples of data migration and mirroring products includes Remote Data Facility RDF products from EMC Corporation. For further discussion of RDF and the use thereof in data recovery and storage techniques see for example U.S. Pat. No. 5 742 792 to Yanai et al. entitled Remote Data Mirroring and U.S. Pat. No. 7 779 291 to Yoder et al. entitled Four Site Triangular Asynchronous Replication which are incorporated herein by reference.

Techniques involving the configuration and use of storage area networks including virtual storage area networks are described for example in U.S. Pat. No. 8 028 062 to Wigmore et al. entitled Non Disruptive Data Mobility Using Virtual Storage Area Networks With Split Path Virtualization which is assigned to EMC Corporation and is incorporated herein by reference that discloses techniques for the creation of a SAN centric storage virtualization layer that allows data mobility and migration without disruption to one or more hosts servers attached to the SAN. Reference is also made to U.S. Pat. No. 7 441 023 to Benjamin et al. entitled Method and Apparatus for Modeling and Analyzing MPLS and Virtual Private Networks U.S. Pat. No. 7 720 003 to Benjamin et al. entitled Model Based Method and Apparatus for Determining MPLS Network Properties and U.S. Pat. No. 7 783 778 to Benjamin entitled Model Based Method and Apparatus for Determining Virtual Private Network Topologies which are assigned to EMC Corporation and are all incorporated herein by reference that disclose various techniques for discovering and identifying network properties and topologies in a network represented by a model.

The selection and or identification of the I O path may be performed according to multiple selection factors and using known path selection techniques. Reference is made for example to U.S. Pat. No. 7 688 753 to Zimran et al. entitled Selection of a Data Path Based on One or More Performance Characteristics of a Computer System which is assigned to EMC Corporation and is incorporated herein by reference and which discloses techniques for data path selection based on performance characteristics of a computer system using a path selection module. Reference is also made to U.S. Pat. No. 6 434 637 to D Errico entitled Method and Apparatus for Balancing Workloads Among Paths in a Multi Path Computer System Based on the State of Previous I O Operations which is assigned to EMC corporation and is incorporated herein by reference and which discloses techniques for selecting one of at least two I O data paths for transmission of the next I O operation based upon a state of previously assigned I O operations queued for transmission over the I O paths.

It is further noted that the system described herein may be used in connection with simulation of data storage systems for evaluation purposes. For example I O data paths of simulated storage system configurations may be evaluated to determine preferred configurations and or identify potential problems of a possible I O data path and elements thereof. Suitable data storage system simulation tools that may be used in connection with the system described herein may include systems and methods like that disclosed in U.S. Pat. No. 7 392 360 to Aharoni et al. entitled Method and System for Capacity Planning and Configuring One or More Data Storage Systems U.S. Pat. No. 7 292 969 to Aharoni et al. entitled Method and System for Simulating Performance on One or More Storage Systems and U.S. patent application Ser. No. 13 065 806 to Smirnov et al. filed Mar. 30 2011 entitled Write Pacing Simulation Tool which are all assigned to EMC Corporation and which are all incorporated herein by reference.

In the illustrated example the console of the controller is shown displaying a SAN and or other appropriate network topology corresponding to one or more I O data paths for a HOST N of the network. The console may include a RESTful representational state transfer interface accessible via the Internet. The console may include a graphical section that shows a visual topology representation of the network and components thereof. For example the section graphical displays the host HOST N coupled via switches to one or more storage devices arrays . Section may display map details of the network elements for example performance measures for particular elements of I O data paths in graph form as well as in text or tabulated form . The performance measures displayed may include those discussed elsewhere herein including workload and performance characteristics such as CPU utilization memory utilization for the host and IOps I O in Mb per sec response time in ms throughput in KBps and queue length for host devices switches arrays etc. among other appropriate measures.

Section indicates that multiple types of detail and performance measures may be displayed in various forms for the application host including features corresponding to the I O data path attributes performance capacity alerts connectivity path details virtual machine VM information data stores configuration changes discovery details and group details. A link button may be provided to view edit performance collection settings. Additionally according to an embodiment portion indicates that historical or past performance data collection may be maintained by the system. For example a user may view past performance data collection from the prior day prior week prior month and or any other customizable date range.

The controller according to the system described herein may provide for analysis and display of alerts for root causes symptoms and impacts via a single application control. In an embodiment the controller may be part of a data collection tool that may provide for collection various types of data concerning performance characteristics of the storage devices and or other elements of the network including I O data paths that may be monitored according to various statistics and measures. Performance characteristics may include for example performance data capacity data discovery data including configuration data and or topology data among other characteristics. Examples of various performance characteristics may include CPU utilization memory utilization for the host and IOps I O in Mb per sec response time in ms throughput in KBps discovered hosts of an I O data path queue length for host devices whether the hosts are virtual e.g. running as guest operating system or a virtual machine VM or not the hosts IP addresses operating systems and versions thereof whether the host is in a group or cluster and or other appropriate performance characteristics.

In various embodiments the component may be an application installed on an application host or other computer providing network administration and or management functionality and or may be installed on one or more of hosts coupled to the network. In an embodiment the component may be used in connection with EMC Ionix products including the EMC Ionix Unified Infrastructure Manager UIM specifically a UIM Operations UIM O component and or EMC Ionix Storage Insight for Availability produced by EMC Corporation of Hopkinton Mass. Other storage management products that may be used in connection with the system described herein may include for example EMC s ProSphere product and or a Vblock platform product produced by VCE Company LLC.

In an embodiment the statistics used by the component according to the system described herein may be gathered by the component according to the data collection techniques discussed elsewhere herein. Reference is made for example to U.S. patent application Ser. No. 13 335 316 to Lim et al. filed Dec. 22 2011 and entitled Path Performance Data Collection which is assigned to EMC Corporation and is incorporated herein by reference that discloses controlled tuning performance data collection provided through a single application controller of a path performance data collection tool and may be used in connection with the system described herein. The performance data collection may be turned on or off from the application host running the tool via a user interface. Lim s tool may automatically e.g. without user intervention update performance data collection characteristics as the application host I O data path changes according to user controlled settings but without requiring further user intervention during the updating. Turning on path performance data collection on the application host via the user interface may automatically set up synchronized performance data collection for all managed objects within an I O data path.

In various embodiments data used in connection with the system described herein may obtained using other data collection devices and mechanisms including products produced by EMC Corporation such as the EMC Workload Analyzer WLA the Symmetrix Performance Analyzer SPA the Symmetrix CLI statistics collection daemon STP the Storage Configuration Advisor SCA and or the ProSphere Discovery Engine among other possible collection devices applications and or appliances. Reference is made for example to U.S. Pat. No. 6 622 221 to Zahavi entitled Workload Analyzer and Optimizer Integration which is assigned to EMC Corporation and is incorporated herein by reference and which discloses techniques used in connection with evaluating the performance of proposed device swap plans in accordance with past performance data collected.

According to the system described herein if the user wants to obtain further information concerning the alert C the user may request further information for example using one or more query buttons and or other appropriate query mechanism submitted via the controller . In an embodiment the user may query the controller to determine the RootCausedBy information for the alert C and determine by traversal of an alert relationship path as further discussed elsewhere herein that alert A is the root cause of alert C and obtain alert A s properties. It is noted that there is a possibility that alert C is caused by multiple alerts in which case the controller may traverse all possible alert relationship paths as further discussed elsewhere herein and determine all root cause alerts of C. A similar algorithm may be applied in a different direction for example to determine impact alerts caused by alert C.

TABLE 1 below shows alert relationship information that may be obtained by the controller based on processing according to the system described herein for a use case example in which A causes B. B causes C. C impacts I1 and A impacts I2.

In the above noted example the controller determines that R1 is a root cause alert of alert S3 and may store this information in a list e.g. a RootCauseList. The controller may then pop alert S2 from the stack and traverse a path through tree beginning with node S2. In this example the controller will traverse S2 back through S1 to R1 to determine that R1 is the root cause via the S2 path. The controller may determine from the RootCauseList that R1 has already been determined as a root cause for the alert S3. If the controller determines that there are no further alerts from the stack to be traversed the controller may then return to the requesting user the RootCauseList alerts for S3 specifically return that R1 is the root cause alert for alert S3. It is noted that a similar algorithm as that noted above may be applied in reverse to determine impact alerts caused by a particular alert. For example for a received alert S4 for which the user desires to know impact alerts caused thereby the controller may traverse the tree in a manner similar to that noted discussed above but in a different direction e.g. paths beginning with alerts S5 and S8 respectively to determine that impact alerts I2 and I3 e.g. which may be stored in an ImpactCausedList are caused by the alert S4.

After the step processing proceeds to a step where an alert relationship path is determined for traversal. For example to determine a root cause of a received alert the beginning of the traversal path will be an immediately preceding cause of the received alert. In the case where there are more than one preceding causes all but one of the causes will be temporarily suspended e.g. pushed onto a stack and or other software structure for later retrieval and the remaining cause will indicate the beginning of the path for traversal. A similar but reverse process may be used for obtaining a desired impact of a received alert. After the step processing proceeds to a step where the selected alert relationship path is traversed. After the step processing proceeds to a step where desired alert relationship information e.g. root cause or impact is obtained and stored.

After the step processing proceeds to a test step where it is determined if another path exists for received alert. For example if the received alert was caused by two different alerts and a first one of the causality alerts was initially used to establish the path for traversal then the second of the causality alerts may be used to establish another path. Accordingly if at the test step it is determined that another path exists then processing proceeds back to the step to determine the next alert relationship path for traversal. For example the next causality alert may be popped from the software stack where it has been temporarily stored. If at the test step it is determined that another path does not exist for the received alert then processing proceeds to a step where the obtained and stored alert relationship information is displayed using a console of a controller for example responsive to a user s query. After the step processing is complete.

Under various circumstances it is desirable to send alerts collected for cloud computing system to external systems. For example alerts may be sent using a syslog protocol. A syslog message however may be limited in size e.g. 1024 bytes see e.g. RFC 5424 The Syslog Protocol which is incorporated herein by reference . Known approaches to deal with an alert message that exceeds the permitted syslog message size is to truncate the alert message and transmit as a truncated syslog message and or to cut the alert message into multiple parts physically based only on the length of the message and send as multiple syslog messages. In a further technique provided by EMC s Unified Infrastructure Manager Operations UIM O product efficient partitioning and or fragmenting of a syslog message may be provided based on the use of various types of message identifications and or certain other information in which by virtue of the partitioning and fragmenting an alert message regardless of its length may be sent using the syslog protocol. Specifically using the message identifications and or the other information the alert syslog message partitioned by the UIM O product may be reconstructed at the external system based on the multiple partitioned fragmented syslog messages. The message IDs and or other information of the partitioned fragmented messages may be further used in connection with checking data integrity of the received messages.

In an embodiment an algorithm for partitioning fragmenting syslog messages may be as follows. If an alert message exceeds a specified size e.g. 1024 bytes the system described herein may divide partition the alert into two or more smaller messages partitions . The alert message may be partitioned so that each part is independent. Each partitioned message may include the following Header Alert ID and Alert Data. The Header and Alert ID may be the same each partition. Where the Alert Data is too large for any one alert message partition any such alert message partition may then be fragmented such that each fragment contains a portion of the Alert Data.

The following information may be added to each fragmented message after the header message ID a unique number for the alert message fragmentation count specifying the number of fragments into which the alert message has been split and fragmentation ID identifying the fragment in the sequence . The message ID and fragmentation count may be the same for all fragments of the alert message or message partition. The message ID may be auto generated based on the alert message or message partition. A client of a syslog service may construct the original alert e.g. Vblock alert message from the partition fragmented messages. It is further noted that the Message ID may be cyclic redundancy check CRC information that may be used to check data integrity.

In some cases it is advantageous to transmit alert messages using mechanisms other than the syslog protocol like that used by prior versions of EMC s UIM O product. According to the system described herein cloud computing system e.g. Vblock alert messages may be advantageously and efficiently sent to external and or remote systems using mechanisms in addition to and or other than the syslog protocol. In an embodiment as further discussed in detail elsewhere herein alert messages may be sent using a simple network management protocol SNMP trap notification see e.g. RFC 1215 A Convention for Defining Traps for use with the SNMP which is incorporated herein by reference . It is noted that the system described herein may further be used with any other appropriate protocol including for example http https and or an e mail protocol. Further the system described herein also provides for additional alert attributes to be used in connection with sending cloud computing system alert messages to external systems using the syslog protocol and or other mechanisms. In an embodiment the system described herein may be used in connection with sending Vblock alert messages to external systems however in other embodiments the system described herein may be used in connection with appropriate storage devices platforms other than specifically Vblock.

According to an embodiment of the system described herein an appropriate version of SNMP such as v1 v2c and or v3 along with an associated Management Information Base MIB may be used to provide for trap notification in connection with the sending of alert messages to an external system. In trap based notification each agent on a managed device may notify a device manager without solicitation in connection with an event involving the managed device. This is done by sending an alert message known as a trap of the event. After the manager receives the message the manager may display the message and choose to take an action based on the event. Trap based notification eliminates the need for unnecessary SNMP polling requests.

According to an embodiment of the system described herein if the alert message is too large and or requires partitioning and or fragmenting for another reasons the alert message may be partitioned and or fragmented. First the alert message may be partitioned in multiple messages and an Alert ID and attribute information may be sent in the first message and fragmentation information may be added to each fragment of alert data contained in the message partitions. In an embodiment the following information may be added to each fragment of alert data after the header message ID a unique number for the alert message fragmentation count specifying the number of fragments into which the alert message has been split fragmentation ID identifying the fragment in the sequence . The message ID and fragmentation count may be the same for all fragments of an alert message. For example an alert message may be partitioned into three smaller messages. The second message may contain a long attribute value pair that requires fragmentation. UIM O may then split the second message into two fragments. Each fragment may include the message ID fragmentation count and fragmentation ID. In various embodiments the syntax of the alert message may correspond to syslog messages and or SNMP trap messages.

The following is a more detailed description of the alert attributes identified in TABLE 2 that may be used in an alert message according to the system described herein 

According to the system described herein to send alert messages to an external system one or more alert messages received and processed by a management component such as EMC s UIM O of a cloud computing system may be sent to the external system according to an alert forward configuration. In various embodiments the alerts may be sent as SNMP trap messages and or syslog messages to the external systems. The external systems may be specified in a forwarding screen of an interface of the management component. Alert messages may be sent to external systems as controlled by a user and or may be configured to be sent automatically at periodic intervals e.g. every 60 seconds . In other embodiments the alert messages may be sent in real time immediately after the alerts are processed.

Parameters of the alert forwarding configuration may be set in connection with controlling when and under what conditions alert messages are sent to an external system. Multiple configurations using same or different protocols can be set for single or multiple external systems. For example different alert forwarding configuration may be established having parameters such as the name of the alert forwarding configuration the hostname or IP address of the system receiving alerts and the UDP port number on the target host where the SNMP traps or syslog messages are to be forwarded. In various embodiments the port number may be between 0 and 65535 inclusive. The configurations may be modified according to whether the alert message is to be sent as an SNMP trap message or a syslog message. Different ports may be used according to an SNMP trap message or syslog message. For example for an SNMP trap message if unspecified a default SNMP port a particular number e.g. port number 162 may be used whereas for a syslog message another default value port number e.g. port number 514 may be used.

The severity level configurations for forwarded alerts may also determine when and under what conditions alerts are forwarded. For example if a severity configuration level is set to a particular level then all the alerts that are at or above that severity level may be forwarded. That is if the severity configuration level is set to WARNING then all alerts that are at a severity of WARNING ERROR or CRITICAL may be sent to the external system. Further in various embodiments existing alerts may be forwarded in addition to new alerts that may occur while in other embodiments only new alerts may be sent. In an embodiment there may be four severity levels for alerts 1 critical 2 major error 3 minor warning and 4 information such as used in the UIM O system. Depending however on the protocol type for the sending of the alert message mapping may be required between the alert message security level e.g. at the UIM O system and the message protocol alert levels.

In an embodiment severity mapping may be used for syslog messages since there are eight levels of severity defined in syslog protocol but as noted may be only four in UIM O. The mapping between the UIM O alert severity levels and the syslog message severity levels is described in the following TABLE 3 

In another embodiment severity mapping may be used for SNMP trap messages and it is noted that the use of severity level designations are similar as between SNMP trap and UIM O. The mapping between the UIM O alert severity levels and the SNMP trap message severity levels is described in the following TABLE 4 

After the step processing proceeds to a step where the alert message of the determined type is prepared according to an appropriate syntax and including particular alert attributes as further discussed elsewhere herein. After the step processing proceeds to a step where it is determined whether the alert message requires partitioning and or fragmenting in accordance with the system described herein. If not then processing proceeds to a step where the alert message is sent to the external system. After step processing is complete.

If at the test step it is determined that the alert message of the specified type requires partitioning and or fragmentation in order to be sent to the external system then processing proceeds to a step where partition and or fragmentation processing is performed. As further discussed elsewhere herein an alert message may partitioned into multiple messages each having a header and alert ID corresponding to an alert and one or more of the partitioned messages may be fragmented into multiple fragments that each contain a portion of fragmented alert data with alert attribute information along with fragmentation identification information including a message ID fragmentation count and fragmentation ID. After the step processing proceeds to a step where the partitioned and or fragmented alert messages are transmitted to the external system which may collect the partitioned fragmented alert message and reconstruct the original alert message thereby permitting transmission of an alert message that without the above noted processing would have been too large for normal message transmission according to syslog message size restrictions for example. It is noted that according to various embodiments of the system described herein the alert message may be reconstructed at the external system even in situations where the partitioned or fragmented messages are received out of order. After the step processing is complete.

Various embodiments discussed herein may be combined with each other in appropriate combinations in connection with the system described herein. Additionally in some instances the order of steps in the flowcharts flow diagrams and or described flow processing may be modified where appropriate. Further various aspects of the system described herein may be implemented using software hardware a combination of software and hardware and or other computer implemented modules or devices having the described features and performing the described functions. Software implementations of the system described herein may include executable code that is stored in a computer readable medium and executed by one or more processors. The computer readable medium may include a computer hard drive ROM RAM flash memory portable computer storage media such as a CD ROM a DVD ROM a flash drive and or other drive with for example a universal serial bus USB interface and or any other appropriate tangible or non transitory computer readable medium or computer memory on which executable code may be stored and executed by a processor. The system described herein may be used in connection with any appropriate operating system.

Other embodiments of the invention will be apparent to those skilled in the art from a consideration of the specification or practice of the invention disclosed herein. It is intended that the specification and examples be considered as exemplary only with the true scope and spirit of the invention being indicated by the following claims.

