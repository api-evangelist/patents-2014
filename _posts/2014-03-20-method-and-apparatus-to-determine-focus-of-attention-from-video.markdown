---

title: Method and apparatus to determine focus of attention from video
abstract: A method and system include identifying, by a processing device, at least one media clip captured by at least one camera for an event, detecting at least one human object in the at least one media clip, and calculating, by the processing device, a region in the at least one media clip containing a focus of attention of the detected human object.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09445047&OS=09445047&RS=09445047
owner: Google Inc.
number: 09445047
owner_city: Mountain View
owner_country: US
publication_date: 20140320
---
This disclosure relates to the field of media content sharing and in particular to detect a region of interest using attention focus detected from multiple streams of video or a set of images.

Multiple cameras may be deployed at an event to simultaneously capture video streams or images from different angles and transmit the captured video streams or images to an editing device. For convenience video streams and images are collectedly referred to as media clips. Human operators may act as the editor to decide which stream of video contains the region of interest e.g. the most salient object or person and select the best video feed among the multiple video streams for any given moment. Lower cost systems such as video conference systems can try to accomplish video editing automatically without the human editor. Currently some of the automated systems try to determine the best video feed based on sound volumes. For example the automated systems may select the video stream that has the highest sound volume as the one that captures the region of interest. However the sound volume may not be a good indicator when sound signals are amplified by sound amplification systems and it does not provide any information as to which particular region of a video stream is the region of interest. Other systems use the amount of motion in video streams as an indicator of region of interest. However the amount of motion may not be reliable for certain situations. For example the speaker at a meeting may not move much but is nevertheless the center of attention for other individuals present at the meeting.

The following is a simplified summary of the disclosure in order to provide a basic understanding of some aspects of the disclosure. This summary is not an extensive overview of the disclosure. It is intended to neither identify key or critical elements of the disclosure nor delineate any scope of the particular implementations of the disclosure or any scope of the claims. Its sole purpose is to present some concepts of the disclosure in a simplified form as a prelude to the more detailed description that is presented later.

Implementations of the disclosure may include a method including identifying by a processing device at least one media clip captured by at least one camera for an event detecting at least one human object in the at least one media clip and calculating by the processing device a region in the at least one media clip containing a focus of attention of the detected human object.

Implementations of the disclosure may include a system including a memory and a processing device communicatively coupled to the processing device to retrieve at least one media clip captured by at least one camera for an event detect at least one human object in the at least one media clip calculate a region in the at least one media clip containing a focus of attention of the detected human object.

Indicators such as sound volumes and amount of motions are not reliable indicators of the best feed among multiple video streams with respect to a region of interest at an event e.g. a meeting a conference a stage performance a sports game etc. . Further even if these indicators may have detected the correct video stream they only characterize that the stream is the interesting video stream but do not determine which region in the video frames is the region of interest.

Aspects of the present disclosure address the above deficiencies by providing a mechanism that can reliably detect the best feed from multiple media clips and the region of interest within the best feed. In particular this mechanism can detect human objects and based on attributes of the detected human objects it can detect a focus of attention of these human objects. The focus of attention is then used to determine a media clip and optionally which sub region of that clip that can best reflect the focus of attention of the human objects. For convenience implementations of the disclosure are discussed with respect to video streams. However it is understood that the implementations without limitation are equally applicable to media clips that may include one or more images.

For example at a video conference multiple cameras can be used to record the conference from different locations and angles. In this scenario the gazes of the conference attendees may be detected in the respective video stream and may further be used to detect the focus of attention of these attendees. The focus of attention may be detected from features extracted from each video stream. The video stream that best reflects the focus of attention may be selected from the multiple video streams. Based on this video stream the area of maximum interest can be identified and used for example for automatic video editing metadata creation search personalization etc.

In another example one or more images of a scene are captured of an event such as a birthday party or a product presentation. In this scenario gazes of the human objects can be used to determine which region in the image s is the focus of attention which can then be used to determine the identity of the person or the object of interest using machine learning techniques.

The client devices A through Z may each include computing devices such as personal computers PCs laptops mobile phones smart phones tablet computers netbook computers etc. In some implementations client device A through Z may also be referred to as user devices. 

In one example server may be part of a content sharing platform and users of client devices A through Z may retrieve and watch contents stored on the content sharing platform. In this example each client device may include a media viewer not shown . In one implementation the media viewers may be applications that allow users to view content such as images videos web pages documents etc. For example the media viewer may be a web browser that can access retrieve present and or navigate content e.g. web pages such as Hyper Text Markup Language HTML pages digital media items etc. served by a web server. The media viewer may render display and or present the content e.g. a web page a media viewer to a user. The media viewer may also display an embedded media player e.g. a Flash player or an HTML5 player that is embedded in a web page e.g. a web page that may provide information about a product sold by an online merchant .

In another example the media viewer may be a standalone application e.g. a mobile app that allows users to view digital media items e.g. digital videos digital images electronic books etc. .

In yet another example the media viewer of client devices A through Z may include software modules that allow client devices A through Z to upload user generated contents to the content sharing platform via network . For example client devices A through Z may include hardware capabilities such as microphones and cameras to record media contents audio video clips . Further the media view of client devices A through Z may include a software plug in that allows a user to select and upload media contents to the content sharing platform .

The media viewers may be provided to the client devices A Z by the content sharing platform. For example the media viewers may be applications that are downloaded from the content sharing platform or a third party app store.

In general functions described in one implementation as being performed by the server can also be performed on a different computer system e.g. client device in other implementations if appropriate. In addition the functionality attributed to a particular component can be performed by different or multiple components operating together. The server can also be accessed as a service provided to other systems or devices through appropriate application programming interfaces and thus is not limited to use in websites.

In one implementation the server may include one or more computing devices such as a rackmount server a router computer a server computer a personal computer a mainframe computer a laptop computer a tablet computer a desktop computer etc. data stores e.g. hard disks memories databases networks software components and or hardware components that may be used to automatically edit media items provided by users of client devices create metadata for media items provided by users of client devices provide users with access to media items including for example allowing a user to consume upload search for approve of like dislike and or comment on media items.

A media item may be consumed via the Internet and or via a mobile device application. For brevity and simplicity an online video also hereinafter referred to as a video is used as an example of a media item throughout this document. As used herein media media item online media item digital media digital media item content and content item can include an electronic file that can be executed or loaded using software firmware or hardware configured to present the digital media item to an entity. In one implementation the server may store hyperlinks to the media items stored on the data stores.

According to some aspects some or all client devices can include cameras that can simultaneously record an event. For example a client device can be a mobile phone or a tablet that has a built in camera capable of capturing a video. In addition or alternatively some client devices can be independent cameras e.g. camcorders or video camera recorders that can capture an event and provide resulting video clips to the server . For example video clips can be uploaded to the server directly from a camera e.g. via a WiFi connection or by connecting a camera to another user device e.g. a personal computer or a tablet and uploading video clips to the server through that other user device.

In one example cameras such as camcorders .A through .C may record a conference from different locations to capture different aspects of the conference. The locations and angles of camcorders may be fixed through the proceeding so that some camcorders may capture the speaker presenter and the presentation and other camcorders may capture audience. During the presentation the roles of the speaker and audience may switch. Thus the attention focus may change. For this reason at any given moment at least one of the camcorders .A through .C may best capture the attention focus of the conference.

Video clips .A through .C captured by camcorders .A through .C may be uploaded to the server as discussed above. Alternatively video clips .A through .C may be uploaded to a different system repository and be retrievable through hyperlinks stored in the server . For the convenience of discussion without limiting the scope of this disclosure it is assumed that video clips .A through .C are stored on the server .

Video clips .A through .C recorded at an event such as the conference may be in the form of raw video clips. Each video clip may include a sequence of video frames e.g. F . . . Fn where n is a frame index and optionally a sound track. Each video frame may include an N M array of pixels where N and M are the spatial resolution of the video clips. In one implementation the resolution of the video clips may match the resolution of camcorders .A through .C. Alternatively the resolutions of video clips may be different from that of camcorders. In one implementation since these video clips are recorded simultaneously at the event these video clips are temporally aligned. Therefore the frames that have the same index numbers in different video clips are recorded at the same time instant.

Server may include an attention focus detection subsystem to detect attention focus using video clips .A through .C. In one implementation subsystem may be a software application executed on a processing device such as a processor of server . Alternatively subsystem may be a hardware component or a combination of hardware and software that may detect attention focus for an event at any time instance. The output of subsystem may be an edited video that is generated by composing video frames selected from video clips .A through .C based on the detected attention focus. For example subsystem may detect that the attention focus is best captured in video clip .A for frame F through F best captured in video clip .B for frames F through F and again best captured in video clip .C for frame F. The resulting video clip may include frames from video clip .A AF AF frames from video clip .B BF BF and a frame from video clip .C CF . The resulting video clip may be made accessible to client devices .A through .Z so that users of these devices may view the edited video clip . Additionally subsystem may further detect the region that has the most salient area of interest in each video frame AF AF BF BF and CF . These regions may be cropped out or labeled and presented in video clip . In this way the user experience of the content sharing platform may be improved.

The detected attention focus and regions of salient interests may be used to automatically edit video clips and create metadata for video clips that facilitate search and personalization of video clips stored on content sharing platform .

Implementations of the disclosure may be further illustrated in the following video conference example. Although video conference is used as an example to illustrate implementations of the disclosure the disclosure is not limited to video conference and may also be applied to other types of recorded events such as concerts or sport events. In addition although some implementations of the disclosure are discussed in connection to camcorders any other cameras capable of capturing a video can be used to provide the functionality discussed herein.

In situations in which the systems discussed herein collect personal information about users or may make use of personal information the users may be provided with an opportunity to control whether the content server collects user information e.g. information about a user s social network social actions or activities profession a user s preferences or a user s current location or to control whether and or how to receive content from the content server that may be more relevant to the user. In addition certain data may be treated in one or more ways before it is stored or used so that personally identifiable information is removed. For example a user s identity may be treated so that no personally identifiable information can be determined for the user or a user s geographic location may be generalized where location information is obtained such as to a city ZIP code or state level so that a particular location of a user cannot be determined. Thus the user may have control over how information is collected about the user and used by the content sharing platform .

In one implementation setting is oriented in a coordinate system referred to as a world coordinate system. In one implementation the world coordinate system is a 2D coordinate system including x and y axes covering the ground floor of the conference room. In another implementation the coordinate system is 3D coordinate system not shown including x y and z axes covering the floor and height of the conference room. Each object in the conference room may be referenced with respect to coordinate system.

In one implementation locations of camcorders .A through .C with respect to the coordinate system may be determined in advance. For example the origin of coordinate system may be at a corner of the conference room. The locations of each camcorder may be measured manually or automatically using various location detection mechanisms.

In one implementation the orientation of each of the cameras may also be determined in advance. The orientation of a camera is the direction of the optical axis of the camera with respect to coordinate system . The orientations of cameras may be determined using various camera calibration methods. Once the locations and orientations of camcorders are determined a 2D or 3D scene may be reconstructed in the sense that a pixel in any video frame may be mapped to a 2D or 3D coordinate in coordinate system using stereo methods. In another implementation the locations of objects may be recovered using known object dimensions e.g. the size of body parts such as face size or eye distance in the case of human objects and their observed pixel size in the media clips. In yet another implementation their locations may be determined based on additional depth sensors. Thus locations of objects such as human objects within the scene may be determined by video frames of the calibrated camcorders .A through .C.

Implementations may further calculate an attention map representing a combined attention of the human objects in the scene. The attention map may be mapped to the reconstructed 2D or 3D scene with respect to coordinate system . In one implementation the attention map may be calculated on a 2D grid that overlays on the ground floor the 2D grid including cells that each includes an attention value. The attention value of each cell may indicate a measurement of the combined attention from human objects in the scene. For example the cells may be one foot by one foot cells on the ground floor.

In another implementation the attention map may be calculated on a 3D grid that fills the space of the 3D scene. Similarly the 3D grid includes cells that each including an attention value indicating a measurement of the combined attention from human objects in the scene. For example the cells may be cubes having sides of one foot length.

In one implementation the attention map may be calculated from locations and gaze directions of human objects such as .A through .E . illustrates the calculation of attention map according to an implementation of the disclosure. Referring to the ground floor may be partitioned into grid cells on which a combined attention value may be calculated and assigned. As discussed above a face detection module may first detect human faces in video frames at a time instance or those video frames with same index number among video clips such as .A through .C . The locations of human objects .A through .E may be determined based on the detected faces in video frames from different cameras such as camcorders .A through .C based on a rough estimate from a single view or stereo estimate from multiple views. Further gaze directions of human objects may be calculated from detected faces in the views of cameras. The calculated gaze directions may indicate where the human objects look at or the attention directions of human objects. In one implementation as shown in a viewing cone may be calculated based on the gaze direction and a determined field of view for each human object. A viewing cone includes multitude of directions from a viewer s perspective from which an intended object can be seen without distortions in which each direction may have an individual weight of attention. The central axis of the cone is the straight line passing through the apex about which the base has a rotational symmetry. In one implementation the determined field of view for human objects is a fixed angle. For example the field of view may be assumed to be 60 for each human object. The viewing cone may extend from the human object to boundaries of the scene.

A map of combined attention may be calculated based on the viewing cones of human objects. In one implementation each cell of grid cells may receive an attention score from a human object if the human object s viewing cone covers the cell. In one implementation the score from the human object is a constant positive value if the viewing cone overlaps with the cell and zero if the viewing cone does not overlap with the cell. The combined attention value for a cell is a total votes from all human objects whose viewing cones overlap with the cell.

In another implementation the score from the human object may be calculated as a function of a distance from the human object and or a distance measure of the cell from a central axis of the viewing cone. For example the score may be inversely proportional to the distance from the human object or the farther way the lower the score and inversely proportional to the distance measure from the central axis of the viewing cone. In another example the score may be modeled after a Gaussian Bell curve with respect to the distance to the central axis and with exponentially decreasing value along the direction from the apex of center axis. Thus the combined attention value for a cell is an accumulated score from all human objects as a function of their respective distance from the cell and angle of overlapping viewing cone.

In yet another implementation the score from the human object may also take into consideration its historical values. Since the score is calculated from video frames at a time instant it has therefore a history of score values that vary over time. Historical values may be used to place weights on the score to achieve temporal smoothing and consistent attention map. In one implementation the historical values may contribute an exponentially decaying weight to the score where the exponent may be adjustable to achieve different levels of smoothing.

Although implementations as shown in are illustrated using 2D scene examples the implementations may be equally applied to 3D scene in similar manners. Further implementations of the disclosure are not limited to grid cells. The attention map may be calculated using non grid methods. For example in one implementation particle filters or Gaussian mixtures may be used to trace viewing cones from human objects and calculate the combined attention value for any locations in the 2D or 3D scenes. The resulting attention map then may be in the form of continuous intensity map.

The calculated attention map may be used for video editing and other purposes. In one implementation subsystem as shown in may determine a video frame out of all video frames for that time instant as the one that best captures the attention focus and select that video frame to be part of the edited video clip. The selected video frames may be combined to form an interest edited video clip . In another embodiment subsystem may further crop or label a region of the attention focus out of the whole video frame and display the cropped out region of attention focus. This region may focus on the current object of interest. For example in one embodiment the region may focus on the face of a human object toward which other human objects are looking at or pay attention to .

In one implementation face recognition may be performed on the focused on human object and generate metadata that includes the identity of the recognized human object.

For simplicity of explanation the methods of this disclosure are depicted and described as a series of acts. However acts in accordance with this disclosure can occur in various orders and or concurrently and with other acts not presented and described herein. Furthermore not all illustrated acts may be required to implement the methods in accordance with the disclosed subject matter. In addition those skilled in the art will understand and appreciate that the methods could alternatively be represented as a series of interrelated states via a state diagram or events. Additionally it should be appreciated that the methods disclosed in this specification are capable of being stored on an article of manufacture to facilitate transporting and transferring such methods to computing devices. The term article of manufacture as used herein is intended to encompass a computer program accessible from any computer readable device or storage media. In one implementation the methods may be performed by the attention detection subsystem as shown in .

Referring to at subsystem of a content sharing platform may identify at least one video clip associated with an event. The video clip s may have been captured by at least one camera at an event such as a conference and uploaded and stored either on the sever or in a data store accessible to subsystem . In case of multiple video clips they may be captured at a scene from different locations and different angles. Further the camera s may be calibrated at locations and angles in a world coordinate system.

At subsystem may detect human objects in the video clip s . In one implementation subsystem may include a face detection component for detecting human faces in video frames of the video clip s . The detected human faces may represent human objects in the scene. Further subsystem may determine locations and gaze directions of these human objects. Based on the locations and gaze directions viewing cones for human objects may be formed as discussed above. The viewing cones may cover the area of attention from human objects.

At subsystem may calculate a focus of combined attentions from the detected human objects. In one implementation the scene may be partitioned into cells each of which may be assigned an accumulated score calculated based on the viewing cones from different human objects.

At subsystem may generate an edited video using the focus of combined attention from the human objects. In one implementation the edited video may be the one selected from video frames of the video clip s that best represents the focus of combined attentions form human objects. In another implementation regions that surround the focus of combined attentions may be cropped or labeled from the edited video for presentation to viewers of the edited video.

The exemplary computer system includes a processing device processor a main memory e.g. read only memory ROM flash memory dynamic random access memory DRAM such as synchronous DRAM SDRAM or Rambus DRAM RDRAM etc. a static memory e.g. flash memory static random access memory SRAM etc. and a data storage device which communicate with each other via a bus .

Processor represents one or more general purpose processing devices such as a microprocessor central processing unit or the like. More particularly the processor may be a complex instruction set computing CISC microprocessor reduced instruction set computing RISC microprocessor very long instruction word VLIW microprocessor or a processor implementing other instruction sets or processors implementing a combination of instruction sets. The processor may also be one or more special purpose processing devices such as an application specific integrated circuit ASIC a field programmable gate array FPGA a digital signal processor DSP network processor or the like. The processor is configured to execute instructions for performing the operations and steps discussed herein.

The computer system may further include a network interface device . The computer system also may include a video display unit e.g. a liquid crystal display LCD a cathode ray tube CRT or a touch screen an alphanumeric input device e.g. a keyboard a cursor control device e.g. a mouse and a signal generation device e.g. a speaker .

The data storage device may include a computer readable storage medium on which is stored one or more sets of instructions e.g. software embodying any one or more of the methodologies or functions described herein e.g. instructions of the annotation subsystem . The instructions may also reside completely or at least partially within the main memory and or within the processor during execution thereof by the computer system the main memory and the processor also constituting computer readable storage media. The instructions may further be transmitted or received over a network via the network interface device .

While the computer readable storage medium is shown in an exemplary implementation to be a single medium the term computer readable storage medium should be taken to include a single medium or multiple media e.g. a centralized or distributed database and or associated caches and servers that store the one or more sets of instructions. The term computer readable storage medium shall also be taken to include any medium that is capable of storing encoding or carrying a set of instructions for execution by the machine and that cause the machine to perform any one or more of the methodologies of the present disclosure. The term computer readable storage medium shall accordingly be taken to include but not be limited to solid state memories optical media and magnetic media.

In the foregoing description numerous details are set forth. It will be apparent however to one of ordinary skill in the art having the benefit of this disclosure that the present disclosure may be practiced without these specific details. In some instances well known structures and devices are shown in block diagram form rather than in detail in order to avoid obscuring the present disclosure.

Some portions of the detailed description have been presented in terms of algorithms and symbolic representations of operations on data bits within a computer memory. These algorithmic descriptions and representations are the means used by those skilled in the data processing arts to most effectively convey the substance of their work to others skilled in the art. An algorithm is here and generally conceived to be a self consistent sequence of steps leading to a desired result. The steps are those requiring physical manipulations of physical quantities. Usually though not necessarily these quantities take the form of electrical or magnetic signals capable of being stored transferred combined compared and otherwise manipulated. It has proven convenient at times principally for reasons of common usage to refer to these signals as bits values elements symbols characters terms numbers or the like.

It should be borne in mind however that all of these and similar terms are to be associated with the appropriate physical quantities and are merely convenient labels applied to these quantities. Unless specifically stated otherwise as apparent from the following discussion it is appreciated that throughout the description discussions utilizing terms such as segmenting analyzing determining enabling identifying modifying or the like refer to the actions and processes of a computer system or similar electronic computing device that manipulates and transforms data represented as physical e.g. electronic quantities within the computer system s registers and memories into other data similarly represented as physical quantities within the computer system memories or registers or other such information storage transmission or display devices.

The disclosure also relates to an apparatus for performing the operations herein. This apparatus may be specially constructed for the required purposes or it may include a general purpose computer selectively activated or reconfigured by a computer program stored in the computer. Such a computer program may be stored in a computer readable storage medium such as but not limited to any type of disk including floppy disks optical disks CD ROMs and magnetic optical disks read only memories ROMs random access memories RAMs EPROMs EEPROMs magnetic or optical cards or any type of media suitable for storing electronic instructions.

The words example or exemplary are used herein to mean serving as an example instance or illustration. Any aspect or design described herein as example or exemplary is not necessarily to be construed as preferred or advantageous over other aspects or designs. Rather use of the words example or exemplary is intended to present concepts in a concrete fashion. As used in this application the term or is intended to mean an inclusive or rather than an exclusive or . That is unless specified otherwise or clear from context X includes A or B is intended to mean any of the natural inclusive permutations. That is if X includes A X includes B or X includes both A and B then X includes A or B is satisfied under any of the foregoing instances. In addition the articles a and an as used in this application and the appended claims should generally be construed to mean one or more unless specified otherwise or clear from context to be directed to a singular form. Moreover use of the term an embodiment or one embodiment or an implementation or one implementation throughout is not intended to mean the same embodiment or implementation unless described as such.

Reference throughout this specification to one embodiment or an embodiment means that a particular feature structure or characteristic described in connection with the embodiment is included in at least one embodiment. Thus the appearances of the phrase in one embodiment or in an embodiment in various places throughout this specification are not necessarily all referring to the same embodiment. In addition the term or is intended to mean an inclusive or rather than an exclusive or. 

It is to be understood that the above description is intended to be illustrative and not restrictive. Many other implementations will be apparent to those of skill in the art upon reading and understanding the above description. The scope of the disclosure should therefore be determined with reference to the appended claims along with the full scope of equivalents to which such claims are entitled.

