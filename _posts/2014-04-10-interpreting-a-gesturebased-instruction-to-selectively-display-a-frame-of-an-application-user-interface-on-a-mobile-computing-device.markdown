---

title: Interpreting a gesture-based instruction to selectively display a frame of an application user interface on a mobile computing device
abstract: In general overview, the present disclosure is directed to a system and method for selectively displaying a frame of an application user interface on a mobile computing device. A user interface analyzer on a mobile computing device analyzes a user interface for an application executing on a remote server. The user interface analyzer identifies frames in the user interface, the positions of the frames, relationships between frames, and horizontal and vertical panning offsets to move between adjacent frames. The mobile computing device receives a user input requesting the display of an adjacent frame. Based on the information the user interface analyzer obtained, the mobile computing device displays an adjacent frame of the user interface.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09467346&OS=09467346&RS=09467346
owner: Citrix Systems, Inc.
number: 09467346
owner_city: Fort Lauderdale
owner_country: US
publication_date: 20140410
---
This application is a continuation of U.S. patent application Ser. No. 13 108 836 filed on May 16 2011 now allowed which claims priority to U.S. Provisional Application No. 61 334 646 entitled Interpreting a Gesture Based Instruction to Selectively Display a Frame of an Application User Interface on a Mobile Computing Device filed on May 14 2010 The entire disclosure of each of the foregoing applications are incorporated herein by reference in their entireties for any and all purposes.

The present disclosure relates generally to selectively displaying on a mobile computing device a frame of an application user interface of an application executing remotely on a server. In particular the present disclosure is directed to interpreting a gesture based or customized instruction to view an adjacent frame of the application user interface and panning the native display of the mobile computing device to the new frame.

Remote access systems have enabled users to access applications on servers from various portals. With the increasing prevalence of mobile computing devices users can also access applications on those servers from handheld devices. However displays on such devices often have low resolution. As a result a user can view only a section of the application user interface on the mobile computing device s screen and the user must scroll around the interface to view other sections of the interface. Thus navigating within an application user interface can be a cumbersome process.

It is common for many applications to have user interfaces that consist of multiple panes or frames of information in a single presentation or view. Often a particular type of data or controls feature is grouped together in a frame of the user interface such as emails in Microsoft Outlook. On devices that have larger screens all of the frames may be displayed at the same time. However when user interface UI designs with multiple frames or panes are delivered to a mobile device with a significantly smaller screen the user will often be limited to a reduced view of all the frames or panes or must view an arbitrarily displayed limited view of the UI making it hard for the user to navigate between the different frames. In some instances the user must manually align the frame they wish to interact with on the devices screen which can be a tedious interaction given the sensitivity of some mobile device s operation.

The present disclosure is directed to a system and method for selectively displaying a frame of an application user interface on a mobile computing device. A user interface analyzer on a mobile computing device analyzes a user interface for an application executing on a remote server. The user interface analyzer identifies frames in the user interface the positions of the frames relationships between frames and horizontal and vertical panning offsets to move between adjacent frames. The mobile computing device receives a user input requesting the display of an adjacent frame. Based on the information the user interface analyzer obtained the mobile computing device displays an adjacent frame of the user interface.

The systems and methods described herein increase the usability of applications with multiple frame by providing the means to smartly manage the locations of the different frames on the application UI and a user interface to easily navigate between the different frames. Users can perform the associated gesture or operation on their mobile devices to automatically align the application frame with the devices screen eliminating the need for the user to manually align the application frame with the device s screen.

The systems and methods described herein may provide easier navigation of static sized UI frames and easy navigation to UI frames of different sizes to either the frame s offset or middle of the frame including zooming a frame in or out to better align with the device s screen. The systems and methods described herein may provide tagged frame navigation which include embedding jump tags into the UI that allow the system to dynamically jump to specified frames. Guidelines may be provided for formatting application UIs using static sized UI frames to be compatible with the systems and methods described herein. Tagging and coding guidelines may be provided to allow the application to leverage the system. APIs may be provided that can be used in the development of the application to leverage more dynamic interaction with the system. APIs may be provided that will allow mobile devices to define the gestures actions to be used to invoke the features of the invention related to how the application UI is positioned on the screen of the device.

In one aspect the present invention is directed to a method for selectively displaying a frame of an application user interface on a mobile computing device. A client agent of a mobile computing device receives an application user interface from an application executing remotely on a server. The client agent stores the application user interface to an extended virtual screen buffer maintained by the mobile computing device. The client agent displays on a native display of the mobile computing device a frame of the extended virtual screen buffer. The client agent interprets user input from a user as a request to view an adjacent frame of the application user interface. The client agent responsive to the request retrieves the adjacent frame and the position of the adjacent frame of the application user interface from the extended virtual screen buffer. The client agent displays the retrieved adjacent frame of the extended virtual screen buffer on the native display of the mobile computing device.

In some embodiments the client agent receives user input from the user via a touch screen of the native display. In some embodiments the client agent identifies a plurality of frames of the received application user interface. In some further embodiments the client agent identifies coordinates of each of the plurality of frames of the received application user interface on the extended virtual screen buffer. In some embodiments the client agent identifies panning offsets of each of the plurality of frames of the received application user interface on the extended virtual screen buffer. In some embodiments the client agent detects jump tags associated with each of the plurality of frames wherein each jump tag identifies a command to execute to display a frame if a user inputs the command. In some embodiments the client agent interprets the user input from the user a vertical swipe as the request to view an adjacent frame of the application user interface as one of above or below the displayed frame. The client agent applies an object recognition algorithm to the extended virtual screen buffer to identify the plurality of frames of the received application user interface. In some embodiments the client agent determines to apply a zoom operation to the frame displayed on the native display of the mobile computing device. In some embodiments the client agent determines dimensions of each of the plurality of frames of the application user interface.

In one aspect the present invention is directed to a system for selectively displaying a frame of an application user interface on a mobile computing device. In some embodiments the system includes a mobile computing device a server an application executing remotely on the server and a client agent executing on a mobile computing device. The client agent receives an application user interface from the application. The client agent stores the application user interface to an extended virtual screen buffer maintained by the mobile computing device. The client agent displays on a native display of the mobile computing device a frame of the extended virtual screen buffer. The client agent interprets user input from a user as a request to view an adjacent frame of the application user interface. The client agent retrieves responsive to the request the adjacent frame and the position of the adjacent frame of the application user interface from the extended virtual screen buffer. The client agent displays the retrieved adjacent frame of the extended virtual screen buffer on the native display of the mobile computing device.

In some embodiments the client agent receives user input from the user via a touch screen of the native display. In some embodiments the client agent identifies a plurality of frames of the received application user interface. In some embodiments the client agent identifies coordinates of each of the plurality of frames of the received application user interface on the extended virtual screen buffer. In some embodiments the client agent identifies panning offsets of each of the plurality of frames of the received application user interface on the extended virtual screen buffer. In some embodiments the client agent detects jump tags associated with each of the plurality of frames wherein each jump tag identifies a command to execute to display a frame if a user inputs the command. In some embodiments the client agent interprets the user input from the user a vertical swipe as the request to view an adjacent frame of the application user interface as one of above or below the displayed frame. In some embodiments the client agent applies an object recognition algorithm to the extended virtual screen buffer to identify the plurality of frames of the received application user interface. In some embodiments the client agent determines to apply a zoom operation to the frame displayed on the native display of the mobile computing device. In some embodiments the client agent determines dimensions of each of the plurality of frames of the application user interface.

The details of various embodiments of the methods and systems described herein are set forth in the accompanying drawings and the description below.

For purposes of reading the description of the various embodiments below the following descriptions of the sections of the specification and their respective contents may be helpful 

In general overview the present disclosure is directed to selectively displaying a frame of an application user interface on the native display of a mobile computing device. The application can publish a user interface with frames that group types of data and or control features. A mobile computing device determines how to pan between adjacent frames of the interface. When the mobile computing device receives a user input requesting the display of an adjacent frame the mobile computing device pans to the adjacent frame.

In one embodiment the computing environment can include an appliance installed between the server s and client machine s . This appliance can manage client server connections and in some cases can load balance client connections amongst a plurality of backend servers.

The client machine s can in some embodiments be referred to as a single client machine or a single group of client machines while server s may be referred to as a single server or a single group of servers . In one embodiment a single client machine communicates with more than one server while in another embodiment a single server communicates with more than one client machine . In yet another embodiment a single client machine communicates with a single server .

A client machine can in some embodiments be referenced by any one of the following terms client machine s client s client computer s client device s client computing device s local machine remote machine client node s endpoint s endpoint node s or a second machine. The server in some embodiments may be referenced by any one of the following terms server s local machine remote machine server farm s host computing device s or a first machine s .

In one embodiment the client machine can be a virtual machine C. In some embodiments the virtual machine C can be managed by a hypervisor such as the Xen hypervisor developed sold by Citrix Systems Inc. Hyper V developed and sold by Microsoft Corp. ESX developed and sold by EMC or any other hypervisor.

The client machine can in some embodiments execute operate or otherwise provide an application that can be any one of the following software a program executable instructions a virtual machine a hypervisor a web browser a web based client a client server application a thin client computing client an ActiveX control a Java applet software related to voice over internet protocol VoIP communications like a soft IP telephone an application for streaming video and or audio an application for facilitating real time data communications a HTTP client a FTP client an Oscar client a Telnet client or any other set of executable instructions. Still other embodiments include a client device that displays application output generated by an application remotely executing on a server or other remotely located machine. In these embodiments the client device can display the application output in an application window a browser or other output window. In one embodiment the application is a desktop while in other embodiments the application is an application that generates a desktop.

The server in some embodiments executes a remote presentation client or other client or program that uses a thin client or remote display protocol to capture display output generated by an application executing on a server and transmits the application display output to a remote client . The thin client or remote display protocol can be any one of the following protocols the Independent Computing Architecture ICA protocol manufactured by Citrix Systems Inc. of Ft. Lauderdale Fla. or the Remote Desktop Protocol RDP manufactured by the Microsoft Corporation of Redmond Wash.

The computing environment can include more than one server A N such that the servers A N are logically grouped together into a server farm . The server farm can include servers that are geographically dispersed and logically grouped together in a server farm or servers that are located proximate to each other and logically grouped together in a server farm . Geographically dispersed servers A N within a server farm can in some embodiments communicate using a WAN MAN or LAN where different geographic regions can be characterized as different continents different regions of a continent different countries different states different cities different campuses different rooms or any combination of the preceding geographical locations. In some embodiments the server farm may be administered as a single entity while in other embodiments the server farm can include multiple server farms .

In some embodiments a server farm can include servers that execute a substantially similar type of operating system platform e.g. WINDOWS NT manufactured by Microsoft Corp. of Redmond Wash. UNIX LINUX or SNOW LEOPARD. In other embodiments the server farm can include a first group of servers that execute a first type of operating system platform and a second group of servers that execute a second type of operating system platform. The server farm in other embodiments can include servers that execute different types of operating system platforms.

The server in some embodiments can be any server type. In other embodiments the server can be any of the following server types a file server an application server a web server a proxy server an appliance a network appliance a gateway an application gateway a gateway server a virtualization server a deployment server a SSL VPN server a firewall a web server an application server or as a master application server a server executing an active directory or a server executing an application acceleration program that provides firewall functionality application functionality or load balancing functionality. In some embodiments a server may be a RADIUS server that includes a remote authentication dial in user service. In embodiments where the server comprises an appliance the server can be an appliance manufactured by any one of the following manufacturers the Citrix Application Networking Group Silver Peak Systems Inc Riverbed Technology Inc. F5 Networks Inc. or Juniper Networks Inc. Some embodiments include a first server A that receives requests from a client machine forwards the request to a second server B and responds to the request generated by the client machine with a response from the second server B. The first server A can acquire an enumeration of applications available to the client machine and well as address information associated with an application server hosting an application identified within the enumeration of applications. The first server A can then present a response to the client s request using a web interface and communicate directly with the client to provide the client with access to an identified application.

The server can in some embodiments execute any one of the following applications a thin client application using a thin client protocol to transmit application display data to a client a remote display presentation application any portion of the CITRIX ACCESS SUITE by Citrix Systems Inc. like the XENAPP or CITRIX PRESENTATION SERVER MICROSOFT WINDOWS Terminal Services manufactured by the Microsoft Corporation or an ICA client developed by Citrix Systems Inc. Another embodiment includes a server that is an application server such as an email server that provides email services such as MICROSOFT EXCHANGE manufactured by the Microsoft Corporation a web or Internet server a desktop sharing server a collaboration server or any other type of application server. Still other embodiments include a server that executes any one of the following types of hosted servers applications GOTOMEETING provided by Citrix Online Division Inc. WEBEX provided by WebEx Inc. of Santa Clara Calif. or Microsoft Office LIVE MEETING provided by Microsoft Corporation.

Client machines can in some embodiments be a client node that seeks access to resources provided by a server . In other embodiments the server may provide clients or client nodes with access to hosted resources. The server in some embodiments functions as a master node such that it communicates with one or more clients or servers . In some embodiments the master node can identify and provide address information associated with a server hosting a requested application to one or more clients or servers . In still other embodiments the master node can be a server farm a client a cluster of client nodes or an appliance.

One or more clients and or one or more servers can transmit data over a network installed between machines and appliances within the computing environment . The network can comprise one or more sub networks and can be installed between any combination of the clients servers computing machines and appliances included within the computing environment . In some embodiments the network can be a local area network LAN a metropolitan area network MAN a wide area network WAN a primary network comprised of multiple sub networks located between the client machines and the servers a primary public network with a private sub network a primary private network with a public sub network or a primary private network with a private sub network . Still further embodiments include a network that can be any of the following network types a point to point network a broadcast network a telecommunications network a data communication network a computer network an ATM Asynchronous Transfer Mode network a SONET Synchronous Optical Network network a SDH Synchronous Digital Hierarchy network a wireless network a wireline network or a network that includes a wireless link where the wireless link can be an infrared channel or satellite band. The network topology of the network can differ within different embodiments possible network topologies include a bus network topology a star network topology a ring network topology a repeater based network topology or a tiered star network topology. Additional embodiments may include a network of mobile telephone networks that use a protocol to communicate among mobile devices where the protocol can be any one of the following AMPS TDMA CDMA GSM GPRS UMTS EV DO LTE or any other protocol able to transmit data among mobile devices like 802.11 Bluetooth and Near Field Communication.

Illustrated in is an embodiment of a computing device where the client machine and server illustrated in can be deployed as and or executed on any embodiment of the computing device illustrated and described herein. Included within the computing device is a system bus that communicates with the following components a central processing unit a main memory storage memory an input output I O controller display devices A N an installation device and a network interface . In one embodiment the storage memory includes an operating system software routines and a client agent . The I O controller in some embodiments is further connected to a key board and a pointing device . Other embodiments may include an I O controller connected to more than one input output device A N.

Embodiments of the computing machine can include a central processing unit characterized by any one of the following component configurations logic circuits that respond to and process instructions fetched from the main memory unit a microprocessor unit such as those manufactured by Intel Corporation those manufactured by Motorola Corporation those manufactured by Transmeta Corporation of Santa Clara Calif. the RS 6000 processor such as those manufactured by International Business Machines a processor such as those manufactured by Advanced Micro Devices or any other combination of logic circuits. Still other embodiments of the central processing unit may include any combination of the following a microprocessor a microcontroller a central processing unit with a single processing core a central processing unit with two processing cores or a central processing unit with more than one processing core.

While illustrates a computing device that includes a single central processing unit in some embodiments the computing device can include one or more processing units . In these embodiments the computing device may store and execute firmware or other executable instructions that when executed direct the one or more processing units to simultaneously execute instructions or to simultaneously execute instructions on a single piece of data. In other embodiments the computing device may store and execute firmware or other executable instructions that when executed direct the one or more processing units to each execute a section of a group of instructions. For example each processing unit may be instructed to execute a portion of a program or a particular module within a program.

In some embodiments the processing unit can include one or more processing cores. For example the processing unit may have two cores four cores eight cores etc. In one embodiment the processing unit may comprise one or more parallel processing cores. The processing cores of the processing unit may in some embodiments access available memory as a global address space or in other embodiments memory within the computing device can be segmented and assigned to a particular core within the processing unit . In one embodiment the one or more processing cores or processors in the computing device can each access local memory. In still another embodiment memory within the computing device can be shared amongst one or more processors or processing cores while other memory can be accessed by particular processors or subsets of processors. In embodiments where the computing device includes more than one processing unit the multiple processing units can be included in a single integrated circuit IC . These multiple processors in some embodiments can be linked together by an internal high speed bus which may be referred to as an element interconnect bus.

In embodiments where the computing device includes one or more processing units or a processing unit including one or more processing cores the processors can execute a single instruction simultaneously on multiple pieces of data SIMD or in other embodiments can execute multiple instructions simultaneously on multiple pieces of data MIMD . In some embodiments the computing device can include any number of SIMD and MIMD processors.

The computing device in some embodiments can include a graphics processor or a graphics processing unit Not Shown . The graphics processing unit can include any combination of software and hardware and can further input graphics data and graphics instructions render a graphic from the inputted data and instructions and output the rendered graphic. In some embodiments the graphics processing unit can be included within the processing unit . In other embodiments the computing device can include one or more processing units where at least one processing unit is dedicated to processing and rendering graphics.

One embodiment of the computing machine includes a central processing unit that communicates with cache memory via a secondary bus also known as a backside bus while another embodiment of the computing machine includes a central processing unit that communicates with cache memory via the system bus . The local system bus can in some embodiments also be used by the central processing unit to communicate with more than one type of I O device A N. In some embodiments the local system bus can be any one of the following types of buses a VESA VL bus an ISA bus an EISA bus a MicroChannel Architecture MCA bus a PCI bus a PCI X bus a PCI Express bus an AGP bus or a NuBus. Other embodiments of the computing machine include an I O device A N that is a video display that communicates with the central processing unit . Still other versions of the computing machine include a processor connected to an I O device A N via any one of the following connections HyperTransport Rapid I O or InfiniBand. Further embodiments of the computing machine include a processor that communicates with one I O device A using a local interconnect bus and a second I O device B using a direct connection.

The computing device in some embodiments includes a main memory unit and cache memory . The cache memory can be any memory type and in some embodiments can be any one of the following types of memory SRAM BSRAM or EDRAM. Other embodiments include cache memory and a main memory unit that can be any one of the following types of memory Static random access memory SRAM Burst SRAM or SynchBurst SRAM BSRAM Dynamic random access memory DRAM Fast Page Mode DRAM FPM DRAM Enhanced DRAM EDRAM Extended Data Output RAM EDO RAM Extended Data Output DRAM EDO DRAM Burst Extended Data Output DRAM BEDO DRAM Enhanced DRAM EDRAM synchronous DRAM SDRAM JEDEC SRAM PC100 SDRAM Double Data Rate SDRAM DDR SDRAM Enhanced SDRAM ESDRAM SyncLink DRAM SLDRAM Direct Rambus DRAM DRDRAM Ferroelectric RAM FRAM Memristor based memory or any other type of memory. Further embodiments include a central processing unit that can access the main memory via a system bus a memory port or any other connection bus or port that allows the processor to access memory .

One embodiment of the computing device provides support for any one of the following installation devices a CD ROM drive a CD R RW drive a DVD ROM drive tape drives of various formats a USB device Secure Digital card NetBoot or iPXE firmware a bootable medium a bootable CD a bootable CD for GNU Linux distribution such as KNOPPIX a hard drive or any other device suitable for installing applications or software. Applications can in some embodiments include a client agent or any portion of a client agent . The computing device may further include a storage device that can be either one or more hard disk drives or one or more redundant arrays of independent disks where the storage device is configured to store an operating system software programs applications or at least a portion of the client agent . A further embodiment of the computing device includes an installation device that is used as the storage device .

The computing device may further include a network interface to interface to a Local Area Network LAN Wide Area Network WAN or the Internet through a variety of connections including but not limited to standard telephone lines LAN or WAN links e.g. 802.11 T1 T3 56kb X.25 SNA DECNET broadband connections e.g. ISDN Frame Relay ATM Gigabit Ethernet Ethernet over SONET wireless connections or some combination of any or all of the above. Connections can also be established using a variety of communication protocols e.g. TCP IP IPX SPX NetBIOS Ethernet ARCNET SONET SDH Fiber Distributed Data Interface FDDI RS232 RS485 IEEE 802.11 IEEE 802.11a IEEE 802.11b IEEE 802.11g CDMA GSM WiMax and direct asynchronous connections . One version of the computing device includes a network interface able to communicate with additional computing devices via any type and or form of gateway or tunneling protocol such as Secure Socket Layer SSL or Transport Layer Security TLS or the Citrix Gateway Protocol manufactured by Citrix Systems Inc. Versions of the network interface can comprise any one of a built in network adapter a network interface card a PCMCIA network card a card bus network adapter a wireless network adapter a USB network adapter a modem or any other device suitable for interfacing the computing device to a network capable of communicating and performing the methods and systems described herein.

Embodiments of the computing device include any one of the following I O devices A N a keyboard a pointing device mice trackpads an optical pen trackballs microphones drawing tablets video displays speakers inkjet printers laser printers and dye sublimation printers or any other input output device able to perform the methods and systems described herein. An I O controller may in some embodiments connect to multiple I O devices A N to control the one or more I O devices. Some embodiments of the I O devices A N may be configured to provide storage or an installation medium while others may provide a universal serial bus USB interface for receiving USB storage devices such as the USB Flash Drive line of devices manufactured by Twintech Industry Inc. Still other embodiments include an I O device that may be a bridge between the system bus and an external communication bus such as a USB bus an Apple Desktop Bus an RS 232 serial connection a SCSI bus a FireWire bus a FireWire bus an Ethernet bus an AppleTalk bus a Gigabit Ethernet bus an Asynchronous Transfer Mode bus a HIPPI bus a Super HIPPI bus a SerialPlus bus a SCl LAMP bus a FibreChannel bus or a Serial Attached small computer system interface bus.

In some embodiments the computing machine can connect to multiple display devices A N in other embodiments the computing device can connect to a single display device while in still other embodiments the computing device connects to display devices A N that are the same type or form of display or to display devices that are different types or forms. Embodiments of the display devices A N can be supported and enabled by the following one or multiple I O devices A N the I O controller a combination of I O device s A N and the I O controller any combination of hardware and software able to support a display device A N any type and or form of video adapter video card driver and or library to interface communicate connect or otherwise use the display devices A N. The computing device may in some embodiments be configured to use one or multiple display devices A N these configurations include having multiple connectors to interface to multiple display devices A N having multiple video adapters with each video adapter connected to one or more of the display devices A N having an operating system configured to support multiple displays A N using circuits and software included within the computing device to connect to and use multiple display devices A N and executing software on the main computing device and multiple secondary computing devices to enable the main computing device to use a secondary computing device s display as a display device A N for the main computing device . Still other embodiments of the computing device may include multiple display devices A N provided by multiple secondary computing devices and connected to the main computing device via a network.

In some embodiments the computing machine can execute any operating system while in other embodiments the computing machine can execute any of the following operating systems versions of the MICROSOFT WINDOWS operating systems such as WINDOWS 3.x WINDOWS 95 WINDOWS 98 WINDOWS 2000 WINDOWS NT 3.51 WINDOWS NT 4.0 WINDOWS CE WINDOWS XP and WINDOWS VISTA the different releases of the Unix and Linux operating systems any version of the MAC OS manufactured by Apple Computer OS 2 manufactured by International Business Machines any embedded operating system any real time operating system any open source operating system any proprietary operating system any operating systems for mobile computing devices or any other operating system. In still another embodiment the computing machine can execute multiple operating systems. For example the computing machine can execute PARALLELS or another virtualization platform that can execute or manage a virtual machine executing a first operating system while the computing machine executes a second operating system different from the first operating system.

The computing machine can be embodied in any one of the following computing devices a computing workstation a desktop computer a laptop or notebook computer a server a handheld computer a mobile telephone a portable telecommunication device a media playing device a gaming system a mobile computing device a netbook a device of the IPOD family of devices manufactured by Apple Computer any one of the PLAYSTATION family of devices manufactured by the Sony Corporation any one of the Nintendo family of devices manufactured by Nintendo Co any one of the XBOX family of devices manufactured by the Microsoft Corporation or any other type and or form of computing telecommunications or media device that is capable of communication and that has sufficient processor power and memory capacity to perform the methods and systems described herein. In other embodiments the computing machine can be a mobile device such as any one of the following mobile devices a JAVA enabled cellular telephone or personal digital assistant PDA such as the i55sr i58sr i85s i88s i90c i95c1 or the im1100 all of which are manufactured by Motorola Corp the 6035 or the 7135 manufactured by Kyocera the i300 or i330 manufactured by Samsung Electronics Co. Ltd the TREO 180 270 600 650 680 700p 700w or 750 smart phone manufactured by Palm Inc any computing device that has different processors operating systems and input devices consistent with the device or any other mobile computing device capable of performing the methods and systems described herein. In still other embodiments the computing device can be any one of the following mobile computing devices any one series of Blackberry or other handheld device manufactured by Research In Motion Limited the iPhone manufactured by Apple Computer Palm Pre a Pocket PC a Pocket PC Phone or any other handheld mobile device.

In some embodiments the computing device may have different processors operating systems and input devices consistent with the device. For example in one embodiment the computing device is a TREO 180 270 600 650 680 700p 700w or 750 smart phone manufactured by Palm Inc. In some of these embodiments the TREO smart phone is operated under the control of the PalmOS operating system and includes a stylus input device as well as a five way navigator device.

In other embodiments the computing device is a mobile device such as a JAVA enabled cellular telephone or personal digital assistant PDA such as the i55sr i58sr i85s i88s i90c i95c1 or the im1100 all of which are manufactured by Motorola Corp. of Schaumburg Ill. the 6035 or the 7135 manufactured by Kyocera of Kyoto Japan or the i300 or i330 manufactured by Samsung Electronics Co. Ltd. of Seoul Korea. In some embodiments the computing device is a mobile device manufactured by Nokia of Finland or by Sony Ericsson Mobile Communications AB of Lund Sweden.

In still other embodiments the computing device is a Blackberry handheld or smart phone such as the devices manufactured by Research In Motion Limited including the Blackberry 7100 series 8700 series 7700 series 7200 series the Blackberry 7520 or the Blackberry Pearl 8100. In yet other embodiments the computing device is a smart phone Pocket PC Pocket PC Phone or other handheld mobile device supporting Microsoft Windows Mobile Software. Moreover the computing device can be any workstation desktop computer laptop or notebook computer server handheld computer mobile telephone any other computer or other form of computing or telecommunications device that is capable of communication and that has sufficient processor power and memory capacity to perform the operations described herein.

In some embodiments the computing device is a digital audio player. In one of these embodiments the computing device is a digital audio player such as the Apple IPOD IPOD Touch IPOD NANO and IPOD SHUFFLE lines of devices manufactured by Apple Computer of Cupertino Calif. In another of these embodiments the digital audio player may function as both a portable media player and as a mass storage device. In other embodiments the computing device is a digital audio player such as the DigitalAudioPlayer Select MP3 players manufactured by Samsung Electronics America of Ridgefield Park N.J. or the Motorola m500 or m25 Digital Audio Players manufactured by Motorola Inc. of Schaumburg Ill. In still other embodiments the computing device is a portable media player such as the Zen Vision W the Zen Vision series the Zen Portable Media Center devices or the Digital MP3 line of MP3 players manufactured by Creative Technologies Ltd. In yet other embodiments the computing device is a portable media player or digital audio player supporting file formats including but not limited to MP3 WAV M4A AAC WMA Protected AAC RIFF Audible audiobook Apple Lossless audio file formats and .mov .m4v and .mp4 MPEG 4 H.264 MPEG 4 AVC video file formats.

In some embodiments the computing device comprises a combination of devices such as a mobile phone combined with a digital audio player or portable media player. In one of these embodiments the computing device is a Motorola RAZR or Motorola ROKR line of combination digital audio players and mobile phones. In another of these embodiments the computing device is an iPhone smartphone manufactured by Apple Computer of Cupertino Calif.

Section B Systems for Interpreting a Gesture Based Instruction to Selectively Display a Frame of an Application User Interface on a Mobile Computing Device

Referring to a block diagram illustrating an exemplary system for selectively displaying a frame of an application user interface on a mobile computing device is shown and described. In brief overview the system includes a server that communicates with a mobile computing device over a network . The processor of the server executes an application and the server transmits output data also referred to herein as the user interface or application user interface of the application to the mobile computing device via the transceiver

The mobile computing device receives the application user interface via transceiver and stores the application user interface on an extended virtual screen . A processor loads a section of the extended virtual screen into a display buffer and the virtual graphics driver transmits this section to the native display . A user interface analyzer also referred to herein as a client agent executed by the processor processes the application user interface to identify the interface s frames coordinates corresponding to the frames positions on the extended virtual screen the relationships between the frames positions and or the offsets to pan from one frame to another also referred to herein as panning offsets . In some embodiments the user interface analyzer detects jump tags associated with the frames each jump tag identifying a command and a new frame to display if a user inputs the command when the jump tag s frame is on display.

The mobile computing device receives a user input via for example a touchscreen of the native display . The mobile computing device interprets the user input as a request to view an adjacent frame of the application user interface. Based on the information about the user interface and the user input the mobile computing device identifies a new frame for display and the new frame s position on the extended virtual screen . The processor loads the section of the extended virtual screen corresponding to the new frame into the display buffer . The virtual graphics driver transmits the new frame to the native display .

Referring now to a block diagram of an exemplary mobile computing device that selectively displays a frame of an application user interface is shown and described. The mobile computing device stores the application user interface from the server on an extended virtual screen . The user interface analyzer processes the application user interface to identify the interface s frames the frames positions the relationships between the frames positions and or the offsets to pan from one frame to another.

The user interface analyzer can process the user interface to identify frames according to numerous methods. In some embodiments the user interface analyzer identifies the frames by scraping the extended virtual screen . For example the user interface analyzer can detect frames by applying an object recognition algorithm on the data in the extended virtual screen . In another example the user interface analyzer queries the underlying programming objects on the server that are associated with the application user interface for information about the frames.

Through such methods the user interface analyzer can determine the position of each frame on the extended virtual screen . The representation of the position can correspond to how the mobile computing device displays frames. For example if the position is represented by coordinates corresponding to a corner of a frame or the four corners of the frame the mobile computing device can match a corner of the frame with a corner of the native display . Thus a mobile computing device can align a frame against two of the boundaries of the native display. In another example if the position is represented by coordinates corresponding to the center of the frame the mobile computing device can positions the frame in the center of the native display .

The user interface analyzer can also determine the dimensions of each frame. The dimensions can be represented by the number of pixels in the length and width of the frame e.g. 320 480 pixels. In some embodiments the user interface analyzer also identifies the orientation of the frame. For example the user interface analyzer can assign a landscape orientation to a 480 320 pixel frame and a portrait orientation to a 320 480 pixel frame thereby indicating how to display the frame to match the dimensions of the native display . The user interface analyzer can store the positions dimensions and or orientation of the frames in memory .

In some embodiments the user interface analyzer can determine if a zoom operation should be applied to a frame upon display. The user interface analyzer can compare the dimensions of a frame with the resolution of the native display . If the dimensions exceed the display resolution the user interface analyzer can store an instruction to zoom out for the frame. The instruction can indicate the magnitude of the zoom and the algorithm to produce the zoom. For example an instruction can include a rate for subsampling a frame. Likewise if the display resolution exceeds the dimensions of the frame the user interface analyzer can store an instruction to zoom in for the frame. Such an instruction can include the parameters for performing nearest neighbor interpolation bilinear interpolation bicubic interpolation or any other upsampling algorithm as would be appreciated by one of ordinary skill in the art. Any of these instructions can be stored in memory in association with the relevant frame.

Further the user interface analyzer can determine the relationships between the frames positions and or the offsets to pan from one frame to another. For example the user interface analyzer can compare the positions and or dimensions of frames to determine which frames are vertically or horizontally adjacent to one another. By iteratively comparing positions and or dimensions of the frames the user interface analyzer can construct the arrangement of frames in the application user interface.

In some embodiments the user interface analyzer can detect that the frames are equally sized and spaced i.e. a static sized user interface . When this occurs whenever the device pans to an adjacent frame the processor can add or subtract the same horizontal and vertical panning offsets to the coordinates corresponding to the frame on the native display. Thus the mobile computing device can derive global horizontal and vertical panning offsets according to the sizing and spacing of the frames.

The user interface analyzer can match the arrangement of frames with a grid template stored in memory . When the user interface analyzer can make a match the user interface analyzer can adopt the horizontal and vertical panning offsets provided by the grid template. In some embodiments the mobile computing device receives from the server the identity of a grid template corresponding to the application user interface. The user interface analyzer can adopt the panning offsets of the identified grid template. Thus the mobile computing device can forgo calculating the panning offsets.

The grid templates can correspond to common arrangements of frames designed for display on mobile computing devices . In some embodiments development guidelines for applications to be used with the present disclosure can publish the grid templates. Based on the guidelines application developers can design applications that output user interfaces compatible with one of the grid templates stored on the mobile computing device . As the user interface analyzer evaluates the application user interface the user interface analyzer can readily match the user interface with a grid template and forego additional calculations for determining panning offsets.

In some embodiments when the arrangement of frames is incompatible with the grid templates the user interface analyzer evaluates the relationships between frames to determine offsets for panning between frames. For example the user interface analyzer can assign identifiers to the frames. For each frame the user interface analyzer can identify adjacent frames and associate the identifiers of the adjacent frames with the frame. The user interface analyzer can calculate vertical and horizontal offsets for panning from the frame to each adjacent frame. For example the user interface analyzer can subtract the coordinates of the frame from the coordinates of an adjacent frame. Based on the polarity of the differences the user interface analyzer can determine the relative position of the adjacent frame e.g. bordering the frame on the left right top or bottom . The user interface analyzer can store the identifiers of adjacent frames the vertical and horizontal offsets for panning to those frames and the relative positions of the frames in the memory .

Further the user interface analyzer can evaluate jump tags associated with the frames of the user interface. The user interface analyzer can extract a command and a new frame to display if a user inputs the command when the jump tag s frame is on display. In some embodiments the jump tag includes the vertical and horizontal offsets for panning from the jump tag s frame to the new frame. In other embodiments the user interface analyzer calculates the offsets by subtracting the coordinates of the jump tag s frame from the coordinates of the new frame. The user interface analyzer stores the offsets in association with the jump tag s frame. When a user inputs the command the processor retrieves the vertical and horizontal offsets of the new frame from memory and transfer data corresponding to the new frame to the display buffer .

As with the grid templates development guidelines for applications to be used with the present disclosure can publish guidelines for associating jump tags with frames. Based on the guidelines application developers can design applications that output user interfaces with frames that include jump tags for panning to designated frames when the mobile computing device detects user inputs corresponding to the jump tags.

Additionally the user interface analyzer can include an application programming interface API that permits users to customize the user interface analyzer . In this manner users can define the commands to view different frames of the application user interface. Through such customization users can tailor the commands to the input interfaces of the mobile computing device substitute their preferred commands for the user interface analyzer s default commands and or create new commands.

For example a user can create new commands for the mobile computing device to pan to adjacent frames in response to tapping on the edges of the touchscreen in addition to the default commands. If the mobile computing device includes an accelerometer the user can create commands to pan to an adjacent frame in the direction of a detected tilt. Further a user can create a command to pan to a predetermined frame whenever the user shakes the mobile computing device . In additional examples a user can create commands to change how the mobile computing device displays each frame. The mobile computing device can center a displayed frame in response to a double tap in the center of the touchscreen or align the displayed frame against a corner of the touchscreen in response to a double tap in that corner.

Although the present disclosure describes a user interface analyzer on the mobile computing device to analyze the application user interface the server can analyze the user interface. The server can perform any of the operations described herein with respect to the user interface and transmit the information to the mobile computing device . For example the server can identify the frames coordinates corresponding to the frames positions on the extended virtual screen the relationships between the frames positions and or the offsets to pan from one frame to another. Then the server can transmit this information to the mobile computing device .

After the mobile computing device stores information about the application user interface the mobile computing device selects a frame for initial display. The processor retrieves the coordinates of the frame from memory and loads the section of the extended virtual screen corresponding to the frame into the display buffer . The virtual graphics driver transmits the data in the display buffer to the native display .

A touch responsive surface also referred to herein as a touchscreen of the native display can receive a touch input from a user of the mobile computing device . The touchscreen can identify the locations where the user touches the surface and transmit the locations to the processor . The processor can evaluate the locations to determine if the user input is a request to view an adjacent frame of the application user interface. For example if the user input is a two finger swipe on the touchscreen the processor interprets the user input as a request to view an adjacent frame not a request to pan across the extended virtual screen based on the magnitude of the input.

The processor evaluates the user input to determine the position of the adjacent frame. The processor can compare the coordinates of the beginning and end of the user input. From this comparison the processor can interpret the user input as a request to view a frame to the left right top or bottom of the frame on the native display. If the user interface analyzer previously detected that the application user interface is static sized the processor can calculate coordinates of the new frame by adjusting the coordinates of the displayed frame by the established global horizontal and or vertical panning offsets. If the application user interface is compatible with a grid template the processor adjusts the coordinates according to the offsets indicated by the grid template.

In further embodiments the processor retrieves the information for the displayed frame and its adjacent frames from memory . When such information includes vertical and horizontal panning offsets to move from the displayed frame to adjacent frames the processor identifies the offsets for the adjacent frame in the direction indicated by the user input. The processor adjusts the coordinates corresponding to the data in the display buffer by these offsets. The processor outputs data corresponding to the new frame to the display buffer adjusting the data for the new frame s orientation. When the information includes the identifiers of the adjacent frames the processor retrieves the coordinates corresponding to the identified adjacent frame in the direction indicated by the user input. The processor outputs data corresponding to the retrieved coordinates to the display buffer according to the adjacent frame s orientation.

In any of these embodiments the processor can retrieve an instruction to apply a zoom operation to the new frame upon display. If a frame includes such an instruction the processor applies the zoom operation to the frame data on the extended virtual screen and outputs this data to the display buffer. If the frame does not include such an instruction the processor copies the frame data on the extended virtual screen to the display buffer . Then the virtual graphics driver transmits the data in the display buffer to the native display to display the new frame.

In some embodiments the mobile computing device copies the frame data on the extended virtual screen to the display buffer regardless of stored zoom instructions for the frame. If the processor interprets additional touch input received on the touchscreen as an instruction to zoom the processor determines if the frame includes a zoom instruction corresponding to the input. The processor applies the zoom instruction to the frame data and outputs this data to the frame buffer.

The operations for panning to a new frame can also be applied to any customized commands as defined by a user through the application programming interface API . For example the processor can retrieve information for a frame adjacent to the displayed frame in response to a double tap on the edge of the touchscreen . Likewise if the user creates a command to view different frames when the mobile computing device is tilted the processor retrieves information for an adjacent frame in response to input from an accelerometer. In this manner the processor can obtain coordinates and or zoom operations for a new frame in response to any user input indicated in a user customized command.

Referring now to a grid depicting an exemplary arrangement of frames in an application user interface is shown and described. The grid includes three frames and that are 320 480 pixels and two frames that are 480 320 pixels. Such a grid can include a horizontal panning offset of 320 pixels between frames and and frames and . The grid can also include a horizontal panning offset of 480 pixels between frames and . When a user instructs the mobile computing device to display the frame below frame the processor can calculate the coordinates of frame from the coordinates of frame and the offset between the frames stored in memory. Additionally the processor can retrieve the orientation of frame . The processor can use this orientation to output the data of frame to the display buffer so the dimensions of the frame match the native display. In this manner a user can simply rotate the mobile computing device to view frame .

Referring now to an exemplary gesture based instruction for viewing a different frame of the application user interface is shown and described. In this embodiment the gesture based instruction is a two finger swipe across a touchscreen. The two finger swipe depicted can pan to adjacent frames to the left or right of the displayed frame.

Referring now to exemplary views of a mobile computing device selectively displaying different frames of an application user interface in response to a gesture based instruction are shown and described. The application user interface is a static sized user interface. The frames group information by function the inbox frame tracks e mails in the inbox the message frame displays contents of a selected e mail and the to do frame tracks appointments. To move from one frame to another the processor adjusts the horizontal coordinates corresponding to data in the display buffer by the width of a frame. The processor adds or subtracts the width of the frame according to the direction of the adjacent frame.

Referring now to a screenshot of an exemplary user interface for panning between applications is shown and described. In these embodiments the mobile computing device can adjust the number of applications positions of the applications and offsets between adjacent applications when a user closes an application or opens a new application. When a user open an application the mobile computing device positions the application user interface on the extended virtual screen . In some embodiments the mobile computing device populates the extended virtual screen with user interfaces according to the frames of a predetermined grid. In this manner the mobile computing device associates applications with frames and uses the horizontal and vertical panning offsets set by the grid.

Further the mobile computing device can allow a user to change the dimensions of an application user interface or the positions of the applications. For example the default frame size can be the size of for example frame . The user can lengthen a frame to the length of frame . When the user alters a frame size the mobile computing device can re position frames re calculate offsets between adjacent frames and update relationships between frames.

For example suppose frame had been originally positioned adjacent to frame . When a user lengthens a frame to the length of frame as depicted in the mobile computing device can shift frame to the right on the extended virtual screen . The processor updates the memory to indicate that frames and are adjacent to one another and calculates the horizontal panning offset between the frames. The processor deletes associations between frames and . Further the processor updates information about frames and to indicate the frames are adjacent to one another and calculates the horizontal and vertical panning offsets.

Section C Methods for Interpreting a Gesture Based Instruction to Selectively Display a Frame of an Application User Interface on a Mobile Computing Device

Now referring to is a flow diagram depicting one embodiment of a method for interpreting a gesture based instruction to selectively display a frame of an application user interface on a mobile computing device. In brief overview at step a client agent such as the user interface analyzer executing on a client mobile device receives an application user interface from an application executing on a server. At step the client agent stores the application user interface to an extended virtual screen buffer. At step the client agent displays on a native display of the mobile device a frame of the application user interface. At step the client agent interprets user input as a request to view an adjacent frame of the application user interface. At step the client agent retrieves the adjacent frame and the position of the frame from the extended virtual screen buffer. At step the client agent displays the retrieved adjacent frame of the extended virtual screen buffer on the native display of the mobile computing device.

Still referring to and in more detail at step a client agent executing on a client mobile device receives an application user interface from an application executing on a server. A client agent executing on a client mobile device establishes a connection to a session hosted by a server . The established connection may be a presentation layer protocol. In some embodiments the established connection is a virtual channel protocol for remoting the display of the hosted application executing on the server . In some embodiments the client agent connects to an existing proxy session hosted by the server. In some embodiments the server establishes a new proxy session responsive to the connection request from the client agent of the mobile client device . In some embodiments the client agent receives snapshots of an application user interface from an application executing on the server . In some embodiments the client agent receives user interface data from the server . In some embodiments the client agent receives a plurality of packets representing the user interface data from the hosted application executing on the server .

At step the client agent stores the application user interface to an extended virtual screen buffer. In some embodiments the client agent receives information from the application on the server and stores the received data in cache until the data transmission is complete and then transmits the data to the extended virtual screen buffer. In some embodiments the client agent receives data as described herein from the server and stores the application user interface to an extended virtual screen buffer if the data exceeds the capacity of the display buffer . In some embodiments the client agent receives the application user interface from the hosted application and stores the data directly to an extended virtual screen buffer . In some embodiments the client agent loads a section of the extended virtual screen into a display buffer . In some embodiments the client agent loads multiple sections of the extended virtual screen into the display buffer . In some embodiments the client agent may process the received application user interface to identify frames of the user interface coordinates corresponding to the frames positions in the extended virtual screen the relationships between the frames positions and the offsets to pan from one frame to another. In some embodiments the user may configure or otherwise specify the relationship of frames. In some embodiments the relationships between frames may be calculated by the client agent responsive to known user preferences. In some embodiments the client agent may determine the relationships between frames to mirror the relationship of the frames in the user interface of the desktop application. In some embodiments the client agent may determine the relationships between frames using rankings of the frames. The rankings of frames may be determined by the manufacture or by depending on usage by the user.

At step the client agent displays on a native display of the mobile device a frame of the application user interface. In some embodiments the client agent displays a frame specified by the user on the native display of the mobile device. In some embodiments the native display of the device may be a touchscreen . In some embodiments the application user interface is retrieved from the extended virtual screen and copied to the virtual graphics driver and then applied to a native frame buffer of the mobile device. The native frame buffer transmits the data to the native display such as the touchscreen .

At step the client agent interprets user input as a request to view an adjacent frame of the application user interface. In some embodiments the user input is received from the user via a touchscreen . In some embodiments the user may interact with the touchscreen through gestures such as a side swipe or a vertical swipe. In some embodiments the user may interact with the mobile device using a stylus or other similar tool used in conjunction with mobile devices to enter data. In some embodiments the client agent receives the user input from a module of the mobile device . In some embodiments the user interface analyzer analyzes the received raw gesture to determine characteristics of the user input such as direction and velocity. In some embodiments the characteristics of the user input are received by the user interface analyzer from a module of the mobile computing device . In some embodiments the client agent references a table or other data storage structure to determine an action to apply responsive to the received user input. For example a horizontal swipe may indicate a request to view a frame to the right or the left of the currently displayed frame. A vertical swipe may indicate a request to view a frame above or below the currently displayed frame. In some embodiments the characteristics of the user input such as the location of the gesture on the native display may indicate scrolling within a frame.

In some embodiments the client agent may receive a plurality of frames from the server . In some embodiments the application may transmit a single user interface and the client agent may parse or transform received data. In some embodiments the client agent identifies a plurality of frames of the received application user interface. In some embodiments the client agent receives coordinates on the extended virtual screen buffer of each of the plurality of frames of the received application user interface. In some embodiments the client agent calculates coordinates of each of the plurality of frames of the received application user interface on the extended virtual screen buffer. In some embodiments the user specifies the coordinates of each of the plurality of frames. In some embodiments the user configures the extended virtual screen buffer to automatically calculate or identify the coordinates of each of the plurality of frames of the received application user interface after analyzing the received frames and determining an efficient layout responsive to the analysis.

In some embodiments the client agent identifies the panning offsets of each of the plurality of frames of the received application user interface on the extended virtual screen buffer. In some embodiments the client agent uses the characteristics to the native display of the mobile device to determine the panning offsets of the plurality of frames to efficiently use the native display. In some embodiments the client agent detects jump tags associated with each of the plurality of frames. In some embodiments the jump tag identifies a command to execute to display a frame if a user inputs the command. In some embodiments the client agent interprets a vertical swipe from the user as the request to view an adjacent frame of the application user interface as one of above or below the displayed frame. In some embodiments the client agent applies an object recognition algorithm to the extended virtual screen buffer to identify the plurality of frames of the received application user interface.

At step the client agent retrieves the adjacent frame and the position of the frame from the extended virtual screen buffer. The client agent may determine the dimensions of each of the plurality of frames of the application user interface. In some embodiments the client agent may retrieve multiple adjacent frames from the extended virtual screen buffer responsive to the velocity of the gesture input received from the user. In some embodiments the client agent retrieves the frame specified by a jump tag from the current frame. In some embodiments the client agent communicates with the server side component to determine that requested adjacent frame contains the most up to date information.

At step the client agent displays the retrieved adjacent frame of the extended virtual screen buffer on the native display of the mobile computing device. In some embodiments the client agent determines to apply a zoom operation to the frame displayed on the native display of the mobile computing device.

In some embodiments the system may include a client agent used to control the actual user interface positioning features on the mobile device and a server side component that can be used to gather additional information about the application user interface being delivered to the mobile device .

The client agent may be used independent from the server side component. In some embodiments the primary function of the client agent is to properly alight the appropriate frame of the larger application user interface on the smaller screen of a device when a control or gesture is used to change the viewable portion of the larger application user interface.

In some embodiments the client agent may overlay the full graphics buffer of the application with horizontal and vertical view points. In some embodiments view points are defined as a 0 0 relative pixel offset on the larger user interface of the application . The view point is aligned at the 0 0 screen offset of the device when a gesture is made to move to the next grid. In some embodiments it is necessary to ascertain the user interface grid is currently being displayed on the screen of the device so that when a gesture is made to pan vertically or horizontally the user interface can be positioned on the screen based on the next defined vertical or horizontal view point. In some embodiments view points can also have associated properties that define actions to be performed when that view point is displayed. In some embodiments an operation is applied to the extended virtual screen to zoom to the center of the application user interface. In some embodiments the client agent applies a zoom operation to the active grid. The zoom operation may include a zoom in or a zoom out by a predetermined percentage to better align the grid on the screen of the mobile computing device .

In some embodiments the viewable portion of the user interface is panned either left or right a defined number of pixels responsive to a horizontal pan received from a user. In some embodiments responsive to a vertical pan received from a user the viewable portion of the user interface is panned either up or down a defined number of pixels. In some embodiments the defined number of pixels may be set at time of manufacture or configured by a user or administrator of the system.

In some embodiments static size user interface frames may be used. In some embodiments the static size may be determined at time of manufacture or configured by an administrator of the system. In some embodiments the number of pixels to pan vertical or horizontal may be fixed allowing each frame to be aligned on the screen of the mobile computing device.

In some embodiments the client agent can be implemented to interrogate the layout of the application being controlled to determine dynamic viewpoints. In some cases a server side component may also be required to better interrogate the layout of the application being controlled to determine dynamic view points. In some embodiments the server side component may communicate the client agent to share view points data and other relevant data to allow the client agent to properly pan the application user interface.

In some embodiments applications requiring highly accurate panning points may communicate with the server side components to ensure that exact view points are defined for the specific application. In some embodiments the client agent may invoke a move to a new view point responsive to a gesture received from a user.

In some embodiments the client agent may invoke modules to initiate the navigation operations on different device type. This allows any number of gestures or controls to be used to initiate the navigation operations using usability methods familiar to the users of a certain type of device platform.

Having described certain embodiments of methods and systems for selectively displaying a user interface frame of an application on a mobile computing device it will now become apparent to one of skill in the art that other embodiments incorporating the concepts of the disclosure may be used. Therefore the invention should not be limited to certain embodiments.

While certain exemplary embodiments have been described and shown in the accompanying drawings it is to be understood that such embodiments are merely illustrative of and not restrictive on the methods and systems described herein. Additionally it is possible to implement the methods and systems described herein or some of its features in hardware programmable devices firmware software or a combination thereof. The methods and systems described herein or parts of the methods and systems described herein may also be embodied in a processor readable storage medium or machine readable medium such as a magnetic e.g. hard drive floppy drive optical e.g. compact disk digital versatile disk etc or semiconductor storage medium volatile and non volatile .

