---

title: Round-trip translation for automated grammatical error correction
abstract: Systems and methods are provided for correcting a grammatical error in a text sequence. A first text sequence in a first language is received. The first text sequence is translated to a second language to provide a first translated text. The first text sequence is translated to a third language to provide a second translated text. The third language is different from the second language. The first translated text is translated to the first language to provide a first back translation. The second translated text is translated to the first language to provide a second back translation. A plurality of candidate text sequences that include features of the first back translation and the second back translation are determined. The plurality of candidate text sequences include alternative grammatical options for the first text sequence. The plurality of candidate text sequences are scored with the processing system.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09342499&OS=09342499&RS=09342499
owner: Educational Testing Service
number: 09342499
owner_city: Princeton
owner_country: US
publication_date: 20140319
---
This application claims priority to U.S. Provisional Patent Application No. 61 803 152 filed Mar. 19 2013 entitled Round Trip Machine Translation for Grammatical Error Correction which is herein incorporated by reference in its entirety.

The technology described in this patent document relates generally to computer based grammatical error correction and more particularly to the use of round trip translations for automatically correcting a grammatical error in a text sequence.

Worldwide there are an estimated 750 million people who use English as a second language as compared to 375 million native English speakers. In addition in the United States alone there are an estimated 4 to 5 million students with limited English proficiency in public schools as well as a large number of international students in American colleges and universities. These numbers highlight a growing need for support for non native speakers of English who must perform academically in English though their English language proficiency may be limited. There are several different types of errors that non native speakers may make some of which include grammatical errors. Although such grammatical errors can be detected and corrected manually by humans a length of time required to perform the detection and correction may be considerable.

The present disclosure is directed to a computer implemented method system and non transitory computer readable storage medium for correcting a grammatical error in a text sequence. In an example computer implemented method of correcting a grammatical error in a text sequence a first text sequence in a first language is received. The first text sequence is translated to a second language with a processing system to provide a first translated text. The first text sequence is translated to a third language with the processing system to provide a second translated text where the third language is different from the second language. The first translated text is translated back to the first language with the processing system to provide a first back translation. The translating of the first translated text back to the first language is performed without reference to the first text sequence. The second translated text is translated back to the first language with the processing system to provide a second back translation. The translating of the second translated text back to the first language is performed without reference to the first text sequence. A plurality of candidate text sequences that include features of the first back translation and the second back translation are determined with the processing system. The plurality of candidate text sequences include alternative grammatical options for the first text sequence. Each of the plurality of candidate text sequences is scored with the processing system where the scoring provides a measure of a grammaticality of each of the candidate text sequences.

An example system for correcting a grammatical error in a text sequence includes a data processor and a computer readable memory in communication with the data processor. The computer readable memory is encoded with instructions for commanding the data processor to execute steps. In executing the steps a first text sequence in a first language is received. The first text sequence is translated to a second language with the data processor to provide a first translated text. The first text sequence is translated to a third language with the data processor to provide a second translated text where the third language is different from the second language. The first translated text is translated back to the first language with the data processor to provide a first back translation. The translating of the first translated text back to the first language is performed without reference to the first text sequence. The second translated text is translated back to the first language with the data processor to provide a second back translation. The translating of the second translated text back to the first language is performed without reference to the first text sequence. A plurality of candidate text sequences that include features of the first back translation and the second back translation are determined with the data processor. The plurality of candidate text sequences include alternative grammatical options for the first text sequence. Each of the plurality of candidate text sequences is scored with the processing system where the scoring provides a measure of a grammaticality of each of the candidate text sequences.

In an example non transitory computer readable storage medium for correcting a grammatical error in a text sequence the computer readable storage medium includes computer executable instructions which when executed cause a processing system to execute steps. In executing the steps a first text sequence in a first language is received. The first text sequence is translated to a second language with the processing system to provide a first translated text. The first text sequence is translated to a third language with the processing system to provide a second translated text where the third language is different from the second language. The first translated text is translated back to the first language with the processing system to provide a first back translation. The translating of the first translated text back to the first language is performed without reference to the first text sequence. The second translated text is translated back to the first language with the processing system to provide a second back translation. The translating of the second translated text back to the first language is performed without reference to the first text sequence. A plurality of candidate text sequences that include features of the first back translation and the second back translation are determined with the processing system. The plurality of candidate text sequences include alternative grammatical options for the first text sequence. Each of the plurality of candidate text sequences is scored with the processing system where the scoring provides a measure of a grammaticality of each of the candidate text sequences.

As described in greater detail below the candidate text sequences are generated based in part on round trip language translations of the first text sequence into and back from multiple different languages using machine translations i.e. translations of text using a computer based translation algorithm with a computer processing system. A round trip translation which may also be referred to as bidirectional translation or back and forth translation involves conducting a machine translation of a sequence of text from a first language to a second language the second language may be referred to as a pivot language and then conducting a machine translation of the initially translated text in the second language back to the first language. The translation of the sequence of text in the second language back to the first language is performed without reference to the original sequence of text. In other words the back translation of text back from the second language to the first language is not simply a reversal of steps that undoes the original translation. Rather the machine translation algorithm for translating text from the first language to the second language utilizes an independent algorithm compared to the algorithm for translating text from the second language to the first language. Thus an example round trip translation includes translating an original sentence in English to French and then translating the sentence in French back to English where the French to English translation is performed without reference to the original English sentence. The machine translation algorithm for translating French to English is an independent algorithm compared to that for translating text from English to French.

With reference again to the first text sequence is in a first language. Although the first text sequence comprises a sentence in the example noted above in other examples the first text sequence could be a phrase a group of sentences or a text sequence of any other size or type. The first text sequence is received at a machine translation module of the computer based system. The machine translation module is implemented in the computer based system with hardware and or software to automatically translate text or speech from one natural language to another. The translations are automatic in the sense that the translations are carried out by the translation algorithm s without the need for human decision making regarding substantive aspects of the translations during the translations. Specifically in the example of the machine translation module is used to translate the first text sequence e.g. in English to a second language e.g. French to provide a first translated text . Similarly the machine translation module is used to translate the first text sequence e.g. in English to a third language e.g. Spanish to provide a second translated text . The third language is different from the second language.

The first and second translated texts are received at machine translation modules which may be part of the same computer application or a different computer application from that which embodies the machine translation module described above. The machine translation modules utilize automated computer based machine translation techniques to generate a first back translation and a second back translation . Specifically the machine translation module translates the first translated text from the second language back to the first language to generate the first back translation . The first back translation is thus an example of a round trip i.e. bidirectional translation where the first text sequence is translated to the second language to provide the first translated text and the first translated text is translated back to the first language to provide the first back translation . In a similar manner the machine translation module translates the second translated text from the third language back to the first language to generate the second back translation .

The translation of the first translated text back to the first language is performed without reference to the first text sequence and similarly the translation of the second translated text back to the first language is performed without reference to the first text sequence . In this regard the generation of the back translations involves different algorithms e.g. utilizing different rule sets or statistical models than those that are used to translate the first text sequence to the second and third languages. As an example the translation of the first text sequence to the second language to generate the first translated text utilizes a first rule set or statistical model where the first rule set or statistical model is configured specifically for translating text from the first language to the second language. By contrast the translation of the first translated text back to the first language to generate the first back translation utilizes a second rule set or statistical model where the second rule set or statistical model is configured specifically for translating text from the second language to the first language. The first and second rule sets or statistical models are configured to perform different types of translations i.e. translations involving different starting and ending languages and thus differ from each other.

Likewise the translation of the first text sequence to the third language to generate the second translated text utilizes a third rule set or statistical model i.e. configured specifically for translating text from the first language to the third language and the translation of the second translated text back to the first language to generate the second back translation utilizes a fourth rule set or statistical model i.e. configured specifically for translating text from the third language to the first language . Because the back translations are generated without reference to the first text sequence and use different rule sets or statistical models than those used in the forward translations the back translations may differ from the first text sequence even though they are now all in the same original language.

As noted above the third language is different from the second language such that in providing the first and second back translations a plurality of pivot languages are used. Although two pivot languages are used in the example of i.e. the second language and the third language in other examples additional pivot languages are used in connection with carrying out additional translations of the first text sequence . In one example the first text sequence is in English and eight 8 pivot languages are used e.g. Arabic Chinese Spanish French Italian German Swedish Russian . As described in greater detail below the example system of combines evidence present in the multiple back translations to correct the one or more grammatical errors in the first text sequence .

In one example to combine the evidence present in the first and second back translations a lattice or diagram generation module may be utilized. The lattice or diagram generation module constructs a lattice or diagram that includes features of the first and second back translations . In an example the lattice or diagram generated by the lattice generation module is a data structure that compactly encodes sequences of words e.g. sequences of words from the first sequence and sequences of words from the first and second back translations along with a likelihood that one word follows another in a text sequence in the first language. Such data structures may be visualized as graphical constructs where circles or other shapes represent nodes of the lattice or diagram and where arrows or lines represent edges connecting the nodes. An example is shown for lattice of which represents in this example a lattice combining the attributes of the first text sequence and only one back translation from the Russian pivot language for simplicity of illustration. However it should be appreciated that a data structure referred to as a lattice or diagram herein need not be graphically illustrated displayed or reproduced e.g. printed in order to be considered a lattice or diagram. To facilitate the description a lattice or diagram as referred to herein may refer to either the underlying data structure or the graphical visualization display or reproduction or both.

More generally a given lattice diagram will include text features from the first text sequence in the first language and multiple back translations each in the first language using multiple pivot languages. Each node of a lattice includes a unit of text e.g. a word phrase punctuation contiguous sequence of words etc. of the first text sequence the first back translation the second back translation and any additional back translations and the edges connect each node to one or more other nodes. As described in greater detail below with reference to the lattice may be constructed based on an alignment procedure whereby units of text in the first text sequence are aligned with corresponding units of text in the first and second back translations . The aligning of the units of text within the first text sequence and the first and second back translations defines alternative formulations for units of text of the first text sequence . Such alternative formulations are added as nodes on the lattice and thus define alternative paths of the lattice . In one example the various alternative paths of the lattice yield different renderings of the first text sequence after forward and back translation using different pivot languages. In the example these different renderings represent various candidates of the original text sequence for which any grammatical errors in the original text sequence may have been remedied. One or more of the candidate text sequences may be chosen e.g. the one or two that are determined to be most fluent and a selected candidate text sequence selected rendering may represent the first text sequence with one or more changes to correct the grammatical errors. Any suitable diagrammatic analysis algorithm s may be used to identify the various nodal and edge connection options represented by multiple back translations of the first text sequence . Each of the back translations of the first text sequence represents a possible path through the lattice but there may be additional paths through the lattice that are not identical to any of the back translations of the first text sequence .

The lattice generated by the lattice generation module is received at a lattice analysis module and the lattice analysis module determines a plurality of paths through the lattice . Each path of the plurality of paths represents a candidate text sequence of the first text sequence and each candidate text sequence includes alternative grammatical options for the first text sequence . It should be understood that the number of paths through the lattice may be greater than the number of pivot languages i.e. greater than the number of back translations. Thus depicts n candidate text sequences where each candidate text sequence presents a variation of the first text sequence with one or more changes thereto which may reflect corrections of the one or more grammatical errors of the first text sequence . Example candidate text sequences are depicted in and discussed in further detail below with reference to that figure. In determining the plurality of paths through the lattice that define the candidate text sequences various statistical measures non statistical measures and natural language processing techniques may be used. In an example a path of the lattice used in generating a candidate text sequence is a path that represents a shortest path through the lattice . As another example a path of the lattice used in generating another candidate text sequence is determined by selecting at each node of the lattice an outgoing edge with a largest weight. Edge weights for the lattice can be chosen in any suitable manner as will be recognized by those of ordinary skill in the art. In some examples edges can be weighted according to their relative prevalence at a given location in the lattice among the available options. In another example edges can be weighted according to prevalence with respect how prevalent a given word sequence e.g. 2 gram 3 gram etc. containing the edge is found in any suitable reference corpus. In other examples all edges may be weighted equally.

The candidate text sequences are received at a scoring engine of the computer based system such that the scoring is carried out by a computer processing system. The scoring engine may comprise an automated scoring system configured to determine scores for each of the candidate text sequences where the scores may be based on a number of grammatical errors included in the candidate text sequences or other measures e.g. a measure of the fluency of the candidate text sequences . The scores thus provide a measure of the grammaticality e.g. grammatical correctness of each of the candidate text sequences . In an example the automated scoring system may be a computer based system for automatically scoring the candidate text sequences . The scoring is automatic in the sense that the scoring is carried out by a scoring algorithm s without the need for human decision making regarding substantive aspects of the scoring during the computer based scoring process. The scoring engine may determine the scores for the candidate text sequences based on various features extracted from the candidate text sequences and a scoring model.

A suitable scoring model for scoring the candidate text sequences determined from the multiple back translations may be generated in various ways. In an example a corpus based statistical model may be used wherein the model has been trained on a large corpus of reference text deemed to be suitable for training the training involving the extraction and counting of sequences of adjacent word and part of speech pairs bigrams to determine a set of statistics against which the scoring model can score the candidate text sequences. In this regard the scoring model may identify multiple bigrams from each candidate text sequence and then identify those bigrams that occur infrequently in the corpus. For a given candidate text sequence each bigram of that candidate text sequence can be checked against the corpus statistics for that same bigram and if that bigram occurs infrequently in the corpus e.g. below a threshold frequency that bigram can be flagged e.g. counted for that text sequence. The total number of bigrams in a given text sequence that fall below a threshold frequency can be the score for that candidate text sequence. For instance if there are 12 bigrams in a given text sequence and the corpus statistics for three of them are below a predetermined frequency threshold the score could be registered as 3. Further that score may be viewed as a raw score that can be further processed into a normalized score based on a suitable normalization e.g based on the length of the candidate text sequence. In this example relatively lower scores would be relatively more grammatically correct and in one example the lowest scoring candidate could be selected as the best choice i.e. the one that most likely corrects the grammatical error s in the first original text sequence. Alternatively the corpus statistics could be used in a linear scoring model wherein bigrams that have high frequencies of occurrence in the corpus are given large weights. In this example a given bigram that occurs in a given text sequence may be given a subscore equal to the number of times that the given bigram occurs in the candidate text sequence multiplied by the weight for that bigram. The subscores for each bigram in that candidate text sequence can be added to provide the overall score for that candidate text sequence. In this example relatively higher scores would be relatively more grammatically correct and in one example the highest scoring candidate could be selected as the best choice i.e. the one that most likely corrects the grammatical error s in the first original text sequence. In other examples each of these scoring models could be extended to go beyond bigrams so as to also include scoring for trigrams in each candidate text sequence for instance in which case the scoring models would need to be trained to include the statistics for trigrams from the corpus.

The example system of does not target specific error types and rather uses the combination of evidence from multiple round trip translations e.g. the first and second round trip translations to correct all types of grammatical errors included in the first text sequence . The example system of may thus be able to correct complex compound errors in the first text sequence e.g. compounded errors of style and grammar etc. . In examples where the first text sequence is a sentence the example system of is configured to provide whole sentence grammatical error correction of all types of grammatical errors. In addition to providing grammar correction for the first text sequence as described above it should be understood that the example system described herein can also process a second text sequence a third text sequence and so on for grammar correction.

As an example of the machine translation performed to generate the plurality of back translations illustrated in generating the Chinese back translation the original sentence in English is translated to Chinese and the resulting Chinese sentence is then translated back to English. In translating the Chinese sentence back to English the translation is performed without reference to the original sentence. The round trip translations in may thus define alternative formulations of the original sentence. In an example the translations may be generated using any suitable machine translation algorithms such as for example the Google Translate application programming interface API known to those of ordinary skill in the art. In other examples various other statistical machine translation SMT systems or techniques may be used such as the Moses and Joshua SMT systems known to those of ordinary skill in the art. The Google Translate API and other systems utilize SMT approaches to generate translations on the basis of statistical models whose parameters are derived from an analysis of multi lingual text corpora. In other examples the machine translation performed in the system described herein may utilize a rule based approach to machine translation.

The original sentence of includes one or more grammatical errors and reads as follows Both experience and books are very important about living. In this original sentence the portion of the sentence about living includes the one or more grammatical errors i.e. without the one or more grammatical errors the sentence may read Both experience and books are very important in life. . As illustrated in the Swedish back translation reads Both experience and books are very important in live. The Italian back translation reads Both books are very important experience and life. The Russian back translation reads And the experience and a very important book about life. The French back translation reads Both experience and the books are very important in life. The German back translation reads Both experience and books are very important about life. The Chinese back translation reads Related to the life experiences and the books are very important. The Spanish back translation reads Both experience and the books are very important about life. The Arabic back translation reads Both experience and books are very important for life. 

In an example a correction of the original sentence of may be determined by selecting a most fluent back translation of the eight back translations generated by the machine translation system. For example the most fluent back translation may be determined using an n gram language modeling technique. Specifically in an example a 5 gram model i.e. an n gram model with n 5 trained on an English gigaword corpus is used to select a most fluent back translation of the eight back translations. In other examples described in greater detail below however the correction of the original sentence is determined by combining features included in multiple of the different back translations. In such other examples the correction may be equivalent to none of the back translations.

In the alignment procedure for each unit of text of the original sentence e.g. each word and punctuation mark a corresponding unit of text is determined in the back translation . In an example to automatically align the original sentence and the back translation i.e. without human intervention or requiring only minimal human intervention a machine translation metric is used. The machine translation metric may consider the original sentence and the back translation and determine a least number of edit operations that can be employed on the back translation to transform the back translation into the original sentence . Various other automated procedures for determining corresponding units of text in the original sentence and the back translation may be used in other examples.

In an example the Ter Plus TERp machine translation metric known to those of ordinary skill in the art may be utilized in aligning an original sentence and a back translation . Applied in this way the TERp metric processes the original sentence and the back translation and determines a least number of edit operations that can be employed on the back translation to transform the back translation into the original sentence . Using the TERp metric in determining the least number of edit operations an alignment is produced between the original sentence and the back translation such that for each unit of text in the original sentence a corresponding unit of text in the back translation is determined. As depicted in the example alignment of in determining the corresponding units of text in the back translation an order of words or phrases in the back translation may be shifted in order to allow for a better alignment with the original sentence or to obtain a smaller edit cost as determined by the TERp metric. For instance as indicated by the asterisk near the word book of the back translation the word book is placed in a different location in the aligned back translation as compared to the Russian back translation shown in .

In each alignment link between the original sentence and the back translation is associated with an edit operation . The edit operations of include the edit operations considered in the TERp metric i.e. those edit operations considered in the TERp metric in determining the least number of edit operations that can be employed to transform the back translation into the original sequence . Specifically the edit operations considered by the TERp metric may include matches denoted by the letter M in the edit operations of substitutions denoted by the letter S insertions denoted by the letter I deletions not depicted in the example alignment procedure of paraphrase substitutions not depicted in the example alignment procedure of synonym matches denoted by the letter Y in and stem matches denoted by the letter T . Additionally in the asterisk next to the word book denotes that the TERp metric shifted the position of the word book before computing an edit operation for the word.

The match operation in TERp is used to indicate an exact identical match between the original sentence and the back translation . For example in the word experience from the original sentence exactly matches the word experience from the back translation . The substitution operation in TERp substitutes a word of the original sentence with a different word of the back translation . In for instance the word the from the back translation is substituted for the word both of the original sentence . The insertion operation in TERp causes the back translation to have an additional unit of text that does not correspond to a unit of text of the original sentence . In the word and at the beginning of the back translation does not have a corresponding unit of text in the original sentence . The deletion operation in TERp deletes a unit of text of the original sentence such that the deleted unit of text is not aligned to a unit of text in the back translation .

The paraphrase substitution operation in TERp also known as a phrase substitution aligns units of text in the original sentence and the back translation if the units of text are paraphrases of each other. The synonym match operation in TERp aligns units of text in the original sentence and the back translation if the units of text are synonyms of each other. In for instance the words living and life from the original sentence and the back translation respectively are determined to be synonyms and are thus aligned based on the synonym match operation. The stem match operation in TERp aligns units of text in the original sentence and the back translation if the units of text share a same stem. In for instance the words books and book from the original sentence and the back translation respectively are determined to share a same stem and are thus matched based on the stem match operation.

In examples where alignment links between the original sentence and the back translation are defined by edit operations e.g. the edit operations in different types of edit operations not described above may be used. For instance in other examples fewer or additional types of edit operations may be used in performing the alignment procedure. In an example only match operations insertion operations deletion operations and substitution operations may be used. Additionally as explained above various other systems and algorithms for automatically determining corresponding units of text in the original sentence and the back translation are used in other examples. Such other systems and algorithms may make alignment decisions based on criteria other than the edit operations described above.

An example of a lattice including evidence from multiple back translations is illustrated in and is described in greater detail below. by contrast illustrates an initial step in constructing such a lattice and thus depicts the lattice that includes evidence of a single back translation. It should be understood that the lattice of can be supplemented as explained below with reference to to generate the lattice that combines features of multiple back translations.

The example lattice of includes evidence of the single back translation where the single back translation is the Russian back translation of i.e. And the experience and a very important book about life. . In constructing the example lattice of an alignment procedure is performed between the original sentence and the Russian back translation. In the example of the original sentence is the original sentence of i.e. Both experience and books are very important about living. containing the one or more grammatical errors. Specifically in performing the alignment procedure between the original sentence and the Russian back translation for each unit of text in the original sentence a corresponding unit of text is determined in the Russian back translation. An example of such an alignment between the original sentence and the Russian back translation is described above with reference to where the original sentence is the original sentence of and the back translation is the Russian back translation of .

Based on the alignment procedure between the original sentence and the Russian back translation the lattice is constructed. Specifically in constructing the lattice a first path of the lattice e.g. a lattice backbone is defined based on the original sentence. The first path includes nodes for units of text e.g. words and punctuation marks of the original sentence and the nodes of the first path are connected in an order that maintains an order of the original sentence. The first path of thus includes the set of connected nodes that form the original sentence Both experience and books are very important about living. Edges between the nodes of the first path are given a weight of 1. 

To include features of the Russian back translation in the lattice nodes are added to the lattice based on the alignment procedure that is performed between the original sentence and the Russian back translation. Each added node defines an alternative unit of text for a corresponding node of the first path where the alternative unit of text is taken from the Russian back translation and where the corresponding node is determined based on the alignment procedure. To illustrate this includes a box with the box including a node of the first path i.e. the node containing the word both and a node that is added to the lattice i.e. the node containing the word the based on the alignment procedure between the original sentence and the Russian back translation. The added node thus defines an alternative unit of text the to the unit of text both of the original sentence. The alternative unit of text is included in the Russian back translation and the correspondence between the nodes is determined based on the alignment procedure between the original sentence and the Russian back translation. Specifically with reference to which depicts an example of this alignment procedure the word both of the original sentence is matched to the word the of the Russian back translation . The alternative units of text can be considered parallel units of text that define alternative paths through the lattice .

In this manner after defining the first path that functions as the backbone of the lattice nodes are added to the lattice based on the alignment procedure between the original sentence and the Russian back translation. In an example nodes are added to the lattice in instances where the alignment procedure does not result in an identical match between the original sentence and the Russian back translation. With reference to nodes are added to the lattice based on all edit operations that are not identical matches. Thus in an example nodes are added to the lattice based on the insertion substitution stemming synonymy and paraphrase operations described above because such edit operations result in corresponding units of text in the original sentence and Russian back translation that do not exactly match.

With reference to both nodes are added for the units of text and the comma book a and life of the Russian back translation because these units of text of the Russian back translation do not exactly match their corresponding units of text of the original sentence. As described above such added nodes provide alternative formulations for the corresponding aligned units of text of the first path. In an example in instances where the alignment procedure results in an identical match between the original sentence and the Russian back translation rather than creating a new separate node a node that would be created based on the Russian back translation is considered to be merged with the existing matching node of the first path. In adding the nodes to the lattice based on the alignment procedure edges produced by the original sentence and the Russian back translation that connect same nodes are merged and their weights added. This is illustrated in the example lattice where certain edges have weights of 2 based on the merging of the edges and the adding of the edges weights.

For each of the seven additional back translations nodes are added to the first path based on alignment procedures that are performed between the original sentence and each of the different back translations. Thus for example to add nodes to the first path based on the Swedish back translation an alignment procedure similar to that described above for is performed between the original sentence and the Swedish back translation. Nodes are then added to the first path in a manner similar to that described above for i.e. nodes are added to the lattice in instances where the alignment procedure does not result in an identical match between the original sentence and the Swedish back translation . In adding the nodes to the lattice based on the multiple alignment procedures edges produced by different back translations that connect same nodes are merged and their weights added. This is illustrated in the example lattice where certain edges have weights of 2 or higher based on the merging of the edges and the adding of the edges weights.

The resulting lattice after adding nodes based on each of the eight back translations of thus includes features of each of the eight different back translations. As described in further detail below with reference to the lattice includes a plurality of paths where each path of the plurality of paths includes a set of connected nodes and edges that defines a candidate text sequence. The candidate text sequences include alternative grammatical options for the original sentence. In an example the candidate text sequences represent the original sentence with one or more changes to correct the grammatical errors present in the original sentence. To determine a best candidate text sequence the candidate text sequences may be ranked and scored based on an automated scoring system.

In determining the plurality of paths through the lattice that define the candidate text sequences various statistical and non statistical measures may be used. It should be appreciated that there may be many possible paths traversing a given lattice and the number of possible paths may exceed the number of back translations carried out. In a candidate text sequence denoted as being based on a Greedy G measure may be generated by traversing the lattice using a greedy best first strategy at each node. In the greedy best first strategy at each node an outgoing edge with a largest weight is followed. As illustrated in the Greedy G measure generates the candidate text sequence Both experience and books are very important about life. Edge weights for the various nodes may be set according to any suitable approach such as described previously herein.

In a candidate text sequence denoted as being based on a 1 best 1 measure may be generated by determining a shortest path of the lattice. In an example the shortest path of the lattice is determined using the OpenFST toolkit known to those of ordinary skill in the art. The OpenFST toolkit is a library for constructing combining and searching weighted finite state transducers FSTs where a weighted finite state transducer may include systems where each transition has an input label an output label and a weight. Finite state transducers may be used to represent binary relations between pairs of strings and the weights can be used to represent the cost of taking a particular transition. As such the OpenFST toolkit may be used in the system described herein to determine the shortest path of the lattice. The 1 best 1 measure may assume similar to the Greedy G measure that the combined evidence from the different back translations present in the lattice is adequate to produce an acceptable correction of the original sentence and that no additional procedure is necessary to further evaluate the shortest path e.g. a fluency of the shortest path is not evaluated to determine a sufficiency of the candidate text sequence defined by the shortest path . In an example in using the 1 best 1 measure edge weights in the lattice are converted into costs where the conversion may be performed by multiplying the weights by 1. As illustrated in the 1 best 1 measure generates the candidate text sequence Both experience and the books are very important about life. 

A candidate text sequence denoted as being based on an LM Re ranked L measure may be generated by first determining the twenty shortest paths of the lattice. In an example the twenty shortest paths of the lattice comprise an n best n 20 list of paths through the lattice that are extracted using the OpenFST toolkit. After determining the twenty shortest paths of the lattice the paths are ranked using an external metric. In an example after determining the twenty shortest paths of the lattice the twenty paths are scored e.g. re ranked using a 5 gram language model trained on the English gigaword corpus where the 5 gram language model may be configured to determine a fluency score for each of the sentences that correspond to the twenty paths. The path with the highest score or rank is extracted as the correction of the original sentence. The LM Re ranked L measure assumes that an external method of evaluating paths of the lattice e.g. an external method of evaluating a fluency of the sentences corresponding to the various paths can help to determine a candidate text sequence that best corrects the grammatical errors of the original sentence. As illustrated in the LM Re ranked L measure generates the candidate text sequence And the experience and the books are very important in life. 

In a candidate text sequence denoted as being based on a Product Re ranked P measure may be generated in a manner that is the same as the LM Re ranked L measure described above except that the re ranking is performed based on a product of i a length of each path e.g. a cost of each hypothesis in the n best n 20 list of paths through the lattice and ii a ranking or score for the path that is generated via an external metric e.g. a fluency score for the path that is generated via the 5 gram language model . Thus in the Product Re ranked P measure the evidence from the back translations e.g. evidence regarding path lengths and the evidence from the external metric e.g. the evidence related to the fluency scores generated by the 5 gram language model are weighted equally. Specifically under the Product Re ranked P measure the n best paths e.g. twenty shortest paths of the lattice may be extracted using the OpenFST toolkit and the n best paths may thereafter be ranked based on the product described above. As illustrated in the Product Re ranked P measure generates the candidate text sequence Both experience and books are very important about life. 

A candidate text sequence denoted as being based on a LM Composition C measure may be generated by first converting edge weights of the lattice to probabilities. The lattice is then composed with a tri gram finite state language model where the tri gram finite state language model is trained on a corpus of 100 000 high scoring student essays in an example. The shortest path through the composed lattice is then extracted as the candidate text sequence. The LM Composition C measure assumes that using the n gram language model during the actual search process is better than using the n gram language model as a post processing tool on an already extracted n best list e.g. as may be used in the LM Re ranked L and Product Re ranked P measures . As illustrated in the Full LM Composition C measure generates the candidate text sequence Both experience and books are very important in life. The candidate text sequence generated by the LM Composition C measure corrects the one or more grammatical errors of the original sentence in the example of .

As is further illustrated in the example of a baseline candidate text sequence may be determined to be a most fluent back translation of the multiple different back translations generated. For instance in the example of where the eight back translations are generated using the eight different pivot languages the Russian back translation e.g. And the experience and a very important book about life as illustrated in the example Baseline correction of may be determined to be the most fluent back translation as measured by a 5 gram language model trained on the English gigaword corpus. Thus in this example the determination of the baseline candidate text sequence does not require use of the lattice in contrast to the Greedy G 1 best 1 LM Re ranked L Product Re ranked P and LM Composition C measures described above.

After determining the candidate text sequences in the manner described above the computer based system may automatically score the candidate text sequences using a scoring model as described previously herein. As mentioned previously one or more of the highest scoring candidate text sequences may be chosen as a most likely corrected version s of the original text sequence.

In another example the system and approaches described herein for correcting grammatical errors in sentences may be combined with spelling correction and focused translation to improve identification of candidate text sequences and scoring. In this regard before a sentence is received by the system for correcting grammatical errors spelling mistakes e.g. spelling mistakes that do not result in a real word may be corrected via an automated spelling correction technique. Further in another example before a sentence is received by the example system for correcting grammatical errors an error detection system or a human may first identify portions of the sentence that contain grammatical errors so that selective machine translation can be used to only translate the text containing errors while still retaining the rest of the words in the sentence. This approach may help to minimize the introduction of new grammatical errors in portions of the sentence that did not originally include grammatical errors.

As an alternative to the selective translation technique described above portions of the sentence including grammatical errors can be extracted and these extracted portions can then be subject to the round trip translation procedure individually i.e. only the extracted portions of the sentence are translated and the sentence as a whole is not translated or received by the machine translation system . As an example of this an original sentence may read Most of all luck is no use without a hard work. In the original sentence the preposition of is omitted prior to the word no and there is an extraneous article a before hard work. Using the alternative approach described here the machine translation system can be provided with the two phrasal spans of the original sentence containing the errors instead of the entire original sentence.

In an example the system for correcting grammatical errors described herein serves as a front end of a natural language processing NLP system. For example an NLP system configured to score text sequences based on content may perform best with input that does not include grammatical errors. In such an NLP system the example system described herein may be applied to correct grammatical errors in text received at the NLP system as a pre processing step thus improving the NLP system s ability to score the text based on the text s content.

In computer readable memories or data stores may include one or more data structures for storing and associating various data used in the example systems for correcting a grammatical error in a text sequence. For example a data structure stored in any of the aforementioned locations may be used to relate a first translated text with a first back translation of the first translated text. As another example a data structure may be used to relate candidate text sequences with scores assigned to the candidate text sequences. Other aspects of the example systems for correcting a grammatical error in a text sequence may be stored and associated in the one or more data structures e.g. lattices generated by a lattice generation module etc. .

A disk controller interfaces one or more optional disk drives to the system bus . These disk drives may be external or internal floppy disk drives such as external or internal CD ROM CD R CD RW or DVD drives such as or external or internal hard drives . As indicated previously these various disk drives and disk controllers are optional devices.

Each of the element managers real time data buffer conveyors file input processor database index shared access memory loader reference data buffer and data managers may include a software application stored in one or more of the disk drives connected to the disk controller the ROM and or the RAM . The processor may access one or more components as required.

A display interface may permit information from the bus to be displayed on a display in audio graphic or alphanumeric format. Communication with external devices may optionally occur using various communication ports .

In addition to these computer type components the hardware may also include data input devices such as a keyboard or other input device such as a microphone remote control pointer mouse and or joystick.

Additionally the methods and systems described herein may be implemented on many different types of processing devices by program code comprising program instructions that are executable by the device processing subsystem. The software program instructions may include source code object code machine code or any other stored data that is operable to cause a processing system to perform the methods and operations described herein and may be provided in any suitable language such as C C JAVA for example or any other suitable programming language. Other implementations may also be used however such as firmware or even appropriately designed hardware configured to carry out the methods and systems described herein.

The systems and methods data e.g. associations mappings data input data output intermediate data results final data results etc. may be stored and implemented in one or more different types of computer implemented data stores such as different types of storage devices and programming constructs e.g. RAM ROM Flash memory flat files databases programming data structures programming variables IF THEN or similar type statement constructs etc. . It is noted that data structures describe formats for use in organizing and storing data in databases programs memory or other computer readable media for use by a computer program.

The computer components software modules functions data stores and data structures described herein may be connected directly or indirectly to each other in order to allow the flow of data needed for their operations. It is also noted that a module or processor includes but is not limited to a unit of code that performs a software operation and can be implemented for example as a subroutine unit of code or as a software function unit of code or as an object as in an object oriented paradigm or as an applet or in a computer script language or as another type of computer code. The software components and or functionality may be located on a single computer or distributed across multiple computers depending upon the situation at hand.

While the disclosure has been described in detail and with reference to specific embodiments thereof it will be apparent to one skilled in the art that various changes and modifications can be made therein without departing from the spirit and scope of the embodiments. Thus it is intended that the present disclosure cover the modifications and variations of this disclosure provided they come within the scope of the appended claims and their equivalents.

