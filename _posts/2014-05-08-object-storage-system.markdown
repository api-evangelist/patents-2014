---

title: Object storage system
abstract: The storage system exports logical storage volumes that are provisioned as storage objects. These storage objects are accessed on demand by connected computer systems using standard protocols, such as SCSI and NFS, through logical endpoints for the protocol traffic that are configured in the storage system. Logical storage volumes are created from a logical storage container having an address space that maps to storage locations of the physical data storage units. Each of the logical storage volumes so created has an address space that maps to the address space of the logical storage container. A logical storage container may span more than one storage system and logical storage volumes of different customers can be provisioned from the same logical storage container with appropriate security settings.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08959312&OS=08959312&RS=08959312
owner: VMware, Inc.
number: 08959312
owner_city: Palo Alto
owner_country: US
publication_date: 20140508
---
This application is a continuation which claims priority from U.S. patent application Ser. No. 13 219 358 filed Aug. 26 2011 entitled Object Storage System. 

As computer systems scale to enterprise levels particularly in the context of supporting large scale data centers the underlying data storage systems frequently employ a storage area network SAN or network attached storage NAS . As is conventionally well appreciated SAN or NAS provides a number of technical capabilities and operational benefits fundamentally including virtualization of data storage devices redundancy of physical devices with transparent fault tolerant fail over and fail safe controls geographically distributed and replicated storage and centralized oversight and storage configuration management decoupled from client centric computer systems management.

Architecturally the storage devices in a SAN storage system e.g. disk arrays etc. are typically connected to network switches e.g. Fibre Channel switches etc. which are then connected to servers or hosts that require access to the data in the storage devices. The servers switches and storage devices in a SAN typically communicate using the Small Computer System Interface SCSI protocol which transfers data across the network at the level of disk data blocks. In contrast a NAS device is typically a device that internally contains one or more storage drives and that is connected to the hosts or intermediating switches through a network protocol such as Ethernet. In addition to containing storage devices the NAS device has also pre formatted its storage devices in accordance with a network based file system such as Network File System NFS or Common Internet File System CIFS . As such as opposed to a SAN which exposes disks referred to as LUNs and further detailed below to the hosts which then need to be formatted and then mounted according to a file system utilized by the hosts the NAS device s network based file system which needs to be supported by the operating system of the hosts causes the NAS device to appear as a file server to the operating systems of hosts which can then mount or map the NAS device for example as a network drive accessible by the operating system. It should be recognized that with the continuing innovation and release of new products by storage system vendors clear distinctions between SAN and NAS storage systems continue to fade with actual storage system implementations often exhibiting characteristics of both offering both file level protocols NAS and block level protocols SAN in the same system. For example in an alternative NAS architecture a NAS head or gateway device is networked to the host rather than a traditional NAS device. Such a NAS gateway device does not itself contain storage drives but enables external storage devices to be connected to the NAS gateway device e.g. via a Fibre Channel interface etc. . Such a NAS gateway device which is perceived by the hosts in a similar fashion as a traditional NAS device provides a capability to significantly increase the capacity of a NAS based storage architecture e.g. at storage capacity levels more traditionally supported by SANs while retaining the simplicity of file level storage access.

SCSI and other block protocol based storage devices such as a storage system shown in utilize a storage system manager which represents one or more programmed storage processors to aggregate the storage units or drives in the storage device and present them as one or more LUNs Logical Unit Numbers each with a uniquely identifiable number. LUNs are accessed by one or more computer systems through a physical host bus adapter HBA over a network e.g. Fiber Channel etc. . Within computer system and above HBA storage access abstractions are characteristically implemented through a series of software layers beginning with a low level device driver layer and ending in an operating system specific file system layers . Device driver layer which enables basic access to LUNs is typically specific to the communication protocol used by the storage system e.g. SCSI etc. . A data access layer may be implemented above device driver layer to support multipath consolidation of LUNs visible through HBA and other data access control and management functions. A logical volume manager typically implemented between data access layer and conventional operating system file system layers supports volume oriented virtualization and management of LUNs that are accessible through HBA . Multiple LUNs can be gathered and managed together as a volume under the control of logical volume manager for presentation to and use by file system layers as a logical device.

Storage system manager implements a virtualization of physical typically disk drive based storage units referred to in as spindles that reside in storage system . From a logical perspective each of these spindles can be thought of as a sequential array of fixed sized extents . Storage system manager abstracts away complexities of targeting read and write operations to addresses of the actual spindles and extents of the disk drives by exposing to connected computer systems such as computer systems a contiguous logical storage space divided into a set of virtual SCSI devices known as LUNs . Each LUN represents some capacity that is assigned for use by computer system by virtue of existence of such LUN and presentation of such LUN to computer systems . Storage system manager maintains metadata that includes a mapping for each such LUN to an ordered list of extents wherein each such extent can be identified as a spindle extent pair and may therefore be located in any of the various spindles .

Even with the advancements in storage systems described above it has been widely recognized that they are not sufficiently scalable to meet the particular needs of virtualized computer systems. For example a cluster of server machines may service as many as 10 000 virtual machines VMs each VM using a multiple number of virtual disks and a multiple number of snapshots each which may be stored for example as a file on a particular LUN or FS volume. Even at a scaled down estimation of 2 virtual disks and 2 snapshots per VM this amounts to 60 000 distinct disks for the storage system to support if VMs were directly connected to physical disks i.e. 1 virtual disk or snapshot per physical disk . In addition storage device and topology management at this scale are known to be difficult. As a result the concept of datastores in which VMs are multiplexed onto a smaller set of physical storage entities e.g. LUN based VMFS clustered file systems or FS volumes such as described in U.S. Pat. No. 7 849 098 entitled Providing Multiple Concurrent Access to a File System incorporated by reference herein was developed.

In conventional storage systems employing LUNs or FS volumes workloads from multiple VMs are typically serviced by a single LUN or a single FS volume. As a result resource demands from one VM workload will affect the service levels provided to another VM workload on the same LUN or FS volume. Efficiency measures for storage such as latency and input output operations IO per second or IOPS thus vary depending on the number of workloads in a given LUN or FS volume and cannot be guaranteed. Consequently storage policies for storage systems employing LUNs or FS volumes cannot be executed on a per VM basis and service level agreement SLA guarantees cannot be given on a per VM basis. In addition data services provided by storage system vendors such as snapshot replication encryption and deduplication are provided at a granularity of the LUNs or FS volumes not at the granularity of a VM s virtual disk. As a result snapshots can be created for the entire LUN or the entire FS volume using the data services provided by storage system vendors but a snapshot for a single virtual disk of a VM cannot be created separately from the LUN or the file system in which the virtual disk is stored.

One or more embodiments are directed to a storage system that is configured to isolate workloads running therein so that SLA guarantees can be provided per workload and data services of the storage system can be provided per workload without requiring a radical redesign of the storage system. In a storage system that stores virtual disks for multiple virtual machines SLA guarantees can be provided on a per virtual disk basis and data services of the storage system can be provided on a per virtual disk basis.

According to embodiments of the invention the storage system exports logical storage volumes referred to herein as virtual volumes that are provisioned as storage objects on a per workload basis out of a logical storage capacity assignment referred to herein as storage containers. For a VM a virtual volume may be created for each of the virtual disks and snapshots of the VM. In one embodiment the virtual volumes are accessed on demand by connected computer systems using standard protocols such as SCSI and NFS through logical endpoints for the protocol traffic known as protocol endpoints that are configured in the storage system.

A method of creating a logical storage volume from a storage system that includes physical data storage units DSUs according to an embodiment of the present invention includes the steps creating a logical storage container having an address space that maps to storage locations of the DSUs and creating a plurality of logical storage volumes each having an address space that maps to the address space of the logical storage container and each of which is a storage object to which IOs are directed.

A storage system cluster according to an embodiment of the present invention includes first and second storage systems each having a set of physical DSUs and a distributed storage system manager that is configured to create a logical storage container that maps to storage locations of DSUs of both the first and second storage systems and to create a plurality of logical storage volumes from the logical storage container.

Embodiments of the present invention further include a non transitory computer readable storage medium storing instructions that when executed by a computer system cause the computer system to perform one of the methods set forth above.

Distributed storage system manager or a single storage system manager or may create vvols e.g. upon request of a computer system etc. from logical storage containers which represent a logical aggregation of physical DSUs. In general a storage container may span more than one storage system and many storage containers may be created by a single storage system manager or a distributed storage system manager. Similarly a single storage system may contain many storage containers. In storage container created by distributed storage system manager is shown as spanning storage system and storage system whereas storage container and storage container are shown as being contained within a single storage system i.e. storage system and storage system respectively . It should be recognized that because a storage container can span more than one storage system a storage system administrator can provision to its customers a storage capacity that exceeds the storage capacity of any one storage system. It should be further recognized that because multiple storage containers can be created within a single storage system the storage system administrator can provision storage to multiple customers using a single storage system.

In the embodiment of each vvol is provisioned from a block based storage system. In the embodiment of a NAS based storage system implements a file system on top of DSUs and each vvol is exposed to computer systems as a file object within this file system. In addition as will be described in further detail below applications running on computer systems access vvols for IO through PEs. For example as illustrated in dashed lines in vvol and vvol are accessible via PE vvol and vvol are accessible via PE vvol is accessible via PE and PE and vvol is accessible via PE . It should be recognized that vvols from multiple storage containers such as vvol in storage container and vvol in storage container may be accessible via a single PE such as PE at any given time. It should further be recognized that PEs such as PE may exist in the absence of any vvols that are accessible via them.

In the embodiment of storage systems implement PEs as a special type of LUN using known methods for setting up LUNs. As with LUNs a storage system provides each PE a unique identifier known as a WWN World Wide Name . In one embodiment when creating the PEs storage system does not specify a size for the special LUN because the PEs described herein are not actual data containers. In one such embodiment storage system may assign a zero value or a very small value as the size of a PE related LUN such that administrators can quickly identify PEs when requesting that a storage system provide a list of LUNs e.g. traditional data LUNs and PE related LUNs as further discussed below. Similarly storage system may assign a LUN number greater than 255 as the identifying number for the LUN to the PEs to indicate in a human friendly way that they are not data LUNs. As another way to distinguish between the PEs and LUNs a PE bit may be added to the Extended Inquiry Data VPD page page 86h . The PE bit is set to 1 when a LUN is a PE and to 0 when it is a regular data LUN. Computer systems may discover the PEs via the in band path by issuing a SCSI command REPORT LUNS and determine whether they are PEs according to embodiments described herein or conventional data LUNs by examining the indicated PE bit. Computer systems may optionally inspect the LUN size and LUN number properties to further confirm whether the LUN is a PE or a conventional LUN. It should be recognized that any one of the techniques described above may be used to distinguish a PE related LUN from a regular data LUN. In one embodiment the PE bit technique is the only technique that is used to distinguish a PE related LUN from a regular data LUN.

In the embodiment of the PEs are created in storage systems using known methods for setting up mount points to FS volumes. Each PE that is created in the embodiment of is identified uniquely by an IP address and file system path also conventionally referred together as a mount point. However unlike conventional mount points the PEs are not associated with FS volumes. In addition unlike the PEs of the PEs of are not discoverable by computer systems via the in band path unless virtual volumes are bound to a given PE. Therefore the PEs of are reported by the storage system via the out of band path.

In the example of distributed storage system manager has created three storage containers SC SC and SC from DSUs each of which is shown to have spindle extents labeled P through Pn. In general each storage container has a fixed physical size and is associated with specific extents of DSUs. In the example shown in distributed storage system manager has access to a container database that stores for each storage container its container ID physical layout information and some metadata. Container database is managed and updated by a container manager which in one embodiment is a component of distributed storage system manager . The container ID is a universally unique identifier that is given to the storage container when the storage container is created. Physical layout information consists of the spindle extents of DSUs that are associated with the given storage container and stored as an ordered list of . The metadata section may contain some common and some storage system vendor specific metadata. For example the metadata section may contain the IDs of computer systems or applications or users that are permitted to access the storage container. As another example the metadata section contains an allocation bitmap to denote which extents of the storage container are already allocated to existing vvols and which ones are free. In one embodiment a storage system administrator may create separate storage containers for different business units so that vvols of different business units are not provisioned from the same storage container. Other policies for segregating vvols may be applied. For example a storage system administrator may adopt a policy that vvols of different customers of a cloud service are to be provisioned from different storage containers. Also vvols may be grouped and provisioned from storage containers according to their required service levels. In addition a storage system administrator may create delete and otherwise manage storage containers such as defining the number of storage containers that can be created and setting the maximum physical size that can be set per storage container.

Also in the example of distributed storage system manager has provisioned on behalf of requesting computer systems multiple vvols each from a different storage container. In general vvols may have a fixed physical size or may be thinly provisioned and each vvol has a vvol ID which is a universally unique identifier that is given to the vvol when the vvol is created. For each vvol a vvol database stores for each vvol its vvol ID the container ID of the storage container in which the vvol is created and an ordered list of values within that storage container that comprise the address space of the vvol. Vvol database is managed and updated by volume manager which in one embodiment is a component of distributed storage system manager . In one embodiment vvol database also stores a small amount of metadata about the vvol. This metadata is stored in vvol database as a set of key value pairs and may be updated and queried by computer systems via the out of band path at any time during the vvol s existence. Stored key value pairs fall into three categories. The first category is well known keys the definition of certain keys and hence the interpretation of their values are publicly available. One example is a key that corresponds to the virtual volume type e.g. in virtual machine embodiments whether the vvol contains a VM s metadata or a VM s data . Another example is the App ID which is the ID of the application that stored data in the vvol. The second category is computer system specific keys the computer system or its management module stores certain keys and values as the virtual volume s metadata. The third category is storage system vendor specific keys these allow the storage system vendor to store certain keys associated with the virtual volume s metadata. One reason for a storage system vendor to use this key value store for its metadata is that all of these keys are readily available to storage system vendor plug ins and other extensions via the out of band channel for vvols. The store operations for key value pairs are part of virtual volume creation and other processes and thus the store operation should be reasonably fast. Storage systems are also configured to enable searches of virtual volumes based on exact matches to values provided on specific keys.

IO manager is a software module also in certain embodiments a component of distributed storage system manager that maintains a connection database that stores currently valid IO connection paths between PEs and vvols. In the example shown in seven currently valid IO sessions are shown. Each valid session has an associated PE ID secondary level identifier SLLID vvol ID and reference count RefCnt indicating the number of different applications that are performing IO through this IO session. The process of establishing a valid IO session between a PE and a vvol by distributed storage system manager e.g. on request by a computer system is referred to herein as a bind process. For each bind distributed storage system manager e.g. via IO manager adds an entry to connection database . The process of subsequently tearing down the IO session by distributed storage system manager is referred to herein as an unbind process. For each unbind distributed storage system manager e.g. via IO manager decrements the reference count of the IO session by one. When the reference count of an IO session is at zero distributed storage system manager e.g. via IO manager may delete the entry for that IO connection path from connection database . As previously discussed in one embodiment computer systems generate and transmit bind and unbind requests via the out of band path to distributed storage system manager . Alternatively computer systems may generate and transmit unbind requests via an in band path by overloading existing error paths. In one embodiment the generation number is changed to a monotonically increasing number or a randomly generated number when the reference count changes from 0 to 1 or vice versa. In another embodiment the generation number is a randomly generated number and the RefCnt column is eliminated from connection database and for each bind even when the bind request is to a vvol that is already bound distributed storage system manager e.g. via IO manager adds an entry to connection database .

In the storage system cluster of IO manager processes IO requests IOs from computer systems received through the PEs using connection database . When an IO is received at one of the PEs IO manager parses the IO to identify the PE ID and the SLLID contained in the IO in order to determine a vvol for which the IO was intended. By accessing connection database IO manager is then able to retrieve the vvol ID associated with the parsed PE ID and SLLID. In and subsequent figures PE ID is shown as PE A PE B etc. for simplicity. In one embodiment the actual PE IDs are the WWNs of the PEs. In addition SLLID is shown as S S etc. The actual SLLIDs are generated by distributed storage system manager as any unique number among SLLIDs associated with a given PE ID in connection database . The mapping between the logical address space of the virtual volume having the vvol ID and the physical locations of DSUs is carried out by volume manager using vvol database and by container manager using container database . Once the physical locations of DSUs have been obtained data access layer in one embodiment also a component of distributed storage system manager performs IO on these physical locations.

In the storage system cluster of IOs are received through the PEs and each such IO includes an NFS handle or similar file system handle to which the IO has been issued. In one embodiment connection database for such a system contains the IP address of the NFS interface of the storage system as the PE ID and the file system path as the SLLID. The SLLIDs are generated based on the location of the vvol in the file system . The mapping between the logical address space of the vvol and the physical locations of DSUs is carried out by volume manager using vvol database and by container manager using container database . Once the physical locations of DSUs have been obtained data access layer performs IO on these physical locations. It should be recognized that for a storage system of container database may contain an ordered list of file entries in the Container Locations entry for a given vvol i.e. a vvol can be comprised of multiple file segments that are stored in the file system .

In one embodiment connection database is maintained in volatile memory while vvol database and container database are maintained in persistent storage such as DSUs . In other embodiments all of the databases may be maintained in persistent storage.

According to embodiments described herein storage capability profiles e.g. SLAs or quality of service QoS may be configured by distributed storage system manager e.g. on behalf of requesting computer systems on a per vvol basis. Therefore it is possible for vvols with different storage capability profiles to be part of the same storage container. In one embodiment a system administrator defines a default storage capability profile or a number of possible storage capability profiles for newly created vvols at the time of creation of the storage container and stored in the metadata section of container database . If a storage capability profile is not explicitly specified for a new vvol being created inside a storage container the new vvol will inherit the default storage capability profile associated with the storage container.

According to embodiments described herein each application has one or more vvols associated therewith and issues IOs to block device instances of the vvols created by operating system pursuant to CREATE DEVICE calls by application into operating system . The association between block device names and vvol IDs are maintained in block device database . IOs from applications are received by a file system driver which converts them to block IOs and provides the block IOs to a virtual volume device driver . IOs from application on the other hand are shown to bypass file system driver and provided directly to virtual volume device driver signifying that application accesses its block device directly as a raw storage device e.g. as a database disk a log disk a backup archive and a content repository in the manner described in U.S. Pat. No. 7 155 558 entitled Providing Access to a Raw Data Storage Unit in a Computer System the entire contents of which are incorporated by reference herein. When virtual volume device driver receives a block IO it accesses block device database to reference a mapping between the block device name specified in the IO and the PE ID WWN of PE LUN and SLLID that define the IO connection path to the vvol associated with the block device name. In the example shown herein the block device name archive corresponds to a block device instance of vvol that was created for application and the block device names foo dbase and log correspond to block device instances of vvol vvol and vvol respectively that were created for one or more of applications . Other information that is stored in block device database includes an active bit value for each block device that indicates whether or not the block device is active and a CIF commands in flight value. An active bit of 1 signifies that IOs can be issued to the block device. An active bit of 0 signifies that the block device is inactive and IOs cannot be issued to the block device. The CIF value provides an indication of how many IOs are in flight i.e. issued but not completed. In the example shown herein the block device foo is active and has some commands in flight. The block device archive is inactive and will not accept newer commands. However it is waiting for 2 commands in flight to complete. The block device dbase is inactive with no outstanding commands. Finally the block device log is active but the application currently has no pending IOs to the device. Virtual volume device driver may choose to remove such devices from its database at any time.

In addition to performing the mapping described above virtual volume device driver issues raw block level IOs to data access layer . Data access layer includes device access layer which applies command queuing and scheduling policies to the raw block level IOs and device driver for HBA which formats the raw block level IOs in a protocol compliant format and sends them to HBA for forwarding to the PEs via an in band path. In the embodiment where SCSI protocol is used the vvol information is encoded in the SCSI LUN data field which is an 8 byte structure as specified in SAM 5 SCSI Architecture Model 5 . The PE ID is encoded in the first 2 bytes which is conventionally used for the LUN ID and the vvol information in particular the SLLID is encoded in the SCSI second level LUN ID utilizing a portion of the remaining 6 bytes.

As further shown in data access layer also includes an error handling unit for handling IO errors that are received through the in band path from the storage system. In one embodiment the IO errors received by error handling unit are propagated through the PEs by I O manager . Examples of IO error classes include path errors between computer system and the PEs PE errors and vvol errors. The error handling unit classifies all detected errors into aforementioned classes. When a path error to a PE is encountered and another path to the PE exists data access layer transmits the IO along a different path to the PE. When the IO error is a PE error error handing unit updates block device database to indicate an error condition for each block device issuing IOs through the PE. When the IO error is a vvol error error handing unit updates block device database to indicate an error condition for each block device associated with the vvol. Error handing unit may also issue an alarm or system event so that further IOs to block devices having the error condition will be rejected.

According to embodiments described herein each VM has one or more vvols associated therewith and issues IOs to block device instances of the vvols created by hypervisor pursuant to CREATE DEVICE calls by VM into hypervisor . The association between block device names and vvol IDs are maintained in block device database . IOs from VMs are received by a SCSI virtualization layer which converts them into file IOs understood by a virtual machine file system VMFS driver . VMFS driver then converts the file IOs to block IOs and provides the block IOs to virtual volume device driver . IOs from VM on the other hand are shown to bypass VMFS driver and provided directly to virtual volume device driver signifying that VM accesses its block device directly as a raw storage device e.g. as a database disk a log disk a backup archive and a content repository in the manner described in U.S. Pat. No. 7 155 558.

When virtual volume device driver receives a block IO it accesses block device database to reference a mapping between the block device name specified in the IO and the PE ID and SLLID that define the IO session to the vvol associated with the block device name. In the example shown herein the block device names dbase and log corresponds to block device instances of vvol and vvol respectively that were created for VM and the block device names vmdk2 vmdkn and snapn correspond to block device instances of vvol12 vvol16 and vvol17 respectively that were created for one or more of VMs . Other information that is stored in block device database includes an active bit value for each block device that indicates whether or not the block device is active and a CIF commands in flight value. An active bit of 1 signifies that IOs can be issued to the block device. An active bit of 0 signifies that the block device is inactive and IOs cannot be issued to the block device. The CIF value provides an indication of how many IOs are in flight i.e. issued but not completed.

In addition to performing the mapping described above virtual volume device driver issues raw block level IOs to data access layer . Data access layer includes device access layer which applies command queuing and scheduling policies to the raw block level IOs and device driver for HBA which formats the raw block level IOs in a protocol compliant format and sends them to HBA for forwarding to the PEs via an in band path. In the embodiment where SCSI protocol is used the vvol information is encoded in the SCSI LUN data field which is an 8 byte structure as specified in SAM 5 SCSI Architecture Model 5 . The PE ID is encoded in the first 2 bytes which is conventionally used for the LUN ID and the vvol information in particular the SLLID is encoded in the SCSI second level LUN ID utilizing a portion of the remaining 6 bytes. As further shown in data access layer also includes an error handling unit which functions in the same manner as error handling unit .

It should be recognized that the various terms layers and categorizations used to describe the components in may be referred to differently without departing from their functionality or the spirit or scope of the invention. For example VMM may be considered separate virtualization components between VM and hypervisor which in such a conception may itself be considered a virtualization kernel component since there exists a separate VMM for each instantiated VM. Alternatively each VMM may be considered to be a component of its corresponding virtual machine since such VMM includes the hardware emulation components for the virtual machine. In such an alternative conception for example the conceptual layer described as virtual hardware platform may be merged with and into VMM such that virtual host bus adapter is removed from i.e. since its functionality is effectuated by host bus adapter emulator .

Management server is further configured with a system manager for managing the computer systems. In one embodiment the computer systems are executing virtual machines and system manager manages the virtual machines running in the computer systems. One example of system manager that manages virtual machines is the vSphere product distributed by VMware Inc. As shown system manager communicates with a host daemon hostd running in computer system through appropriate hardware interfaces at both management server and computer system to receive resource usage reports from computer system and to initiate various management operations on applications running in computer system .

At step the storage system manager receives the request to generate the vvol via the management interface e.g. management interface or and accesses the selected storage container s metadata section in container database to verify that the request context comprising the computer system and application has sufficient permissions to create a vvol in the selected storage container. In one embodiment an error message is returned to computer system if the permission level is not sufficient. If the permission level is sufficient a unique vvol ID is generated at step . Then at step the storage system manager scans the allocation bitmap in the metadata section of container database to determine free partitions of the selected storage container. The storage system manager allocates the free partitions of the selected storage container sufficient to accommodate the requested vvol size and updates the allocation bitmap in the storage container s metadata section of container database . The storage system manager also updated vvol database with a new vvol entry. The new vvol entry includes the vvol ID generated at step ordered list of newly allocated storage container extents and metadata of the new vvol expressed as key value pairs. Then at step the storage system manager transmits the vvol ID to computer system . At step computer system associates the vvol ID with the application that requested creation of the vvol. In one embodiment one or more vvol descriptor files are maintained for each application and the vvol ID is written into a vvol descriptor file maintained for the application that requested the creation of the vvol.

As shown in not all vvols are connected to PEs. A vvol that is not connected to a PE is not aware of IOs issued by a corresponding application because an IO session is not established to the vvol. Before IOs can be issued to a vvol the vvol undergoes a bind process as a result of which the vvol will be bound to a particular PE. Once a vvol is bound to a PE IOs can be issued to the vvol until the vvol is unbound from the PE.

In one embodiment the bind request is issued by computer system via an out of band path to the storage system using a bind virtual volume API. The bind request identifies the vvol to be bound using the vvol ID and in response the storage system binds the vvol to a PE to which computer system is connected via an in band path. is a flow diagram of method steps for the computer system to discover PEs to which it is connected via an in band path. PEs configured in SCSI protocol based storage devices are discovered via an in band path using the standard SCSI command REPORT LUNS. PEs configured in NFS protocol based storage devices are discovered via an out of band path using an API. The method steps of are carried out by the computer system for each connected storage system.

At step the computer system determines whether the connected storage system is SCSI protocol based or NFS protocol based. If the storage system is SCSI protocol based the SCSI command REPORT LUNS is issued by the computer system in band to the storage system step . Then at step the computer system examines the response from the storage system in particular the PE bit associated with each of the PE IDs that are returned to distinguish between the PE related LUNs and the convention data LUNs. If the storage system is NFS protocol based an API call is issued by the computer system out of band from plug in to the management interface e.g. management interface or to get IDs of available PEs step . At step which follows steps and the computer system stores the PE IDs of PE related LUNs returned by the storage system or the PE IDs returned by the management interface for use during a bind process. It should be recognized that the PE IDs returned by SCSI protocol based storage devices each include a WWN and the PE IDs returned by NFS protocol based storage devices each include an IP address and mount point.

Continuing with the example described above where an application requests IO access to a block device associated with vvol that has not yet been bound to a PE computer system at step determines from the block device database or the vvol ID of the vvol. Then at step computer system issues through the out of band path a request to bind the vvol to the storage system.

The storage system manager receives the request to bind the vvol via the management interface e.g. management interface or at step and then carries out step which includes selecting a PE to which the vvol is to be bound generating SLLID and generation number for the selected PE and updating connection database e.g. via IO manager . The selection of the PE to which the vvol is to be bound is made according to connectivity i.e. only the PEs that have an existing in band connection to computer system are available for selection and other factors such as current IO traffic through the available PEs. In one embodiment the storage system selects from the processed and cached list of PEs the computer system sent to it according to the method of . SLLID generation differs between the embodiment employing the storage system cluster of and the embodiment employing the storage system cluster of . In the former case an SLLID that is unique for the selected PE is generated. In the latter case a file path to the file object corresponding to the vvol is generated as the SLLID. After the SLLID and the generation number have been generated for the selected PEs connection database is updated to include newly generated IO session to the vvol. Then at step ID of the selected PE the generated SLLID and the generation number are returned to computer system . Optionally in the embodiment employing the storage system cluster of a unique NFS file handle may be generated for the file object corresponding to the vvol and returned to computer system along with the ID of the selected PE the generated SLLID and the generation number. At step computer system updates block device database or to include the PE ID the SLLID and optionally the NFS handle and the generation number returned from the storage system. In particular each set of PE ID SLLID and optionally the NFS handle and the generation number returned from storage system will be added as a new entry to block device database or . It should be recognized that the generation number is used to guard against replay attacks. Therefore in embodiments where replay attacks are not a concern the generation number is not used.

On subsequent bind requests to the same vvol initiated by a different application desiring to issue IOs to the same vvol the storage system manager may bind the vvol to the same or different PE. If the vvol is bound to the same PE the storage system manager returns the ID of the same PE and the SLLID previously generated and increments the reference count of this IO connection path stored in connection database . On the other hand if the vvol is bound to a different PE the storage system manager generates a new SLLID and returns the ID of the different PE and the newly generated SLLID and adds this new IO connection path to the vvol as a new entry to connection database .

A virtual volume unbind request may be issued using an unbind virtual volume API. An unbind request includes the PE ID and SLLID of the IO connection path by which a vvol has been previously bound. The processing of the unbind request is however advisory. The storage system manager is free to unbind the vvol from a PE immediately or after a delay. The unbind request is processed by updating connection database to decrement the reference count of the entry containing the PE ID and SLLID. If the reference count is decremented to zero the entry may be deleted. It should be noted in this case that the vvol continues to exist but is not available for IO using the given PE ID and SLLID any more.

In the case of a vvol that implements a virtual disk of a VM the reference count for this vvol will be at least one. When the VM is powered off and an unbind request is issued in connection therewith the reference count will be decremented by one. If the reference count is zero the vvol entry may be removed from connection database . In general removing entries from connection database is beneficial because I O manager manages less data and can also recycle SLLIDs. Such benefits become significant when the total number of vvols stored by the storage system is large e.g. in the order of millions of vvols but the total number of vvols being actively accessed by applications is small e.g. tens of thousands of VMs . Additionally when a vvol is not bound to any PEs the storage system has greater flexibility in choosing where to store the vvol in DSUs . For example the storage system can be implemented with asymmetrical hierarchical DSUs where some DSUs provide faster data access and others provide slower data access e.g. to save on storage costs . In one implementation when a vvol is not bound to any PE which can be determined by checking the reference count of entries of the vvol in connection database the storage system can migrate the vvol to a slower and or cheaper type of physical storage. Then once the vvol is bound to a PE the storage system can migrate the vvol to a faster type of physical storage. It should be recognized that such migrations can be accomplished by changing one or more elements of the ordered list of container locations that make up the given vvol in the vvol database and updating the corresponding extent allocation bitmap in the metadata section of container database .

Binding and unbinding vvols to PEs enables the storage system manager to determine vvol liveness. The storage system manager may take advantage of this information to perform storage system vendor specific optimizations on non IO serving passive and IO serving active vvols. For example the storage system manager may be configured to relocate a vvol from a low latency high cost SSD to a mid latency low cost hard drive if it remains in a passive state beyond a particular threshold of time.

Method begins at step where an application such as application shown in or VM shown in issues an IO to a raw block device. At step virtual volume device driver or generates a raw block level IO from the IO issued by the application. At step the name of the raw block device is translated to a PE ID and SLLID by virtual volume device driver or and also to an NFS handle by NFS client or in the embodiment employing the storage device of . At step the data access layer or carries out the encoding of the PE ID and SLLID and also the NFS handle in the embodiment employing the storage device of into the raw block level IO. Then at step the HBA NIC issues the raw block level IO.

For non VM applications such as application shown in method begins at step . At step the application issues an IO to a file stored on a vvol based block device. Then at step the file system driver e.g. file system driver generates a block level IO from the file IO. After step steps and which are identical to steps and are carried out.

For VM applications such as VM shown in method begins at step . At step the VM issues an IO to its virtual disk. Then at step this IO is translated to a file IO e.g. by SCSI virtualization layer . The file system driver e.g. VMFS driver then generates a block level IO from the file IO at step . After step steps and which are identical to steps and are carried out.

In some situations an application application or VM management server and or the storage system manager may determine that a binding of a vvol to a particular PE is experiencing issues such as when the PE becomes overloaded with too many bindings. As a way to resolve such issues a bound vvol may be rebound by the storage system manager to a different PE even while IO commands are being directed to the vvol. is a flow diagram of method steps for issuing and executing a vvol rebind request according to one embodiment using a rebind API.

As shown method begins at step where the storage system manager determines that a vvol should be bound to a second PE that is different from a first PE to which the vvol is currently bound. At step the storage system manager issues via an out of band path a request to a computer system e.g. computer system running an application issuing IO to the vvol to rebind the vvol. At step computer system receives from the storage system manager the rebind request and in response issues a request to bind the vvol to a new PE. At step the storage system manager receives the rebind request and in response binds the vvol to the new PE. At step the storage system manager transmits to the computer system an ID of the new PE to which the vvol is now also bound and an SLLID to access the vvol as described above in conjunction with .

At step the computer system receives from the storage system manager the new PE ID and the SLLID. In block device database or the active bit of the new PE connection is set to 1 initially meaning that a new IO session for the vvol via the new PE has been established. The computer system also sets the active bit of the first PE connection to 0 signifying that no more IOs can be issued to the vvol through this PE connection. It should be recognized that this PE connection should not be unbound immediately upon deactivation because there may be IOs to the vvol through this PE connection that may be in flight i.e. issued but not completed. Therefore at step the computer system accesses block device database or to see if all commands in flight CIFs issued to the vvol through the first PE connection have been completed i.e. if CIF 0. The computer system waits for the CIF to go to zero before executing step . In the meantime additional IOs to the vvol are issued through the new PE since the active bit of the new PE connection is already set to 1. When the CIF does reach zero step is carried out where a request to unbind the first PE connection is issued to the storage system manager. Then at step the storage system manager unbinds the vvol from the first PE. Also the computer system issues all additional IOs to the vvol through the new PE at step .

As described above a VM may have multiple virtual disks and a separate vvol is created for each virtual disk. The VM also has metadata files that describe the configurations of the VM. The metadata files include VM configuration file VM log files disk descriptor files one for each of the virtual disks for the VM a VM swap file etc. A disk descriptor file for a virtual disk contains information relating to the virtual disk such as its vvol ID its size whether the virtual disk is thinly provisioned and identification of one or more snapshots created for the virtual disk etc. The VM swap file provides a swap space of the VM on the storage system. In one embodiment these VM configuration files are stored in a vvol and this vvol is referred to herein as a metadata vvol.

At step the host computer creates a block device instance of the metadata vvol using the CREATE DEVICE call into the host computer s operating system. Then at step the host computer creates a file system e.g. VMFS on the block device in response to which a file system ID FSID is returned. The host computer at step mounts the file system having the returned FSID and stores the metadata of the VM into the namespace associated with this file system. Examples of the metadata include VM log files disk descriptor files one for each of the virtual disks for the VM and a VM swap file.

At step the host computer initiates the method for creating a vvol for each of the virtual disks of the VM each such vvol referred to herein as data vvol in the manner described above in conjunction with pursuant to which the storage system manager at step creates the data vvol and returns the vvol ID of the data vvol to the host computer. At step the host computer stores the ID of the data vvol in the disk descriptor file for the virtual disk. The method ends with the unbinding of the metadata vvol not shown after data vvols have been created for all of the virtual disks of the VM.

Upon receiving a VM power ON command at step the ID of the metadata vvol corresponding to the VM is retrieved at step . Then at step the metadata vvol undergoes a bind process as described above in conjunction with . The file system is mounted on the metadata vvol at step so that the metadata files for the data vvols in particular the disk descriptor files can be read and data vvol IDs obtained at step . The data vvols then undergo a bind process one by one as described above in conjunction with at step .

Upon receiving a VM power OFF command at step the data vvols of the VM are marked as inactive in the block device database e.g. block device database of and the host computer waits for the CIFs associated with each of the data vvols to reach zero step . As the CIF associated with each data vvol reaches zero the host computer at step requests the storage system to unbind that data vvol. After the CIFs associated with all data vvols reach zero the metadata vvol is marked as inactive in the block device database at step . Then at step when the CIF associated with the metadata vvol reaches zero the host computer at step requests the metadata vvol to be unbound.

The method for extending the size of a data vvol for a VM s virtual disk begins at step where the host computer determines if the VM is powered ON. If the host computer determines at step that the VM is not powered ON the host computer retrieves the ID of the metadata vvol corresponding to the VM at step . Then the bind process for the metadata vvol is initiated by the host computer at step . After the bind at step the host computer mounts a file system on the metadata vvol and retrieves the ID of the data vvol corresponding to the virtual disk from the disk descriptor file for the virtual disk which is a file in the file system mounted on the metadata vvol. Then at step the host computer sends an extend vvol API call to the storage system at step where the extend vvol API call includes the ID of the data vvol and the new size of the data vvol.

If the VM is powered ON the host computer retrieves the ID of the data vvol of VM s virtual disk to be extended at step . It should be recognized from the method of that this ID can be obtained from the disk descriptor file associated with the VM s virtual disk. Then at step the host computer sends an extend vvol API call to the storage system at step where the extend vvol API call includes the ID of the data vvol and the new size of the data vvol.

The extend vvol API call results in the vvol database and the container database e.g. vvol database and container database of being updated in the storage system to reflect the increased address space of the vvol. Upon receiving acknowledgement that the extend vvol API call has completed the host computer at step updates the disk descriptor file for the VM s virtual disk with the new size. Then at step the host computer determines if the VM is powered ON. If it is not the host computer at step unmounts the file system and sends a request to unbind the metadata vvol to the storage system. If on the other hand the VM is powered ON the method terminates.

The method for moving a vvol of a VM currently bound to a PE from a source storage container to a destination storage container where both the source storage container and the destination storage container are within the scope of the same storage system manager begins at step where the container IDs of the source and destination storage containers SC and SC respectively and the vvol ID of the vvol to be moved are received. Then at step the vvol database e.g. vvol database of and the extent allocation bitmap of the container database e.g. container database of are updated as follows. First the storage system manager removes the vvol extents in SC from SC s entry in the container database and then assigns these extents to SC by modifying SC s entry in the container database . In one embodiment the storage system may compensate for the loss of storage capacity due to removal of vvol storage extents in SC by assigning new spindle extents to SC and make up for the increase in storage capacity due to addition of vvol storage extents in SC by removing some unused spindle extents from SC. At step the storage system manager determines whether the currently bound PE is able to optimally service IO to the vvol s new location. An example instance when the current PE is unable to service IO to the vvol s new location is if the storage administrator has statically configured the storage system manager to assign different PEs to vvols from different customers and hence different storage containers. If the current PE is unable to service IO to the vvol the vvol at step undergoes a rebind process and associated changes to a connection database e.g. the connection database of which is described above in conjunction with . After step step is carried out where an acknowledgement of successful move completion is returned to the host computer. If at step the storage system manager determines that the current PE is able to service IO to the new location of the vvol step is bypassed and step is performed next.

When a vvol is moved between incompatible storage containers e.g. between storage containers created in storage devices of different manufacturers data movement is executed between storage containers in addition to the changes to the container database the vvol database and the connection database . In one embodiment data movement techniques described in U.S. patent application Ser. No. 12 129 323 filed May 29 2008 and entitled Offloading Storage Operations to Storage Hardware the entire contents of which are incorporated by reference herein are employed.

If at step the storage system manager determines that the data vvols of the template VM and the new VM are not compatible an error message is returned to the host computer. Upon receipt of this error message the host computer at step issues a create vvol API call to the storage system to create new data vvols. At step the storage system manager creates new data vvols by generating new data vvol IDs updating the allocation bitmap in container database and adding new vvol entries to vvol database and returns the new data vvol IDs to the host computer. At step the host computer executes data movement according to techniques described in U.S. patent application Ser. No. 12 356 694 filed Jan. 21 2009 and entitled Data Mover for Computer System the entire contents of which are incorporated by reference herein step . After step steps and are carried out as described above.

At step the host computer initiates the method for creating a data vvol for each of the virtual disks of the VM in the manner described above in conjunction with pursuant to which the storage system manager at step creates the data vvol and returns the vvol ID of the data vvol to the host computer. At step the host computer stores the ID of the data vvol in the disk descriptor file for the virtual disk. The method ends with the unbinding of the metadata vvol not shown after data vvols have been created for all of the virtual disks of the VM.

As described above in conjunction with when a new vvol is created from a storage container and a storage capability profile is not explicitly specified for the new vvol the new vvol will inherit the storage capability profile associated with the storage container. The storage capability profile associated with the storage container may be selected from one of several different profiles. For example as shown in the different profiles include a production prod profile a development dev profile and a test profile collectively referred to herein as profiles . It should be recognized that many other profiles may be defined. As shown each profile entry of a particular profile is of a fixed type or a variable type and has a name and one or more values associated with it. A fixed type profile entry has a fixed number of selectable items. For example the profile entry Replication may be set to be TRUE or FALSE. In contrast a variable type profile entry does not have pre defined selections. Instead a default value and a range of values are set for a variable type profile entry and the user may select any value that is within the range. If no value is specified the default value is used. In the example profiles shown in variable type profile entries has three numbers separated by commas. The first number is the lower end of the specified range and the second number is the higher end of the specified range. The third number is the default value. Thus a vvol that inherits the storage capability profile defined in production profile will be replicated Replication.Value TRUE and the recovery time objective RTO for the replication may be defined in the range of 0.1 to 24 hours the default being 1 hour. In addition snapshots are allowed for this vvol Snapshot.Value TRUE . The number of snapshots that are retained is in the range of 1 to 100 the default being 1 and the frequency of snapshots is in the range of once per hour to once per 24 hours the default being once per hour. The Snapinherit column indicates whether the given profile attribute and its values should be propagated to a derivative vvol when a given vvol is snapshotted to create a new vvol that is a derivative vvol. In the example of production profile only the first two profile entries Replication and RTO may be propagated to a snapshot vvol of the given vvol with production profile . The values of all other attributes of the snapshot vvol will be set to the default values specified in the profile. In other words any customizations for example a non default value of snapshot frequency of these other attribute on the given vvol will not be propagated to the snapshot vvol due to their corresponding Snapinherit column being FALSE. The profile also contains other columns such as CloneInherit not shown and ReplicaInherit not shown that control which attribute values are propagated to clones and replicas respectively of a given vvol.

When a storage container is created according to the method of types of storage capability profiles that can be defined for vvols created from the storage container may be set. The flow diagram in illustrates the method for creating a storage container shown in with step inserted between steps and . At step the storage administrator selects one or more of profiles for the storage container being created. For example a storage container created for one customer may be associated with production profile and development profile such that a vvol that is of a production type will inherit the storage capability profile defined in production profile with default values or customer specified values as the case may be and a vvol that is of a development type will inherit the storage capability profile defined in development profile with default values or customer specified values as the case may be.

At step the storage system manager determines whether values to be used in the storage capability profile have been specified in the request to create the vvol. If they are not the storage system manager at step employs the storage capability profile associated with the vvol s storage container as the vvol s storage capability profile with default values. If the values to be used in the storage capability profile have been specified the storage system manager at step employs the storage capability profile associated with the vvol s storage container as the vvol s storage capability profile with the specified values in lieu of the default values.

In one embodiment the storage capability profile of a vvol is stored in vvol database as key value pairs. Once the storage capability profile of a vvol has been defined and stored in vvol database as key value pairs and as long as replication and snapshotting related attributes and values are part of this profile as shown in the example profiles of the storage system is able to perform replication and snapshotting for the vvol with no further instructions issued by the host computer.

After snapshots are created in the manner described above key value pairs stored in vvol database are updated to indicate that the snapshot vvols are of type snapshot. Also in embodiments where a generation number is maintained for the snapshots the generation number being incremented each time a snapshot is taken or set to be equal to date time the generation number is stored as a key value pair. The parent vvol ID of a snapshot vvol is also stored as a key value pair in snapshot vvol entries. As a result a host computer may query vvol database for snapshots corresponding to a particular vvol ID. It is also possible for the host computer to issue a query to vvol database for snapshots corresponding to a particular vvol ID and a particular generation number.

The various embodiments described herein may employ various computer implemented operations involving data stored in computer systems. For example these operations may require physical manipulation of physical quantities usually though not necessarily these quantities may take the form of electrical or magnetic signals where they or representations of them are capable of being stored transferred combined compared or otherwise manipulated. Further such manipulations are often referred to in terms such as producing identifying determining or comparing. Any operations described herein that form part of one or more embodiments may be useful machine operations. In addition one or more embodiments also relate to a device or an apparatus for performing these operations. The apparatus may be specially constructed for specific required purposes or it may be a general purpose computer selectively activated or configured by a computer program stored in the computer. In particular various general purpose machines may be used with computer programs written in accordance with the teachings herein or it may be more convenient to construct a more specialized apparatus to perform the required operations.

The various embodiments described herein may be practiced with other computer system configurations including hand held devices microprocessor systems microprocessor based or programmable consumer electronics minicomputers mainframe computers and the like.

One or more embodiments may be implemented as one or more computer programs or as one or more computer program modules embodied in one or more computer readable media. The term computer readable medium refers to any data storage device that can store data which can thereafter be input to a computer system computer readable media may be based on any existing or subsequently developed technology for embodying computer programs in a manner that enables them to be read by a computer. Examples of a computer readable medium include a hard drive network attached storage NAS read only memory random access memory e.g. a flash memory device a CD Compact Discs CD ROM a CD R or a CD RW a DVD Digital Versatile Disc a magnetic tape and other optical and non optical data storage devices. The computer readable medium can also be distributed over a network coupled computer system so that the computer readable code is stored and executed in a distributed fashion.

Although one or more embodiments have been described in some detail for clarity of understanding it will be apparent that certain changes and modifications may be made within the scope of the claims. For example SCSI is employed as the protocol for SAN devices and NFS is used as the protocol for NAS devices. Any alternative to the SCSI protocol may be used such as Fibre Channel and any alternative to the NFS protocol may be used such as CIFS Common Internet File System protocol. Accordingly the described embodiments are to be considered as illustrative and not restrictive and the scope of the claims is not to be limited to details given herein but may be modified within the scope and equivalents of the claims. In the claims elements and or steps do not imply any particular order of operation unless explicitly stated in the claims.

In addition while described virtualization methods have generally assumed that virtual machines present interfaces consistent with a particular hardware system the methods described may be used in conjunction with virtualizations that do not correspond directly to any particular hardware system. Virtualization systems in accordance with the various embodiments implemented as hosted embodiments non hosted embodiments or as embodiments that tend to blur distinctions between the two are all envisioned. Furthermore various virtualization operations may be wholly or partially implemented in hardware. For example a hardware implementation may employ a look up table for modification of storage access requests to secure non disk data.

Many variations modifications additions and improvements are possible regardless the degree of virtualization. The virtualization software can therefore include components of a host console or guest operating system that performs virtualization functions. Plural instances may be provided for components operations or structures described herein as a single instance. Finally boundaries between various components operations and data stores are somewhat arbitrary and particular operations are illustrated in the context of specific illustrative configurations. Other allocations of functionality are envisioned and may fall within the scope of embodiments described herein. In general structures and functionality presented as separate components in exemplary configurations may be implemented as a combined structure or component. Similarly structures and functionality presented as a single component may be implemented as separate components. These and other variations modifications additions and improvements may fall within the scope of the appended claims s .

