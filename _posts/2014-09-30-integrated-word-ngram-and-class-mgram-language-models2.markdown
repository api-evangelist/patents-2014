---

title: Integrated word N-gram and class M-gram language models
abstract: Systems and processes for discourse input processing are provided. In one example process, a discourse input can be received from a user. An integrated probability of a candidate word in the discourse input and one or more subclasses associated with the candidate word can be determined based on a conditional probability of the candidate word given one or more words in the discourse input, a probability of the candidate word within a corpus, and a conditional probability of the candidate word given one or more classes associated with the one or more words. A text string corresponding to the discourse input can be determined based on the integrated probability. An output based on the text string can be generated.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09606986&OS=09606986&RS=09606986
owner: Apple Inc.
number: 09606986
owner_city: Cupertino
owner_country: US
publication_date: 20140930
---
This application claims priority from U.S. Provisional Ser. No. 62 057 177 filed on Sep. 29 2014 entitled INTEGRATED WORD N GRAM AND CLASS M GRAM LANGUAGE MODELS which is hereby incorporated by reference in its entirety for all purposes.

This relates generally to discourse input processing and more specifically to the use of integrated word and class language models for discourse input processing.

Statistical language modeling can be implemented in many prediction and recognition applications such as for example speech or handwriting recognition and text input prediction. An effective language model can be desirable to constrain the underlying pattern analysis guide the search through various text hypotheses and or contribute to the determination of the final outcome. Conventionally the paradigm for statistical language modeling has been to convey the probability of occurrence in the language of all possible strings of n words. Given a vocabulary of interest for the expected domain of use this can be achieved through a word n gram model which can be trained to provide the probability of a word given the n 1 previous word s . Training word n gram models can typically involve large machine readable text databases comprising representative documents in the expected domain. However due to the finite size of such databases many occurrences of n word strings are seen infrequently yielding unreliable parameter values for all but the smallest values of n. Further in some applications it can be cumbersome or impractical to gather a large enough amount of training data. In other applications the size of the resulting model may exceed what can reasonably be deployed.

Systems and processes for processing discourse input are provided. In one example process a discourse input can be received from a user. A text string corresponding to the discourse input can be determined. A conditional probability of a candidate word in the discourse input given one or more words in the discourse input can be determined using a first language model. A probability of the candidate word within a corpus can be determined using a second language model. A conditional probability of the candidate word given one or more classes associated with the one or more words can be determined using a third language model. An integrated probability of the candidate word and one or more subclasses associated with the candidate word can be determined based on the conditional probability of the candidate word given the one or more words the probability of the candidate word within the corpus and the conditional probability of the candidate word given the one or more classes. The text string can be determined based on the integrated probability. An output can be determined based on the text string.

In the following description of examples reference is made to the accompanying drawings in which it is shown by way of illustration specific examples that can be practiced. It is to be understood that other examples can be used and structural changes can be made without departing from the scope of the various examples.

The present disclosure relates to discourse input processing using an integrated word and class language model. As described in greater detail below it can be desirable to combine a word language model with a class language model to leverage the advantages of both models. However using conventional interpolation or maximum entropy formulation to combine the models can result in a relatively coarse model combination where the relative influence of the two models is specified in a global fashion based on broad ranges of language model probabilities. It can thus be desirable to integrate the word model and the class model at a finer level of granularity. For example the word and class language models can be integrated based on the specific identity of the local context such as the frequency of occurrence of the current word in the training corpus. Processing discourse inputs using such an integrated model can result in greater accuracy and robustness.

In one example process a discourse input can be received from a user. An integrated probability of a candidate word in the discourse input and one or more subclasses associated with the candidate word can be determined based on a conditional probability of the candidate word given one or more words in the discourse input a probability of the candidate word within a corpus and a conditional probability of the candidate word given one or more classes associated with the one or more words. The probability of the candidate word within the corpus can determine the relative contribution of the conditional probability of the candidate word given the one or more words and the conditional probability of the candidate word given one or more classes to the integrated probability. A text string corresponding to the discourse input can be determined based on the integrated probability. An output based on the text string can be generated.

Specifically a digital assistant can be capable of accepting a user request at least partially in the form of a natural language command request statement narrative and or inquiry. Typically the user request can seek either an informational answer or performance of a task by the digital assistant. A satisfactory response to the user request can be a provision of the requested informational answer a performance of the requested task or a combination of the two. For example a user can ask the digital assistant a question such as Where am I right now Based on the user s current location the digital assistant can answer You are in Central Park near the west gate. The user can also request the performance of a task for example Please invite my friends to my girlfriend s birthday party next week. In response the digital assistant can acknowledge the request by saying Yes right away and then send a suitable calendar invite on behalf of the user to each of the user s friends listed in the user s electronic address book. During performance of a requested task the digital assistant can sometimes interact with the user in a continuous dialogue involving multiple exchanges of information over an extended period of time. There are numerous other ways of interacting with a digital assistant to request information or performance of various tasks. In addition to providing verbal responses and taking programmed actions the digital assistant also can provide responses in other visual or audio forms e.g. as texts alerts music videos animations etc.

An example of a digital assistant is described in Applicant s U.S. Utility application Ser. No. 12 987 982 for Intelligent Automated Assistant filed Jan. 10 2011 the entire disclosure of which is incorporated herein by reference.

As shown in in some examples a digital assistant can be implemented according to a client server model. The digital assistant can include a client side portion hereafter DA client executed on a user device and a server side portion hereafter DA server executed on a server system . The DA client can communicate with the DA server through one or more networks . The DA client can provide client side functionalities such as user facing input and output processing and communication with the DA server . The DA server can provide server side functionalities for any number of DA clients each residing on a respective user device .

In some examples the DA server can include a client facing I O interface one or more processing modules data and models and an I O interface to external services . The client facing I O interface can facilitate the client facing input and output processing for the DA server . The one or more processing modules can utilize the data and models to process speech input and determine the user s intent based on natural language input. Further the one or more processing modules perform task execution based on inferred user intent. In some examples the DA server can communicate with external services through the network s for task completion or information acquisition. The I O interface to external services can facilitate such communications.

Examples of the user device can include but are not limited to a handheld computer a personal digital assistant PDA a tablet computer a laptop computer a desktop computer a cellular telephone a smart phone an enhanced general packet radio service EGPRS mobile phone a media player a navigation device a game console a television a television set top box a remote control a wearable electronic device or a combination of any two or more of these data processing devices or other data processing devices. More details on the user device are provided in reference to an exemplary user device shown in .

Examples of the communication network s can include local area networks LAN and wide area networks WAN e.g. the Internet. The communication network s can be implemented using any known network protocol including various wired or wireless protocols such as for example Ethernet Universal Serial Bus USB FIREWIRE Global System for Mobile Communications GSM Enhanced Data GSM Environment EDGE code division multiple access CDMA time division multiple access TDMA Bluetooth Wi Fi voice over Internet Protocol VoIP Wi MAX or any other suitable communication protocol.

The server system can be implemented on one or more standalone data processing apparatus or a distributed network of computers. In some examples the server system can also employ various virtual devices and or services of third party service providers e.g. third party cloud service providers to provide the underlying computing resources and or infrastructure resources of the server system .

Although the digital assistant shown in can include both a client side portion e.g. the DA client and a server side portion e.g. the DA server in some examples the functions of a digital assistant can be implemented as a standalone application installed on a user device. In addition the divisions of functionalities between the client and server portions of the digital assistant can vary in different implementations. For instance in some examples the DA client can be a thin client that provides only user facing input and output processing functions and delegates all other functionalities of the digital assistant to a backend server.

For example a motion sensor a light sensor and a proximity sensor can be coupled to the peripherals interface to facilitate orientation light and proximity sensing functions. One or more other sensors such as a positioning system e.g. GPS receiver a temperature sensor a biometric sensor a gyro a compass an accelerometer and the like can also be connected to the peripherals interface to facilitate related functionalities.

In some examples a camera subsystem and an optical sensor can be utilized to facilitate camera functions such as taking photographs and recording video clips. Communication functions can be facilitated through one or more wired and or wireless communication subsystems which can include various communication ports radio frequency receivers and transmitters and or optical e.g. infrared receivers and transmitters. An audio subsystem can be coupled to speakers and a microphone to facilitate voice enabled functions such as voice recognition voice replication digital recording and telephony functions. The microphone can be configured to receive a speech input from the user.

In some examples an I O subsystem can also be coupled to the peripherals interface . The I O subsystem can include a touch screen controller and or other input controller s . The touch screen controller can be coupled to a touch screen . The touch screen and the touch screen controller can for example detect contact and movement or break thereof using any of a plurality of touch sensitivity technologies such as capacitive resistive infrared surface acoustic wave technologies proximity sensor arrays and the like. The other input controller s can be coupled to other input control devices such as one or more buttons rocker switches thumb wheel infrared port USB port and or a pointer device such as a stylus.

In some examples the memory interface can be coupled to memory . The memory can include any electronic magnetic optical electromagnetic infrared or semiconductor system apparatus or device a portable computer diskette magnetic a random access memory RAM magnetic a read only memory ROM magnetic an erasable programmable read only memory EPROM magnetic a portable optical disc such as CD CD R CD RW DVD DVD R or DVD RW or flash memory such as compact flash cards secured digital cards USB memory devices memory sticks and the like. In some examples a non transitory computer readable storage medium of the memory can be used to store instructions e.g. for performing the processes and described below for use by or in connection with an instruction execution system apparatus or device such as a computer based system processor containing system or other system that can fetch the instructions from the instruction execution system apparatus or device and execute the instructions. In other examples the instructions e.g. for performing the processes and described below can be stored on a non transitory computer readable storage medium not shown of the server system or can be divided between the non transitory computer readable storage medium of memory and the non transitory computer readable storage medium of server system . In the context of this document a non transitory computer readable storage medium can be any medium that can contain or store the program for use by or in connection with the instruction execution system apparatus or device.

In some examples the memory can store an operating system a communication module a user interface module a sensor processing module a phone module and applications module . The operating system can include instructions for handling basic system services and for performing hardware dependent tasks. The communication module can facilitate communicating with one or more additional devices one or more computers and or one or more servers. The user interface module can facilitate graphic user interface processing and output processing using other output channels e.g. speakers . The sensor processing module can facilitate sensor related processing and functions. The phone module can facilitate phone related processes and functions. The application module can facilitate various functionalities of user applications such as electronic messaging web browsing media processing Navigation imaging and or other processes and functions.

As described herein the memory can also store client side digital assistant instructions e.g. in a digital assistant client module and various user data e.g. user specific vocabulary data preference data and or other data such as the user s electronic address book to do lists shopping lists user specified name pronunciations etc. to provide the client side functionalities of the digital assistant.

In various examples the digital assistant client module can be capable of accepting voice input e.g. speech input text input touch input and or gestural input through various user interfaces e.g. the I O subsystem of the user device . The digital assistant client module can also be capable of providing output in audio e.g. speech output visual and or tactile forms. For example output can be provided as voice sound alerts text messages menus graphics videos animations vibrations and or combinations of two or more of the above. During operation the digital assistant client module can communicate with the DA server using the communication subsystems .

In some examples the digital assistant client module can utilize the various sensors subsystems and peripheral devices to gather additional information from the surrounding environment of the user device to establish a context associated with a user the current user interaction and or the current user input. In some examples the digital assistant client module can provide the context information or a subset thereof with the user input to the digital assistant server to help infer the user s intent. In some examples the digital assistant can also use the context information to determine how to prepare and deliver outputs to the user.

In some examples the context information that accompanies the user input can include sensor information e.g. lighting ambient noise ambient temperature images or videos of the surrounding environment etc. In some examples the context information can also include the physical state of the device e.g. device orientation device location device temperature power level speed acceleration motion patterns cellular signals strength etc. In some examples information related to the software state of the user device e.g. running processes installed programs past and present network activities background services error logs resources usage etc. of the user device can be provided to the digital assistant server as context information associated with a user input.

In some examples the DA client module can selectively provide information e.g. user data stored on the user device in response to requests from the DA server. In some examples the digital assistant client module can also elicit additional input from the user via a natural language dialogue or other user interfaces upon request by the DA server . The DA client module can pass the additional input to the digital assistant server to help the digital assistant server in intent deduction and or fulfillment of the user s intent expressed in the user request.

In various examples the memory can include additional instructions or fewer instructions. For example the DA client module can include any of the sub modules of the digital assistant module described below in . Furthermore various functions of the user device can be implemented in hardware and or in firmware including in one or more signal processing and or application specific integrated circuits.

The digital assistant system can include memory one or more processors an input output I O interface and a network communications interface . These components can communicate with one another over one or more communication buses or signal lines .

In some examples the memory can include a non transitory computer readable medium such as high speed random access memory and or a non volatile computer readable storage medium e.g. one or more magnetic disk storage devices flash memory devices or other non volatile solid state memory devices .

In some examples the I O interface can couple input output devices of the digital assistant system such as displays keyboards touch screens and microphones to the user interface module . The I O interface in conjunction with the user interface module can receive user inputs e.g. voice input keyboard inputs touch inputs etc. and processes them accordingly. In some examples e.g. when the digital assistant is implemented on a standalone user device the digital assistant system can include any of the components and I O and communication interfaces described with respect to the user device in . In some examples the digital assistant system can represent the server portion of a digital assistant implementation and can interact with the user through a client side portion residing on a user device e.g. the user device shown in .

In some examples the network communications interface can include wired communication port s and or wireless transmission and reception circuitry . The wired communication port s can receive and send communication signals via one or more wired interfaces e.g. Ethernet Universal Serial Bus USB FIREWIRE etc. The wireless circuitry can receive and send RF signals and or optical signals from to communications networks and other communications devices. The wireless communications can use any of a plurality of communications standards protocols and technologies such as GSM EDGE CDMA TDMA Bluetooth Wi Fi VoIP Wi MAX or any other suitable communication protocol. The network communications interface can enable communication between the digital assistant system with networks such as the Internet an intranet and or a wireless network such as a cellular telephone network a wireless local area network LAN and or a metropolitan area network MAN and other devices.

In some examples memory or the computer readable storage media of memory can store programs modules instructions and data structures including all or a subset of an operating system a communications module a user interface module one or more applications and a digital assistant module . In particular memory or the computer readable storage media of memory can store instructions for performing the process described below. The one or more processors can execute these programs modules and instructions and reads writes from to the data structures.

The operating system e.g. Darwin RTXC LINUX UNIX OS X WINDOWS or an embedded operating system such as VxWorks can include various software components and or drivers for controlling and managing general system tasks e.g. memory management storage device control power management etc. and facilitates communications between various hardware firmware and software components.

The communications module can facilitate communications between the digital assistant system with other devices over the network communications interface . For example the communications module can communicate with the communication module of the device shown in . The communications module can also include various components for handling data received by the wireless circuitry and or wired communications port .

The user interface module can receive commands and or inputs from a user via the I O interface e.g. from a keyboard touch screen pointing device controller and or microphone and generate user interface objects on a display. The user interface module can also prepare and deliver outputs e.g. speech sound animation text icons vibrations haptic feedback and light etc. to the user via the I O interface e.g. through displays audio channels speakers and touch pads etc. .

The applications can include programs and or modules that are configured to be executed by the one or more processors . For example if the digital assistant system is implemented on a standalone user device the applications can include user applications such as games a calendar application a navigation application or an email application. If the digital assistant system is implemented on a server farm the applications can include resource management applications diagnostic applications or scheduling applications for example.

The memory can also store the digital assistant module or the server portion of a digital assistant . In some examples the digital assistant module can include the following sub modules or a subset or superset thereof an input output processing module a speech to text STT processing module a natural language processing module a dialogue flow processing module a task flow processing module a service processing module a discourse input processing module and a text translation module . Each of these modules can have access to one or more of the following data and models of the digital assistant module or a subset or superset thereof language models ontology vocabulary index user data task flow models and service models .

In some examples using the processing modules data and models implemented in the digital assistant module the digital assistant can perform at least some of the following converting speech input into text identifying a user s intent expressed in a natural language input received from the user actively eliciting and obtaining information needed to fully infer the user s intent e.g. by disambiguating words games intentions etc. determining the task flow for fulfilling the inferred intent and executing the task flow to fulfill the inferred intent.

In some examples as shown in the I O processing module can interact with the user through the I O devices in or with a user device e.g. a user device in through the network communications interface in to obtain user input e.g. a speech input and to provide responses e.g. as speech outputs to the user input. The I O processing module can optionally obtain context information associated with the user input from the user device along with or shortly after the receipt of the user input. The context information can include user specific data vocabulary and or preferences relevant to the user input. In some examples the context information also includes software and hardware states of the device e.g. the user device in at the time the user request is received and or information related to the surrounding environment of the user at the time that the user request was received. In some examples the I O processing module can also send follow up questions to and receive answers from the user regarding the user request. When a user request is received by the I O processing module and the user request can include speech input the I O processing module can forward the speech input to the STT processing module or speech recognizer for speech to text conversions.

The STT processing module can include an automatic speech recognition ASR system. The ASR system can process the speech input that is received through the I O processing module to produce a recognition result. The ASR system can include a front end speech pre processor. The front end speech pre processor can extract representative features from the speech input. For example the front end speech pre processor can perform a Fourier transform on the speech input to extract spectral features that characterize the speech input as a sequence of representative multi dimensional vectors. Further the ASR system can implement one or more speech recognition engines and one or more speech recognition models. Examples of speech recognition engines can include the dynamic time warping based engines and weighted finite state transducers WFST based engines. Examples of speech recognition models can include Hidden Markov Models Gaussian Mixture Models Deep Neural Network Models n gram models and other statistical models. In some examples the speech recognition models can include the language models of the STT processing module . The language models can include one or more of a word n gram language model a class m gram language model a word unigram language model and an integrated word and class language model. The one or more speech recognition models and the one or more speech recognition engines can be used to process the extracted representative features of the front end speech pre processor to produce intermediate recognitions results e.g. phonemes sequence of phonemes and sub words and ultimately text recognition results e.g. words sequence of words or sequence of tokens . In some examples the speech input can be processed at least partially by a third party service or on the user s device e.g. user device to produce the recognition result. Once the STT processing module produces recognition results containing text e.g. words or sequence of words or sequence of tokens the recognition result can be passed to the natural language processing module for intent deduction.

In some examples the STT processing module can include and or access a vocabulary of recognizable words via a phonetic alphabet conversion module . Each vocabulary word can be associated with one or more candidate pronunciations of the word represented in a speech recognition phonetic alphabet. For example the vocabulary may include the word tomato in association with the candidate pronunciations of tuh may doe and tuh mah doe. In some examples the candidate pronunciations for words can be determined based on the spelling of the word and one or more linguistic and or phonetic rules. In some examples the candidate pronunciations can be manually generated e.g. based on known canonical pronunciations.

In some examples the candidate pronunciations can be ranked based on the commonness of the candidate pronunciation. For example the candidate pronunciation tuh may doe can be ranked higher than tuh mah doe because the former is a more commonly used pronunciation e.g. among all users for users in a particular geographical region or for any other appropriate subset of users . In some examples one of the candidate pronunciations can be selected as a predicted pronunciation e.g. the most likely pronunciation .

When a speech input is received the STT processing module can be used to determine the phonemes corresponding to the speech input e.g. using an acoustic model and then attempt to determine words that match the phonemes e.g. using a language model . For example if the STT processing module can first identify the sequence of phonemes tuh may doe corresponding to a portion of the speech input it can then determine based on the vocabulary index that this sequence corresponds to the word tomato. 

In some examples the STT processing module can use approximate matching techniques to determine words in an utterance. Thus for example the STT processing module can determine that the sequence of phonemes duh may doe corresponds to the word tomato even if that particular sequence of phonemes is not one of the candidate phonemes for that word.

In some examples the STT processing module can be capable of determining a combined result based on two or more recognition results. For example STT processing module can be capable of performing blocks and of process described below.

The natural language processing module natural language processor of the digital assistant can take the sequence of words or tokens token sequence generated by the STT processing module and attempt to associate the token sequence with one or more actionable intents recognized by the digital assistant. An actionable intent can represent a task that can be performed by the digital assistant and can have an associated task flow implemented in the task flow models . The associated task flow can be a series of programmed actions and steps that the digital assistant takes in order to perform the task. The scope of a digital assistant s capabilities can be dependent on the number and variety of task flows that have been implemented and stored in the task flow models or in other words on the number and variety of actionable intents that the digital assistant recognizes. The effectiveness of the digital assistant however can also be dependent on the assistant s ability to infer the correct actionable intent s from the user request expressed in natural language.

In some examples in addition to the sequence of words or tokens obtained from the STT processing module the natural language processing module can also receive context information associated with the user request e.g. from the I O processing module . The natural language processing module can optionally use the context information to clarify supplement and or further define the information contained in the token sequence received from the STT processing module . The context information can include for example user preferences hardware and or software states of the user device sensor information collected before during or shortly after the user request prior interactions e.g. dialogue between the digital assistant and the user and the like. As described herein context information can be dynamic and can change with time location content of the dialogue and other factors.

In some examples the natural language processing can be based on for example ontology . The ontology can be a hierarchical structure containing many nodes each node representing either an actionable intent or a property relevant to one or more of the actionable intents or other properties. As noted above an actionable intent can represent a task that the digital assistant is capable of performing i.e. it is actionable or can be acted on. A property can represent a parameter associated with an actionable intent or a sub aspect of another property. A linkage between an actionable intent node and a property node in the ontology can define how a parameter represented by the property node pertains to the task represented by the actionable intent node.

The natural language processing module can receive the token sequence e.g. a text string from the STT processing module and determine what nodes are implicated by the words in the token sequence. In some examples if a word or phrase in the token sequence is found to be associated with one or more nodes in the ontology via the vocabulary index the word or phrase can trigger or activate those nodes. Based on the quantity and or relative importance of the activated nodes the natural language processing module can select one of the actionable intents as the task that the user intended the digital assistant to perform. In some examples the domain that has the most triggered nodes can be selected. In some examples the domain having the highest confidence value e.g. based on the relative importance of its various triggered nodes can be selected. In some examples the domain can be selected based on a combination of the number and the importance of the triggered nodes. In some examples additional factors are considered in selecting the node as well such as whether the digital assistant has previously correctly interpreted a similar request from a user.

User data can include user specific information such as user specific vocabulary user preferences user address user s default and secondary languages user s contact list and other short term or long term information for each user. In some examples the natural language processing module can use the user specific information to supplement the information contained in the user input to further define the user intent. For example for a user request invite my friends to my birthday party the natural language processing module can be able to access user data to determine who the friends are and when and where the birthday party would be held rather than requiring the user to provide such information explicitly in his her request.

In some examples once the natural language processing module identifies an actionable intent or domain based on the user request the natural language processing module can generate a structured query to represent the identified actionable intent. In some examples the structured query can include parameters for one or more nodes within the domain for the actionable intent and at least some of the parameters are populated with the specific information and requirements specified in the user request. For example the user may say Make me a dinner reservation at a sushi place at 7. In this case the natural language processing module can be able to correctly identify the actionable intent to be restaurant reservation based on the user input. According to the ontology a structured query for a restaurant reservation domain may include parameters such as Cuisine Time Date Party Size and the like. In some examples based on the speech input and the text derived from the speech input using the STT processing module the natural language processing module can generate a partial structured query for the restaurant reservation domain where the partial structured query includes the parameters Cuisine Sushi and Time 7 pm . However in this example the user s utterance contains insufficient information to complete the structured query associated with the domain. Therefore other necessary parameters such as Party Size and Date may not be specified in the structured query based on the information currently available. In some examples the natural language processing module can populate some parameters of the structured query with received context information. For example in some examples if the user requested a sushi restaurant near me the natural language processing module can populate a location parameter in the structured query with GPS coordinates from the user device .

In some examples the natural language processing module can pass the structured query including any completed parameters to the task flow processing module task flow processor . The task flow processing module can be configured to receive the structured query from the natural language processing module complete the structured query if necessary and perform the actions required to complete the user s ultimate request. In some examples the various procedures necessary to complete these tasks can be provided in task flow models . In some examples the task flow models can include procedures for obtaining additional information from the user and task flows for performing actions associated with the actionable intent.

As described above in order to complete a structured query the task flow processing module may need to initiate additional dialogue with the user in order to obtain additional information and or disambiguate potentially ambiguous utterances. When such interactions are necessary the task flow processing module can invoke the dialogue flow processing module to engage in a dialogue with the user. In some examples the dialogue flow processing module can determine how and or when to ask the user for the additional information and receives and processes the user responses. The questions can be provided to and answers can be received from the users through the I O processing module . In some examples the dialogue flow processing module can present dialogue output to the user via audio and or visual output and receives input from the user via spoken or physical e.g. clicking responses. Continuing with the example above when the task flow processing module invokes the dialogue flow processing module to determine the party size and date information for the structured query associated with the domain restaurant reservation the dialogue flow processing module can generate questions such as For how many people and On which day to pass to the user. Once answers are received from the user the dialogue flow processing module can then populate the structured query with the missing information or pass the information to the task flow processing module to complete the missing information from the structured query.

Once the task flow processing module has completed the structured query for an actionable intent the task flow processing module can proceed to perform the ultimate task associated with the actionable intent. Accordingly the task flow processing module can execute the steps and instructions in the task flow model according to the specific parameters contained in the structured query. For example the task flow model for the actionable intent of restaurant reservation can include steps and instructions for contacting a restaurant and actually requesting a reservation for a particular party size at a particular time. For example using a structured query such as restaurant reservation restaurant ABC Caf date Mar. 12 2012 time 7 pm party size 5 the task flow processing module can perform the steps of 1 logging onto a server of the ABC Caf or a restaurant reservation system such as OPENTABLE 2 entering the date time and party size information in a form on the website 3 submitting the form and 4 making a calendar entry for the reservation in the user s calendar.

In some examples the task flow processing module can employ the assistance of a service processing module to complete a task requested in the user input or to provide an informational answer requested in the user input. For example the service processing module can act on behalf of the task flow processing module to make a phone call set a calendar entry invoke a map search invoke or interact with other user applications installed on the user device and invoke or interact with third party services e.g. a restaurant reservation portal a social networking website a banking portal etc. . In some examples the protocols and application programming interfaces API required by each service can be specified by a respective service model among the service models . The service processing module can access the appropriate service model for a service and generate requests for the service in accordance with the protocols and APIs required by the service according to the service model.

For example if a restaurant has enabled an online reservation service the restaurant can submit a service model specifying the necessary parameters for making a reservation and the APIs for communicating the values of the necessary parameter to the online reservation service. When requested by the task flow processing module the service processing module can establish a network connection with the online reservation service using the web address stored in the service model and send the necessary parameters of the reservation e.g. time date party size to the online reservation interface in a format according to the API of the online reservation service.

In some examples the natural language processing module dialogue flow processing module and task flow processing module can be used collectively and iteratively to infer and define the user s intent obtain information to further clarify and refine the user intent and finally generate a response i.e. an output to the user or the completion of a task to fulfill the user s intent.

The discourse input processing module can be capable of converting various forms of discourse input e.g. handwriting input gesture input and the like into corresponding text. For example the discourse input processing module can include handwriting or gesture recognition models and algorithms for converting handwriting or gesture input into corresponding text strings. In some examples the discourse input processing module can utilize the language models to convert various forms of discourse input into corresponding text.

The text translation module can be capable of converting text input of a first language into corresponding text of a second language. For example the text translation module can include a machine translation engine and various machine translation models for determining the text string of the second language that corresponds to the text input of the first language. In some examples the text translation module can use the language models to convert the text input of the first language into corresponding text of the second language.

It should be recognized that in some examples one or more of the modules e.g. the STT processing module discourse input processing module the text translation module etc. in the digital assistant module can be stored separate from the digital assistant module in the memory . Further in some examples the digital assistant system can further include a text prediction module not shown stored in memory . The text prediction module can be capable of determining a predicted text based on one or more inputted words. In some examples the text prediction module can utilize one or more language models e.g. the language models to determining the predicted text.

Additional details on digital assistants can be found in the U.S. Utility application Ser. No. 12 987 982 entitled Intelligent Automated Assistant filed Jan. 10 2011 and U.S. Utility application Ser. No. 13 251 088 entitled Generating and Processing Task Items That Represent Tasks to Perform filed Sep. 30 2011 the entire disclosures of which are incorporated herein by reference.

As described above word n gram models can yield unreliable results for infrequently occurring n word strings due to the finite sizes of text databases used to train the word n gram models. To address this limitation it can be expedient to rely on class m gram models. Class m gram models are based on the assignment of words to a small number of classes e.g. parts of speech classes as well as the probability of a class given the m 1 previous classes observed. Because the inventory of classes in a class m gram model can typically be much smaller than the underlying vocabulary of words a class m gram model can be significantly more compact than a word n gram model even with m n. As a result significantly less training data is necessary to achieve reliable estimation.

While class m gram models can be advantageous in being more compact and easier to estimate than word n grams class m gram models can also be less accurate due to the inherent quantization involved. On the other hand class m gram models can avoid the more or less arbitrary smoothing techniques that are typically implemented to alleviate the data sparsity issues inherent to word n gram models with n that is 3 or more. In particular class m gram models can tend to be more robust in the sense that they can generalize better in situations of data sparsity.

The characteristics of word n gram models and class m gram models can be recognized as complementary. In particular the word n gram model can provide a more accurate estimation for events that are well represented in the training corpus while the class m gram can provide a more robust estimation for rarer and unseen events. It can thus be desirable to integrate word n gram and class m gram models together to leverage the advantages of both models. Traditionally word n gram and class m gram models can be integrated via simple or deleted interpolation or maximum entropy formulation. Both techniques can utilize weight parameters to determine how much to rely on one model versus the other. The weight parameters can typically be selected empirically or estimated based on held out data. Utilizing weight parameters can result in relatively coarse model combination where the relative influence of the two models is specified in a global fashion based on broad ranges of language model probabilities. To achieve higher accuracy and robustness it can be desirable to balance the two models at a finer level of granularity. For example the word n gram and class m gram models can be integrated based not only on general frequency of occurrence but also on the specific identity of the local context. This enables weighting to be performed on a local rather than global level. Such an approach is described in greater detail below.

For reference let W w. . . wwdenote a string of n words. Generally wcan refer to the current word and the preceding n 1 words Wcan refer to the word history. For text prediction wcan refer to the predicted word and the preceding n 1 words Wcan refer to the context or the word history used to determine the predicted word w. For applications such as speech recognition handwriting recognition machine translation and the like wcan refer to the candidate word and the preceding n 1 words Wcan refer to the word history.

Further let C c. . . ccdenote a string of m classes corresponding to the string of n words W. Each class cin the string of m classes Ccan take values from a small set of subclasses 1 k K for q m 1 i q . A subclass can include any predetermined category used to classifying the words of a given language such as parts of speech classes or semantic classes. For example with respect to parts of speech classes each class ccan include one more of the following subclasses noun verb adjective adverb conjunction pronoun article preposition interjection and the like. Further in some examples the parts of speech classes can be divided according to semantic classes. For example nouns can be broken into semantic classes such as location nouns e.g. cities landmarks restaurants etc. person nouns e.g. names of individuals nouns associated with food and the like. Because each class cin the string of m classes Ccan include one or more subclasses the string of m classes Ccan be a set of strings of the form . . . where each index kranges over its respective interval for q m 1 i q .

In an illustrative example a string of n 3 words Wcan be I like you. The parts of speech subclass corresponding to the words I and you can each be pronoun while the parts of speech subclasses corresponding to the word like can include verb adjective preposition noun adverb and conjunction. Therefore in this example the string of classes Ccorresponding to the string of words W I like you can include the following set of class strings pronoun verb pronoun pronoun adjective pronoun pronoun preposition pronoun pronoun noun pronoun pronoun adverb pronoun and pronoun conjunction pronoun.

The probability of a current word wgiven the word history W e.g. the relevant string of n 1 words can be expressed as 1 Pr w Win expression 1 can be determined using a word n gram language model.

Further the probability of the current word wgiven the current class values cas well as the relevant class history Ccan be expressed as . 2 In expression 2 the quantity Pr w can denote the assignment of the current word wto a specific subclass e.g. a specific part of speech and Pr C can denote the probability of this specific subclass given the class m gram history Ccorresponding to the word history W. Pr w C in expression 2 can be determined using a class m gram language model. It should be recognized that at the word level the class m gram model can have the same order as a word unigram model where context is only taken into account at the class level.

As described above each class c in the string of m classes Ccan include one or more subclasses. Thus the class history Cdoes not typically represent a unique string of m 1 classes but rather a set of strings . . . each of length m 1. The probability Pr Cin expression 2 can be expressed in terms of individual subclasses as 

As described above it can be desirable to integrate the word n gram model and the class m gram model at a finer level of granularity. Since the word n gram model can be expressed as 1 and the class m gram model can be expressed as 2 it would be desirable to combine expressions 1 and 2 to obtain an integrated language model without utilizing arbitrarily interpolation or maximum entropy formulation. In order to achieve model integration at a finer level of granularity expressions 1 and 2 can be integrated by means of joint probability. In particular the joint probability of the current word wand all its possible subclasses c given all relevant history can be computed as follows 4 

Although expression 4 enables the word n gram model and the class m gram model to be integrated at a finer level of granularity the right hand side of expression 4 can be challenging to evaluate due to its complexity. It can therefore be desirable to simplify the terms in the summation of Pr w WC while implementing as few assumptions as possible with respect to the structure of the word and class history.

By expanding the numerator and denominator of the right side of expression 5 Pr w WCcan be further expressed as 

To simplify the expression in square brackets in expression 7 the numerator of the expression in square brackets can be expanded as follows 8 

In the first term Pr CwW on the right side of expression 8 the current subclass can be conditioned on both the class history Cthe word history W. To simplify expression 8 an assumption can be made that the class history Cis sufficient to evaluate Pr CwW and that knowledge of the precise identity of the words Wis not necessary. In other words it can be assumed that 9 It should be appreciated that the assumption in expression 9 can be a relatively mild assumption since the dependence of the current subclass on the word history Wcan typically be weak. Thus applying this assumption to the integrated model represented by expression 4 may not significantly impact the accuracy and robustness of the model.

In the second term Pr C wW on the right side of expression 8 the class history Ccan be conditioned on the word history Was well as the current word w. Here it can be assumed that the class history Wis sufficient to evaluate Pr C wWand that knowledge of the current word wis not necessary. In other words it can be assumed that . 10 Similar to the assumption of expression 9 the assumption in expression 10 can also be a relatively mild assumption since the dependence of the class history Con the current word wcan typically be weak. Thus applying this assumption to the integrated model represented by expression 4 may also not significantly impact the accuracy and robustness of the model.

Applying the two assumptions of expressions 9 and 10 to expression 7 the term Pr C W can be cancelled from both the numerator and denominator of expression 7 to thereby obtain the simplified expression of 11 

The right side of expression 11 can be further simplified by assuming that the probability of the current word wgiven the current subclass also known as the class assignment probability does not depend on the class history C. In other words it can be assumed that . 13 The assumption of expression 13 can also be a relatively mild assumption since the dependence of the current word won the class history Ccan typically be weak.

It should be recognized that the numerator of the expression in brackets on the right side of expression 15 is identical to the right side of expression 2 . Expression 2 can thus be applied to expression 15 to arrive at the following 

Expression 16 can be a simplification of expression 4 after implementing the assumptions of expressions 9 10 and 13 . Therefore expression 16 can be a simplified expression of the integrated word n gram and class m gram model represented by expression 4 . The integrated word n gram and class m gram model of expression 16 can incorporate information from the class m gram model that is relative to a word unigram with no class information. In particular as shown in expression 16 the word n gram probability Pr w W can be modified by the weight expression

In some examples the integrated language model represented by expression 16 can be utilized to disambiguate discourse inputs. For example discourse input can be received by an electronic device in the form of speech or handwriting input. Due to the inherent background noise and inaccuracies associated with such discourse inputting means a robust language model can be utilized to disambiguate the discourse inputs. Specifically a robust language model can be utilized to determine the most likely sequence of words represented by the speech or handwriting input. In other examples the integrated model represented by expression 16 can be utilized as a robust language model for predicting text. In particular based on a string of words inputted by a user a predicted word can be generated using the robust language model. In some examples the integrated language model represented by expression 16 can be implemented in the digital assistant system described above. Specifically the integrated language model of expression 16 can be a language model implemented by the STT processing module of the digital assistant. Exemplary processes e.g. processes and for using the integrated model represented by expression 16 for disambiguating discourse inputs and text prediction are described in greater detail below.

Referring back to expression 3 described above it can be shown by induction on m which is the order of a standard m gram model trained at the class level that the probability Pr C can be expressed according to expression 3 . For example considering m 2 the probability Pr C can be reduced to Pr c where ccan take values from a small set of subclasses 1 k K. Accordingly Pr c can be expressed as follows 

Next assuming that expression 3 holds at order m we consider whether expression 3 holds at order m 1. At order m 1 19 where ccan take values from a small set of subclasses 1 k KAccordingly Pr C can be expressed as follows 

It should be recognized that when the terms involving Cand Care expanded expression 19 can corresponds to expression 3 at order m 1. Accordingly expression 3 is proven by induction.

Referring to processes and can be suitable for processing discourse inputs such as speech input handwriting input text translation input and the like to determine a text string corresponding to the discourse input. The text string can be determined using one or more language models. For example the integrated language model represented by expression 16 can be used to accurately determine the text string corresponding to the discourse input.

At block of process a discourse input can be received from a user. The discourse input can include a string of words. Each word wof the string of words can be associated with a class c. Further each class ccan include one or more subclasses . A subclass can be any predetermined category for classifying the word w. For example the string of words of the discourse input can include the word book which can be associated with a class. The class with which the word book is associated can include the subclasses noun adjective and verb.

In some examples the discourse input can be a speech input that is received in the course of or as part of an interaction with a digital assistant e.g. the digital assistant implemented on the digital assistant system . The speech input can be received in the form of sound waves an audio file or a representative audio signal analog or digital . The speech input can include a string of words in spoken form. In some examples the speech input can be sound waves that are received by the microphone e.g. microphone of the electronic device. In other examples the speech input can be a representative audio signal or a recorded audio file that is received by the audio subsystem e.g. audio subsystem the peripheral interface e.g. peripheral interface or the processor e.g. processor of the electronic device. In yet other examples the speech input can be a representative audio signal or a recorded audio file that is received by the I O interface e.g. I O interface or the processor e.g. processor of the digital assistant system.

It should be appreciated that the discourse input can be various other forms of input that represent a string of words of a language. For instance in some examples the discourse input can be handwriting input that includes a string of words in handwritten form. The handwriting input can be received from a user via a handwriting recognition interface e.g. a handwriting recognition interface displayed on the touchscreen of the electronic device. In other examples the discourse input can be in the form of physical gestures or expressions received via an optical sensor e.g. optical sensor of the electronic device. In one such example the discourse input can include a string of words expressed in the form of sign language and received via the optical sensor of the electronic device. In yet other examples the discourse input can be a text input of a first language that includes a string of words. In these examples the text input can be received via an interface e.g. a virtual keyboard displayed on touchscreen or other input control devices . The interface can be any suitable interface of the electronic device for inputting text such as for example the interface can be a keyboard keypad a touch screen implementing a virtual keyboard a remote control e.g. television remote control a scroll wheel interface an audio input interface implementing speech to text conversion and the like.

At block of process a text string corresponding to the discourse input can be determined. In examples where the discourse input is a speech input the speech input can be processed using an ASR system e.g. the ASR system of STT processing module of the digital assistant to determine the text string corresponding to the speech input. As described above representative features can be extracted from the speech input using a front end speech pre processor of the ASR system. Further using speech recognition models and speech recognition engines of the ASR system intermediate recognition results e.g. phonemes sequence of phonemes and sub words can be determined from the extracted representative features and the text string corresponding to the speech input can then be determined from the intermediate recognition results.

For discourse inputs of other forms such as handwriting input gesture input and the like the text string corresponding to the discourse input can be determined using the discourse input processing module e.g. discourse input processing module of the electronic device. Further in examples where the discourse input is a text input of a first language the text string determined at block can be a text string of a second language corresponding to the text input of the first language. The text string of the second language can be determined from the text input of the first language using a text translation module e.g. text translation module of the electronic device.

The text string at block can be determined using one or more language models e.g. language models . The one or more language models can serve to disambiguate uncertainty that is inherent in for example speech recognition handwriting recognition gesture recognition and machine translation. The text string having the highest likelihood of corresponding to the discourse input can thus be determined using the one or more language models. As shown in block can include blocks through which describe the use of an integrated word n gram and class m gram language model to determine the text string corresponding to the discourse input. In particular in some examples the use of the integrated language model represented by expression 16 can enable greater accuracy and robustness in determining the text string at block .

At block of process a conditional probability of a candidate word in the discourse input given one or more words in the discourse input can be determined using a first language model. The first language model can be stored e.g. in the language models on the electronic device. The conditional probability of the candidate word in the discourse input given the one or more words in the discourse input can be expressed as Pr w W . In process and similarly process wcan refer to the candidate word and Wcan refer to the one or more words in the discourse input. In some examples the one or more words can be the word history preceding the candidate word in the discourse input. In some examples the first language model can be a word n gram language model. Thus the conditional probability Pr w W of the candidate word in the discourse input given the one or more words in the discourse input can be determined using a word n gram language model.

At block of process a probability of the candidate word within a corpus can be determined using a second language model. The second language model can be trained using the corpus. Thus the probability of the candidate word within the corpus can represent the frequency of occurrence of the candidate word within the corpus used to train the second language model. The probability of the candidate word within the corpus can be expressed as Pr w . In some examples the second language model can be a word n gram language model. In some examples the second language model can be the same language model as the first language model used to determine the conditional probability at block . In other examples the second language model can be a word unigram language model. The second language model can be stored e.g. in language models of the electronic device.

At block of process a conditional probability of the candidate word given one or more classes associated with the one or more words can be determined using a third language model. The conditional probability of the candidate word given one or more classes associated with the one or more words can be expressed as Pr w C where wcan be the candidate word and Ccan be the one or more classes associated with the one or more words Wthe discourse input. In some examples the one or more classes Ccan be the class history corresponding to the word history Wpreceding the candidate word w. In some examples third language model can be a class m gram language model. The third language model can be stored e.g. in language models of the electronic device. The conditional probability Pr w W of the candidate word in the discourse input given the one or more words in the discourse input can be determined using a class m gram language model.

As described above a class c of the one or more classes Ccan include one or more subclasses . Because each class c of the one or more classes Ccan include one or more subclasses the one or more classes Ccan comprise a set of strings of subclasses . . . each of length m 1 where m 1 can be the number of words in the one or more words.

At block of process an integrated probability of the candidate word and one or more subclasses associated with the candidate word can be determined based on the conditional probability of the candidate word given the one or more words the probability of the candidate word within the corpus and the conditional probability of the candidate word given the one or more classes. In some examples the integrated probability can be determined using expression 16 . In these examples the integrated probability can be the joint probability of the candidate word wand the one or more subclasses c associated with the candidate word wgiven the one or more words Wand the one or more classes C. The integrated probability can thus be determined by solving the right side of expression 16 using the conditional probability Pr w W of the candidate word given the one or more words determined at block the probability Pr w of the candidate word within the corpus determined at block and the conditional probability Pr w C of the candidate word given the one or more classes determined at block .

As described above expression 16 was derived by applying several assumptions to simplify the expression. In examples where the integrated probability of block is determined using expression 16 the assumption used to derive expression 16 can be inherent to the integrated probability. In particular with reference to expression 9 it can be assumed that knowledge of the precise identity of the one or more words Wis not necessary to determine Pr wW . Thus the integrated probability at block can be based on the assumption that the conditional probability Pr CW of a subclass associated with the candidate word given the candidate word the one or more words and the one or more classes is equal to the conditional probability Pr Cw of the subclass of the class associated with the candidate class given the candidate word and the one or more classes.

In addition with reference to expression 10 it can be assumed that knowledge of the candidate word wis not necessary to determine Pr C wW . Thus the integrated probability at block can be based on assuming that the conditional probability Pr C wW of the one or more classes given the candidate word and the one or more words is equal to a conditional probability Pr C W of the one or more classes given the one or more words.

Further with reference to expression 13 it can be assumed that the probability Pr w C of the current word wgiven the current class does not depend on the class history C. Thus the integrated probability at block can be based on assuming that the conditional probability Pr w C of the candidate word given the subclass associated with the candidate class and the one or more classes is equal to the conditional probability Pr w of the candidate word given the subclass associated with the candidate word.

Further based on the expression on the right side of expression 16 it can be appreciated that the contribution of the conditional probability Pr wC of the candidate word given the one or more words to the integrated probability Pr wc WC can be based on the probability Pr w of the candidate word within the corpus. In particular as the probability Pr w of the candidate word within the corpus increases the conditional probability Pr w C of the candidate word given the one or more words in the expression

Referring back to block the integrated probability determined at blocks through can be used to determine the text string corresponding to the discourse input. For example the integrated probability determined at block can be used to determine the candidate word with the highest likelihood. In this way the text string having the highest likelihood of corresponding to the discourse input can be determined based on the integrated probability determined at block .

At block of process an output based on the text string can be generated. In some example the text string corresponding to the discourse input can contain a user request. The natural language processor e.g. natural language processing module the dialogue flow processing module e.g. dialogue flow processing module the task flow processing module e.g. task flow processing module and the service processing module e.g. service processing module of the digital assistant can be used to infer the user s intent based on the text string and generate an appropriate output that is responsive to the user request. In a specific example the text string corresponding to the discourse input can include the request Find me a sushi restaurant. Based on this text string the actionable intent of find restaurant can be determined using the natural language processor. The task flow processing module and the service processing module can then perform the tasks associated with the actionable intent of find restaurant. In particular the task flow processing module and the service processing module can invoke a search of sushi restaurants within the locality of the user and displaying the results of the search on the electronic device. Thus in this example generating the output based on the text string can include displaying e.g. on the touch screen the results of the restaurant search on the electronic device.

It should be recognized that in other examples the generated output can include any output generated by the electronic device that is responsive to the discourse input. For instance in some examples the generated output can comprise the display of the text string on the electronic device. Specifically the text string can be displayed on the electronic device for applications involving speech recognition handwriting recognition gesture recognition and the like. Further in machine translation applications the text string in the translated language can be displayed on the electronic device. In other examples the generated output can include speech music or video output that is responsive to the discourse input. In yet other examples the generated output can include data e.g. email text files etc. sent from the electronic device that is responsive to the discourse input.

Referring now to process can be similar to process described above. At block of process a discourse input can be received from a user. The discourse input can include a string of words. Block can be similar or identical to block of process described above.

At block of process a text string corresponding to the discourse input can be determined. Block can be similar to block of process except that block substitutes blocks through . The text string can be determined using one or more language models. In particular as described in blocks and a weighted conditional probability of a candidate word in the discourse input given one or more words in the discourse input can be determined using one or more language models. In some examples the weighted conditional probability can be determined using expression 16 described above. The text string corresponding to the discourse input can then be determined based on the weighted conditional probability.

At block of process a conditional probability of the candidate word in the discourse input given the one or more words in the discourse input can be determined using a first language model. Block can be similar or identical to block of process described above. The conditional probability of the candidate word in the discourse input given one or more words in the discourse input can be expressed as Pr w W where wcan be the candidate word and Wcan be the one or more words in the discourse input. In some examples the one or more words can be the word history preceding the candidate word in the discourse input. In some examples the first language model can be a word n gram language model.

At block of process a weight can be applied to the conditional probability of the candidate word given the one or more words to obtain a weighted conditional probability of the candidate word given the one or more words. In some examples the weight can be based on a conditional probability of the candidate word given one or more classes associated with the one or more words. Further in some examples the weight can be based on a probability of the candidate word within a corpus.

In some examples the weighted conditional probability can be determined using expression 16 described above. In these examples the applied weight can be the expression

In examples where expression 16 is implemented to obtain the weighted conditional probability of block the weighted conditional probability of the candidate word can be proportional to the conditional probability Pr w C of the candidate word given the one or more words. Further the weight

In examples where the weighted conditional probability is determined using expression 16 the weighted conditional probability can be based on the same assumptions used to derive expression 16 . In particular the weighted conditional probability can be based on similar or identical assumptions as described above with respect to the integrated probability at block .

At block of process an output can be generated based on the text string. Block can be similar or identical to block of process described above.

Referring to processes and can be suitable for processing discourse input such as text input to determine a predicted word. The predicted word can be determined using one or more language models. For example the integrated language model represented by expression 16 can be used to accurately determine a predicted word based on one or more words in the text input.

At block of process a text input can be received from a user. In some examples the text input can be received via an interface of the electronic device e.g. touch screen or other input control devices of user device . The interface can be any suitable device for inputting text. For example the interface can be a keyboard keypad a touch screen implementing a virtual keyboard a remote control e.g. television remote control a scroll wheel interface an audio input interface implementing speech to text conversion a handwriting recognition interface implementing handwriting to text conversion or the like. The received text input can be in any language and can include one or more words. For example the text input can be a sequence of words. In some cases a character e.g. symbols and punctuation can be considered a word.

At block of process a conditional probability of a predicted word given one or more words in the text input can be determined using a first language model. The first language model can be stored e.g. in the language models on the electronic device. In some example the first language model can be a word n gram language model. The conditional probability of the predicted word given the one or more words in the text input can be expressed as Pr w W . In process and similarly process wcan refer to the predicted word and Wcan refer to the one or more words in the text input. In some examples the predicted word wcan be a word that is predicted to succeed the one or more words of the text input. In a specific example the one or more words of the text input can be once upon a and the predicted word can be time. Block and similarly blocks and described below can be performed by a text prediction module of the electronic device.

At block of process a probability of the predicted word wwithin a corpus can be determined using a second language model. The second language model can be trained using the corpus. The second language model can be stored e.g. in the language models on the electronic device. In some examples the second language model can be a word n gram model. In one such example the second language model can be the same language model as the first language model used to determine the conditional probability of the predicted word at block . In other examples the second language model can be a word unigram language model. The probability of the predicted word within the corpus can be expressed as Pr w .

At block of process a conditional probability of the predicted word given one or more classes associated with the one or more words can be determined using a third language model. The third language model can be stored e.g. in the language models on the electronic device. In some examples the third language model can be a class m gram language model. The conditional probability of the predicted word given the one or more classes associated with the one or more words can be expressed as Pr w C where Ccan refer to the one or more classes associated with the one or more words Win the text input. In some examples a class cof the one or more classes Ccan include one or more subclasses . In these examples the one or more classes Ccan comprise a set of strings of subclasses 2 . . . each of length m 1 where m 1 can be the number of words in the one or more words in the text input.

At block of process an integrated probability of the predicted word and one or more subclasses associated with the predicted word can be determined based on the conditional probability of the predicted word given the one or more words the probability of the predicted word within the corpus and the conditional probability of the predicted word given the one or more classes. In some examples the integrated probability can be determined using expression 16 . In these examples the integrated probability can be the joint probability of the predicted word wand one or more subclasses c associated with the predicted word wgiven the one or more words Wand the one or more classes C. The integrated probability can thus be determined by solving expression 16 using the conditional probability Pr w W of the predicted word given the one or more words determined at block the probability Pr w of the predicted word within the corpus determined at block and the conditional probability Pr w C of the predicted word given the one or more classes determined at block . In these examples the integrated probability can inherit the properties of expression 16 as described above.

In some examples the integrated probability determined at block can be greater than the conditional probability Pr w W of the predicted word wgiven the one or more words Wwhen the conditional probability Pr w C of the predicted word wgiven the one or more classes Cis greater than the probability Pr w of the predicted word wwithin the corpus. Conversely the integrated probability determined at block can be less than the conditional probability Pr w W of the predicted word wgiven the one or more words Wwhen the conditional probability Pr w C of the predicted word wgiven the one or more classes Cis less than the probability Pr w of the predicted word wwithin the corpus. In some examples the integrated probability determined at block can be less dependent on the conditional probability Pr wC of the predicted word given the one or more classes when the probability Pr w of the predicted word within the corpus is higher than when the probability Pr w of the predicted word within the corpus is lower.

Further in examples where expression 16 is used to determine the integrated probability at block the integrated probability can be based on the same assumptions e.g. expressions 9 10 and 13 that were used to derive expression 16 . In particular the integrated probability determined at block can be based on similar or identical assumptions as described above with respect to the integrated probability at block .

At block of process the predicted word can be displayed based on the integrated probability determined at block . For example the predicted word can be displayed on the display e.g. touch screen of the electronic device. In some examples it can be determined whether a predetermined criterion is met based on the integrated probability. In these examples the predicted word can be displayed in response to determining that the predetermined criterion is met. Further in some examples the predicted word can be one of several predicted words displayed. In these examples the position at which the predicted word is displayed on the electronic device can be based on the integrated probability. For example the predicted word can be ranked with respect to other predicted words based on the integrated probability. The position at which the predicted word is displayed relative to other predicted words can be based on the rank of the predicted word with respect to other predicted words.

Referring now to process can be similar to process described above. At block of process a text input from a user can be received. Block can be similar or identical to block of process described above.

At block of process a conditional probability of a predicted word given one or more words in the text input can be determined using a first language model. Block can be similar or identical to block of process described above. Block and similarly block described below can be performed using a text prediction module of the electronic device. The conditional probability of the predicted word given one or more words in the text input can be expressed as Pr w W where wcan be the predicted word and Wcan be the one or more words in the text input. In some examples the first language model can be a word n gram language model.

At block of process a weight can be applied to the conditional probability of the predicted word given the one or more words to obtain a weighted conditional probability of the predicted word given the one or more words. In some examples the weight can be based on a conditional probability of the predicted word given one or more classes associated with the one or more words. Further in some examples the weight can be based on a probability of the predicted word within a corpus.

In some examples the weighted conditional probability can be determined using expression 16 described above. In these examples the applied weight can be the expression

In examples where the weighted conditional probability is determined using expression 16 the weighted conditional probability of the predicted word can be proportional to the conditional probability Pr wC of the predicted word given the one or more words. Further the weight

Further in examples where expression 16 is implemented to obtain the weighted conditional probability of block the weighted conditional probability can be based on the same assumptions used to derive expression 16 . In particular the weighted conditional probability can be based on similar or identical assumptions as described above with respect to the integrated probability at block .

At block the predicted word can be displayed based on the integrated probability. Block can be similar or identical to block described above.

As shown in the electronic device can include a touch screen display unit configured to display a user interface e.g. a virtual keyboard or a handwriting recognition interface an audio input unit configured to receive audio input e.g. speech input an optical sensor unit configured to receive images or gesture input and a speaker unit configured to output audio e.g. speech . The electronic device can further include a processing unit coupled to the touch screen display unit the audio input unit the optical sensor unit and the speaker unit. In some examples the processing unit can include a receiving unit a determining unit an applying unit a generating unit and a displaying unit .

Processing unit is configured to receive e.g. from the touch screen display unit the audio input unit or the optical sensor unit and using the receiving unit a discourse input from a user. Processing unit is configured to determine e.g. using the determining unit using a first language model a conditional probability of a candidate word in the discourse input given one or more words in the discourse input. Processing unit is configured to determine e.g. using the determining unit using a second language model a probability of the candidate word within a corpus. Processing unit is configured to determine e.g. using the determining unit using a third language model a conditional probability of the candidate word given one or more classes associated with the one or more words. Processing unit is configured to determine e.g. using the determining unit an integrated probability of the candidate word and one or more subclasses associated with the candidate word based on the conditional probability of the candidate word given the one or more words the probability of the candidate word within the corpus and the conditional probability of the candidate word given the one or more classes. Processing unit is configured to determine e.g. using the determining unit a text string corresponding to the discourse input based on the integrated probability. Processing unit is configured to generate e.g. using the touch screen display unit or the speaker unit and using the generating unit an output based on the text string.

In some examples the integrated probability is a joint probability of the candidate word and the one or more subclasses associated with the candidate word given the one or more words and the one or more classes.

In some examples the integrated probability is determined based on assuming that a conditional probability of a subclass associated with the candidate word given the candidate word the one or more words and the one or more classes is equal to a conditional probability of the subclass associated with the candidate word given the candidate word and the one or more classes.

In some examples the integrated probability is determined based on assuming that a conditional probability of the candidate word given a subclass associated with the candidate word and the one or more classes is equal to the conditional probability of the candidate word given the subclass associated with the candidate word.

In some examples the integrated probability is determined based on assuming that a conditional probability of the one or more classes given the candidate word and the one or more words is equal to a conditional probability of the one or more classes given the one or more words.

In some examples the integrated probability is greater than the conditional probability of the candidate word given the one or more words when the conditional probability of the candidate word given the one or more classes is greater than the probability of the candidate word within the corpus.

In some examples the integrated probability is less than the conditional probability of the candidate word given the one or more words when the conditional probability of the candidate word given the one or more classes is less than the probability of the candidate word within the corpus.

In some examples the integrated probability is determined at least in part by dividing the conditional probability of the candidate word given the one or more classes by the probability of the candidate word within the corpus.

In some examples the integrated probability is less dependent on the conditional probability of the candidate word given the one or more classes when the probability of the candidate word within the corpus is higher than when the probability of the candidate word within the corpus is lower.

In some examples the first language model is a word n gram language model. In some examples the first language model and the second language model are a same language model. In some examples the third language model is a class m gram language model.

In some examples processing unit is configured to receive e.g. from the touch screen display unit the audio input unit or the optical sensor unit and using the receiving unit a discourse input from a user. Processing unit is configured to determine e.g. using the determining unit using a first language model a conditional probability of a candidate word in the discourse input given one or more words in the discourse input. Processing unit is configured to apply e.g. using the applying unit a weight to the conditional probability of the candidate word given the one or more words to obtain a weighted conditional probability of the candidate word given the one or more words. The weight is based on a conditional probability of the candidate word given one or more classes associated with the one or more words. Processing unit is configured to determine e.g. using the determining unit a text string corresponding to the discourse input based on the weighted conditional probability of the candidate word. Processing unit is configured to generate e.g. using the touch screen display unit or the speaker unit and using the generating unit an output based on the text string.

In some examples the weight is further based on a probability of the candidate word within a corpus. In some examples the weight is inversely proportional to the probability of the candidate word within the corpus. In some examples the weight comprises the conditional probability of the candidate word given the one or more classes divided by the probability of the candidate word within the corpus.

In some examples the weighted conditional probability of the candidate word is greater than the conditional probability of the candidate word given the one or more words when the conditional probability of the candidate word given the one or more classes is greater than the probability of the candidate word within the corpus. In some examples the weighted conditional probability of the candidate word is less than the conditional probability of the candidate word given the one or more words when the conditional probability of the candidate word given the one or more classes is less than the probability of the candidate word within the corpus.

In some examples the weighted conditional probability of the candidate word is a joint probability of the candidate word and one or more subclasses associated with the candidate word given the one or more words and the one or more classes.

In some examples the weight is proportional to the conditional probability of the candidate word given the one or more classes. In some examples the weighted conditional probability of the candidate word is proportional to the conditional probability of the candidate word given the one or more words.

In some examples the weighted conditional probability of the candidate word comprises a dot product of the weight and the conditional probability of the candidate word given the one or more words.

In some examples the weighted conditional probability of the candidate word is based on an assumption that a conditional probability of a subclass associated with the candidate word given the candidate word the one or more words and the one or more classes is equal to a conditional probability of the subclass associated with the candidate word given the candidate word and the one or more classes. In some examples the weighted conditional probability of the candidate word is based on an assumption that a conditional probability of the candidate word given a subclass associated with the candidate word and the one or more classes is equal to a conditional probability of the candidate word given the subclass associated with the candidate word. In some examples the weighted conditional probability of the candidate word is based on an assumption that a conditional probability of the one or more classes given the candidate word and the one or more words is equal to a conditional probability of the one or more classes given the one or more words.

As shown in the electronic device can include a touch screen display unit configured to display a user interface e.g. a virtual keyboard or a handwriting recognition interface an audio input unit configured to receive audio input e.g. speech input an optical sensor unit configured to receive images or gesture input and a speaker unit configured to output audio e.g. speech . The electronic device can further include a processing unit coupled to the touch screen display unit the audio input unit the optical sensor unit and the speaker unit. In some examples the processing unit can include a receiving unit a determining unit an applying unit a generating unit and a displaying unit .

Processing unit is configured to receive e.g. from the touch screen display unit the audio input unit or the optical sensor unit and using the receiving unit receiving a text input from a user. Processing unit is configured to determine e.g. using the determining unit using a first language model a conditional probability of a predicted word given one or more words in the text input. Processing unit is configured to determine e.g. using the determining unit using a second language model a probability of the predicted word within a corpus. Processing unit is configured to determine e.g. using the determining unit using a third language model a conditional probability of the predicted word given one or more classes associated with the one or more words. Processing unit is configured to determine e.g. using the determining unit an integrated probability of the predicted word and one or more subclasses associated with the predicted word based on the conditional probability of the predicted word given the one or more words the probability of the predicted word within the corpus and the conditional probability of the predicted word given the one or more classes. Processing unit is configured to display e.g. using the touch screen display unit and using the displaying unit the predicted word based on the integrated probability.

In some examples the integrated probability is a joint probability of the predicted word and the one or more subclasses associated with the predicted word given the one or more words and the one or more classes.

In some examples the integrated probability is determined based on assuming that a conditional probability of a subclass associated with the predicted word given the predicted word the one or more words and the one or more classes is equal to a conditional probability of the subclass associated with the predicted word given the predicted word and the one or more classes.

In some examples the integrated probability is determined based on assuming that a conditional probability of the predicted word given a subclass associated with the predicted word and the one or more classes is equal to the conditional probability of the predicted word given the subclass associated with the predicted word.

In some examples the integrated probability is determined based on assuming that a conditional probability of the one or more classes given the predicted word and the one or more words is equal to a conditional probability of the one or more classes given the one or more words.

In some examples the integrated probability is greater than the conditional probability of the predicted word given the one or more words when the conditional probability of the predicted word given the one or more classes is greater than the probability of the predicted word within the corpus.

In some examples the integrated probability is less than the conditional probability of the predicted word given the one or more words when the conditional probability of the predicted word given the one or more classes is less than the probability of the predicted word within the corpus.

In some examples the integrated probability is determined at least in part by dividing the conditional probability of the predicted word given the one or more classes by the probability of the predicted word within the corpus.

In some examples the integrated probability is less dependent on the conditional probability of the predicted word given the one or more classes when the probability of the predicted word within the corpus is higher than when the probability of the predicted word within the corpus is lower.

In some examples the first language model is a word n gram language model. In some examples the first language model and the second language model are a same language model. In some examples the third language model is a class m gram language model.

In some examples processing unit is configured to receive e.g. from the touch screen display unit the audio input unit or the optical sensor unit and using the receiving unit receive a text input from a user. Processing unit is configured to determine e.g. using the determining unit using a first language model a conditional probability of a predicted word given one or more words in the text input. Processing unit is configured to apply e.g. using the applying unit a weight to the conditional probability of the predicted word given the one or more words to obtain a weighted conditional probability of the predicted word given the one or more words. The weight is based on a conditional probability of the predicted word given one or more classes associated with the one or more words. Processing unit is configured to display e.g. using the touch screen display unit and using the displaying unit the predicted word based on the integrated probability.

In some examples the weight is further based on a probability of the predicted word within a corpus. In some examples the weight is inversely proportional to the probability of the predicted word within the corpus. In some examples the weight comprises the conditional probability of the predicted word given the one or more classes divided by the probability of the predicted word within the corpus.

In some examples the weighted conditional probability of the predicted word is greater than the conditional probability of the predicted word given the one or more words when the conditional probability of the predicted word given the one or more classes is greater than the probability of the predicted word within the corpus.

In some examples the weighted conditional probability of the predicted word is less than the conditional probability of the predicted word given the one or more words when the conditional probability of the predicted word given the one or more classes is less than the probability of the predicted word within the corpus. In some examples the weighted conditional probability of the predicted word is a joint probability of the predicted word and one or more subclasses associated with the predicted word given the one or more words and the one or more classes.

In some examples the weight is proportional to the conditional probability of the predicted word given the one or more classes.

In some examples the weighted conditional probability of the predicted word is proportional to the conditional probability of the predicted word given the one or more words. In some examples the weighted conditional probability of the predicted word comprises a dot product of the weight and the conditional probability of the predicted word given the one or more words.

In some examples the weighted conditional probability of the predicted word is based on an assumption that a conditional probability of a subclass associated with the predicted word given the predicted word the one or more words and the one or more classes is equal to a conditional probability of the subclass associated with the predicted word given the predicted word and the one or more classes. In some examples the weighted conditional probability of the predicted word is based on an assumption that a conditional probability of the predicted word given a subclass associated with the predicted word and the one or more classes is equal to a conditional probability of the predicted word given the subclass associated with the predicted word. In some examples the weighted conditional probability of the predicted word is based on an assumption that a conditional probability of the one or more classes given the predicted word and the one or more words is equal to a conditional probability of the one or more classes given the one or more words.

Although examples have been fully described with reference to the accompanying drawings it is to be noted that various changes and modifications will become apparent to those skilled in the art. Such changes and modifications are to be understood as being included within the scope of the various examples as defined by the appended claims.

