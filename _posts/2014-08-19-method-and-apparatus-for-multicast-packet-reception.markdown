---

title: Method and apparatus for multicast packet reception
abstract: Roughly described, incoming data packets are delivered by the NIC directly to at least two user level endpoints. In an aspect, only filters that cannot be ambiguous are created in the NIC. In another aspect, the NIC maintains a filter table supporting direct delivery of incoming unicast and multicast data packets to one user level endpoint. Additional requests to join the same multicast group cause replacement of the NIC filter with one in the kernel. In another aspect, a NIC has limited capacity to maintain multicast group memberships. In response to a new multicast filter request, the kernel establishes it in the NIC only if the NIC still has sufficient capacity; otherwise it is established in the kernel.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09083539&OS=09083539&RS=09083539
owner: SOLARFLARE COMMUNICATIONS, INC.
number: 09083539
owner_city: Irvine
owner_country: US
publication_date: 20140819
---
This application is a continuation of prior U.S. application Ser. No. 13 347 545 filed 10 Jan. 2012 entitled METHOD AND APPARATUS FOR MULTICAST PACKET RECEPTION by David Riddoch Martin W. Porter and Steven L. Pope which application is a continuation of prior U.S. application Ser. No. 11 351 177 filed 8 Feb. 2006 entitled METHOD AND APPARATUS FOR MULTICAST PACKET RECEPTION by David Riddoch Martin W. Porter and Steven L. Pope now U.S. Pat. No. 8 116 312 both of which applications are incorporated herein by reference in their entirety.

The invention relates to network interfaces and more particularly to mechanisms for hardware support of multicast data packet reception.

When data is to be transferred between two devices over a data channel such as a network each of the devices must have a suitable network interface to allow it to communicate across the channel. Often the network is based on Ethernet technology. Devices that are to communicate over a network are equipped with network interfaces that are capable of supporting the physical and logical requirements of the network protocol. The physical hardware component of network interfaces are referred to as network interface cards NICs although they need not be in the form of cards for instance they could be in the form of integrated circuits ICs and connectors fitted directly onto a motherboard or in the form of macrocells fabricated on a single integrated circuit chip with other components of the computer system.

Most computer systems include an operating system OS through which user level applications communicate with the network. A portion of the operating system known as the kernel includes protocol stacks for translating commands and data between the applications and a device driver specific to the NIC and the device drivers for directly controlling the NIC. By providing these functions in the operating system kernel the complexities of and differences among NICs can be hidden from the user level application. In addition the network hardware and other system resources such as memory can be safely shared by many applications and the system can be secured against faulty or malicious applications.

The kernel executes in kernel mode also sometimes called trusted mode or a privileged mode whereas application level processes also called user level processes execute in a user mode. Typically it is the processor subsystem hardware itself which ensures that only trusted code such as the kernel code can access the hardware directly. The processor enforces this in at least two ways certain sensitive instructions will not be executed by the processor unless the current privilege level is high enough and the processor will not allow user level processes to access memory locations including memory mapped addresses associated with specific hardware resources which are outside of a user level physical or virtual address space already allocated to the process. As used herein the term kernel space or kernel address space refers to the address and code space of the executing kernel. This includes kernel data structures and functions internal to the kernel. The kernel can access the memory of user processes as well but kernel space generally means the memory including code and data that is private to the kernel and not accessible by any user process. The term user space or user address space refers to the address and code space allocated by a code that is loaded from an executable and is available to a user process excluding kernel private code data structures. As used herein all four terms are intended to accommodate the possibility of an intervening mapping between the software program s view of its own address space and the physical memory locations to which it corresponds. Typically the software program s view of its address space is contiguous whereas the corresponding physical address space may be discontiguous and out of order and even potentially partly on a swap device such as a hard disk drive.

Although parts of the kernel may execute as separate ongoing kernel processes much of the kernel is not actually a separate process running on the system. Instead it can be thought of as a set of routines to some of which the user processes have access. A user process can call a kernel routine by executing a system call which is a function that causes the kernel to execute some code on behalf of the process. The current process is still the user process but during system calls it is executing inside of the kernel and therefore has access to kernel address space and can execute in a privileged mode. Kernel code is also executed in response to an interrupt issued by a hardware device since the interrupt handler is found within the kernel. The kernel also in its role as process scheduler switches control between processes rapidly using the clock interrupt and other means to trigger a switch from one process to another. Each time a kernel routine is called the current privilege level increases to kernel mode in order to allow the routine to access the hardware directly. When the kernel relinquishes control back to a user process the current privilege level returns to that of the user process.

When a user level process desires to communicate with the NIC conventionally it can do so only through calls to the operating system. The operating system implements a system level protocol processing stack which performs protocol processing on behalf of the application. In particular an application wishing to transmit a data packet using TCP IP calls the operating system API e.g. using a send call with data to be transmitted. This call causes a context switch to invoke kernel routines to copy the data into a kernel data buffer and perform TCP send processing. Here protocol is applied and fully formed TCP IP packets are enqueued with the interface driver for transmission. Another context switch takes place when control is returned to the application program. Note that kernel routines for network protocol processing may be invoked also due to the passing of time. One example is the triggering of retransmission algorithms. Generally the operating system provides all OS modules with time and scheduling services driven by the hardware clock interrupt which enable the TCP stack to implement timers on a per connection basis. The operating system performs context switches in order to handle such timer triggered functions and then again in order to return to the application.

It can be seen that network transmit and receive operations can involve excessive context switching and this can cause significant overhead. The problem is especially severe in networking environments in which data packets are often short causing the amount of required control work to be large as a percentage of the overall network processing work.

One solution that has been attempted in the past has been the creation of user level protocol processing stacks operating in parallel with those of the operating system. As used herein a user level operation is one that does not require changing to a higher privilege level to perform. User level protocol processing stacks can enable data transfers using standard protocols to be made without requiring data to traverse the kernel stack. It is desirable for the network interface device to be capable of supporting standard transport level protocols such as TCP UDP RDMA and ISCSI at user level. TCP is defined in RFC 0793 Transmission Control Protocol. J. Postel. Sep. 1 1981 and UDP is defined in RFC 0768 User Datagram Protocol. J. Postel. Aug. 28 1980 both incorporated by reference herein. By supporting transport protocols at user level data transfers that require use of standard protocols can be made without requiring data to traverse the kernel stack without requiring context switches and without requiring changing to a higher privilege level.

There are a number of difficulties in implementing transport protocols at user level. Most implementations to date have been based on porting pre existing kernel code bases to user level. Examples of these are Arsenic and Jet stream. These have demonstrated the potential of user level transports but have not addressed a number of the problems required to achieve a complete robust high performance commercially viable implementation.

TCP receive side processing takes place and the destination port is identified from the packet. If the packet contains valid data for the port then the packet is engaged on the port s data queue step iii and that port marked which may involve the scheduler and the awakening of blocked process as holding valid data.

The TCP receive processing may require other packets to be transmitted step iv for example in the cases that previously transmitted data should be retransmitted or that previously enqueued data perhaps because the TCP window has opened can now be transmitted. In this case packets are enqueued with the OS NDIS driver for transmission.

In order for an application to retrieve a data buffer it must invoke the OS API step v for example by means of a call such as recv select or poll . This has the effect of informing the application that data has been received and in the case of a recv call copying the data from the kernel buffer to the application s buffer. The copy enables the kernel OS to reuse its network buffers which have special attributes such as being DMA accessible and means that the application does not necessarily have to handle data in units provided by the network or that the application needs to know a priori the final destination of the data or that the application must pre allocate buffers which can then be used for data reception.

It should be noted that on the receive side there are at least two distinct threads of control which interact asynchronously the up call from the interrupt and the system call from the application. Many operating systems will also split the up call to avoid executing too much code at interrupt priority for example by means of soft interrupt or deferred procedure call techniques.

The send process behaves similarly except that there is usually one path of execution. The application calls the operating system API e.g. using a send call with data to be transmitted Step vi . This call copies data into a kernel data buffer and invokes TCP send processing. Here protocol is applied and fully formed TCP IP packets are enqueued with the interface driver for transmission.

If successful the system call returns with an indication of the data scheduled by the hardware for transmission. However there are a number of circumstances where data does not become enqueued by the network interface device. For example the transport protocol may queue pending acknowledgments or window updates and the device driver may queue in software pending data transmission requests to the hardware.

A third flow of control through the system is generated by actions which must be performed on the passing of time. One example is the triggering of retransmission algorithms. Generally the operating system provides all OS modules with time and scheduling services driven by the hardware clock interrupt which enable the TCP stack to implement timers on a per connection basis.

If a standard kernel stack were implemented at user level then application might be linked with the user level transport libraries rather than directly with the OS interface. The structure can be very similar to the kernel stack implementation with services such as timer support provided by user level packages and the device driver interface replaced with user level virtual interface module. Advantageously the NIC delivers incoming data packets directly to the appropriate instantiation of the user level transport library bypassing the kernel stack whenever possible.

In order that incoming data packets be delivered to the data port of the correct application a mechanism is required which examines the header information of the packet and looks up the associated destination queue. Such filtering is commonly performed on a number of fields of the header including source and destination ports and addresses. In order to maximize efficiency it is preferable that the filtering be accomplished in hardware on the network interface device rather than in software within the kernel or user level drivers. Where the NIC supports hardware filtering of incoming IP packets the kernel in response to an application s request for a filter may program the filter information into the NIC. In this case the NIC will forward to the kernel only those incoming data packets that satisfy one of its programmed filters and will ignore any that do not. The kernel then forwards the packet on to the endpoint designated in the matching filter. Typically NICs that support hardware filtering have only limited capacity to store them so if applications have requested too many filters the kernel simply programs the NIC to forward all incoming data packets to the kernel and performs the filtering entirely in software.

One means of filtering packets in the network interface card is by presenting the packet header information to a content addressable memory CAM which associates each combination of header bit information with a specified receive port. Alternatively iterative lookup can be performed through a table comparing the header bit information with various entries in the table successively until a match is found. In yet another alternative one might consider a hashed lookup which is a form of hybrid between the parallel approach of a CAM and the sequential approach of an iterative lookup. In a hashed lookup the table is subdivided into a number of subsets of entries. The input data is passed through a consistent mathematical hashing function which converts it to an index called a hash code pointing to the particular list or bucket within which the data belongs. When new data is to be added to the table it is simply inserted into an available free entry of the list pointed to by its hash code. When input data is to be located in the table an iterative search is performed within the list pointed to by the hash code. Many hashing algorithms are known and some examples may be found in Knuth Volume 3 Sorting and Searching 2nd Edition incorporated herein by reference.

Most data packets on a typical network are unicast packets. A unicast data packet is transmitted from a sender to a single destination endpoint. The term endpoint is used herein in the sense of communications protocols to denote the entity on one end of a transport layer connection. An endpoint is not precluded from forwarding a data packet on to yet further destinations. On IP based networks a unicast data packet is identified by source and destination IP addresses and port numbers in the header. The destination IP address designates the destination host computer network interface to which the packet should be delivered and the destination port number designates a particular destination endpoint typically a software endpoint within the host. An application that expects to receive unicast data packets through a particular network interface typically makes calls to the operating system kernel to set up a filter so that any incoming data packet having specified header information destination IP address destination port number protocol and optionally source IP address and source port will be delivered to that application.

In addition to unicast data packet transmission many network protocols including IP also support broadcast data packet transmission. Broadcast packets are far less frequent than unicast packets and are intended to be received by all host interfaces connected to the network. Broadcast packets are typically used for network configuration and management purposes rather than content transmission. On IP based networks a broadcast data packet is identified by a destination address of 255.255.255.255 which is a 32 bit address of all 1 s. Virtually all NICs will receive broadcast data packets and forward them on to the kernel for further processing regardless of whether the NIC performs hardware filtering.

Many network protocols including IP also support multicast data packet transmission. Multicasting is the transmission of a data packet to a set of zero or more host network interfaces who have subscribed to a multicast group . Multicasting is used mainly for transmitting or streaming identical content to numerous recipients for example an audio or video feed or a continuous financial information feed. On IP based networks a multicast data packet is identified by a destination address within the range 224.0.0.0 to 239.255.255.255 that is a 32 bit address in which the high order four bits are 1110 . The remaining 28 bits designate a multicast group number except that the address 224.0.0.0 is not to be assigned to any group and 224.0.0.1 is assigned to the permanent group of all IP hosts including gateways . This is used to address all multicast hosts on the directly connected network. Multicast data packets are delivered to all members of the designated multicast group. Membership in a multicast group is dynamic that is hosts may join and leave groups at any time and a host may be a member of more than one group at a time.

Internetwork forwarding of IP multicast data packets is handled by multicast capable routers which may be co resident with or separate from internet gateways. A host transmits an IP multicast datagram as a local network multicast which reaches all immediately neighboring members of the destination multicast group. If the packet has an IP time to live greater than 1 the multicast router s attached to the local network take responsibility for forwarding it towards all other networks that have members of the destination group. On those other member networks that are reachable within the IP time to live an attached multicast router completes delivery by transmitting the datagram as a local multicast.

Within a single host computer system more than one endpoint can request membership in a multicast group. In a conventional arrangement such as that of all incoming multicast data packets are reported to the kernel for forwarding to any applications that are members of the multicast group. illustrates this arrangement symbolically. Since the kernel needs to know which application processes are members of each multicast group it maintains a table such as that shown in . Each entry in the table contains a multicast group number or full multicast group IP address a destination IP port number the number of software endpoints in the host that are members of the group and a pointer to a linked list of the socket handles of the member endpoints. Multicast group membership is specific to each network interface so if a particular host has more than one IP address assigned to it at either one or more physical network connections then the kernel maintains a separate table like that shown in for each interface. Alternatively it might maintain a single table for all interfaces with a further field in each entry of the particular interface identifying to which interface the membership applies.

Note that in a different embodiment setSockOpt calls might be made by the transport library rather than directly by an application. This might occur for example if the NIC driver software or OS itself implements a protocol stack for a multicast protocol. In the case of user level transport libraries which operate within user level rather than a kernel process it is unimportant whether the call is made by the transport library or by the application directly in both cases the call is made by a user level process.

After step or if the host already does have a member of this group on this interface step then in step the kernel adds the socket s port number to its list of memberships for the specified multicast group at the specified local interface. In step the kernel increments its count of the number of memberships in the specified multicast group at the specified interface and in step the kernel returns to the calling application process.

It can be seen that in the environment of where all incoming multicast data packets come up through the kernel for delivery to the individual user level endpoint members of the particular multicast group the kernel s table of multicast group memberships is central. But as previously mentioned it is advantageous to implement network protocol stacks at user level so as to bypass the kernel as much as possible. illustrates this arrangement symbolically. In this arrangement at least some incoming multicast data packets are delivered directly from the NIC to the individual member application processes bypassing the kernel. It is not at all clear in the arrangement of whether or where to implement group membership tables or how to maintain them.

Accordingly it is an object of the invention to implement multicast data packet reception in an environment that takes advantage to whatever extent possible of an architecture in which network protocol stacks are implemented at user level and incoming data packets are delivered from the NIC to the user level stacks without intervention by the kernel.

Roughly described in an aspect of the invention incoming multicast data packets are delivered by the NIC to a user level endpoint directly bypassing the kernel.

In another aspect of the invention incoming data packets are delivered by the NIC to at least two user level endpoints in a single host computer system bypassing the kernel.

In another aspect of the invention roughly described incoming unicast packets of supported protocols are delivered by the NIC directly to a user level endpoint. When an application process makes a system call that involves the establishment of a filter for the delivery of incoming data packets then it is first determined whether the call is of a character that permits a second user level process to make a request to deliver the same incoming data packets also to a second user level endpoint. If not then the requested filter may be established in hardware on the NIC assuming all other requirements are met. Future incoming data packets matching the filter criteria will then be passed from the NIC directly to the user level driver without requiring a context switch to the kernel. But if the call is of a character that does permit a second user level process to make a request to deliver the same incoming data packets also to a second user level endpoint then the kernel instead establishes the filter in its own tables and the NIC delivers matching packets to the kernel for forwarding to the appropriate user level process es .

In another aspect of the invention the NIC maintains a receive filter table that supports direct delivery of incoming data packets to one user level endpoint. Unicast filters are maintained in the NIC s filter table as are multicast filters for multicast groups having only one endpoint member running in the host. The NIC need not know whether a particular filter is for unicast or multicast reception. Instead in response to a user level request to join a multicast group the kernel or another process running in the host programs the multicast filter into the NIC only if the NIC does not already have a filter for that multicast IP address and destination port number combination. Thus if a request to join a multicast group is the first such request for the specified group and port number then the kernel programs a filter into the NIC receive filter table just as it programs a unicast filter. The only difference is that the destination IP address specified in the filter is a multicast group IP address rather than a unicast destination IP address. If the request to join the multicast group is the second or subsequent such request for the same multicast group and port number then the filter is established in the kernel multicast group membership table instead.

More generally a NIC has limited capacity to maintain multicast group membership information. For example the NIC might support multicast group membership of only some predetermined number N endpoints have joined the group or it might support multicast group membership as long as the total number of endpoint memberships within the host computer system including all multicast groups does not exceed some maximum number M. In this aspect when an application process makes a system call that involves the establishment of a filter for the delivery of incoming multicast data packets then it is first determined whether the NIC has sufficient capacity to add the requested filter. If so then the requested filter is established in hardware on the NIC. Future incoming data packets matching the filter criteria will then be passed from the NIC directly to the user level driver potentially as well as being passed from the NIC directly to user level drivers of other processes as well but without requiring a context switch to the kernel. But if the NIC does not have sufficient capacity to add the requested filter then the filter is instead established in the kernel s own tables and the NIC delivers matching packets to the kernel for forwarding to the appropriate user level process es . In some embodiments the kernel or the user level driver may also cause all entries in the NIC s multicast group membership tables for the same multicast group or for the same multicast group and port combination to be purged and transferred to the kernel tables as well.

The following description is presented to enable any person skilled in the art to make and use the invention and is provided in the context of a particular application and its requirements. Various modifications to the disclosed embodiments will be readily apparent to those skilled in the art and the general principles defined herein may be applied to other embodiments and applications without departing from the spirit and scope of the present invention. Thus the present invention is not intended to be limited to the embodiments shown but is to be accorded the widest scope consistent with the principles and features disclosed herein.

The network interface card provides an interface to outside networks including an interface to the network and is coupled via network to corresponding interface devices in other computer systems and one or more routers such as . Network may comprise many interconnected computer systems and communication links. These communication links may be wireline links optical links wireless links or any other mechanism for communication of information. While in one embodiment network is the Internet in other embodiments network may be any suitable computer network or combination of networks. In and embodiment described herein network supports an Ethernet protocol.

Host memory subsystem typically includes a number of memories including a main random access memory RAM for storage of instructions and data during program execution and a read only memory ROM in which fixed instructions and data are stored. One or more levels of cache memory may also be included in the host memory subsystem . For simplicity of discussion the host memory subsystem is sometimes referred to herein simply as host memory . As used herein virtual memory is considered part of the host memory subsystem even though part of it may be stored physically at various times on a peripheral device. The host memory contains among other things computer instructions which when executed by the processor subsystem cause the computer system to operate or perform functions as described herein. As used herein processes and software that are said to run in or on the host or the computer execute on the processor subsystem in response to computer instructions and data in the host memory subsystem including any other local or remote storage for such instructions and data.

The communication channel provides a mechanism for allowing the various components and subsystems of computer system to communicate with each other. In one embodiment the communication channel comprises a PCI Express bus. Other embodiments may include other buses and may also include multiple buses. The PCI bus and its progeny including the version known as PCI Express support burst transfer protocols such as that described above. PCI express is described in PCI Special Interest Group PCI Express Base Specification 1.0 Apr. 15 2003 incorporated herein by reference.

Computer system itself can be of varying types including a personal computer a portable computer a workstation a computer terminal a network computer a television a mainframe a server or any other data processing system or user devices. Due to the ever changing nature of computers and networks the description of computer system depicted in is intended only as a specific example for purposes of illustrating an embodiment of the present invention. Many other configurations of computer system are possible having more or less components and configured similarly or differently than the computer system depicted in .

The NIC supports resources of a number of types i.e. resources having capabilities of different natures. Examples include DMA queues event queues timers and support resources for virtual to physical memory mapping of the type described in WO2004 025477 incorporated by reference herein. Each type of resource is provided from a dedicated hardware resource pool which can support numerous instances of resources of the respective type. In order for such an instance to be made operational it is configured by means of instructions from the computing device as described in more detail below.

The NIC communicates with the computing device over the bus . In this example the bus is a PCI bus but the invention is not limited to such a bus. Data transmitted over the PCI bus is associated with a destination address and is received by whichever entity that is connected to the bus has had that address allocated to it. In a typical PC implementation the addresses are allocated in pages of 4 or 8 kB. One or more of these pages may be allocated to the NIC . Blocks and represent allocated pages on the PCI bus .

The NIC has a bus interface controller BIC a resource configuration unit RCU and a bus mapping table . The resource configuration unit processes communications received from the computer that provide instructions on the allocation re allocation and de allocation of resources on the NIC and configures the resources in accordance with such instructions. The kernel driver stores a record of which resources on the NIC are allocated. When a resource is to be allocated the driver identifies a suitable free resource of the required type on the NIC and transmits an allocation instruction to the NIC . The instruction identifies the resource and specifies the details of how it is to be allocated including details of the internal configuration of the resource e.g. in the case of a timer the amount of time it is to run for . That instruction is passed to the resource configuration unit. The resource configuration unit then loads the specified configuration into the identified resource. The instruction also includes an ownership string which may be an identification of which application or process on the computer is using the resource. The resource configuration unit stores these in a row of the bus mapping table. When a resource is to be re allocated the relevant entries in the resource s own configuration store and in the bus mapping table are altered as necessary. When a resource is to be de allocated it is disabled and any rows of the bus mapping table that relate to it are deleted.

During setup of the system one or more pages on the bus are allocated to the NIC . Part of this address space page can be used by the kernel driver to send instructions to the NIC . Other pages e.g. page can be used for communication between application processes such as application and the resources . The resource configuration unit stores a record of the pages that are allocated to the NIC for use by resources. Note that in some embodiments some or all of the functions of the resource configuration unit may alternatively be provided by the kernel driver itself.

When an application wishes to open a data connection over the network it calls a routine in the user level transport library to cause the NIC resources that are required for the connection to be allocated. Standard types of network connection require standard sets of resources for example an event queue transmit and receive DMA command queues and a set of DMA able memory buffers. For example a typical set may contain one TX command queue one RX command queue two timers and on the order of 100 1000 DMA memory buffers.

The user level transport library includes routines that can be called directly by the application process and that initiate the allocation of such standard sets of resources including set numbers of resources of different types. The transport library also includes routines that allow a resource of each type to be allocated re allocated or de allocated individually. The presence of both these types of instruction means that standard connections can be set up efficiently and yet non standard groups of resources can be created and existing connections can be reconfigured on a resource by resource basis. As used herein a user level process is a process that runs in unprotected mode. Similarly a user level stack is any protocol processing software that runs in unprotected mode. A protocol stack is the set of data structures and logical entities associated with the networking interfaces. This includes sockets protocol drivers and the media device drivers.

The routines for allocation re allocation and de allocation of resources require access to restricted memory mapped addresses such as page for sending configuration instructions to the NIC . Since the user level transport library lacks the necessary privilege level to perform these accesses these routines in the user level transport library make calls to the kernel driver . In a Unix environment for example such calls might take the form of IOCtl system calls. These calls cause an initial context switch to a kernel level process which in turn communicate the instructions to the NIC for the allocation of the resources as specified in the routines. Those instructions specify the identity of the application or process with which the resources are to be associated and the nature of the resources. The instructions are processed by the resource configuration unit of the NIC .

The space on the bus that is allocated to the NIC can be split dynamically between the resources on the bus . Once one or more pages have been allocated to the NIC for use by resources those resources can be allocated one or more individual sub page addresses within that page corresponding to locations as illustrated at . Thus each resource can have a part of the total space allocated to it. A record of which part of the total space is allocated to which resource is stored in the bus mapping table . The effect is that a single page of the bus can be used for communication to resources of multiple types and or resources that relate to multiple connections and or resources that are associated with multiple applications or processes on the computer . As a result the total bus space can be used relatively efficiently.

The usage of the allocated bus space is managed by the kernel driver . When a resource is to be allocated the RCU identifies using a data store whose content it manages an unused block in the space on the bus that has already been allocated for use by resources of the NIC the space being of the size required for the resource. It then stores in that data store the identity of the resource resource ID the address of the block within the allocated space sub page ID and the identity of the application or process that is to use the resource process tag and sends a message to the resource configuration unit to cause it to store corresponding data in the bus mapping table . If the RCU finds that table indicates the address to be already occupied then it returns an error code to the driver. The sub page address may need to be supplemented with the address of the page in which the sub page lies if that cannot be inferred as a result of only a single page having been allocated for use by the resources. If the total space allocated for use by resources is insufficient then the kernel driver allocates it more space. Having allocated the resources the RCU returns a success message to the kernel driver. The allocated page and sub page addresses are returned to and mapped into the virtual address space of the user level process that requested the resources in order that it can access them by means of that data. Another context switch then takes place back to the user level calling process.

An application that has had resources allocated to it can access them by sending data e.g. by means of load store cycles through a virtual memory mapping to the relevant bus page at the sub page address corresponding to the respective resource. Since these addresses are part of the application s virtual address space no context switch to any kernel level processes are required in order to perform these accesses. Any data sent to pages allocated to resources is picked off the bus by the bus interface controller . It directs that data to the appropriate one of the resources by performing a look up in the table to identify the identity of the resource to which the sub page address has been allocated. An application can also access a resource by means other than a bus write for example by means of direct memory access DMA . In those instances the NIC checks that the identity of the application process from which the access has been received matches the identity indicated in the table for the resource. If it does not match the data is ignored. If it matches it is passed to the relevant resource. This adds to security and helps to prevent corruption of the resources by other applications.

The set of resources allocated to an application or process is sometimes referred to herein as a virtual network interface VNIC .

Once a virtual interface has been composed it may be reconfigured dynamically. As one example of dynamic reconfiguration a resource that is no longer required may be freed up. To achieve this the application using the resource calls a de allocation routine in the user level transport library . The de allocation routine calls the kernel driver which instructs the RCU to de allocate the resource by disabling it clearing its status and deleting its row in the table .

As another example of dynamic reconfiguration additional resources may be added to the VNIC. The process is analogous to that described above for initial composition of the VNIC.

As yet another example of dynamic reconfiguration resources may be passed from one application or process to another. This is most useful in the situation where a single application has multiple processes and wants to pass control of a resource from one process to another for example if data from the network is to be received into and processed by a new process. To achieve this the application using the resource calls a re allocation routine in the transport library . The re allocation routine calls the kernel driver which instructs the RCU to re allocate the resource modifying its row in the table to specify the identity of the application or process that is taking over its control.

In some instances it may be desirable for resources of one type to communicate with resources of another type. For example data received from the network may be being passed to an application for processing. The application has a queue in a memory connected to the bus . The queue is managed in part by the transport library which provides a DMA queue resource on the NIC with an up to date pointer to the next available location on the queue . This is updated as the application reads data from the queue . When data is received from the network it is passed to an event queue resource which writes it to the location identified by the pointer and also triggers an event such as an interrupt on the computing device to indicate that data is available on the queue. In order for this to happen the event queue resource must learn the pointer details from the DMA queue resource . This requires data to be passed from the DMA queue resource to the event queue resource.

To achieve this the process tag column of the table can be treated more generally as an ownership tag and can link the DMA queue to the related event queue. To achieve this the ownership tag of the event queue can be set to the identity of the related DMA queue. When the DMA queue needs to pass data to the related event queue it can identify the event queue from the table by performing a look up on its own identity in the ownership tag column.

Data intended to be passed from one resource to another can be checked by the bus interface controller to ensure that it is compatible with the settings in the table . Specifically when data is to be sent from one resource to another the bus controller checks that there is a row in the table that has the identity of the resource that is the source of the data in the ownership tag field and the identity of the resource that is the intended destination of the data in the resource ID field. If there is no match then the data is prevented from reaching its destination. This provides additional security and protection against corruption. Alternatively or in addition it may be permitted for one resource to transmit data to another if both are in common ownership in this example if their resource ID fields indicate that they are owned by the same process application or other resource.

The identities of resources linked in this way can also be reconfigured dynamically by means of the re configuration routines in the transport library.

 I TCP code which performs protocol processing on behalf of a network connection is located both in the transport library and in the OS kernel. The fact that this code performs protocol processing is especially significant.

 ii Connection state and data buffers are held in kernel memory and memory mapped into the transport library s address space

 iii Both kernel and transport library code may access the virtual hardware interface for and on behalf of a particular network connection

 iv Timers may be managed through the virtual hardware interface these correspond to real timers on the network interface device without requiring system calls to set and clear them. The NIC generates timer events which are received by the network interface device driver and passed up to the TCP support code for the device.

It should be noted that the TCP support code for the network interface device is in addition to the generic OS TCP implementation. This is suitably able to co exist with the stack of the network interface device.

As a result of the above mechanisms the operating system and many application programs can each maintain multiple TX RX and Event DMA command queues. illustrates this feature. As can be seen the operating system maintains via kernel driver TX RX and Event data queues. Each such queue has an associated DMA command queue not shown in but maintained in the host memory by the kernel driver . Multiple applications can also be running in the computer system each with its own instance of user level driver . Each such application maintains via its respective user level driver instance TX RX and Event data queues. As with the kernel queues each such TX RX and Event data queue has an associated DMA command queue not shown in but maintained in the host memory by the respective user level driver . Note that the kernel driver is also able to communicate data packets received by the kernel to the user level driver of individual target applications. This communication occurs by standard interprocess communication mechanisms of the operating system.

Individual buffers may be either 4 k or 8 k bytes long in one embodiment and they are chained together into logically contiguous sequences by means of physically contiguous descriptors in a buffer descriptor table stored in the NIC . For example one receive queue might occupy buffers and in host memory which are discontiguous and possibly out of order regions of memory. They are chained together into a single logically contiguous space by the physically contiguous entries and in the buffer descriptor table . The entries and are written and managed by the host kernel driver and are viewed as a wrap around ring. So for example if the host wishes to define a receive queue having 64 k entries for receive data buffer descriptors and each buffer is 4 k in size then the host will allocate a physically contiguous sequence of 16 entries in buffer descriptor table for this receive queue. Similarly one event queue might occupy buffers and in host memory . These buffers are discontiguous and possibly out of order in host memory but are chained together into a single logically contiguous wrap around space by the physically contiguous entries and in the buffer descriptor table . The buffer descriptor table is indexed by buffer ID and each of its entries identifies among other things the base address of the corresponding buffer in host memory .

In order to keep track of the state of each of the transmit receive and event queues for the many user level applications that might be in communication with NIC at the same time the NIC includes a receive queue descriptor table a transmit queue descriptor table and an event queue descriptor table . The transmit receive and event queue descriptor tables are shown in as separate tables each containing the entire table but it will be appreciated that in different embodiments the three tables can be implemented as a single unified table or one of the tables can be implemented as separate sub tables divided by columns or by rows or by both or some combination of these variations might be implemented. Each receive queue has a corresponding receive queue ID which is used as an index into the receive queue descriptor table . The designated entry in the receive queue descriptor table is the starting point for describing the state and other characteristics of that particular receive queue as viewed by the NIC . Each such entry identifies among other things 

The host maintains host centric versions of the read and write pointers as well and when it has added additional receive buffers to the queue it so notifies the NIC by writing its updated host centric receive queue write pointer into the address on the NIC of the device centric receive queue write pointer for the particular receive queue.

As shown in the NIC also includes a filter table and logic block . Because the NIC can support multiple simultaneous connections between user level applications and remote agents on LAN and because the NIC supports these using multiple transmit and receive queues one function performed by the NIC is to direct each incoming data packet to the correct receive queue. The mechanisms used by NIC to make this determination are described in detail hereinafter but generally the filter table and logic maintains a correspondence between packet header information and destination receive queue ID. The filter table and logic thus uses the header information from the incoming packet to determine the ID of the proper destination receive queue and uses that receive queue ID to index into the receive queue descriptor table . The receive queue ID is the starting point for the NIC to obtain all required information about the destination receive queue for proper forwarding of the packet data.

In an embodiment in which the NIC is able to deliver incoming multicast data packets to more than one endpoint in the host the filter table and logic maintains the correspondence between the multicast group ID from the packet header and the ID s of the several receive queues to which it is to be delivered. The structure and operation of the filter table and logic to accomplish this is described hereinafter.

In various embodiments as described in more detail hereinafter the filter table and logic may contain a primary filter table for supporting primarily unicast reception as well as one or more secondary filter tables for supporting primarily multicast reception. In an embodiment in which no secondary filter table is included the one remaining filter table is sometimes nevertheless referred to herein as a primary filter table. Similarly in an embodiment that does include one or more secondary filter tables the term filter table as used herein refers to the primary and secondary filter tables collectively. In addition it will be appreciated that the logical structure of one or more tables need not necessarily correspond to distinct physical table structures in a hardware implementation. As used herein therefore the term table does not necessarily imply any unity of structure. For example two or more separate tables when considered together still constitute a table as that term is used herein.

Thus logically described in order to deliver a received data packet to the destination receive queue in host memory the NIC first uses the header information of the data packet to look up in the filter table the appropriate destination receive queue ID s . It then uses the ID of each particular receive queue to look up in the receive queue descriptor table the buffer ID of the base buffer containing the receive descriptor queue. The NIC also obtains from the same place the current device centric read pointer into that receive descriptor queue. It then uses the base buffer ID as a base and the device centric read pointer high order bits as an offset into the buffer descriptor table to obtain the base address in host memory of the buffer that contains the particular receive queue. The NIC then uses that base address as a base and as an offset the device centric receive queue read pointer low order bits times the number of bytes taken up per descriptor as a starting host memory address for retrieving entries from the particular receive descriptor queue. The NIC does not allocate separate space for maintaining a write pointer into any local cache into which these entries will be written.

Entries for kernel receive descriptor queues can identify the buffer physical address itself rather than a buffer ID because the kernel is trusted to write the correct physical address whereas a user level queue is not.

The NIC then uses the buffer ID of the current receive data buffer as another index into buffer descriptor table to retrieve the buffer descriptor for the buffer into which the current receive data is to be written. Note this buffer descriptor is an individual entry in buffer descriptor table unlike the descriptors for buffers containing receive queues or event queues this buffer descriptor is not part of a ring. The NIC obtains the physical address in host memory of the current receive data buffer and then using that physical address as a base and the 2 byte aligned offset from the receive descriptor queue entry as an offset it determines the physical starting address in host memory into which the data transfer should begin. The NIC then transfers the received data packet into host memory beginning at that address. If the packet is a multicast packet then the NIC maintains a count of the number of transfers to host memory that remain for the received data packet. When this count reaches zero the NIC can release its own buffer for the packet.

The system handles transmit queues in a similar manner except that multicast transmit packets are transmitted only once to the multicast group IP address.

The receive queue descriptor table entry designated by the receive queue ID as previously mentioned also contains the ID of the receive event queue associated with the particular receive queue. Similarly the transmit queue descriptor table entry designated by the transmit queue ID contains the ID of the event queue associated with the particular transmit queue. All of the event queues for all the applications are described by respective entries in the event queue descriptor table . The entry in the event queue descriptor table identified by a queue ID from the receive or transmit queue descriptor table or is the starting point for describing the state and other characteristics of that particular event queue as viewed by the NIC .

Note that as illustrated in whereas each slot e.g. shown in the buffer descriptor table represents a single descriptor each slot e.g. in the host memory represents a memory page of information. A page might be 4 k or 8 k bytes long for example so if a receive data buffer descriptor in a receive queue occupies either 4 or 8 bytes then each slot or as shown in might hold 512 1 k or 2 k receive data buffer descriptors.

In a host receive event management module in the user level transport library for a given application process receives an indication from other software to expect data from the network step . The module is also activated in response to receipt by the host of a receive queue empty event as described hereinafter step . The module also may be activated periodically on expiration of a polling loop or timer step . The host subsystem will push receive buffers onto this receive queue in response to these triggers but limited so as to guarantee that the corresponding event queue will not overflow. The host subsystem therefore will not queue more data buffers for receive data than can be accommodated in the receive event queue by the number of receive completion events that would be generated.

After determining the amount of space currently available in the receive event queue in step the host subsystem determines a number M being the lesser of the number of data buffers available for queuing of receive data and the minimum number of receive data buffers that can be represented by receive completion events in the space available in the receive event queue as determined in step .

In step it is determined whether M is greater than or equal to some minimum threshold. Preferably the minimum threshold is 1 but in other embodiments a larger number may be chosen for the threshold. If M is less than the threshold then the host receive event queue management module simply goes inactive to await the next activation event step .

If M is greater than or equal to the minimum threshold then in step the host subsystem updates modulo increments its host centric receive queue write pointer by M entries. In step the host subsystem writes M available receive data buffer descriptors into the receive queue beginning at the entry previously before step designated by the host centric receive queue write pointer. In step the host subsystem notifies the NIC of the updated write pointer and in step the NIC updates its own device centric receive queue write pointer for the specified receive queue. In one embodiment steps and are combined into a single step in which the host subsystem writes the updated write pointer into a memory mapped location of the device centric receive queue write pointer. In step the host receive event queue management module goes inactive to await the next activation event.

In step the NIC writes data from the incoming packet into the receive data buffer designated by the retrieved descriptor beginning at the specified offset. Writing continues by DMA until either the end of the current data buffer is reached or the end of the incoming data packet is reached or both.

The NIC detects and reports a queue empty alert when it believes it has retrieved and used the last buffer descriptor in the particular receive queue. This alert is combined into a single event descriptor with the receive completion event. In particular the NIC determines in step whether it believes it has used the last receive buffer identified by a descriptor in the receive queue. The NIC can determine this by comparing its device centric receive queue read pointer to its device centric receive queue write pointer for the particular receive queue. If not that is the NIC knows there are more receive buffer descriptors in the receive queue then no alert is necessary and in step the NIC determines whether end of packet has been reached. If not then the NIC receive data module returns to step to retrieve the descriptor for the next receive data buffer. No event is asserted to indicate Receive Data Buffer Full in this embodiment. The host will become aware of which receive data buffers are full based on the receive data buffers identified consecutively in the receive queue beginning at the host centric RX queue read pointer.

If step determines that end of packet was reached then in step the NIC asserts a receive completion event to cover all the receive data buffers that contain data from the packet. The receive completion event descriptor format includes a receive descriptor queue empty flag rx desc q empty but in the receive completion event written in step this flag is not set because the NIC has determined in step that additional receive buffer descriptors remain in the receive queue. Note that in this embodiment only one receive completion event will be asserted even if the packet data spans multiple buffers in receive data buffers. Multiple buffers are chained together by consecutive entries in the receive queue. Note also that if end of packet does not coincide with the end of a receive buffer then the remaining space in the buffer is left unused.

Returning to step if the NIC believes that the last receive data buffer identified by a descriptor in the receive queue has been retrieved in step then the NIC does not wait until end of packet before reporting the receive completion event. Instead in step the NIC asserts a receive completion event to cover all the receive data buffers that contain data from the packet. In this receive completion event the rx desc q empty flag is set. If packet data remains in the NIC s RX FIFO when this occurs it is lost.

In an embodiment in which the NIC supports more than one network port the NIC does not batch receive completion events. Receive completion events do not indicate completion of more than one receive data buffer. This embodiment supports both standard size data packets in which data packets have a relatively small maximum length and the receive data buffers are at least as large as the maximum data packet length and jumbo data packets in which a data packet can be longer and can span more than one data buffer. A given receive queue is either in standard mode or jumbo mode. If the queue is in standard mode then absent an error every receive data buffer filled will contain an end of packet so no receive completion event will indicate completion of more than one data buffer and the problem will not arise. If the queue is in jumbo mode then it is still the case that no receive completion event will indicate completion of more than one data buffer since the NIC writes a receive completion event for each data buffer it fills. The receive completion event format includes a RX Jumbo Cont bit which the NIC sets in order to notify the host subsystem that the subject data buffer does not contain an end of packet i.e. there will be a continuation buffer . This embodiment therefore does not batch receive completion events. The receive completion event still includes a copy of the NIC s updated device centric receive queue read pointer which now points to the specific descriptor from the receive queue for whose data buffer the event indicates completion. The receive completion event format also indicates the NIC port number from which the packet was received.

Returning to the embodiment after both steps and once the NIC has asserted a receive completion event the NIC receive data module then returns to an inactive state step .

In both steps and the NIC asserts a receive completion event containing certain information. is a flowchart detail of this step. In step the NIC writes the receive completion event into the corresponding receive event queue beginning at the entry identified by the device centric receive event queue write pointer for that event queue. In step NIC correspondingly updates its own receive event queue write pointer. In step if enabled the NIC generates a wake up event for this event queue and writes it into an event queue associated with a char driver in the kernel. In step again if enabled the NIC generates an interrupt to activate the host char driver event queue handler then disables interrupts. In step the host char driver event queue handler upon reaching the wake up event activates the receive event handler in the process that owns the specified receive queue.

In step the host subsystem retrieves the event descriptor at the location in the event queue designated by the receive event queue read pointer. If this new event is not in the cleared state step then the receive event queue contains events for handling at this time. In step it is determined whether the new event is a receive completion event. In one embodiment receive event queue cannot contain any events other than receive completion events but in another embodiment it can. Thus if the current event is something other than a receive completion event such as a management event then it is handled in step .

If the current event is a receive completion event then in step the host determines whether the Receive Queue Empty flag is set. If so then the module in step triggers the host receive event queue management module in order to replenish the receive queue with additional receive data buffers. Additional receive data buffers can be posted at other times as well since it is highly desirable to avoid any possibility of a receive queue empty condition. In step the host determines further whether any of a variety of error types are indicated by the receive completion event descriptor. If so then in step the host handles the error. Note that some of the error types included in step may actually be detected before or after the receive queue empty test of step some may bypass the replenishment triggered by step of receive buffer descriptors in receive queue for the time being and some may bypass processing of the data packet in step . The details of such error handling are not important for an understanding of the invention.

In step assuming no serious error has been detected the host processes the newly received packet data including protocol processing. This may require chaining together several receive data buffers in sequence as designated by consecutive receive queue entries. The host knows the starting buffer and offset of the packet from the buffer descriptor in the receive queue pointed to by the host centric receive queue read pointer and knows the end of the packet either from the receive packet byte count identified in the receive completion event or from the copy of the device centric receive queue read pointer that might be included in the receive completion event. After processing the packet data in these buffers the host may release the buffers back into a pool for eventually re writing into the receive queue for re use by different incoming packet data.

In step if the higher level software is so designed the host subsystem may reprogram the receive queue entry pointed to by the host centric receive queue read pointer with a descriptor for a new available receive data buffer and may do the same with respect to all consecutively subsequent receive queue entries up to but not including the receive queue entry pointing to the beginning of data for the next receive packet. In step the host subsystem modulo increments the host centric receive queue read pointer for the receive queue by the number of buffers represented in the current receive completion event. In step the host subsystem clears the event descriptor at the location in receive event queue identified by the current receive event queue read pointer and in step the host subsystem modulo increments the receive event queue read pointer. The module then loops back to step to retrieve the next event descriptor and so on until a cleared entry is retrieved and the module goes inactive step .

If in step it is determined that the retrieved next event descriptor is cleared then the receive event queue contains no more events for handling at this time. In one embodiment the host receive event handler would then simply go inactive to await the next activation trigger step . In another embodiment in step if the host centric receive event queue read pointer has changed then the host writes the updated pointer value into the NIC s device centric receive event queue read pointer. The host receive event handler then goes inactive in step .

In step the kernel driver programs initial search limits for four kinds of filter table hash searches into the NIC . Before explaining this step it will be useful to understand the organization of the primary receive filter table. There are many ways to organize the receive filter table but in the present embodiment all types of entries are intermixed in a single table address space and the same table entry format at least functionally is used in both the NIC s receive filter table and in the kernel s host centric copy. Four types of entries are supported TCP full TCP wildcard UDP full and UDP wildcard. Table I below illustrates the format for an entry in the receive filter table. For purposes of the present discussion any multicast extensions to the table format are not yet described except to note that many kinds of multicast data are carried in UDP packets.

It can be seen that each entry contains up to five fields for identifying a particular TCP or UDP endpoint in the host subsystem protocol TCP or UDP source IP address source port number destination IP address destination port number plus one for the associated receive queue ID. The queue ID field points to the entry in the receive queue descriptor table into which an incoming packet should be delivered when the endpoint data specified in the entry matches that of the header of the incoming packet.

The four fields source IP address source port number destination IP address and destination port number are referred to herein as the endpoint data portion of the entry. For a TCP full entry all four fields of endpoint data are filled. An incoming data packet will not match such an entry unless it is a TCP packet and all four endpoint data fields match corresponding fields in the packet header. For a TCP wildcard entry the destination IP address and destination port number are filled in and the remaining fields contain zeros. An incoming data packet will match a TCP wildcard entry if the incoming packet is a TCP packet and the two filled in endpoint fields of the entry match corresponding fields in the packet header regardless of the source IP address and port. For a UDP full entry all four fields of endpoint data are filled. An incoming data packet will not match such an entry unless it is a UDP packet and all four endpoint data fields match corresponding fields in the packet header. For a UDP wildcard entry like the TCP wildcard entry only the destination IP address and ports are filled and the remaining fields contain zeros. The UDP wildcard entry format differs from that of the TCP wildcard entry format since the destination port number is located in the field in which for the TCP full entry contains the source port number. An incoming data packet will match a UDP wildcard entry if the incoming packet is a UDP packet and the two filled in endpoint data fields of the entry match corresponding fields in the packet header again regardless of the source IP address and port.

It will be appreciated that another implementation might support different table entry formats different protocols and or different entry types. As one example entries may contain other kinds of numeric range indications rather than a wildcard for the entire field. As another example particular bits of an address or port field may be wildcarded out such as the low order bits of an IPv6 address . As can be seen as used herein therefore the matching of an entry to particular header information does not necessarily require complete equality. It is sufficient that it satisfies all conditions specified for a match.

All invalid entries in the table are written with all zeros. Obsolete entries are removed tombstoned at least in the first instance by marking them with all zeros as well.

Although all four entry types co exist in the same filter table separate searches are performed through the table for each type. If the incoming data packet is a UDP packet then a search is performed first for a matching UDP full entry and if that fails then for a matching UDP wildcard entry. If the incoming data packet is a TCP packet then a search is performed first for a matching TCP full entry and if that fails then for a matching TCP wildcard entry. All four kinds of searches are hashed searches described in detail below. It will be seen that these searches will proceed through only a limited number of iterations before aborting. In one embodiment the search limits for all four types of entries are the same but in the present embodiment they can be different and can be adjusted dynamically.

Accordingly in step the kernel driver initialization routine programs into the NIC the initial search limits for each of the four types of filter table entries. In step the driver routine returns to the kernel.

In a step when the application first starts up its libraries are initialized. This includes the user level transport library which is initialized into the application s virtual address space.

Step begins an example sequence of steps in which the application process uses a UDP transport protocol. In step the application makes a call to the socket routine of the user level transport library specifying that it would like a UDP socket. In step the application process calls the bind routine of the user level transport library in order to bind the socket to a port. In step the application process makes a call to the recvfrom routine of the user level transport library specifying the socket handle the receive buffer the buffer length and the source IP address and port from which an incoming packet is expected. As described below the recvfrom routine of the User Level Transport Library among other things writes an appropriate UDP full type tuple into the receive filter table on the NIC and in the host . If the application wishes to receive packets from any IP address and Port these latter two fields may contain zeros. In this case the recvfrom routine of the User Level Transport Library will write an appropriate UDP wildcard type tuple into the receive filter table on the NIC and in the host . Note that in the unicast case reel from listen and connect are typically the first calls made by an application process that request the establishment of a filter table entry. In the multicast case the first such call is generally a setSockOpt call with the add membership option.

After the recvfrom call the application blocks until a packet satisfying the specified criteria comes up from the NIC or from the kernel driver . At that time in step the application processes the received data and returns in this example to step to await the next incoming data packet.

Steps and are repeated many times most likely interspersed with many other functions performed by the application process. When the application has finished with the socket that it had created in step then in step the application makes a call to the close routine of the user level transport library in order to close the socket.

Alternatively to the UDP sequence beginning with step step begins an example sequence of steps in which the application process uses a TCP transport protocol. In step instead of calling the socket routine of the user level transport library to specify the UDP protocol it calls the socket routine to specify the TCP protocol. In step the application process calls the bind routine similarly to step in order to bind the socket to a port. In step since the transport protocol is now TCP the application process calls the listen routine of the user level transport library in order to listen for an incoming TCP connection request. Alternatively in step the application process may call the accept routine of the user level transport library . Both the listen and accept routines invoke the kernel to write into the receive filter table on the NIC and in the host a new IP tuple of type TCP wildcard so that any incoming TCP connection request SYN having the appropriate destination IP address and port number in its header will be sent up to the present application process for handling.

In step the application process makes a call to the recv function of the user level transport library specifying a socket handle the buffer into which data should be delivered and the buffer length. At this point the application blocks until a packet satisfying the specified criteria comes up from the NIC or from the kernel driver . At that time in step new incoming data arrives in the buffer and the application processes it. The application then may return to the recv step to await the next incoming data packet. Again many other steps may take place in between those illustrated in the flow chart. In step the application terminates or the connection may be terminated at which time the application makes a call to the close routine of the user level transport library in order to close the socket.

Note that for TCP connections initiated by the present application process instead of calling the listen routine in step typically the application will make a call to the connect routine of the User Level Transport Library specifying the socket handle and the destination IP address and port number. The connect routine of the User Level Transport Library will among other things invoke the kernel driver to write into the receive filter table on the NIC and the host a new IP tuple of type TCP full so that only those incoming TCP packets having four matching endpoint data fields will be sent up into the present application process.

In step as part of the initialization of the user level transport library a resource allocation routine in the kernel driver is invoked. The kernel level routine is required for allocating resources in the network interface card and the host memory subsystem since these resources are outside the virtual address space of the application or involve direct hardware accesses that advisedly are restricted to kernel processes. After resource allocation the user level driver initialization routine may perform a number of other steps before it returns to the application in step .

In step the kernel routine allocates a minimum set of the buffers for each of the transmit receive and event queues requested and programs their buffer IDs into the transmit receive and event queue descriptor tables and . In step the kernel routine determines the doorbell address in the NIC for each of the transmit and receive queues and maps them as well into the application s virtual address space. The doorbell address is the address to which the user level transport library will write a value in order to notify the NIC either that a transmit buffer is ready or that a receive buffer can be released. For transmit queues the doorbell address is the address of the device centric transmit queue read pointer in the transmit queue descriptor table entry for the particular transmit queue. For receive queues the doorbell address is the address of the device centric receive queue write pointer in the receive queue descriptor table entry for the particular receive queue.

In step the resource allocation routine returns to the application with handles for the resources allocated with the base virtual addresses of the transmit receive and event queues and virtual memory addresses corresponding to the doorbells allocated in the transmit and receive queue descriptor tables and .

Note that the user level recv routine is in pertinent part the same as the recvFrom routine of except that since the filter has already been applied steps and are omitted.

The user level accept routine is in pertinent part the same as the listen routine of except that the routine will more likely program the new TCP full entry into the filter table because the application is more likely to follow up with a fork into a new address space.

Note that the User Level Transport Library routines that invoke the kernel to set up a new filter also maintain a local copy of the filters that they already had set up. In this way they can avoid the context switch to the kernel to duplicate work that has already been performed. This local copy also avoids duplication in the multicast case since in that case by the time of a recv or recvFrom call the filter usually has already been established in response to a setSockOpt add membership call as in step .

If the option is anything other than ADD MEMBERSHIP or DROP MEMBERSHIP then the routine proceeds to handle that option as appropriate in step . Thereafter the routine terminates and returns to the calling application process step .

Before describing the CreateFilterEntry and RemoveFilterEntry kernel routines it will be helpful to describe generally several different approaches for supporting multicast reception in the environment of user level receive queues. In a first approach whereas incoming unicast packets of supported protocols are delivered by the NIC directly to the user level endpoint incoming multicast packets are not. When an application process makes a call that involves the establishment of a unicast filter the kernel CreateFilterEntry routine establishes the filter either on the NIC or in an internal software redirect table as appropriate. But when an application process makes a call that involves the establishment of a multicast filter such as a request to join a multicast group the kernel CreateFilterEntry routine recognizes this and creates the filter in its own multicast group membership tables.

In a second approach the NIC maintains a receive filter table that supports direct delivery of incoming data packets to one user level endpoint. Unicast filters are maintained in the NIC s filter table as are multicast filters for multicast groups having only one endpoint member running in the host. The NIC need not know whether a particular filter is for unicast or multicast reception. Instead in response to a user level request to join a multicast group the kernel or another process running in the host programs the multicast filter into the NIC only if the NIC does not already have a filter for that multicast group. Thus if a request to join a multicast group is the first such request for the specified group then the kernel programs a filter into the NIC receive filter table just as it programs a unicast filter. The only difference is that the destination IP address specified in the filter is a multicast group IP address rather than a unicast destination IP address. If the request to join the multicast group is the second or subsequent such request for the same multicast group then the filter is established in the kernel multicast group membership table instead.

In a third approach basically a generalization of the second a NIC has limited capacity to maintain multicast group membership information. For example the NIC might support multicast group membership of only some predetermined number N endpoints or it might support multicast group membership as long as the total number of endpoint memberships within the host system including all multicast groups does not exceed some maximum number M. In these approaches when an application process makes a system call that involves the establishment of a filter for the delivery of incoming multicast data packets then the kernel first determines whether the NIC has sufficient capacity to add the requested filter. If so then the kernel establishes the requested filter in hardware on the NIC. Future incoming data packets matching the filter criteria will then be passed from the NIC directly to the user level driver potentially as well as being passed from the NIC directly to user level drivers of other processes as well but without requiring a context switch to the kernel. But if the NIC does not have sufficient capacity to add the requested filter then the kernel instead establishes the filter in its own tables and in a typical embodiment the kernel also moves existing filters from the NIC to the kernel tables as well. The NIC then delivers matching packets to the kernel for forwarding to the appropriate user level process es .

It should be noted that an endpoint and a user level process are not necessarily the same. One user level process may own more than one endpoint. In addition in the present embodiment each endpoint is implemented with a single corresponding receive queue for receipt of packets destined for the corresponding endpoint. Therefore identification of a receive queue also identifies a single endpoint in the present embodiment and also identifies a single process running in the host system. Similarly identification of an endpoint also identifies a single receive queue in the present embodiment and also a single process running in the host system. But identification of a process does not by itself also identify an endpoint and a receive queue unless the process owns exactly one endpoint and receive queue. If the process owns more than one endpoint then identification of the process is by itself insufficient to identify a single endpoint and receive queue. Given this terminology requests for a filter and requests to join a multicast group are made by processes not by endpoints but they specify by a socket handle in the present embodiment to which endpoint and receive queue the incoming packets should be delivered. Incoming packets are then delivered to the indicated endpoint by way of the corresponding receive queue and thereby to the process that owns the indicated endpoint. In most embodiments discussed herein a process has only one endpoint and therefore the terms process endpoint and receive queue are often used interchangeably. But in an embodiment in which a process owns more than one endpoint when it is stated herein that a packet is delivered to a particular process this phraseology does not require that the delivery be made through any particular one of the process s endpoints.

If the protocol is TCP or UDP then in step the routine performs a hashed search in the host receive filter table for an empty slot. For UDP filters a search of the UDP wildcard entries is performed if the ip src and srcport arguments are null. If either the ip src or srcport argument contains a value then a search of the UDP full entries is performed. Alternatively the API can include a separate parameter to indicate whether a full or wildcard search should be performed. For TCP filters a search of the TCP wildcard entries is performed if the ip src and srcport arguments are null. If either the ip src or srcport argument contains a value then a search of the TCP full entries is performed. In each case the search is bounded by the appropriate search limit for the TCP full TCP wildcard UDP full or UDP wildcard protocol as programmed in step . The search algorithm itself is unimportant for an understanding of the present invention except to note that in the present embodiment it is the same as that used by the NIC against the NIC copy of the filter table upon receipt of a unicast packet from the network. Another embodiment could use another kind of search algorithm such as a linear search or a CAM.

In step if an empty slot was found before the appropriate search limit was reached then the routine programs both the NIC and the host receive filter tables at the identified entry with the queue ID and the endpoint data as provided in step .

If the search limit was reached before an empty slot was found then the routine makes a decision whether to increase the search limit for the particular kind of entry step . If not then in step the routine simply sets up a software redirect for this set of endpoint data. The redirect is a data structure that the kernel driver consults on receipt of every packet to see whether the packet should be delivered to the kernel stack or a user transport managed endpoint. In one embodiment it is a separate table of filter entries structured similarly to the host receive filter table.

If the routine does decide to increase the search limit then in step the routine simply continues the hashed search of step until an empty slot is found. When one is found then in step the routine programs the NIC and host receive filter tables at the identified entry with the queue ID and endpoint data from step . Then in step the routine programs the new search limit for the particular type of entry into the NIC and then in the kernel. It will be appreciated that steps and may be reversed in sequence or combined so that the number of additional hops required to find an available entry can be taken into account in the decision of whether to increase the search limit.

If the maximum chain length would be reduced then in step the routine removes the identified entry from the table as well as all tombstones in the table or only those in the chain if the chain is very long compared to others in the table reprogramming the table to bring up into the previously tombstone the locations entries and that were previously father down in the search chain. Finally in step the routine programs the new smaller search limit for the particular kind of entry into the NIC and then makes it effective in the kernel as well. In an embodiment the routine may skip step in the event that the new search limit would be smaller than some predetermined minimum chain length for the particular kind of entry. Many other optimizations will be apparent.

As mentioned when packets arrive the filter table and logic unit first determines the queue ID of the receive queue into which the packet should be delivered. is a detail of the filter table and logic unit as implemented in the first approach. No particular accommodation is made in the NIC in this approach for multicast reception. The filter table itself is implemented as two sub tables and collectively . The software is unaware of this implementation detail and instead sees only a single table. The hardware in the NIC decodes the write address from the kernel driver software and places even numbered entries in filter sub table and odd numbered entries in filter sub table . Thus filter sub table contains entries 0 2 4 6 and so on whereas filter sub table contains entries 1 3 5 and so on. The implementation of the filter table as two sub tables enables parallel entry lookup per cycle to reduce the total lookup time. It will be appreciated that other implementations can use a single sub table or more than two sub tables.

Both of the filter sub tables and are addressed by a 13 bit index provided by filter table search logic . A13 bit index can address up to 8192 entries which for two sub tables comes to 16 384 entries numbered 0 through 16 383. Four index values are reserved for the kernel NET driver queues so only 16 380 entries are represented in the table. The filter table search logic receives the header data of the incoming data packet and uses it to derive a hash key then uses the hash key to derive a hash code which is the first 13 bit index to be searched in the filter table . The filter table search logic also calculates subsequent entries to search if the first entry in neither of the two filter sub tables matches that of the incoming header data and also forces the selection of a default kernel queue for delivery of the current data packet if the search limit is reached before a match is found. The filter table search logic also determines a match type TCP full TCP wildcard UDP full or UDP wildcard in dependence upon the header data and the state of the search algorithm.

The various formats for an entry in the filter table are set forth in the table above. As shown in the endpoint data from the selected entry of filter sub table is provided to one input of match logic and the endpoint data from the selected entry of filter sub table is provided to the corresponding input of match logic . The other input of each of the match logic units and collectively receives the header data of the incoming data packet. The match type is provided from the filter table search logic to both match logic units each of which then outputs a match signal to a hit logic unit . If the match type is TCP full then match logic units at will indicate a match only if the incoming packet type is TCP and all four fields of endpoint data match the corresponding fields of the incoming header data. If the match type is TCP wildcard then the match logic units will indicate a match if the incoming packet type is TCP and bits of the endpoint data in the table contains the same destination IP address and destination port as the incoming header data. The source IP address and source port as indicated in the header data are ignored. If the match type is UDP full then match logic units at will indicate a match only if the incoming packet type is UDP and all four fields of endpoint data match the corresponding fields of the incoming header data. If the match type is UDP wildcard then match logic units will indicate a match if the incoming packet type is UDP and bits of the filter endpoint data contain the same destination IP address and bits of the endpoint data contain the same destination port number as indicated in the header data of the incoming packet.

If either match logic unit or indicates a match then hit logic so notifies the filter table search logic . The Q ID queue ID fields of the currently selected entries in both filter sub tables are provided to two of three inputs of a multiplexer and hit logic provides a select input so that the multiplexer will select the queue ID from the currently selected entry of the correct filter sub table or . As mentioned if no matching entry has been found after the search limit has been reached then the filter table search logic provides a signal to the multiplexer to select to the default queue ID provided on a third input of the multiplexer . Note that in the first approach this will always be the case for received multicast data packets since no filter is ever established in the NIC filter table which matches the header information of incoming multicast data packets. The default queue ID in one embodiment is queue 0 which is defined to be a kernel queue. In other embodiments the default queue ID can be programmable. In any event whether or not a match has been found the multiplexer outputs the queue ID indicating the particular receive queue to which the NIC should deliver the incoming data packet.

As previously mentioned if the incoming data packet is mal formed or uses a protocol that is not supported in the filter table or if it uses the supported protocol but a match was not found in the filter table before the appropriate search limit s was were reached then the NIC will deliver the incoming data packet to a receive queue of the kernel driver . All incoming multicast packets will be so delivered in the first approach since no filter matching the header information of any incoming multicast data packet is ever established in the NIC filter table . is a flow chart showing pertinent steps that the kernel driver performs upon receipt of such a data packet.

Initially in step the kernel determines from the destination IP address whether the incoming data packet is a unicast or multicast packet. If it is a unicast packet then in step the kernel routine determines whether the packet uses the TCP or UDP protocol. If not then in step the kernel driver processes the packet in whatever manner is appropriate for the particular packet format. If the incoming data packet does use TCP or UDP then in step the kernel driver performs a hashed search with no search limit imposed of the software redirect table. In step if no match was found then the kernel driver simply delivers the packet to a normal kernel network stack step . If a match was found then in step the kernel driver delivers the packet to the proper user level receive process. In order to avoid contention with the NIC attempting to deliver its own data packets to receive queues in an application s transport library the delivery of the packet from the kernel driver to the user level transport library occurs by some communication channel other than through the use of the receive queue. Typical standard operating system mechanisms can be used to notify the user level driver of the availability of this packet.

If in step the kernel determines that the incoming data packet is a multicast packet then in step the routine switches on the multicast packet type. If the packet is an IGMP host membership query then the kernel processes the query in the same manner as set forth above with respect to . If the packet is a special multicast message other than an IGMP host membership query then it is handled as appropriate in step . If the packet is a multicast data packet then the kernel processes it in a manner similar to that described above with respect to . In particular in step and the kernel searches its table for the interface at which the packet was received for memberships in the multicast group indicated by the destination IP address and port number in the packet header. The table may allow full or partial wildcards for the destination port number. If no memberships are found step then the routine quietly ignores the packet step . If at least one is found then in step the routine proceeds to deliver the packet to the user level process of each socket in the linked list pointed to by the table entry at which the group membership was found. As with unicast packets routed by means of the kernel s software redirect table standard operating system interprocess communication mechanisms rather than the receive queues are used to notify the user level driver of the availability of this packet.

It can be seen that even though the NIC in this first approach implements no particular accommodations for delivering incoming multicast data packets directly to user level processes multicast reception is still possible in an environment in which the NIC does deliver many unicast data packets directly to user level processes. This is made possible in part by the determination made by the kernel in its CreateFilterEntry routine to program a requested filter into the NIC only if the filter applies to unicast packets. More generally multicast reception is made possible in an environment in which the NIC delivers many unicast data packets directly to user level processes in the first approach because the kernel when it receives a request to deliver future incoming data packets having particular packet header properties to a particular user level endpoint first determines from the specified properties whether they are of a character that permits a second or subsequent user level process to make a similar request to deliver the same future incoming data packets also to the second or subsequent user level endpoint. The kernel then programs the NIC with a filter for the specified packet header properties but only if they are not of such a character and assuming of course that all other restrictions are met as well . If the specified packet header properties are of a character that permits a second or subsequent user level process to make a similar request to deliver the same future incoming data packets also to the second or subsequent user level endpoint for example if the specified packet header properties indicate multicast packets then the kernel may instead establish the filter in its own internal tables. An incoming packet matching the specified header properties therefore will not match any entry in the NIC s filter tables and thus will be delivered by default to the kernel to deliver in accordance with the filter it had established in the kernel s internal tables.

In the second approach to multicast reception the NIC also implements no particular accommodations specifically for delivering incoming multicast data packets directly to user level processes. However through special treatment of user level requests to join multicast groups the NIC can be programmed to do so as long as no more than one user level endpoint is a member of any given multicast group. In particular if a request to join a multicast group is the first such request for the specified group then the kernel programs a filter into the NIC receive filter table just as it programs a unicast filter. The only difference is that the destination IP address specified in the filter is a multicast group IP address rather than a unicast destination IP address a difference of which the NIC need not be aware. If the request to join the multicast group is the second or subsequent such request for the same multicast group then the filter is established in the kernel multicast group membership table instead.

For the first approach the kernel receive filter table is at least functionally the same as that of the NIC. For the second approach some modifications can be implemented as shown in Table II 

In an alternative embodiment of the second approach the kernel receive filter is the same as that set forth above with respect to the first approach the same as the NIC . Instead of a modified table the kernel implements a second table for multicast filters separate from the kernel receive filter table for unicast filters. The table for multicast filters has the format of Table III 

With this second approach the user level setSockOpt routine operates as described above with respect to . is a flowchart of pertinent steps of the kernel s CreateFilterEntry routine for implementing the second approach. In step the routine first looks up the queue ID from the socket handle specified. In step the routine determines from the specified destination IP address whether the requested filter is for unicast or multicast reception. If the request is for a unicast filter then in step similarly to step the kernel establishes this filter in the NIC if possible or in the kernel s own unicast software redirect filter table if not.

If the request is for a multicast filter then unlike in the first approach the routine first determines whether a linked list for the indicated multicast group already exists in the kernel s multicast group membership tables step . If so then the kernel simply adds the new receive queue ID into the associated linked list step . If not then in step the routine determines whether a filter for the specified multicast group IP address already exists as a unicast filter either in the NIC receive filter table or in the kernel s unicast software redirect table. The kernel can determine the former by looking in its own host centric copy of the NIC s receive filter table. If a filter does not already exist in the unicast filter tables then the current request is the first request from an application process to join the indicated multicast group. In step the routine establishes the filter just as if it were a unicast filter. The procedure to accomplish this is that same as that set forth above with respect to even though the destination IP address is a multicast group address rather than a unicast IP address. Assuming the protocol of the multicast group is UDP which is the case for many multicast groups the filter will be established either in the NIC step or in the kernel s software redirect table step . An appropriate IGMP host membership report may also be transmitted not shown in .

If in step the routine determines that a filter does already exist in the unicast filter tables for the specified multicast group IP address then the current request is the second request from a user level application process to join the same multicast group. To satisfy the second request the routine would have to program a second destination queue ID into the NIC filter table for incoming data packets having the same header characteristics as already programmed for the first filter. But since the NIC in this second approach is designed for unicast reception it does not have the structure to deliver an incoming data packet to more than one receive queue. Accordingly in steps the kernel moves the existing filter into the kernel s tables and adds the new receive queue ID into the associated linked list. In particular in step the kernel allocates a linked list for the current multicast group establishes an entry in the kernel s software redirect table of the appropriate multicast format set forth above in Table II sets the multicast bit in that entry and writes a pointer into the queue ID field of the entry to point to the newly created linked list. In step the kernel removes the existing filter from the unicast filter tables and in step adds the queue ID from that entry into the newly created linked list. The routine then continues at step for adding the queue ID of the current createFilterEntry call into the newly created linked list as well.

In this embodiment of the second approach the NIC filter table is allowed to contain only one entry for each multicast group IP address. If the embodiment supports full or partial wildcarding in the destination port field of the NIC receive filter table then multicast filters with wildcards of a type supported can be established in the NIC filter table as well. If the embodiment does not support wildcarding in the destination port field of the NIC receive filter table or does not support the type of wildcarding requested by the application then the filter is established in the kernel and not the NIC. Once a filter is established in the NIC filter table for a particular multicast group IP address if a new request is received for a different endpoint to join the same multicast group even if a different destination IP port is specified then the entire multicast group filter is moved to the kernel. In another embodiment the kernel allows the establishment of multiple filters in the NIC filter table for the same multicast group as long as the specified destination port numbers are different. If wildcarding is supported in the destination port field of the NIC receive filter table then the kernel can still allow the establishment of multiple filters in the NIC filter table for the same multicast group as long as the specified destination port number ranges do not overlap.

If the filter to be removed is not located in the unicast filter tables then it is a multicast filter located in the kernel s multicast group membership tables. Thus in step the routine removes the queue ID of the specified socket from the kernel s linked list associated with the multicast group ID indicated by the destination IP address field of the filter. In step the routine determines whether this linked list now has only one member. If not then the routine returns to the caller step . If there is only one destination queue ID remaining in the linked list associated with the current multicast group then in the filter for that destination process is now moved to the unicast filter tables. In particular in step similarly to step the kernel establishes a corresponding unicast filter in the NIC if possible or in the kernel s own unicast software redirect filter table if not a unicast filter for delivery of incoming multicast packets of the multicast group to the queue ID specified in the one remaining entry of the linked list. The procedure to accomplish this is that same as that set forth above with respect to even though the destination IP address is a multicast group address. In step the routine removes the one remaining entry of the linked list and deletes the list. In step the routine removes the kernel software redirect that it had previously set up in step for this multicast group. The routine then returns to the caller step .

As previously mentioned in this second approach the NIC makes no distinction between incoming unicast and incoming multicast data packets. In both cases the NIC first searches its own receive filter table for a match and if it finds one before the appropriate search limit is reached then the NIC will deliver the incoming data packet to the identified receive queue. This mechanism bypasses the kernel stack both for unicast packets and if exactly one user level endpoint in the host system is then a member of a multicast group for multicast packets of that multicast group as well. On the other hand if the incoming data packet is mal formed or uses a protocol that is not supported in the filter table or if it uses a supported protocol but a match was not found in the filter table before the appropriate search limit is reached then the NIC will deliver the incoming data packet to a receive queue of the kernel driver . Again these can be unicast or multicast packets in this second approach.

It can be seen that in this second approach not only incoming unicast packets but also incoming multicast packets in certain circumstances will be delivered by the NIC directly to the user level driver without requiring a context switch to the kernel. Direct delivery of multicast packets in this second approach requires that exactly one user level endpoint within the host system be a member of the multicast group or in an embodiment the multicast group and destination port combination to which the packet belongs.

The third approach for supporting multicast reception is a generalization of the second. In the third approach the NIC has the capacity to deliver incoming multicast data packets directly to user level processes but as in most things there is some limit on its capacity to do so. In one embodiment the limit is very large so that effectively it does not exist and no steps need be taken to protect against exceeding it. In another embodiment the limit is more stringent. In this third approach the kernel driver takes advantage of whatever capacity exists whenever it can. The second approach is a special case of the third in which the NIC s capacity to directly deliver incoming multicast data packets is limited to a single destination receive queue. If a user level process requests membership in a particular multicast group the kernel createFilterEntry routine will create the filter in the NIC if possible i.e. if one is not already there and if search limits are not violated or will create it in the kernel s own internal tables if the NIC s capacity of one destination receive queue would be exceeded.

In another embodiment of the third approach the NIC has capacity to store a fixed number N for example N 2 destination queue ID s for each multicast group. In one implementation of the this embodiment the NIC receive filter table is widened to include N fields rather than only one field for storing queue ID s of destination receive queues. illustrates such a filter table structure. The N fields are designated through N. In a implementation if an incoming data packet has header information matching the filter criteria of a particular filter then the NIC delivers the packet to each valid queue ID specified in the filter. As in the second approach no distinction is made by the NIC between incoming unicast and incoming multicast data packets the only difference is that a filter for unicast packets will never contain more than one valid queue ID whereas a filter for multicast packets may. A disadvantage of the implementation is that in most computer systems since the great majority of receive filters will be unicast filters the great majority of queue ID fields in the NIC receive filter table will remain unused.

The three implementations above for the third approach should be seen only as examples. Depending on the embodiment example implementations may or may not support wildcarding in the destination port number field of multicast filter entries. Yet other implementations can be constructed from hybrids of those in and . For example the primary filter table may have fields for NO destination queue IDs and then makes reference to a secondary table like or only for endpoints beyond the first N0. In yet another implementation the overspill list is stored in host memory rather than in NIC based SRAM. This reduces the need for dedicated resources on the NIC which may be rarely used. The reader will appreciate that many other implementations are possible.

If the request is for a multicast filter then the routine first determines whether a linked list for the indicated multicast group already exists in the kernel s multicast group membership tables step . If so then membership for this group has already exceeded the NIC s capacity to deliver packets of this group directly or there is some other reason why the NIC cannot support direct delivery of packets of this multicast group. The kernel therefore simply adds the new receive queue ID into the kernel s associated linked list step . If no linked list for the indicated multicast group already exists in the kernel s multicast group membership tables then in step the routine determines whether a filter for the specified multicast group IP address already exists in the NIC filter tables. The kernel can determine this by looking in its own host centric copy of the NIC s receive filter table.

If the NIC does not already have a filter for the indicated multicast group then the current request is the first request from an application process to join the indicated multicast group. Therefore in step the routine establishes a filter for the indicated multicast group in the NIC and in step the routine adds the indicated queue ID into the NIC s delivery list for this multicast group. The routine also transmits an appropriate IGMP host membership report not shown in . In an implementation like step might be the same as establishing a unicast filter and step might involve inserting the indicated queue ID into the first of the N delivery destination fields of the filter entry. In an implementation like or step might involve setting up the filter in the NIC s primary receive filter table and inserting a pointer to the NIC s secondary filter table and step might involve inserting the indicated queue ID into an available entry in the particular secondary table or into the particular linked list in the secondary table .

If in step the routine determines that a filter does already exist in the NIC for the specified multicast group IP address then the current request is the second or subsequent request from a user level application process to join the same multicast group. The NIC may or may not have capacity to add the specified queue ID to the delivery list for this multicast group. In step therefore the kernel determines whether the NIC is already at its maximum capacity for the indicated multicast group. In implementations like this might involve determining whether the NIC is already storing N queue IDs for the particular group. In an implementation like this might involve determining whether the NIC is already storing M queue IDs over all the multicast groups. Note that this determination is in the present embodiment made by the kernel. Obviously it is preferred that this determination always be accurate but it will be appreciated that circumstances may arise in which the kernel s view of the NIC s current capacity differs from the NIC s actual current capacity. As used herein therefore the making of a determination requires merely that a determination be made. It does not require that the determination be correct.

If the NIC is determined to be not yet at maximum capacity then the kernel adds the indicated queue ID into the NIC s list for the indicated multicast group step . If it is then in steps the kernel moves the existing destination list from the NIC into the kernel s tables and adds the new receive queue ID into the associated linked list. In particular in step the kernel allocates a linked list for the current multicast group establishes an entry in the kernel s software redirect table of the appropriate multicast format set forth above in Table II sets the multicast bit in that entry and writes a pointer into the queue ID field of the entry to point to the newly created linked list. In step the kernel moves all the queue IDs listed in the NIC for this multicast group from the NIC to the newly created linked list. In step the kernel removes the existing filter from the NIC s filter table or from the NIC s primary filter table in the implementations of . The routine then continues at step for adding the queue ID of the current createFilterEntry call into the newly created kernel linked list as well. Note that in another embodiment the existing filter might not be removed from the NIC s tables and moved to the kernel. Instead the embodiment might leave all existing filters intact in the NIC and merely add a flag indication that the packet should also be delivered to the kernel for possible delivery to additional destination receive queues. In yet another embodiment the embodiment might move only the last filter pre existing filter from the NIC to the kernel replacing it in the NIC with an indication that the packet should also be delivered to the kernel for possible delivery to additional destination receive queues. Many other variations will be apparent to the reader.

If the filter was for multicast reception then in step the routine determines whether it is located in the NIC filter tables. If so then in step the kernel removes the subject queue ID from the NIC s multicast delivery list for the particular multicast group. This can involve invalidating the queue ID s field in an implementation like invalidating the queue ID s entry in the secondary filter table for this multicast group in an implementation like or unlinking the queue ID s entry in the secondary filter table in an implementation like and adding it to the free list. In step the routine determines whether the NIC s delivery list for the particular group has any remaining queue ID s. If so then step is performed for also removing the main filter entry for this multicast group from the primary filter table. Again the routine described above with respect to can be used for this purpose. Otherwise the routine terminates step without any further removal of entries from the NIC tables.

If the filter to be removed is not located in the NIC filter tables then it is located in the kernel s multicast group membership tables. Thus in step the routine removes the queue ID of the specified socket from the kernel s linked list associated with the multicast group ID indicated by the destination IP address field of the filter. In step the routine determines whether this linked list now contains few enough members to be within the capacity of the NIC to deliver packets of the particular multicast group to all members directly. If not then the no further changes are made to filter tables and the routine returns to the caller step . If the kernel s linked list does now contain few enough members then the kernel moves the existing destination list from the kernel s tables into the NIC. In particular in step the kernel establishes the main filter entry for the multicast group in the NIC s primary receive filter table and copies in all the remaining queue IDs from the kernel s delivery list. In step the kernel removes the remaining queue IDs from the kernel s linked list for this multicast group and deletes the list. In step the routine removes the kernel software redirect that it had previously set up for this multicast group. The routine then returns to the caller step .

As used herein the identification of an item of information does not necessarily require the direct specification of that item of information. Information can be identified in a field by simply referring to the actual information through one or more layers of indirection or by identifying one or more items of different information which are together sufficient to determine the actual item of information. In addition the term indicate is used herein to mean the same as identify .

Additionally as used herein a given signal event or value is responsive to a predecessor signal event or value if the predecessor signal event or value influenced the given signal event or value. If there is an intervening processing element step or time period the given signal event or value can still be responsive to the predecessor signal event or value. If the intervening processing element or step combines more than one signal event or value the signal output of the processing element or step is considered responsive to each of the signal event or value inputs. If the given signal event or value is the same as the predecessor signal event or value this is merely a degenerate case in which the given signal event or value is still considered to be responsive to the predecessor signal event or value. Dependency of a given signal event or value upon another signal event or value is defined similarly.

The foregoing description of preferred embodiments of the present invention has been provided for the purposes of illustration and description. It is not intended to be exhaustive or to limit the invention to the precise forms disclosed. Obviously many modifications and variations will be apparent to practitioners skilled in this art. As an example whereas in the embodiments described herein it is the header fields of an incoming packet which are compared to fields in the filter table to detect a matching filter table entry in another embodiment other aspects of the content of the incoming packet can be compared instead. As another example whereas the filter table in the NIC in the embodiments described herein have a tabular format it will be appreciated that a table is only one possible format for what is more generally known as a database. Another embodiment might implement a filter database having a different structure that need not be tabular. The embodiments were chosen and described in order to best explain the principles of the invention and its practical application thereby enabling others skilled in the art to understand the invention for various embodiments and with various modifications as are suited to the particular use contemplated. It is intended that the scope of the invention be defined by the following claims and their equivalents.

