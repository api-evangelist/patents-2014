---

title: Storage gateway activation process
abstract: Methods, apparatus, and computer-accessible storage media for activating a gateway to a remote service provider. The gateway serves as an interface between processes on a customer network and the provider, for example to store customer data to a remote data store. A gateway sends a public key and metadata describing the gateway to the provider. The gateway receives an activation key from the provider and exposes the activation key on the customer network. The customer obtains the key and communicates to the provider using the key to provide customer information including a name for the gateway and to authorize registration of the gateway. The provider provides the customer information to the gateway. The gateway requests security credentials from the provider using the customer information and the key. The provider sends a security credential to the gateway. The gateway may then obtain configuration information from the customer via the provider.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09225697&OS=09225697&RS=09225697
owner: Amazon Technologies, Inc.
number: 09225697
owner_city: Reno
owner_country: US
publication_date: 20140808
---
This application is a continuation of U.S. application Ser. No. 13 174 513 filed Jun. 30 2011 now U.S. Pat. No. 8 806 588 which is hereby incorporated by reference in its entirety.

Many companies and other organizations operate computer networks that interconnect numerous computing systems to support their operations such as with the computing systems being co located e.g. as part of a local network or instead located in multiple distinct geographical locations e.g. connected via one or more private or public intermediate networks . For example data centers housing significant numbers of interconnected computing systems have become commonplace such as private data centers that are operated by and on behalf of a single organization and public data centers that are operated by entities as businesses to provide computing resources to customers. Some public data center operators provide network access power and secure installation facilities for hardware owned by various customers while other public data center operators provide full service facilities that also include hardware resources made available for use by their customers. However as the scale and scope of typical data centers has increased the tasks of provisioning administering and managing the physical computing resources have become increasingly complicated.

The advent of virtualization technologies for commodity hardware has provided benefits with respect to managing large scale computing resources for many customers with diverse needs allowing various computing resources to be efficiently and securely shared by multiple customers. For example virtualization technologies may allow a single physical computing machine to be shared among multiple users by providing each user with one or more virtual machines hosted by the single physical computing machine with each such virtual machine being a software simulation acting as a distinct logical computing system that provides users with the illusion that they are the sole operators and administrators of a given hardware computing resource while also providing application isolation and security among the various virtual machines. Furthermore some virtualization technologies are capable of providing virtual resources that span two or more physical resources such as a single virtual machine with multiple virtual processors that spans multiple distinct physical computing systems.

As another example virtualization technologies may allow data storage hardware to be shared among multiple users by providing each user with a virtualized data store which may be distributed across multiple data storage devices with each such virtualized data store acting as a distinct logical data store that provides users with the illusion that they are the sole operators and administrators of the data storage resources.

The conventional Web model allows clients to access Web resources e.g. applications services and data via an HTTP client program such as a Web browser. A technology referred to as Web services has been developed to provide programmatic access to Web resources. Web services may be used to provide programmatic access to Web resources including technology platforms e.g. applications and services and data e.g. product catalogs and other databases hosted on Web connected computers such as Web server systems via a Web service interface. Generally speaking a Web service interface may be configured to provide a standard cross platform API Application Programming Interface for communication between a client requesting some service to be performed and the service provider. In some implementations a Web service interface may be configured to support the exchange of documents or messages including information describing the service request and response to that request. Such documents or messages may be exchanged using standardized Web protocols such as the Hypertext Transfer Protocol HTTP for example and may be formatted in a platform independent data format such as eXtensible Markup Language XML for example.

While embodiments are described herein by way of example for several embodiments and illustrative drawings those skilled in the art will recognize that embodiments are not limited to the embodiments or drawings described. It should be understood that the drawings and detailed description thereto are not intended to limit embodiments to the particular form disclosed but on the contrary the intention is to cover all modifications equivalents and alternatives falling within the spirit and scope as defined by the appended claims. The headings used herein are for organizational purposes only and are not meant to be used to limit the scope of the description or the claims. As used throughout this application the word may is used in a permissive sense i.e. meaning having the potential to rather than the mandatory sense i.e. meaning must . Similarly the words include including and includes mean including but not limited to.

Various embodiments of methods apparatus and computer accessible storage media for providing a local gateway to remote storage are described. Embodiments of a storage gateway are described herein in the context of a service provider that provides over an intermediate network such as the Internet a storage service to one or more customers of the service provider. The storage gateway may be implemented as a virtual or physical appliance that is installed on premise at a customer s data center and that acts as a gateway between the customer s data center and the storage service. The storage gateway may be configured as an interface to and local cache for a primary storage provided remotely via the storage service and or as an interface that shadows primary storage implemented on the customer s network to remote storage provided by the storage service. The storage gateway may present standard data access interfaces to the customer s applications at the front end of the appliance convert the data accesses into storage service requests at the back end of the appliance and transfer the data over the network to the storage service according to the storage service interface. In at least some embodiments the storage service interface may be implemented as a Web service interface.

Embodiments of the storage gateway may provide an on premise interface to virtually unlimited flexible scalable remote storage provided via the storage service. The storage gateway may provide a cost effective flexible and more easily scalable alternative to conventional on premise storage solutions. While the cost of storage devices may be decreasing the administrative and other hardware and software costs of conventional on premise storage solutions have remained relatively constant or in some cases increased. Embodiments of the storage gateway may allow customers of a service provider to lower the total cost of storage ownership passing at least some administrative and other costs to the service provider.

In at least some embodiments the storage service may store the customer s data in the remote data store according to block storage technology. In at least some embodiments the storage gateway may expose block storage protocols e.g. iSCSI GNBD Global Network Block Device etc. file storage protocols e.g. NFS Network File Storage CIFS Common Internet File System etc. and or object storage protocols e.g. REST Representational State Transfer at the front end to the customer s applications. A block storage protocol such as iSCSI enables direct access to the underlying data blocks of the remote data store.

Files written by an application to a remote data store via file storage protocols such as NFS or CIFS exposed by the storage gateway may be stored to the remote data store according to block storage technology. Through an exposed file storage protocol such as NFS and CIFS the storage gateway presents the customer s data stored in the remote data store according to block storage technology to the customer s applications as files before they are transmitted from the gateway over the customer network to the customer s applications. The exposed block storage protocol e.g. iSCSI transfers the blocks to the customer s applications thus requiring the application to handle interpretation of the data blocks into whatever format the application expects.

A block storage protocol such as iSCSI is a low level block storage protocol and thus may enable a wider range of use cases than file storage protocols such as NFS and CIFS. A block storage protocol may enable support for applications that typically write to a block store such as Microsoft SharePoint and Oracle databases and may also be configured to provide underlying storage for CIFS or NFS file servers. Thus in at least some embodiments of the storage gateway a block storage protocol such as iSCSI may be employed as the exposed interface to customer applications.

A customer of the service provider may be referred to herein as a service customer or simply customer and may be any entity that implements a computer network or networks coupled to an intermediate network such as the Internet to provide networked computing services to one or more users on a local network or network including one or more services remotely provided by service provider . A service customer may be a business enterprise an educational entity a government entity or in general any entity that implements a computer network or networks that provide networked computing services to users. While shows a single client network there may be multiple client networks . Each client network may correspond to a different service customer or two or more client networks may correspond to different data centers or localities of the same service customer for example different regional offices of a business enterprise or different campuses of a school system. In at least some embodiments each customer of the service provider may have an account with the service provider and may be provided with security credentials e.g. an account name and or identifier password etc. via which one or more customer representatives e.g. a client network administrator may log in to interfaces e.g. Web pages to the service provider to manage the customer s resources provided by one or more services including but not limited to a storage service offered by the service provider .

Embodiments of storage gateway may be implemented in hardware software or a combination thereof. In at least some embodiments storage gateway may be implemented as a virtual appliance that may for example execute within a virtual machine instantiated on a host system. In at least some embodiments storage gateway may be implemented as a virtual appliance that may be downloaded or otherwise installed activated and configured on one or more computing devices such as server systems coupled to a local network infrastructure at a service customer s data center e.g. client network . Alternatively storage gateway may be implemented as a dedicated device or appliance that may be coupled to a local network infrastructure at a service customer s data center e.g. client network the dedicated device or appliance may include software and or hardware that implements the functionality of the storage gateway . illustrates an example computer system on which embodiments of a storage gateway may be implemented. In at least some implementations storage gateway communicates with the service provider network via an intermediate network e.g. the Internet through firewall technology. Note that the service provider network may also include front end technology e.g. firewall technology border router technology load balancer technology etc. through which network traffic from and to intermediate network passes.

At least some embodiments of the storage gateway may be implemented according to a security model that provides data protection for the customer as well as protection against misuse and unauthorized use e.g. pirating of the gateway by the customer or third parties. Communications between the storage gateway and the storage service may be secured and encrypted. An activation process is described later in this document in which a newly installed storage gateway initiates a connection with and is identified to the service provider network to obtain security credentials. In at least some embodiments during the activation process the customer logs into the customer s account with the service provider and provides information to the service provider that is used in registering the gateway . However the customer does not log in to the storage gateway and therefore the customer s security credentials and other account information are not exposed on the gateway . This may minimize the security risk for the customer.

In at least some embodiments an aspect of the security model is that the storage gateway only accepts externally initiated connections to one or more data ports e.g. iSCSI ports exposed to the customer process es on the client network . The storage gateway initiates all other connections to external processes external processes cannot initiate any other connections to the gateway. For example in at least some embodiments the storage gateway initiates gateway management and other connections to the service provider the service provider does not initiate connections to the gateway . As another example a client network s network administrator process cannot directly connect to the storage gateway to configure and manage the gateway . Instead configuration and management of the storage gateway by the network administrator process may be performed through the service provider for example via console process on the service provider network. Thus in at least some embodiments a user network manager or process e.g. network administrator process or customer process es on the client network cannot directly log in to the storage gateway nor can a user manager or process on the service provider network e.g. console process and storage service or on some other external network initiate a connection to the storage gateway . This helps protect the security credentials and other operational information on the storage gateway from being intentionally or unintentionally compromised by persons or processes on the client network or by external persons or processes.

Embodiments of the storage gateway may be installed activated and configured for use with a storage service to provide one or more of several data store functionalities. For example a storage gateway may be installed activated configured and employed with a storage service to serve as 

Note that the file system gateway and the cloud volume gateway are similar in that both serve as gateways to a remote data store and both may locally cache data e.g. frequently and or recently used data. In both the file system gateway and the cloud volume gateway data reads from customer processes may be serviced from the local cache if possible or from the remote data store if not. In contrast in the shadowing gateway data reads are passed through the gateway to the customer s local data store. For the purposes of this document the file system gateway and cloud volume gateway may collectively be referred to as a cached gateway to distinguish these implementations from the shadowing gateway.

Block driver interfaces a customer process with the storage gateway . generally block driver allows a customer process to interact with the storage gateway e.g. via read write requests . Since the storage gateway is on site with the customer process from the perspective of the process it appears that data is stored locally. However the storage gateway interfaces with storage service to store the data to a remote data store provided by the storage service . For cached gateways the primary data store is remote data store while frequently accessed data may be locally cached by the gateway . Reads may be satisfied from the local cache or from virtual data storage writes are handled so as to appropriately update data blocks in the local cache and or in virtual data storage . For shadowing gateways the primary data store is local data store reads are passed through to local data store and writes are shadowed to virtual data storage as well as being sent to local data store .

Block driver intercepts read write requests from the customer process and passes the requests to the storage controller . In at least some embodiments block driver may provide a block storage protocol e.g. iSCSI or GMBD as an interface to the customer process . In some embodiments instead of or as an alternative to a block storage protocol interface block driver may provide a file storage protocol interface e.g. NFS or CIFS and may use file system semantics as an interface to the storage controller . Note that while shows one block driver there may be more than one block driver.

Storage controller acts as a mediator between block driver and storage via a cache manager . Responsibilities of storage controller may include forwarding read and write requests from block driver to storage and callbacks to block driver when storage responds with data. Block driver may also maintain statistics such as the number of requests in progress.

In at least some embodiments storage controller on one storage gateway may communicate with a cache manager on another storage gateway . In at least some embodiments each storage gateway may send heartbeat messages for discovery and detecting failures. A consistent hashing may be used to identify the storage gateway that is responsible for a given object and the request to get data may be forwarded to the cache manager on the target storage gateway . The cache manager may respond by invoking a callback provided by storage controller .

In cached gateway embodiments cache manager may manage a local cache that for example provides storage for frequently accessed data. Local cache may be implemented on internal volatile and or non volatile memory of storage gateway or alternatively may be implemented at least in part on an external local data store provided by the customer. In at least some embodiments the local cache represents data stored in the virtualzied data storage writes from a customer process may not directly affect the local cache .

In at least some embodiments employing multiple gateways a distributed local cache may be used and consistent hashing on keys may be used to identify the cache responsible for holding a given key. In at least some embodiments locality aware request distribution may be used to reduce communication between the gateways which may require additional load balancing.

All write requests to a given volume in the remote data store may go to a particular gateway node. Since all write requests for a volume are forwarded to a particular gateway node network partitioning may not be an issue.

In at least some embodiments the cache manager may include or may interface with a staging component. Staging may include or may have access to a write log . In at least some embodiments a data structure may be built over the write log and used as a metadata store . The metadata store may allow quick access to all writes to a particular block. The metadata store may for example be used in applying mutations to different segments within the block. When write data is received from the customer process the data is appended to the write log . Metadata for the write data relative to a block e.g. offset and length may be stored to the metadata store . In at least some embodiments write log may be implemented as a one dimensional data buffer implemented as either a linear or a circular queue. In at least some embodiments metadata store may be a key value store for example implemented as a Berkeley Database. Other implementations of both the write log and the metadata store may be used in some embodiments.

In cached gateway implementations when a read is performed the original block may be obtained from the local cache or from the remote data store and any pending mutations indicated by the write log may be applied before returning the data to the respective customer process .

In some embodiments if a gateway fails e.g. crashes in memory write data may be lost unless the data has already been written to the local data store . In some embodiments if there are multiple gateways at the customer site another gateway may take responsibility of keys owned by the crashed gateway restore writes from a snapshot on local data store if there are any and start accepting requests directed to the respective volume. In some embodiments a write log and or metadata store may be replicated over two or more gateways to provide redundancy and better durability. In case of failure of the gateway one of the other gateways may take over the failed gateway s write log and metadata store . However in at least some embodiments the metadata store may be maintained only on the owner gateway . In these embodiments in case of failure of the gateway one of the other gateways may take over and parse the primary write log to rebuild the metadata store .

In cached gateway implementations block fetcher fetches required segments of blocks from remote data store via storage service . In at least some embodiments block fetcher may employ a lazy fetching technique to fetch complete blocks for caching. For both cached gateways and shadowing gateways block store pushes data from staging to remote data store via storage service . In at least some embodiments block store may employ a lazy pushing technique to push the blocks.

In at least some embodiments during read operations for cached gateways block driver sends the read request including a volume ID start offset and length to storage controller . In at least some embodiments storage controller may translate the volume ID and offset to an object key. Storage controller may pass the read request information to cache controller which may attempt to satisfy the read request from an appropriate local cache . If the data are not present in the local cache the request is forwarded to block fetcher which fetches the data from the appropriate volume on remote data store via storage service . Once the data is obtained local cache is updated mutations from write log are applied and a read response is returned to customer process . In at least some embodiments if multiple blocks are requested multiple read responses may be returned each indicating a relative offset for a respective block. In at least some embodiments if sequential reads are detected sequential blocks may be prefetched.

In at least some embodiments during write operations block driver sends the write request including a volume ID and the write data to the storage controller that is responsible for the volume. The write data is written to the write log and metadata store is updated to include a reference to the mutated data in buffer pool .

In at least some embodiments a buffer pool resides between storage controller and local data store . Buffer pool may perform one or more of but not limited to the following tasks. Note that some tasks may apply only to cached gateways 

In at least some embodiments buffer pool may employ a database for example a Berkeley database BDB as its metadata store . Table 1 shown below shows information that may be stored in a metadata store according to at least some embodiments. Note that the entries in Table 1 are not intended to be limiting according to content or arrangement.

In at least some embodiments the physical disk offset is at a set boundary for example at a 4 MB boundary. In at least some embodiments this includes boundaries for data in both the volumes and in the write log . In at least some embodiments the writes for a specific volume may be sequential writes and thus fragmentation on disk may not need to be considered. Note that a chunk may correspond to a block or to one or more blocks.

Note that the metadata store may include both S snapshot and C chunk entries and these need to be kept up to date with the scheme via which the storage controller attempts to access blocks. For example a block may be referred the first time using a snapshot ID but every time after that using the chunk ID. This may be preserved in the metadata store . Upon a Snapshot Complete storage controller may refer to the blocks from the snapshot using the snapshot ID hence the C chunk entries in metadata store may be converted into corresponding S snapshot entries.

In at least some embodiments when a read request is received the write log entry or entries for the block are looked up in the metadata store . If the read request can be satisfied using the write log entry or entries then all required entries are looked up in the metadata store read into buffers flattened and the required pieces are returned. If the read request cannot be satisfied only using the write log entry or entries the offset for the cache data block e.g. a 4 MB block is calculated from the offset in the read request. The location of the block is looked up in the metadata store . If the block is in local cache the block is read from the local cache and if not it is fetched from remote data store . The required write log entries are fetched as described above flattened with the block and the required pieces are returned. If the block is fetched from remote data store the block is cached to local cache and recorded in the metadata store . The last access time for the block in the local cache is also updated.

In at least some embodiments when a write request is received the mutations are recorded at the next write log offset and the metadata i.e. offset and length is recorded in the metadata store .

In at least some embodiments when a block upload completes the latest version of the block with the applied mutations is added to the local cache and recorded in the metadata store . If a previous version of the block is present in local cache this block is marked as free in metadata store .

In at least some embodiments when a snapshot completes the metadata store may need to be reorganized as described above. That is the block entries belonging to the snapshot may be converted into the corresponding snapshot entries on the remote data store .

In at least some embodiments when a write request is received the write data is recorded at the next write log offset and the appropriate metadata for the write is recorded in the metadata store . The write request is also passed to the local data store .

In at least some embodiments to upload a block to remote data store an upload process calls buffer pool to read the write log . The buffer pool uses metadata store to perform the translation from the logical write log offset to the physical offset and the data is then read into memory buffers. The buffers are then presented to the upload process. The upload process uploads the blocks to the remote data store and releases the blocks to the buffer pool .

In at least some embodiments if the write log needs to be purged buffer pool obtains a write log offset for a volume for which the write log can be purged. In at least some embodiments the write log offset may be determined from metadata store for example by performing a walk over the database which checks offsets for each entry. To purge the write log the existing write log entries corresponding to the purgeable part of the log may be marked as free entries.

Customer processes A and B represent physical and or virtual machines or systems connected to a client network of a service customer. As an example of a function provided by storage service a user via a customer process may create and mount data volumes in remote data store via storage service . From the perspective of users on a client network the data volumes provided by storage service may appear as if they are local storage hence such a data volume may be referred to as a virtual data volume . A virtual data volume actually maps to one or more physical storage devices or storage systems on which remote data store is instantiated however this mapping is handled by the storage service and is thus transparent from the perspective of the users on the client network . A user of a customer process may simply see a volume mounted on the desktop or in a device listing. The user of a customer process may create data modify data delete data and in generally perform any data related function on virtual data volume just as if the volume was implemented on a locally attached storage device.

Storage gateway may for example be installed activated and configured to serve as a file system gateway as a cloud volume gateway collectively referred to as cached gateways or as a shadowing gateway. A file system gateway serves as a NAS storage interface e.g. using CIFS or NFS protocols to the storage service . The remote data store may be presented to the customer as an object store e.g. REST while actually implemented as block storage. A cloud volume gateway serves as an interface to virtualized volume storage provided by the storage service . The volume storage may be implemented as block storage. The gateway provides local network access points with the remote data store which may also be referred to as a cloud volume serving as backend storage that provides flexible and essentially unlimited primary storage capacity. A shadowing gateway acts as a bump in the wire between a customer s applications and the customer s local data store to provide shadowing of the customer s write data e.g. iSCSI writes to remote storage provided by the storage service . The remote data store may be implemented as block storage.

In cached gateway implementations storage gateway may store a local cache of frequently accessed data on a local data store while securely encrypting and accelerating data movement back to service provider . Similarly shadowing gateway implementations may securely encrypt and accelerate the movement of write data to service provider . This accelerated data movement as compared to a standard Internet connection may for example be achieved using one or more of data deduplication compression parallelization and TCP window scaling techniques. Storage gateway may significantly reduce the cost utilization maintenance and provisioning headaches that are typically associated with managing on site storage arrays as primary storage or backup storage. Storage gateway may accomplish this by replacing the 100 s of terabytes to petabytes of data a customer may otherwise store in house on expensive hardware e.g. NAS or SAN hardware with a cost effective appliance. With the storage gateway customers may benefit from the low access latencies of on site storage provided by the local cache maintained by the gateway in cached gateway implementations while leveraging the durable available and scalable distributed storage infrastructure provided by the service provider .

Embodiments of the storage gateway may work seamlessly with customers on site applications. In at least some embodiments customers may configure the storage gateway to support SAN iSCSI NAS NFS Microsoft CIFS or Object REST storage. In at least some embodiments an iSCSI interface provided by the storage gateway may enable integration with on site block storage applications such as Microsoft SharePoint and Oracle databases. In at least some embodiments customers may utilize NFS and CIFS interfaces provided by the storage gateway to consolidate file storage across environments including but not limited to Windows Linux and UNIX environments. In at least some embodiments the storage gateway may also be configured to support REST based requests.

In at least some embodiments storage gateway may be implemented as a virtual device or appliance that may be downloaded or otherwise installed activated and configured on one or more computing devices such as server systems coupled to the client network infrastructure at a customer data center. Alternatively storage gateway may be implemented as a dedicated device or appliance that may be coupled to the client network infrastructure the dedicated device or appliance may include software and or hardware on which functionality of the gateway may be implemented.

In at least some implementations storage gateway communicates with the service provider network via an intermediate network e.g. the Internet . The coupling of storage gateway to intermediate network may generally be via a high bandwidth connection provided by the service customer s client network as large amounts of data may be transferred across intermediate network between storage service and storage gateway . For example at peak times the connection may need to support the transfer of data at rates of 100 megabits second 100 Mbit s or higher. However in at least some embodiments techniques such as a data deduplication technique may be employed to reduce bandwidth usage when uploading data from storage gateway to storage service and thus more of the connection s bandwidth may be available for other applications. Example data deduplication techniques that may be employed in at least some embodiments are described in U.S. patent application Ser. No. 12 981 393 titled RECEIVER SIDE DATA DEDUPLICATION IN DATA SYSTEMS which is hereby incorporated by reference in its entirety and in U.S. patent application Ser. No. 12 981 397 titled REDUCED BANDWIDTH DATA UPLOADING IN DATA SYSTEMS which is hereby incorporated by reference in its entirety.

In at least some embodiments bandwidth on a connection between client network and service provider over intermediate network may be allocated to storage gateway and to other customer applications for example via a network administrator process at client network . Storage gateway may continuously or nearly continuously upload mutated new or changed data to storage service for example according to a data deduplication technique. However the mutation rate of data at client network may vary over time for example during the day the customer process write throughput may be higher while at night the write throughput may be lower. Thus at busy times when the mutation rate is high storage gateway may fall behind in uploading the mutated data if the bandwidth allocated to the storage gateway is not high enough to keep up storage gateway may then catch up at less busy times when the mutation rate is not as high. In at least some embodiments if the storage gateway falls behind more than a specified threshold the storage gateway may request the allocation of additional bandwidth. In at least some embodiments the storage gateway may raise an alarm to demand more bandwidth if necessary.

While shows a direct connection between storage gateway and storage service note that the connection between storage gateway and storage service may go through local network .

In at least some embodiments of a storage gateway rather than retrieving data from remote data store on demand large blocks or chunks of data even entire volumes of data may be locally cached to a local data store . Storage gateway may include or may have access to physical data storage and or memory local data store on which a local cache of data for example frequently accessed data or critical data may be maintained. Local data store may be volatile or non volatile storage or memory or a combination thereof. Maintaining a local cache of frequently accessed data may generally improve data access times for customer processes since many or most data accesses can be serviced from the local cache rather than retrieving the data from remote data store . However remote data store may serve as the primary data store for the service customer s client network thus storage gateway may communicate with storage service via an intermediate network to periodically aperiodically or continuously upload new or modified data from the local cache to remote data store and to download requested data from remote data store when necessary.

In storage A B C . . . of remote data store illustrates that the remote data store may be implemented on or across several storage devices or systems connected to a local network of service provider . Thus a service customer s data may be spread across two or more physical storage devices or systems on the back end. The back end storage devices may be but are not necessarily multi tenant devices that are shared with other customers. However as noted in reference to from the perspective of the users and processes on client network the client s data may be presented as virtual volumes or files.

In at least some embodiments a service provider as described in reference to may also provide hardware virtualization technologies and possibly other virtualization technologies to customers. A service provider may provide a range of virtualized computing technology and virtualized storage technology including block storage technology that provides block storage capabilities i.e. a block based storage system to customers. Virtual computing environments or systems implemented according to the hardware virtualization technology provided by the service provider may be supported by the block storage technology. The block storage technology may provide a virtualized storage system that for example is able to interact with virtual computing systems through standardized storage calls that render the block level storage functionally agnostic to the structural and functional details of the volumes that it supports and to the operating systems executing on the virtual computing systems or other systems to which it provides storage availability.

Embodiments of a storage gateway may integrate with on site customer applications and the virtualized computing and storage technology provided by service provider providing customers with access to elastic cloud based computing and storage resources. For example customers using the storage gateway for SAN storage may create consistent point in time block based snapshots of their data. These snapshots may then be processed by hardware virtualization technology applications or instances see e.g. virtual computing system s in requiring the high I O and low latency data access that a block based storage system provides. As another example customers may configure the storage gateway for NAS storage via NFS or CIFS file protocols and may create point in time snapshots of their file data accessible from hardware virtualization technology instances.

In some embodiments objects written using a REST based interface provided by storage gateway may be accessed directly from virtualized storage technology provided by the service provider via HTTP or other protocols or may be distributed using integrated content delivery technology provided by the service provider. In some embodiments customers may also utilize highly scalable distributed infrastructure provided by the virtualized storage technology for parallelized processing of these objects on hardware virtualization technology instances.

Hardware virtualization technology may enable multiple operating systems to run concurrently on a host computer i.e. as virtual machines VMs on the host . The VMs may for example be rented or leased to the customers of the service provider . A hypervisor or virtual machine monitor VMM on a host presents the VMs on the host with a virtual platform and monitors the execution of the VMs . Each VM may be provided with one or more IP addresses the VMM on a host may be aware of the IP addresses of the VMs on the host. A local network of service provider may be configured to route packets from the VMs to Internet destinations e.g. to service client s on client network and from Internet sources e.g. service client s to the VMs .

Service provider may provide a service customer s client network coupled to intermediate network via local network the ability to implement virtual computing systems via a hardware virtualization service coupled to intermediate network and to the local network of service provider . In some embodiments hardware virtualization service may provide an interface for example a Web service interface via which a service client may access functionality provided by the hardware virtualization service . At the service provider each virtual computing system may represent a virtual machine VM on a host system that is leased rented or otherwise provided to a service customer.

From an instance of a virtual computing system a user may access the functionality of storage service as previously described. Thus embodiments of a virtualized system as illustrated in may allow a client to create local instances of virtual computing systems implemented on VMs provided by the service provider and to access data from and store data to a remote data store implemented by the service provider from the local instances of the virtual computing systems .

As previously described one or more storage gateways may be instantiated at the client network . At least one of the gateways may be a cached gateway implementation that locally caches at least some data for example frequently accessed or critical data. The storage gateway s may communicate with storage service via one or more high bandwidth communications channels for example to upload new or modified data from the local cache so that the primary store of data the remote data store is maintained in cached gateway implementations or to upload new or modified data write data to a snapshot of a local primary data store on remote data store in shadowing gateway implementations.

Once storage gateway is installed activated and configured a network administrator process of client network may for example create new data volumes or mount existing data volumes on remote data store via storage service . Create volume requests and other service requests may be made to the service via service provider front end . The front end may also manage connections and communications to and from storage gateway . The front end may include one or more of but is not limited to firewalls border routers load balancers gateway servers gateway proxies console processes and in general any networking device and or process that may be necessary to expose the storage service to client network s and to interface the storage service to storage gateway s .

In at least some embodiments storage gateway initiates all connections to the service provider via service provider front end the service provider does not initiate connections to the gateway . In addition the network administrator process does not initiate connections directly to the gateway access by the network administrator process to the gateway for example to configure and manage the gateway is through the service provider via service provider front end .

Storage gateway exposes one or more data ports e.g. iSCSI ports to the customer process es on the client network . A customer process may be any hardware software and or combination thereof that exists on the client network and that can connect to and communicate with the storage gateway via the data protocol of the gateway s data ports e.g. the iSCSI protocol . A customer process may be for example a storage application such as Microsoft SharePoint and Oracle databases a server e.g. an SQL server a Microsoft Exchange server etc. a database application e.g. an SQL database application and Oracle database application a Microsoft Exchange application or any other application or process executing on one or more devices on the client network that is operable to communicate with the storage gateway data port s . Note that a customer process as used herein encompasses any software process that may be executing on one or more devices in the client network however the underlying hardware on which the process executes may be involved in or perform the connections and communications to the storage gateway data port s on behalf of the process.

A mounted volume may be presented to the customer process es by storage gateway . Customer process es may then perform reads from and writes to the volume via the data ports exposed by the storage gateway for example according to iSCSI protocol. Storage gateway handles all read and write requests to volume . While the volume s on remote data store serves as the primary data store storage gateway may also store a local cache of frequently accessed data on a local data store . Local data store may be implemented on storage hardware internal to the storage gateway on storage hardware external to the storage gateway provided by the service customer or on a combination thereof.

For reads storage gateway may first check the local cache to see if a given read can be satisfied from the cache. If the read cannot be satisfied from the local cache then storage gateway may request the data from storage service which gets the requested data or a block or chunk of data that includes the requested data from remote data store and returns the requested data to the storage gateway . Storage gateway may store the block or chunk of data received from storage service to the local cache.

For writes storage gateway may write the new or updated data to the local cache. In at least some embodiments the write data may be appended to a block based write log implemented in the local cache. Storage gateway may include a sender side data upload process not shown that communicates with a receiver side data upload process not shown at service provider to periodically aperiodically or continuously upload new or modified data in the local cache to the primary data store . The uploading of write data from the write log may be performed asynchronously to the processing of the read and write operations from the initiating processes to the local data store . In at least some embodiments this upload process may employ one or more of data deduplication compression parallelization and TCP window scaling techniques. Example data deduplication techniques that may be employed in at least some embodiments as illustrated in are described in U.S. patent application Ser. Nos. 12 981 393 and 12 981 397 which were previously incorporated by reference in their entireties.

The local cache may be limited in size while the remote data store may provide essentially unlimited storage space. Thus storage gateway may remove replace or overwrite older and or relatively inactive data blocks in the local cache with newer and or active data blocks.

In the embodiment illustrated in local data store serves as the primary data store for the customer process es on client network in contrast to the cached gateway implementation in where remote data store serves as the primary data store. Once storage gateway is installed activated and configured as a shadowing gateway the storage gateway exposes one or more data ports e.g. iSCSI ports to the customer process es on the client network . The customer process es on client network may then read from and write to the local data store via the storage gateway data port s . A customer process may be any hardware software and or combination thereof that exists on the client network and that can connect to and communicate with the storage gateway via the data protocol of the gateway s data ports e.g. the iSCSI protocol . A customer process may be for example a storage application such as Microsoft SharePoint and Oracle databases a server e.g. an SQL server a Microsoft Exchange server etc. a database application e.g. an SQL database application and Oracle database application a Microsoft Exchange application or any other application or process executing on one or more devices on the client network that is operable to communicate with the storage gateway data port s . Note that a customer process as used herein encompasses any software process that may be executing on one or more devices in the client network however the underlying hardware on which the customer process executes may be involved in or perform the connections and communications to the storage gateway data port s on behalf of the process.

The read and write requests may be received by the gateway data port s . For reads the requests may be passed directly to the local data store without further interference or processing by gateway and the requested data may be passed directly from local data store to customer process . Write requests directed to the local data store are also passed to the local data store by storage gateway . However in addition to passing the write requests to the local data store the storage gateway may shadow the new or updated data indicated by the write requests to the remote data store via the storage service .

In at least some embodiments to shadow new or updated data to the remote data store storage gateway may locally store or buffer the write data to be uploaded to the to the remote data store for example in a first in first out FIFO write log. In at least some embodiments the write log may be implemented in a block storage format with the write log comprising one or more blocks e.g. 4 MB blocks . Write data received in the write requests may be appended to the write log. The write data from two or more write requests may be written to the same block in the write log. Metadata for the write data relative to a block e.g. offset in the write log block and length as well as an offset in the target data store may be stored to a metadata store.

Storage gateway may include a sender side data upload process not shown that communicates with a receiver side data upload process not shown at service provider to periodically aperiodically or continuously upload the locally stored write data from the write log to the shadowed data volume at remote data store . The uploading of write data from the write log may be performed asynchronously to the processing of the read and write operations from the initiating processes to the local data store . The upload process may upload the write data from the write log in blocks. Once a write log block has been successfully uploaded the corresponding block may be marked as free in the write log.

In at least some embodiments the upload process may employ one or more of data deduplication compression parallelization and TCP window scaling techniques. Example data deduplication techniques that may be employed in at least some embodiments as illustrated in are described in U.S. patent application Ser. Nos. 12 981 393 and 12 981 397 which were previously incorporated by reference in their entireties.

Note that a service provider front end may manage connections to storage gateway . In at least some embodiments storage gateway initiates connections to the service provider via front end the service provider does not initiate connections to the gateway . The front end may include one or more of but is not limited to firewalls border routers load balancers gateway servers gateway proxies console processes and in general any networking device and or process that may be necessary to expose the storage service to client network s and to interface the storage service to storage gateway s .

In at least some embodiments storage gateway initiates all connections to the service provider via service provider front end the service provider does not initiate connections to the gateway . In addition the network administrator process does not initiate connections directly to the gateway access by the network administrator process to the gateway for example to configure and manage the gateway is through the service provider via service provider front end .

As a shadowing gateway the shadowing operations provided by the storage gateway may be effectively transparent to from the perspective of users on the client network . The customer process es perform reads and writes to the data port s e.g. iSCSI port s exposed by the storage gateway on the client network . From the customer process perspective the storage gateway may appear as any other data target e.g. iSCSI target . Read requests from the customer process es received on the data port s are passed on to the local data store that serves as the primary data store. Write requests from the customer process es received on the data port s are passed on to the local data store and shadowed to the remote data store . The shadowing operations of the gateway may be performed in the background without significantly affecting performance of the primary data store or of the client network .

An example use case for the bump in the wire shadowing gateway configuration illustrated in is for disaster recovery. Storage gateway sends updates of data from client network to storage service which stores the data in a shadow volume or volumes also referred to as a snapshot . The data may be stored in the snapshot in a block storage format. The data are also stored to a local data store . If something happens that results in the corruption or loss of a portion or all of a locally stored volume the corrupted or lost data may be recovered from a snapshot of the volume stored in data store . Storage provider may provide an interface via which a customer network administrator e.g. via network administrator process may request the recovery of a snapshot of a portion or all of a locally stored volume from a shadowed volume on remote data store . In at least some embodiments at least a portion of the write log maintained by storage gateway may be uploaded to the remote data store prior to recovering a snapshot of the data to ensure that the shadowed volume from which data is to be recovered is as up to date as possible. Note that in some cases at least some data may be recovered directly from the write log maintained by storage gateway .

As previously described a customer administrator via network administrator process may communicate with storage gateway e.g. a shadowing gateway via the service provider front end for example to configure the gateway . In at least some embodiments one or more customer processes may also be configured to communicate with the storage gateway via the service provider front end to make requests of the gateway . For example a customer process may be an SQL server that is configured to communicate with storage gateway via the service provider front end.

As illustrated in once storage gateway is installed activated and configured as a shadowing gateway the storage gateway exposes one or more data ports e.g. iSCSI ports to the customer process es on the client network . The customer process es on client network may then read from and write to the local data store via the storage gateway data port s . The read and write requests are passed to the local data store and the write data indicated by the write requests are shadowed to the remote data store so that snapshot s of the local data store may be updated.

However when a shadowing gateway comes online in a customer s network either when initially installed activated and configured or after being offline for some reason there may be data in the local data store that is not in the snapshot s on the remote data store . Thus at least some embodiments may provide a bootstrapping process for shadowing gateways during which at least some data from the local data store may be uploaded to the remote data store so that the snapshot s can be populated and or updated to accurately reflect the data that is currently on the local data store .

As indicated at the shadowing gateway may begin uploading pre existing data from the local data store to the remote data store if necessary. For example if this is a new shadowing gateway and the local data store is already populated the existing data in the local data store needs to be uploaded to the remote data store so that a consistent snapshot can be generated. As another example if an existing shadowing gateway comes back online or resumes shadowing operations upon exiting pass through mode new data may have been written to the local data store and thus the snapshot on the remote data store needs to be made consistent with the data currently on the local data store.

As indicated at the shadowing gateway may begin accepting reads and writes from the customer processes via the gateway data port s exposed on the customer s network. As indicated at the shadowing gateway may begin caching write data from the writes to a write log and begin uploading write data from the write log to the remote data store as indicated at .

The upload of data from the local data store begun at may be performed in the background while the shadowing gateway accepts read and write requests and performs its shadowing function on the customer s network. When the upload of data from the local data store is complete the shadowing gateway continues performing its shadowing function.

Note that the order of the elements in may be different. For example element may be performed after any one of elements through . In other words the shadowing gateway may begin accepting reads and writes and performing its shadowing function prior to beginning to upload the pre existing data from the local data store.

When the shadowing gateway determines that the pass through mode can be exited for example by receiving an indication that a detected problem that caused the pass through mode has been addressed the gateway may restart shadowing i.e. start caching and uploading write data as indicated at .

Upon exiting pass through mode there may be data in the local data store that has not been uploaded to the remote data store. Since the gateway continues to receive and process write requests during pass through mode new data may have been written to the local data store. Thus the shadowing gateway may perform a bootstrap as illustrated in to upload at least some data from the local data store to the remote data store to recover from the pass through mode as indicated at .

In at least some embodiments an optimized bootstrapping process for shadowing gateways may be employed to reduce the amount of data that is uploaded from the local data store to the remote data store. The optimized bootstrapping process may detect blocks of data that have already been uploaded to the remote data store and thus avoid uploading blocks that have already been uploaded. The optimized bootstrapping process may leverage tracking data that is generated and maintained for a storage gateway process during general uploading of data from a gateway to the remote data store.

As indicated at the storage gateway may periodically or aperiodically update a token manifest at the service provider and purge at least a portion of the locally tracked tokens. The storage gateway may have to track a large number of tokens. In at least some embodiments a manifest may be provided on the remote data store that may relieve the storage gateway of the burden of having to locally track a large number of tokens. The storage gateway may periodically or aperiodically call the storage service to update the manifest with token s that the gateway has received and may purge the respective locally stored tokens.

In at least some embodiments the optimized bootstrapping process may leverage the manifest to determine what blocks have and have not been uploaded by making a call to check hashes of each of the blocks in the manifest to determine which blocks indicated by the manifest match blocks on the local data store versus which blocks indicated by the manifest do not match blocks on the local data store and thus need to be uploaded. In other words the manifest is used to detect which blocks on the local data store are dirty blocks and which are not. Thus the optimized bootstrapping process attempts to determine via the manifest which blocks have already been uploaded so that the already uploaded blocks are not uploaded again and only dirty blocks are uploaded. In at least some embodiments for the blocks that the optimized bootstrapping process determines do need to be uploaded the dirty blocks a data deduplication technique may be applied when uploading these blocks to reduce the amount of data that is actually uploaded from the dirty blocks.

Embodiments of the storage gateway may be implemented according to a security model that provides data protection for the customer as well as protection against misuse and unauthorized use e.g. pirating of the gateway by the customer or third parties. illustrates aspects of a storage gateway security model according to at least some embodiments.

In at least some embodiments an aspect of the security model is that a storage gateway is delivered and initially installed on a client network without security credentials or other identifying information for the gateway to use in communications with the service provider . An activation process may be employed via which a storage gateway on a customer network can register with the service provider . In at least some embodiments of the activation process the storage gateway may initiate a connection e.g. an SSL Secure Socket Layer TCP connection with and identify itself to the service provider as a correct gateway for a respective customer account to obtain the necessary security credentials. During the activation process the service customer specifies a name for the gateway . In at least some embodiments the service customer logs into the customer s account with the service provider and provides information to the service provider including but not limited to the gateway name that is used in registering the gateway . However the service customer does not log in to the storage gateway and therefore the service customer s security credentials and other account information are not exposed on the gateway . This may minimize the security risk for the service customer. This gateway name along with other metadata related to the gateway and to the service customer may be stored by the service provider and used in tracking and identifying the respective gateway . Note that a service customer may have one or more gateways installed and activated on a client network with each having a unique identifying name and other metadata. further described below in the section titled Storage gateway activation process illustrate an activation process that may be employed in at least some embodiments. In the activation process the gateway may initiate a connection to the service provider and provide metadata about the gateway platform along with a public key to the service provider . The service provider may then provide a temporary unique activation key to the gateway that is used in the activation process. In addition a service customer may be required to log in to the customer s account via a service provider console process to activate the gateway thus the gateway can be matched with the account of the service customer that attempts to activate the gateway . The security credentials and other metadata e.g. the customer supplied gateway name obtained by the storage gateway via the activation process may then be used by the storage gateway in communications with various processes of the service provider network to identify the gateway to the service provider processes.

In at least some embodiments another aspect of the security model as illustrated in is that the storage gateway only accepts externally initiated connections to one or more data ports e.g. iSCSI ports exposed to the customer process es on the client network . The storage gateway does not accept other externally initiated connections and initiates all necessary connections to external processes. For example in at least some embodiments the storage gateway initiates at least one secure connection e.g. an SSL Secure Socket Layer TCP connection to the service provider the service provider however cannot initiate connections to the gateway . An example method for remote gateway management using gateway initiated connections and a long polling technique that may be used in at least some embodiments is illustrated in .

In addition as illustrated in in at least some embodiments the service customer e.g. network administrator process does not directly connect to the storage gateway to configure and manage the gateway instead configuration and operation requests for the storage gateway are made through the service provider which passes the requests to the gateway via the secure communications channel initiated by the gateway . For example as illustrated in configuration and operation requests for a gateway may be performed by or via a network administrator process through a console process on the service provider network. In at least some embodiments the console process forwards a received configuration request or operation request directed to the customer s gateway to a gateway control plane that maintains gateway initiated connections . The gateway control plane locates a current connection to the gateway that is the target of the request for example a connection maintained on a particular gateway control server and the request is forwarded to the gateway via the connection.

Thus in at least some embodiments a user network administrator or process of the customer cannot directly initiate connections to or log in to the storage gateway nor can external persons or processes such as an operator or process on the service provider network initiate a connection to the storage gateway . This along with other aspects of the gateway security model may help to protect the security credentials and other operational information on the storage gateway from being intentionally or unintentionally compromised by external persons or processes.

In another aspect of the security model all communications between the storage gateway and the storage service during activation and operation of the gateway may be secured and encrypted. As noted above an aspect of the security model is that communications between the storage gateway and the storage service are performed over gateway initiated secure connections e.g. SSL TCP connections . An encryption technique for example public private key encryption may be used in communications over the gateway initiated secure connections.

Embodiments of a storage gateway may for example serve as an on premise storage device and as an interface between a service customer s network and a storage service provided by a service provider. In at least some embodiments the storage gateway may be implemented as a virtual device or appliance that may be downloaded or otherwise installed on one or more computing devices such as server systems coupled to a local network infrastructure of the customer at a customer data center. Alternatively the storage gateway may be implemented as a dedicated device or appliance that may be coupled to a local network infrastructure of the customer. The dedicated device or appliance may include software and or hardware that implements the functionality of the gateway.

In at least some embodiments in order to use a storage gateway after the gateway is installed the gateway must be activated with the service provider. This section describes a method via which identification authentication and authorization of a storage gateway may be performed during bootstrapping or activation of the storage gateway. In the gateway activation method the storage gateway is identified and associated with the customer s service provider account. However the customer s credentials are not exposed to the storage gateway during the activation process. In at least some embodiments the customer logs into the customer s account with the service provider and provides information to the service provider including but not limited to a gateway name that is used in registering the gateway . However the customer does not log in to the storage gateway and therefore the customer s security credentials and other account information are not exposed on the gateway. This may minimize the security risk for the customer. In at least some embodiments the service provider account that is used by the customer in the activation process may be the same account that the customer used to manage other resources that are provided to the customer by the service provider including but not limited to other storage resources provided by a storage service and virtualized hardware resources provided by a hardware virtualization service as illustrated in .

After receiving the activation key from gateway control the gateway advertises the activation key within the client network at a fixed port IP address port on the gateway VM or device. The customer via network administrator process may then access the fixed port of the gateway to obtain the activation key the access is redirected to the service provider SP console process with the activation key in the query string.

In at least some embodiments the activation key is valid for a fixed time or lifespan for example 30 minutes after which the activation key expires. In at least some embodiments since the activation key is valid only for a specified lifespan a background garbage collection process may be provided at the service provider that removes expired activation keys. In at least some embodiments the lifespan for an activation key may be longer on the service provider side than on the gateway to handle borderline cases for example 45 minutes on the service provider side 30 minutes on the gateway .

The customer may also be prompted by SP console to enter additional information for example a name for the gateway . After viewing and verifying the displayed metadata the customer may authorize registration of the gateway with gateway control via SP console for example by selecting a confirm or activate or register user interface element. When the customer authorizes registration of the gateway via SP console SP console may pass the activation key obtained from the customer to gateway control . Customer information such as a customer supplied name for the gateway the customer account ID and so on may also be passed to gateway control . The customer supplied activation key is matched against the activation key previously provided to gateway control by gateway . The customer information e.g. the name of the gateway is stored by gateway control along with for example the metadata previously provided by the gateway .

In at least some embodiments all data exchanged between SP console and SP gateway control and between gateway and SP gateway control may be encrypted. In at least some embodiments sensitive data such as the customer s credentials access key or secret key is not passed in the activation process.

Referring again to in at least some embodiments the SP gateway control is responsible for maintaining all information pertaining to registration and activation of the gateway . The gateway meanwhile continuously polls SP gateway control asking for information to generate a certificate signing request CSR . Once SP gateway control has received authorization from the customer via SP console as illustrated in and matches the customer supplied activation key to the activation key provided by gateway SP gateway control may respond to the gateway GET request by providing metadata including but not limited to at least some of the customer information received from the customer as indicated in . The gateway then generates a CSR and sends to SP gateway control . In response to the CSR SP gateway control generates a certificate and signs the certificate with gateway s previously provided public key. In at least some embodiments the certificate may contain customer and or gateway information for example the customer account ID and the customer supplied gateway name. SP gateway control then responds by sending the self signed certificate encrypted with the public key previously provided by gateway to the gateway . The certificate may then be used for authentication in future communications from the gateway to the service provider .

In at least some embodiments to help prevent a customer from activating multiple gateways using the same activation key system hardware specific information may also be included along with the activation key which is published to the SP gateway control by the gateway .

At of if the gateway has not been previously activated the activation process proceeds to element of where the gateway checks if it has any persisted customer information for generating a certificate signing request CSR . If the gateway has the persisted customer information the process proceeds to element of . If the gateway does not have the persisted customer information the process goes to element of . At the gateway generates a public key e.g. an RSA keypair . The gateway may also collect metadata about the hardware and or software of the device that the gateway has been installed on. For example the metadata may include an IP address a MAC address or other hardware and software characteristics of the device. The gateway then publishes the public key and metadata to the SP gateway control as indicated at . At the gateway receives an activation key from the SP gateway control. At the gateway advertises the activation key on a fixed port IP address port on the service customer s network.

As indicated at through of the gateway may then poll the SP gateway control for customer information that is required for generating a CSR. The customer information may include but is not limited to an account ID of the customer and a customer specified name for the gateway. At the gateway may pause e.g. for a minute or for some other period and then check to see if it has received the information from the SP gateway control. At if the information has not been received then the gateway checks to see if the activation key has expired as indicated at . In at least some embodiments the activation key is valid for a fixed time or lifespan for example 30 minutes after which the activation key expires. At if the activation key has not expired then the activation process returns to element of to continue polling the SP gateway control. At if the activation key has expired then the activation process returns to element of to obtain a new activation key from the SP control plane.

At of if the customer information has been received from the SP gateway control then the activation process proceeds to element of where the gateway stores the customer information to persistent memory. In at least some embodiments the received customer information may be encrypted and therefore the gateway may decrypt the information before storing the information. The process then proceeds to element of .

Referring to at the gateway may check to see if it already has a certificate. At if the gateway does already have a certificate the process may proceed to element of where the gateway may obtain configuration information from the SP gateway control. At if the gateway does not have a certificate the process proceeds to element . At the gateway generates a CSR and sends the CSR to the SP control plane. At the gateway receives a security certificate from the SP control plane in response to receiving the CSR the certificate may serve as security credentials for the gateway. At the gateway may disable the advertisement of the activation key see step of . At the gateway may save its current state to persist information certificate customer specified gateway name etc. that has been obtained in the activation process.

At this point the activation process is complete. At the gateway may obtain configuration information from the SP gateway control. In at least some embodiments once the customer has been notified that the gateway has been successfully activated the customer may configure the installed and activated gateway via the SP console. The SP console may provide a user interface for example a web interface to which the customer can log on to the customer s account select the gateway which may be identified by the customer specified name and specify a configuration for the gateway. In at least some embodiments the SP console passes this configuration on to the SP gateway control which then configures the specified gateway via a connection e.g. and SSL TCP connection initiated by the gateway itself.

As indicated at of the activation key is made available at a public IP address on the service customer s network and may be passed unencrypted from the customer to the SP console in the query string. Although the activation key has a limited lifespan and the IP address is only known to the customer there is still a short window of time in which the activation key is exposed at the IP Port. While the activation key by itself is no good without the metadata that is also published by the gateway to the SP gateway control the gateway may be vulnerable to some extent during this short window of time. In at least some embodiments the customer may utilize security groups or other security measures to help prevent malicious users or processes from obtaining an activation key and activating someone else s gateway. In addition since the customer is required to log in to the SP console process to activate a gateway the gateway can be matched with the customer account that attempts to activate it.

Embodiments of a storage gateway may for example serve as an on premise storage device and as an interface between a service customer s network and a storage service provided by a service provider. In at least some embodiments an installed storage gateway may be activated tracked configured and managed remotely via gateway control technology implemented at the service provider. is a high level block diagram that illustrates example gateway control architecture that may be employed in at least some embodiments. In at least some embodiments as illustrated in gateway control may include a group of two or more gateway control servers e.g. gateway control servers A B C . . . . The multiple gateway control servers may provide load balancing and high availability. During operation at a given time a particular installed and activated storage gateway on a service customer s network is connected to a particular one of the gateway control servers . However note that the storage gateway may be connected to a different gateway control server at some other time.

A gateway control server that is currently connected to storage gateway may manage the storage gateway by sending requests or commands to the storage gateway via intermediate network . Requests initiated from the gateway control server to manage the storage gateway may include but are not limited to configuration change requests and operation requests. However since the storage gateway may be deployed behind a client network firewall a gateway control server may not be able to reach the gateway from outside the firewall unless an exception rule is created for the gateway . In addition in at least some embodiments the security model for the storage gateway may dictate that external processes including but not limited to service provider processes are not allowed to initiate connections to the storage gateway .

In at least some embodiments to enable a gateway control server to send requests or commands to storage gateway while enforcing the security model that does not allow the service provider to establish connections to the gateway methods and apparatus for remote gateway management using gateway initiated connections are provided. In the remote gateway management method a gateway initiates a connection to the service provider by sending a connection request. In at least some embodiments the connection is established to a particular gateway control server via a load balancer . However the gateway does not send requests messages to the service provider via the gateway initiated connection. Instead the service provider e.g. a gateway control server holds the connection pending requests to be sent to the gateway while the gateway waits for a response. Upon receiving a request for the gateway for example from a network administrator process or some other process on the client network on which the gateway is instantiated the service provider e.g. a gateway control server sends the request to the gateway via the gateway initiated connection that the service provider e.g. a gateway control server has been holding. The gateway may also send a response to a request to the service provider via the gateway initiated connection.

In at least some embodiments a gateway control server to which a connection from gateway is established e.g. gateway control server A may register the connection with registration service . If a gateway control server receives a request for a gateway to which it does not hold a connection the gateway control server may query the registration service to find out which gateway control server holds the connection and forward the request to the gateway control server that holds the connection to the gateway . In some embodiments as an alternative a gateway control server that receives a request for a gateway to which it does not hold a connection may simply broadcast the request to two or more other gateway control servers .

In at least some embodiments the service provider may employ a ping process to monitor the gateway initiated connections. In the ping process a gateway control server that maintains a connection to a gateway may periodically or aperiodically send a ping message to the gateway . The gateway responds to the ping message. Upon detecting that the gateway has not responded to the ping message s for some specified time out period the gateway control server may drop the connection and may un register the connection with the registration service .

In at least some embodiments the ping messages may be sent to the gateway s at periodic intervals. At least some embodiments may adjust the ping intervals according to the reliability of the connections to specific gateways so that ping messages are sent at shorter intervals to a gateway for which the connection has been unreliable and at longer intervals to a gateway for which the connection has been generally reliable. The ping interval may be increased over time to a given gateway as the connection remains reliable and may be decreased to a given gateway for which the connection has been unreliable.

In at least some embodiments a gateway may detect if its gateway initiated connection has been terminated or dropped. Upon detecting that the connection has terminated the gateway may send another connection request to the service provider to re establish the connection. Note that the connection may be re established to a different gateway control server than the one that formerly held the connection. In at least some embodiments a gateway may determine that its gateway initiated connection has been dropped by monitoring the ping messages and determining that a ping message has not been received over the connection for a specified time out period.

Thus in the remote gateway management method a gateway establishes a connection to the service provider anticipating and waiting for request s from the service provider. The service provider holds the connection pending requests for the gateway . Upon receiving a request for the gateway the service provider forwards the request to the respective gateway over the gateway initiated connection. The service provider and the gateway both monitor and manage the connection so that if the connection drops for some reason the drop is detected and the gateway re establishes the connection.

Referring again to a service customer may access the service provider console to initiate configuration change requests or operation requests for an indicated storage gateway . For example a network administrator via network administrator process . may send a request to a gateway via a console process . The console process may then send the request to a gateway control server behind load balancer . However the gateway control server to which the console process sends the request may not be the gateway control server that holds the connection to the respective gateway . For example gateway control server B may hold the connection to gateway while the request for gateway may be sent to gateway control server A. Therefore a gateway control server that receives the request from console process e.g. gateway control server A may need to forward the request to the gateway control server that holds the connection to the gateway e.g. gateway control server B in order to deliver the request to the appropriate gateway . Thus at least some embodiments may provide a method or methods for a gateway control server e.g. server A to get a request for a particular gateway received from the console process to the gateway control server e.g. server B that currently holds a connection to the particular gateway indicated by the request.

In some embodiments to accomplish this a gateway control server e.g. server A that receives a request for a gateway to which the server does not hold a connection may broadcast the request to all of its peer gateway control servers . is a flowchart of a method for a gateway control server to broadcast a gateway request to its peer servers according to some embodiments. As indicated at when each gateway control server is instantiated the server may register with a registration service . When a gateway control server exits the server is unregistered from the registration service . The registration service may for example be backed by a database service or a distributed storage service. As indicated at a gateway control server e.g. server A may receive a request for a gateway to which the server does not hold a connection. To broadcast the request to its peer gateway control servers the gateway control server e.g. server A may poll the registration service to discover its peer gateway control servers e.g. servers B and C as indicated at . The gateway control server e.g. server A may then forward the gateway request to all of the servers discovered via the registration service as indicated at . The gateway control server that currently holds the connection to the gateway indicated by the request e.g. server B may then send the request to the respective gateway .

In at least some embodiments when a request is delivered to and handled by a gateway a status is returned from the gateway to the gateway control server that currently holds the connection to the gateway e.g. server B which subsequently returns the status to the gateway control server from which it previously received the forwarded request e.g. server A which then returns the status to the console process . The console process may then provide an indication of results of the request to the customer process e.g. network administrator process that initiated the request. If a request fails to reach the target gateway for some reason for example if the gateway indicated by the request is unavailable or cannot be found the console process may provide an indication of failure of the request to the customer process e.g. network administrator process that initiated the request. The customer process may retry the request if necessary or desired.

As indicated at the gateway control process may drop the connection. For example in at least some embodiments the gateway control process may periodically or aperiodically ping the gateway over the connection and may upon detecting that the gateway is not responding to the ping drop the connection. If registered with a registration service the gateway control process may unregister the connection.

As indicated at the gateway may detect that the connection has been dropped. For example in at least some embodiments the gateway control process may periodically or aperiodically ping the gateway over the connection. The gateway may detect that the connection has been dropped by determining that pings from the service provider are not being received over the connection.

Note that other methods for detecting dropped connections from either the service provider side or the client network gateway side may be employed in some embodiments.

A storage gateway that is installed and activated initiates a secure connection request e.g. an SSL TCP connection request to the gateway proxy nodes via the CIP . The proxy node in this example proxy node B that receives the connection request examines the gateway s certificate associated with the connection request to find the gateway identifier and customer account identifier of the gateway that initiated this connection. The customer and gateway may be authenticated using the gateway identifier and customer account identifier from the certificate. After authenticating the customer and gateway the proxy node then publishes to the proxy store that it is the authoritative proxy to communicate with the connected gateway . The proxies e.g. proxy A and B may query the proxy store to discover other proxies that currently hold connections to particular gateways.

In at least some embodiments proxy store may be implemented as a database. The database may be either a distributed or a centralized database. In at least some embodiments the proxy store may store the following associations 

When a message is to be sent to a gateway a proxy may query the proxy store to find which proxy has a connection to the gateway . In at least some embodiments there exists only one entry per gateway in the proxy store .

In at least some embodiments a ping process may be implemented that is used by the proxies in managing the gateway initiated connections. In at least some embodiments a gateway initiates a secure connection e.g. an SSL TCP connection to a gateway proxy via the CIP as previously described. The gateway proxy may periodically or aperiodically send a ping message to the gateway . Each ping message may include a timeout if the gateway does not receive a ping within the time interval it closes the current connection and re initiates a connection via the CIP . In at least some embodiments there is only one proxy gateway mapping in the proxy store at any point in time. If a gateway proxy sends a ping and does not get a response from the gateway it closes its connection to the gateway .

In at least some embodiments on every ping the gateway proxy checks to see if it is the authoritative proxy for a given gateway by querying the proxy store to determine if another proxy has published a connection to the gateway . If it is not the authoritative proxy the proxy closes the connection to the gateway . This may handle cases where multiple connections to the proxy nodes have been initiated by the same gateway for example if the certificate of a gateway has been copied to another gateway and both gateways try to initiate connections.

In at least some embodiments a ping follows the path as shown in . A gateway proxy node in this example proxy B sends a ping message via the SIP . The message hits one of the gateway proxy nodes in this example proxy A. Proxy A finds the authoritative proxy in this example proxy B for the gateway by querying the proxy store and forwards the pin message to proxy B. Proxy B forwards the message to the gateway and the reply from the gateway follows the same path. In at least some embodiments once proxy B gets a reply to a ping from the gateway it increases its ping interval to the gateway . If a gateway connection breaks the ping interval may be reset to a minimum value. Thus poor gateway proxy connections tend to get pinged more often.

The end to end ping method described above in which the proxy initiates the ping message by first sending the ping message to the SIP may help to ensure that the gateway proxy nodes are reachable from the control plane. If a ping fails the proxy may assume that it is not reachable from the control plane e.g. due to a network partition and close the connection to the gateway .

In some embodiments a long polling technique may be used for gateway initiated connections. Referring back to long polling is a polling technique that emulates an information push from a server e.g. a gateway control server to a client e.g. the storage gateway . In the long polling technique a client e.g. the storage gateway initiates a long polling connection to the server e.g. a gateway control server and requests information from the server as in a standard client server poll. However if the server does not have any information available for the client instead of sending an empty response the server holds the client s request and waits for information for the client to become available. Once the information becomes available the server e.g. a gateway control server may respond to the client s long polling request the response including the information to be sent to the client e.g. the storage gateway .

In a gateway initiated connection method that uses long polling the gateway establishes a connection to a gateway control server via a long polling request. For example the gateway may establish an outbound SSL TCP connection with the gateway control server through a load balancer as illustrated in via a long polling request. The gateway control server holds on to the request and keeps the connection alive. The gateway control server receives a request for the gateway . For example a gateway control server may receive a configuration request or operation request for the gateway from the respective network administrator process via a console process as illustrated in . After the gateway control server receives the request for the gateway the gateway control server sends a response to the gateway s long polling request the response includes the request for the gateway e.g. a configuration request or operation request . In some embodiments as an alternative the gateway control server may send the received request to the gateway on the established connection to the gateway that the gateway control server is maintaining without responding to the long polling request.

Embodiments of a storage gateway may be implemented as a cached gateway or a shadowing gateway as previously described. In an example embodiment a cached gateway may be though of as an on premise block based appliance that leverages on premise local storage for most frequent accessed data and remote storage provided by a storage service for essentially infinite total capacity. is a high level block diagram that broadly illustrates the architecture of and data flow in an example network environment in which an embodiment of a cached gateway is implemented. A cached gateway may serve as an interface between a service customer s local network and a storage service at a service provider s network. In at least some embodiments a cached gateway may expose an iSCSI interface to processes on the customer network although other data interfaces may be exposed in some embodiments. As such the cached gateway may appear as a data interface target e.g. an iSCSI target operating within the client network e.g. the cached gateway may appear on the client network as a storage array. The cached gateway may for example expose logical unit numbers LUNs e.g. block based storage devices such as hard disks to processes executing on devices within the client network. The processes in turn may initiate data sessions e.g. SCSI sessions with LUNs and send data commands e.g. SCSI commands to the cached gateway.

In at least some embodiments both the write log and data cache may be implemented in a common local block based data store . The block data store may be implemented in volatile memory non volatile memory or in a combination thereof. The block data store may be implemented on physical memory within the physical device on which cached gateway is implemented on memory external to the physical device on which cached gateway is implemented e.g. on one or more storage devices allocated to the gateway by the customer or on a combination thereof.

Write log data and cached read data may both be stored to the block data store in a block storage format for example as 4 MB four megabyte blocks. The cached read blocks in the block data store may be considered as a read cache and the write log blocks in the block data store may be considered as a write buffer. The metadata store may contain entries for locating both read cache blocks and write log blocks in the block data store . Blocks may be read from the read cache or from the write log to satisfy read requests and blocks may be uploaded from the write log to the remote data store via an upload process. In at least some embodiments when uploading a write block from the write log the uploaded data may be added to the read cache as a new read block. The uploaded write log blocks may be marked as free in the block data store and the metadata store appropriately updated to reflect the changes to the block data store .

In at least some embodiments a write request may modify or mutate only a relatively small portion of a block. Thus in at least some embodiments when uploading a block from write log only the mutated portion may be uploaded to remote data store for example using a data deduplication technique as previously mentioned. In addition the write log may include two or more overlapping writes i.e. writes to the same logical block stored in different write log blocks. When uploading write data from the write log the two or more overlapping writes may be combined for uploading. This combining may be performed outside the data store e.g. in a block in block buffer the blocks in write log itself are not mutated.

As mentioned above in at least some embodiments when uploading a write block from the write log the uploaded data may be added to the read cache as a new read block. For at least some cases for example when a write block includes numerous mutations and or when a large portion of the write block has been mutated the write block is simply copied to the read cache as a new read block and the metadata store is updated. However as mentioned above a write request may modify or mutate only a relatively small portion of a write log block. Thus in at least some cases the respective block may first be fetched from remote data store and the fetched block updated with the mutation s from the write log before adding the block to the read cache to ensure that the entire block in read cache is up to date. As mentioned the write log may include two or more overlapping writes i.e. writes to the same logical block stored in different write log blocks and thus the fetched block may be updated according to one or more write log blocks. In at least some embodiments the fetched block may be stored to block buffer for updating from the write log blocks before being added to the read cache .

Generally new writes are stored to previously freed write log blocks in the block data store however if the block data store is detected as being full or nearly full one or more cached read blocks may be purged to make room for the write data. Note that read blocks may be purged from the block data store for other reasons for example to clear space for new read data. Different techniques or policies may be used to purge read blocks from the block data store in various embodiments. For example in some embodiments a least recently used LRU policy may be applied to purge the stalest read blocks from the block data store .

In at least some embodiments the cached gateway may provide an interface to two or more volumes on the remote data store . In at least some embodiments a separate write log and read cache may be maintained by the cached gateway for each volume . In at least some embodiments the separate write logs and read caches for two or more volumes may be implemented in the same block data store . However in at least some embodiments the write logs and read caches for different volumes may be logically or physically separated on the block data store . In addition in at least some embodiments separate metadata stores may be maintained for the separate volumes .

While shows read cache and write log as logically separate in block data store in at least some embodiments read blocks and write log blocks for a given volume may be physically intermixed in block data store . For example a first physical block may be a read block a second through fifth physical blocks may be write blocks the next two physical blocks may be read blocks and so on.

As mentioned illustrates a general architecture for and data I O operations of a cached gateway according to at least some embodiments. However a storage gateway may also be configured as a shadowing gateway for example as illustrated in . illustrates a general architecture for and data I O operations of a shadowing gateway according to at least some embodiments. A shadowing gateway may include a similar architecture components and data I O operations as illustrated and described for cached gateway in except that a shadowing gateway does not include a read cache or entries in metadata store for the read cache and the read related operations described above for a cached gateway are not performed. Write operations for a shadowing gateway may be similar to those for a cached gateway except that writes are not added to a read cache. In addition read and write requests from customer process es are forwarded to a local data store . Write data from the write requests however are shadowed to remote data store . In at least some embodiments the write data are appended to the write log in block data store and the write data in the write log are periodically or aperiodically uploaded to the remote data store which maintains a snapshot of the primary data store on local data store .

In at least some embodiments the write log and write operations for cached gateways for example as illustrated in and for shadowing gateways for example as illustrated in may be optimized for write performance. In at least some embodiments at least some I O operations of a gateway may use block data store as a sequential data store. In particular the write log may be treated as a sequential data structure and write operations to the write log may be implemented as sequential write operations. In at least some embodiments the write log may be treated as a one dimensional data buffer implemented as a linear or circular queue. For cached gateways data downloaded from remote data store may be stored in read cache separately from the write data sent from the customer process es to the gateway which is stored in write log . For both cached gateways and shadowing gateways write requests may be received from the customer process es in any order i.e. the write requests may be non ordered or non sequential and write data indicated by the non ordered write requests received from the customer process es may be of arbitrary sizes and may be directed to arbitrary locations or offsets in the target data store. However the arbitrary write data received from the customer process es in non ordered write requests is sequentially written and appended to the write log . In at least some embodiments the appending may be done at a sub block level that is two or more instances of write data may be appended within the same block in the write log . Metadata for the updates to the write log e.g. offset and length of the write data in the write log blocks as well as offset in the target data store is stored to the metadata store .

In at least some embodiments it may not always be possible to write all write log data to contiguous locations in the block data store . For example there may be a read cache block between two write log blocks. Thus at embodiments may attempt to write the write log data to contiguous locations as much as possible but may have to skip some locations e.g. blocks if the locations are marked as being used. The metadata store is appropriately updated so that the write log data can be located even if the data are not stored in contiguous blocks.

As described above logically the arbitrary write data is appended to the end of the write log. To implement this in at least some embodiments the block buffer is reserved in blocks of the same size used in the write log e.g. 4 MB blocks . An allocated buffer block is appended to until full. Another buffer block may be allocated for appending new write data full buffer blocks may be asynchronously and sequentially flushed to the write log on the block data store. Full blocks in the write log may be asynchronously and sequentially uploaded to the remote data store by the upload interface uploaded blocks from the write log may be marked as free .

In cached gateway implementations as illustrated in to maintain data consistency read data may need to be merged with write data before the gateway returns the requested data to a customer process . is a flowchart of a method for satisfying a read request according to at least some embodiments of a cached gateway. As indicated at a read request is received from a customer process . In at least some embodiments when a read request is received from a customer process the gateway looks up the data range of the read in the metadata store to determine if there is data in the write log that overlaps the read range. At of if overlapping data is found in the write log that fully covers the read range the data from the write log may be used to directly satisfy the read request as indicated at . Otherwise at of if overlapping data is found in the write log that partially covers the read range the read cache may be checked to see if data is present for the data range as indicated at . If data is in the read cache then one or more data block s may be fetched from the read cache as indicated at . Otherwise one or more blocks may be fetched from remote data store as indicated at . Note that in some embodiments blocks may be fetched from both the read cache and remote data store to satisfy some read requests. At of the fetched data blocks may then be updated with mutated data from the write log . At of the mutated data may be returned to the requesting process to satisfy the read request. In some embodiments the updated blocks may be added to the read cache as indicated at of .

In some embodiments blocks read from the remote data store to satisfy a read request may be added to the read cache and updated from the write log prior to sending the blocks to the requesting process . Alternatively the blocks may be buffered for example to block buffer and updated in the buffer. The updated blocks may then be sent from the buffer to the requesting process and added to the read cache from buffer .

In some embodiments blocks in read cache that are to be used to satisfy a read request may be updated in place with data from the write log and then sent from the read cache to the requesting process to satisfy the read request. Alternatively the blocks may be read from the read cache and buffered for example to block buffer and updated in the buffer. The updated blocks may then be sent from the buffer to the requesting process and added to the read cache from buffer . The previous versions of the blocks in the read cache that were read into the buffer may be marked as free and or overwritten by the newly updated blocks.

At of if no overlapping data is found in the write log the read cache may be checked to see if the read request can be satisfied from the read cache as indicated at of . At of if the read request can be satisfied from the read cache then data from the read cache may be returned to the customer process to satisfy the read request as indicated at of . At of if the read request cannot be satisfied from the read cache one or more data block s may be fetched from remote data store as indicated at of . Data from the fetched blocks may be returned to the customer process to satisfy the read request as indicated at of . In some embodiments the blocks fetched from remote data store to satisfy a read request may be added to the read cache as indicated at of .

In at least some embodiments a gateway may allow customers to request a snapshot of the write log to be taken and uploaded to the remote data store for example through a console process provided by the service provider. In addition or instead the gateway may periodically or aperiodically automatically take and upload a snapshot of the write log to the remote data store . Uploading a snapshot of the write log may for example provide protection of data from hardware and software failures. In at least some embodiments the snapshot is a point in time snapshot only mutated data that is in the write log at the time the snapshot is requested is uploaded in the snapshot. In at least some embodiments for cached gateway implementations when the mutated data is uploaded the locally stored read cache may also be updated with at least some of the data being uploaded so that the data does not need to be downloaded from the remote data store for future reads. After the mutated data is uploaded to the remote data store the data in the write log and the corresponding data in the metadata store can be discarded e.g. marked as free and the space can be reused.

As previously described write log blocks may be periodically or aperiodically uploaded to the remote data store. In at least some embodiments a data deduplication technique may be used in uploading the write log blocks. However the described data deduplication technique operates during the upload process on whatever data is in the block s that are staged to be uploaded. Since arbitrary writes from the customer process es are sequentially appended to the write log and the customer process es may write more than once to the same location in the target data store a write log block or blocks may include more than one write directed to the same location e.g. offset and or range of the target data store.

Thus at least some embodiments may implement a pre upload coalescing technique for the write data in the write log blocks. In this technique the metadata for a write log block or blocks being staged for uploading may be examined to determine if there is more than one write in the write log block s directed to the same location in the target data store. If there is more than one write to given location then the earlier write s may be suppressed when building a buffer block to be uploaded. Thus a block that is passed to the upload process for uploading e.g. according to the data deduplication technique may include only one write the most recent write to a given location rather than possibly two or more writes to the same location that may be present if the pre upload coalescing technique was not applied.

In at least some embodiments a computer system that implements a portion or all of one or more of the storage gateway technologies as described herein may include a general purpose computer system that includes or is configured to access one or more computer accessible media such as computer system illustrated in . In the illustrated embodiment computer system includes one or more processors coupled to a system memory via an input output I O interface . Computer system further includes a network interface coupled to I O interface .

In various embodiments computer system may be a uniprocessor system including one processor or a multiprocessor system including several processors e.g. two four eight or another suitable number . Processors may be any suitable processors capable of executing instructions. For example in various embodiments processors may be general purpose or embedded processors implementing any of a variety of instruction set architectures ISAs such as the x86 PowerPC SPARC or MIPS ISAs or any other suitable ISA. In multiprocessor systems each of processors may commonly but not necessarily implement the same ISA.

System memory may be configured to store instructions and data accessible by processor s . In various embodiments system memory may be implemented using any suitable memory technology such as static random access memory SRAM synchronous dynamic RAM SDRAM nonvolatile Flash type memory or any other type of memory. In the illustrated embodiment program instructions and data implementing one or more desired functions such as those methods techniques and data described above for storage gateway technologies are shown stored within system memory as code and data .

In one embodiment I O interface may be configured to coordinate I O traffic between processor system memory and any peripheral devices in the device including network interface or other peripheral interfaces. In some embodiments I O interface may perform any necessary protocol timing or other data transformations to convert data signals from one component e.g. system memory into a format suitable for use by another component e.g. processor . In some embodiments I O interface may include support for devices attached through various types of peripheral buses such as a variant of the Peripheral Component Interconnect PCI bus standard or the Universal Serial Bus USB standard for example. In some embodiments the function of I O interface may be split into two or more separate components such as a north bridge and a south bridge for example. Also in some embodiments some or all of the functionality of I O interface such as an interface to system memory may be incorporated directly into processor .

Network interface may be configured to allow data to be exchanged between computer system and other devices attached to a network or networks such as other computer systems or devices as illustrated in the other Figures described herein for example. In various embodiments network interface may support communication via any suitable wired or wireless general data networks such as types of Ethernet network for example. Additionally network interface may support communication via telecommunications telephony networks such as analog voice networks or digital fiber communications networks via storage area networks such as Fibre Channel SANs or via any other suitable type of network and or protocol.

In some embodiments system memory may be one embodiment of a computer accessible medium configured to store program instructions and data as described above in reference to the other Figures for implementing embodiments of storage gateway technologies. However in other embodiments program instructions and or data may be received sent or stored upon different types of computer accessible media. Generally speaking a computer accessible medium may include non transitory storage media or memory media such as magnetic or optical media e.g. disk or DVD CD coupled to computer system via I O interface . A non transitory computer accessible storage medium may also include any volatile or non volatile media such as RAM e.g. SDRAM DDR SDRAM RDRAM SRAM etc. ROM etc that may be included in some embodiments of computer system as system memory or another type of memory. Further a computer accessible medium may include transmission media or signals such as electrical electromagnetic or digital signals conveyed via a communication medium such as a network and or a wireless link such as may be implemented via network interface .

Various embodiments may further include receiving sending or storing instructions and or data implemented in accordance with the foregoing description upon a computer accessible medium. Generally speaking a computer accessible medium may include storage media or memory media such as magnetic or optical media e.g. disk or DVD CD ROM volatile or non volatile media such as RAM e.g. SDRAM DDR RDRAM SRAM etc. ROM etc as well as transmission media or signals such as electrical electromagnetic or digital signals conveyed via a communication medium such as network and or a wireless link.

The various methods as illustrated in the Figures and described herein represent exemplary embodiments of methods. The methods may be implemented in software hardware or a combination thereof. The order of method may be changed and various elements may be added reordered combined omitted modified etc.

Various modifications and changes may be made as would be obvious to a person skilled in the art having the benefit of this disclosure. It is intended to embrace all such modifications and changes and accordingly the above description to be regarded in an illustrative rather than a restrictive sense.

