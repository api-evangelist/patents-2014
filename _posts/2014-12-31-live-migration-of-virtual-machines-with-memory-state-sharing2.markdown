---

title: Live migration of virtual machines with memory state sharing
abstract: Embodiments described herein rapidly migrate child virtual machines (VM) by leveraging shared memory resources between parent and child VMs. In a first, proactive phase, parent VMs are migrated to a plurality of potential target hosts. In a second, reactive phase, after a request is received to migrate a child VM to a selected target host, memory blocks that are unique to the child VM are migrated to the selected target host. In some examples, memory blocks are compressed and decompressed as needed. In other examples, the operation environment is modified. Aspects of the disclosure offer a high performance, resource efficient solution that outperforms traditional approaches in areas of software compatibility, stability, quality of service control, resource utilization, and more.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09626212&OS=09626212&RS=09626212
owner: VMware, Inc.
number: 09626212
owner_city: Palo Alto
owner_country: US
publication_date: 20141231
---
This application claims the benefit of U.S. Provisional Patent Application Ser. No. 62 041 047 filed Aug. 23 2014 entitled Live Migration of Virtual Machines with Memory State Sharing U.S. Provisional Patent Application Ser. No. 62 018 582 filed Jun. 28 2014 entitled Live Migration with Pre Opened Shared Disks and U.S. Provisional Patent Application Ser. No. 62 018 580 filed Jun. 28 2014 entitled Using Active Active Asynchronous Replicated Storage for Live Migration all of which are incorporated by reference herein in their entireties.

This application is related to U.S. Non Provisional Patent Applications entitled Using Active Active Asynchronous Replicated Storage for Live Migration and Live Migration with Pre Opened Shared Disks filed concurrently herewith both of which are incorporated by reference herein in their entireties.

Live migration programs such as with vMotion from VMware Inc. migrate a memory image of a source virtual machine VM to a target VM. In some instances the migration is implemented while the parent VM is being executed and thus while memory pages are being modified. A set of synchronization cycles enable the source and target VMs to catch up to each other.

Some known live migration techniques anticipate only one target VM and migrate the entirety of the source VM upon receipt of the request for migration. This technique restricts users or software from selecting target hosts based on dynamic conditions at the time of migration. Moreover migrating the entirety of the source VM may be costly in terms of bandwidth and resources over long distances.

Examples of the present disclosure facilitate rapid migration of virtual machines VMs . The present disclosure transfers memory blocks from a parent VM to a plurality of target hosts before a request for migration is received. Upon receiving a request to migrate a child VM memory blocks that are unique to the child VM are migrated to a selected target host.

This summary introduces a selection of concepts in a simplified form that are described in more detail below. This summary is not intended to identify essential features nor is it to be used as an aid in determining the scope of the claimed subject matter.

Aspects of the disclosure contemplate a hot cloning technology that enables virtual machines VMs to be created rapidly using a forking process whereby a parent VM suspended in memory is used as a memory template for creation of child VMs. The child VMs initially share all memory pages but start to build up a set of their own unique pages based on their activity. The memory pages of the parent VM are shared ahead of time with each target host to which a child VM might need to migrate. When moving the child VM only the unique memory pages are transferred rather than all the memory pages. The unique memory pages may represent only a fraction of all the memory pages and they may be further compressed on the fly for further reductions of approximately 50 . The unique memory pages are logically rebound to the parent VM on the target host and the VM starts executing. The pages that were transferred compressed may be decompressed on demand by the hypervisor during the initial post migration phase and decompression continues in the background at a controlled rate to avoid sustained performance impact.

By lowering the cost of long distance live migration operations in this manner global migrations are enabled based on changing resource costs and workload requirements. New revenue models are further contemplated whereby small charges are levied for policy driven migrations that reduce the ongoing cost of VM execution. In summary forking based live migration transfers a one VM from one location to another within the emerging hybrid cloud architecture.

While described with reference to a particular type of forking as described herein referred to as VMFork by VMware Inc. in some examples those skilled in the art will note that any infrastructure operations components and or configuration of hardware software and or firmware implementing the operations or their equivalents or variations described herein are within the scope of the disclosure.

It is also understood that although some examples describe migration of a specific child VM any child VM forked from the original parent VM template may be migrated in the manner described below.

Examples of the disclosure leverage the memory state inheritance aspect of VMFork for migration purposes by pre migrating memory blocks which are shared by a child VM with a parent VM to multiple target hosts. A copy of at least some portions of a parent VM template is created on each of the target hosts. Once a target host is selected the remaining unshared memory blocks of the child VM are migrated to the target host. This results in faster migration reduced bandwidth utilization reduced downtime reduced CPU cycles reduced I O storage consumption and maximized memory sharing.

Host computing device may include a user interface device for receiving data from a user and or for presenting data to user . User may interact indirectly with host computing device via another computing device such as VMware s vCenter Server or other management device. User interface device may include for example a keyboard a pointing device a mouse a stylus a touch sensitive panel e.g. a touch pad or a touch screen a gyroscope an accelerometer a position detector and or an audio input device. In some examples user interface device operates to receive data from user while another device e.g. a presentation device operates to present data to user . In other examples user interface device has a single component such as a touch screen that functions to both output data to user and receive data from user . In such examples user interface device operates as a presentation device for presenting information to user . In such examples user interface device represents any component capable of conveying information to user . For example user interface device may include without limitation a display device e.g. a liquid crystal display LCD organic light emitting diode OLED display or electronic ink display and or an audio output device e.g. a speaker or headphones . In some examples user interface device includes an output adapter such as a video adapter and or an audio adapter. An output adapter is operatively coupled to processor and configured to be operatively coupled to an output device such as a display device or an audio output device.

Host computing device also includes a network communication interface which enables host computing device to communicate with a remote device e.g. another computing device via a communication medium such as a wired or wireless packet network. For example host computing device may transmit and or receive data via network communication interface . User interface device and or network communication interface may be referred to collectively as an input interface and may be configured to receive information from user .

Host computing device further includes a storage interface that enables host computing device to communicate with one or more datastores which store virtual disk images software applications and or any other data suitable for use with the methods described herein. In some examples storage interface couples host computing device to a storage area network SAN e.g. a Fibre Channel network and or to a network attached storage NAS system e.g. via a packet network . The storage interface may be integrated with network communication interface .

The virtualization software layer supports a virtual machine execution space within which multiple virtual machines VMs may be concurrently instantiated and executed. Hypervisor includes a device driver layer and maps physical resources of hardware platform e.g. processor memory network communication interface and or user interface device to virtual resources of each of VMs such that each of VMs has its own virtual hardware platform e.g. a corresponding one of virtual hardware platforms each virtual hardware platform having its own emulated hardware such as a processor a memory a network communication interface a user interface device and other emulated I O devices in VM . Hypervisor may manage e.g. monitor initiate and or terminate execution of VMs according to policies associated with hypervisor such as a policy specifying that VMs are to be automatically restarted upon unexpected termination and or upon initialization of hypervisor . In addition or alternatively hypervisor may manage execution VMs based on requests received from a device other than host computing device . For example hypervisor may receive an execution instruction specifying the initiation of execution of first VM from a management device via network communication interface and execute the execution instruction to initiate execution of first VM .

In some examples memory in first virtual hardware platform includes a virtual disk that is associated with or mapped to one or more virtual disk images stored on a disk e.g. a hard disk or solid state disk of host computing device . The virtual disk image represents a file system e.g. a hierarchy of directories and files used by first VM in a single file or in a plurality of files each of which includes a portion of the file system. In addition or alternatively virtual disk images may be stored on one or more remote computing devices such as in a storage area network SAN configuration. In such examples any quantity of virtual disk images may be stored by the remote computing devices.

Device driver layer includes for example a communication interface driver that interacts with network communication interface to receive and transmit data from for example a local area network LAN connected to host computing device . Communication interface driver also includes a virtual bridge that simulates the broadcasting of data packets in a physical network received from one communication interface e.g. network communication interface to other communication interfaces e.g. the virtual communication interfaces of VMs . Each virtual communication interface for each VM such as network communication interface for first VM may be assigned a unique virtual Media Access Control MAC address that enables virtual bridge to simulate the forwarding of incoming data packets from network communication interface . In an example network communication interface is an Ethernet adapter that is configured in promiscuous mode such that Ethernet packets that it receives rather than just Ethernet packets addressed to its own physical MAC address are passed to virtual bridge which in turn is able to further forward the Ethernet packets to VMs . This configuration enables an Ethernet packet that has a virtual MAC address as its destination address to properly reach the VM in host computing device with a virtual communication interface that corresponds to such virtual MAC address.

Virtual hardware platform may function as an equivalent of a standard x86 hardware architecture such that any x86 compatible desktop operating system e.g. Microsoft WINDOWS brand operating system LINUX brand operating system SOLARIS brand operating system NETWARE or FREEBSD may be installed as guest operating system OS in order to execute applications for an instantiated VM such as first VM . Aspects of the disclosure are operable with any computer architecture including non x86 compatible processor structures such as those from Acorn RISC reduced instruction set computing Machines ARM and operating systems other than those identified herein as examples.

Virtual hardware platforms may be considered to be part of virtual machine monitors VMM that implement virtual system support to coordinate operations between hypervisor and corresponding VMs . Those with ordinary skill in the art will recognize that the various terms layers and categorizations used to describe the virtualization components may be referred to differently without departing from their functionality or the spirit or scope of the disclosure. For example virtual hardware platforms may also be considered to be separate from VMMs and VMMs may be considered to be separate from hypervisor . One example of hypervisor that may be used in an example of the disclosure is included as a component in VMware s ESX brand software which is commercially available from VMware Inc.

The host computing device may include any computing device or processing unit. For example the computing device may represent a group of processing units or other computing devices such as in a cloud computing configuration. The computing device has at least one processor and a memory . The processor includes any quantity of processing units and is programmed to execute computer executable instructions for implementing aspects of the disclosure. The instructions may be performed by the processor or by multiple processors executing within the computing device or performed by a processor external to computing device . In some examples the processor is programmed to execute instructions such as those illustrated in the figures.

The memory includes any quantity of computer readable media associated with or accessible by the computing device . The memory or portions thereof may be internal to the computing device external to computing device or both.

The memory stores a plurality of VM templates . In some examples VM templates are arranged in a hierarchy such as a tree hierarchy. However aspects of the disclosure are operable with VM templates stored in any structure. In such examples VM templates include a plurality of powered on parent VM templates . The powered on parent VM templates may be created and maintained by the computing fabric cloud service and or by cloud services or by any other computing device . The parent VM templates may be classified categorized or otherwise described as derived VM templates and standalone VM templates. Derived VM templates are derived from one of the parent VM templates and inherit one or more disk blocks e.g. common disk blocks from that corresponding parent VM template . The standalone VM templates lack any such inherited disk block from parent VM templates . Aspects of the disclosure are operable with any form of disk block inheritance such as via a redo log array level snapshots e.g. using block reference counting etc.

In some examples each parent VM template includes a virtual device state for one of VMs shown in and a memory state for that VM . In some examples one or more powered on parent VMs templates include pre launched applications published by the processor and or the hypervisor shown in . In one example the hypervisor brings memory and or code pages into memory to publish the applications . The applications are locked in memory by any means known including memory page stuffing.

Memory further stores data describing a plurality of powered on child VMs . A powered on child VM is instantiated from one of parent VM templates using for example a powerOnChildren function call. The powerOnChildren function call leverages fast VM instantiation techniques such as those as described herein to quickly spawn VMs with minimal processor overhead. The powered on child VM shares one or more memory pages with an associated parent VM. The powered on child VM is entered into memory and any modification and or change of the child VMs is entered into memory as COW. The powered on child VMs may also be powered off or reset using the powerOffChildren function call and the powerResetChildren function call respectively.

When powered off the powered on child VM becomes a powered off child VM . Storage stores data describing a plurality of powered off child VMs . A powered off child VM is instantiated on demand from a parent VM template .

Child VMs and or are registered e.g. to a cloud operating system or other management logic . The cloud operating system is executed by the computing device . Registration of one of child VMs and or includes identifying child VMs and or to the cloud operating system and occurs in some examples before powered off child VM is powered on or otherwise executed. In this manner powered off child VM is said to be pre registered with the cloud operating system. In other examples the child VM and or is registered after powering on. In some examples the cloud operating system is the hypervisor . By registering powered off child VMs the cloud operating system is no longer in the critical path when cloud services commission VMs thus reducing the amount of time needed for child VMs to become available. However aspects of the disclosure are also operable with registration occurring on the child VM and or instantiation path.

Child VMs and or have one or more properties characteristics or data associated therewith. In some examples the child VM properties for each child VM e.g. second VM may be referred to as configuration data . In some examples configuration data for the child VM and or is defined created received and or registered prior to receiving a request to fork the child VM e.g. from a management level application . In other examples configuration data is defined in response to receiving the request to fork the child VM and or . Configuration data may be defined from default values set by an administrator received in the request from the management level application and or populated with data from other sources. Example configuration data for the child VM and or includes an Internet Protocol IP address a MAC address a hostname a domain identity a processor size memory size a set of attributes and or any other state data to be applied when customizing the identity of the child VM and or . In some examples configuration data is stored in a file such as a .vmx file with one file per child VM and or . Configuration data may be registered with virtualization software such as the cloud operating system.

In some examples cloud service specifies whether to create a standalone template or a derived VM template e.g. from another parent VM template . For example cloud service creates a defined quantity of registered e.g. to a cloud operating system but powered off child VMs using the createChildren function call. The createChildren function call also takes as input a childProperties argument which defines for example the identities e.g. hostname IP MAC address etc. and particular processor and or memory sizes of the child VMs . If the sizes are different from that of an existing parent VM template the computing fabric cloud service may either add those resources when powering on a powered off child VM such that the powered off child VM becomes a powered on child VM e.g. a hot add or create a new parent VM template having the desired resources and creating a new child VM from the new parent VM template . In addition the childProperties argument also specifies how the child VM and or behaves when powered on and or reset. For example the child VM and or may act as an ephemeral entity that returns to a parent state or a regular VM that goes through a usual boot process.

In some examples the computing device defines a virtual device state of the child VM and or based on virtual device state of the parent VM. For example defining the virtual device state of the child VM and or includes copying virtual device state from the powered on parent VM templates . As another example defining the virtual device state of the child VM and or includes creating a COW delta disk referencing virtual device state of the parent VM. Alternatively the virtual device state of the child VM and or depends for example on user criteria the system capabilities or the applications the child VM is running.

The computing device in some examples defines creates receives and or registers persistent storage for the child VM and or based on persistent storage .vmdk of the powered on parent VM templates . In some examples persistent storage for the child VM and or is stored in a file such as a .vmdk file. For example defining the persistent storage for the child VM and or includes referencing persistent storage and or disk of the parent VM. In some examples referencing persistent storage and or disk of the parent VM includes creating a read only base disk referencing persistent storage and or disk of the parent VM and creating a COW delta disk associated with the child VM to store changes made by the child VM and or to the base disk.

In some examples computing device defines creates receives and or registers memory for the child VM and or based on memory state of the parent VM. In some examples referencing memory state of the parent VM includes creating COW memory associated with the child VM to store changes made by the child VM and or to memory state of the parent VM. In this manner the child VM and or shares memory state of the parent VM with COW memory pages in contrast with linked clones that use COW delta disks.

The computing device executes e.g. powers on the powered off child VM which becomes powered on child VM . Execution of the powered off child VM includes configuring an identity of child VM and or using configuration data . In some examples execution of the powered off child VM includes configuration and execution of a boot process or bootup process to access and apply configuration data to the powered on child VM . In this manner the powered on child VM customizes itself during bootup. The now executing powered on child VM has a virtual device state that is a copy of virtual device state of the parent VM with persistent storage referencing persistent storage and or disk of the parent VM.

In some examples the bootup process is executed by a guest operating system on the powered on child VM . The bootup process includes for example a command to perform a synchronous remote procedure call RPC to the cloud operating system to obtain and apply configuration data . An example format for the RPC is rpc info get .

The powered on child VM or simply child VM also known as the forked VM may be configured in different ways dependent in part on a type of guest operating system executing on child VM and or . One example for configuring an identity of child VM and or is next described.

In some examples of the disclosure the boot process applies customization to the child VM and or . The boot process includes a blocking agent that prevents the powered off child VM from completing bootup until certain operations have completed. For example the blocking agent is injected into the boot process to prevent the guest operating system on the powered on child VM from accepting user level commands until the identity of the powered on child VM has been configured.

The child VM and or in some examples accesses configuration data that specifies a domain identity to be applied to the child VM and or . The domain identity is one of a plurality or pool of previously created domain identities available to the child VM and or . The plurality of domain identities are created for example by an administrator before the virtual device state of the child VM and the persistent storage and or disk of the parent VM are defined.

The domain identity is pre selected e.g. explicitly identified in configuration data in some examples or selected during execution of the bootup process e.g. based on characteristics of executing powered on child VM . The specified domain identity is from the pool of previously created identities. Then the obtained domain identity is applied to the powered on child VM . In some examples applying the obtained domain identity includes performing an offline domain join operation or any method that allows a computer system to join a domain without a reboot.

In operation preparing the powered on parent VM template for forking may be performed for example by a guest agent residing inside a guest operating system of the powered on parent VM template . The guest agent issues a fork command to quiesce the powered on parent VM template into the ready to fork state at an appropriate boot stage. As provisioning operations are initiated the one or more powered off child VMs are forked without a committed identity. As the boot process begins inside each powered on child VM the various identities are applied to each powered on child VM . For example due to the forking process as described herein a copy of the guest agent from the powered on parent VM template appears in each powered on child VM . The copy of the guest agent resumes execution inside each powered on child VM as part of the boot process of the guest operating system. In this post fork stage for each powered on child VM the guest agent obtains e.g. from a data store available to the guest operating system of the powered on child VM and applies one or more identities to the powered on child VM . For example the identities or other parameters are stored as part of configuration data in a .vmx file or other file stored by the cloud operating system and accessible via API from within the guest operating system. In operation the guest operating system synchronously requests and receives one of the identities from the cloud operating system to perform an offline domain join e.g. update the identity in place before proceeding through the tail end of the bootup process e.g. before the system launches the logon service .

The operations discussed above may be embodied as computer executable instructions stored on one or more computer readable media. The instructions when executed by processor configure an identity of a forked VM based on a pool of available domain identities.

The forking and state customization operations illustrated and described may be implemented using templates and an API to configure and deploy the powered off child VM in response to a request from cloud service . In an example computing device creates and maintains a hierarchy of powered on parent VM templates and powered off child VMs which are ready to be executed. Powered on parent VM templates are created in some examples in response to a request from at least one of cloud services . Alternatively or in addition powered on parent VM templates are created on demand by computing device after detecting patterns in VM provisioning requests from cloud services . Maintaining the set of powered on parent VM templates includes for example powering on each of powered on parent VM templates . Each powered off child VM is instantiated from one of powered on parent VM templates in response to a request for the child VM. Maintaining the set of powered off child VMs includes for example pre registering each instantiated powered off child VM to the cloud operating system e.g. before being initiated or otherwise powered on .

Alternatively or in addition one or more of cloud services may create and maintain one or more of powered on parent VM templates .

In a teardown phase parent VM templates and child VMs and or may be destroyed using function calls such as destroyParentTemplate and destroyChildren . Depending on whether parent VM template is part of the template hierarchy e.g. a derived VM template or a standalone template destroying the template may not remove it completely from disk. The destroyChildren function call turns off powered on child VM e.g. power down and resets the child VM and or properties such as identity etc.

In an automatic mode rather than have powered on parent VM templates be explicitly created via the function calls available in a manual mode parent VM templates are automatically generated based on demand. For example cloud service uses a function call such as createChildrenAuto to create child VMs. When a particular type of child VM is requested repeatedly e.g. a plurality of requests are received for the same type of child VM computing fabric cloud service creates a new powered on parent VM template deriving it from the appropriate powered on parent VM template in the hierarchy. This optimization further simplifies the setup and teardown phases by eliminating the need for cloud services to explicitly create destroy and otherwise manage powered on parent VM templates . In some examples the new powered on parent VM template is created only if additional requests are expected for such VMs. For example if the request for a particular VM is a one off request the new parent VM template is not created.

VM instantiation operations are performed on VMs stored in one or more datastores. Example VM instantiation operations include but not limited to cloning copying forking and the like. VM instantiation operations may be performed by virtualization products such as VMware s ESX brand software e.g. in a kernel layer . In some examples VM instantiation operations implement fast suspend resume technology with COW page references e.g. rather than handing over pages entirely . While described in some examples herein with reference to VM forking routines those of ordinary skill in the art will note that the disclosure is not limited to these VM forking routines. Rather the disclosure is operable with any fast VM instantiation routines in environments where there are common base images with sharing opportunities across VMs in a cluster. However the VM forking routines described herein enable the elimination of any pre flight comparison between the source side parent and the destination side parent as the VM forking routines confer on the destination side parent the same memory state from the source side parent. Aspects of the disclosure enable ease of identification of shared memory versus non shared memory and the ability to pre emptively transmit shared state thereby avoiding compute intensive identification e.g. by assuming shared state exists on the target hosts . VMFork allows shared state to be assumed across the cluster without wasting resources and without constant hash based comparisons that consume processing resources.

Although references are made throughout the description of the Figures to powered on or powered off VMs it is understood that operations which are made can be made in either configuration unless it is expressly stated that the operation cannot be performed in the alternate configuration.

Although in only the source VM is illustrated it is understood that the source VM is both the powered on parent VM template and the child VM and or . When the present disclosure is operating in the proactive phase the source VM is the powered on parent VM template . Once the present disclosure moves into the reactive phase the source VM is the child VM and or . Throughout the disclosure references to the source VM are understood to be to both the powered on parent VM template and the child VM and or depending on which phase of the disclosure is detailed.

Each host contains for example a processor and a memory area. One or more VMs may be contained within the memory area of each host. In the example of the source host is located in California and the target host is located in Massachusetts however the hosts may be located anywhere. In some examples the source host and target host communicate directly with each other. The source host and target host also communicate with their respective storage disks such as storage disk and storage disk respectively through an application programming interface API . The storage disks may be one of any number of examples that are locally or remotely accessible including a virtual storage array NFS VMFS VVOL and vSAN. The storage disks may be accessible through a network. In some examples such as in the storage disks are managed by a vendor provider VP .

Collectively a virtualization platform the source VM and target VM and the source host and target host may be referred to as a virtualization environment . The APIs represent the interface between the virtualization environment and storage hardware . The storage hardware includes the VP and the storage disks of the source VM and the target VM .

In the example of the source VM is located on the source host and the target VM is located on the target host . The source host and target host communicate directly in some examples. In other examples the source host and target host communicate indirectly through the virtualization platform . Storage disks in the illustrated example are managed by VPs or other array providers that allow shared access to the storage disks e.g. VVOLs . The storage disks illustrated in are maintained by one of the VPs . In this example the source host and target host communicate with the storage disks through a network.

Aspects of the disclosure contemplate a policy based driver mechanism to replicate and instantiate the powered on parent VM template on each target host for example each hypervisor in a cluster. In some examples VMs are provisioned using a VMFork based strategy that involves maintaining essentially a powered on parent VM template in memory on each target host in a cluster and every cluster on the hybrid cloud where VMs may be migrated. The parent VM image is specific to each OS type. For example if a cluster is used for Linux Windows 7 and Windows 2008R2 three generic parent VM images are available on each host in some examples.

This memory overhead is taken into account for planning purposes so the sum of memory allocations for each VMFork parent VM is deducted from projections for the memory available to working VMs. Because live migration of child VMs and or e.g. using vMotion from VMware Inc. presumes logical attachment of unique memory pages to an identical parent VM image on the target host the VMFork powered on parent VM template is replicated from the source host on which it was prepared and suspended onto all target hosts.

Creating separate VMFork parents on each target host will not work because each parent VM will be slightly different and will not be able to logically bind their shared pages to children from other hosts. Instead aspects of the disclosure boot up a parent VM on one host e.g. the source host get the parent VM to a state for suspending and copy the powered on parent VM template image to each hypervisor of the other hosts in the cluster or target hosts in the remote clusters containing target vMotion hosts e.g. via vMotion to create replicas on those target hosts. The suspended child VM and or may then be resumed on any host in the cluster because all VMFork parent VMs on the cluster will be identical. Likewise child VMs and or may be migrated to any target host with an identical powered on parent VM template resident in memory. Aspects of the disclosure contemplate a policy based driver mechanism to replicate and instantiate the powered on parent VM template on each of the hypervisors in the cluster or to clusters of hosts or hosts at remote datacenters.

Five types of regions are illustrated on the machine pages . The first type of machine pages illustrated are boot pages illustrated with lines slanting upwards from left to right . Shared application pages shared app pages are illustrated with lines slanting downwards from left to right. Unmapped pages are illustrated by white area on the representation of the parent VM. The unmapped pages are represented in this example only on the powered on parent template VM template . In other examples unmapped pages may also be illustrated on the powered on child VM . However since they are unmapped unmapped pages are not illustrated on the machine pages . Other pages created by the VMs are illustrated by crosshatches. Newly mapped pages are illustrated by horizontal lines.

After forking but before creating any new content the powered on child VM has no independent pages but rather relies on the stored pages on the parent VM disk . The computing device tags marks configures or otherwise indicates that persistent storage and or disk of the parent VM is COW. Tagging the persistent storage and or disk and memory of the powered on parent VM template as COW prevents the parent VM from modifying persistent storage and or disk or memory that the powered on child VM is relying upon. Instead if the powered on parent VM template attempts to modify either persistent storage or memory a copy of that data is created for the powered on parent VM template leaving the original persistent storage or memory intact.

Once the powered on child VM writes it creates its own copy of a page a copy on write COW version of that page. In the example of the child VM writes to the shared application pages thus creating a COW page the newly mapped page in the figure. Once this new write has occurred the powered on parent VM template still points to the shared application pages but the powered on child VM now points to its newly mapped page . reflects that after the COW pages are created the powered on child VM in this example does not point to the shared application pages of the powered on parent VM template and thus the reference count for the shared application pages drop to 1. The reference counts for the newly mapped pages increase to 1 since the powered on child VM created that new page and now points to it. The reference counts for the boot pages and the other pages remain at since in the example illustrated both the child VM and the powered on parent VM template still point to those pages.

After the powered on child VM has created a newly mapped page the powered on child VM writes that page to the physical machine pages . After that newly mapped page is written there is one reference to it by the powered on child VM . In the example of there are two newly mapped pages created.

The first newly mapped page is a modification of an existing page stored by the powered on parent VM template . In some examples the newly mapped page points back to the shared application pages which it modifies and only the changes made by the powered on child VM to the shared application pages are recorded on the newly mapped pages . In other examples the powered on child VM no longer relies on the powered on parent VM template for the modified shared application pages and instead the powered on child VM only utilizes its newly created page.

The second newly mapped page is original content created by the powered on child VM . That newly mapped page does not indicate that it is a modification of any previously existing page. Instead that newly mapped page is solely tied to the powered on child VM and only the powered on child VM references it in some examples.

In the proactive phase of the present disclosure the memory blocks which are shared between the powered on parent VM template and the child VM and or are migrated to a plurality of target hosts under the methods disclosed and illustrated by . In the example illustrated in the boot pages the shared app pages and the other pages are shared by both the powered on parent VM template and the child VM and or . Only these memory blocks are migrated because these memory blocks are common to both the powered on parent VM template and the child VM and or .

In the reactive phase of the present disclosure after a target host is selected and the command to fully migrate the child VM and or is received the memory blocks which are unique to the child VM and or are migrated. In the example illustrated in the newly mapped pages are the only memory blocks unique to the child VM and or . Therefore the newly mapped pages are migrated during the reactive phase of the present disclosure. In some examples the newly mapped pages are COW pages modified memory blocks from the powered on parent VM template . In other examples the newly mapped pages are new pages created by the child VM and or .

For example locking is different for VVOL versus NFS VMFS . In NFS VMFS the systemdisk.vmdk contains the name of the system and points to a flat file e.g. data containing file such as systemdiskflat.vmdk . The locks are placed on the flat file itself e.g. the extent .

For VVOLs the systemdisk.vmdk contains a VVOL ID which points to the VVOL backend and to a lock file such as VVOL UUID.lck file . UUID refers to universally unique identifier in some examples. For VVOLs the lock is not on the backend data itself e.g. VVOL which has no lock primitives but instead on the proxy file VVOL UUID.lck file .

As described herein the target VM opens disks prior to the source VM being stunned e.g. the target VM pre opens the disks with the target VM taking exclusive ownership of the disks after completion of the migration. The process of locking and unlocking is common to both source VMs. However it is also possible that the disks associated with the system are not locked. While some examples are described herein with reference to shared disk locks .lck files and the like the disclosure contemplates any form of shared disks with or without locks. Some examples do not take any locks against the disks e.g. VVOL .lck files and or do not create new .lck files for the target VM . In these examples the disclosure is operable with shared disks but unshared disk locks e.g. there are no disk locks . Aspects of the disclosure are operable with any mechanism for taking exclusive ownership of the disk and or any mechanism allowing the target VM to open the disks .

Each VVOL in some examples is provisioned from a block based storage system. In an example a NAS network attached storage based storage system implements a file system on top of data storage units DSUs and each VVOL is exposed to computer systems as a file object within this file system.

In general VVOLs have a fixed physical size or are thinly provisioned and each VVOL has a VVOL ID identifier which is a universally unique identifier that is given to the VVOL when the VVOL is created. For each VVOL a VVOL database stores for each VVOL the VVOL ID the container ID of the storage container in which the VVOL is created and an ordered list of values within that storage container that comprise the address space of the VVOL . The VVOL database is managed and updated by a volume manager which in an example is a component of a distributed storage system manager. In an example the VVOL database also stores a small amount of metadata about the VVOL . This metadata is stored in the VVOL database as a set of key value pairs and may be updated and queried by computer systems via an out of band path at any time during existence of the VVOL . Stored key value pairs fall into three categories in some examples. The first category is well known keys e.g. the definition of certain keys and hence the interpretation of their values are publicly available . One example is a key that corresponds to the virtual volume type e.g. in virtual machine examples whether the VVOL contains the metadata or data of a VM . Another example is the App ID which is the ID of the application that stored data in the VVOL . The second category is computer system specific keys e.g. the computer system or its management module stores certain keys and values as the metadata of the virtual volume . The third category is storage system vendor specific keys. These allow the storage system vendor to store certain keys associated with the metadata of the virtual volume. One reason for a storage system vendor to use this key value store for its metadata is that all of these keys are readily available to storage system vendor plug ins and other extensions via the out of band channel for VVOLs . The store operations for key value pairs are part of virtual volume creation and other processes and thus the store operations are reasonably fast. Storage systems are also configured to enable searches of virtual volumes based on exact matches to values provided on specific keys.

The operations of the example method of are carried out by a processor associated with the child VM and or . The hypervisor coordinates operations carried out by the processors associated with the source host and target host and their associated VMs. described below illustrates the sequence of the following events.

Optionally at the VP switches and or changes the replication mode from active active asynchronous to active active synchronous or near synchronous or approximately asynchronous in some examples . In some examples this change in operating environment is in response to notification from a virtualization software implementing a virtualization platform or environment such as VMware Inc. s VirtualCenter invokes an API such as part of API e.g. PrepareForBindingChange . In some examples the replication mode may already be active active asynchronous when the VP issues the request. In some examples the VP also drains queued replication data I O as necessary. This call blocks further I O commands for as long as needed to switch the replication state to be synchronous. The PrepareForBindingChange API function call or other function call is issued against the shared storage disk of the source VM . Switching from asynchronous replication to synchronous replication during the live migration ensures that any writes to the child VM and or that occur during the live migration are duplicated by the target VM . Aspects of the disclosure ensure that the underlying replication solution flushes whatever writes are occurring synchronously to the replica LUN disk storage e.g. storage disk . The target VM in some examples does not actually issue duplicate I O commands.

At the management of the disks is evaluated. It is determined whether the disks are managed by VP at . If the disks of the source VM which at this stage is the powered on parent VM template are not managed by a VP with the workload of the source VM still running the powered on parent VM template changes and or downgrades its disk locks from exclusive locks to multiwriter e.g. shared disk locks and or non exclusive mode at . If the disks of the powered on parent VM template are managed by a VP at the hypervisor requests that the VP change and or downgrade the disk locks of the powered on parent VM template to non exclusive mode.

In another example the disk locks may be changed and or downgraded to an authorized user status. The authorized users may be established as the source VMs both the powered on parent VM template and the child VM and or and the target VM . This operation is omitted in the event that there are no locks on the disks . This may occur any time prior to stunning the powered on parent VM template . In some examples the powered on parent VM template sends a message to the target VM that multiwriter mode is available for the disks to be migrated. In some examples the target VM is instructed not to write to the disks .

At an instance of the powered on parent VM template is created or registered at and or with the target host . In order to register the powered on parent VM template the powered on parent VM template shares its configuration including information regarding its disks . For example the new instance of the powered on parent VM template registered at the target host points to the replicated read only disk content on the disk of the powered on parent VM template .

After the powered on parent VM template is registered at the target host at the newly created target VM binds and opens all disks in non exclusive mode e.g. multiwriter lock mode at . The destination VMS opens disks at . At the memory of the powered on parent VM template is pre copied from the source host to the target host . For example ESXi servers using the vMotion network pre copy the memory state of the powered on parent VM template . This may take anywhere from seconds to hours. Pre copying is complete when the memory at the target VM is approximately the same as the memory at the powered on parent VM template . Any form of memory copy is contemplated. The disclosure is not limited to pre copy. Further the memory copy may be performed at any time even post switchover. Only memory which is not already present at the target host is copied. In some examples some of the identified blocks to be pre copied are compressed while some remain uncompressed. In examples where compression occurs selectively identified memory blocks are compressed for example based on system and usage restraints or based on protocols defined by the user .

The powered on parent VM template is stunned frozen or otherwise suspended at . Stunning freezes or otherwise suspends execution of the powered on parent VM template but does not quiesce the powered on parent VM template in some examples. For example no cleanup or shutdown operations normally associated with quiescing are performed. In some examples suspending a process includes removing the process from execution on the kernel or otherwise stopping execution of the process on the OS. For example while execution of the process is halted the process is not terminated or otherwise deleted from memory.

The duration of the suspended execution in some examples is about one second. Several operations may be performed during this duration or interval 

A. Any remaining dirty memory state is transferred from the powered on parent VM template to the target VM . This may be performed as part of a checkpoint transfer at .

Once stunned at the virtual device state of the powered on parent VM template is serialized for transmission to the target VM . Serializing the virtual device state of the powered on parent VM template on the source host in some examples includes closing disks e.g. VM file systems logical unit numbers etc. and releasing exclusive disk locks. These operations are often collectively referred to as checkpoint transfer. The virtual device state includes for example memory queued input output the state of all virtual devices of the powered on parent VM template and any other virtual device side memory.

Upon receipt of the information in the checkpoint transfer the target VM engages in a checkpoint restore at . For example the target VM restores the virtual device state of the powered on parent VM template at the target VM once the VP indicates that the disks have been opened successfully in multiwriter mode for the target VM . However there is no need to open the disks at this point because that occurred earlier at .

In some examples the target VM then transmits an explicit message to the powered on parent VM template that the target VM is ready to start executing at . The powered on parent VM template in this example replies with a Resume Handshake at . A handshake is performed between the target VM and the parent VM template . In other examples the powered on parent VM template sends a message to the target VM confirming receipt of the message from the target VM . In another example the processor e.g. processor queries and updates both the source and the destination VMs for status reports regarding the checkpoint transmission and restoration.

After the handshake between the target VM and the powered on parent VM template at the powered on parent VM template in some examples may terminate at . In other examples the powered on parent VM template continues to run. If the powered on parent VM template terminates or closes that includes releasing its multiwriter disk locks. The target VM with the workload already running and issuing disk I O transparently changes and or upgrades its locks from multiwriter to exclusive ownership.

Optionally at the replication mode is changed from active active asynchronous to active active synchronous or near synchronous or approximately asynchronous in some examples . In some examples this change in operating environment is in response to notification from a virtualization software implementing a virtualization platform or environment such as VMware Inc. s VirtualCenter invokes an API such as part of API e.g. PrepareForBindingChange . In some examples the replication mode may already be active active asynchronous when the VP issues the request. In some examples the VP also drains queued replication data I O as necessary. This call blocks further I O commands for as long as needed to switch the replication state to be synchronous. The PrepareForBindingChange API function call or other function call is issued against the shared storage disk of the source VM which in this phase is still the powered on parent VM template . Switching from asynchronous replication to synchronous replication during the live migration ensures that any writes to the powered on parent VM template that occur during the live migration are duplicated by the target VM . Aspects of the disclosure ensure that the underlying replication solution flushes whatever writes are occurring synchronously to the replica LUN disk storage e.g. storage disk . The target VM in some examples does not actually issue duplicate I O commands.

In some examples the powered on parent VM template is forked at . Since the powered on parent VM template is resident on the target hosts it is optionally forked either before or after migration of the memory blocks associated with the powered on parent VM template . This operation is the conclusion of the proactive phase of the disclosure.

At a request is received to migrate a child VM and or to a selected target host. The selected target host is selected from amongst all of the target hosts based on some criteria. The criteria in some examples is hardware based. For instance the criteria may be relative processing power of the target hosts. In other examples the criteria is based on bandwidth considerations the selected target host may have a greater bandwidth for transmission and receipt of memory blocks. The criteria may be defined by users or administrators. The criteria is in some examples established by the hypervisor.

The request to migrate the child VM and or is considered a triggering event. The triggering event may be for example in response to a request from a user or an administrator. In other examples the request for migration of the child VM and or may be in response to changes in hardware or software availability and capability. For example a target host may have more processing resources available than the source host does or the child VM and or may be requested by multiple users. As further examples the source host and or source VM is live migrated for backup purposes in order to make it more accessible to a different user . Requests for live migration are in some examples periodic or otherwise occurring at regular intervals. In other examples requests for live migration are made during system downtime when I O commands fall below a threshold amount established for instance by users . In other examples requests for live migration are in response to system conditions such as anticipated hardware upgrades downtimes or other known or predicted hardware or software events.

The selected target host is chosen from the plurality of target hosts . In some examples a target host is selected based on various criteria. In one example a target host is manually selected by a user. In another example a target host is automatically selected based on a predetermined policy algorithm proactive resource management component auction model and or any other mechanism that enables the target host to function as described herein. Although not illustrated in the selected target host will be identified as selected target host . This operation begins the reactive phase of the disclosure. In some examples multiple child VMs and or are migrated to one or more target hosts .

Optionally at the VP switches and or changes the replication mode from active active asynchronous to active active synchronous or near synchronous or approximately asynchronous in some examples . In some examples this change in operating environment is in response to notification from a virtualization software implementing a virtualization platform or environment such as VMware Inc. s VirtualCenter invokes an API such as part of API e.g. PrepareForBindingChange . In some examples the replication mode may already be active active asynchronous when the VP issues the request. In some examples the VP also drains queued replication data I O as necessary. This call blocks further I O commands for as long as needed to switch the replication state to be synchronous. The PrepareForBindingChange API function call or other function call is issued against the shared storage disk of the source VM which at this point is the child VM and or . Switching from asynchronous replication to synchronous replication during the live migration ensures that any writes to the child VM and or that occur during the live migration are duplicated by the target VM . Aspects of the disclosure ensure that the underlying replication solution flushes whatever writes are occurring synchronously to the replica LUN disk storage e.g. storage disk . The target VM in some examples does not actually issue duplicate I O commands.

At the management of the disks is evaluated. It is determined whether the disks are managed by VP at . If the disks of the child VM and or are not managed by a vendor provider VP with the workload of the child VM and or still running the child VM and or changes and or downgrades its disk locks from exclusive locks to multiwriter e.g. shared disk locks and or non exclusive mode at . If the disks of the child VM and or are managed by a VP at the hypervisor requests that the VP change and or downgrade the disk locks of the child VM and or to non exclusive mode.

In another example the disk locks may be changed and or downgraded to an authorized user status. The authorized users may be established as the source VMs both the powered on parent VM template and the child VM and or and the target VM . This operation is omitted in the event that there are no locks on the disks . This may occur any time prior to stunning the child VM and or . In some examples the child VM and or sends a message to the target VM that multiwriter mode is available for the disks to be migrated. In some examples the target VM is instructed not to write to the disks .

At an instance of the child VM and or is created or registered at the selected target host . In order to register the child VM and or the child VM and or shares its configuration including information regarding its disks . For example the new instance of the child VM and or registered at the selected target host points to the replicated read only disk content on the disk of the child VM and or .

After the child VM and or is registered at the selected target host at the newly created target VM which is now the selected target VM not illustrated binds and opens disks in non exclusive mode e.g. multiwriter lock mode at .

At the memory blocks of the child VM and or are compared to the memory blocks of the powered on parent VM template . The unique blocks of the child VM and or are isolated. In some examples the unique blocks will be the COW blocks created by the child VM and or . The unique blocks of the child VM and or are any memory blocks not shared with the powered on parent VM template as illustrated in . In the example of the newly mapped pages would be identified as unique blocks.

It is determined at whether child VM includes unique memory blocks. If no unique blocks are identified then the sequence moves to operation . If unique blocks are identified they may optionally be compressed into a staging memory area at . In some examples some of the identified blocks are compressed while some remain uncompressed. In examples where compression occurs selectively identified memory blocks are compressed for example based on system and usage restraints or based on protocols defined by the user . Upon a request for any of the retrieved memory blocks if they are compressed the compressed memory blocks are decompressed. In some examples all of the retrieved memory blocks are compressed. In alternative examples none of the retrieved memory blocks is compressed. Alternatively some memory blocks are compressed and the remainder are uncompressed. In some examples the blocks are compressed at a fixed rate. In other examples the blocks are decompressed at a variable rate. For instance the compressed blocks are decompressed based on frequency of use. Alternatively decompression is a function of available resources or decompression is in some examples performed in accordance with an algorithm.

When memory blocks are compressed any memory page accessed upon completion of the live migration automatically decompresses and the rest of the memory pages may be decompressed in the background but without holding up VM execution in some examples. The majority of memory pages on the selected target host may already remain uncompressed because they belong to the powered on parent VM template which sits in memory uncompressed. In some models built based on observation of VDI sharing ratios there are 90 of memory pages remaining shared with the VMFork powered on parent VM template . On a 1 GB powered on parent VM template this means that only 100 MB of data is unique and would need to be transferred on the WAN during a long distance live migration for instance utilizing vMotion by VMWare Inc. Compression of this dataset may reduce it to 40 MB which in some examples may be transferred rapidly on the hybrid cloud resource grid. Additional WAN deduplication technologies may also be used in some examples to complement the data reduction strategies described here. Examples of such WAN optimization technology include vTO WAN optimization virtual appliance by VMware Inc. or third party commercial solutions.

At the unique memory of the child VM and or is copied from the source host to the selected target host . For example ESXi servers from VMware Inc. using the vMotion network copy the memory state of the child VM and or . This may take anywhere from seconds to hours. Any form of memory copy is contemplated.

In some examples upon receipt of all unique memory pages the hypervisor registers the migrated VM in the local virtual center. In some examples ESX performs the registration in the case of long distance vMotion or in the local vCenter in the case of regular vMotion. The resulting VM instantiated on the selected target host is a new VMFork child whose memory pages are comprised of the standard pointers to each memory page of the powered on parent VM template which now exists on the selected target host plus the unique memory pages transferred from the source host and which take precedence over the shared pages of the powered on parent VM template at the same memory location.

The child VM and or is stunned frozen or otherwise suspended at . Stunning freezes or otherwise suspends execution of the powered on parent VM template but does not quiesce the child VM and or in some examples. For example no cleanup or shutdown operations normally associated with quiescing are performed. In some examples suspending a process includes removing the process from execution on the kernel or otherwise stopping execution of the process on the OS. For example while execution of the process is halted the process is not terminated or otherwise deleted from memory.

The duration of the suspended execution in some examples is about one second. Several operations may be performed during this duration or interval 

A. Any remaining dirty memory state is transferred from the child VM and or to the target VM . This may be performed as part of a checkpoint transfer at .

Once stunned at the virtual device state of the child VM and or is serialized for transmission to the target VM . Serializing the virtual device state of the child VM and or on the source host in some examples includes closing disks e.g. VM file systems logical unit numbers etc. and releasing exclusive disk locks. These operations are often collectively referred to as checkpoint transfer. The virtual device state includes for example memory queued input output the state of all virtual devices of the child VM and or and any other virtual device side memory.

Upon receipt of the information in the checkpoint transfer the target VM engages in a checkpoint restore at . For example the target VM restores the virtual device state of the child VM and or at the target VM once the VP indicates that the disks have been opened successfully in multiwriter mode for the target VM . However there is no need to open the disks at this point because that occurred earlier at .

In some examples the target VM then transmits an explicit message to the child VM and or that the target VM is ready to start executing at . The child VM and or in this example replies with a Resume Handshake. In other examples the child VM and or sends a message to the target VM confirming receipt of the message from the target VM . In another example the processor e.g. processor queries and updates both the child VM and or and the selected target VM for status reports regarding the checkpoint transmission and restoration.

After the handshake between the selected target VM and the child VM and or at the selected target VM begins executing at . At the selected target VM confirms that it has executed. The child VM and or terminates at . If child VM and or terminates or closes that includes releasing its multiwriter disk locks at . The selected target VM with the workload already running and issuing disk I O transparently changes and or upgrades its locks from multiwriter to exclusive ownership at .

In some examples the process of cleanup occurs. This includes invoking another of APIs e.g. CompleteBindingChange that allows the storage vendor to reverse the environment changes made above. The VP reverts replication to operating asynchronously e.g. active active asynchronous and may change replication direction or bias and restore an original recovery point objective RPO at .

Each VM is comprised of a static set of shared pages plus a growing set of unique memory pages. For any given vMotion target host the static set of shared pages can be moved prior to the need to perform the actual vMotion operation and need be performed only once per host. By proactively moving the VMFork parent s memory image to set of possible target hosts ahead of vMotion requests only the set of unique memory pages need be moved during the actual vMotion event e.g. responsive to a migration request or decision . For child VMs such as virtual desktop infrastructure VDI desktops with as little as 10 unique memory pages only 200 MB may need to be migrated rather than 2 GB in some examples. In some examples high performance compression algorithms may further reduce this by at least 50 by compressing these memory pages e.g. using ESX s built in memory compression format which allows for decompression to proceed on demand once the VM is fully migrated to the target host. In the context of long distance vMotion there is an opportunity to move the parent VM s memory image to a set of ESX hosts located at one or more hosting facilities where a company may wish to subsequently move their VMFork based VMs. This heavy lifting portion of setup may be done at off peak hours to minimize the impact of the heavy transfer. Later on when there is a need request and or decision to migrate a child VM within the hybrid cloud only unique memory pages need be moved and logically attached to the already present parent VM image.

By radically reducing the amount of data that must be moved on the WAN much smaller WAN networks become viable for VM migration on a grid of hybrid cloud resources. The transactional overhead is reduced both in cost of WAN consumption and in time to complete the migration operation. This order of magnitude level of reduction in transactional overhead broadens the value of geo optimization opening the door to advanced policy driven migration strategies that lower cost of execution while maintaining workload specific service level agreements SLAs . This capability facilitates revenue models such as charging small commission fees for VM migrations on the hybrid cloud within the context of cost reducing geo optimizations. VMFork based provisioning in tandem with VMFork aware vMotion form the backbone of these new advanced hybrid cloud optimization opportunities.

Aspects of the disclosure dramatically reduces the amount of data that must be transferred when live migrating a virtual machine from one datacenter to another. The traditional vMotion operation copies full memory state of the virtual machine from one hypervisor to another. In the case of long distance migrations this may result in the need to transfer many gigabytes of data. The examples described herein may reduce this data by as much as 90 in many cases through a combination of logical deduplication achieved by leveraging VMFork s knowledge of shared versus unique blocks and compression.

Aspects of the disclosure reduce the bandwidth required for migration to a bare minimum. Aspects of the disclosure reduce the cost of long distance migrations to a level where are considered modest compared to the overall cost reductions achievable with policy based migrations.

Aspects of the disclosure enable economically driven migrations that enable VMs to execute at a location where resources corresponding to their needs are priced advantageously.

Aspects of the disclosure reduce the amount of data processing on the host increase the speed of the host and other devices reduce an amount of data being transferred during live migration improve the functioning of the host itself use less memory for the same set of VMs reduce power consumption use of memory takes power reduce processor load reduce network bandwidth usage improve operating system resource allocation and or increase efficiency.

VMFork aware vMotion includes focus on both the COW memory page transfer and also the linked clone transfer for long distance vMotion. Similarly the parent VM storage image and the parent VM memory image can be transferred before the event. To vMotion a child VM the linked clone and the memory image are transferred to the remote datacenter in some examples except in examples where synchronous replication is in play.

While the present disclosure allows for a change to the replication environment alternative replication environments are available and replication may occur continuously in order to facilitate a more rapid migration upon demand. Replication copies the data associated with a VM from one location to another e.g. from one host to another host for backup disaster recovery and or other purposes. Replication can occur every hour nightly continuously etc. There are several types of replication. In active active replication both hosts have access to their respective copies of the VM. That is the active active nature of replication ensures that both sides can concurrently write to the replicated storage without issue.

Further replication can be synchronous or asynchronous. Synchronous replication requires round trips on the write path whereas asynchronous replication does not. Each party in some examples can freely write to disk. Aspects of the disclosure are operable with any mechanism e.g. locking generation number tracking etc. that ensures that one can in a distributed manner determine where the latest version of any given item of data is stored.

In active active synchronous replication one host notifies the other host of a planned data write and provides the data to be written and both hosts perform the write at the same time. There may be significant latency involved to coordinate the write operations in this way especially over long distances.

In active active asynchronous replication one host notifies the other host of a planned data write and asks whether that host has a dirty copy of the data block to be written e.g. an updated version of the data block . If the other host has a dirty copy of that data block the data block is synchronized and then the data write is performed by both hosts. In this manner both hosts coordinate their writes to ensure that they do not write to the same data block without first checking to make sure they have the most updated version of the data block.

In active passive replication only one side can write to their copy of the VM. In this manner one host is considered active and the other host is considered passive. The active host is able to write to its copy of the VM whereas the passive host is not able to initiate writes to its copy of the VM as the passive host merely maintains a copy of the VM. In the event of failure of the active host the passive host becomes the active host and resumes execution of the VM.

Replication may be described in some examples at the VM level e.g. replication of VMs or a subset of the disks of the VMs such as in Host Based Replication HBR and or vSphere Replication. Alternatively or in addition replication may be described at a deeper level with reference to logical unit numbers LUNs a group of LUNs in a consistency group and or the like. In general aspects of the disclosure are operable with replication in which at least one host writes to a LUN which backs one or more of the disks of a VM on one site with another host at another site leveraging the replicated LUN content.

Various methods of replication may be leveraged to ensure that the proactive phase of the present disclosure is complete at the time a request for live migration is received. For example the parent VM templates may be replicated through active passive replication to all target hosts continuously. In that example when a request for live migration is anticipated the replication environment may be altered.

Aspects of the disclosure contemplate various revenue models for determining where to place the shared data and when to place the shared data. For example aspects of the disclosure contemplate proactive resource management e.g. resource scheduling load balancing etc. and or an auction model to determine where it will be cheapest to execute a VM based on its workload resource profile and compare this to cost of moving the VM there.

Efficient long distance live migration of VMs allows platforms to advertise resources e.g. in a marketplace including a price for I O a price for storage per gigabyte etc. This allows analysis of workload attributes of VMs e.g. CPU memory etc. to develop a workload profile and then compare that to the advertisements from the host providers to choose a host provider by estimating cost over time and or savings over time while factoring in movement costs.

Similar considerations are performed to determine when to migrate the shared data. For example the migration may occur as part of a management operation when setting up a hybrid cloud operation. The migration may occur in response to manual input e.g. manual selection of hosts and or the result of a policy driven framework e.g algorithmic selection . Some policy examples include evaluating intermittently or regularly e.g. weekly to get a set of target hosts calculating the costs of moving to those hosts and the costs of storing at those hosts. One or more of the hosts may then be selected based on their calculations e.g. select one or more hosts with the lowest costs . VMFork based child VMs used linked clone technology which allows for their parent VM s storage image to be transferred to any target host along with the parent VM s memory image. At the point of vMotion for child VMs based on VMFork only the linked clone file and the unique memory pages will need to be transferred to the target host. The data reduction on the WAN is thus two fold and includes a reduction in the size of the live memory image and the storage blocks that define the child VM.

The operations described herein may be performed by a computer or computing device. The computing devices communicate with each other through an exchange of messages and or stored data. Communication may occur using any protocol or mechanism over any wired or wireless connection. A computing device may transmit a message as a broadcast message e.g. to an entire network and or data bus a multicast message e.g. addressed to a plurality of other computing devices and or as a plurality of unicast messages each of which is addressed to an individual computing device. Further in some examples messages are transmitted using a network protocol that does not guarantee delivery such as User Datagram Protocol UDP . Accordingly when transmitting a message a computing device may transmit multiple copies of the message enabling the computing device to reduce the risk of non delivery.

By way of example and not limitation computer readable media comprise computer storage media and communication media. Computer storage media include volatile and nonvolatile removable and non removable media implemented in any method or technology for storage of information such as computer readable instructions data structures program modules or other data. Computer storage media are tangible non transitory and are mutually exclusive to communication media. In some examples computer storage media are implemented in hardware. Example computer storage media include hard disks flash memory drives digital versatile discs DVDs compact discs CDs floppy disks tape cassettes and other solid state memory. In contrast communication media typically embody computer readable instructions data structures program modules or other data in a modulated data signal such as a carrier wave or other transport mechanism and include any information delivery media.

Although described in connection with an example computing system environment examples of the disclosure are operative with numerous other general purpose or special purpose computing system environments or configurations. Examples of well known computing systems environments and or configurations that may be suitable for use with aspects of the disclosure include but are not limited to mobile computing devices personal computers server computers hand held or laptop devices multiprocessor systems gaming consoles microprocessor based systems set top boxes programmable consumer electronics mobile telephones network PCs minicomputers mainframe computers distributed computing environments that include any of the above systems or devices and the like.

Examples of the disclosure may be described in the general context of computer executable instructions such as program modules executed by one or more computers or other devices. The computer executable instructions may be organized into one or more computer executable components or modules. Generally program modules include but are not limited to routines programs objects components and data structures that perform particular tasks or implement particular abstract data types. Aspects of the disclosure may be implemented with any number and organization of such components or modules. For example aspects of the disclosure are not limited to the specific computer executable instructions or the specific components or modules illustrated in the figures and described herein. Other examples of the disclosure may include different computer executable instructions or components having more or less functionality than illustrated and described herein.

Aspects of the disclosure transform a general purpose computer into a special purpose computing device when programmed to execute the instructions described herein.

The examples illustrated and described herein as well as examples not specifically described herein but within the scope of aspects of the disclosure constitute example means for performing live migration with memory state sharing. For example the elements illustrated in the figures such as when encoded to perform the operations illustrated in the figures constitute example means for transferring one or more memory blocks from a parent VM to one or more target hosts example means for receiving a request to migrate a child VM associated with the parent VM example means for identifying one or more memory blocks in the child VM that are unique to the child VM and example means for transferring the one or more identified memory blocks to the one or more target hosts.

At least a portion of the functionality of the various elements illustrated in the figures may be performed by other elements in the figures or an entity e.g. processor web service server application program computing device etc. not shown in the figures. While some of the examples are described with reference to products or services offered by VMware Inc. aspects of the disclosure are operable with any form type origin or provider of the products or services described.

In some examples the operations illustrated in the figures may be implemented as software instructions encoded on a computer readable medium in hardware programmed or designed to perform the operations or both. For example aspects of the disclosure may be implemented as a system on a chip or other circuitry including a plurality of interconnected electrically conductive elements.

The order of execution or performance of the operations in examples of the disclosure illustrated and described herein is not essential unless otherwise specified. That is the operations may be performed in any order unless otherwise specified and examples of the disclosure may include additional or fewer operations than those disclosed herein. For example it is contemplated that executing or performing a particular operation before contemporaneously with or after another operation is within the scope of aspects of the disclosure.

When introducing elements of aspects of the disclosure or the examples thereof the articles a an the and said are intended to mean that there are one or more of the elements. The terms comprising including and having are intended to be inclusive and mean that there may be additional elements other than the listed elements. The term exemplary is intended to mean an example of. 

Having described aspects of the disclosure in detail it will be apparent that modifications and variations are possible without departing from the scope of aspects of the disclosure as defined in the appended claims. As various changes could be made in the above constructions products and methods without departing from the scope of aspects of the disclosure it is intended that all matter contained in the above description and shown in the accompanying drawings shall be interpreted as illustrative and not in a limiting sense.

