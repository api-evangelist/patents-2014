---

title: System and method for managing data policies on application objects
abstract: Described herein are systems and methods for providing data policy management over application objects in a storage system environment. An application object may comprise non-virtual or virtual objects (e.g., non-virtual-based applications, virtual-based applications, or virtual storage components). An application object manager may represent application objects by producing mapping graphs and/or application object data that represent application objects in a standardized manner. A mapping graph for an application object may describe a mapping between the application object and its underlying storage objects on a storage system. Application object data may describe a mapping graph in a standardized format. Application object data representing application objects may be received by an application policy manager that manages data policies on the application objects (including virtual applications and virtual storage components) based on the received application object data. Data policies may include policies for backup, service level objectives, recovery, monitoring and/or reporting.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09275083&OS=09275083&RS=09275083
owner: NETAPP, INC.
number: 09275083
owner_city: Sunnyvale
owner_country: US
publication_date: 20140124
---
This application is a continuation of claims the benefit of and priority to previously filed U.S. patent application Ser. No. 12 939 098 filed Nov. 3 2010 entitled SYSTEM AND METHOD FOR MANAGING DATA POLICIES ON APPLICATION OBJECTS now U.S. Pat. No. 8 650 165 the subject matter of which is hereby incorporated by reference in its entirety.

The present invention relates to storage systems and more specifically to a system and method for representing application objects in standardized form for policy management.

A storage system typically comprises one or more storage devices into which information may be entered and from which information may be obtained as desired. The storage system includes a storage operating system that functionally organizes the system by inter alia invoking storage operations in support of a storage service implemented by the system. The storage system may be implemented in accordance with a variety of storage architectures including but not limited to a network attached storage environment a storage area network and a disk assembly directly attached to a client or host computer. The storage devices are typically disk drives organized as a disk array wherein the term disk commonly describes a self contained rotating magnetic media storage device. The term disk in this context is synonymous with hard disk drive HDD or direct access storage device DASD .

The storage operating system of the storage system may implement a high level module such as a file system to logically organize the information stored on volumes as a hierarchical structure of data containers such as files and logical units LUs . A known type of file system is a write anywhere file system that does not overwrite data on disks. An example of a write anywhere file system that is configured to operate on a storage system is the Write Anywhere File Layout WAFL file system available from NetApp Inc. Sunnyvale Calif.

The storage system may be further configured to allow many servers to access data containers stored on the storage system. In this model the server may execute an application such as a database application that connects to the storage system over a computer network such as a point to point link shared local area network LAN wide area network WAN or virtual private network VPN implemented over a public network such as the Internet. Each server may request the data services of the storage system by issuing access requests read write requests as file based and block based protocol messages in the form of packets to the system over the network.

A plurality of storage systems may be interconnected to provide a storage system architecture configured to service many server. In some embodiments the storage system architecture provides one or more aggregates each aggregate comprising a set of one or more storage devices e.g. disks . Each aggregate may store one or more storage objects such as and one or more volumes. The aggregates may be distributed across a plurality of storage systems interconnected as a cluster. The storage objects e.g. volumes may be configured to store content of data containers such as files and logical units served by the cluster in response to multi protocol data access requests issued by servers.

Each storage system node of the cluster may include i a storage server referred to as a D blade adapted to service a particular aggregate or volume and ii a multi protocol engine referred to as an N blade adapted to redirect the data access requests to any storage server of the cluster. In the illustrative embodiment the storage server of each storage system is embodied as a disk element D blade and the multi protocol engine is embodied as a network element N blade . The N blade receives a multi protocol data access request from a client converts that access request into a cluster fabric CF message and redirects the message to an appropriate D blade of the cluster.

The storage systems of the cluster may be configured to communicate with one another to act collectively to increase performance or to offset any single storage system failure within the cluster. The cluster provides data service to servers by providing access to a shared storage comprising a set of storage devices . Typically servers will connect with a storage system of the cluster for data access sessions with the storage system. During a data access session with a storage system a server may submit access requests read write requests that are received and performed by the storage system. Each server typically executes numerous applications requiring the data services of the cluster.

A current trend in storage system environments is to virtualize application and storage resources in the cluster. A virtual server environment may typically include multiple physical servers accessing the storage system having multiple storage devices for storing client data. Each server may include multiple virtual machines VMs that reside and execute on the server. Each VM sometimes referred to as a virtual application or virtual server may comprise a separate encapsulation or instance of a separate operating system and one or more applications that execute on the server. As such each VM on a server may have its own operating system and set of applications and function as a self contained package on the server and multiple operating systems may execute simultaneously on the server.

Each VM on a server may be configured to share the hardware resources of the server. Each server may include a VM manager module engine e.g. VMware ESX Microsoft Hyper V Citrix XenServer etc. that executes on the server to produce and manage the VMs. The VM manager module engine may also virtualize the hardware and or software resources of the servers for use by the VMs. The operating system of each VM may utilize and communicate with the resources of the server via the VM manager engine. The virtual server environment may also include a plurality of clients connected with each server for accessing client data stored on the storage system. Each client may connect and interface interact with a particular VM of a server to access client data of the storage system. From the viewpoint of a client the VM may comprise a virtual server that appears and behaves as an actual physical server or behaves as an actual desktop machine. For example a single server may by virtualized into 1 2 4 8 or more virtual servers or virtual desktops each running their own operating systems and each able to support one or more applications.

A storage system may be configured to allow servers to access its data for example to read or write data to the storage system. A server may execute an application that connects to the storage system over a computer network such as a shared local area network LAN a wide area network WAN or a virtual private network VPN implemented over a public network such as the Internet. The application may send an access request read or write request to the storage system for accessing particular data stored on the storage system. Each server may also include multiple VMs each VM being used by and connected with a client through a computer network. Each VM may also execute an application for sending read write requests received from the connected client for accessing data on the storage system. The VM applications executing on the server may service the connected clients by receiving the client access requests and submitting the access requests to the storage system for execution.

There are several advantages in implementing VMs on a server. Having multiple VMs on a single server enables multiple clients to use multiple different operating systems executing simultaneously on the single server. Also multiple VMs executing their own applications may be logically separated and isolated within a server to avoid conflicts or interference between the applications of the different VMs. As each VM is separated and isolated from other VMs a security issue or application crash in one VM does not affect the other VMs on the same server. Also VMs can rapidly and seamlessly be shifted from one physical server to any other server and optimally utilize the resources without affecting the applications. Such a virtualization of the servers and or virtualization of the storage network environment allows for efficiency and performance gains to be realized.

As discussed above the VM manager module engine of a physical server may virtualize the hardware and or software resources for use by the VMs. For each physical server these resources may include storage resources objects e.g. volumes logical units etc. distributed on one or more storage systems. Each storage system may allocate its storage objects to one or more physical servers each allocated storage object being mounted i.e. made available to a particular physical server. For example a storage system may allocate one or more logical units LUs to a physical server each LU being mounted and made available to the physical server. Each physical server may have one or more LUs available for use from one or more storage systems. A mounted storage object may appear to the server as a direct attached physical storage device such as a direct attached Small Computer System Interface SCSI or Serial ATA SATA disk device.

Some or all storage resources objects e.g. LUs that are made available to a physical server may be virtualized by the VM manager module for use by the VMs. The VM manager module may virtualize the storage objects by producing virtual storage components for use by the VMs and virtual storage information that describes these virtual storage components. For example a virtual storage component may comprise a virtual hard disk VHD or virtual machine disk VMDK allocated to a VM. For each VM the VM manager module may allocate one or more virtual storage components for use by the VM for accessing and storing data. The virtual storage information may be used by a VM to locate and access its virtual storage component s .

To a VM each virtual storage component may appear as a directly attached physical storage device e.g. a drive or disk that is directly accessed by the VM. But in fact each virtual storage component is supported by an underlying corresponding storage resource object residing somewhere on one of the storage systems. As used herein an underlying storage object corresponding to a virtual storage component comprises the storage object on the storage system that stores the actual data for the virtual storage component. As such data accesses e.g. read write accesses to and from the virtual storage component by the VM ultimately comprises data accesses to and from the underlying storage object corresponding to the virtual storage component.

Another current trend in storage system environments is to provide policy management over storage objects stored on the storage systems. Policy management may provide services such as maintenance monitoring backup etc. of the storage objects. Current policy management programs however do not provide policy management at a higher level than the storage objects on the storage systems and do not provide policy management for any virtual entities such as virtual applications and virtual storage components . As such there is a need for a system and method for providing a more flexible and efficient higher level policy management for non virtual as well as virtual applications and storage resources.

The embodiments described herein provide a systems and method for managing application objects and storage objects in a storage system. The storage system may provide data services to a plurality of application objects. The data services may comprise at least one underlying storage object for each application object. An application policy manager may be configured for receiving application object data regarding each of the plurality of application objects. The application object data may comprise at least one attribute of each of the application objects as well as information to locate within the storage system at least one underlying storage object of each of the plurality of application objects.

Based on the received application object data the application policy manager may group the plurality of application objects into a plurality of datasets. Each of the plurality of datasets comprises at least two application objects. In some embodiments an application object may comprise either a virtual object or a non virtual object. For example an application object may comprise a virtual based application a non virtual based application or a virtual storage component used by a virtual based application.

The application policy manager may then assign at least one application policy to each of the datasets. In some embodiments an application policy may comprise a storage system characteristic for the application object and its underlying storage objects. In some embodiments an application policy may comprise a backup policy BRAC policy or a service level objective.

In some embodiments the application policy manager may then implement the storage system characteristic from the application policy onto each of the application objects. The implementation of the storage system characteristic onto an application object may further be implemented on each of the underlying storage objects of the application object. In some embodiments the implementation of the storage system characteristic on an application object s underlying storage object is accomplished by using the received application object data that may comprise information to locate within the storage system the underlying storage object of an application object.

In the following description numerous details are set forth for purpose of explanation. However one of ordinary skill in the art will realize that the embodiments described herein may be practiced without the use of these specific details. In other instances well known structures and devices are shown in block diagram form in order to not obscure the description with unnecessary detail.

The description that follows is divided into four sections. Section I contains terms used herein. Section II describes a virtual server environment in which some embodiments operate. Section III describes an application object manager for representing application objects in a standardized manner. Section IV describes an application policy manager for managing data policies for application objects.

Application object As used herein an application object may comprise an application or a virtual storage component used by a virtual based application. An application may comprise a non virtual based application e.g. email or database application or a virtual application e.g. VM . An application object may comprise a non virtual object or virtual object. A non virtual object may comprise a non virtual based application. A virtual object may comprise a virtual based application or a virtual storage component used by a virtual based application. As such an application object may comprise a non virtual application a virtual application or a virtual storage component used by a virtual application. An application object may be represented described by a mapping graph and or application object data comprising graph metadata and graph data .

Application object manager engine As used herein an application object manager engine may reside on a host server and be used for representing describing application objects in a standardized manner. In some embodiments the application object manager engine may do so by producing mapping graphs and or application object data that represents describes application objects in a standardized manner. The application object manager engine may sometimes be referred to as a host service engine.

Application policy manager engine As used herein an application policy manager engine may reside on a management server and be used for managing data policies on application objects. In some embodiments the application object data representing application objects are received from the application object manager engine and the application policy manager engine manages data policies on the application objects based on the received application object data. The application policy manager engine may sometimes be referred to as a central service engine.

Cluster storage system As used herein a cluster storage system may comprise a set of one or more storage systems. In some embodiments the cluster may comprise one storage system. As such the terms cluster and storage system may sometimes be used interchangeably. In other embodiments a cluster comprises a plurality of storage systems.

Storage object As used herein a storage object comprises any logically definable storage element hosted stored or contained within a cluster storage system. Each storage object may be stored in one or more storage systems on one or more storage devices of the cluster storage system. A non exhaustive list of storage object examples include aggregates volumes or virtual volumes e.g. flexible volumes logical units LUs in a q tree q trees in a volume etc. In other embodiments storage objects comprise any other logically definable storage element stored or contained within the cluster storage system.

Virtual object As used herein a virtual object may comprise a virtual based application or a virtual storage component used by a virtual based application. For example in a VMware environment a virtual object may comprise a virtual application such as a VM or a virtual storage component such as a datastore or virtual machine disk VMDK . For example in a Hyper V environment a virtual object may comprise a virtual application such as a VM or a virtual storage component such as a virtual hard drive VHD or local drive. In some embodiments a virtual storage component may reside and be hosted on a server and does not reside on a storage system. A virtual storage component may have an underlying storage object stored on a storage system.

A client system may comprise a computer system that may interact with a server system for submitting read write access requests and for receiving or transmitting data from or to the server systems over the network . In a virtual server environment a client system may interact over the network with one or more virtual machines VMs executing on a server system for submitting read write access requests and for receiving or transmitting data from or to the storage system over the network .

A server system may comprise a computer system that may execute one or more applications shown as that interacts with the client systems for receiving read write access requests and receiving or transmitting data from or to the client systems over the network . A server system may be connected to the client systems over a network such as a local area network LAN an Ethernet subnet a PCI or PCIe subnet a switched PCIe subnet a wide area network WAN a metropolitan area network MAN the Internet or the like. In some embodiments a server system may comprise a chassis hosting multiple instances of server systems each server system hosting multiple client systems embodied as virtual machines one virtual machine per each client system . The network and or subnets of networks may be physically embodied within such a chassis.

An application executing on a server system may provide data access services to client systems by receiving and processing access requests from the client systems for data from the storage system s . In turn an application utilizes the services of the storage system to access store and manage data in a set of storage devices . As such a server system may execute one or more applications that submit access requests for accessing particular storage objects on the storage devices. Each application may submit access requests for accessing particular storage objects on the storage systems of the cluster and the cluster may perform the received requests on the storage objects. An application may comprises a non virtual based application such as a typical email exchange application or database application. In other embodiments an application may comprise a virtual based application such as a virtual machine discussed below .

A storage system may be coupled locally to a server system over a network such as a local area network LAN an Ethernet subnet a PCI or PCIe subnet a switched PCIe subnet a wide area network WAN a metropolitan area network MAN the Internet or the like. In some embodiments a server system may comprise a chassis hosting multiple instances of server systems within a single chassis e.g. a blade server chassis with each instance of a server system in communication with each other instance of a server system in the chassis via network .

Interaction between the server systems and the storage system s can enable the provision of storage services. That is the server systems may request the services of the storage system s by submitting read write access requests and the storage system s may respond to read write access requests of the server systems by receiving or transmitting data to the server systems over the network e.g. by exchanging data packets through a connection over the network .

Communications between a storage system and any of server systems are typically embodied as packets sent over the computer network . A server system may send an access request a read write access request to the storage system for accessing particular data stored on the storage system. The server system may request the services of the storage system by issuing storage access protocol messages formatted in accordance with a conventional storage access protocol for accessing storage devices such as CIFS NFS etc. . Access requests e.g. read write access requests may be implemented by issuing packets using file based access protocols such as the Common Internet File System CIFS protocol or Network File System NFS protocol over the Transmission Control Protocol Internet Protocol TCP IP when accessing data in the form of files and directories. Alternatively the server system may issue access requests by issuing packets using block based access protocols such as the Fibre Channel Protocol FCP or Internet Small Computer System Interface iSCSI Storage Area Network SAN access when accessing data in the form of blocks.

Each application executing on a server system may utilize services of the cluster to store and access its data. The storage system may comprise a computer system that stores data in a set of one or more storage devices as storage objects. A storage device may comprise writable storage device media such as disk devices video tape optical devices DVD magnetic tape flash memory Magnetic Random Access Memory MRAM Phase Change RAM PRAM or any other similar media adapted to store information including data and parity information .

As known in the art a storage device may comprise storage objects comprising one or more storage volumes where each volume has a file system implemented on the volume. A file system implemented on the storage devices may provide multiple directories in a single volume each directory containing zero or more filenames. A file system provides a logical representation of how data files are organized on a volume where data files are represented as filenames that are organized into one or more directories. Examples of common file systems include New Technology File System NTFS File Allocation Table FAT Hierarchical File System HFS Universal Storage Device Format UDF UNIX file system and the like. For the Data ONTAP storage operating system available from NetApp Inc. of Sunnyvale Calif. which may implement a Write Anywhere File Layout WAFL file system there is typically a WAFL file system within each volume and within a WAFL file system there may be one or more logical units LUs .

Whereas servers of a NAS based network environment have a storage viewpoint of files the servers of a SAN based network environment have a storage viewpoint of blocks or disks. To that end the multi protocol storage system presents exports disks to storage area network SAN servers through the creation of vdisk objects. A vdisk object hereinafter vdisk is a special file type that is implemented by the virtualization system and translated into an emulated disk as viewed by the SAN servers. The multi protocol storage system thereafter makes these emulated disks accessible to the SAN servers through controlled exports as described further herein. A vdisk may also be referred to as a logical unit LU having an associated logical unit number LUN that uniquely identifies the vdisk LU within a volume of the storage system. In some embodiments a volume may only contain a limited number of vdisks LUs up to a maximum number of allowed vdisks LUs.

In the illustrative embodiment the memory comprises storage locations that are addressable by the processor and adapters for storing software program code and data structures. The processor and adapters may in turn comprise processing elements and or logic circuitry configured to execute the software code and manipulate the various data structures. The storage operating system portions of which are typically resident in memory and executed by the processing elements functionally organizes the storage system by inter alia invoking storage operations in support of the storage service implemented by the storage system. It will be apparent to those skilled in the art that other processing and memory implementations including various computer readable media may be used for storing and executing program instructions pertaining to the inventive system and method described herein.

The network adapter couples the storage system to a plurality of servers over point to point links wide area networks virtual private networks implemented over a public network Internet or a shared local area network hereinafter referred to as an illustrative Ethernet network . Therefore the network adapter may comprise a network interface card NIC having the mechanical electrical and signaling circuitry needed to connect the storage system to a network switch such as a conventional Ethernet switch . For this NAS based network environment the servers are configured to access information stored on the multi protocol storage system as files. The servers communicate with the storage system over network by exchanging discrete frames or packets of data according to pre defined protocols such as the Transmission Control Protocol Internet Protocol TCP IP .

The servers may be general purpose computers configured to execute applications over a variety of operating systems including the UNIX and Microsoft Windows operating systems. Server systems generally utilize file based access protocols when accessing information in the form of files and directories over a NAS based network. Therefore each server may request the services of the storage system by issuing file access protocol messages in the form of packets to the storage system over the network . For example a server running the Windows operating system may communicate with the storage system using the Common Internet File System CIFS protocol. On the other hand a server running the UNIX operating system may communicate with the multi protocol storage system using the Network File System NFS protocol over TCP IP . It will be apparent to those skilled in the art that other servers running other types of operating systems may also communicate with the integrated multi protocol storage system using other file access protocols.

The storage network target adapter also couples the multi protocol storage system to servers that may be further configured to access the stored information as blocks or disks. For this SAN based network environment the storage system is coupled to an illustrative Fibre Channel FC network . FC is a networking standard describing a suite of protocols and media that is primarily found in SAN deployments. The network target adapter may comprise a FC host bus adapter HBA having the mechanical electrical and signaling circuitry needed to connect the storage system to a SAN network switch such as a conventional FC switch . In addition to providing FC access the FC HBA may offload fibre channel network processing operations for the storage system.

The servers generally utilize block based access protocols such as the Small Computer Systems Interface SCSI protocol when accessing information in the form of blocks disks or vdisks over a SAN based network. SCSI is an input output I O interface with a standard device independent protocol that allows different peripheral devices such as disks to attach to the storage system . In SCSI terminology servers operating in a SAN environment are initiators that initiate requests and commands for data. The multi protocol storage system is thus a target configured to respond to the requests issued by the initiators in accordance with a request response protocol. The initiators and targets have endpoint addresses that in accordance with the FC protocol comprise worldwide names WWN . A WWN is a unique identifier e.g. a node name or a port name consisting of an 8 byte number.

The multi protocol storage system supports various SCSI based protocols used in SAN deployments including SCSI encapsulated over TCP iSCSI SCSI encapsulated over FC FCP and Fibre Channel Over Ethernet FCoE . The initiators hereinafter servers may thus request the services of the target hereinafter storage system by issuing iSCSI and FCP messages over the network to access information stored on the disks. It will be apparent to those skilled in the art that the servers may also request the services of the integrated multi protocol storage system using other block access protocols. By supporting a plurality of block access protocols the multi protocol storage system provides a unified and coherent access solution to vdisks LUs in a heterogeneous SAN environment.

The storage adapter cooperates with the storage operating system executing on the storage system to access information requested by the servers. The information may be stored on the disks or other similar media adapted to store information. The storage adapter includes I O interface circuitry that couples to the disks over an I O interconnect arrangement such as a conventional high performance FC serial link topology. The information is retrieved by the storage adapter and if necessary processed by the processor or the adapter itself prior to being forwarded over the system bus to the network adapters where the information is formatted into packets or messages and returned to the servers.

Storage of information on the storage system is preferably implemented as one or more storage volumes that comprise a cluster of physical storage disks defining an overall logical arrangement of disk space. The disks within a volume are typically organized as one or more groups of Redundant Array of Independent or Inexpensive Disks RAID . RAID implementations enhance the reliability integrity of data storage through the writing of data stripes across a given number of physical disks in the RAID group and the appropriate storing of redundant information with respect to the striped data. The redundant information enables recovery of data lost when a storage device fails. It will be apparent to those skilled in the art that other redundancy techniques such as mirroring may be used in accordance with the present invention.

In accordance with an illustrative embodiment of the present invention a server includes various software layers or modules executing thereon. For example the server may be executing a Network File System NFS layer that implements the NFS protocol and cooperates with a TCP IP layer to enable the server to access files stored on the storage system using the NFS protocol. The server may also include a Fibre Channel FC driver for communicating with the storage system utilizing the Fibre Channel protocol.

A server may also execute in an illustrative embodiment a mount daemon which interacts with the storage operating system of the storage system to enable transparent access to blocks such as vdisks stored on a storage system using a file based protocol such as NFS. The mount daemon operates in conjunction with the NFS Proxy layer described further below to provide appropriate device addresses to the storage system . The mount daemon may be implemented as a background process thread or may be a remotely callable library of procedures that performs the various functionality described below. A method and apparatus for allowing a server transparent access to blocks such as vdisks stored on a storage system using a file based protocol. The process of allowing a server transparent access to a vdisk using a file based protocol may sometimes be referred to herein as transparent access process technique. 

It should be noted that the software layers that are shown for server are exemplary only and that they may be varied without departing from the spirit and scope of the invention. Additionally it should be noted that the NFS layer is shown for exemplary purposes only. Any file based protocol may be utilized in accordance with the teachings of the present invention including for example CIFS.

To facilitate access to the disks the storage operating system implements a write anywhere file system of a virtualization system that virtualizes the storage space provided by disks . The file system logically organizes the information as a hierarchical structure of named directory and file objects hereinafter directories and files on the disks. Each on disk file may be implemented as set of disk blocks configured to store information such as data whereas the directory may be implemented as a specially formatted file in which names and links to other files and directories are stored. The virtualization system allows the file system to further logically organize information as a hierarchical structure of named vdisks on the disks thereby providing an integrated NAS and SAN storage system approach to storage by enabling file based NAS access to the named files and directories while further enabling block based SAN access to the named vdisks on a file system based storage platform. The file system simplifies the complexity of management of the underlying physical storage in SAN deployments.

As noted a vdisk is a special file type in a volume that derives from a normal regular file but that has associated export controls and operation restrictions that support emulation of a disk. Unlike a file that can be created by a server using e.g. the NFS or CIFS protocol a vdisk is created on the multi protocol storage system via e.g. a user interface UI as a special typed file object . Illustratively the vdisk is a multi inode object comprising a special file inode that holds data and at least one associated stream inode that holds attributes including security information. The special file inode functions as a main container for storing data such as application data associated with the emulated disk. The stream inode stores attributes that allow LUs and exports to persist over e.g. reboot operations while also enabling management of the vdisk as a single disk object in relation to NAS servers.

In the illustrative embodiment the storage operating system may comprise Data ONTAP storage operating system available from NetApp Inc. of Sunnyvale Calif. which implements a Write Anywhere File Layout WAFL file system. However it is expressly contemplated that any appropriate storage operating system including a write in place file system may be enhanced for use in accordance with embodiments described herein. As such where the term WAFL is employed it should be taken broadly to refer to any storage operating system that is otherwise adaptable to the teachings of this embodiment.

As used herein the term storage operating system generally refers to the computer executable code operable on a computer that manages data access and may in the case of a multi protocol storage system implement data access semantics such as the Data ONTAP storage operating system which is implemented as a microkernel. The storage operating system can also be implemented as an application program operating over a general purpose operating system such as UNIX or Windows or as a general purpose operating system with configurable functionality which is configured for storage applications as described herein.

In addition it will be understood to those skilled in the art that the inventive system and method described herein may apply to any type of special purpose e.g. storage serving storage system or general purpose computer including a standalone computer or portion thereof embodied as or including a storage system. Moreover the teachings of this embodiment can be adapted to a variety of storage system architectures including but not limited to a network attached storage environment a storage area network and disk assembly directly attached to a server or host computer. The term storage system should therefore be taken broadly to include such arrangements in addition to any subsystems configured to perform a storage function and associated with other equipment or systems.

The file system protocol layer also includes in the illustrative embodiment a NFS proxy layer . In some embodiments the NFS proxy layer examines each NFS Open or look up commands received from a server to determine if the command is to utilize the transparent access technique. The NFS proxy layer performs this function by examining the filename field of the received Open command described further below. It should be noted that an NFS proxy layer is shown for exemplary purposes only. The teachings of the present embodiment may be utilized with any file based protocol including for example CIFS or HTTP. In such alternate embodiments an appropriate proxy layer would be implemented within the storage operating system.

An iSCSI driver layer provides block protocol access over the TCP IP network protocol layers while a FC driver layer operates with the FC HBA to receive and transmit block access requests and responses to and from the integrated storage system. The FC and iSCSI drivers provide FC specific and iSCSI specific access control to the LUs vdisks and thus manage exports of vdisks to either iSCSI or FCP or alternatively to both iSCSI and FCP when accessing a vdisk on the multi protocol storage system. In addition the storage operating system includes a disk storage layer that implements a disk storage protocol such as a RAID protocol and a device driver layer e.g. disk driver layer that implements a device control protocol such as small computer system interface SCSI integrated drive electronics IDE etc. .

Bridging the disk software layers with the integrated network protocol stack layers is a virtualization system . The virtualization system is implemented in the illustrative embodiment by a file system cooperating with virtualization modules illustratively embodied as e.g. vdisk module and SCSI target module . It should be noted that the vdisk module file system and SCSI target module can be implemented in software hardware firmware or a combination thereof.

The vdisk module interacts with the file system to provide a data path from the block based SCSI target module to blocks managed by the file system. In essence the vdisk module manages SAN deployments by among other things implementing a comprehensive set of vdisk LU commands that are converted to primitive file system operations primitives and that interact with the file system and the SCSI target module to implement the vdisks.

The SCSI target module in turn initiates emulation of a disk or LU by providing a mapping procedure that translates logical block access to LUs specified in access requests into virtual block access to the special vdisk file types and for responses to the requests vdisks into LUs. The SCSI target module is illustratively disposed between the iSCSI and FC drivers and the file system to thereby provide a translation layer of the virtualization system between the SAN block LU space and the file system space where LUs are represented as vdisks.

In addition the SCSI target module includes one or more transparent access processes . The transparent access processes described further below enable servers to transparently access a vdisk by utilizing a file based protocol. These transparent processes cooperate with the mount daemon executing on the server to implement the novel system and method for transparently accessing vdisks using a file based protocol.

The file system illustratively implements the WAFL file system having an on disk format representation that is block based using e.g. 4 kilobyte KB blocks and using inodes to describe the files. The WAFL file system uses files to store metadata describing the layout of its file system these metadata files include among others an inode file. A file handle i.e. an identifier that includes an inode number is used to retrieve an inode from disk.

Broadly stated all inodes of the file system are organized into the inode file. A file system FS info block specifies the layout of information in the file system and includes an inode of a file that includes all other inodes of the file system. Each volume has an FS info block that is preferably stored at a fixed location within e.g. a RAID group of the file system. The inode of the root FS info block may directly reference point to blocks of the inode file or may reference indirect blocks of the inode file that in turn reference direct blocks of the inode file. Within each direct block of the inode file are embedded inodes each of which may reference indirect blocks that in turn reference data blocks of a file or vdisk.

As described above the transparent access process technique may enable an application to issue a file based protocol Open command and transparently access a virtual disk stored on a storage system using a block based protocol. The server may utilize the convenient namespace of the file based protocol while obtaining the performance benefits of the high speed data access path associated with the vdisk. The server may execute a mount daemon that interacts with a NFS proxy layer or other file based protocol proxy layer executing on the storage system .

The transparent access is initiated by prepending a predetermined and special prefix to a filename contained in an Open command sent through the file based protocol. The NFS proxy layer executing within the storage operating system of the storage system identifies that the Open command is directed to a filename that contains the predetermined prefix and initiates the transparent access process file to vdisk conversion . The NFS proxy layer in conjunction with the SCSI target module of the storage operating system ensures that the file requested to be opened is represented as a vdisk which is exported to the requesting server. If the file to be opened is not already a vdisk the procedure also converts it to one using conventional file to vdisk conversion routines. The storage system then communicates with the mount daemon executing on the server . The mount daemon ensures that the exported vdisk is mapped to the server and if it is not already causes the server to rescan the SCSI devices connected thereto to identify the newly mounted vdisk. Thus a server may open a vdisk utilizing a file based protocol and its associated namespace but perform later data access operations using a block based protocol data path.

Thus applications and clients users of servers served by the storage system may utilize the namespace available from the file based protocol while benefiting from the high speed data connection provided by an underlying transport media such as Fibre Channel connected to the vdisk. This enables an application executing on a server of the storage system to issue an Open command to access the vdisk using a conventional file based protocol such as the NFS protocol. The application may invoke the use of a transparent access process executing on the storage system by prepending a predetermined and special prefix to the filename in the Open command.

A NFS proxy layer of a storage operating system checks the filename sent in the filename field of the Open command. If the filename does not include the special prefix then the storage operating system processes the request using the conventional file based protocol. However if the file name includes the predetermined and special prefix then the NFS proxy layer maps the vdisk associated with the file to be opened to the server that issued the Open command. A logical unit number LUN associated with this vdisk LU is also mapped to the server by the NFS proxy layer. The NFS proxy layer also via a remote procedure call RPC or similar application to application communication method communicates with a mount daemon executing on the server.

The mount daemon communicates to the storage system a set of major and minor device numbers to be associated with a newly created character device instance. A character device is a block level interface that permits an arbitrary number of bytes to be written to a device. Typically the number of bytes written is a multiple of the size of the data blocks on the storage device. Using the returned major minor numbers the NFS proxy layer generates a response to the Open command and returns that response to the server. The server upon receiving the response generates a new instance of a character device using the major minor device numbers. The server then returns a file descriptor to the issuing application.

Thus to the issuing application and users thereof the file has been opened using conventional NFS or other file based protocol commands and its associated namespace. However the NFS proxy layer in conjunction with the mount daemon has generated a new character device on the server that enables the server to read and write raw data over the Fibre Channel or other block based network infrastructure. Thus a server experiences the ease of use of the file based namespace while benefiting from the performance of the block based network infrastructure.

In some embodiments the storage system is used in a virtual server environment. shows a conceptual diagram of a virtual server environment comprising a plurality of clients and a plurality of physical servers . . . accessing a plurality of storage systems comprising a cluster . Each storage system may include a set of storage devices not shown storing a set of storage objects shown as e.g. volumes logical units LUs q trees etc. for storing data of an application . For illustrative purposes in the embodiments discussed below an application comprises a virtual based application such as a virtual machine VM . However embodiments described below may also apply to an application comprising a non virtual based application.

Each server may include one or more VM VMs shown as that reside and execute on the server . A VM may be referred to as a virtual based application or virtual application. Each VM may comprise a separate encapsulation or instance of a separate operating system and one or more applications that execute on the server. As such each VM on a server may have its own operating system and set of applications and function as a self contained package on the server and multiple operating systems may execute simultaneously on the server.

Each VM on a server may be configured to share the hardware resources of the server. Each server may also include a VM manager module engine that executes on the server to produce and manage the VMs. The VM manager module engine may also virtualize the hardware and or software resources including virtual storage components of the servers for use by the VMs . The operating system of each VM may utilize and communicate with the resources of the server via the VM manager.

The virtual server environment may also include a plurality of clients connected with each server for accessing client data stored on the storage system . Each client may connect and interface interact with a particular VM of a server to access client data of the storage system. From the viewpoint of a client the VM may comprise a virtual server that appears and behaves as an actual physical server or behaves as an actual desktop machine.

A storage system may be configured to allow servers to access its data for example to read or write data to the storage system. A server may execute an application that connects to the storage system over a computer network to send an access request read or write request to the storage system for accessing particular data stored on the storage system. Each server may also include multiple VMs each VM being used by and connected with a client through a computer network. Each VM may also execute an application for sending read write requests received from the connected client for accessing data on the storage system. The VM applications executing on the server may service the connected clients by receiving the client access requests and submitting the access requests to the storage system for execution.

Typically different types of VM manager modules engines may be used e.g. VMware ESX Microsoft Hyper V etc. . Each different type of VM manager may produce a different type of VM using a different type of virtual storage component. In particular a first VM produced by a first type of VM manager e.g. VMware ESX may have different attributes than a second VM produced by a second type of VM manager e.g. VMware Microsoft Hyper V . As such the attributes of a VM may differ depending on the type of VM manager module engine used to produce the VM. In addition a first set of virtual storage components allocated to the first VM may have different attributes than a second set of virtual storage components allocated to the second VM. As such the organization and mapping of a VM to its virtual storage components on the server to its underlying storage objects on the cluster may differ depending on the type of VM manager module engine used to produce the VM and virtual storage components.

The server processors are the central processing units CPUs of the server system and thus control the overall operation of the server system . Server processors may include one or more programmable general purpose or special purpose microprocessors digital signal processors DSPs programmable controllers application specific integrated circuits ASICs programmable logic devices PLDs or the like or a combination of such devices.

A server network adapter may comprise mechanical electrical and signaling circuitry needed to connect the server system to the network and to receive and transmit data over the network. The server network adapter may comprise a network port controller e.g. Ethernet cards specialized network adapters or any other physical device that controls the receiving and transmitting of data over a network. A server network adapter may provide one or more network ports i.e. data access ports for coupling the server system to one or more other client systems through a network . A connection with a client system may be established using a network port of the server network adapter to receive and transmit data though the network .

Server memory comprises storage locations that are addressable by the processor and adapters e.g. a server network as well as other devices for storing software program code such as the software described herein. The server processor and server adapters may in turn comprise processing elements and or logic circuitry configured to execute the software code. Server memory can be a random access memory RAM a read only memory ROM or the like or a combination of such devices. It will be apparent to those skilled in the art that other processing and memory means including various computer readable media may be used for storing and executing program instructions pertaining to the invention described herein.

Server memory is for storing software program instructions and data structures such as a server operating system an application object manager module engine one or more virtual machines an application object data structure a management application programming interface MAPI an input output application programming interface IOAPI and a virtual server environment monitoring application VSEMA .

The server operating system may be for example UNIX Windows NT Linux or any other operating system. The server operating system may further include components discussed above in relation to such as mount daemon Fibre Channel FC driver TCP IP Network File System NFS layer not shown . A server system loads information instructions par and data structures into server memory from which they are accessed and executed or processed by server processors via a bus .

The VM manager module engine may comprise any module engine capable of producing and configuring VMs. The server operating system may comprise one or more different types of VM manager modules etc. e.g. VMware ESX Microsoft Hyper V etc. . The different types of VM manager modules may produce different types of VMs .

Server storage may comprise a local writable storage device such as disk devices video tape optical devices DVD magnetic tape flash memory Magnetic Random Access Memory MRAM Phase Change RAM PRAM or any other similar media adapted to store information including data and parity information . As shown in the example of server storage may store VM data and the application object data structure .

A VM may be represented by data that describes the VM referred to herein as VM data . In the example shown in the server storage may store VM data for one or more VMs . VM data may be stored and used later for producing and deploying the VM represented by the VM data on a server . In some embodiments VM data for a VM specifies hardware and or software resources on a server and or storage system that the VM uses during operation. As such a VM may be viewed as being composed of the specified hardware and or software resources.

The VM data for a VM may comprise a set of one or more VM files that describes hardware and or software resources used by the VM. For example the VM data may specify one or more virtual storage components that the VM uses such as one or more virtual hard disks VHDs one or more datastores and or one or more virtual machine disks VMDKs . The VM data may also comprise a VM configuration file specifying various components that the VM uses such as an operating system network adaptor IP address hard disks etc.

In some embodiments an application object manager module engine may reside and execute on the server for performing embodiments described herein. The application object manager module engine may be configured to do so automatically without human initiation interaction or intervention. In some embodiments the application object manager module engine may comprise a software module or hardware engine that resides and executes on a server hosting VMs . In other embodiments the application object manager module engine may reside on a storage system or on a server that does not host VMs and is dedicated for executing the application object manager module engine . In further embodiments the application object manager module engine may be executed by a VM dedicated for executing the application object manager module engine . The application object manager module engine may be referred to as a host service module engine.

The application object manager module engine may be configured to operate in conjunction with other software modules of the server system and software modules of the storage system to collectively perform the embodiments described herein. For example the application object manager module may use functions of a management application programming interface MAPI to perform some embodiments described herein. The management application programming interface may be configured for managing and configuring hardware and software components on the server such as system and network devices . An example of a management application programming interface is the Windows Management Instrumentation WMI program that is typically included in current Windows operating systems.

The application object manager module may also use functions of an input output application programming interface IOAPI to perform some embodiments described herein. The input output application programming interface may be configured for sending low level commands to hardware devices such as storage devices the commands being at a level lower than a file system read write command. An example of a management application programming interface is the Windows IOCTL DeviceIoControl API program

The application object manager module may further use functions of a virtual server environment monitoring application VSEMA to perform some embodiments described herein. The VSEMA may be configured for monitoring objects comprising hardware and or software components throughout a virtual server environment including VMs and storage objects . Objects for monitoring may be added to a database of the VSEMA to initiate monitoring of the objects. In some embodiments the application object manager module may add at least one VM and its corresponding storage objects to the database as objects for monitoring. An example of the VSEMA is the Microsoft System Center Operations Manager SCOM application for monitoring storage system environments.

In some embodiments the application object manager module engine may be configured to operate in conjunction with one or more plug ins to collectively perform the embodiments described herein. In these embodiments different types of plug in may be installed and used for different types of VM managers . Each plug in may be configured specifically for a particular type of VM manager and comprise an interface between the VM manager and the application object manager to perform embodiments described herein. For example a first type of plug in may be installed and used for a first type of VM manager e.g. VMware ESX and a second type of plug in may be installed and used for a second type of VM manager e.g. VMware Microsoft Hyper V .

Some or all storage objects that are made available to a physical server may be virtualized by the VM manager for use by the VMs. The VM manager module may virtualize the storage objects by producing virtual storage components for use by the VMs and virtual storage information that describes these virtual storage components . For each VM the VM manager module may allocate one or more virtual storage components for use by the VM. The virtual storage information may be used by a VM to locate and access its virtual storage component s .

To a VM each virtual storage component may appear as a directly attached physical storage device e.g. a drive or disk that is directly accessed by the VM. But in fact each virtual storage component hosted on a server is supported by an underlying corresponding storage object residing somewhere on one of the storage systems of the cluster as indicated by the dashed lines in .

As discussed above different types of VM manager modules engines may produce different types of VM having different attributes that use different types of virtual storage components. As such the organization and mapping of a VM to its virtual storage components on the server to its underlying storage objects on the cluster may differ depending on the type of VM manager module engine used to produce the VM and the virtual storage components. In the embodiments described below examples are for a VM manager comprising Microsoft Hyper V or VMware ESX to illustrate the differences in virtual storage components between different types of VM managers . However in other embodiments another type of VM manager may be used.

As known in the art for a VHD the server may format an underlying storage object with a file system create a VHD file and expose the VHD file to the VM as a virtual hard disk. As used herein the terms VHD and VHD file may sometimes be used interchangeably. Each VHD may be allocated assigned to a particular VM for its data storage use.

Each VHD may be located and reside on a particular local drive . Each local drive shown as may have a drive identifier e.g. drive D E F etc. and or volume mount point and a volume global Globally Unique IDentifier GUID . Each local drive may map to and be supported by an underlying storage object on a storage system of the cluster . For example for a storage area network SAN environment a storage object may comprise a logical unit LU .

As such each VM produced by the Hyper V VM manager may be allocated at least one VHD each VHD being deployed and residing on a local drive on the server each local drive being mapped to an underlying storage object on a storage system of the cluster . Note that the VM VHD and local drive are all hosted on the server and the underlying storage objects are hosted on the cluster . In some embodiments the application object manager module engine may represent and or describe an application object e.g. VM VHD or local drive in a standardized manner the representation description of the application object including its mappings to the underlying storage objects .

As known in the art a VMDK comprises a disk image file. In turn each VMDK may map to and be supported by an underlying storage object on a storage system shown as of the cluster . For example for a storage area network SAN environment a storage object may comprise a logical unit LU . For example for a network attached storage NAS environment a storage object shown as may comprise a volume.

As such each VM produced by the VMWare VM manager may be allocated at least one VMDK hosted on at least one datastore on the server each datastore being mapped to one or more underlying storage objects on a storage system of the cluster . Note that the VM datastore and VMDK are all hosted on the server and the underlying storage objects are hosted on the cluster . In some embodiments the application object manager module engine may represent and or describe an application object e.g. VM datastore or VMDK in a standardized manner the representation description of the application object including its mappings to the underlying storage objects

The application object manager module engine may be configured to operate in conjunction with modules engines of the server system and modules engines of the storage system to collectively perform the method . For example the application object manager module engine may be configured to operate in conjunction with one or more plug ins to collectively perform the method . In these embodiments a different type of plug in may be installed and used for each different type of application object manager . In some embodiments the application object manager module engine and plug in s may reside and or execute on a server a storage system a dedicated computer system or any combination thereof.

In some embodiments regardless of the type of application object manager used the method may produce a representation description of one or more application objects in a standardized manner the representation description of an application object including its mappings to the underlying storage objects . The method may be initiated by and administrator or initiated automatically e.g. on regular time intervals when a new application object is deployed etc. . The method may be initiated for a particular server to produce a representation description of each application object being hosted on the server . The method may then be repeated for each server .

The method begins by discovering determining at a listing of all application objects hosted and residing on the server and storing the listing to an application object data structure . In some embodiments the listing of application objects comprises a non virtual application e.g. email or database application a virtual application e.g. VM or a virtual storage component used by a virtual application e.g. datastore VMDK VHD local drive etc. or any combination thereof.

In some embodiments the listing of application objects may comprise a listing of unique identifiers of the application objects. For example for an application object comprising a VM when the VM is produced by the VM manager the VM manager may produce and assign a Globally Unique IDentifier GUID to each VM. For virtual storage components each application may assign unique identifiers to its virtual storage components. For example the method may determine at a listing of VMs using a management application programming interface MAPI such as Windows Management Instrumentation WMI . For example to request the application object listing the method may connect to the root virtualization WMI namespace and invoke SELECT FROM Msvm ComputerSystem whereby the response for this command lists the GUIDs of all VMs hosted and executing on the server. In other embodiments the method may use other methods known in the art to determine the list of hosted application objects.

Also the method stores at the listing of discovered application objects to the application object data structure . shows an exemplary application object data structure used in some embodiments. In some embodiments the application object data structure comprises a plurality of application object entries each entry representing an application object. Each entry may comprise a plurality of data fields for storing data describing the corresponding application object. In some embodiments an application object entry representing a application object may contain data fields for an application object identifier a mapping graph graph metadata and graph data . At step the method may produce an entry in the application object data structure for each discovered application object and store data for an application object identifier for each entry . The application object identifier may uniquely identify the application object in the server. In some embodiments the plug in s discovers the application objects and identifiers and sends the information to the application object manager which stores the received information to the application object data structure .

For each discovered application object the method then determines at high level storage object information for each storage object used by the application object. The high level storage object information of a storage object may comprise information used by the application object to locate and access a virtual storage component allocated for use by the application object that corresponds and maps to the storage object. As such the high level storage object information may provide a mapping from the application object to a virtual storage component corresponding to the storage object. The high level storage object information may comprise information used and understood by software and or hardware components on the server to locate storage objects. In some embodiments the plug in s determines the high level storage object information and sends the information to the application object manager .

For example for an application object comprising a VM the high level storage object information may comprise virtual storage component information that describes virtual storage components allocated to the VM. As discussed above the virtual storage information may be used by the VM to locate and access its virtual storage component s . For example for each storage object of a VM the high level storage object information may comprise an identifier for a datastore and an identifier for a VMDK in a VMWare environment or may comprise an identifier for a VHD an identifier for a local drive in a Hyper V environment .

For example for an application object comprising a datastore the high level storage object information may comprise identifiers of one or more VMDKs hosted by the datastore. Note that each datastore comprises a pool of storage resources and hosts VMDKs and builds from VMDKs. For an application object comprising a VMDK the high level storage object information may comprise the identifier of the VMDK itself. For example for an application object comprising a VHD the high level storage object information may comprise an identifier of a VHD file hosted on a file system that the VHD hosts on. For an application object comprising a local drive the high level storage object information may comprise the identifier of the local drive itself.

For each discovered application object the method also determines at low level storage object information for each storage object used by the application object. The low level storage object information may comprise information used by a storage system to locate and access a storage object corresponding to a virtual storage component used by the application object. As such the low level storage object information may provide a mapping from an application object s virtual storage components to its underlying storage objects. The low level storage object information may comprise information used and understood by software and or hardware components on the storage system to locate storage objects corresponding to virtual storage components and may indicate the physical storage location of the storage objects on a storage system . In some embodiments the application object manager determines the low level storage object information.

For example the low level storage object information for a storage object may comprise an identifier for a virtual storage component and an identifier for a storage system and an identifier for a logical unit LU on the storage system the logical unit comprising the storage object that corresponds to and underlies the virtual storage component. For example the low level storage object information for a storage object may comprise an identifier for a virtual storage component and an identifier for a storage system and an identifier for a volume on the storage system the volume comprising the storage object that corresponds to and underlies the virtual storage component.

For each application object the method then determines at a mapping relationship between the application object and each storage object it is allocated and uses for data storage. The method may do so using the high level and low level storage object information for each storage object of an application object. In particular for each storage object the method may combine the high level storage object information which maps the application object to a virtual storage component with the low level storage object information which maps the virtual storage component to its underlying storage object to produce a mapping between the application object and its underlying storage objects. In other embodiments other methods may be used to determine a mapping between the application object and its underlying storage objects.

For each application object the method then produces at a mapping graph representing the application object and describing the mapping between the application object and each of its underlying storage objects and stores the mapping graph to the application object data structure .

A top hierarchical level of the mapping graph may comprise a top server node representing a server that hosts the application object represented by the mapping graph . Below the top hierarchical level the mapping graph may comprise a first set of hierarchical levels comprising one or more levels of application object nodes representing the application object and related application objects residing on the server . The first set of hierarchical levels may include an application object node that represents the application object for which the mapping graph is produced as well as application object nodes that represent related application objects such as virtual storage components that the application object uses. As shown in the example of the first set of levels comprises a parent node representing a VM VM1 the parent node having a first child node representing a first VHD VHD1 and a second child node representing a second VHD VHD2 . The first set of levels also comprises a child node for each VHD node for representing a local drive e.g. Drive D and Drive E that the VHD is located on.

Below the first set of hierarchical levels the mapping graph may also comprise a second set of hierarchical levels comprising one or more levels of storage object nodes representing the storage objects that the application objects represented in the first set of hierarchical levels map to. As shown in the example of the second set of levels comprises a child node for each local drive node for representing a logical unit e.g. LUN1 and LUN2 that the local drive maps to. The second set of levels may also comprise a child node for each logical unit node for representing a volume e.g. VOL1 and VOL2 that the logical unit maps to.

Below the second set of hierarchical levels the mapping graph may also comprise a bottom hierarchical level comprising storage system nodes representing storage systems containing the storage objects represented in the second set of hierarchical levels. As shown in the example of the bottom level comprises a child node for each volume node for representing a storage system e.g. StorageSystem1 and StorageSystem2 that stores the volume.

As such the mapping graph represents an application object e.g. VM by representing describing the mapping relationship between the application object and each of its storage objects. In particular the mapping graph represents describes the relationships between the server on which the application object resides the relationships between the application object and its virtual storage components e.g. VM1 is allocated virtual storage components VHD1 VHD2 local drive D and local drive E the mapping relationships between the virtual storage components e.g. VHD1 is located on local drive D the mapping relationship between the virtual storage components and their underlying storage objects e.g. VHD1 and local drive D maps to volume1 and the storage system that stores the underlying storage objects e.g. StorageSystem1 stores volume1 .

A top hierarchical level of the mapping graph may comprise a top server node representing a server that hosts the application object represented by the mapping graph . Below the top hierarchical level the mapping graph may comprise a first set of hierarchical levels comprising one or more levels of application object nodes representing application objects residing on the server . The first set of hierarchical levels may include an application object node that represents the application object for which the mapping graph is produced as well as application object nodes that represent related application objects such as virtual storage components that the application object uses. As shown in the example of the first set of levels comprises a parent node representing a VM VM1 the parent node having a child node representing each of its allocated VMDKs VMDK 1 and VMDK2 . The first set of levels also comprises child nodes for the VMDK nodes for representing datastores e.g. datastore1 that host each VMDK.

Below the first set of hierarchical levels the mapping graph may also comprise a second set of hierarchical levels comprising one or more levels of storage object nodes representing the storage objects that the application objects represented in the first set of hierarchical levels map to. As shown in the example of the second set of levels comprises a child node for each datastore node for representing a logical unit e.g. LUN1 and LUN2 that the datastore maps to. The second set of levels may also comprise a child node for each logical unit node for representing a volume e.g. VOL1 and VOL2 that the logical unit maps to.

Below the second set of hierarchical levels the mapping graph may also comprise a bottom hierarchical level comprising storage system nodes representing storage systems containing the storage objects represented in the second set of hierarchical levels. As shown in the example of the bottom level comprises a child node for each volume node for representing a storage system e.g. StorageSystem1 and StorageSystem2 that stores the volume.

As such the mapping graph represents an application object e.g. VM by representing describing the mapping relationship between the application object and each of its storage objects. In particular the mapping graph represents describes the relationships between the server on which the application object resides the relationships between the application object and its virtual storage components e.g. VM1 is allocated virtual storage components datastore1 VMDK1 and VMDK2 the mapping relationships between the virtual storage components e.g. VMDK 1 and VMDK2 are hosted on datastore1 the mapping relationship between the virtual storage components and their underlying storage objects e.g. datstore1 maps to volume1 and volume2 and the storage system that stores the underlying storage objects e.g. StorageSystem1 stores volume1 .

For each application object the method also stores at each mapping graph or a pointer to the mapping graph in the entry for the application object in the application object data structure . As shown in each entry for an application object includes a mapping graph field for storing a mapping graph of the application object the mapping graph field comprising data describing the mapping graph or a pointer to the mapping graph .

For each application object the method then produces at application object data describing the mapping graph produced for the application object and stores the application object data to the application object data structure . The application object data may be used by an application policy manager to manage data policies on the application object based on the application object data. The application object data may comprise graph metadata and graph data. In some embodiments the graph metadata comprises information used by the application policy manager for understanding and properly processing the information contained in the graph data. As such the graph metadata describes data in the graph data and the graph data describes the mapping graph.

In some embodiments the graph metadata may specify 1 one or more types of application objects included in the mapping graph 2 data policies that are permitted to be implemented on each type of application object and or 3 features of each type of application object included in the mapping graph. In other embodiments the graph metadata may include other metadata for describing the graph data.

The graph metadata may specify types of application objects in terms of whether the application object is a type of non virtual application a virtual application or virtual storage component. This indicates that the application policy manager will receive these types of application object types in the graph data. For virtual objects the graph metadata may further specify types of application objects in terms of the type of VM manager used to produce the application objects. For example for the mapping graph of the graph metadata may specify the one or more types of application objects included in the mapping graph by specifying the application object types Hyper V VM VHD and local drive. For example for the mapping graph of the graph metadata may specify the one or more types of application objects included in the mapping graph by specifying the application object types VMWare VM datastore and VMDK. As such the application object type also indicates the type of VM manager used to produce the application objects.

Data policies that are permitted to be implemented on an application object may include for example policies for data backup SLO management RBAC management etc. as discussed below in Section IV . For each type of application object in the mapping graph the graph metadata may specify zero or more data policies that are permitted to be implemented for each type of application object in the mapping graph. In some embodiments the type of data policies that may be implemented may vary differ depending on the type of application object. For example a first set of one or more data policies may be permitted for a first type of application object and a second set of one or more data policies may be permitted for a second type of application object the first and second sets being different sets of data policies.

Further for VMs the type of data policies that may be implemented may vary differ depending on the type of VM manager used to produce the VM. For example the type of data policies that may be implemented may differ for a VM produced by a Hyper V VM manager and a VM produced by a VMWare VM manager . The type of data policies that are permitted to be implemented for each type of application object may be specified for example by an administrator or specified by default settings for each type of application object.

Features of each type of application object may comprise various characteristics of the application object type depending on the application object type . A non exhaustive list of examples of features for an application object type include maximum data size creation date modification date date and time of last backup etc.

For each application object the method also stores at the graph metadata to the application object data structure . As shown in each entry for an application object includes a graph metadata field for storing graph metadata for the mapping graph. As shown in the example of the graph metadata field may store data specifying a first data policy permitted to be implemented on a first type of VM and a second data policy permitted to be implemented on a second type of VM the first and second data policies being different data policies e.g. SLO management is permitted for a Hyper V VM and RBAC management is permitted for a VMWare VM .

In some embodiments the graph data describes the various components of the mapping graph including descriptions of the nodes connectors and hierarchical relationships between the nodes. In some embodiments for each node in the mapping graph the graph data specifies 1 the type of node 2 a node identifier and or 3 a node name. In other embodiments the graph data may include other data for describing the mapping graph.

For 1 the type of node the graph data may specify for each node for example a server node representing a server an application object node representing an application object residing on the server a storage object node representing a storage object residing on a storage system or a storage system node representing a storage system . For each application object node the graph data may further specify the type of application object represented by the application object node. Note that the graph data may specify the type of application object in terms of whether the application object is a type of non virtual application a virtual application or virtual storage component. For virtual objects the graph data may further specify types of application objects in terms of the type of VM manager used to produce the application objects such as a Hyper V VM VHD local drive VMWare VM datastore VMDK etc. For each storage object node the graph data may further specify the type of storage object represented by the storage object node such as logical unit or volume etc.

For 2 the node identifier the graph data may specify for each node for example an identifier of the node that uniquely identifies the node within the mapping graph for the node type and within all mapping graphs produced for the server. For example the node identifier may be determined from the item it represents. For example for a server node representing a server the node identifier may comprise the server identifier. For an application object node representing an application object the node identifier may comprise the application object identifier which may vary depending on the application object type. In some embodiments for example the VM manager module engine may determine or assign a unique identifier for each application object and application object node. For a storage object node representing a storage object the node identifier may comprise the storage object identifier. The storage object identifier may be unique for the storage object type and can be used to locate the storage object in the cluster. The storage object identifier may vary depending on the storage object type. For example for a logical unit the storage object identifier may comprise its serial number for a volume the storage object identifier may comprise its GUID etc. For a storage system node representing a storage system the node identifier may comprise the storage system identifier. In other embodiments other methods are used to determine unique identifiers for the nodes.

For 3 the node name the graph data may specify for each node for example a name of the node specified by a user or administrator for the item the node represents. For example the node name may comprise a server name application object name storage object name or storage system name that has been specified by a user or administrator for the server application object storage object or storage system that the node represents.

Also the graph data may also describe the hierarchical relationships between the nodes in the mapping graph. For example the hierarchical relationships may be described in terms of node pairs in parent child relationships each node pair specifying a parent node and a child node. As such the mapping graph may be described by a plurality of node pairs in parent child relationships. In other embodiments other methods are used to describe the hierarchical relationships between the nodes in the mapping graph.

For each application object the method also stores at the graph data to the application object data structure . As shown in each entry for an application object includes a graph data field for storing graph data for the mapping graph. As shown in the example of the graph data field may store for each node in mapping graph 1 node type 2 node identifier and 3 node name. In other embodiments other data describing each node may also be stored in the graph data field . The graph data field may also store data describing a plurality of node pairs in parent child relationships showing hierarchical relationships between the nodes.

For each application object the method then sends at the application object data comprising graph metadata and graph data representing the application object to the application policy manager for data policy management of the application objects. For example the data policies may include policies for backup service level objectives recovery monitoring and or reporting. For each application object the application policy manager engine may apply one or more policies to the application object by applying the one or more policies to the underlying storage objects of the application object by using the application object data that specifies mappings to the underlying storage objects. The method may do so by sending for each application object the graph metadata and the graph data stored in the entry for the application object in the application object data structure to the application policy manager. In some embodiments the method sends the graph metadata before sending the graph data so that the application policy manager may use the graph metadata to understand and properly process the graph data .

As described above a standardized manner of representing describing application objects may be provided by representing describing application objects as mapping graphs. As such use of the mapping graphs may provide a standardized manner for representing different types of application objects e.g. regardless of the type of VM manager used . In addition a standardized manner for describing the mapping graph of an application object is provided by producing application object data comprising graph metadata and graph data describing the mapping graph the application object data being formatted into a standardized formatted as discussed above.

As such the application object data represents describes an application object in a standardized format that therefore may be easily processed by the application policy manager. In these embodiments the mapping graph and application object data may be used to represent the application object to the application policy manager that manages data policies on the application object based on the mapping graph and application object data. Since the application object data is standardized the application policy manager may process and manage the application objects in a similar manner no matter the type of application object. The application policy manager is described below in Section IV.

Each storage system may have a distributed architecture. For example each storage system may include separate N module network module and D module data module components not shown . In such an embodiment the N module is used to communicate with the server systems while the D module includes the file system functionality and is used to communicate with the storage devices . In another embodiment the storage server may have an integrated architecture where the network and data components are all contained in a single box or unit. The storage system may be coupled through a switching fabric not shown to other storage systems in the cluster . In this way all the storage systems of the cluster may be interconnect to form a single storage pool that may be accessed by the connected server systems .

The storage systems comprise functional components that cooperate to provide a distributed storage system architecture providing consolidated data services to the server systems . A server system may comprise a computer system that utilizes services of the cluster storage system to store and manage data in the storage devices of the storage systems . Interaction between a server system and a storage system can enable the provision of storage services. That is server system may request the services of the storage system and the storage system may return the results of the services requested by the server system by exchanging packets over the connection system . The server system may request the services of the storage system by issuing packets using file based access protocols such as the Common Internet File System CIFS protocol or Network File System NFS protocol over the Transmission Control Protocol Internet Protocol TCP IP when accessing information in the form of files and directories. Alternatively the server system may issue packets including block based access protocols such as the Fibre Channel Protocol FCP or Internet Small Computer System Interface iSCSI Storage Area Network SAN access when accessing information in the form of blocks.

The storage system may comprise a computer system that stores data in a set of storage devices preferably on one or more writable storage device media such as magnetic disks video tape optical DVD magnetic tape and any other similar media adapted to store information including data and parity information . The storage system may implement a file system to logically organize the data as storage objects on the storage devices . A storage system or a server system may execute one or more applications discussed above in Section III that submit access requests for accessing particular storage objects on the storage devices .

The server processors are the central processing units CPUs of the management server and thus control the overall operation of the management server . Server processors may include one or more programmable general purpose or special purpose microprocessors digital signal processors DSPs programmable controllers application specific integrated circuits ASICs programmable logic devices PLDs or the like or a combination of such devices. The server network adapter comprises a plurality of ports adapted to couple the management server to one or more other computer systems such as servers or storage systems over point to point links wide area networks virtual private networks implemented over a public network Internet or a shared local area network. The server network adapter thus may comprise the mechanical electrical and signaling circuitry needed to connect the storage system to the network.

The output component may be of any type generally used by a computer system to provide information to an end user e.g. administrator . For example the output component could include a monitor an audio speaker or an alphanumeric display. Similarly the input component may be of any type that allows an end user to provide input into a computer system. For example the input component may be a keyboard a mouse or a speech recognition system. In some embodiments the input component may be used by an administrator inputting policy information or grouping datasets.

Server memory can be a random access memory RAM a read only memory ROM or the like or a combination of such devices. It will be apparent to those skilled in the art that other processing and memory means including various computer readable media may be used for storing and executing program instructions pertaining to the embodiments described herein. Server memory comprises storage locations that are addressable by the processor and adapters for storing software program code such as software described herein. The server processor and server adapters may in turn comprise processing elements and or logic circuitry configured to execute the software code. Such software code may include an application policy manager module and an application policy management data structure . In some embodiments the various modules may configure hardware components of the management server to produce an application policy manager module and an application policy management data structure .

Server local storage is a storage device that stores data needed by the application policy management module and application policy management data structure for performing the embodiments described herein. Such data may include all application policies application objects and application datasets. The management server loads data stored on the server local storage into server memory from which they are accessed by server processors . The server local storage may also store data produced by the application policy management module and application policy management data structure upon performing the embodiments described herein. For example such data may include application policies and associated application datasets that need to be implemented.

In some embodiments the application policy manager module and application policy management data structure for performing the embodiments described herein reside and execute on the management server which is external and separate from the server and storage systems . In other embodiments the application policy manager module and application policy management data structure may be distributed and reside and execute on one or more servers and or one or more storage systems .

The application policy manager module may be configured to associate at least one policy to each application object of a dataset and to each underlying storage object of each application object. In some embodiments the application policy manager module may receive information about application objects and storage objects. The application policy manager module may group application objects into datasets based on the received information. The received information and the datasets may be stored into a data structure. The application policy manager module may assign a policy to each dataset and implement the policy on each application object of each dataset and each underlying storage object of each application object.

In some embodiments some of the steps of method are performed or caused to be performed by an application policy manager module executing on a management server . The application policy manager module may be configured to operate in conjunction with other software modules of the management server server system and software modules of the storage system to collectively perform the embodiments described herein.

The method begins by receiving at step data about application objects and storage objects. The data may comprise all of the information that describes the application object. In some embodiments the data about application objects and storage objects may be received from an application object manager . The data about application objects and storage objects may comprise but is not limited to identification information mapping graph information to locate an underlying storage object as described above in section III type of application object and attributes of an application object.

In some embodiments an application object represents an application or a virtual storage component used by a virtual based application. An application object may comprise a virtual object or a non virtual object. As such an application object may comprise a virtual based application or a non virtual based application. For example an application object may comprise a virtual based application such as a virtual machine VM or a non virtual based application such as an email application or database application.

As described above in some embodiments an application object may represent a virtual storage component used by a virtual based application. For example an application object may represent a datastore. Thus in some embodiments an application object may represent a virtual application such as a VM a non virtual application such as an email or database application and or a virtual storage component such as a datastore.

The method may group at step application objects into datasets based on the received data. In some embodiments a dataset comprises at least two application objects. For example a dataset may comprise a virtual based application and a non virtual based application. In the same or an alternative embodiment a dataset may comprise a virtual based application a non virtual based application and a datastore. For example a dataset may comprise a VM a datastore used by the VM and an email application. As such a dataset may comprise any combination of virtual based applications non virtual based applications and virtual storage components.

In some embodiments the grouping of application objects into datasets is based on the received data. For example the grouping of application objects may be based on the received data with regard to attributes of the application objects. As such in some embodiments application objects with similar attributes may be grouped together into the same dataset. In some embodiments the application objects may be grouped according to attributes such as but not limited to backup options recovery options restoring of VMs monitoring and reporting settings type of application object origination of application object service level objectives etc. Thus in some embodiments application objects with similar backup settings may be grouped together into one dataset. In other embodiments application objects of the same type may be grouped into datasets. For example the application objects may comprise a first VM a second VM an email application and a database application. The application objects may be grouped into a dataset based on whether the application objects are virtual based applications or non virtual based applications. Thus in some embodiments a first dataset may comprise the first VM and the second VM while a second dataset may comprise the email application and the database application.

The step may in some embodiments be performed automatically without human initiation interaction or intervention. In other embodiments the step may be performed by human interaction such as operations by an administrator.

The method may store at step the received data about application objects and storage objects and datasets into an application policy management data structure . In some embodiments the application policy management data structure may be located within the management server . shows an exemplary application policy management data structure used in some embodiments. In some embodiments the application policy management data structure comprises a plurality of dataset entries each entry representing a dataset application object data and a policy discussed below . Each entry may comprise a plurality of data fields for storing data describing the datasets information about application objects and storage objects and policies.

In some embodiments an application policy management data structure entry representing a dataset may contain data fields for a dataset identifier application object identifier application object attributes storage objects and policies . The dataset identifier may comprise for example information identifying each dataset that has been created by grouping application objects. The application object identifier may comprise for example an entry for each application object of each dataset. The application object attributes may comprise attributes of each application object of each dataset. For example the application attributes may list the type of each application object the attributes of each application object backup options of each application object and or service level objectives of each application object etc. The storage objects may comprise a list of all storage objects of each application object of each dataset. For example the storage object may comprise a list of all underlying storage objects of each application object as well as all storage objects that correspond to a virtual storage component. The policy may comprise information corresponding to at least one policy that will be implemented upon a dataset such that the policy will be implemented on each application object of the dataset as well as each underlying storage object of each application object as will be discussed below.

In some embodiments the application objects field attributes field and storage objects field may be derived from or correspond to the received data about application objects and storage objects as discussed in step .

In some embodiments the method may store data for the dataset field application objects field attributes field and storage objects field at step . In some embodiments the data for the policy field is stored at step discussed below . As such the method receives information about application objects and storage objects groups application objects into datasets and assigns a policy to each dataset and stores information for each application object of each dataset.

The method may assign at step at least one policy to each dataset. A policy may relate to a storage system characteristic attribute or behavior. As such in some embodiments the assigning of at least one policy to a dataset comprising application objects and storage objects may relate to how a storage system will interact manage or behave with the application objects and each underlying storage object of each of the application objects. In some embodiments a plurality of policies may be applied to each dataset. In some embodiments the policy may comprise a backup policy a protection policy service level objectives SLOs role based access control RBAC recovery policy monitoring and reporting policy and any other policy relating to the administration provisioning maintenance and or management of application objects or storage objects. Thus for example a backup policy and a RBAC policy may be assigned to a single dataset. As such the backup policy and RBAC policy will be applied to each application object of the dataset as well as the underlying storage objects of each application object.

In some embodiments a backup policy may comprise a backup schedule and schedule backup settings. The backup schedule may for example comprise a backup to occur daily at 9 AM. The schedule backup settings may comprise backup settings applicable to the application objects that comprise the dataset. In some embodiments the backup policy comprises schedule specific backup settings. For example the backup schedule may comprise a backup occurring at 9 AM and at 9 PM. The schedule backup settings for the 9 AM backup may be different than the settings for the 9 PM settings. Thus in some embodiments the backup settings of the backup policy may be schedule specific. Other backup policy settings may comprise but are not limited to a backup retention setting specifying the minimum or maximum number of storage backups for application objects of a dataset that may be stored retention duration daily retention count weekly retention count monthly retention count a lag warning that indicates whether the storage system should generate a warning event when the newest backup copy is older than a defined threshold identification for a backup script to be invoked before and after a backup the time of the day to schedule a backup event day of the week to schedule a backup event time of the day to schedule a backup event and the amount of time between scheduled backup events.

In some embodiments the backup policy may comprise settings with regard to application objects and their respective time zones. For example if a first application object is a virtual machine created in California Pacific Standard Time and a second application object is a virtual machine created in New York Eastern Standard Time the backup policy may comprise different settings for each of the virtual machines in the Pacific Standard Time zone and the Eastern Standard Time zone. For example the backup policy may comprise a setting to create a backup of application objects in a dataset in the Pacific Standard Time zone at 6 AM local time and another setting to create a backup of application objects in a dataset in the Eastern Standard Time zone at 10 AM local time. Thus there is a separate backup of each of the application objects from the two different time zones in a dataset.

In some embodiments a policy may comprise information related to a service level objective SLO . In some embodiments each workload has at least one service level objective SLO specified for the workload and a workload comprises an application object requesting storage services of the storage system or a storage object stored on the storage system. As such an SLO policy may in some embodiments determine storage system characteristics for an application object requesting storage services of the storage system.

Each workload may have zero or more specified service level objectives SLOs . Each SLO of a workload comprises a minimum or maximum target value threshold of a SLO metric the minimum or maximum target value to be achieved by the cluster when servicing the workload. A SLO metric may relate to a storage system characteristic or attribute. In some embodiments a SLO metric comprises a performance or protection metric. A non exhaustive list of examples of performance SLO metrics include data throughput data latency processor utilization storage device utilization input output operations per second IOPS for storage devices etc. A non exhaustive list of examples of protection SLO metrics include recovery point objective allowed time to restore data after data loss disaster situation and generally defines acceptable loss in a disaster situation recovery time objective allowed time to restore a business process after disruption or data loss disaster situation recovery consistency objective data consistency objectives defining a measurement for the consistency of distributed business data within interlinked systems after a disaster incident etc. As such in some embodiments a policy may be comprised of at least one SLO metric.

In some embodiments a policy may comprise role based access control RBAC information. RBAC is an approach to restricting access to authorized users. As such the RBAC policy information may specify user authorization for the application objects and the underlying storage objects. The RBAC information may comprise user authorization for managing and administering application objects and storage objects. For example the RBAC information may comprise user authorization for creating or deleting application objects managing application objects or storage objects specify authorized users who can create backups of application objects and storage objects specify authorized users for restoring application objects and storage objects and any other action which may require user authorization.

In some embodiments a policy may comprise recovery information. For example the policy may comprise information on when to recover an application object such as a virtual machine and the application object s underlying storage objects. In some embodiments the policy may comprise monitoring and reporting information. For example the policy may comprise information on how to monitor an application object and when and in particular circumstances to issue a report on an application object. In some embodiments the policy may contain information for a type of report to be created. For example if an application object comprises a virtual object then the policy may contain information for creating a report appropriate for virtual objects.

In some embodiments the policy may comprise information specific to virtual based application objects. For example a policy may comprise settings or attributes that are only applicable to a virtual based object such as a virtual machine or a datastore. In some embodiments the policy may comprise information specific to non virtual based application objects such as an email application or a database application.

Storage administrators may be wary of giving control of the backups to an administrator of an application because of the consumption of too many storage resources. In some embodiments an application policy may address concerns of the storage administrator. For example in some embodiments a storage administrator may specify the limits of what is allowed in the storage service and allow an administrator of an application configure an application policy within those limits. In some embodiments the limits could be expressed in terms of number of snapshots frequency of backup schedule etc.

In some embodiments the application policy may be used in conjunction with the storage service. For example the storage service may specify limits and settings for a remote backup while the application policy may have settings for a local backup. This duality between remote backups and local backups may also help avoid proliferation of a storage service. As such in some embodiments multiple application administrators may share the same storage service and create different application policies to use different local backup settings.

In some embodiments a group based mechanism may be used for delegation. For example a storage service administration may create an empty group for an application administrator and pre assign delegated privileges on the group. In some embodiments the application administrator may then create datasets and application policies within the group. These datasets and policies may not be visible to the other application administrators who do not have any privileges on the application objects. In this manner each application administrator is segregated and can control their own datasets and application policies while sharing the storage service.

The method may then implement at step the policy on each application object of each dataset and each underlying storage object of each application object. As discussed above in section III the implementation of a policy on an application object may further implement the policy on each underlying storage object of an application object.

As discussed above a server system may comprise a computer system that utilizes services of the cluster storage system to store and manage storage objects of the storage systems . The interaction between a server system and a storage system can enable the provision of storage services or storage objects. That is server system may request the services of the storage system and the storage system may return the results of the services requested by the server system by exchanging packets over the connection system .

Each server system may comprise an application object manager shown as . As discussed above in section III each application object manager may determine graph metadata and graph data for an application object.

The graph metadata and graph data gathered by an application object manager within a server system may be sent via the connection system to the application policy manager module located in management server . As such the application policy manager module may receive graph metadata and graph data from a plurality of application object managers located within a plurality of server systems . Path illustrates an example of the graph metadata and graph data from an application object manager transmitted through a connection system and being received by an application policy manager module within the management server . A similar path may be present for each application object manager within each server system . Thus the application policy manager module within the management server may receive graph metadata and graph data from a plurality of application object managers .

As discussed above the application policy manager module may receive graph metadata and graph data for an application object from an application object manager . The application policy manager module may group application objects into datasets based on the received graph metadata and or graph data and may assign at least one policy to each dataset comprising application objects. In some embodiments the application policy manager module may store the received graph metadata and graph data and dataset information into an application policy data structure .

Information from the application policy management data structure may be transmitted over the connection network to each storage system . The information may be transmitted by the application policy manager module within the management server . In some embodiments the information transmitted may comprise any or all of the information from the application policy management data structure . For example the information transmitted by the application policy manager module to a storage system may comprise graph metadata graph data and or policy information. In some embodiments the application policy manager module may transmit the information to each storage system within the cluster storage system . In some embodiments the application policy manager module may transmit certain information from at least one entry to a storage system while transmitting information from another entry to a second storage system .

Path illustrates an example of the information from the application policy manager module transmitting through a connection system and being received by a storage system within the cluster storage system . As such the information transmitted from the application policy manager module to at least one storage system may relate to a storage system characteristic attribute or behavior for an application object.

In some embodiments some of the steps of method are performed or caused to be performed by an application policy manager module executing on a management server . The application policy manager module may be configured to operate in conjunction with other software modules of the management server server system and software modules of the storage system to collectively perform the embodiments described herein.

The method begins by receiving at step graph metadata of an application object. As discussed above in section III graph metadata may comprise information for understanding and properly processing the information contained in the graph data. For example the application object graph metadata may comprise one or more types of application objects included in a mapping graph policies that are permitted to be implemented on each type of application object and or features of each type of application object included in a mapping graph. In some embodiments the graph metadata may be received by the application policy manager from an application object manager .

The method may receive at step graph data of an application object. As discussed above in section III graph data may describe a mapping graph. In some embodiments the graph data may describe the various components of the mapping graph including descriptions of the nodes connectors and hierarchical relationships between the nodes. For example graph data may comprise a node type node identifier and node name for each node as well as a plurality of node pairs in parent child relationships showing hierarchical relationships between the nodes.

The method may group at step application objects into datasets based on the graph metadata and or the graph data. As discussed above a dataset comprises at least two application objects and may comprise any combination of virtual based applications non virtual based applications and virtual storage components. In some embodiments the application objects may be grouped into datasets based on the graph metadata. For example the application objects may be grouped into datasets based on the type of application objects included in a mapping graph. In some embodiments the application objects may be grouped into datasets based on policies permitted to be implemented on each type of application object and or features of each type of application object in a mapping graph.

The method may store at step the received graph metadata and graph data into an application policy management data structure . In some embodiments the application policy management data structure may be located within the management server . shows an exemplary application policy management data structure used in some embodiments. In some embodiments the application policy management data structure comprises a plurality of dataset entries each entry representing a dataset. Each entry may comprise a plurality of data fields for storing data describing the datasets application objects graph metadata graph data and a policy.

In some embodiments an application policy management data structure entry representing a dataset may contain data fields for a dataset identifier application object identifier graph metadata graph data and policy . The dataset identifier may comprise for example information identifying each dataset that has been created by grouping application objects. The application identifier may comprise for example an entry for each application of each dataset. For example the application identifier may comprise information to identify a virtual machine an email application or a datastore as an application object. The graph metadata may comprise graph metadata for an application object. For example the graph metadata may comprise the type of application objects included in a mapping graph policies that are permitted to be implemented on application objects of the mapping graph and or features of each type of application object in the mapping graph. The graph data may describe a mapping graph by including descriptions of each node type identifier name connectors between nodes and or the hierarchical relationships between nodes. The policy may comprise information corresponding to at least one policy that will be implemented upon a dataset such that the policy will be implemented on each application object of the dataset and each underlying storage object of each application object through the use of the graph data.

In some embodiments the method may store data for the dataset application objects graph metadata and graph data at step . In some embodiments the policy is stored at step discussed below . As such the method receives graph metadata and graph data groups application objects into datasets and assigns a policy to each dataset and stores information for each application object of each dataset.

The method may assign at step at least one policy to each dataset. As discussed above a policy may relate to a storage system characteristic attribute or behavior. As such in some embodiments the assigning of at least one policy to a dataset comprising application objects may relate to how a storage system will interact manage or behave with the application objects and each underlying storage object of each of the application objects. In some embodiments a plurality of policies may be applied to each dataset. In some embodiments the policy may comprise a backup policy a protection policy service level objectives SLOs or a role based access control RBAC policy.

The method may then implement at step the policy on each application object of each dataset and each underlying storage object of each application object by using the graph data. As discussed above in section III the implementation of a policy on an application object may further implement the policy on each underlying storage object of an application object through the use of the graph data.

As such in some embodiments the method may implement a policy on a dataset containing an application object such as a virtual storage component. As discussed above in section III a virtual storage component may comprise for example a datastore or a virtual machine disk VMDK . The method may receive graph metadata for an application object such as a datastore. In some embodiments the graph metadata for a datastore may comprise data identifying that the application object type comprises a datastore policies that are permitted to be implemented on a datastore and features of the datastore. For example the graph metadata for the datastore may indicate that the application object type is a datastore indicate that the permitted policies for a datastore are RBAC and backup and describe various features of the datastore.

Next the method may receive graph data for the datastore. The graph data may describe a mapping graph. In some embodiments the mapping graph for a datastore may describe a node type node identifier and node name for each node in the mapping graph and a plurality of node pairs in parent child relationships showing the hierarchical relationship between the nodes. The method may assign a policy to the dataset comprising the datastore as an application object. For example the method may assign a policy to the dataset from the permitted policies in the graph metadata. Next the method may implement the policy on the dataset. The method may implement the policy on the datastore and the policy is implemented to the collection of storage objects resources that comprise map to and underlies the datastore. In some embodiments the method may implement the policy to a datastore s underlying storage objects resources through the use of the graph data.

Some embodiments may be conveniently implemented using a conventional general purpose or a specialized digital computer or microprocessor programmed according to the teachings herein as will be apparent to those skilled in the computer art. Some embodiments may be implemented by a general purpose computer programmed to perform method or process steps described herein. Such programming may produce a new machine or special purpose computer for performing particular method or process steps and functions described herein pursuant to instructions from program software. Appropriate software coding may be prepared by programmers based on the teachings herein as will be apparent to those skilled in the software art. Some embodiments may also be implemented by the preparation of application specific integrated circuits or by interconnecting an appropriate network of conventional component circuits as will be readily apparent to those skilled in the art. Those of skill in the art would understand that information may be represented using any of a variety of different technologies and techniques.

Some embodiments include a computer program product comprising a computer readable medium media having instructions stored thereon in and when executed e.g. by a processor perform methods techniques or embodiments described herein the computer readable medium comprising sets of instructions for performing various steps of the methods techniques or embodiments described herein. The computer readable medium may comprise a non transitory computer readable medium. The computer readable medium may comprise a storage medium having instructions stored thereon in which may be used to control or cause a computer to perform any of the processes of an embodiment. The storage medium may include without limitation any type of disk including floppy disks mini disks MDs optical disks DVDs CD ROMs micro drives and magneto optical disks ROMs RAMs EPROMs EEPROMs DRAMs VRAMs flash memory devices including flash cards magnetic or optical cards nanosystems including molecular memory ICs RAID devices remote data storage archive warehousing or any other type of media or device suitable for storing instructions and or data thereon in.

Stored on any one of the computer readable medium media some embodiments include software instructions for controlling both the hardware of the general purpose or specialized computer or microprocessor and for enabling the computer or microprocessor to interact with a human user and or other mechanism using the results of an embodiment. Such software may include without limitation device drivers operating systems and user applications. Ultimately such computer readable media further includes software instructions for performing embodiments described herein. Included in the programming software of the general purpose specialized computer or microprocessor are software modules for implementing some embodiments.

Those of skill would further appreciate that the various illustrative logical blocks circuits modules algorithms techniques processes or method steps of embodiments described herein may be implemented as computer electronic hardware computer software or combinations of both. To illustrate this interchangeability of hardware and software various illustrative components blocks modules circuits and steps have been described herein generally in terms of their functionality. Whether such functionality is implemented as hardware or software depends upon the particular application and design constraints imposed on the overall system. Skilled artisans may implement the described functionality in varying ways for each particular application but such implementation decisions should not be interpreted as causing a departure from the embodiments described herein.

The various illustrative logical blocks modules and circuits described in connection with the embodiments disclosed herein may be implemented or performed with a general purpose processor a digital signal processor DSP an application specific integrated circuit ASIC a field programmable gate array FPGA or other programmable logic device discrete gate or transistor logic discrete hardware components or any combination thereof designed to perform the functions described herein. A general purpose processor may be a microprocessor but in the alternative the processor may be any conventional processor controller microcontroller or state machine. A processor may also be implemented as a combination of computing devices e.g. a combination of a DSP and a microprocessor a plurality of microprocessors one or more microprocessors in conjunction with a DSP core or any other such configuration.

The algorithm techniques processes or methods described in connection with embodiments disclosed herein may be embodied directly in hardware in software executed by a processor or in a combination of the two. In some embodiments any software application program tool module or layer described herein may comprise an engine comprising hardware and or software configured to perform embodiments described herein. In general functions of a software application program tool module or layer described herein may be embodied directly in hardware or embodied as software executed by a processor or embodied as a combination of the two. A software application layer or module may reside in RAM memory flash memory ROM memory EPROM memory EEPROM memory registers hard disk a removable disk a CD ROM or any other form of storage medium known in the art. An exemplary storage medium is coupled to the processor such that the processor can read data from and write data to the storage medium. In the alternative the storage medium may be integral to the processor. The processor and the storage medium may reside in an ASIC. The ASIC may reside in a user device. In the alternative the processor and the storage medium may reside as discrete components in a user device.

While the embodiments described herein have been described with reference to numerous specific details one of ordinary skill in the art will recognize that the embodiments can be embodied in other specific forms without departing from the spirit of the embodiments. Thus one of ordinary skill in the art would understand that the embodiments described herein are not to be limited by the foregoing illustrative details but rather are to be defined by the appended claims.

