---

title: Efficient pipeline parallelism using frame shared memory
abstract: A systems and methods are disclosed that provide an efficient parallel pipeline for data processing using a multi-core processor. Embodiments allocate a shared memory portion of the memory that is accessible from more than one context of execution and/or process a frame in a plurality of processing stages processed by a context of execution. In some embodiments, each of the plurality of processing stages may be bound to a processing core of the multi-core processor. In other embodiments include one or more processing stages with a point-to-point communication mechanism that operates in shared memory.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09092339&OS=09092339&RS=09092339
owner: F5 Networks, Inc.
number: 09092339
owner_city: Seattle
owner_country: US
publication_date: 20140430
---
This application is a continuation of U.S. patent application Ser. No. 13 590 307 filed Aug. 21 2012 by Giacomoni et al. entitled EFFICIENT PIPELINE PARALLELISM USING FRAME SHARED MEMORY. which is a continuation of U.S. patent application Ser. No. 12 106 684 filed Apr. 21 2008 by Giacomoni et al. entitled EFFICIENT PIPELINE PARALLELISM USING FRAME SHARED MEMORY. which claims the benefit of U.S. Provisional Application No. 60 912 979 filed Apr. 20 2007 entitled Efficient Pipeline Parallelism. Each of these applications is herein incorporated by reference in its entirety for all purposes.

This invention was made with Government support under Grant No. DAAD19 01 1 0484 awarded by the U.S. Army Research Office. The Government has certain rights in the invention.

Designers have turned to fully custom processing engines to increase processing rates. Recently however designers have shifted to special purpose programmable platforms e.g. network processors . This shift has dramatically reduced both the cost and time needed to develop a system. Unfortunately these special purpose platforms typically expose excessive low level platform specific implementation details that developers must properly manage to achieve full performance. For example network processors typically scale the memory wall and achieve the performance constraints imposed by modern networks by exposing their architecture to the developers. Exposed elements have included processor interconnections explicitly managed memory hierarchies and lightweight threading. Thus developers are forced to forgo the niceties of general purpose languages and traditional operating system support. These details complicate application development and harm portability by coupling the software to a specific platform.

A computer system is provided according to one embodiment that includes a general purpose multi core processor coupled with memory and at least one network interface device. The general purpose multi core processor may include a plurality of processing cores. The memory may include instructions to allocate a shared memory portion of the memory that is accessible from more than one context of execution. The memory may also include instructions to process a frame in a plurality of processing stages processed by a context of execution. In some embodiments instructions may be included which bind and or pin each of the plurality of processing stages is to a processing core of the multi core processor. Instructions may also be included to connect one or more processing stages with a point to point communication mechanism that operates in shared memory.

A method for establishing parallel frame processing is also provided according to one embodiment. A plurality of frames is received at a general purpose multi core processor that operates contexts of execution. Frame processing may be segmented into a plurality of stages. Each of the plurality of stages may be associated with a processing core corresponding to a processing core of the general purpose multi core processor. Some stages may then be associated with a point to point communication mechanism that uses shared memory to communicate data between at least two stages.

A method for sequentially processing a plurality of frames is also proved according to one embodiment. Shared memory is allocated in a general purpose multi core processor that operates contexts of execution. In some embodiments the shared memory may be accessible by multiple contexts of execution. A first frame may be received at the general purpose multi core processor and placed in shared memory. The first frame may be processed with a first processing core of the general purpose multi core processor during a first time period. A second frame may be received at the general purpose multi core processor after the first frame is received and placed in shared memory. The first frame may be processed with the second processing core during a second time period that occurs after the first time period. The second frame may be processed with the first processing core during the second time period.

A method for enqueuing data is provided according to another embodiment. The method includes determining whether a first memory location associated with a general purpose multi core processor includes an empty symbol for example NULL. If the first memory location includes the empty symbol then data is placed in the first memory location. If the first memory location does not include the empty symbol the method is paused. The pause may include for example waiting until the first memory location includes the empty symbol returning and notifying an invoking method and or sleeping on a condition variable until notified. The method in some embodiments may also include determining whether a second memory location includes the empty symbol and placing data in the second memory location if the second memory location includes the empty symbol.

A method for dequeuing data is provided according to another embodiment. A determination is made whether a first memory location of a general purpose multi core processor includes the empty symbol for example NULL. If the first memory location does not include the empty symbol then data is read from the memory location. If the first memory location does not include the empty symbol then the empty symbol is placed in the first memory location. If the first memory location does include the empty symbol then pausing. In some embodiments the method may also include determining whether a second memory location includes the empty symbol. If the second memory location does not include the empty symbol then reading data from the second memory location and placing the empty symbol in the second memory location.

Another method for enqueuing data is provided according to another embodiment. A head pointer is pointed to the oldest empty memory location in the set of the most recent entries where data was enqueued in a general purpose multi core processor. A tail pointer is pointed to the oldest memory location in the set of the most recent entries where data was enqueued. A determination is made whether the memory location pointed to by the head pointer is empty. If the memory location pointed to by the head pointer is empty then data may be placed in the memory location pointed to by the head pointer and the head pointer is incremented. In some embodiments the head pointer may be stored in a memory location in a first cache line of the general purpose multi core processor. In some embodiments the tail pointer is stored in a memory location in a second cache line. In some embodiments the first cache line and the second cache line comprise different cache lines. In yet other embodiments the distance between the head pointer and the tail pointer is calculated. A determination is made whether the distance between the head pointer and the tail pointer is less than an offset. If the distance between the head pointer and the tail pointer is less than the offset then pausing at least one of dequeue and enqueue. If the distance between the head pointer and the tail pointer is less than the offset then waiting a period of time.

Another method for dequeuing data is provided according to another embodiment. A head pointer is pointed to the oldest empty memory location in the set of the most recent entries where data was enqueued in a general purpose multi core processor. A tail pointer is pointed to the oldest memory location in the set of the most recent entries where data was enqueued. A determination is made whether the memory location pointed to by tail pointer is empty. If the memory location pointed to by the tail pointer is not empty then data pointed to by the tail pointer is read. The memory location pointed to by the tail pointer is set equal to the empty symbol and the tail pointer is incremented.

Further areas of applicability of the present disclosure will become apparent from the detailed description provided hereinafter. It should be understood that the detailed description and specific examples while indicating various embodiments are intended for purposes of illustration only and are not intended to necessarily limit the scope of the disclosure.

The ensuing description provides preferred exemplary embodiment s only and is not intended to limit the scope applicability or configuration of the disclosure. Rather the ensuing description of the preferred exemplary embodiment s will provide those skilled in the art with an enabling description for implementing a preferred exemplary embodiment. It should be understood that various changes may be made in the function and arrangement of elements without departing from the spirit and scope as set forth in the appended claims.

As used throughout this disclosure the term general purpose processor includes a processor that runs arbitrary applications. A general purpose processor may also include a processor that operates with virtual memory. Moreover a general purpose processor may also include a processor that is not supported by specific hardware or devices.

As used throughout this disclosure the term context of execution includes a process thread or a schedulable unit or any combination thereof. Moreover a context of execution may operate in user space kernel space or any other operating space.

As used throughout this disclosure the term virtual memory includes memory that gives an application program or context of execution the impression that it has contiguous working memory while in fact it is physically fragmented and may even overflow on to disk storage. That is virtual memory is based on tricking programs into thinking they are using large blocks of contiguous physical memory. Virtual memory is often divided into user space and kernel space.

As used throughout this disclosure the term kernel includes the central component of a computer operating systems operating on a general purpose processor. Its responsibilities may include managing the system s resources and or the communication between hardware and software components. A kernel may provide the lowest level abstraction layer for the resources such as memory processors and I O devices that application software must control to perform its function. The kernel may make these facilities available to application processes through inter process communication mechanisms and system calls.

As used throughout this disclosure the term kernel space includes memory that may be used for running the kernel kernel extensions and or some device drivers. In general purpose processors kernel space is not accessible by user applications. In most operating systems and in all operating systems operating on a general purpose processor kernel memory is almost never swapped out to disk.

As used throughout this disclosure the term user space includes memory where most user mode applications work. User space memory can be swapped out to a disk when necessary. User space memory is set up by the kernel for operating a user s context of execution process thread and or application.

As used throughout this disclosure the term pinning and or binding as well as pin bind and bound include that act of requiring a context of execution on to run on a specific processor processing core or portion of a processor. This act may occur through the scheduler. The terms are used synonymously throughout the disclosure. The terms may also refer to hard binding which turns off the scheduler and or hard affinity which gives the scheduler hints regarding where to process a context of execution.

As used throughout this disclosure the term multi core processor includes a processor that combines two or more independent processing cores. The independent processing cores in some embodiments may be included in a single package. A processing core may refer to an independent processing unit. For example a dual core processor is a multi core processor operating with two independent processing cores. shows an example of a quad core multi core processor . The multi core processor includes four processing cores . Each processing core includes a logic unit a register and cache memory . Some multi core processors include a shared cache . The multi core processor may be coupled with main memory through an interconnect . According to another embodiment a multi core processors may be implemented with shared functional units. For example Intel s Hyper Threading technology allows an Intel Pentium 4 core to be treated as two logically distinct cores. In yet another embodiment a multi core processor may include a multi threaded core that has multiple instructional fetch units and or shares functional units such as Symmetric Multi Threading. In such embodiments while the processor may include a single processor the operating system sees multiple cores.

As used throughout this disclosure the term pointer includes a value that refers directly or indirectly to another value stored elsewhere in memory using the memory address of the other value. A pointer may also refer to any data structure buffer entry data element etc. that refers to another value stored elsewhere. A pointer may include the memory address of another value and a context of execution may dereference the pointer to return the value. A pointer may include an index value. The index value specifies a number of memory locations beyond a base memory location where the data of interest may be found. For example the pointer may include the number 7 as an index. Therefore in this example the pointer refers to the data stored in the 7th memory location from a base value.

A multi core processor may be any type of processor such as for example Pentium processors like the Intel Xeon or a similar processor. For example the processors may also be two four eight sixteen thirty two or more processors combined in a single processing core such as for example a dual core or quad core Pentium Xeon processor.

Embodiments of the present invention provide for high throughput data processing. Such embodiments may allow for data processing at frame rate. Moreover some embodiments of the invention may provide instructions software application programming interface methods and or processes that operate on a general purpose multi core processor.

Another embodiment provides for a multi core processor that allocates a shared memory region accessible by both user space and kernel space. In such an embodiment the shared memory space may be used to transfer frame data between stages such as using a point to point communication device. For example frame processing may require an input stage in kernel space an application stage in user space and or an output stage in kernel space. The application stage may be decomposed into any number of applications or substages. The application may perform processing work on the frame data or perform processing work in response to the frame data. Each of these stages may be pinned to a single processing core of a general purpose multi core processor. The shared memory space may be used to transfer the data from the input process to the application process and or from the application process to the output process. The shared memory according to this embodiment therefore should permit fast data sharing between the kernel and user space.

Another embodiment provides for enqueuing data into a shared memory buffer using a single pointer and dequeuing data from the shared memory buffer using a different pointer. Such an embodiment may permit one application to independently write to the buffer and permit another application to independently read from the buffer. For example a head pointer may be used to point to the oldest memory location where data was written and a tail pointer may be used to point to the oldest memory location where data was read. The two applications may not or rarely communicate with each other regarding the status of the data being read or written from the buffer such as sharing information regarding the head and tail pointers. If the memory location pointed to by the head pointer is empty as signified by an empty symbol then data may be written into the memory location. The head pointer may then be incremented. If the memory location is not empty however then data is not written into the buffer until the memory location is empty. If the memory location pointed to by the tail pointer is not empty then data may be read from the buffer. If the memory location is empty then data is not read from the memory location until the memory location is written to. Thus according to embodiments writing data to a memory location may occur independent from the tail pointer likewise reading data from a memory location may occur independent from the head pointer.

The application stage may then pull the frame data from the shared memory using the pointers stored in the up queue . Any data application may occur in the application stage without limitation. Once the application stage has completed its processing the addresses associated with data that has been processed is then returned to the allocation queue . As frames are received from the network they are first processed by the input stage and then the application stage . Because the processing has been segmented in some embodiments one frame may be processed in the input stage while another frame is being processed in the application stage. These two stages may be operating on different processing cores of a multi core processor.

In the up queue and the down queue may be a point to point communication mechanism. Various embodiments described herein may also be used for the for the up queue and or the down queue. Moreover other queuing and or dequeuing techniques may be used such as Lamport s enqueue and dequeue.

The input stage of Frame 1 is processed during the first time period ton processor P1. During the second time period t the application stage of Frame 1 is processed on processor P2 and the input stage of Frame 2 is processed on processor P1. During the third time period t the output stage of Frame 1 is processed on processor P3 the application stage of Frame 2 is processed on processor P2 and the input stage of Frame 3 is processed on processor P1. During the fourth time period t the output stage of Frame 2 is processed on processor P3 the application stage of Frame 3 is processed on processor P2 and the input stage of Frame 4 is processed on processor P1. During the fifth time period t the output stage of Frame 3 is processed on processor P3 the application stage of Frame 4 is processed on processor P2 and the input stage of Frame 5 is processed on processor P1.

As shown in the input stage of Frame 1 is processed during the first time period ton processor P1. During the second time period t the first half of the application stage of Frame 1 is processed on processor P2 and the input stage of Frame 2 is processed on processor P1. During the third time period t the second half of the application stage of Frame 1 is processed on processor P2 the first half of the application stage of Frame 2 is processed on processor P3 and the input stage of Frame 3 is processed on processor P1. During the fourth time period t the output stage of Frame 2 is processed on processor P4 the second half of the application stage of Frame 2 is processed on processor P2 the first half of the application stage of Frame 3 is processed on processor P3 and the input stage of Frame 4 is processed on processor P1. During the fifth time period t the output stage of Frame 3 is processed on processor P4 the second half of the application stage of Frame 2 is processed on processor P3 the first half of the application stage of Frame 3 is processed on processor P2 and the input stage of Frame 5 is processed on processor P1.

As shown in the input stage of Frame 1 is processed during the first time period ton processor P1. During the second time period t the application stage of Frame 1 is processed on processor P2 and the input stage of Frame 2 is processed on processor P1. During the third time period t the application stage of Frame 2 is processed on processor P2 and the input stage of Frame 3 is processed on processor P1. During the fourth time period tthe application stage of Frame 3 is processed on processor P2 and the input stage of Frame 4 is processed on processor P1. During the fifth time period t the application stage of Frame 4 is processed on processor P2 and the input stage of Frame 5 is processed on processor P1.

As shown in the application stage of Frame 1 is processed during the first time period ton processor P1. During the second time period t the output stage of Frame 1 is processed on processor P2 and the application stage of Frame 2 is processed on processor P1. During the third time period t the output stage of Frame 2 is processed on processor P2 and the application stage of Frame 3 is processed on processor P1. During the fourth time period t the output stage of Frame 3 is processed on processor P2 and the application stage of Frame 4 is processed on processor P1. During the fifth time period t the output stage of Frame 4 is processed on processor P2 and the application stage of Frame 5 is processed on processor P1.

Various other methods may be envisioned that segment frame processing into a number of processes operative on a number of different processing cores. Those skilled in the art will recognize that various other stages may be implemented. Complex applications may be segmented and performed on different processing cores. For example a first processor may perform an input stage a second processor may perform a first application stage a third processor may perform a second application stage etc.

In some embodiments of the invention each stage may not take the same amount of processing time as shown in . For example using Gigabit Ethernet and small frame sizes for example 64 bytes each frame may be processed on average in less than approximately 672 ns in order to maintain line rate processing. Not every stage will operate at approximately 672 ns. As shown in the stage frame processing diagram of the input IP processing stage is much shorter than the application APP and output OP processing stages. The processing may still occur in three different processing cores of a multi core processor as shown in . By segmenting frame processing into three stages and pinning the processing to independent processing cores of a multi core processor these three stages may be processed in parallel as shown in timing chart in . However because the IP and OP stages are shorter than the APP stage in some embodiments processing overhead may be added to the input or output stages in order to equalize the timing of the stages.

In some embodiments a point to point communication mechanism may be employed that provides frame data between processes running on different processing cores. When stages operate in both kernel space and user space a point to point communication mechanism may operate in shared memory. A shared memory location may allow processes operating in kernel space and user space to read and write frame data. The point to point communication mechanism may be a low latency mechanism such that the amount of time it takes for the processor to access the data is minimal. For example frame data may be saved in the shared cache of a multi core processor. For example cache lines may be read from the shared memory into each processing core s processor cache as needed.

Frames and or frame data may be passed from one processing stage to the next processing stage using shared memory queues for example using one or more point to point queues. shows an example of shared memory allocation that may be used as a part of a point to point queue according to one embodiment. Blocks and show user space memory allocations for two applications. Block shows a kernel space memory allocation. A portion of each memory allocation overlaps in the shared memory region . Thus both user space and kernel space applications may access data stored in this memory space.

Frame data may then be read from the input queue at block during the input stage on a first processing core . An application may then perform one or more functions on the data during the application stage whereupon the resulting data from the application stage may be written into an output queue at block .

Optionally each stage may then be bound or pinned to a specific processor at block . Binding a stage with a processing core dedicates the processing core to the stage with which it is bound or pinned. Those skilled in the art will recognize various ways of binding and or pinning a stage with a processing core.

The stages may then be coupled using shared memory at block . Using shared memory allows stages in kernel space and user space to quickly and easily share frame data. Using shared memory may provide a low latency point to point communication mechanism that does so without copying the data from user space to kernel space or vice versa. Enqueuing and or dequeuing embodiments described herein may be used to read and write data to a shared buffer from one stage to another stage. Any type of reading and or writing techniques may be used to read or write data to the shared buffer.

At this point of the example the dequeue function begins to read data from the cache line as shown in . Data1 is read from memory location A and the tail pointer is incremented. During the same time period Data4 has been written into memory location D and the head is incremented to point to A. shows Data5 written to memory location A and the head pointer incremented to point to B. Meanwhile the tail pointer now points to memory location C and memory locations A and B have been filled with NULL. shows Data3 having been read from memory location C. shows Data4 having been read from memory location D. shows the tail pointer caught up with the head pointer. Data5 has since been read from the buffer. NULL has been written into each memory location where data has been read.

In some embodiments the head and tail pointers are designed to never meet at the same memory location. Thus somewhere before tail pointer gets near the head pointer a slip or delay in the dequeue function is implemented to allow the head pointer to move along.

In some instances a head and a tail pointer for enqueuing and dequeuing data to and or from a buffer as described in relation to and come close to one another. For example the tail pointer may read data faster than the head pointer writes the data to the buffer. In such cases a slip between the two pointers may be implemented as shown in . At the start of the method a counter is reset at block . Following which a dequeue and or an enqueue operation is performed at block . The counter is then compared with a repeat value at block . The repeat value may comprise any value. The value may be machine specific or application specific. If the counter is less than the repeat value then the counter is incremented at block . Following which another enqueue and or dequeue operation occurs at block .

The distance between the head pointer and the tail pointer may then be calculated at block . For example the distance may be calculated from the number of memory locations between the two pointers and or the number of cache lines between the two pointers. In another embodiment the distance may be a measure of the number of entries in the butter or a measure of the number of empty memory locations in the buffer. Various other schemes for calculating the distance between the head and tail pointer may used. At block it may be determined whether the distance between the head and tail pointers is greater than a danger distance. The danger distance for example may be calculated dynamically based on the performance of the enqueuing and dequeuing. In other embodiments the danger distance may be a set figure for example the danger distance may be 2 or more cache lines. If the distance between the tail and head pointers is greater than the danger distance then the system returns to block where the counter is reset. If the distance between the head and tail pointer is less than the danger distance dequeuing is paused at block . The dequeue may be paused a number of frames a time period a number of iterations or a number of cache lines processed by the head pointer according to one embodiment. The dequeue may also be paused according to a set figure or on a figure that depends on the distance calculated between the head and tail pointer.

The enqueue and dequeue operations as described above in regard to may be applied in different and or independent address spaces for example kernel space user space and or shared memory. The enqueue and or dequeue functionality may be fully decoupled and or concurrent. Moreover the enqueue and dequeue functions may be used to share data between processors when implementing pipeline parallel processing as described above or data parallelism.

It is noted that the embodiments may be described as a process which is depicted as a flowchart a flow diagram a data flow diagram a structure diagram or a block diagram. Although a flowchart may describe the operations as a sequential process many of the operations can be performed in parallel or concurrently. In addition the order of the operations may be rearranged. A process is terminated when its operations are completed but could have additional steps not included in the figures. A process may correspond to a method a function a procedure a subroutine a subprogram etc. When a process corresponds to a function its termination corresponds to a return of the function to the calling function or the main function.

Furthermore embodiments may be implemented by hardware software scripting languages firmware middleware microcode hardware description languages and or any combination thereof. When implemented in software firmware middleware scripting language and or microcode the program code or code segments to perform the necessary tasks may be stored in a machine readable medium such as a storage medium. A code segment or machine executable instruction may represent a procedure a function a subprogram a program a routine a subroutine a module a software package a script a class or any combination of instructions data structures and or program statements. A code segment may be coupled to another code segment or a hardware circuit by passing and or receiving information data arguments parameters and or memory contents. Information arguments parameters data etc. may be passed forwarded or transmitted via any suitable means including memory sharing message passing token passing network transmission etc.

For a firmware and or software implementation the methodologies may be implemented with modules e.g. procedures functions and so on that perform the functions described herein. Any machine readable medium tangibly embodying instructions may be used in implementing the methodologies described herein. For example software codes may be stored in a memory. Memory may be implemented within the processor or external to the processor. As used herein the term memory refers to any type of long term short term volatile nonvolatile or other storage medium and is not to be limited to any particular type of memory or number of memories or type of media upon which memory is stored.

Moreover as disclosed herein the term storage medium may represent one or more memories for storing data including read only memory ROM random access memory RAM magnetic RAM core memory magnetic disk storage mediums optical storage mediums flash memory devices and or other machine readable mediums for storing information. The term machine readable medium includes but is not limited to portable or fixed storage devices optical storage devices wireless channels and or various other storage mediums capable of storing that contain or carry instruction s and or data.

In the appended figures similar components and or features may have the same reference label. Further various components of the same type may be distinguished by following the reference label by a dash and a second label that distinguishes among the similar components. If only the first reference label is used in the specification the description is applicable to any one of the similar components having the same first reference label irrespective of the second reference label.

While the principles of the disclosure have been described above in connection with specific apparatuses and methods it is to be clearly understood that this description is made only by way of example and not as limitation on the scope of the disclosure.

