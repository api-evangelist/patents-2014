---

title: Mesh network of simple nodes with centralized control
abstract: A mesh network of wired and/or wireless nodes is described in which a centralized controller provides seamless end-to-end service from the edge of the mesh network to mesh nodes located proximate to subscriber devices. The controller operates to provide a central configuration point for configuring forwarding planes of the mesh nodes of the mesh network, so as to set up transport data channels to transport traffic from the edge nodes via the mesh nodes to the subscriber devices.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09634928&OS=09634928&RS=09634928
owner: Juniper Networks, Inc.
number: 09634928
owner_city: Sunnyvale
owner_country: US
publication_date: 20140929
---
A wide variety of devices connect to networks to access resources and services provided by packet based data networks such as the Internet enterprise intranets content providers and virtual private networks VPNs . For example many fixed computers utilize fixed communication links such as optical digital subscriber line or cable based connections of networks to access the packet based services. In addition a vast amount of mobile devices such as cellular or mobile smart phones and feature phones tablet computers and laptop computers utilize mobile connections such as cellular radio access networks to access the packet based services. Some types of networks require enormous scale. Examples include mesh networks of sensors or motes internet of things IoT networks large service provider access networks using small cells WiFi mesh networks mobile ad hoc networks MANET etc.

Networks typically make use of a host of communication equipment and technologies. It can be a challenge to make these networks robust economically viable and profitable for the operators. The network elements may potentially be deployed in the thousands or hundreds of thousands.

Each service provider network typically provides an extensive access network infrastructure to provide packet based data services to the offered services. The access network typically includes a vast collection of access devices and high speed edge routers interconnected by communication links. These devices may execute various protocols and exchange signaling messages to anchor and manage subscriber sessions and communication flows associated with the subscribers.

In general a mesh network is described in which a centralized controller facilitates seamless end to end data forwarding from a core facing edge of the mesh network through mesh nodes located proximate to subscriber devices. The controller provides a central point for configuring the mesh nodes of the mesh network to provide transport services to transport traffic as needed between mesh nodes. In some examples the mesh nodes may be wireless mesh nodes and or wired mesh nodes.

The mesh nodes can be configured by the centralized controller to forward traffic along transport data channels defined within the wireless mesh network. Mesh nodes at the borders of the mesh network can operate as endpoints for the data channels to map subscriber traffic into and out of the data channels. In this way the controller provides a centralized cloud based control plane to configure the mesh nodes to provide transport data channels between edge nodes and the mesh nodes such as for transport of subscriber traffic.

Each mesh node may provide minimal control plane function and essentially operate as low cost slave devices to the centralized controller. As such the mesh nodes may be lower cost and have less management complexity than high end devices with more complex control plane functions.

Further a Cloud Control Protocol CCP is described which the mesh nodes can use to automatically discover neighboring nodes and automatically connect to the centralized controller. As described CCP simplifies topology discovery and path provisioning process within the mesh network and provides a simple highly available scalable protocol for use within the mesh network even though the mesh nodes provide little to no control plane functionality.

In one example a system includes a mesh network comprising a plurality of mesh nodes wherein each of the plurality of mesh nodes is configured to communicate with one or more subscriber devices one or more edge nodes to couple the mesh network to a core network and a centralized controller in communication with the plurality of mesh nodes and edge nodes. The centralized controller includes a topology module that executes a control protocol to receive topology information for the mesh network from the plurality of mesh nodes and a path computation module PCM that computes forwarding information for one or more data channels based at least in part on the topology information wherein the data channels are for transporting network packets to and from the subscriber devices via the mesh nodes. Each of the mesh nodes operates a reduced control plane without execution of a layer three L3 routing protocol that maintains routing information for the mesh network and generates forwarding information for the respective mesh node. The centralized controller outputs one or more messages to the mesh nodes to communicate and install within each of the mesh nodes the forwarding information for the one or more data channels.

In a further example a method is described for configuring a mesh network having a plurality of mesh nodes to transport network packets between mesh nodes and a plurality of edge nodes. The method includes by a centralized controller of the mesh network receiving topology information from the plurality of mesh nodes and a plurality of edge nodes and by a path computation module PCM of the centralized controller of the mesh network computing forwarding information for one or more transport data channels based at least in part on the topology information wherein the data channels are for transporting network packets between the mesh nodes and the edge nodes. The method further includes outputting with the centralized controller of the network one or more messages to communicate and install within each of the mesh nodes the forwarding information for establishing the one or more data channels wherein each of the mesh nodes operates a reduced control plane without execution of a layer three L3 routing protocol that maintains routing information for the mesh network and generates forwarding information for the mesh nodes. The method also includes receiving the network packets associated with a plurality of subscriber devices and forwarding with the plurality of mesh nodes the network packets along the data channels between the mesh nodes and the edge nodes.

In a further example a system includes a mesh network comprising a plurality of mesh nodes each configured to communicate with one or more subscriber devices and centralized controller in communication with the plurality of mesh nodes and edge nodes. Each of the mesh nodes forwards one or more Discover messages to neighboring mesh nodes are positioned along paths toward the centralized controller wherein each of the Discover messages specifies an intermediate node list that specifies layer two addresses and interfaces for the mesh nodes that the respective Discover message traversed from an originating one of the mesh nodes. The centralized controller is configured to upon the centralized controller receiving the Discover messages establish a Source Routed Tunnel SRT control channel with each of the mesh nodes based on the intermediate node lists specified by the Discover messages. The centralized controller outputs one or more messages to one or more of the mesh nodes via the SRT control channel to perform one or more control functions on behalf of the mesh nodes.

The techniques described herein may provide one or more advantages. For example the techniques may allow for reducing total operating cost through use of centralized control and use of nodes that are easy to manage and have no persistent configuration. As described herein in some examples the techniques may be used within wireless mesh networks to unify disparate wireless enabled devices into a single service environment for business residential and mobile applications. Moreover the techniques may allow a mesh network to easily scale with the number of subscriber devices. For example ease of scaling may be achieved by limiting the size of data channel state data structures on nodes in the mesh network and in some examples also limiting each node s awareness of other nodes to only its direct neighbors.

The details of one or more examples are set forth in the accompanying drawings and the description below. Other features objects and advantages will be apparent from the description and drawings and from the claims.

In some examples mesh network may be a network such as a wireless sensor network WSN in which WDs are distributed remote sensing devices including or connected to sensors to monitor physical or environmental conditions such as temperature sound pressure or others. For example WDs may be wireless enabled sensors for forest fire detection air pollution monitoring landslide detection water quality monitoring natural disaster prevention machine health monitoring structural health monitoring or other types of sensing or monitoring.

In some examples mesh network may be an Internet of Things IoT network in which WDs are various items communicating wirelessly via WAPs or via one or more wired connections. For example WDs may be wireless enabled household appliances. In some examples WDs may be wired mesh nodes connected via one or more wired connections such as an Ethernet connection. In other examples mesh network may operate as a private network that provides packet based network services to computing devices A C computing devices such as a service provider access network using small cells. In some aspects mesh network may be a WiFi mesh network or a mobile ad hoc network MANET . In any of the above examples wireless mesh nodes of mesh network devices could be communicating using any of a variety of wireless protocols such as Bluetooth a WiFi protocol or Zigbee for example. Although some aspects are described herein for purposes of example with respect to wireless mesh networks the techniques of this disclosure are equally applicable to wired mesh networks in which the mesh nodes are connected to one another by physical wired connections e.g. Ethernet connections. For example mesh nodes e.g. WDs and WAPs can include one or more of wired mesh nodes that communicate via a wired connection wireless mesh nodes that communicate via wireless communications and mesh nodes that communicate via both wired and wireless connections.

Computing devices may be for example personal computers laptop computers or other types of computing device associated with subscribers. Computing devices may comprise for example mobile telephones laptop or desktop computers having e.g. a 3G wireless card wireless capable netbooks tablet devices video game devices pagers smart phones personal data assistants PDAs or the like. Each of computing devices may run a variety of software applications such as word processing and other office support software web browsing software software to support voice calls video games videoconferencing and email among others. One or more of computing devices may be mesh nodes within mesh network in the sense that the computing devices can serve as an intermediate hop along a data channel to a subscriber device. Any of WDs or computing devices can be subscriber devices i.e. endpoints to data channels via mesh network .

In the example of network system includes a centralized controller that provides complete control plane functionality for mesh network . As described herein controller may facilitate end to end data forwarding from a core facing edge of the mesh network through wireless access points and WDs located proximate to computing devices . The controller provides a central point for configuring the wireless access points and WDs of the mesh network to provide transport services to transport traffic as needed between WDs .

Although described for purposes of example as including both WAPs and WDs mesh network may in some examples include only WAPs that communicate with computing devices . In some examples WAPs and WDs may be referred to herein generally as wireless devices wireless nodes wireless mesh nodes wired mesh nodes or simply mesh nodes. 

Mesh network and aggregation network can provide transport services for network traffic associated with subscribers . Aggregation network may in some examples include one or more aggregation nodes AG such as internal routers and switches that provide transport services between WDs and edge nodes ENs A D ENs . In some examples after authentication and establishment of network access through network any one of computing devices may begin exchanging data packets with public network with such packets traversing network and AGs . Although not shown aggregation network may include other devices to provide security services load balancing billing deep packet inspection DPI and other services for mobile traffic traversing aggregation network .

As described herein controller operates to provide a central configuration point for configuring AGs of aggregation network and WAPs and WDs of mesh network such as to provide transport services to transport traffic between devices of mesh network and edge nodes . AGs may in some examples operate as label switched routers LSRs that forward traffic along transport label switched paths LSPs defined within aggregation network . Edge nodes may operate as endpoints for the LSPs to map subscriber traffic into and out of the LSPs. For example edge nodes may map network services to individual LSPs within aggregation network .

Mesh nodes within mesh network e.g. WAPs and or WDs can be simple and inexpensive by design with a minimal control plane and basic data forwarding support. The mesh nodes can support plug and play deployment control channel establishment to controller and participate in auto discovery of topology as described in further detail herein. Other higher level control functionality is performed on controller which configures all the functions on the mesh nodes. The simplicity of processing required in the WDs and the use of standard forwarding mechanisms is expected to reduce the cost of hardware required in the nodes. Devices in network system may therefore cost less and have a lower management complexity than conventional high end routers typically used within a wired or wireless mesh network and an aggregation network since such devices often provide complex control plane functions. For example when the software running on the mesh node has very few features software upgrades may rarely be needed. This coupled with centralized management and trouble shooting can in some examples reduce the overall total cost of ownership associated with mesh network .

The architecture described herein uses a control protocol referred to herein a Cloud Control Protocol CCP to allow the mesh nodes in network system to be as simple as possible with minimal control functionality while allowing the controller to perform the complex control functions. Controller and the mesh nodes operate in accordance with the CCP to set up a Source Routed Tunnel SRT control channel between controller and one or more of the mesh nodes. Controller can output one or more messages to one or more of the mesh nodes via the SRT control channel to perform one or more control functions on behalf of the mesh nodes. For example the one or more control functions can include one or more of configuration of the mesh nodes monitoring status of the mesh nodes image download to the mesh nodes gathering traffic statistics about network traffic at the mesh node gathering information about local load conditions on the mesh nodes gathering information about error rates on the mesh nodes or other control functions. In some examples the one or more control functions include computing forwarding information for one or more data channels based at least in part on topology information received from the one or more mesh nodes via the discover messages wherein the data channels are for transporting network packets to and from subscriber devices via the mesh nodes.

As shown in mesh network is comprised of a collection of mesh nodes and wired or wireless links between the mesh nodes. WAPs may have a wired connection to AGs and or ENs . WAPs may be 802.11 access points running an 802.11 protocol for example.

In some examples WAPs and or WDs may be access nodes that provide Ethernet services to an Endpoint e.g. one of subscribers . Such services may be subjected to per packet policy and Class of Service CoS based policing uplink only . The access node maps the port through which an Endpoint is connected to a data channel that carries the Endpoint s traffic to from the network service located at an Edge Node . An access node may also locally switch traffic directly between two ports or directly between itself and another access node. WAPs and or WDs may be configuration less at boot time and acquires its entire configuration from the controller . WAPs and or WDs may be use CCP to discover its neighbors and set up a control channel to the Controller .

ENs map data channels to network services. ENs may also apply downlink policy and CoS policing to the traffic admitted to the data channel. Network services are configured and managed on the ENs . In some examples network system may include more than one controller . ENs may be configured and connected directly to some management network where controller s reside. The ENs may be configured with the IP Address of the controller s . In the example of network system includes multiple edge nodes that provide multiple exit points to mesh network . For example ENs A B provide exit points to public network and ENs C D provide exit points to data center .

An Edge Node discovers its neighbors via CCP on the interfaces to which CCP is configured. Since the EN is connected to the Controller via the Internet Protocol IP management network the EN need not discover a control channel. Rather EN directly uses the configured IP addresses to communicate with the Controller . The EN also has the responsibility to indicate its Network Services to the Controller . These network services may then be used by the Controller to identify which ENs host which services when sessions are to be admitted into the network.

AGs do not have to perform any access or edge functions. AGs collectively serve as a switching fabric over which the data services are delivered to the Endpoints. The AG uses CCP for neighbor discovery and control channel establishment. Note that a node can be both an access or mesh node if it has endpoints connected to it as well as an AG to other access or mesh nodes whose traffic is transiting through it.

In wireless networks many links share the same set of resources e.g. the same RF spectrum. For example when WD B sends a packet to WD A WD B might not be able to simultaneously send a packet to WD C because WD B is using the same radio for both. Bandwidth across a wireless link may be variable over time for example due to environmental factors. Error rates are much higher in wireless links than wired links so the wireless protocols use the error rates to determine how fast they can transmit. In some examples wireless nodes can include within discover messages a generic metric that tries to capture all these features. Link attributes can be updated continuously periodically in real time as things change.

In the example of a wireless mesh of access points using WiFi location services can run on top of a northbound API to locate where the user is relative to an access point. The northbound API gives information about different attributes from the network and simplify for controller what is needed by the application. Controller can receive policies from applications via the northbound interface.

As further described below controller includes a path computation module PCM that handles topology computation and path provisioning for the whole of mesh network . That is the PCM of controller processes topology information for mesh network performs path computation and selection in real time based on a variety of factors including current load conditions of subscriber traffic and provisions the data channels within the aggregation network.

Once controller computes the paths and the detours the controller configures forwarding information in the ENs AGs WAPs and WDs with the appropriate forwarding information for both upstream and downstream directions so that traffic forwarding is enabled. In addition to the primary forwarding entries controller configures the secondary forwarding entries as well so that switchover in case of link or node failure can happen without involvement of controller .

Once a mesh node has joined the network and is forwarding traffic it continues to send periodic messages to its neighbors. If a link or node fails it is discovered via this mechanism. The nodes around the failure each independently figure out this change and switch the impacted paths to their pre configured detours. While the data plane continues its operation uninterrupted each node exchanges messages with its neighbors to check which links and nodes are active. Each node then reports this information via a Hello message which is sent to the neighboring node with the shortest path to the Controller . The latter forwards this message to the Controller which is thus notified of the topology change. The Controller re computes the topology and the paths and configures the required changes if any into the relevant nodes. The path is repaired in a make before break fashion at the node adjacent to the failure. Then the old portion of the path is removed. Note that if a detour becomes unused it should not be deleted until all the paths that rely on it have been re assigned.

The Controller computes and configures paths based on the bandwidth required by that path total of all the traffic carried over it . If for some reason a link capacity on that path changes e.g. fading due to rain on a wireless backhaul link this information is conveyed to the Controller which then adapts to changes in link capacity and re computes an alternate path for the traffic and re configures the impacted nodes to switch the traffic over in a make before break fashion.

The CCP boundary illustrates the region in which devices communicate using CCP with the addition of controller . In one example CCP supports automatic discovery of neighboring WDs and WAPs and provides a mechanism by which any WDs and WAPs can describe its neighboring nodes to controller . For example CCP provides an elegant mechanism for establishing a control channel by which controller configures and controls WDs and WAPs within mesh network . Moreover CCP allows the control channel to be established independent of the data plane of mesh network being operational yet does not require a parallel control network. Further CCP provides mechanisms for programming forwarding information into WAPs WDs and AGs data planes detour next hops per CoS policers and per interface packet scheduling.

In some examples mesh network may be a radio access network with an access node that provide computing devices with access to aggregation network via radio signals. For example computing devices may be connected to one or more wireless radios or base stations not shown to wirelessly exchange packetized data with computing devices such as by converting modulated optical signals to electrical signals for transmission to computing devices via wireless signals.

Aggregation network provides session management mobility management and transport services to support access by computing devices to public network . Edge node provides an anchor point of active sessions for computing devices . Edge node may maintain session data and operate as a termination point for communication sessions established with computing devices that are currently accessing packet based services of public network via aggregation network . Examples of a high end mobile gateway device that manages subscriber sessions for mobile devices are described in U.S. patent application Ser. No. 13 248 834 entitled MOBILE GATEWAY HAVING REDUCED FORWARDING STATE FOR ANCHORING MOBILE SUBSCRIBERS the entire content of which is incorporated herein by reference.

In some examples wireless interfaces of WDs and or WAPs provide an execution environment for a plurality of schedulers one for each port coupled to the wireless interface i.e. one for each radio channel. Each scheduler dynamically services data transmission requests for the set of devices communicating at the given channel thereby allowing controller to dynamically schedule data transmissions so as to utilize otherwise unused communication bandwidth.

In some examples mesh network may comprise an optical access network. For example one or more of WDs may comprise an optical line terminal OLT connected to one or more endpoints or optical network units ONUs via optical fiber cables. In this case WDs may convert electrical signals from aggregation network to optical signals using an optical emitter i.e. a laser and a modulator. WDs then transmits the modulated optical signals over one or more optical fiber cables to customer premises equipment CPEs which act as termination points of the optical access network. As one example WDs converts received modulated optical signals to electrical signals for transmission to computing devices A over copper cables. As one example WDs or WAPs may comprise a switch located in a neighborhood or an office or apartment complex capable of providing access to a plurality of computing devices A. In other examples such as fiber to the home FTTH WDs or WAPs may comprise a gateway located directly at a single family premise or at an individual business capable of providing access to the one or more computing devices A at the premise. In the case of a radio access network the EPs may be connected to wireless radios or base stations and convert the modulated optical signals to electrical signals for transmission to computing devices B via wireless signals.

In one example WDs or WAPs may provide optical interfaces that are each capable of optically communicating with a plurality of different endpoints through a common optical interface. WDs or WAPs may for example communicate with endpoints through a passive optical network using wave division multiplexing. Further endpoints may be low cost optical emitter free endpoints that incorporate a specialized optical interface that utilizes reflective optics for upstream communications. In this way multiple endpoints are able to achieve bi directional communication with WDs or WAPs through a single optical interface of the access router even though the EPs are optical emitter e.g. laser free. In some examples mesh network may further utilize optical splitters not shown for the optical communications associated with each of the different wavelengths provided by the optical interfaces of WDs or WAPs . Further example details of an optical access network that uses wave division multiplexing and dynamic scheduling in conjunction with emitter free EPs can be found in U.S. Provisional Patent Application OPTICAL ACCESS NETWORK HAVING EMITTER FREE CUSTOMER PREMISE EQUIPMENT AND ADAPTIVE COMMUNICATION SCHEDULING filed Dec. 16 2012 the entire contents of which are incorporated herein by reference.

In some examples wired interfaces of WDs provide an execution environment for a plurality of schedulers one for each virtual port on a shared wired medium e.g. Passive Optical Network PON . Each scheduler dynamically services data transmission requests for the set of devices communicating on the shared medium thereby allowing controller to dynamically schedule data transmissions so as to utilize otherwise unused communication bandwidth.

Further examples of a central controller are described in U.S. Pat. No. 8 693 374 issued Apr. 8 2014 entitled CENTRALIZED CONTROL OF AN AGGREGATION NETWORK WITH A REDUCED CONTROL PLANE and U.S. Pat. No. 8 711 855 issued Apr. 29 2014 entitled TOPOLOGY DISCOVERY CONTROL CHANNEL ESTABLISHMENT AND DATAPATH PROVISIONING WITHIN AN AGGREGATION NETWORK WITH CENTRALIZED CONTROL the entire contents of each of which are incorporated by reference herein.

Edge node provides an anchor point of active sessions for computing devices A. In this sense edge node may maintain session data and operate as a termination point for communication sessions established with computing devices A that are currently accessing packet based services of public network via aggregation network . In one example instance Edge Nodes terminate the discovery process and forwards all Discover packets received to the CCP controller . The edge nodes are also connected to existing network s e.g. core networks and may provide a connection for receiving network services from existing networks for example. Example operation of one implementation of CCP is as follows. Mesh nodes discover their neighbors by sending Hello messages on all of their CCP links. The examples described below with reference generally to nodes are applicable to both WAPs and WDs . The links may be any type of computer network transmission medium. For instance a link may be a fiber optic cable an Ethernet cable a wireless connection and so on. When a Hello Reply is received a neighbor is discovered. Once a neighbor is discovered on a link the link is declared as active and it is added to a neighbor set for that node. A neighbor set is defined herein as a set specifying each of the active interfaces and an identifier of the neighbor reachable by the interface. The neighbor set is then sent across all active links via a Discover packet.

In one example a Discover packet contains a generation number the neighbor list and an intermediate node list that is initially empty. The Discover packet is sent out all active links. The receiver of the Discover packet first checks to see if the receiver is on the intermediate node list. If the receiver is on the list this implies that the packet has visited the node before and the packet is dropped. If the receiver of the Discover packet is not on the list the node adds itself to the list and then floods the packet out all active links other than its ingress link.

In some examples the Edge Nodes do not flood Discover packets out of their active links. Instead edge nodes send Discover packets directly to the controller . When the controller receives the Discover packet controller compares the generation number against the current generation number received for that node. If the generation number is newer controller updates the neighbor list and the path to the node. The path to the node is computed by reversing the path the Discover packet took as recorded in the intermediate node list. This path is referred to as a Source Routed Tunnel SRT control channel and is used for the duration of this generation number for all CCP communications with the node.

The controller responds to the first Discover packet of a given generation number by issuing a Flood Reply via the SRT to the node. When a node receives a Flood Reply which carries the Source Route List it now has an SRT back to the controller . At this point the controller and the node are in sync with respect the node s neighbor list and the SRT used to send additional CCP control messages. The node sends Keepalive packets to the controller to ensure the state of the SRT. The controller responds with a Keepalive Reply. If no Keepalive Reply occurs the node generates a new Discover packet with a new generation number to force the acceptance at controller of a new SRT. The SRT control channel may now be used to program the forwarding plane of node via other control messages.

In one example the CCP may include features such as being simple highly available and scalable. The CCP can support neighbor discovery with fast keep alives. CCP provides a mechanism whereby a node can describe its neighbors to a controller. CCP allows for Establishing a Control Channel independent of the data plane being operational yet does not require a parallel control network. CCP provides FIB programming facilities including detour next hops per CoS policers and per interface packet scheduling. CCP runs over the standard Ethernet MAC or simulations thereof. HELLO HELLO REPLY messages may be exchanged over Ethernet interfaces to discover and maintain neighbor state. Messages include the node specific interface index so that the global topology may be understood.

CCP may have one or more advantages relative to other protocols such as automatic Control Channel establishment independent of data plane there is no need to provide a parallel control network minimal Control plane complexity versus IGPs only maintains local neighbor state and directed FIB state better control plane scaling versus IGPs the entire routing table is not sent to all nodes convergence issues are reduced via the controller can avoid soft state rollback issues versus RSVP minimal Forwarding plane functionality detour path support simpler switching silicon Plug and Play no on box configuration.

The techniques described herein may provide certain advantages. For example the techniques may allow a service provider to achieve a reduction in total operating cost through use of centralized controller in conjunction with high speed aggregation nodes that are easy to manage and have no persistent configuration. Controller provides a network operator with a single touch point into the network to monitor and troubleshoot without having to query multiple nodes in the network. Moreover the techniques may be utilized within aggregation networks to unify disparate edge networks into a single service delivery platform for business residential and mobile applications. Moreover the techniques can provide a mesh network architected to easily scale as the number of computing devices increases.

Network system includes computing devices e.g. computing devices A D computing devices which may be for example personal computers laptop computers tablets smart phones or other types of computing device associated with subscribers. Computing devices may comprise for example mobile telephones laptop or desktop computers having e.g. a 3G wireless card wireless capable netbooks tablet devices video game devices pagers smart phones personal data assistants PDAs or the like. Each of computing devices may run a variety of software applications such as word processing and other office support software web browsing software software to support voice calls video games videoconferencing and email among others.

CCP boundary illustrates the logical region in which devices communicate using CCP. In one example the home entertainment system runs the controller e.g. as a virtual machine running on a CPU of home entertainment system and controller can set up paths within the mesh network. In one example home entertainment system may be a Playstation device. When a new device comes into the home within range of home entertainment system the device can communicate its wireless link information up to controller via discover messages in the wireless mesh network as described herein and controller of home entertainment system can set up a data channel for the new device.

Controller learns topology of the wireless mesh via Discover messages. The discover messages can include information such as capacity of the links what the technology is how much bandwidth is required by the application the wireless device needs to run over the link and controller determines the optimal path based on some metrics constraints per above and then configures the wireless device in its native data plane protocol. Controller configures data plane tables to set up forwarding paths as needed. Controller can establish and install state for primary paths and backup paths in the data plane in the first case. If a node in the middle dies the surrounding nodes figure it out and communicate to controller and controller computes alternate paths for the mesh. In the meantime the remaining nodes can use the backup paths that were installed in this manner redundancy is built in to the system. For example if a person suddenly happens to be standing in the path of a wireless link such that traffic would need to be routed around that link the alternate path can be computed by the controller based on its topology information received from the wireless devices via Discover messages and provided to the wireless devices.

One or more processors in one example are configured to implement functionality and or process instructions for execution within WD . For example processors may be capable of processing instructions stored by storage device . Examples of one or more processors can include any one or more of a microprocessor a controller a DSP an ASIC a FPGA or equivalent discrete or integrated logic circuitry.

One or more storage devices may be configured to store information within WD during operation. Storage devices in some examples include a computer readable storage medium or computer readable storage device. In some examples storage devices include a temporary memory meaning that a primary purpose of storage device is not long term storage. Storage devices in some examples include a volatile memory meaning that storage device does not maintain stored contents when power is not provided to storage device . Examples of volatile memories include random access memories RAM dynamic random access memories DRAM static random access memories SRAM and other forms of volatile memories known in the art. In some examples storage devices are used to store program instructions for execution by processors . Storage devices in some examples are used by software or applications running on WD e.g. sensor analysis module to temporarily store information during program execution.

In some examples storage devices may further include one or more storage device configured for longer term storage of information. In some examples storage devices include non volatile storage elements. Examples of such non volatile storage elements include magnetic hard discs optical discs floppy discs flash memories or forms of electrically programmable memories EPROM or electrically erasable and programmable EEPROM memories.

WD in some examples also includes one or more communication units . WD in one example utilizes communication unit to communicate with external devices via one or more networks such as one or more wireless networks. Communication unit may be a network interface card such as an Ethernet card an optical transceiver a radio frequency transceiver or any other type of device that can send and receive information. Other examples of such network interfaces may include Bluetooth 3G and WiFi radios computing devices as well as Universal Serial Bus USB . In some examples WD utilizes communication unit to wirelessly communicate with an external device such as mobile computing device . Communication units can be controlled by telemetry module .

WD in one example also includes one or more input devices . Input device in some examples is configured to receive input from a user such as through tactile audio or video sources. Examples of input device include a presence sensitive device such as a presence sensitive display a mouse a keyboard a voice responsive system video camera microphone or any other type of device for detecting a command from a user. In some examples a presence sensitive display includes a touch sensitive display.

One or more output devices may also be included in WD . Output device in some examples is configured to provide output to a user using tactile audio or video stimuli. Output device in one example includes a presence sensitive display a sound card a video graphics adapter card or any other type of device for converting a signal into an appropriate form understandable to humans or machines. Additional examples of output device include a speaker a CRT monitor a LCD OLED or any other type of device that can generate intelligible output to a user. In some examples UI device may include functionality of one or more of input devices and or output devices .

WD also can include UI device . In some examples UI device is configured to receive tactile audio or visual input. In addition to receiving input from a user UI device can be configured to output content such as a GUI for display at a display device such as a presence sensitive display. In some examples UI device can include a presence sensitive display that displays a GUI and receives input from a user using capacitive inductive and or optical detection at or near the presence sensitive display. In some examples UI device is both one of input devices and one of output devices .

WD may include operating system . Operating system in some examples controls the operation of components of WD . For example operating system in one example facilitates the communication of UI module and sensor analysis module with processors communication units storage devices input devices output devices and sensor . UI module sensor analysis module telemetry module notification module and vicinity module can each include program instructions and or data that are executable by WD e.g. by one or more processors . As one example UI module can include instructions that cause WD to perform one or more of the operations and actions described in the present disclosure.

In examples in which WD is one of WDs WD can include one or more sensors . Sensors can be for example one or more sensors for forest fire detection air pollution monitoring landslide detection water quality monitoring natural disaster prevention machine health monitoring structural health monitoring or other types of sensing or monitoring. In some examples sensors can be configured to generate a signal indicative of the sensor data. Sensor analysis module can then receive the signal indicative of the sensor data.

WD can include additional components that for clarity are not shown in . For example WD can include a battery to provide power to the components of WD . Similarly the components of WD shown in may not be necessary in every example of WD .

WD executes a cloud control protocol CCP module that operates in accordance with a cloud control protocol also referred to herein as a discovery protocol or an open centralized control protocol. In some examples cloud control protocol module outputs a hello message e.g. a Cloud Control Protocol CCP Hello message on each interface and or link. Each of the Hello messages includes an identifier that is unique to WD e.g. an aggregation node or access node that sent the hello message and the interface on which the Hello message was sent. In accordance with the protocol WD also outputs a Hello Reply message on each interface on which a Hello message was received. Cloud control protocol module maintains a neighbor node list that identifies neighboring nodes from which WD received Hello messages and the interfaces on which the Hello messages were received. Although described for purposes of example in terms of interfaces and links the techniques of this disclosure apply to wireless interfaces radio channels and other wireless communication links.

Responsive to receiving CCP Hello Reply messageon a link WD may in some examples declare the link as an active link and adds the neighboring node to the neighbor node list . Cloud control protocol module outputs discover messages that each specify the neighbor node list identifying neighboring nodes and interfaces on which neighboring nodes are reachable from WD .

In addition upon receiving a discover message and determining that the discover message does not include a layer two address for a recipient one of the nodes cloud control protocol module updates a stored intermediate node list IM node list of the discover message that specifies for example layer two addresses and interfaces for the nodes that the discover message traversed from an originating one of the nodes.

Upon updating the discover message cloud control protocol module forwards the discover messages to the neighboring nodes that are positioned along paths toward a central controller e.g. controller of and the centralized controller upon receiving the discover messages establishes a Source Routed Tunnel SRT control channel with each of the nodes including WD based on the intermediate node lists specified by the discover messages. WD executes the cloud control protocol module without executing an Interior Gateway Protocol IGP within a control plane not shown of WD .

The centralized controller computes the topology information for the network and computes the forwarding information for the transport data channel s in accordance with the neighbor node list within each of the discover messages received from the network.

WD receives from the controller and via the respective SRT control channels the pre computed forwarding information computed by the centralized controller for configuring WD to forward the network packets on the data channels. The pre computed forwarding information may in some examples include directed forwarding information state including information needed for WD to forward packets on a data channel. In some examples the directed forwarding information state includes policers to police ingress traffic for the data channel according to the computed bandwidth. In some examples WD maintains link scheduling information . WD may maintain information regarding error rates for transmission on wireless links in error rates and may update this information in real time as network conditions change. Error rates may be expressed in a particular time it takes to transfer 100 bytes of data based on sampled measurements for example. WD can include information regarding error rates link scheduling load conditions for the wireless links in discover messages sent to the centralized controller. WD can indicate that particular links are shared.

Based on the forwarding information and link information the centralized controller also computes one or more backup data channels for the network and outputs one or more messages to WD to communicate and install within WD forwarding information for the backup data channels. WD stores the forwarding information for the data channels and the backup data channels to forwarding information . The messages received from the centralized controller can in some examples include directed link scheduling state to install within link scheduling information to indicate how traffic should be schedule on wireless links e.g. when the links are at full utilization as computed by the controller. The controller can determine the demand put on the network because it has the end to end path requirements. WD forwards packets based on forwarding information and link scheduling information . In response to a network event forwarding component may re route at least a portion of the network packets along the backup data channel. The network event may be for example a link or node failure. The controller may also compute detour data channels to handle fast reroute for any interior node failure.

In some examples when forwarding the discover messages WD modifies the discover messages to include one or more link characteristics associated with the interfaces and the centralized controller computes the forwarding information for the data channels based at least in part on quality of service QoS metrics requested for the data channels and the link characteristics received from the discover messages.

In some examples WD sends Keepalive packets to the centralized controller network device to ensure a state of the SRT control channel and responsive to determining that no Keepalive Reply is received from centralized controller network device within a time period WD generates a new Discover message with a new generation number to force acceptance at a centralized controller network device of a new SRT control channel.

In this manner WD has a reduced control plane that does not execute a Multiprotocol Label Switching MPLS protocol for allocation and distribution of labels for LSPs and does not execute a routing protocol such as an interior gateway protocol IGP . Instead WD executes the cloud control protocol module to receive forwarding information directly from a central controller e.g. controller of without requiring conventional forwarding plane signaling. In various examples the messages exchanged between WD and the centralized controller may conform to any of the message formats described herein.

In one embodiment WD may comprise one or more dedicated processors hardware and or computer readable media storing instructions to perform the techniques described herein. The architecture of WD illustrated in is shown for example purposes only. In other embodiments WD may be configured in a variety of ways.

In the example of controller includes a control unit coupled to a network interface to exchange packets with other network devices by inbound link and outbound link . Control unit may include one or more processors not shown in that execute software instructions such as those used to define a software or computer program stored to a computer readable storage medium again not shown in such as non transitory computer readable mediums including a storage device e.g. a disk drive or an optical drive or a memory such as Flash memory or random access memory RAM or any other type of volatile or non volatile memory that stores instructions to cause the one or more processors to perform the techniques described herein. Alternatively or additionally control unit may comprise dedicated hardware such as one or more integrated circuits one or more Application Specific Integrated Circuits ASICs one or more Application Specific Special Processors ASSPs one or more Field Programmable Gate Arrays FPGAs or any combination of one or more of the foregoing examples of dedicated hardware for performing the techniques described herein.

Control unit provides an operating environment for network services applications access authorization provisioning module path computation element topology module path provisioning module and edge authorization provisioning module . In one example these modules may be implemented as one or more processes executing on one or more virtual machines of one or more servers. That is while generally illustrated and described as executing on a single controller aspects of these modules may be delegated to other computing devices.

Network services applications represent one or more processes that provide services to clients of a wireless mesh network that includes controller to manage connectivity in the path computation domain according to techniques of this disclosure. Network services applications may provide for instance include Voice over IP VoIP Video on Demand VOD bulk transport walled open garden IP Mobility Subsystem IMS and other mobility services and Internet services to clients of the service provider network. Networks services applications require services provided by path computation element such as node management session management and policy enforcement. Each of network services applications may include client interface by which one or more client applications request services. Client interface may represent a command line interface CLI or graphical user interface GUI for instance. Client may also or alternatively provide an application programming interface API such as a web service to client applications.

Network services applications can issue path requests to path computation element to request paths in a path computation domain controlled by controller . In general a path request may include a required bandwidth or other constraint and two endpoints representing wireless devices that communicate over the path computation domain managed by controller for example. Path requests may further specify time date during which paths must be operational and CoS parameters for instance bandwidth required per class for certain paths .

Path computation element accepts path requests from network services applications to establish paths between the endpoints over the path computation domain. Paths may be requested for different times and dates and with disparate bandwidth requirements. Path computation element reconciling path requests from network services applications to multiplex requested paths onto the path computation domain based on requested path parameters and anticipated network resource availability.

To intelligently compute and establish paths through the path computation domain path computation element includes topology module to receive topology information describing available resources of the path computation domain including wireless devices interfaces thereof and interconnecting communication links.

In some instances topology module provides an interface by which controller receives topology information from aggregation network . Topology module may for example use a communication protocol to receive topology information from one or more of WAPs and WDs where the WAPs and WDs automatically discover the topology of mesh network in accordance with the CCP as described herein. One example protocol is OpenFlow is a communication protocol that provides direct access to the data plane of a network switch or router. In some instances controller may receive messages from one or more of WAPs and WDs in accordance with a protocol where payloads of the protocol messages encapsulate CCP messages. In other examples topology module may receive CCP messages directly from WAPs and or WDs . Further example details of the OpenFlow protocol is described in OpenFlow Switch Specification Open Networking Foundation Version 1.2 Dec. 5 2011 incorporated herein by reference.

In some examples topology module may receive topology information from the WAPs and or WDs of mesh network and a path computation module PCM computes forwarding information for transport data channels in accordance with the topology information. In other examples topology module may receive the topology information from a centralized topology database. In a further example topology module may execute an interior routing protocol to receive the topology information from the WAPs and or WDs .

As further explained below in one example implementation path computation module handles topology computation for the whole of mesh network and programs forwarding information into WAPs and WDs by way of path provisioning module . Like topology module path provisioning module may use a communication protocol or the CCP to communicate forwarding information to WAPs and WDs for configuring data planes of the WAPs and WDs .

Path computation module of path computation element computes requested paths through the path computation domain. In general paths are unidirectional. Upon computing paths path computation module schedules the paths for provisioning by path provisioning module . A computed path includes path information usable by path provisioning module to establish the path in the network. Provisioning a path may require path validation prior to committing the path to provide for packet transport.

Access authorization and provisioning module can in some examples program authorization policy provisioning information records as well as packet policers into wireless devices such as WAPs and WDs of . Similarly edge authorization and provisioning module may program authorization policy provisioning information records as well as packet policers into WAPs and WDs . Network services applications may comprise applications for governing and controlling the distribution of such control information to the access nodes and the edge routers.

As further described below the cloud control protocol CCP allows network nodes to discover their neighbors and report these neighbors to controller . Controller can compute the topology of the network based on the reports received by controller . Given this topology in some examples the controller may then compute paths through the network and install forwarding tables in the network nodes to support packet switching between any two nodes in the network. This protocol does not rely on the data plane to be established before the topology can be discovered. Controller can establish a control channel between controller and each of WAPs and or WDs independent of the data channel.

Controller operates in accordance with the CCP to set up a Source Routed Tunnel SRT control channel between controller and one or more of the mesh nodes. Controller can output one or more messages to one or more of the mesh nodes via the SRT control channel to perform one or more control functions on behalf of the mesh nodes. For example other control functions module represents any of a variety of functionality by which controller can perform one or more control functions on behalf of the mesh nodes. These other control functions can include for example one or more of configuration of the mesh nodes monitoring status of the mesh nodes image download to the mesh nodes gathering traffic statistics about network traffic at the mesh node gathering information about local load conditions on the mesh nodes gathering information about error rates on the mesh nodes or other control functions.

In one example the Cloud Control Protocol may enable controller to provide certain advantages. First as one example CCP may enable controller to simplify the implementation of network nodes such as the WDs described herein. The mesh nodes need not each execute a routing protocol that learns routing information and generates forwarding information for the mesh node device. For example each of the mesh nodes may utilize the CCP protocol to automatically discover its neighbors and to report those neighbors to a controller. Second use of the Cloud Control Protocol may enable controller to centralize the complex functions of topology discovery path selection and traffic engineering. Centralization may help achieve the goal of maintaining simplicity of each individual mesh node e.g. by removing complex control plane software from each of the wireless nodes. Centralization may also allow for deterministic provisioning of traffic engineered paths and their failure paths through the network. In some instances centralized path computation by controller may by simpler than distributed computation and may converge more quickly. Centralization may also allow for control and management of an entire mesh network from a single entity e.g. a set of one or more controllers thereby simplifying and enhancing operations by providing a high level view of the mesh network. Third use of CCP may enable separation of the control protocol from the paths used by the data plane so that even when there are failures the control protocol is robust enough to continue to operate. In addition a centralized algorithm may not have convergence issues and can be deterministic.

Path computation module includes data structures to store path information for computing and establishing requested paths. These data structures include constraints path requirements operational configuration and path export . Network services applications may invoke northbound API to install query data from these data structures. Constraints represent a data structure that describes external constraints upon path computation. Constraints allow network services applications to e.g. modify link attributes before path computation module computes a set of paths. In some examples Radio Frequency RF modules may edit information associated with wireless links to indicate that resources are shared between a group and resources must be allocated accordingly. In some examples RF modules may edit information relating to link scheduling and or link loading. Network services applications may modify attributes of links to effect resulting traffic engineering computations in accordance with CCP. In such instances link attributes may override attributes received from topology indication module and remain in effect for the duration of the node attendant port in the topology. A link edit message to constraints may include a link descriptor specifying a node identifier and port index together with link attributes specifying a bandwidth expected time to transmit shared link group and fate shared group for instance. PCE may send the link edit message to the relevant wireless device s in the wireless mesh network.

Operational configuration represents a data structure that provides configuration information to path computation element to configure the path computation algorithm with respect to for example class of service CoS descriptors and detour behaviors. Operational configuration may receive operational configuration information in accordance with CCP. An operational configuration message may specify CoS value queue depth queue depth priority scheduling discipline over provisioning factors detour type path failure mode and detour path failure mode for instance. A single CoS profile may be used for the entire path computation domain. Example CoS values are described as follows 

Queue Depth Queue Depth represents the amount of time a packet can sit in a queue before it becomes stale. For TCP traffic this time is generally the round trip time of the TCP session 150 msec . For VoIP this time is generally 10 to 50 msec. Different nodes may have different buffer capacities. It may not be possible to guarantee a specific time allotment per queue. Nodes should therefore be able to size queues according to the available buffer space and the service class for the queue.

Queue Depth Priority When a class of service is active over some interface the interface queues are sized to buffer at the indicated depth based on the bandwidth for the class. If there is insufficient buffer space queue size is reduced according to queue depth priority. Lower priority classes are reduced before higher priority classes.

Scheduling Discipline Scheduling Discipline determines how the queue is scheduled with respect to other queues. Deficit weighted round robin DWRR may be used together with Strict scheduling for voice traffic. In some examples controller configures the schedulers on all node interfaces according to the bandwidth and scheduling class for each CoS active on the interface.

Over Provisioning Factor When a path is routed through the network path computation domain the path received allocated bandwidth from each link over which the path is routed. For some classes of service is it appropriate to over provision the network. This allows the policers at the edge and access to admit more traffic into the network than the network may actually be able to handle. This might be appropriate in cases where the traffic is best effort for example. By over provisioning certain classes of traffic the network operator may realize better network utilization while still providing required QoS for other classes that are not over provisioned.

Detour Type Specifies the traffic engineering requirements for computed detours. Due to resource restrictions users may elect to configure detours that have fewer constraints than the primary paths. Detour paths may for instance take on one of the following values None Best effort CoS only Strict TE. The None value specifies do not compute detours. The Best effort value specifies compute detours but ignore TE bandwidth and CoS requirements. CoS is dropped from the packet header and therefore the detour traffic gets best effort CoS. The CoS only value specifies preserve CoS but do not traffic engineer the detour. Under these conditions traffic competes with other primary path traffic equally for available resources therefore interface congestion may occur when the detour is active. The Strict TE value specifies preserve CoS and traffic engineering for the detour.

Path Failure Mode Defines the per CoS behavior to take when the primary path computation fails due to resource constraints. The Proportional Path Reduction PPR Ignore and Fail options are available. The PPR option specifies all paths traversing the congested links are reduced proportionally until all paths can be accommodated over the points of congestion. The Ignore option specifies raise an alert message but otherwise allow the network to operate in this oversubscribed manner. The Fail option specifies failure to compute the remainder of the paths and do not admit traffic for failed paths into the network.

Detour Path Failure Mode Defines the behavior of the system when detour paths cannot be computed due to resource constraints. This attribute may only be applicable when Detour Type is Strict TE.

Service Class To make configuration of CoS parameters easier each CoS can be associated with a specific service class that has default values for each parameter. Example service classes are defined in the following table.

The Service Class assigned to a Class of Service may be independent of the node as an attribute of the path computation domain.

Path export represents an interface that stores path descriptors for all paths currently committed or established in the path computation domain. In response to queries received via northbound API path export returns one or more path descriptors. Queries received may request paths between any two edge and access nodes terminating the path s . Path descriptors may be used by network services applications to set up forwarding configuration at the wireless devices terminating the path s . In some examples path descriptor may include an Explicit Route Object ERO . A path descriptor or path information may be sent responsive to a query from an interested party in accordance with CCP. A path export message delivers path information including path type primary or detour bandwidth for each CoS value and for each node in the ordered path from ingress to egress a node identifier ingress label and egress label.

Path requirements represent an interface that receives path requests for paths to be computed by path computation module and provides these path requests including path requirements to path engine for computation. Path requirements may be received in accordance with CCP or may be handled by the PCE. In such instances a path requirement message may include a path descriptor having an ingress node identifier and egress node identifier for the nodes terminating the specified path along with request parameters including CoS value and bandwidth. A path requirement message may add to or delete from existing path requirements for the specified path.

Topology module includes topology indication module to handle topology discovery and where needed to maintain control channels between path computation element and nodes of the path computation domain. Topology indication module may include an interface to describe received topologies to path computation module .

Topology indication module may use CCP topology discovery or some other topology discovery protocol to describe the path computation domain topology to path computation module . Using CCP topology discovery topology indication module may receive a list of node neighbors with each neighbor including a node identifier local port index and remote port index as well as a list of link attributes each specifying a port index bandwidth expected time to transmit shared link group and fate shared group for instance.

Topology indication module may communicate with a topology server such as a routing protocol route reflector to receive topology information for a network layer of the network. Topology indication module may include a routing protocol process that executes a routing protocol to receive routing protocol advertisements such as Open Shortest Path First OSPF or Intermediate System to Intermediate System IS IS link state advertisements LSAs or Border Gateway Protocol BGP UPDATE messages. Topology indication module may in some instances be a passive listener that neither forwards nor originates routing protocol advertisements. In some instances topology indication module may alternatively or additionally execute a topology discovery mechanism such as an interface for an Application Layer Traffic Optimization ALTO service. Topology indication module may therefore receive a digest of topology information collected by a topology server e.g. an ALTO server rather than executing a routing protocol to receive routing protocol advertisements directly.

In some examples topology indication module receives topology information that includes traffic engineering TE information. Topology indication module may for example execute Intermediate System to Intermediate System with TE extensions IS IS TE or Open Shortest Path First with TE extensions OSPF TE to receive TE information for advertised links. Such TE information includes one or more of the link state administrative attributes and metrics such as error rates and or bandwidth available for use at various priority levels of links connecting routers of the path computation domain. In some instances indication module executes BGP TE to receive advertised TE information for inter autonomous system and other out of network links. Additional details regarding executing BGP to receive TE info are found in U.S. patent application Ser. No. 13 110 987 filed May 19 2011 and entitled DYNAMICALLY GENERATING APPLICATION LAYER TRAFFIC OPTIMIZATION PROTOCOL MAPS which is incorporated herein by reference in its entirety.

Traffic engineering database TED stores topology information received by topology indication module for a network that constitutes a path computation domain for controller to a computer readable storage medium not shown . TED may include one or more link state databases LSDBs where link and node data is received in routing protocol advertisements received from a topology server and or discovered by link layer entities such as an overlay controller and then provided to topology indication module . In some instances an operator may configure traffic engineering or other topology information within MT TED via a client interface.

Path engine accepts the current topology snapshot of the path computation domain in the form of TED and computes using TED CoS aware traffic engineered paths between nodes as indicated by configured node specific policy constraints and or through dynamic networking with external modules via APIs. Path engine may further compute detours for all primary paths on a per CoS basis according to configured failover and capacity requirements as specified in operational configuration and path requirements respectively .

In general to compute a requested path path engine determines based on TED and all specified constraints whether there exists a path in the layer that satisfies the TE specifications for the requested path for the duration of the requested time. Path engine may use the Djikstra constrained SPF CSPF path computation algorithms for identifying satisfactory paths though the path computation domain. If there are no TE constraints path engine may revert to SPF. If a satisfactory computed path for the requested path exists path engine provides a path descriptor for the computed path to path manager to establish the path using path provisioning module . A path computed by path engine may be referred to as a computed path until such time as path provisioning programs the scheduled path into the network whereupon the scheduled path becomes an active or committed path. A scheduled or active path is a temporarily dedicated bandwidth channel for the scheduled time in which the path is or is to become operational to transport flows.

Path manager establishes computed scheduled paths using path provisioning module which in this instance includes forwarding information base FIB configuration module illustrated as FIB CONFIG. policer configuration module illustrated as POLICER CONFIG. and CoS scheduler configuration module illustrated as COS SCHEDULER CONFIG. .

FIB configuration module programs forwarding information to data planes also referred to herein as forwarding planes of wireless devices of the path computation domain. FIB configuration module may implement for instance a protocol such as the OpenFlow protocol to provide and direct the wireless devices to install forwarding information to their respective data planes. Accordingly the FIB may refer to forwarding tables in the form of for instance one or more OpenFlow flow tables each comprising one or more flow table entries that specify handling of matching packets. FIB configuration module may in addition or alternatively implement other interface types such as a Simple Network Management Protocol SNMP interface path computation element protocol PCEP interface a Device Management Interface DMI a CLI Interface to the Routing System IRS or any other node configuration interface.

FIB configuration module interface establishes communication sessions with wireless devices to install forwarding information to receive path setup event information such as confirmation that received forwarding information has been successfully installed or that received forwarding information cannot be installed indicating FIB configuration failure . Additional details regarding PCEP may be found in J. Medved et al. U.S. patent application Ser. No. 13 324 861 PATH COMPUTATION ELEMENT COMMUNICATION PROTOCOL PCEP EXTENSIONS FOR STATEFUL LABEL SWITCHED PATH MANAGEMENT filed Dec. 13 2011 and in Path Computation Element PCE Communication Protocol PCEP Network Working Group Request for Comment 5440 March 2009 the entire contents of each of which being incorporated by reference herein. Additional details regarding IRS are found in Interface to the Routing System Framework Network Working Group Internet draft Jul. 30 2012 which is incorporated by reference as if fully set forth herein.

FIB configuration module may add change i.e. implicit add or delete forwarding table entries in accordance with information received from path computation module according to CCP. A CCP FIB configuration message from path computation module to FIB configuration module may specify an event type add or delete a node identifier a path identifier one or more forwarding table entries each including an ingress port index ingress label egress port index and egress label and a detour path specifying a path identifier and CoS mode. FIB configuration module can program any of a variety of forwarding objects to the mesh nodes including for example configuring firewall filters pseudowires and port and MAC based forwarding entries.

Policer configuration module may be invoked by path computation module to request a policer be installed on a particular wireless device for a particular data channel ingress. In some examples the FIBs for wireless devices may include policers at data channel ingress. Policer configuration module may receive policer configuration requests according to CCP. A CCP policer configuration request message may specify an event type add change or delete a node identifier a data channel identifier and for each class of service a list of policer information including CoS value maximum bandwidth burst and drop remark. FIB configuration module configures the policers in accordance with the policer configuration requests.

CoS scheduler configuration module may be invoked by path computation module to request configuration of CoS scheduler on the aggregation nodes or access nodes. CoS scheduler configuration module may receive the CoS scheduler configuration information in accordance with CCP. A CCP scheduling configuration request message may specify an event type change a node identifier a port identity value port index and configuration information specifying bandwidth queue depth and scheduling discipline for instance.

Various example Control Packet Formats will now be described. With reference to for example these control packets may be exchanged between controller and WAPs between controller and aggregation nodes between controller and WDs and between controller and WD for example.

In one example embodiment the control packets have the structure illustrated in . is a block diagram illustrating an example Base Packet Structure for a control packet according to the techniques of this disclosure.

The Ethernet Header is a standard Ethernet II header. The Ethernet header is used so that the CCP Control plane can be run natively over standard Ethernet interfaces. If other physical or logical interfaces are used the only requirement placed on those interfaces is that they can transport an Ethernet frame. Generally the source MAC address is the address of the sending node and the destination address is the address of the receiving node or all Fs in the case of broadcast Flood packets. The Ether type is TBD1 currently using 0xA000 for packets without a Source Route List and TBD2 0xA001 for packets with a Source Route List.

CCP Source Route List is an optional field. CCP Source Route List inclusion is dependent on the type of message. CCP Source Route List is present in messages that are sent using an SRT. CCP Source Route List is an ordered list of node specific ingress and egress link indexes that are used by nodes to source route a packet from one node to another. The CCP Message Header includes the message type. CCP Message Payload The CCP Message Payload is the payload for the specified message type.

Size field includes the total size in bytes of all the route list entries. Offset field includes the Offset in bytes into the Route List Entry List. Offset field is initialized to 0 from the control plane of the sending node and incremented by 2 by each node including the sending node as the packet is transmitted out the egress link. The Route List Entry RLE fields A N include the ingress and egress link indexes for the receiving node. When an SRT packet is received its ingress link index MUST match the ingress link index specified in the RLE.

In one example the Hello message has the following structure as shown in . The Link Index field specifies the senders Link Index. The link index is local to the sender. Links are indexed from 0 to 0xFE. The Link Index of 0xFF is reserved for the control plane of the node. Therefore CCP nodes are restricted to 255 interfaces. The Reserved field is reserved and must be set to zero and ignored by the receiver.

The Hello Reply message not shown is a unicast message used to reply to a Hello is set to active by the receiver. The sender of the Hello Reply message sets the Source Address of the Ethernet Header to its MAC address. The sender sets the Destination address of the Ethernet Header to the Source Address of the corresponding Hello message. The Hello Reply message is sent on the same link from which the Hello message was received. The structure of the Hello Reply message is the same as the Hello message. The Link Index is set to the local link index on which the corresponding Hello message was received.

In one example the Discover message has the following structure as shown in . The Instance ID is a unique number for the instance of the node. A node should generate a new instance ID each time it reboots or otherwise resets its software state. The instance ID is used to disambiguate a Discover message with the same generation number between resets. The instance ID may be a random number or a monotonically increasing integer for nodes having some ability to store information between reboots.

The Generation Number is a monotonically increasing number. The controller ignores any Discover message with a generation number less than the most recently received generation number unless the R bit is set .

The Intermediate Node List INL Start is the offset from the beginning of the CCP Message Payload to the start of the Intermediate Node List. This offset is required since the Neighbor Node List is variable in length. The INL End is the offset from the beginning of the CCP Message Payload to the end of the Intermediate Node List. The Neighbor Node List element is the list of Neighbors associated with this node. Each element in the list includes the Neighbor s MAC address the local link index on which a Hello Reply message was received and the Neighbor s link index as indicated in the Hello Reply message. The Intermediate Node List element includes the MAC addresses and their corresponding ingress and egress links through which this packet traversed en route from the originating node to the terminating edge node EN inclusive.

Neighbor MAC Address specifies the MAC Address of the neighbor as reported in the Ethernet Source MAC of the Hello Reply Message. The Local Link field specifies the local link index over which the Hello Reply was received. Remote Link The remote link index as reported in the Link Index of the Hello Reply packet.

Intermediate MAC Address field specifies the MAC address of a node that received the Discover message and re sent the packet. Ingress Link field specifies The index of the link on which the packet was received. Egress Link field specifies The index of the link on which the packet was sent. Note that when the packet is sent the Egress Link is modified for each link over which the packet is sent.

In one example the Flood Reply Message Structure is as shown in . The Generation number is used to correlate the Flood Reply with the original Discover message. In one example if the Generation number does not match the current generation number the node will discard the message. If they do match the node can initiate keepalive processing on the reversed SRT associated with this Flood Reply message.

In some aspects a Keepalive message is used to maintain liveness of an SRT. The Keepalive message is periodically sent by a node after it has received a Flood Reply for the current generation number. The Keepalive message is sent via an SRT from a node to the controller. Otherwise the Keepalive message has no additional content.

A Keepalive Reply message is sent by the controller upon receiving a Keepalive message. The Keepalive Reply message has no content. The Keepalive Reply message is sent via an SRT to the sender of the corresponding Keepalive message.

A SRT Down message see may be used to indicate to a sending node that the SRT over which it has sent packet has broken. This SRT Down message may provide immediate feedback to the sender that the SRT is down. With this indication the sender does not have to wait for a keepalive timeout before taking corrective action. When WAP or WD receives an SRT Down message in some examples the WAP or WD will increment its generation number and generate a new Discover message to establish a new SRT with the controller.

In some examples when a controller receives an SRT Down message controller may modify its state for the effected node such that the next Discover message from the node of equal to or greater than generation number is immediately accepted. This avoids the condition where a Flood Reply for a given generation number is not able to follow the SRT specified and all Floods from the node would be ignored since they specify a different INL.

In one example the node detecting the SRT Down may construct the SRT Down message according to the follow procedure 

In some examples Link Attributes are encoded as TLVs to support extensibility. is a block diagram illustrating an example Link Attributes TLV Format according to the techniques of this disclosure. In this example Link Attribute fields are described using a set of Type Length Value triplets. The TLV is not padded to four octet alignment Unrecognized types are ignored.

One example Link Attribute TLV Structure is shown in . The Type field lists the TLV Type. Example types may include types such as the IS IS types described in H. Gredler Advertising Link State Information in BGP Inter Domain Routing Internet Draft draft gredler bgp to 01 Jul. 11 2011 the entire contents of which are incorporated by reference herein. The Length field defines the length of the value portion in octets thus a TLV with no value portion would have a length of zero .

The Value field specifies the Contents of TLV. See the specific TLV description for more information. A Max Bandwidth Link Attribute may be a 32 bit floating point BW in Bytes per second. A RF Group Link Attribute may be some unique ID that ties this link to some other link for which BW is being shared. An Expected Transmission Time specifies the time expected to transmit a packet of 1K bytes across the link. Time is measured in microseconds and is encoded as a 32 bit unsigned integer.

In this example the FIB Config message has the structure shown in . A reserved field is for future use. Total Entries field indicates how many FIB Entry elements are being downloaded in this message. FIB Entry field is the Forwarding Information for each data channel.

Value field specifies The new CoS value to be used when the M bit is 1 . Incoming Label specifies label for an incoming packet. Value of 1 is considered valid only on Ingress LSR. PATH ID field specifies A 32 bit identifier for this FIB element. Namespace is managed by controller . Primary Port field specifies Primary path Port Index local to network node.

PA field specifies action to be operated on an incoming packet when it takes the detour path. The actions are 

Primary Egress Label field specifies The Label to be pushed or swapped on to the outgoing packet. Value of 1 is considered valid only on Egress device.

DA1 field specifies First action to be operated on an incoming packet when it takes the detour path. actions are 

Detour Egress Label 1 The Label value used by the label operations specified in DA1. DA2 field specifies Second action to be operated on an incoming packet when it takes the detour path. actions are 

Detour Egress Label 2 field specifies The Label value used by the label operations specified in DA2. R field R fields are reserved for future use.

FIB Config Reply Message Structure is as follows as shown in . The total Entries field indicates how many FIB Parse Status Entry elements are being reported. Value 0 indicates no entry element is being reported which implies the FIB Config message was parsed successfully. FIB Parse Status Entry field each entry reports the status for parsing the FIB Config entry element. FIB Parse Status Entry fields may not be present when Total Entries has 0 value.

The cloud control protocol may include other messages besides those described herein. For example the cloud control protocol may include a mechanism for a node to signal to a controller that the node is detecting multiple neighbors on a link. Multiple neighbors are not allowed since P2P links are assumed.

A WD such as WD D discover neighboring mesh node WD C WD B WD E and WD H and WAP C. WD D may receive one or more discover messages from its neighboring WDs and WD D outputs a discover message to WAP C toward controller . The discover message can include information about wireless links of WD D and also includes information that was included in the discover messages received from its neighboring WDs. WD D operates a reduced control plane without execution of a layer three L3 routing protocol that maintains routing information for the wireless mesh network and generates forwarding information for the wireless devices.

A centralized controller of the mesh network such as controller receives topology information from the plurality of mesh nodes of the network . Specifically controller receives the discover message from WD D as forwarded by intermediate mesh nodes and may also receive other discover messages with link information from other mesh nodes. Controller stores the link information to its topology information database and by a path computation module PCM of the centralized controller of the mesh network computes forwarding information for one or more transport data channels in accordance with the topology information wherein the data channels are for transporting network packets between the mesh nodes .

Controller outputs one or more messages to communicate and install within the mesh nodes including WD D the forwarding information for establishing the data channels . WD D receives the message sent by controller via intermediate devices and installs the forwarding information . In some examples the forwarding information can include link scheduling information. When WD D receives network packets associated with one or more subscriber devices WD D can forward the network packets along the data channel based on the installed forwarding information .

In the example of network device includes a control unit that comprises data plane and control plane . Data plane includes forwarding component . In addition network device includes a set of interface cards IFCs A N collectively IFCs for communicating packets via inbound links A N collectively inbound links and outbound links A N collectively outbound links . Links can be any combination of wired and or wireless links. Network device may also include a switch fabric not shown that couples IFCs and forwarding component .

In examples in which network device is one of WDs network device can include sensor analysis module to receive signals indicative of sensor data. Sensor analysis module can analyze signals indicative of sensor data associated with for example one or more sensors for forest fire detection air pollution monitoring landslide detection water quality monitoring natural disaster prevention machine health monitoring structural health monitoring or other types of sensing or monitoring. Sensor analysis module may provide analyzed sensor data information to cloud control protocol module to be included in discover messages to the controller.

Network device executes a cloud control protocol CCP module that operates in accordance with a cloud control protocol also referred to herein as a discovery protocol. In some examples cloud control protocol module outputs a hello message e.g. a Cloud Control Protocol CCP Hello message on each interface and or link. Each of the hello messages includes an identifier that is unique to network device e.g. an aggregation node or mesh node that sent the hello message and the interface on which the hello message was sent. In accordance with the discovery protocol network device also outputs a hello reply message on each interface on which a hello message was received. Cloud control protocol module maintains a neighbor node list that identifies neighboring nodes from which network device received hello messages and the interfaces on which the hello messages were received.

Responsive to receiving hello reply messages e.g. a CCP Hello Reply message on a link network device declares the link as an active link and adds the neighboring node to the neighbor node list . Cloud control protocol module outputs discover messages that each specify the neighbor node list identifying neighboring nodes and interfaces on which neighboring nodes are reachable from network device .

In addition upon receiving a discover message and determining that the discover message does not include a layer two address for a recipient one of the nodes cloud control protocol module updates a stored intermediate node list IM node list of the discover message that specifies layer two addresses and interfaces for the nodes that the discover message traversed from an originating one of the nodes.

Upon updating the discover message cloud control protocol module forwards the discover messages to the neighboring nodes that are positioned along paths toward a central controller e.g. controller of and the centralized controller upon receiving the discover messages establishes a Source Routed Tunnel SRT control channel with each of the nodes including network device based on the intermediate node lists specified by the discover messages. Network device executes the cloud control protocol module without executing an Interior Gateway Protocol IGP within a control plane of network device .

In some examples the centralized controller computes the topology information for the network and computes the forwarding information for the transport data channels in accordance with the neighbor node list within each of the discover messages that are received from the network. In these examples network device receives from the controller and via the respective SRT control channels the pre computed forwarding information computed by the centralized controller for configuring forwarding component of network device to forward the network packets on the data channels. The pre computed forwarding information may include directed forwarding state for network device to use for sending packets on a data channel. In some examples the pre computed forwarding information can program network device with any of a variety of forwarding objects including for example configuring firewall filters pseudowires and port and MAC based forwarding entries.

In some examples the directed FIB state includes policers to police ingress traffic for the data channel according to the computed bandwidth. In some examples WD maintains link scheduling information . WD may maintain information regarding error rates for transmission on wireless links in error rates and may update this information in real time as network conditions change. Error rates may be expressed in a particular time it takes to transfer 100 bytes of data based on sampled measurements for example. WD can include information regarding error rates link scheduling load conditions for the wireless links in discover messages sent to the centralized controller. WD can indicate that particular links are shared.

Based on the forwarding information and link information the centralized controller may also compute one or more backup data channels for the network and outputs one or more messages to network device to communicate and install within network device forwarding information for the backup data channels. Network device stores the forwarding information for the data channels and the backup data channels to forwarding information . Based on forwarding information forwarding component forwards packets received from inbound links to outbound links that correspond to next hops associated with destinations of the packets. In response to a network event forwarding component may re route at least a portion of the network packets along the backup data channel. The network event may be for example a link or node failure. The controller may also compute detour data channels to handle fast reroute for any interior node failure.

In one example the centralized controller computes based on the forwarding information one or more backup data channels for the network and outputs from the centralized controller one or more messages to network device to communicate and install within network device forwarding information for the backup data channels. In response to a network event forwarding component of network device re route at least a portion of the network packets along the backup data channel.

In some examples when forwarding the discover messages network device modifies the discover messages to include one or more link characteristics associated with the interfaces and the centralized controller computes the forwarding information for the data channels based at least in part on quality of service QoS metrics requested for the data channels and the link characteristics received from the discover messages.

In some examples network device sends Keepalive packets to the centralized controller network device to ensure a state of the SRT control channel and responsive to determining that no Keepalive Reply is received from the centralized controller within a time period network device generates a new Discover message with a new generation number to force acceptance at a centralized controller network device of a new SRT control channel.

In this manner network device has a reduced control plane that in some examples does not execute a Multiprotocol Label Switching MPLS protocol for allocation and distribution of labels for the data channels and does not execute a routing protocol such as an interior gateway protocol IGP . Instead network device executes the cloud control protocol module to receive forwarding information directly from a central controller e.g. controller of without requiring conventional protocol signaling such as MPLS signaling using a label distribution protocol such as LDP or RSVP. The centralized controller network device provides a centralized cloud based control plane to configure network device to provide transport data channels between the edge nodes and the mesh nodes for transport of subscriber traffic. In various examples the messages exchanged between network device and the centralized controller may conform to any of the message formats described herein.

In some examples after the SRT control channel is established between network device and the centralized controller the centralized controller may not necessarily compute and install forwarding information to network device but may instead use the SRT control channel to output one or more messages to network device via the SRT control channel and perform one or more control functions on behalf of the network device . For example network device can receive messages from the centralized controller for performing control functions such as configuration of network device monitoring status of the network device image download to network device gathering traffic statistics about network traffic at network device gathering information about local load conditions on network device or gathering information about error rates on network device for example.

In one embodiment forwarding component may comprise one or more dedicated processors hardware and or computer readable media storing instructions to perform the techniques described herein. The architecture of network device illustrated in is shown for example purposes only. In other embodiments network device may be configured in a variety of ways. In one embodiment for example control unit and its corresponding functionality may be distributed within IFCs .

The techniques described in this disclosure may be implemented at least in part in hardware software firmware or any combination thereof. For example various aspects of the described techniques may be implemented within one or more processors including one or more microprocessors digital signal processors DSPs application specific integrated circuits ASICs field programmable gate arrays FPGAs or any other equivalent integrated or discrete logic circuitry as well as any combinations of such components. The term processor or processing circuitry may generally refer to any of the foregoing logic circuitry alone or in combination with other logic circuitry or any other equivalent circuitry. A control unit comprising hardware may also perform one or more of the techniques of this disclosure.

Such hardware software and firmware may be implemented within the same device or within separate devices to support the various operations and functions described in this disclosure. In addition any of the described units modules or components may be implemented together or separately as discrete but interoperable logic devices. Depiction of different features as modules or units is intended to highlight different functional aspects and does not necessarily imply that such modules or units must be realized by separate hardware or software components. Rather functionality associated with one or more modules or units may be performed by separate hardware or software components or integrated within common or separate hardware or software components.

The techniques described in this disclosure may also be embodied or encoded in a computer readable medium such as a computer readable storage medium containing instructions. Instructions embedded or encoded in a computer readable medium may cause a programmable processor or other processor to perform the method e.g. when the instructions are executed. Computer readable media may include non transitory computer readable storage media and transient communication media. Computer readable storage media which is tangible and non transitory may include random access memory RAM read only memory ROM programmable read only memory PROM erasable programmable read only memory EPROM electronically erasable programmable read only memory EEPROM flash memory a hard disk a CD ROM a floppy disk a cassette magnetic media optical media or other computer readable storage media. It should be understood that the term computer readable storage media refers to physical storage media and not signals carrier waves or other transient media.

Various aspects of this disclosure have been described. These and other aspects are within the scope of the following claims.

