---

title: System and method for simulating a persistent byte addressable storage device on a persistent block addressable storage device
abstract: A persistent random-access, byte-addressable storage device may be simulated on a persistent random-access, block-addressable storage device of a storage system configured to enable asynchronous buffered access to information persistently stored on the block-addressable device. Buffered access to the information is provided, in part, by a portion of kernel memory within the storage system allocated as a staging area for the simulated byte-addressable storage device to temporarily store the information destined for persistent storage. One or more asynchronous interfaces may be employed by a user of the simulated byte-addressable device to pass metadata describing the information to a driver of the device, which may process the metadata to copy the information to the staging area. The driver may organize the staging area as one or more regions to facilitate buffering of the information (data) prior to persistent storage on the block-addressable storage device. Each asynchronous access interface is configured to ensure that an order of changes to the data in the persistent storage is consistent with the order of arrival of the changes at the driver.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09389787&OS=09389787&RS=09389787
owner: NetApp, Inc.
number: 09389787
owner_city: Sunnyvale
owner_country: US
publication_date: 20140515
---
The subject matter herein relates to storage systems and more specifically to simulating a persistent random access byte addressable storage device on a persistent random access block addressable storage device of a storage system.

A storage system may include a file system configured to provide storage service to one or more clients relating to storage and retrieval of information on persistent random access block addressable storage devices such as disks. To improve the reliability and stability of such storage service the storage system may employ a persistent random access byte addressable storage device such as a non volatile random access memory NVRAM . The NVRAM typically includes a back up battery or other built in last state retention capability e.g. non volatile semiconductor memory that is capable of maintaining information in light of a failure to the storage system.

In addition the performance of the storage service provided by the storage system may be improved using the NVRAM. Widely accepted file system standards such as the Network File System NFS specify that a storage system should not reply to a client with respect to completion of a modifying data access request e.g. a write operation and associated data until the results of the request are written to persistent storage. The storage system may utilize the NVRAM to record or log the modifying request as processed by the file system. By logging the modifying request to NVRAM a reply can be returned to the client with respect to completion of the request before the results of the request have been written to disk.

For a storage system that may not employ a NVRAM a disk or other similar secondary storage may be used as persistent storage for simulating the NVRAM. However the performance of such a storage system may be impacted as the logging of modifying requests to NVRAM is faster than writing of the results of the request to disk. Moreover users of the NVRAM e.g. a file system typically expect certain properties from the NVRAM including preservation of an order of changes associated with the modifying requests to the NVRAM.

The subject matter described herein provides a system and method for simulating a persistent random access byte addressable storage device on a persistent random access block addressable storage device of a storage system configured to enable asynchronous buffered access to information persistently stored on the block addressable device. Buffered access to the information is provided in part by a portion of kernel memory within the storage system allocated as a staging area for the simulated byte addressable storage device to temporarily store the information destined for persistent storage. One or more asynchronous interfaces may be employed by a user of the simulated byte addressable device to pass metadata describing the information to a driver of the device which may process the metadata to access i.e. copy the information to the staging area. To that end the driver may organize the staging area as one or more regions to facilitate temporary storage of the information e.g. data of one or more write operations prior to persistent storage on the block addressable storage device. Notably each asynchronous access interface is configured to ensure that an order of changes to the data in the persistent storage is consistent with the order of arrival of the changes at the driver. As described herein the system and method reduces the number of copy operations between each region and the block addressable storage device while maintaining ordering.

Illustratively the access interfaces may be embodied as application programming interfaces whereas the simulated byte addressable storage device may be embodied as a simulated non volatile random access memory NVRAM device i.e. a simulated device and the block addressable storage device may be embodied as local persistent storage i.e. a backing store. Moreover the user of the simulated device may be a software module executing on the storage system and configured to employ one or more of the access interfaces to copy the data to the staging area and to the backing store depending on semantics of the interface. The driver i.e. a virtual NVRAM VNVRAM driver may translate the metadata to one or more incoming requests describing the location and ordering of the data. The incoming requests may reside within one or more streams specified by an argument to the interface wherein each incoming request is assigned a transfer identifier XID .

In an aspect of the subject matter the VNVRAM driver may be configured to support a first access interface that provides a non volatile direct memory access nvdma write interface having an argument that imposes ordering of requests within a stream. Illustratively an incoming request associated with the write interface may be marked with a special flag i.e. an XORDER flag that specifies dependency to all previous requests in the stream. Specifically the XORDER flag instructs the VNVRAM driver to ensure that no portions of overlapping previous requests e.g. describing previous write operations of data may be persistently stored after the incoming request has been committed to the backing store. The XORDER flag thus has the same effect as ensuring that all previous requests of the stream are committed i.e. processed and persistently stored on the backing store before the incoming request marked with the flag is processed and written to the backing store but does not necessarily require that all write operations through the access interface be committed to the backing store. For example a write operation that is completely overwritten by a subsequent write operation need not be written to the backing store.

In addition a second access interface may provide a nvdma ordered write interface that imposes ordering of an incoming request with respect to one or more requests among streams. The incoming request associated with the ordered write interface may specify one or more previous requests i.e. pre requisite requests that provide dependency to the incoming request. That is the semantics of the ordered write interface ensure that the incoming request may not be persistently stored in the backing store until all of the specified pre requisite requests upon which the dependency exists are persistently stored written in the backing store. Illustratively each pre requisite request is identified by a stream XID pair wherein the stream is an identifier of the stream on which the pre requisite request resides and the XID is the transfer identifier of the pre requisite request.

It will be apparent to those skilled in the art that other processing and memory means including various computer readable media may be used for storing and executing program instructions pertaining to the subject matter herein. It is also expressly contemplated that the various processes threads software layers architectures and procedures described herein can be implemented in hardware firmware software or a combination thereof. Moreover it is expressly contemplated that the various software programs processes threads and layers described herein may be embodied as modules configured to operate in accordance with the disclosure e.g. according to the functionality of a similar program process or layer.

The network adapter may include one or more ports adapted to couple the system to one or more clients over computer network which may include one or more point to point links wide area networks virtual private networks implemented over a public network Internet or a shared local area network. The network adapter may thus include the mechanical electrical and signaling circuitry needed to connect the storage system to the network which illustratively may embody an Ethernet network or a Fibre Channel FC network. Each client may be a general purpose computer configured to execute one or more applications to interact with the system in accordance with a client server model of information delivery. That is the client may request the storage service of the storage system and the system may return the results of the service requested by the client by exchanging packets over the network . The client may issue packets including file based access protocols such as the Common Internet File System CIFS protocol or Network File System NFS protocol over the Transmission Control Protocol Internet Protocol TCP IP when accessing information in the form of storage containers such as files and directories. Alternatively the client may issue packets including block based access protocols such as the Small Computer Systems Interface SCSI protocol encapsulated over TCP iSCSI and SCSI encapsulated over Fibre Channel FCP when accessing information in the form of storage containers such as blocks or logical units luns .

In an aspect of the subject matter the local persistent storage may be embodied as one or more persistent random access block addressable storage devices such as solid state drives or hard disk drives utilized by the system to persistently store information provided by one or more processes that execute as user and or kernel mode processes or threads on the system. The storage adapter illustratively cooperates with the storage operating system executing on the storage system to access information requested by the client. The information may be stored on any type of attached array of writable storage devices such as video tape optical DVD magnetic tape bubble memory electronic random access memory micro electro mechanical and any other similar storage device media adapted to store information including data and parity information. The storage adapter may include one or more ports having input output I O interface circuitry that couples to the storage devices over an I O interconnect arrangement such as a conventional serial attached SCSI SAS or FC link topology.

To facilitate access to the storage devices the storage operating system illustratively implements a high level module such as a write anywhere file system that cooperates with one or more virtualization modules to virtualize the storage space provided by devices. The file system logically organizes the information as a hierarchical structure of named storage containers such as directories files and or aggregates having one or more volumes that hold files and or luns on the devices. Each on disk file may be implemented as set of disk blocks configured to store information such as data whereas the directory may be implemented as a specially formatted file in which names and links to other files and directories are stored. The virtualization module s allow the file system to further logically organize information as blocks on the disks that are exported as named luns. In an aspect of the subject matter the storage operating system is preferably the NetApp Data ONTAP operating system available from NetApp Inc. Sunnyvale Calif. that implements a Write Anywhere File Layout WAFL file system. However it is expressly contemplated that any appropriate storage operating system may be enhanced for use in accordance with the subject matter described herein.

In an aspect of the subject matter an iSCSI driver layer provides block protocol access over the TCP IP network protocol layers and a FC driver layer is configured to receive and transmit block access requests and responses to and from the storage system. The FC and iSCSI drivers provide FC specific and iSCSI specific access control to the blocks and thus manage exports of luns to either iSCSI or FCP or alternatively to both iSCSI and FCP when accessing the blocks on the system . In addition the storage operating system includes a Redundant Array of Independent or Inexpensive Disks RAID layer or module that implements a disk storage protocol such as a RAID protocol a storage driver layer that implements a disk access protocol such as e.g. a SCSI protocol and a FlexLog module that provides a generic non volatile logging architecture. A VNVRAM driver is illustratively implemented as a kernel mode process configured to provide services to the RAID and file system modules and to that end includes a flush thread configured to store write information to the local persistent storage as described herein.

Bridging the disk software layers with the integrated network protocol stack layers is the file system module configured to implement a virtualization system of the storage operating system through the interaction with one or more virtualization modules embodied as e.g. a virtual disk vdisk module not shown and a SCSI target module . The vdisk module enables access by administrative interfaces in response to a user system administrator issuing commands to the system . The SCSI target module is generally disposed between the iSCSI and FC drivers and the file system to provide a translation layer of the virtualization system between a logical unit space and a file system space where logical units are represented as named storage containers within the file system space.

The file system is illustratively a message based system that provides logical volume management capabilities for use in access to the information stored on the storage devices such as disks. That is in addition to providing file system semantics the file system provides functions normally associated with a volume manager. These functions include i aggregation of the disks ii aggregation of storage bandwidth of the disks and iii reliability guarantees such as mirroring and or parity RAID . The file system illustratively implements the WAFL file system having an on disk format representation that is block based using e.g. 4 kilobyte KB blocks and using index nodes inodes to identify files and file attributes such as creation time access permissions size and block location . The file system uses files to store meta data describing the layout of its file system these meta data files include among others an inode file. A file handle i.e. an identifier that includes an inode number is used to retrieve an inode from disk.

As used herein the term storage operating system generally refers to the computer executable code operable on a computer to perform a storage function that manages data access and may in the case of a storage system implement data access semantics of a general purpose operating system. The storage operating system can also be implemented as a microkernel an application program operating over a general purpose operating system such as UNIX or Windows XP a general purpose operating system with configurable functionality or as one or more processes configured for storage applications as described herein.

In addition it will be understood to those skilled in the art that the subject matter described herein may apply to any type of special purpose e.g. file server filer or storage serving appliance or general purpose computer including a standalone computer or portion thereof embodied as or including a storage system. Moreover the described subject matter can be adapted to a variety of storage system architectures including but not limited to a network attached storage environment a storage area network and disk assembly directly attached to a client or host computer. The term storage system should therefore be taken broadly to include such arrangements in addition to any subsystems configured to perform a storage function and associated with other equipment or systems. It should be noted that while this description is written in terms of a write anywhere file system the subject matter herein may be utilized with any suitable file system including a write in place file system.

As noted a storage system may employ a persistent random access byte addressable storage device such as a non volatile random access memory NVRAM to improve the reliability stability and performance of the storage service provided by the storage system. In response to receiving a modifying data access request such as a write request from a client the file system may use an address of kernel memory i.e. an area of memory allocated to the portions of the storage operating system residing in memory to create an operation such as a write operation including write data. The data may then be temporarily stored at the kernel memory address i.e. source buffer while metadata describing the buffer such as source address of the source buffer as well as an offset and length in NVRAM may be passed as a NVRAM request to a direct memory access DMA controller for recording logging in the NVRAM using one or more application programming interfaces i.e. access interfaces. The DMA controller may load the NVRAM request into a queue and program DMA circuitry to transfer the data contents of the source buffer to NVRAM. Upon completion of the transfer the DMA controller may mark the NVRAM request as done. The file system can then either inquire as to the state of the NVRAM request or wait until the controller notifies it that the request has been stored in NVRAM. At that point the file system can acknowledge completion of request processing to the client.

For a storage system that may not employ a NVRAM a combination of byte addressable memory and a block addressable disk or other similar secondary storage may be used as persistent storage for simulating the NVRAM. In an implementation of such a storage system platform an access interface may be employed to access i.e. copy the data to the persistent storage used for simulating the NVRAM. However the access interface for the platform without the NVRAM may be different from the access interface for the platform having the NVRAM thereby requiring the storage operating system to switch among those interfaces depending on the storage system platform. In addition the performance of the storage system platform without the NVRAM may be impacted as the logging of modifying requests to the NVRAM is faster than writing of the results data of the request to disk. Moreover users of the NVRAM e.g. a file system typically expect certain properties from the NVRAM including preservation of an order of changes of the data associated with the modifying requests to the NVRAM.

The subject matter described herein provide a system and method for simulating a persistent random access byte addressable storage device on a persistent random access block addressable storage device of a storage system configured to enable asynchronous buffered access to information persistently stored on the block addressable device. Buffered access to the information is provided in part by a portion of kernel memory within the storage system allocated as a staging area for the simulated byte addressable storage device to temporarily store the information destined for persistent storage. One or more asynchronous interfaces may be employed by a user of the simulated byte addressable device to pass metadata describing the information to a driver of the device which may process the metadata to access i.e. copy the information to the staging area. To that end the driver may organize the staging area as one or more regions to facilitate temporary storage of the information e.g. data of one or more write operations prior to persistent storage on the block addressable storage device. Notably each asynchronous access interface is configured to ensure that an order of changes to the data in the persistent storage is consistent with the order of arrival of the changes at the driver. As described herein the system and method reduces the number of copy operations between each region and the block addressable storage device while maintaining ordering.

Illustratively the access interfaces may be embodied as application programming interfaces APIs whereas the simulated byte addressable storage device may be embodied as a simulated NVRAM device i.e. a simulated device and the block addressable storage device may be embodied as local persistent storage i.e. a backing store. Moreover the user of the simulated device may be a software module executing on the storage system and configured to employ one or more of the access interfaces to copy the data e.g. from the source buffer to the staging area and then to the backing store depending on semantics of the interface. The driver i.e. a virtual NVRAM VNVRAM driver may translate the metadata to one or more incoming requests describing the location and ordering of the data. The incoming requests may reside within one or more streams specified by an argument to the interface wherein each incoming request is assigned a transfer identifier XID by the driver.

In an aspect of the subject matter the user e.g. caller of the access interface may use the XIDs to inquire as to the completion of the incoming requests. Notably the XIDs are ordered and monotonically increasing beginning at a small integer value at system initialization and increasing until cessation of system operation. A first XID with a value lower than a second XID is configured to represent a request transaction initiated earlier in time. Advantageously the VNVRAM driver may use the monotonically increasing nature of the XIDs to minimize the use of resources required to represent the status of previous requests. It may not be necessary to preserve requests prior to the lowest XID which has not been completed. Instead a single lower limit on pending XIDs may be maintained and all other records of previous requests may be discarded since a comparison of the XID to the lowest incomplete XID is sufficient to declare any previous XID done.

Using the access interfaces described herein the file system or other software module such as the RAID module may pass incoming requests to the VNVRAM driver which may interpret the processing requested by the interface semantics as provided by the file system . In an aspect of the subject matter the requests passed to the VNVRAM driver may be organized within one or more streams by the file system which creates and registers the stream s with the driver. In response to such interpretation the VNVRAM driver may create corresponding incoming VNVRAM requests that include the information associated with the passed requests. is a block diagram of a VNVRAM request that may be advantageously used with the subject matter described herein. The VNVRAM request is illustratively a data structure that includes a source address of the source buffer within the allocated memory of the file system where data associated with an operation e.g. a modifying data access request such as a write operation resides and a destination address where the data is to be stored in the simulated device. In order to process i.e. log the request to the simulated device the VNVRAM driver writes the data of the VNVRAM request to the staging area used as e.g. a destination buffer prior to writing the data to the backing store. The request also includes a size of the data associated with the operation flags associated with the request and if appropriate an array of pre requisite requests each of which may be identified by a stream XID pair . Notably the VNVRAM driver maintains the processing order specified by interface semantics provided by the file system.

Each stream is essentially a group of VNVRAM requests that the VNVRAM driver may process according to the semantics provided by the access interfaces. In an aspect of the subject matter the file system may register a stream e.g. via a stream management interface associated with a region with the VNVRAM driver . The file system may also register a region e.g. via a region management interface with specified start and length with the driver. The VNVRAM driver may further create an arbitrary number of lists of VNVRAM requests associated with streams regions or other groupings advantageous to the processing of the requests. The driver may process the requests in the stream using any of these lists such as the per stream list or the per region list to ensure necessary processing efficiency. For example the VNVRAM driver may organize VNVRAM requests received from one or more streams within the per region list e.g. a linked list data structure based on the destination addresses within the staging area or region that the requests modify and based on the stream e.g. as provided by an argument to the access interface. It should be noted that the argument may also be used to insert a request on the per stream list . The VNVRAM driver may create these lists along with additional lists not shown such as a global list of all allocated requests and a free list of available requests.

In an aspect of the subject matter the VNVRAM requests of the streams are independent such that their order of execution is not dependent on requests of other streams. That is the VNVRAM driver may execute i.e. process the requests in each stream based on an order of arrival or in any order without restriction. In this case the file system or any other user module of the storage operating system may not render any assumption with respect to the order of processing the requests. However if the file system requires ordering of requests within a stream or among streams such ordering may be imposed using the arguments of the access interfaces.

In an aspect of the subject matter the VNVRAM driver may be configured to support a first access interface that provides a non volatile direct memory access nvdma write interface having an argument that imposes ordering of requests within a stream. Illustratively the file system may impose ordering of an incoming request within a stream and associated with the write interface by marking the request with a special flag i.e. an XORDER flag that specifies dependency to all previous requests in the stream. Specifically the XORDER flag instructs the VNVRAM driver to ensure that no portions of overlapping previous requests e.g. describing previous write operations of data may be persistently stored after the incoming request has been committed to the backing store. The XORDER flag thus has the same effect as ensuring that data of all previous requests of the stream are committed i.e. processed and persistently stored on the backing store local persistent storage before the data of the incoming request marked with the XORDER flag is processed and written to the backing store but does not necessarily require that all write operations through the access interface be committed to the backing store.

For instance a write operation that is completely overwritten overlapped by a subsequent write operation need not be written to the backing store. As an example assume requests and are passed issued by the file system to the VNVRAM driver in a stream followed by request which is marked with the XORDER flag and then request . According to the semantics of the first interface the VNVRAM driver may process requests and e.g. in any order but must ensure that persistent storage of their associated data is prior to or coincident with persistent storage of the data associated with request . It should be noted that the phrase prior to or coincident with in connection with persistent storage of data also denotes the processing of any previous overlapping requests such that no part of any previous overlapping request may be persistently stored in the backing store after the incoming request marked with the XORDER flag is reported as completed done . The semantics of the first interface are illustratively provided by an extension to an nvdma write API i.e. the XORDER flag associated with the first interface.

In an aspect of the subject matter the VNVRAM driver may organize the VNVRAM requests in any number of lists which represent the requests and their dependencies in an advantageous manner. For example the VNVRAM driver may organize the VNVRAM requests of any list according to a processing order specified by the semantics of the first interface and thereafter transit the list in any advantageous order to retrieve and process each request. Assume that the data and operations specified by requests and may modify respective areas e.g. regions of the simulated NVRAM e.g. including the backing store as specified by the destination addresses of the requests thus the data associated with the requests may be copied by the driver to the respective region of the kernel memory staging area . The VNVRAM driver may then copy the data of request which modifies another area region of simulated NVRAM and the backing store to the appropriate staging area where it is buffered with the data of requests and . Yet according to the semantics specified by the XORDER flag of the first interface the driver may not copy the data of request to the staging area until request completes i.e. until its data is processed and persistently stored on the backing store as described above.

In an aspect of the subject matter the flush thread of the VNVRAM driver may implement a flush technique algorithm configured to determine which portions of the staging area are modified by the VNVRAM requests and store write the data of those requests to the backing store local persistent storage . Illustratively the flush thread may be invoked to interpret the interface semantics associated with the VNVRAM requests to write the data to the backing store in accordance with a manner and behavior as expected by the file system . Accordingly the thread may interpret the interface imposed ordering semantics i.e. the XORDER flag to determine that requests and are not ordered but that request is ordered. The thread may further determine that the data of requests and may be written to the backing store to thereby simulate writing of those requests to the NVRAM. The flush thread may then determine that the data of request may be written to the backing store. Thus even though they are temporarily stored in the staging area the data of VNVRAM requests and are written to the backing store sequentially i.e. in order according to one or more disk write operations as specified by the interface semantics as provided by the file system and with the resulting behavior of the NVRAM as expected by the file system . Illustratively each write operation to the backing store occurs at a disk transfer granularity of 4 kilobytes kB . Once the data of request is written to the backing store the flush thread may process request to e.g. copy the data of the request to the staging area and thereafter write that request to the backing store. According to the semantics of the first access interface processing of request is delayed waiting for completion of request .

In an aspect of the subject matter the VNVRAM driver may delay the copying of data provided through the access interface from the source buffer until such time as may be advantageous to the efficiency of writing the data to the backing store and the preservation of the ordering semantics expressed in the interface. Moreover the VNVRAM driver may agglomerate gather VNVRAM requests to minimize the number of disk transfers required to the backing store in order to preserve the semantics expressed through the access interface. After obtaining an XID for the VNVRAM request the caller of the access interface may not modify the data contents of the source buffer without previously inquiring as to the status of the XID and receiving a response that the operation represented by the access interface is completed.

In an aspect of the subject matter the driver may recognize and categorize one or more types of dependencies in the lists dependency lists to allow for efficient processing of the dependencies to preserve maintain semantic order. For example one type of dependency maintained may be none . In this case the data of the VNVRAM request may be immediately copied into the region of the staging area and transferred to the backing store as convenient provided that the request is not indicated as complete until such transfer has completed. Another type of dependency maintained may be a dependency upon the copying of data for a previous request. Such a dependency may arise from the specification of XORDER in a VNVRAM request when the data lies within the same atomically transferable unit e.g. 4 kB disk transfer block as the request upon which the dependency exists. In such cases it may be necessary only to have copied the previous request data into the block prior to the copy of the data for the second request since it is only necessary to ensure that the final result of the copies into the block are conveyed to the backing store. Yet another type of dependency maintained may be a dependency upon the commitment of a previous request to the backing store. Such a dependency exists if the two requests are in different atomically transferable units disk transfer blocks of the backing store. Such dependencies may result between requests within the same stream or in different streams.

In an aspect of the subject matter the VNVRAM driver may address the dependencies by following a dependency list e.g. the global list back to the first independent request and then processing all dependent requests until the desired XID dependencies have been satisfied. The driver may suspend the processing for purposes of satisfying performance policies. Such processing need not necessarily handle all requests in order since there may be multiple lists of dependencies extant at once in the global list. As provided in the access interface the VNVRAM driver may copy data from the source buffer to a region of the staging area whenever it is convenient and semantically appropriate to do so in order to satisfy the semantic requirements of the interface.

In an aspect of the subject matter the VNVRAM driver may maintain any combination of data and algorithms used to determine which requests should be completed at what time in order to i minimize the number of transfers to the backing store ii ensure a certain maximum latency for requests through an access interface iii optimize the total number of requests that can be processed through the access interface in a given time iv prioritize processing of requests associated with one stream over another or v such other factors as required to achieve the desired performance of the system. The above determination is referred to as one or more performance policies. Factors in performance policy may include a the number of outstanding requests on a stream and comparison of the same to a fixed or adjustable limit b the time that a request is outstanding and comparison of the same to a fixed or adjustable limit and c the adjustment of limits for outstanding count or outstanding time based on statistical measurement of past performance of the system in servicing requests.

In an aspect of the subject matter the file system may impose ordering of a request with respect to one or more requests organized among different streams by issuing a second access interface i.e. an ordered write interface nvdma ordered write API which specifies the completion of one or more requests prior to completion of the request associated with the second interface. Here an incoming request associated with the ordered write interface may specify one or more previous requests i.e. pre requisite requests that provide dependency to the incoming request i.e. a dependent write . That is the semantics of the ordered write interface ensure that the data of the incoming request may not be persistently stored in the backing store until data for all of the specified pre requisite requests upon which the dependency i.e. the dependent write exists are persistently stored written in the backing store. Illustratively each pre requisite request is identified by a stream XID pair wherein the stream is an identifier of the stream on which the pre requisite request resides and the XID is a transfer identifier of the pre requisite request.

For example assume that the file system issues request via stream and request via stream . In the absence of any specified ordering by the file system the VNVRAM driver may process these requests in any order. Yet if the file system wants to ensure that the data of request is processed and persistently stored prior to persistent storage of the data for request the second interface may be employed called to specify request as dependency to request . In other words the second interface semantics instruct the driver to ensure that the data for request is persistently stored in the backing store prior to or coincident with persistent storage of the data of request in the backing store.

However if ordering is required step the VNVRAM driver interprets the semantics including arguments of the access interface to ensure that the data associated with the request is not persistently stored e.g. on the block addressable device until data for all dependent previous requests specified by the interface are persistently stored step . For example the VNVRAM driver may examine the arguments of the first access interface to determine that ordering of the request within a stream is imposed in accordance with the XORDER flag. Accordingly the VNVRAM driver is instructed to ensure that the data of all previous requests of the stream are persistently stored on the block addressable device i.e. backing store before or coincident with the data for the request marked with the XORDER flag being persistently stored in the backing store. In addition the VNVRAM driver may examine the arguments of the second access interface to determine that ordering of the request with respect to previous requests among streams is imposed in accordance with the pre requisite requests specified in the dependent write. Accordingly the VNVRAM driver is instructed to ensure that the data of the request is not persistently stored until the data of all previous requests as specified by the pre requisite requests are persistently stored in the backing store. At step a determination is made as to whether the previous requests are persistently stored. If not the procedure returns to step otherwise if the previous requests are persistently stored the procedure proceeds to step where the request is processed and at step its associated data is persistently stored. The procedure then ends at step .

While there have been shown and described illustrative subject matter for simulating a persistent random access byte addressable storage device on a persistent random access block addressable storage device of a storage system configured to enable asynchronous buffered access to information persistently stored on the block addressable device it is to be understood that various other adaptations and modifications may be made within the spirit and scope of the subject matter herein. For example the subject matter has been shown and described herein with relation to ordering dependency among requests within one or more streams. In the situation that requires ordering dependency of requests within other streams processing of a request in a first stream may be interrupted paused as mandated by a dependent write ordered write request of the second access interface to enable persistent storage of data for one or more pre requisite requests in another stream. Here the VNVRAM driver may obtain the pre requisite requests specified by stream XID pairs from the second interface and may locate those requests for pre requisite processing and persistent storage within the other stream s . Once it has completed those requests the driver may return to the paused stream and process the paused dependent write request for persistent storage. However a restriction with the dependent write request associated with the second access interface is that there may not be a circular dependency back to the paused stream or else a deadlock situation may arise. Note that the stream XID pairs of pre requisite requests are provided to the VNVRAM driver before processing of the request for persistent storage by the driver so that it may obviate any circular dependency.

Advantageously the staging area of the kernel memory may be utilized to temporarily store a sufficient amount of requests data destined for persistent storage so as to amortize overhead of disk write operations to the backing store among all the requests. Although temporary storage buffering of the requests may present some latency in replying acknowledgments of completion to the client an increase in input output I O per second throughput may be realized by the buffering associated with the staging area as described herein. Additionally the access interfaces may be utilized to preserve an order of changes associated with modifying requests to the simulated device including the backing store as expected by users of the simulated device.

The foregoing description has been directed to specific subject matter. It will be apparent however that other variations and modifications may be made to the described subject matter with the attainment of some or all of its advantages. For instance it is expressly contemplated that the components and or elements described herein can be implemented as software encoded on a tangible non transitory computer readable medium e.g. disks and or CDs having program instructions executing on a computer hardware firmware or a combination thereof. Accordingly this description is to be taken only by way of example and not to otherwise limit the scope of the subject matter herein. Therefore it is the object of the appended claims to cover all such variations and modifications as come within the true spirit and scope of the subject matter herein.

