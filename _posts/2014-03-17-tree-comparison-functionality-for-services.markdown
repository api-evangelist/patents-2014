---

title: Tree comparison functionality for services
abstract: The techniques described herein provide for comparison of tree structures. In some examples, a system according to this disclosure may receive at least a first item including a first tree structure and a second item including a second tree structure. The system may compare the first item and the second item. In particular, in performing the comparison, the system may detect a sub-tree structure type in the first tree structure and in the second tree structure. In some examples, the sub-tree structure type is one of one or more sub-tree structure types that have corresponding matching processes. Once determined, the system described herein may perform the corresponding matching process of the detected sub-tree structure type for the first tree structure and the second tree structure.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09483387&OS=09483387&RS=09483387
owner: Amazon Technologies, Inc.
number: 09483387
owner_city: Seattle
owner_country: US
publication_date: 20140317
---
Software architects often engage in a process of improving software after deployment of the software. The improvements may be implemented by modifying a software system or by creating a new software system e.g. a replacement system where the modified or new software system is intended to replace or operate beside the current software system. Deployment of the modified or the new software system may have an impact on hardware that supports the software system e.g. require more or less processing power and or time may impact outcomes resulting from user interaction e.g. satisfy annoy or frustrate users etc. or may have other possible outcomes e.g. include bugs etc. . Therefore it is desirable to perform a comparison test to compare results following execution of the modified or new software system against results following execution of the current software system prior to a full deployment of the modified or new software system. However comparison tests of current software systems with modified or new software systems may result in failure to detect differences and or in the detection of differences that are unimportant or otherwise not meaningful. In particular some comparison tests may stop comparing two data structures such as tree structures at the first imperfect match which for example could be the result of random differences or due to the representation of the same value differently in the modified system e.g. floating point value precision or Boolean representation . As such meaningful differences may not be presented to users.

This disclosure is directed in part to the comparison of data structures. In particular the techniques and systems disclosed herein may be utilized to compare tree structures as well as to compare other types of data structures. The attributes of items to be compared such as a product or person may be represented in a tree structure. The items may then be compared by finding differences in the respective tree representations. In some implementations if the trees meet a threshold level of differences the items may be considered to not be a match. In some implementations the tree structures may be represented in a serialization format such as Extensible Markup Language XML YAML Ain t Markup Language YAML or JavaScript Object Notation JSON .

In some implementations the tree comparison techniques may be used for software testing. For example some implementations may comprise a testing service that may operate to perform testing using at least one intercepted request which is a duplicate of or is based on a production request to a current e.g. a deployed or live production software system. The testing service may operate a candidate software system that may be a candidate version of the production software system or a similarly functioning software system e.g. a trial or test version a replacement software system a new implementation etc. . In some implementations the testing service may further operate an authority software system which may be a software system or version of a software system which is used for validating the candidate software system or candidate version of the software system e.g. the authority software system may be a mirror copy of the production software system .

The testing service may be part of the production system a separate system or part of another system. The production software system may update production system data and may transmit data back to the end users while the intercepted request handled by the candidate software system and any authority software system s does not output to the users and or affect the production system. In some implementations the testing service may operate to compare a response to a candidate request from the candidate system to a response to a corresponding authority or production request from the corresponding authority or production system. The testing service may also operate to compare the processing of the respective systems in generating the responses.

Such a comparison operation may be referred to herein as a candidate test. In contrast to typical A B testing the testing of the candidate software system may occur without updating production system data and thus may be used to test system functionality and performance when executing requests that are based on actual client requests i.e. that were or are processed with the production software system .

The tree structure comparison techniques described herein may be utilized in many ways by the testing service. For example the candidate and authority responses may include tree structures that are compared to validate the candidate software system. In another example metadata or other data about the processing of the candidate and authority requests to generate the candidate and authority responses may be stored in a tree structure. These structures may be compared to for example test the performance or other characteristics of the candidate software system e.g. over a large number of responses . These are merely examples and the tree comparison techniques described herein may be used to compare any other data represented in a tree or similar structure by the testing service.

As alluded to above in some implementations the testing service may operate to allow for the above described functions to be performed with respect to different software systems software implementations and or different versions. In other words in some implementations the candidate software system is not limited to a new version of a production software system. For example the candidate software system of some implementations may be a different implementation of the production software system based on a different framework and or may include a different interface or the like.

The techniques and systems described herein may be implemented in a number of ways and are not limited to those specifically discussed herein. In particular though the description of the tree comparison techniques herein is primarily in the context of a testing service this is merely for ease of illustration and is not meant to be viewed as limiting on the disclosed techniques and systems. For example the disclosed tree comparison techniques and systems may be applied in a wide variety of other applications such as reverse matching or in determining whether differences between two items outweigh similarities between the items. The implementations provided below with reference to the figures are merely examples.

In operation the user e.g. a downstream consumer or user may using a user device transmit a client request for electronic data from the production system . However in some implementations the client request may be a request generated by another service the production system or another process and may not be a human generated request. The production system may be part of an electronic marketplace an electronic financial service a messaging service a social network and or any other service that exchanges electronic data with users. The production system may operate various versions of a software system that are executable in a framework and processed by production system resources. The versions may include the version of the software system utilized by the production stack that is currently deployed to fulfill user requests such as client request .

The interceptor intercepts at least some requests sent to the production system such as the client request and forwards or publishes the requests to the production stack as production requests . In addition the interceptor or another component of the testing service system may store the intercepted requests . The production stack processes the production requests normally using the production software system and replies with production responses . For example if the client request requested electronic data from the production system the corresponding production response may include the requested data. In the example implementation shown in the interceptor may act as a relay receiving the production responses and forwarding the production responses to their respective recipients. For example the interceptor relays the production response that corresponded to the client request to the user device as a client response . While the example implementation shown in shows the interceptor operating as a relay in the manner discussed above this is not limiting and has been done for ease of illustration. In other implementations the production stack could reply directly without the interceptor acting as relay.

In addition to forwarding production requests to the production stack the interceptor may forward the intercepted requests to the testing service for use by the testing service in testing. To handle testing in general the testing service system may use a protocol for testing with standardized meta data for requests and responses. For example regarding the meta data the interceptor may extract some basic meta data about the intercepted request service and or realm and store the meta data for use by the testing service along with or as part of the intercepted request . The interceptor may operate so as to allow the requests to be intercepted in an asynchronous non blocking manner to minimize the potential for disruption of the production system due to for example failures in the testing service system such as a failure of the interceptor . Though not shown in the illustrated implementation in some implementations similar interception and meta data extraction operations may be performed for the production responses . For example the intercepted production responses may be used in place of authority responses. In some such implementations the interceptor may provide the intercepted requests and intercepted responses to the testing service at the same time.

In some implementations the interception of requests and or responses for the testing service may be configurable such as on a per application programming interface API level. Some configurable parameters may include a publishing percentage a sampling methodology etc. Further the interceptor may operate based on multiple sets of interception rules scenarios tests etc. For example in some implementations the interceptor may be configured to intercept and forward a first percentage e.g. 50 of an indicated first type of client request e.g. product search purchase order etc. and to intercept and forward a second percentage e.g. 40 of an indicated second type of client request . Further the interceptor may be configured to cap the forwarding of intercepted requests. For example the interceptor may be configured to cap the interception and forwarding of the first type of client request at five 5 client requests per second and the interception and forwarding of the second type of client request at eight 8 client requests per second. In another example the interceptor may be configured to intercept and forward an indicated percentage of all client requests with a cap of twenty five 25 requests per second. Moreover these are merely examples of the configuration of the interceptor and implementations may include any combination of these and or other configurable parameters.

The testing service processes the intercepted requests . Depending on the processing desired the testing service operates to replay the intercepted requests to one or more of the production stack candidate stack the authority stack and or other stacks. This is illustrated in as the candidate requests and the authority request respectively.

As mentioned above the candidate stack is a stack operating a candidate software system which is to be validated such as an altered application stack or software system that is to be validated or a new software system or implementation of the software system being adopted for the production system . The authority stack is a stack operating software system s which may be used for validating the software system operated by the candidate stack in some types of testing.

In some implementations the authority stack may be a most recent version of a software system of the production system known to have acceptable functionality and performance. The software system operated by the authority stacks may be a mirror copy of the software system of the production stack operated by the production system . In some implementations the production stack may be operated to perform the functions of the authority stack . In such a case in some implementations the authority request may be sent to the production system by the testing service and may be tagged such that the production stack knows the authority request is a shadow requests and should be returned to the testing service instead of the user device and that the processing of the authority request should not result in changes in production system data used to perform production requests .

In some implementations the testing service may operate to dynamically modify at least some of the parameters of the intercepted requests before replaying the requests as shadow requests to the candidate stack and authority stack . In such an implementation the testing service may preserve the integrity of the modified shadow requests apart from the intended modifications to faithfully replay the shadow requests.

In operation the candidate stack and the authority stack receive the candidate request and the authority request respectively from the testing service and each processes the received request according to its respective software system. As mentioned above in some implementations unlike the processing performed by the production system for the production request the processing at the candidate stack and authority stack s may not be revealed or reported to users e.g. user and or may not modify data used by the production system . Thus any outputs and or manipulations of data from the candidate stack and authority stack may not be seen by the user and or used to generate data that is later output to the user . Instead the processing by the candidate stack and the authority stack is used to test execution of the software system operated by the candidate stack . Upon completion of the processing of each of the candidate requests and the authority request the candidate stack and the authority stack send a candidate response or authority response to the testing service respectively. While shows the candidate stack and authority stack as operating separately as independent entities implementations are not so limited. Rather in various implementations the operations of the candidate stack and authority stack may be performed in parallel sequentially or at other times by the same or different computing devices of the testing system or another system.

To perform the candidate test upon receiving a candidate response and corresponding authority response the testing service may compare the fields contained in the candidate response and the authority response along with other information such as latency data or other performance metrics and logs the results. The results of the comparison and the logs are then available for use by the components of the testing service and dashboard service . In some implementations the responses and or the other information about the candidate testing may include one or more tree data structures for example in a serialization format e.g. XML JSON and other serialization formats . The testing service may perform various operations to determine differences between the tree data structures of the responses and which will be described with respect to .

Except where explicitly noted otherwise with regard to the remaining discussion authority responses and production responses being used in the candidate test will be discussed as authority or control responses due to the similar treatment of the responses. Still as would be recognized by one of ordinary skill in the art the treatment of the authority responses and intercepted production responses may differ in some implementations.

The computing architecture may include one or more processor s and computer readable media that store various modules applications programs or other data. The processor s may be a single processing unit or a number of processing units all of which may include single or multiple computing units or multiple cores. The processor s can be implemented as one or more hardware processors such as microprocessors microcomputers microcontrollers digital signal processors central processing units state machines logic circuitries and or any devices that manipulate signals based on operational instructions. Among other capabilities the processor can be configured to fetch and execute computer readable instructions stored in the computer readable media a mass storage device or other computer readable media. The computer readable media may include instructions that when executed by the one or more processors cause the processors to perform the operations described herein for the testing service . In some embodiments the computer readable media may store a replay module a comparator module and associated components a metrics module and associated components a logger module and associated components and a controller module and associated components which are described in turn. The components may be stored together or in a distributed arrangement.

The replay module may operate to replay the intercepted requests to the candidate stack and in a least some cases the authority stack . In the following discussion it should be understood that the authority stack may not be utilized for all operations of the testing service e.g. in operations in which production responses and or the results of authority requests to the production system are utilized . Thus simultaneous discussion of the operations of the candidate stack and authority stack is for convenience and not limitation.

In summary in some implementations the replay module operates to impersonate the entity making the request and interacts with the candidate stack and authority stack in accordance with this role. In some implementations the replay module operates to dynamically modify at least some of the parameters of the intercepted requests before replaying the requests to the candidate stack and authority stack as the candidate requests and authority requests . For example the replay module may modify candidate requests to the candidate stack to simulate specific behavior for test purposes. In such an implementation the replay module may preserve the integrity of the modified shadow request apart from the intended modifications to faithfully replay the shadow request.

As mentioned above in some implementations the candidate stack may operate a candidate software system which is a different implementation of the software system operated by the production stack or the authority stack e.g. an implementation utilizing a different framework or interface to similar core logic . The candidate stack may also operate a candidate software system which is an entirely different software system to that operated by the production stack or the authority stack . In these and similar scenarios the replay module may operate to modify the intercepted requests to match a specification of the candidate software system operated by candidate stack .

Upon receiving the candidate response and authority response corresponding to a particular intercepted request the replay module may extract meta data for the responses and publish the responses and meta data to the comparator module . Some examples of meta data that may be extracted include information that may be used to derive latency data or other performance metrics.

The comparator module may receive the candidate response and authority response and perform a candidate test between the candidate response and the authority response . In some implementations the comparator module tags and or classifies at least some of the differences that are ascertained between the responses. For example the comparator may tag or classify candidate test differences which are specified to be important or unacceptable to the functioning of the software system.

In some implementations extensible modeling language based definitions may be used to define the comparison and replay by the testing service based on a standardized format. Using such definitions the comparator module may allow differences based on planned functionality changes in the candidate stack to be suppressed e.g. ignored . In some implementations such suppression of differences based on planned functionality changes in the candidate stack may be implemented at a variety of levels and or other modules rather than by the comparator module . The results of the comparison module are provided to the metrics module and the logger module .

It should be noted that the differences determined in the candidate test are not limited to any particular type of differences. For example the differences that are tagged may also include processing differences not discussed herein. An example of a processing difference is a difference in the processing of the request which may not change the result of the request or result in a latency difference in the response but which causes non critical error messages or issues unnecessary or superfluous internal requests and may represent an additional processing burden on another system or process. A large number of such processing differences may cause other services or systems to become overwhelmed without an apparent difference in the timing or content of the response to the request.

As mentioned above in some implementations the candidate response and the authority response may be or may include tree structures or other similar data structures that are to be compared. In general the attributes of items such as a product or a person may be represented in a tree structure. The two items can then be compared by finding differences in the respective tree structure representations. Depending on the implementation if the two trees meet a threshold of similarities or differences the trees may be considered to be or not to be a match.

In some implementations including that illustrated in the comparator includes a tree comparator that operates to perform comparisons for tree structures. Although some implementations may perform a simple exact matching algorithm to determine if the trees are a match such an approach may result either in differences not being detected or the detection of an apparent difference that is either not really a difference or in the detection of a difference that is not meaningful for example due to different representations of the same float or Boolean value. In other cases the exact matching algorithm may stop comparing at the first node of the tree structure that has an apparent difference and not even attempt to match the children of the apparently different node. In some implementations a user may desire to see the differences between the children.

In some implementations the tree comparator may operate to address one or more of the following example problems with an exact matching algorithm 1 differences in attributes of nodes with matching tags blocking further comparison 2 different representations of the same value causing false mismatch 3 differences in node ordering not being detected causing false mismatch 4 missing nodes and or extra nodes leading to sub tree mismatch. It should be noted that depending on the details of the implementation some of the above listed example problems may or may not need to be addressed. For example the first example problem may occur in the case of a comparison of the following XML data structures 

More generically the problem may result from a compound declaration resulting in a premature determination of a mismatch. In the example shown in Table 1 the comparison is of two statements that are compound declaration as they both declare the tag and an attribute of the node. The tag of the two nodes is the same Michael. However the attributes are different one being age and the other being experience. Thus if an exact matching approach is used to compare the tree structures of Table 1 the comparison will find a mismatch. In implementations that include functionality to or are configurable to address this problem the two nodes may be compared as if the data structures were presented in an expanded non compound form such as that shown in Table 2 

Thus when a comparison is performed the tags are compared and matched. Then since the tags match the attributes are compared but found to have a mismatch in attribute type. Thus such implementations may recognize the tree nodes as having the same tag Michael but as having different attributes.

Depending on the implementation and use of the tree matching techniques disclosed herein this may or may not be a meaningful difference. For example in a candidate test scenario in which the two structures are intended to convey the same information about the tag Michael this difference may be significant and meaningful. On the other hand in a lookup or matching scenario the difference is not indicative of a mismatch. Rather since the difference is in the attribute i.e. in the type of data being compared some implementations may ignore the difference and find the trees to match or at least determine that the trees relate to the same tag.

In some implementations the comparator may or may be configurable to omit and or filter some of the results that the comparator provides to the metrics module . For example the comparator may omit and or filter the results based on whether the differences are determined to be meaningful. In some implementations such filtering and or omitting may also be performed by selectively suppressing such differences in the results such that some differences for a candidate response will be reported while other differences will be suppressed.

The metrics module may generate metrics from the results of the processing by the candidate stack and the authority stack that were provided by the comparator module . In some implementations the statistical analyzer may determine a trend in the number of differences identified by the comparator module to be meaningful the number of differences identified by the comparator module to be unacceptable determine the number of unacceptable differences identified capture the trend and or cause an alarm to be sent to the dashboard service and so on. The statistical analyzer may determine positive or negative trends for the candidate software system operated by the candidate stack . For example the statistical analyzer may determine that a particular client request is indicative of or correlated with a particular outcome either good or bad . The statistical analyzer may then indicate or record the trend to enable the dashboard service to report the trend and allow for appropriate action to be taken if necessary. The statistical analyzer may also use confidence levels when determining the trends. The performance analyzer may determine or measure performance trends based on performance of each of the candidate stack and the authority stack . The performance analyzer may determine how the system resources are responding to use of the different versions or software systems include processing of spikes in activity response time memory allocation throughput bandwidth or other system performance measurement attributes. The system performance may be analyzed using business metrics system level metrics e.g. memory usage processor usage etc. and or application level metrics e.g. bugs errors difference count etc. . For example the performance analyzer may provide statistics on latency differences between the candidate software system of the candidate stack and the authority software system of the authority stack . The metrics module or the comparator module may also determine when a candidate software system operated by the candidate stack includes a bug or other error. Further in some embodiments the results of the metrics module and or the comparator module may be used to identify a failing service in a cascading sequence of service calls where the failing service is a downstream service that is causing differences in one or more upstream services. The results of the statistical analyzer and performance analyzer may be output at least to the logger module . As with the comparator the operations of the metrics module may take into account whether differences between the candidate stack response and or processing and the authority stack response and or processing are determined to be meaningful. Such operations may be configurable to allow for adjustable inclusion or reporting thresholds based on a determined likelihood that a candidate test difference is a meaningful difference. In some implementations different classifications of candidate test differences may be treated differently based on the determined likelihood that the candidate test differences are meaningful.

The logger module shown in may comprise at least two components a request log generator and a performance report generator . The request log generator logs data related to the intercepted requests candidate requests and authority requests which have been processed by the production stack candidate stack authority stack replay module comparator module and or metrics module . The request log generator may log all data relating the intercepted requests or some appropriate subset depending on the particular implementation and configuration settings. In some implementations the request log generator may store the requests responses and differences. For example the request log generator may store the requests responses and differences in distributed computing based storage with indexed fields for searching. The performance report generator may generate a performance report which may be based at least in part on an output of the performance analyzer .

As mentioned above many operations of the replay module the comparator module the metrics module and the logger module as well as the interceptor are configurable. In the implementation shown in the configuration settings are controlled at least in part by a controller module . In particular a sampling manager of the controller module controls aspects of the interceptor and the testing service relating to determining which of the client requests are to be intercepted and forwarded as the intercepted requests which of the intercepted requests are actually processed by the testing service as described above and so on. The sampling manager may consult the configuration manager which interacts with the various systems and users such as the dashboard service to obtain the configuration settings for the testing service . Each of the interceptor replay module comparator module metrics module and logger module may consult the configuration manager to obtain configuration information or the configuration manager may directly configure the other modules. One example operation performed by the sampling manager may be to receive a predetermined confidence level and then calculate the number of samples intercepted requests necessary to achieve the predetermined confidence level. Such a confidence level may be determined based on various factors such as a number of unacceptable differences per a number of intercepted requests a requirement that some measurement of code paths have been exercised or a mix of use cases to be covered during the testing. In addition to the configurability discussed above the testing service system of some implementations may allow for pluggable modules based on a standardized interface. Such implementations may allow for custom modules which adhere to the standardized interface to be plugged into the testing service system in place of the default modules e.g. a custom comparator module and custom metrics module in place of the default modules .

Similar to the computing architecture the computing architecture may include one or more processors and computer readable media that stores various modules applications programs or other data. The computer readable media may include instructions that when executed by the one or more processors cause the processors to perform the operations described herein for the dashboard service . In some embodiments the computer readable media may store a reporting module a replay module a testing control module and a user interface module which are described in turn. The components may be stored together or in a distributed arrangement.

As mentioned above the dashboard service provides for interaction with and or control of the testing service . In some implementations the dashboard service provides the interaction and or control in at least two regards. First the dashboard service collects and parses the results logged by the logger module providing users of the dashboard service with this information. Second the dashboard service interacts with the controller module to configure the testing service configure the interceptor and or to setup and request replay of one or more intercepted requests . For example the dashboard service may setup and request the replay of a set of the intercepted requests represented in the logs generated by the request log generator or the intercepted requests as received from the interceptor . To select the one or more logged or stored intercepted requests to be replayed the dashboard service may provide search and display capability for stored requests and differences.

For example subsequent to a change in the candidate stack the dashboard service may request that the testing service replay the intercepted requests that resulted in meaningful unacceptable differences between the candidate responses and authority response to a new altered different candidate stack and in some implementations to the authority stack as well. Once the intercepted requests have been replayed either the testing service or the dashboard service may make a comparison between the new responses and the original responses to determine if the unacceptable differences have been resolved. The general purpose of modules in the example implementation shown in is discussed below followed by a discussion of the example operations performed by or caused to be performed by these modules.

The reporting module may operate to collect or receive the data generated by the logger module and any other data and prepare the data for presentation to a user via the user interface module . For example the reporting module may collect the trend data generated by the metrics module and prepare this data for presentation in a graph.

In some implementations in which candidate test differences that are determined to likely not be meaningful are tagged but not omitted by the components of the testing service the dashboard service may provide for a variety of user interface controls to allow a dashboard service user to adjust the inclusion or omission of candidate test differences in reports or presentations generated by the reporting module . In some implementations the presentation or formatting of the candidate test differences presented to the user may provide a visual distinction between the likely meaningful differences and likely not meaningful differences. Further the presentation may have a combination of these features. More particularly an adjustable threshold or other factor may be set for inclusion of likely not meaningful differences and a formatting or other visual distinction may be provided for those differences included based on the likelihood of the particular differences being meaningful. In a more concrete example in an output report showing differences on a line by line basis candidate test differences that are likely meaningful may be presented with black text highlighted in yellow and candidate test differences that are likely not meaningful may be presented as grey text without highlighting. Of course these are merely examples of the utilization of the likelihood of candidate test differences being meaningful and many variations are possible.

The replay module may operate in the manner discussed above to cause one or more of the logged intercepted requests to be replayed. In some implementations this is performed by requesting that the testing service replay the intercepted requests possibly with any desired changes in the setup. Though not illustrated in the figures in some implementations the replay module may include a copy of the candidate stack the authority stack and or a new altered different candidate stack or the replay module may interact directly with the software system of one or more of these stacks or the production stack . In such an implementation the replay module may replay the intercepted requests directly to the appropriate software system and or make the appropriate analysis of the results. As discussed above one example reason for replaying the intercepted requests may be to determine if a changed candidate software system has reduced eliminated or exacerbated any unacceptable meaningful differences between the candidate response and authority response . The results of the replay of the intercepted requests would be passed for example to the reporting module for preparation for presentation to the user via user interface module possibly after being analyzed by the comparator module the metrics module the logger module and or other similar modules .

As mentioned above the testing control module may operate to allow for configuration and or control of the testing service by for example a user of the dashboard service interacting with the dashboard service through the user interface module . An example control that may be performed by the control module would be to configure comparator module to tag differences in specific fields for audit and display purposes rather than all fields. Another example control that may be performed by the control module would be to configure the intercept parameters of the interceptor e.g. the percentage of client requests to intercept the maximum number of client requests to be intercepted in a given time period types of client requests to intercept etc. Another example control that the control module may provide to a user of the dashboard service would be to provide an interface for configuring the candidate testing and the behavior of the various modules of the testing service that result from different scenarios of such testing. For example as discussed above the testing service may be configured to omit filter suppress or otherwise distinguish candidate test differences that do not appear to be caused by differences between the candidate and authority software systems or that are not meaningful. In some implementations the control module may be utilized to set thresholds categorical treatments and or other factors for determining what type of treatment a determined difference is to be given e.g. omitted included partially included visually set off etc. As indicated above the user interface module of the dashboard service may present a user interface to dashboard service users to allow for interaction by the dashboard user with the testing service system.

As alluded to previously through interaction with the dashboard service a dashboard user is able to configure the duration of the testing such as by configuring conditions upon which the interceptor stops intercepting requests to the production system . Some types of conditions are described below.

One example condition for controlling the duration of the testing is a specified mix of use cases represented by the intercepted requests such as a number m of first use case requests a number n of second use case requests and so on. Use cases of particular intercepted requests could be determined by the tagging and or classifying function of the comparator module discussed above. In addition to using the mix of use cases to drive the duration of the testing the dashboard service could use the determined use cases to provide information on the distribution of use cases to the dashboard users via the reporting module and user interface module . In some implementations the use case reporting may be updated on a real time basis as intercepted requests are received by the testing service and processed. Such use case information could be presented in a textual manner or in a visualization such as a chart for ease of comprehension. The determination of use cases and subsequent presentation of the distribution of the use cases represented by the intercepted requests that have been processed may also be performed without the use of this information to control the duration of the testing.

Another example condition for controlling the duration of the testing is a measure of code coverage. For example the testing service system could be configured to continue the testing until a defined percentage or other measurement of the code of the candidate stack has been tested to a satisfactory degree. One example implementation to determine code coverage of an intercepted request would be to instrument code of the candidate stack to be tested such that when a portion of the code is executed it outputs an indication of its execution. Such instrumenting could be coded into the source code of the candidate software system but selectively compiled based on a flag during the compilation process. Thus when a candidate software system is to be generated by the compiler for testing the flag would be set and the code coverage instrumentation code would be compiled into the candidate software system. When the candidate software system is to be used as a production software system the flag would not be set and the compiler would ignore the code coverage instrumentation code.

Further the testing service system described herein may also be integrated with a source code control system of the software system being tested to allow for identification of code changes that resulted in deviance from expected results and or to identify the code paths which map to the differences in responses between the candidate stack and the authority stacks . For example for a meaningful difference that occurs between a candidate software system and an authority software system that are versions of the same software system a developer may be provided with information related to changes in the source code of the software system between the candidate software system and the authority software system and that are also associated with the generation of the logical location of the difference if applicable.

Integration with the source code control system may also allow the testing service system to include an automatic source code rollback function for the candidate software system of the candidate stack . For example based on threshold of meaningful unacceptable differences latency increases or the like the dashboard service either through program logic or explicit user instruction could instruct the source code control system to rollback changes to the source code of the candidate software system being tested. In addition to using the code coverage to drive the duration of the testing the dashboard service could use the determined code coverage to provide information on the code coverage to dashboard users via the reporting module and the user interface module . As with the use case reporting in some implementations the code coverage reporting may be updated on a real time basis as intercepted requests are received by the testing service and processed. Such code coverage information could be presented in a textual manner or in a visualization such as a chart or graph for ease of comprehension. Of course the determination of code coverage and subsequent presentation thereof may be performed without the use of this information to control the duration of the testing.

In addition the dashboard service may provide a dashboard user with a user interface e.g. via the user interface module to cause the testing control module to configure the testing service the candidate stack and the authority stacks for a given test. For example prior to executing a given test the user may be able to configure the software systems software system versions end points fleets and the like to be used for the candidate stack and or authority stack s .

In a first particular example the dashboard user may utilize the dashboard service to select system resources to operate one or more of the candidate stack the authority stack s the interceptor or other aspects of the system e.g. one or more machines of a fleet of machines one or more distributed computing resources available for provisioning etc. . The dashboard user may then utilize the dashboard service to select the software systems software versions end points fleets and the like to be used for the candidate stack and or authority stack s . Once system resources are selected and system parameters are input the dashboard user may cause the dashboard service to control the startup of the candidate stack the authority stack and or other aspects of the testing service based on the parameters selected by the dashboard user. In an example startup of the candidate stack the user may select one or more machines included in available system resources choose a particular candidate software system and cause the selected machines to be provisioned with the candidate software system i.e. install the candidate software system on the machines and perform any other setup process es needed to provision the selected machines .

In a second particular example the dashboard user may utilize the dashboard service in the same manner to select the parameters for the testing service except that the user may select system resources already provisioned with the software systems and the like to be utilized. In such a case the user may be provided with user interface controls to select any endpoint that matches the parameters of the software systems indicated.

While the above discussion includes particular examples of controls that may be provided to the dashboard user by the dashboard service implementations are not so limited and such details may vary from implementation to implementation. For example in some implementations the user may be provided with a combination of the particular examples of selecting parameters for the testing service . In a particular example some implementations of the dashboard service may provide functionality to select either or both pre provisioned and unprovisioned system resources for utilization by the testing service . These and other variations would be apparent to one of ordinary skill in the art in view of this disclosure.

At the interceptor intercepts a client request from the user to the production system . At the interceptor forwards a production request to the production stack and forwards a duplicate of the request to the testing service as an intercepted request . At the production stack processes the production request normally such that a production response is sent back to the user device as a client response . In the implementation illustrated in the interceptor may optionally intercept and forward the production response to the testing service as an intercepted response.

At the testing service receives an instruction to initiate testing and based on the instruction sends at least some of the intercepted requests to the candidate stack and authority stack for processing as the candidate requests and authority requests .

At the candidate stack and authority stack receive the candidate requests and authority requests respectively. Then the candidate stack and authority stack process the requests based on their respective software systems and return the candidate responses and authority responses to the testing service respectively. As stated above regarding in some implementations the functions of the authority stack may be fulfilled by the production system and more particularly the software system operated by the production stack . Also in some implementations the candidate stack and authority stack may need to interact with devices outside of the testing service system such as the production stack or other production systems in order to process the candidate requests and authority requests . In such cases the interactions with the outside devices may be marked as testing interactions to prevent the outside devices operating on the testing interactions as if the testing interactions were production interactions that modify the production system state and or data. For example in the case of stateful transactions some implementations may support storing stateful data e.g. transaction data as candidate transaction data which will be ignored by production systems. The candidate transaction data may be written by the candidate stack and the testing service loads the candidate transaction data and compares it to production transaction data or authority transaction data after processing each intercepted request . Depending on the details of the implementation authority transaction data may also be marked in a similar manner to candidate transaction data. 

Other implementations may provide support for stateless testing for transaction based i.e. stateful services. For example such implementations may provide hooks in the software system of the candidate stack to avoid the side effect of storing data in a persistent data store. This may allow requests to be sent to the candidate stack without resulting in storage of transactional data.

At the testing service performs a candidate test using the candidate response and authority response . Based on the results of the candidate test the testing service may determine or evaluate the candidate test difference for meaningfulness. Such a meaningfulness evaluation may provide the evaluation as a value on a scale 0 100 a true or false value scale or another manner of representing the result. The testing service also analyzes the responses and based on one or more candidate authority and authority authority response pairs may derive metrics for the stacks on both a request by request basis and an aggregate basis.

At the testing service may log the results of the comparison and derivation analysis with the requests and responses as well as any other data regarding the processing to this point depending on the implementation . The testing service may store the logged information in a variety of ways.

In some implementations the logged intercepted requests and associated information may be stored in a searchable catalog organized in a hierarchical manner. For example the following might be paths in the hierarchy 

For each node in the hierarchy the testing service may provide support to replay all or a subset of the intercepted requests under that node.

In some implementations the stored logs provide support for an additional type of testing not explicitly mentioned above. In particular using the stored logs including stored requests and responses the testing service may also provide support for regression testing. In other words the testing service may be capable of running a full regression suite from a node in the request response catalog against a candidate software system by replaying the stored requests and comparing the candidate responses against the stored responses e.g. production or authority responses . This way a new candidate software system may be thoroughly regression tested using a large number of realistic production requests as much as hundreds of thousands millions or more . Such testing is based on the principle that the behavior in production or the behavior of an authority version may be presumed to be correct and therefore the stored responses can be used to qualify new candidate software systems for example prior to the testing described above with respect to .

Another storage option is to create an index where each intercepted request is labeled with a unique ID. Such an index may resemble the following 

This second option allows for a single request to be mapped to multiple scenarios. To express the hierarchical paths in such an index the testing service could use set intersection. The generation of the request repository and generation of the meta data index may be automated and regenerated from production requests. In some implementations the repository generation process may continue until a specified index is complete meaning each entry in the index maps to at least one request or even that specific combinations of indexes exist e.g. Non Company SOR AND E book. Such an index may provide for very specific use cases to be regression tested with limited numbers of other use cases being exercised. By utilizing this or another indexing scheme some implementations may provide indexing based on the code coverage or use cases represented by the indexed requests. Thus in some implementations rather than testing one hundred thousand to ten million requests and relying on the assumption that the large number of previously tested requests provide the coverage needed a smaller number of requests may be tested with a higher degree of certainty that the coverage is provided. Further when a regression test fails a user may immediately know what use case or code path failed.

In such a system the use case information or code coverage information may be used to create a test case repository of test cases that map to sets of logged requests. Such test cases may be generated to be small sets of requests that exercise desired levels of code coverage e.g. the smallest set of requests that give the desired code coverage . For example in building a test case for a code coverage instrumented candidate stack as each new request that may be added to the test case is processed the testing service may determine if code not previously exercised by the test case request set is exercised by the new request. If so the new request may be added to the test case request set. If not the new request may not be added to the test case request set. In this way the overall code coverage of the test case may be increased without substantial increase in the number of requests in the test case set. Depending on the implementation and the purpose of the test case many variations are possible. For example the determination as to whether a new request should be added to the test case may be based on how many requests in the test case set already exercise the code exercised by the new request. For example for some code the test case developer may desire multiple requests be processed. At the same time for other code the developer may desire two requests be added for the purpose of exercising the code.

In another variation the system may determine the particular code exercised by the requests. This information may be stored with the request as a request signature. When building a test case the system may add requests based on the number of requests with the same test signature already present in the test case set. For example a developer may desire two requests be included for each test signature. Alternatively or additionally the developer may desire that two requests be included for each test signature but for some set of indicated test signatures a different number be included. Further in such a system the request may be indexed in a repository by the test signatures.

In some implementations if the user knows the behavior of the software is going to change between the authority or production software system and the candidate software system the user may be able to exempt use cases based on the meta data affected by the behavior change. In addition or alternatively some implementations may index the requests based on other criteria such as candidate test differences latency differences processing differences amount or other measure of any candidate test difference e.g. an absolute value a ratio a percentage etc. . As such the additional or alternative indexes may be utilized to provide requests that reflect such criteria.

In general the tree comparison module may compare the trees presented to the module using a matching algorithm. In addition the tree comparison module may include functionality to address the issues discussed above regarding . One example of such functionality was discussed above regarding that addressed compound declarations of tags and attributes of nodes resulting in mismatches. As discussed above in some implementations including such functionality the tree comparison module may be configurable to ignore attribute differences in tag statements or to otherwise address the issue. Some other example functionalities of the tree comparison module are discussed below. In particular example functionalities are provided below that address the second problem of different representations of the same value causing false mismatch the third problem of differences in node ordering not being detected causing false mismatch and the fourth problem of missing nodes and or extra nodes leading to sub tree mismatch.

As mentioned above another functionality of the tree comparison module may operate to address the second example problem different representations of the same value causing a false mismatch. An example of this problem is shown in Table 3 

In the above example the Boolean values of the key nodes Yes and 1 would normally be found to be a mismatch. Such a mismatch may be a false mismatch. This may be addressed in some implementations by defining for each type of data that may have multiple representations equivalence sets or precision levels. For example Table 4 may provide such an equivalence set for Boolean values 

Thus when two values are compared rather than determining the values to be different if the values are not an exact match the tree comparison module may determine if the first value is in one or more equivalence sets. If so the tree comparison module may determine if the second value is also in an equivalence set that includes the first value. For example with regard to the example comparison shown in Table 3 the tree comparison module may determine that the value Yes in the True equivalence set. The tree comparison module may then determine if 1 is also in that set. Since 1 is in the same set the values are determined to be true.

A similar operation may be performed for values that may be represented with different levels of precision such as Real numbers. For example a string comparison of 4 and 4.0 will result in a mismatch that is likely false. Similarly a developer or user may wish for values to be found to match even if not exactly the same if the values are equivalent to a particular level of precision. For example in some implementations the tree comparison module may be configured or configurable to compare floating point numbers to three decimal points precision such that 99.7778 is matched to 99.7776 but not 99.7791. 

In some implementations the equivalence sets and or precision levels may be configurable for each portion of a tree structure such as for particular sub trees nodes attributes or the like. Thus one portion may be compared to three levels of precision while another portion is compared to nine levels of precision.

Another functionality of the tree comparison module may operate to address the third example problem differences in node ordering not being detected thereby causing false mismatches. In particular some implementations of the tree comparison module may operate to detect certain data types types of trees or types of subtrees and perform a matching operation based on the detected type. Two example types of trees or subtrees are lists and maps as discussed below.

A sub tree of a node may be detected as a list if the immediate children of the node have the same tag. An example of a list is shown in Table 5.

In the example above the node with tag items may be treated as a list because all the node s immediate children have the same tag name item . Once a list is detected in both of two trees that are being compared a cross matching between the list items may be performed to determine whether the trees match. In this context cross matching operates to determine matches between items in two unordered structures. Table 6 illustrates two lists to be matched.

An exact matching algorithm would detect Tree 1 and Tree 2 as a mismatch upon comparing item a to item b the first items in the respective lists . The list comparison operation on the other hand would compare each list item in Tree 1 to each list item in Tree 2 and find correspondence for the items in lists. Thus the list comparison operation would determine Tree 1 and Tree 2 to match.

Maps the other example type of sub tree mention above may be detected in a similar type of operation. A map is a list of entries. A sub tree of a node is defined as being of type map if the immediate children of the node have a common tag name and each of the immediate child elements has two immediate child elements. Each set of two grandchildren elements are pairs in which one element is treated as a key and the other element is treated as a value. An example map sub tree is shown in Table 7.

In the example above the node with tag addressMap would be detected as a map sub tree because each immediate child of the addressMap node has the same tag entry and each immediate child has two children i.e. grandchildren of the addressMap node . The content of the map example of Table 7 could be represented in the simpler form shown in Table 8.

Once a map is detected in both of two trees that are being compared a map matching operation may be performed to determine whether the maps are a match. Table 9 illustrates two maps to be matched.

A simple similarities algorithm would incorrectly detect Tree 1 and Tree 2 as a match because the same lines appear in both trees even though the content is not a match. Table 10 illustrates the map examples of Table 9 in a simpler form to illustrate the mismatch of content.

As can be seen in Table 10 the country values are reversed. The example map matching algorithm described here operates to compare nodes entries with the same keys. Thus the entry with the key billing in Tree 1 is paired for comparison with the entry with the key billing in Tree 2. In view of this pairing the trees are found to be a mismatch.

This is more efficient than the list matching operation described above because the tree comparison module does not have to perform a cross match between the sub trees of the entries and because comparisons of keys requires far fewer operations. Once matching entries based on keys are found the sub trees of the key matched entries are compared. If the entries of each map are found to have corresponding matches in the other map the maps may be determined to be a match.

Maps and lists are merely examples of data types that may have particular matching operations. Other data types or tree or sub tree types may have particular matching operations as would be apparent to one of ordinary skill in the art in view of this disclosure. An example of another data type that may have a particular matching process is shown in Table 11 below. In particular the data type shown in Table 11 is similar to the map data type but uses tags of the entries as keys to identify the value rather than the key node and value node pair representation discussed above.

In an example detection and matching process for the sub tree type shown in Table 11 the process may detect a list of items in each tree at the high level on a first pass. On the next pass the process may determine that the list items include child nodes with no repeating tags. Based on this information the process may determine that the tags may be used as keys to pair various portions of the items for comparison. For example the process may use the tags as keys to pair the nodes based on tag matches across the sub trees of the responses similar to the use of key nodes in the discussion of .

Another functionality of the tree comparison module may operate to address the fourth example problem discussed above missing nodes and or extra nodes leading to sub tree mismatch. Some examples of this problem occur in lists. For example the problem may occur when one of two lists being compared has an extra node or when each of two lists being compared has a node that does not appear in the other list. Two example techniques for addressing this issue are discussed below.

In the first example technique a similarity function may be defined that matches nodes to provide a best match. The similarity function may be defined to maximize similarity. This technique may be implemented in a similar manner to solutions of the stable marriage problem. Of the two example techniques for addressing this problem discussed herein this technique has higher runtime complexity but may provide more information.

In the second technique ignored nodes or fields may be defined to avoid differences caused by extra and or missing nodes or fields. For example in a testing system such as that shown in when it is known that one or more new fields have been added to a response that one or more old fields have been removed from the response or both the field s may be defined as an ignored field to eliminate meaningless differences in the candidate test differences and to achieve a lower runtime complexity. An example of this technique may be seen with respect to the example trees of Table 12.

Using the above discussed list matching technique Tree 1 and Tree 2 will be determined as a mismatched. However in a candidate test scenario in which the user knows that item a has been removed and item d has been added this mismatch is an expected result and inclusion of these differences in the candidate test differences may not be desired. Thus items a and d may be defined as ignored fields to remove the differences.

The above described functionalities of the tree comparison module are not limiting. Other functionalities and or variations of the above defined functions would be apparent to one of ordinary skill in the art in view of this disclosure.

In some implementations the tree comparison module may operate to pair nodes for comparison differently using a plurality of modes depending on the types of elements being compared. For example in a first mode the elements may be paired for comparison using a relatively low complexity technique such as a pairing technique based on tags. In a second mode the elements may be paired for comparison using a relatively more complex technique such as a pairing technique based on tags structure content and ignored fields. Some implementations may pair simple elements using the first mode and pair more complex elements using the second mode. To determine which mode should be used for a given element some implementations build knowledge about the structure of the trees being compared prior to the comparison.

Alternatively or in addition to using different modes for pairing items for comparison some implementations may be configured to or configurable to cap the level of depth into a tree that is considered when pairing the nodes of the tree for comparison. Similarly some implementations may be configured to or configurable to cap the level of depth into a tree that is compared when matching nodes. In some implementations the tree comparison module may operate to allow the level of depth cap to be set and or adjusted. For example the level of depth cap may be changeable depending on the portion of the tree or based on detected structure or content of the portion of the tree.

Moreover the tree comparison module may perform operations for different portions or levels of depth at adjustable rates. For example the tree comparison module may be configurable by a user to compare or pair nodes of the tree structures using five levels of depth for four of five pairs of trees and to compare or pair nodes using ten levels of depth for the fifth pair of trees. Further the tree comparator module may be configurable to compare a first portion of the trees for two out of five pairs a second portion for three of five pairs and the remainder of the tree for every pair. These are merely examples and many variations would be apparent in view of this disclosure.

The adaptation module may utilize information obtained from the user and the results of the tree comparisons as training data to adapt the comparison logic utilized by the tree comparison module to perform tree comparisons with the goal of improving the accuracy and or usefulness of the tree comparisons . Some example adaptations may include adapting the level of depth cap s adapting the complexity levels for applying different modes for pairing nodes for comparison developing logic for comparing types of structures within trees adapting the rates at which different portions or depths are compared and so on. For example if a portion has consistently included no differences the adaptation module may adjust the rate of comparison for the portion down. Similarly if that portion subsequently includes differences the tree comparison module may operate to increase the rate of comparison for that portion.

Another example adaptation may occur where very few differences are being found in a depth capped comparison operation. After a period of time without or with few differences the adaptation module may increase the depth cap until differences are found. Similarly if the increase in depth of comparison does not result in additional a higher rate of differences being found the adaptation module may reduce the depth cap for the portion to reduce the cost of the comparison operations.

Another example adaptation may occur when many differences are being found in where the level of depth for pairing nodes for comparison is capped. For example if the level of depth is to low when operating to pair nodes of a map the pairing would be based at the most depth on the tag which is not greatly informative. As such entries with different keys may be paired for comparison. Thus in some implementations the adaptation module may increase the level of depth used for pairing nodes for comparison when large numbers of mismatches are detected.

The adaptations discussed above are not intended to be limiting. As one of ordinary skill in the art would understand in view of this disclosure many variations and alternative adaptations are possible.

At the tree comparison module receives a pair of items for comparison. For example the items for comparison may be candidate and authority responses items for comparison in a lookup or matching operation etc. At the tree comparison module operates to detect tree structures in at least one of the items.

At the tree comparison module selects a portion or level of tree structure for comparison. For example the tree comparison module may begin by recursively performing a comparison operation until the level of depth of a cap is reached. At once the portion or level for comparison is selected the tree comparison module may operate to determine if ignored fields are present in the selected portion or level. If so the ignored fields are removed from the comparison operation or otherwise not considered in the comparison operations.

At the tree comparison module determines if the selected portion or level is of a structure type with a particular comparison treatment or process. For example the tree comparison module may operate to detect if the selected portion is a list or a map as described above regarding . If a list is detected the process continues to . If a map is detected the process continues to . Otherwise the process continues to .

At the tree comparison module performs a matching of the list items as discussed above regarding . For example cross matching may be used to match unordered list items. At the tree comparison module determines the map structures and matches the structures by the keys i.e. the first child nodes of the immediate children of the map root nodes .

At for each determined difference the tree comparison module determines if the difference is the result of a difference in representation of the same value e.g. a float a Boolean etc. or an ignored field. If a difference is the result of difference in representation or an ignored field the difference is removed from the differences report or otherwise ignored.

At the differences that remain may be meaningful and are output or added to a difference report and the process returns to until no portion or level of the tree remains for comparison.

The process illustrated in is merely an example and is not intended to be limiting. As one of ordinary skill in the art would understand in view of this disclosure many variations and alternative processes are possible.

In an example scenario for in the authority system a Boolean appears in the ItemList list that indicates whether the items in the list are gifts. In the candidate system this Boolean value has been removed. Prior to performing the candidate test the developer may add the Gift field to the ignored fields list for the comparison. Because once the Gift field is removed from consideration all of the items in the ItemList of the authority response have the same tag item the tree comparison module detects the sub tree as a list. Upon performing a similar operation for the candidate response a list of items is also detected. The tree comparison module then performs the cross matching operation of the list items discussed above to determine if the pair of responses match.

In particular in analyzing the structure of the authority response the tree comparison module determines that each immediate child node of the items node has an item tag to detect that the items node is a list. In examining each item node of the items list the tree comparison module may note that each child of the nodes with the item tags has the same tag entry and each entry node includes two immediate children. As such the item nodes are determined to be maps. After a similar operation is performed on the candidate response the candidate response is determined to include a list of maps similar to that of the authority response .

The tree comparison module may then perform a cross matching operation of the list items as discussed above to determine if the pair of responses match. In performing this cross matching operation the tree comparison module may utilize the key based matching discussed above with respect to to compare the map nodes with the item tag. For example the first entry of the first item of each response has a name tag. As such the second children of these entries are compared and a mismatch is found. The process is repeated with respect to the first item of the authority response and the second item in the candidate response . The first entry of this item pair is a match. The taxable entries of the items are then compared and determined to be a match based on reference to a Boolean equivalence set such as that discussed above with regard to . Similarly the cost entries of the items are compared and determined to be different representations of the same numeric value. Thus the first item of the authority response and the second item of the candidate response are determined to be a match. After repeating this operation with regard to the second item of the authority response and the first item of the candidate response another match is found and the response pair is determined to be a match. However after determining the second item of the authority response and the first item of the candidate response to be matching items the comparison of the taxable entries finds a difference that is not accounted for by the Boolean equivalence set. In particular the authority response indicates ProductY is taxable but the candidate response indicates that ProductY is not taxable. This difference may be output as a meaningful difference or may be subsequently eliminated as an ignored difference e.g. if ProductY s tax treatment has been intentionally changed between the authority and candidate software systems .

Of course the functionality described with regard to may be applied to different items and is not limited to candidate and authority responses. Other applications and or variations of the above defined functions would be apparent to one of ordinary skill in the art in view of this disclosure.

At the tree comparison module of the testing service may operate based on the current iteration of the comparison logic e.g. the current parameters for the type of testing level of depth for pairing rates of testing for fields or portions and or any other machine or user setting . The resulting difference report s generated at are output normally and received at .

At after one or more difference reports have been generated and output the adaptation module may operate to analyze the one or more difference reports to determine difference trends comparison cost data or similar data and or may also receive user feedback of a similar nature. For example the adaptation module may determine that much processing time is being spent performing a deep comparison of a portion of the responses that has not had a difference in some period. In another example determination differences may be determined to have occurred in a portion that is compared for only some fraction of response pairs due to a historical lack of differences. Moreover a user or developer may indicate that significant changes have been made in the portion of the candidate software system that generates a particular portion of the responses.

At in view of the analysis of the difference reports and or the feedback the adaptation module may adapt the comparison logic of the tree comparison module such as by changing the parameters of comparison of the response pairs. For example the information may be used as training data to adapt the comparison logic using machine learning.

At the adaptation module may output the adapted comparison logic to the tree comparison module . In turn the tree comparison module may begin using the adapted comparison logic in future comparisons of response pairs.

This disclosure provides various example implementations as described and as illustrated in the drawings. However this disclosure is not limited to the implementations described and illustrated herein but can extend to other implementations as would be known or as would become known to those skilled in the art in view of this disclosure. For example in some implementations the tree comparison techniques described herein may be utilized in systems of than the testing service described above. Some examples applications are discussed below.

A first example application of the tree comparison techniques described herein is to find reverse matches. This has broad application in various use cases for eliminating choices i.e. for use in matching algorithms where a user wants to find strong reasons to eliminate items from consideration when performing reverse matching. This is the reverse of the normal matching algorithms which for example seek to find similar items such as friends a user may want to connect to candidates a user may want to hire or products a user may want to purchase. Instead this application of the techniques disclosed herein takes the opposite approach to for example find persons the user would not want to connect with or candidates the user would not want to hire or products the user would not want to buy. In particular by performing the tree comparison techniques disclosed herein many differences may be found as opposed to the exact matching techniques typically used which would merely stop comparing at the first difference. If the differences found for a possible friend are similar to differences found in other persons a user has chosen to not make friends the possible friend may be eliminated from consideration.

In a similar application of the disclosed techniques a service e.g. the testing service a dating service a recommendation service social networking service search service etc. may operate to see if differences outweigh similarities between two items. As stated above by performing the tree comparison techniques disclosed herein many differences may be found as opposed to the exact matching techniques typically used which would merely stop comparing at the first difference. This may allow some implementations to take differences into account when finding similarities. If the number of differences outweighs the number of similarities then the two items may not be considered similar. An example scenario for this application may be shown with regard to .

In the illustrated example the tree structure of the potential hire includes various characteristics of the hire divided into two maps one for job history and education the other for ratings provided by an interviewer for the candidate. The tree structure for the ideal hire includes similar entries but for most such entries includes ranges of values within which an ideal hire would fall. In some implementations no particular characteristic may be sufficient to disqualify a potential hire and as such evaluation of potential hires on all of the characteristics may be necessary. Of course this is not a limitation and in other implementations a mismatch of some or any of the characteristics may be sufficient to disqualify a potential hire. Turning to the illustrated example the potential hire matches each characteristic with two exceptions infield and ownership. In the illustrated example infield is the amount of time the potential hire has worked in the pertinent field and the ownership characteristic is the interviewer s evaluation of the potential hire s willingness and or ability to take ownership of tasks or projects to which the potential hire is assigned. Depending on the implementation the infield difference may be ignored in view of the matches of the other characteristics and the small degree of the difference. However in the same or other implementations the failure to match the ownership characteristic may be treated as a disqualifying mismatch. On the other hand the matching of each of the other characteristics may be sufficient to override the ownership mismatch. These and other variations on the usage of the tree comparison techniques disclosed herein to determine if differences outweigh similarities between tree structures would be apparent to one of ordinary skill in the art in view of this disclosure.

A further application may allow a service e.g. the testing service a dating service a recommendation service social networking service search service etc. to find and use hidden similarities. As described above the tree matching algorithm can find similarities that the exact matching algorithm might otherwise ignore. So for example if an age node differs between items i.e. people but the astrological sign which is a descendent of the age node is the same then the two people may be close enough to be considered similar. Similarly the tree comparison techniques disclosed herein may be utilized to de duplicate a catalog. In particular catalogs in e commerce are susceptible to duplicate listings when multiple sellers list items in the catalogs. A first seller and a second seller may list the same item with similar but distinct product information and descriptions. As a result the item has duplicate listings. The tree comparison techniques and systems herein may be utilized to detect such duplicate listings that are similar enough to be likely duplicates even when a high level node e.g. the product name or title is different. Once such suspected duplicates are located some implementations may provide the suspected duplicates to a user who may in turn review the suspected duplicates. If the user agrees that the items are duplicates the user may cause the items to be merged. Example tree structures that illustrate a usage of the tree comparison techniques disclosed herein for de duplicating is illustrated in .

In the illustrated example the tree structures of product X and product Y include various entries of the products similar to those of the tree structures of . In the illustrated example most of the entries do not match between the tree structures and with the exception of a matching UPC or Universal Product Code. As such an exact matching algorithm would indicate that the products are not duplicate listings since at least one entry is a mismatch. However using the tree comparison techniques disclosed herein the hidden match of the UPC entry may be ascertained which is a strong indication that either the listings are duplicates that are presented differently or that there is an error in the UPC of one of the products. For example the tree structure comparison techniques disclosed herein may operate to determine the tree structures and to include map structures and compare the entries of the maps based on the key values the first child of each entry . When the match of the UPC entries is ascertained the pair may be flagged for review. The listings may be reviewed and if the reviewer finds the listings to be duplicates one listing may be removed or the listings may be merged. These and other variations on the usage of the tree comparison techniques disclosed herein to determine similarities that the exact matching algorithm might otherwise ignore would be apparent to one of ordinary skill in the art in view of this disclosure.

A still further application of the disclosed techniques may be to find dissimilar items. A typical approach for finding similarities is people who bought this item also bought the following items. Using the disclosed techniques the tree comparison module may extend the concept of reverse matching above by showing people who did not like this item also did not like that item. 

Although the subject matter has been described in language specific to structural features and or methodological acts it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described. Rather the specific features and acts are disclosed as illustrative forms of implementing the claims.

