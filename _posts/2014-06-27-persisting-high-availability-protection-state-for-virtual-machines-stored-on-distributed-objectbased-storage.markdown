---

title: Persisting high availability protection state for virtual machines stored on distributed object-based storage
abstract: Techniques are disclosed for persisting high availability (HA) protection state for virtual machines (VMs) running on host systems of a host cluster, where the host cluster aggregates locally-attached storage resources of the host systems to provide an object store, and where persistent data for the VMs is stored as per-VM storage objects across the locally-attached storage resources comprising the object store. In one embodiment, a host system in the host cluster executing a HA module determines an identity of a VM that has been powered-on in the host cluster. The host system then persists HA protection state for the VM in a storage object of the VM, where the HA protection state indicates that the VM should be restarted on an active host system in the case of a failure in the host cluster.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09367414&OS=09367414&RS=09367414
owner: VMware, Inc.
number: 09367414
owner_city: Palo Alto
owner_country: US
publication_date: 20140627
---
Virtual machine high availability referred to herein simply as high availability or HA and hypervisor converged object based HC OB storage are two emerging technologies in the field of computer virtualization. HA is designed to minimize virtual machine VM downtime by monitoring the availability of host systems and VMs in a host cluster. If an outage such as a host or network failure causes one or more VMs to stop executing HA detects the outage and automatically restarts the affected VMs on active host systems in the cluster. In this way HA ensures that guest applications running within the VMs continue to remain operational throughout the outage. One exemplary HA implementation is described in commonly assigned U.S. Patent Application Publication No. 2012 0278801 published Nov. 1 2012 entitled Maintaining High Availability of a Group of Virtual Machines Using Heartbeat Messages. 

HC OB storage is a distributed software based storage technology that leverages the local or direct attached storage resources e.g. solid state disks spinning hard disks etc. of host systems in a host cluster by aggregating these locally attached resources into a single logical storage pool. Thus this technology effectively re purposes the host cluster to also act as a distributed storage cluster. A hypervisor based storage system layer referred to herein generically as a VSAN layer comprising VSAN modules manages the logical storage pool and enables interactions between the logical storage pool and storage clients such as VMs running on host systems in the cluster. For example the VSAN layer allows the VMs to access the logical storage pool during VM runtime in order to store and retrieve persistent VM data e.g. virtual disk data .

The qualifier object based in hypervisor converged object based storage refers to the manner in which VMs are maintained within HC OB storage in particular the state of each VM is organized as a hierarchical collection of distinct storage objects or simply objects . For example the files that hold the metadata configuration of a VM may reside in a file system that is created within a namespace object also known as a file system object the virtual disks of the VM may reside in virtual disk objects and so on. Each of these storage objects may be composed of multiple component objects. The VSAN layer provisions manages and monitors each of these storage objects individually. For instance in order to meet a particular storage policy for a particular virtual disk VMDK1 the VSAN layer may determine that the component storage objects that make up the virtual disk object corresponding to VMDK1 should be striped across the locally attached storage of three different host systems. Through these and other mechanisms HC OB storage can provide improved ease of management scalability and resource utilization over traditional storage solutions. One exemplary implementation of an HC OB storage system is described in commonly assigned U.S. patent application Ser. No. 14 010 293 filed Aug. 26 2013 entitled Scalable Distributed Storage Architecture. 

Unlike non object based storage systems the state of a VM is not contained within a larger coarse storage container e.g. a LUN . Having such storage containers provide a couple of benefits. First a coarse storage container provides a convenient location to store information common to all VMs that use the container. For example it is possible to create a file system on top of a LUN create a directory within the file system for each VM whose state is stored on the underlying storage device s and then create a directory at the root to store shared information. Second for a given class of failures one can reason about the availability accessibility of all of the VM data stored within a storage container by reasoning about the availability accessibility of the container itself. For instance one can determine whether a network failure impacts the accessibility of the VM data by determining if the container is accessible. As a result there is no need to track the accessibility of each individual VM stored in a single storage container instead it is sufficient to track the accessibility of the container itself.

The lack of coarse storage containers raises unique challenges when attempting to use HC OB storage and HA concurrently in the same virtualized compute environment. As one example existing HA implementations typically maintain information known as HA protection state that identifies the VMs in a host cluster that should be failed over restarted in the event of a failure. The master HA module in the cluster i.e. the HA module that is responsible for detecting failures and orchestrating VM failovers restarts manages this HA protection state by persisting it to a centralized file or set of files on the storage tier. If there is an outage that affects a subset of host systems in the cluster one or more new master HA modules may be elected. Each newly elected master HA module may then retrieve the file from the storage tier to determine which VMs are HA protected. This approach works well if the storage tier is implemented using dedicated shared storage since the HA protection file can be placed in the storage container storing the configurations for the protected VMs. On the other hand if the storage tier is implemented using HC OB storage there is no convenient location to store such information that is shared across VMs.

As another example in existing HA implementations when a master HA module detects a failure that requires one or more VMs to be failed over restarted the master HA module executes a conventional failover workflow that involves 1 identifying active host systems for placing the VMs that can meet the VMs resource needs and 2 initiating VM restarts on the identified host systems. If the VMs are stored on dedicated shared storage these two steps are generally sufficient for successfully completing the failover. However if the VMs are stored on HC OB storage there may be cases where a VM cannot be restarted because one or more of its storage objects are not yet accessible to the host system executing on the master HA module and or to the host system on which the restart is being attempted . This situation cannot be uncovered using conventional coarse grained storage accessibility checks. This in turn can cause the conventional failover workflow to break down or result in multiple continuous restart attempts which can increase the load on the affected host systems.

As yet another example there are certain types of network partitions that can further complicate the HA protection state persistence and VM failover restart workflows noted above. As one example if there is a failure that causes the VSAN modules to observe a partition while the HA modules do not there may be instances where the host system on which the master HA module is running does not have access visibility to a particular VM and thus cannot update retrieve HA protection state information for the VM or determine its accessibility for failover purposes while the host systems of other slave HA modules do have such access visibility.

Accordingly it would be desirable to have techniques for integrating HA with distributed object based storage systems like HC OB storage that overcome these and other similar issues.

The present disclosure describes techniques that allow high availability HA to be provided for VMs that are stored on distributed object based storage. In one set of embodiments the techniques can include persisting by a master HA module HA protection state for VMs in a host cluster on a per VM basis rather than in a centralized file in the logical storage pool of the distributed object based storage system. For example in a particular embodiment the master HA module can persist HA protection state for a given VM as metadata for the VM s namespace object. With this approach the master HA module can avoid scenarios where it can access the VM itself but cannot access the VM s protection state or vice versa . This approach also provides performance benefits when a newly elected master HA module needs to retrieve the persisted protection state information.

In another set of embodiments the techniques can include modifying the conventional failover restart workflow performed by the master HA module to account for potential VM inaccessibility at the time of failover. With these modifications the master HA module can more gracefully deal with situations where a VM has been identified as a failover candidate but some critical subset of the VM s storage objects remains unavailable.

In yet another set of embodiments the techniques can include enhancements to both the HA protection state persistence and restart failover workflows mentioned above so that they can operate more effectively in specific network partition scenarios.

In the sections that follow numerous examples and details are set forth in order to provide a thorough understanding of various embodiments. It should be appreciated however that certain embodiments can be practiced without some of these details or can be practiced with modifications or equivalents thereof. For instance although examples are provided for integrating HA with HC OB storage in particular the techniques of the present disclosure may also be used to integrate HA functionality with other types of storage systems that exhibit characteristics similar to HC OB systems e.g. distributed object based storage systems that expose multiple distinct ingress access points to storage clients . Further although certain embodiments are discussed in the context of a HA system that relies on a master slave model comprising at least one master HA module and multiple slave HA modules the present techniques are applicable to any distributed high availability system designed to monitor the liveliness of VMs and failover such VMs in the case of failures regardless of the manner in which the HA modules interoperate e.g. master slave peer to peer etc. . For example in a peer to peer HA implementation the functions attributed to the master HA module in the sections below may be performed by one or more peer HA modules. One of ordinary skill in the art will recognize many variations modifications and alternatives.

For purposes of this disclosure a VM is said to be stored on distributed object based e.g. HC OB storage if the VM s configuration e.g. vmx file is maintained within the storage system s logical storage pool sometimes referred to herein as the object store . Other state information for the VM e.g. virtual disk data may also be maintained within the HC OB storage pool or may be stored on traditional shared storage.

Further a VM is said to be accessible by to a host system or a module thereof such as a HA module running on the host system if all of its storage objects can be accessed by the host system. The VM is inaccessible if these requirements are not met. Alternatively in some embodiments a VM may be considered accessible if some user defined subset of required storage objects can be accessed by the host system. One exemplary object structure for a VM which can include e.g. a namespace object one or more virtual disk objects and so on is described in Section 1.1 below.

To provide context for the techniques described herein depicts a virtualized compute environment that implements both HC OB storage and HA according to an embodiment. As shown environment includes a host cluster comprising host systems M a virtualization management platform and a conceptual HC OB storage component .

Host cluster and constituent host systems M provide virtualized compute resources within environment . For example each host system M includes a virtualization layer or hypervisor M through which the host system can execute one or more VMs M . As part of its responsibilities each hypervisor can manage the physical hardware M of its respective host system e.g. CPU memory etc. to make these hardware resources available in appropriate amounts to running VMs.

Virtualization management platform is an entity that allows users to perform administrative and management tasks with respect to host cluster such as configuring and managing the lifecycles of VMs M running on host systems M . Virtualization management platform can also interact with host systems M to enable facilitate certain inter host features such as VM migrations distributed resource scheduling and so on.

HC OB storage is a conceptual representation of a distributed software based storage tier that provides storage services to host cluster . For instance HC OB storage can be used to store the persistent state of VMs M running on host systems M . As shown in HC OB storage comprises a logical storage pool i.e. object store that is an aggregation of local storage resources M that are locally attached i.e. housed in or attached via a point to point link to host systems M . These local storage resources may include e.g. commodity SSDs commodity spinning hard disks and other types of non volatile memory. Since object store of HC OB storage is composed entirely of aggregated host side storage there is no physical distinction between the storage tier of environment represented by HC OB storage and the compute tier of environment represented by host cluster . They are from a physical perspective one and the same.

Per its designation as an object based storage component HC OB storage stores data such as VM state in the form of distinct storage objects. As noted in the Background section these storage objects can be provisioned managed and monitored on an individual basis. Thus unlike non object based storage systems that typically group together the states for multiple VMs into a coarse storage container e.g. a LUN or file system container HC OB storage can provide granular performance isolation per VM or per virtual disk quality of service and other features that are beneficial in virtualized compute environments like environment .

To manage and provide access to HC OB storage hypervisors M of host systems M include a software based storage management i.e. VSAN layer comprising VSAN modules M . As shown VSAN modules M are interconnected via a management network . Although a comprehensive discussion of the operation of VSAN modules M is beyond the scope of this disclosure VSAN modules M are responsible for among other things enabling interactions between the storage tier represented by HC OB storage and storage clients or other external entities. For example in one set of embodiments VSAN modules M can automate the creation of storage objects in object store for new VMs and or virtual disks that are provisioned by e.g. administrators via virtualized management platform . In another set of embodiments VSAN modules M can process VM I O requests that are directed to e.g. virtual disks swap space etc. on object store such that those I O requests are properly routed to the host systems in cluster that house the local storage resources backing the storage objects associated with the requested virtual disk data. In yet another set of embodiments VSAN modules M can expose APIs that allow other entities modules to e.g. modify storage object metadata query information regarding the status or accessibility of certain storage objects or groups of objects and more.

To clarify how VSAN modules M may create a storage object representation of a newly provisioned VM in object store depicts an example object structure for a VM running on host system according to an embodiment. As shown object structure includes a top level namespace object that is one of multiple namespace objects in object store . Namespace object corresponds to a representation of a file system such as VMFS NFS etc. that is used to store the files of VM that represent part of the VM s state. In the specific embodiment of namespace object includes a configuration file for VM identified by the path vsan fs1 vm1.vmx and a virtual disk descriptor file for a virtual disk used by VM identified by the path vsan fs1 disk1.vmdk .

The virtual disk descriptor file within namespace object includes in turn a pointer to a composite virtual disk object distinct from namespace object that conceptually represents virtual disk . Object includes metadata that describes a storage organization or configuration for virtual disk referred to as an object blueprint . In one embodiment VSAN modules M may create this object blueprint at the time virtual disk is provisioned based on one or more storage profiles for the virtual disk that are specified by e.g. an administrator. Object also includes references to a number of component objects . Component objects hold the actual data for virtual disk in accordance with the storage configuration identified in object blueprint . For example in component objects correspond to data stripes in a RAID 1 RAID 0 configuration per object blueprint .

Finally as shown via the lines interconnecting component objects with host systems and these component objects map to specific storage locations on the local storage resources of host systems respectively that back the data stripes represented by the objects. Like object blueprint VSAN modules M may determine these mappings in a way that satisfies one more storage profiles defined for virtual disk .

It should be appreciated that object structure of is illustrative and not intended to limit the embodiments described herein. Generally speaking object structure may include objects corresponding to any type of storage entity that VM may consume use such as swap objects memory checkpoint objects file shares which are themselves contained within objects and so on. One of ordinary skill in the art will recognize many variations modifications and alternatives.

As part of the process of creating object structure within object store and at potentially other times VSAN modules M can cache in an in memory database that is synchronized and replicated on every host system detailed information regarding object structure such as what composite and component objects are included in the structure the relationships between objects metadata associated with each object and the mapping of objects to physical storage locations. In this way each VSAN module M can have this information readily available for e.g. recovery or I O request processing purposes without needing to access object store . VSAN modules M may also use in memory database to cache other types of information that are relevant to their management of HC OB storage such a physical inventory of the local storage resources in host cluster the performance characteristics of each local storage resource quality of service requirements cluster topology health and more.

In addition to VSAN modules M each hypervisor M of host cluster includes a HA module M . In the embodiment of HA modules M are communicatively coupled via a management network that is separate from management network interconnecting VSAN modules M . However in alternative embodiments HA modules M and VSAN modules M may share the same management network.

Collectively HA modules M allow for high availability of VMs M running within host cluster . For example according to a typical HA implementation when HA is first turned on via e.g. virtualization management platform HA modules M can communicate with each other to elect a master HA module. The master HA module is generally responsible for e.g. monitoring the health of hosts and VMs in the cluster orchestrating VM failovers restarts in case of a failure reporting cluster state and failover actions to virtualization management platform and managing HA protection state. This last function involves managing information regarding which VMs in host cluster should be protected for HA purposes. In existing HA implementations the master HA module typically persists this HA protection state information to a centralized file on the storage tier discussed in greater detail in Section 2 below .

The remaining non master modules can configure themselves to act as slave HA modules. Each slave HA module is generally responsible for e.g. monitoring and forwarding local host VM state changes to the master HA module locally restarting VMs on its host system when directed by the master HA module and participating in master election.

When the master HA module detects a host VM network outage by e.g. determining that it is no longer in communication with one or more slave HA modules the master HA module can execute a failover restart workflow for automatically restarting the VMs that are rendered inoperable by the failure. At a high level this workflow can include identifying the VMs to be restarted placing those VMs on active host systems in cluster that have available capacity and then transmitting commands to the slave HA modules on the respective host systems to initiate the restart process for each VM. In this manner the master HA module together with the slave HA modules can ensure that the guest applications running within the affected VMs remain operational throughout the outage.

In addition whenever one or more of HA modules M are no longer in contact with a master HA module those modules can perform a re election to elect a new master. This may occur if e.g. a failure in management network causes some subset of HA modules M to become partitioned from another subset including the current master HA module or if the host system running the master HA module fails or becomes network isolated. In these situations the cluster may comprise multiple master HA modules one master per partition . This will remain the case until the failure s causing the network partition s are resolved at which point all but one master HA module will abdicate.

For purposes of illustration depicts an exemplary HA module comprising various functional components according to an embodiment. These functional components generally map to the host VM monitoring failover orchestration HA protection state management local restart and master election functions attributed to HA modules M in the description above. Depending on the HA implementation HA module can also include other components or sub components that are not specifically shown. or exclude one or more of the existing components. For instance some HA implementations may not use a master slave model and instead implement group messaging among HA modules in virtual synchrony. In these and similar embodiments there would be no need for e.g. master election component . One of ordinary skill in the art will recognize many variations modifications and alternatives.

As mentioned previously in existing HA implementations the master HA module of a host cluster generally saves HA protection state for the VMs running within the cluster to one or more centralized files on persistent storage. This arises out of the need for newly elected master HA modules to determine at the time they are elected which VMs should be protected for HA purposes. Without persisting this information in some way the newly elected masters have no way of knowing which VMs they should monitor and failover restart in the case of a failure since the original master HA module that previously maintained this information may have failed or become network isolated or partitioned .

By way of example depicts an exemplary flowchart that may be performed by a master HA module for persisting HA protection state to a centralized file according to an existing HA implementation. In particular illustrates HA protection state processing that occurs when a VM is first powered in the master HA module s host cluster.

At block a user issues a power on command for a given VM. At block the host system on which the VM is registered performs a power on operation and reports the powered on status of the VM to the virtualization management platform for the cluster. In response the virtualization management platform informs the master HA module that the VM should be HA protected block .

At block the protection state management component of the master HA module stores the HA protection state for the VM in memory so that it can be quickly retrieved by the master HA module on demand . Finally at block the protection state management component writes the HA protection state for the VM to a centralized protection file on the storage tier of the cluster so that it can be retrieved by other newly elected master HA modules in the case that the current master goes down or becomes unavailable .

While the approach for persisting HA protection state shown in works well in environments where the storage tier is implemented using dedicated shared storage e.g. a SAN or NAS based storage array it can be problematic for environments where the storage tier is implemented using distributed object based storage like HC OB storage of . This is due to the varying and potentially asymmetric nature of object accessibility in such distributed storage systems if the HA protection state is maintained in a centralized file in HC OB storage which in turn would be persisted as one or more storage objects across the HC OB object store there may be cases where a master HA module cannot access the file even if the master HA module can access the VMs themselves. For example assume that the centralized HA protection file is stored as two storage objects across hosts H and H of a host cluster comprising hosts H H H H and H. If a failure causes H and H to be partitioned from H H and H a master HA module in the H H H partition will not be able to restart any VMs because it will not have access to the HA protection file in the H H partition. This would remain true even if all of the VMs in the cluster were maintained within and thus accessible from the H H H partition. Further after a network partition is resolved merging updates to the HA protection file that are performed during the partition can be complicated or may not be possible at all .

To address the foregoing and other similar issues in certain embodiments each master HA module can persist the HA protection state for a VM within one or more storage objects of the VM on the HC OB storage tier rather than in a centralized file. Stated another way each master HA module can co locate the HA protection state for a VM with the persistent configuration of that specific VM. With this approach if a host master HA module can access the VM s configuration information on HC OB storage the host master HA module can also access the VM s protection state information and thus the master HA module can make an attempt to failover restart the VM as needed . This also ensures that when a VM is powered on the master HA module in the cluster partition where the VM is registered will always be able to persist the HA protection state for the VM.

There are a number of different ways in which HA protection state can be co located with VM state in HC OB storage e.g. as a distinct file the VM s file system as object metadata etc. . According to a particular embodiment each master HA module can store the HA protection state for a given VM as part of the metadata of the VM s namespace object in the HC OB object store. This embodiment can be advantageous in certain scenarios. For example recall that each VSAN module caches in a synchronized in memory database i.e. database of information about the storage objects in the object store. As part of this process in some embodiments each VSAN module can automatically cache the metadata for each VM namespace object and potentially other storage objects in the in memory database. Thus in these embodiments when a newly elected master HA module needs to retrieve the HA protection state for all VMs in a cluster partition e.g. following a failure or at other times the newly elected HA module can query the VSAN layer which in turn can quickly retrieve the HA protection state from the in memory database rather than from HC OB storage . This can significantly speed up the protection state retrieval process which is important since any lag between the time a master HA module is elected and the time at which it knows which VMs to protect can potentially result in unhandled failures. If the HA protection state for each VM were stored as a separate file within each VM s file system the VSAN layer or some other component would need to discover all of the file locations enumerate the files and then open read close each file which can be a very time consuming process.

At block master HA module X can receive a list of one or more VMs to be HA protected from e.g. virtualization management platform . This may occur when one or more VMs are powered on as in flowchart of or at other times. In one embodiment the list of VMs identifies the VMs as well as the host systems on which the VMs are registered for execution.

At block master HA module X can enter a loop for each VM. Within the loop protection state management component of master HA module X can first store HA protection state for the VM in memory. The HA protection state can be embodied in various ways such as a protected flag or some other value.

Protection state management component can then invoke e.g. a SET API exposed by the VSAN layer for persisting the VM s HA protection state as metadata for the VM s namespace object in HC OB object store block . In a particular embodiment this API can be a generic metadata SET API that can be used to update any metadata field of the namespace object. In this embodiment the generic SET API can take as input a key value pair that identifies the metadata field to be populated and the corresponding value. The generic SET API can also optionally take as input a service identifier that identifies the service component or layer that is performing the metadata update in this case HA . In other embodiments the SET API can be specifically tailored to set HA protection state within the namespace object.

Upon receiving the invocation of the SET API the VSAN layer in particular the VSAN module of the host system on which the master HA module is located in concert with potentially other VSAN modules on other host systems can cause the HA protection state for the VM to be added to the VM s namespace object metadata in object store block . As noted above the HA protection state can be stored as e.g. a protected flag or some similar indicator value. The VSAN layer can subsequently cache the HA protection state in in memory database and return a completion message to master HA module X . In response master HA module X can reach the end of the VM loop block and iterate though blocks until all of the VMs have been processed.

It should be noted that although flowchart and certain other subsequent flowcharts illustrate VMs as being processed serially in a loop in alternative embodiments these steps can be carried out on multiple VMs at the same time e.g. via batching . Further in cases where the host system on which master HA module X is running cannot access HC OB storage master HA module X can cause the SET API described above to be executed via a slave HA module. For example master HA module X can send a message to the slave HA module to invoke the API or can directly invoke the API via a remote procedure call. This embodiment is described in further detail in Section 4.2 below.

At block protection state management component of master HA module Y can invoke a VSAN GET API for retrieving the HA protection state for the VMs in host cluster from object store . Like the SET API discussed with respect to block of this GET API can be a generic API that can be used to retrieve the value s of any metadata field of any namespace object or group of namespace objects or a non generic API that is specifically designed for retrieving HA protection state.

At block the VSAN layer can receive the API invocation and can consult in memory database to retrieve and return the requested HA protection state to master HA module Y . This step can comprise e.g. accessing the cached namespace object metadata for each VM in database . As mentioned previously since the VSAN layer does not need to hit physical storage in order to perform this retrieval operation master HA module Y can more quickly determine which VMs it needs to protect when compared to other persistence techniques.

Finally at block protection state management component of master HA module Y can determine the currently known set of HA protected VMs based on the information returned by the VSAN layer at block .

In certain embodiments the API invocation at block of flowchart will only return the HA protection state information that the VSAN layer can access from HC OB storage at that time. Due to failures it is possible some HA protection state information is not accessible. To account for this situation in one embodiment not shown master HA module Y can re invoke the GET API after the VSAN layer informs the master HA module of a change in accessibility of any storage objects. This re invocation may cause the master HA module to determine that additional VMs are in fact HA protected.

It should be appreciated that the embodiments described above are illustrative and that other approaches are also possible for persisting managing HA protection state for VMs stored on HC OB storage. For example according to one alternative approach the VSAN layer itself may be modified to support the storage of a file that can be concurrently accessed by host systems in different network partitions. With this approach HA protection state can remain within a centralized file as in current HA implementations since all master HA modules would be able to read it regardless of how the cluster is partitioned . One implementation of this approach may for simplicity allow only one master HA module to write to the centralized file at a time. This write enabled master may be selected via e.g. a third party witness. Another implementation may allow multiple concurrent writers with the host system of each master HA module owning a segment of the file for write purposes.

In addition to protection state management another aspect of HA that is affected by the use of a HC OB storage system for VM data storage is the VM restart failover workflow orchestrated by master HA modules in the event of a failure. As discussed in Section 1.2 above a conventional version of this workflow comprises 1 identifying the VMs to be restarted 2 placing those VMs on active host systems that have available capacity and which can access the VM s devices including storage and 3 transmitting commands to the slave HA modules on the respective host systems to initiate the restart process for each VM. Unfortunately this conventional version fails to take into account the fact that when a VM is stored on HC OB storage the accessibility of the VM s objects cannot be determined via a storage container check and hence there may be cases where the VM cannot be restarted because one or more of its storage objects are not yet accessible to the host system executing on the master HA module and or to the host system on which the restart is being attempted . This in turn can result in multiple continuous VM restart attempts which can increase the load on the affected host systems and can delay the restart of other VMs.

To address this depict a modified HA failover restart workflow comprising flowcharts and respectively that can be performed by a master HA module e.g. X of according to an embodiment. At a high level this modified workflow can check whether a given VM is accessible by the host system on which the master HA module is located before the master HA module attempts to restart it thereby reducing the total number of unsuccessful VM restart attempts and thus the load on the system .

Starting with flowchart of at block host VM monitoring component of master HA module X can detect a failure within host cluster that necessitates the failover of one or more VMs. At block failover orchestration component of master HA module X can identify the VM s to be failed over restarted in response to the failure. For example in one embodiment failover orchestration component can identify these VMs based on the HA protection state information discussed with respect to Section 2 above.

At block failover orchestration component can enter a loop for each VM identified at block . Within the loop failover orchestration component can first determine whether the VM is a VSAN VM i.e. a VM whose state is stored on HC OB storage block . If not failover orchestration component can add the VM to a restart list of VMs to be immediately placed and restarted block . As discussed with respect to flowchart of below another thread process of failover orchestration component can asynchronously process this restart list to place and restart the listed VMs. The VM loop can then end block and failover orchestration component can return to block to iterate through the loop again if needed to handle additional VMs.

If failover orchestration component determines that the VM is not a VSAN VM at block component can move on to checking whether the VM is accessible block . For example in one embodiment block can comprise determining whether all of the VM s storage objects can be accessed by the host system of master HA module X from HC OB storage . In a particular embodiment this check can be carried out by invoking an appropriate query API exposed by the VSAN layer. If the VM is accessible failover orchestration component can add the VM to the restart list as described above block .

On the other hand if the VM is inaccessible failover orchestration component can add the VM to a wait list of VMs waiting for an accessibility change block . As discussed with respect to flowchart of below another thread process of failover orchestration component can asynchronously process this wait list to wait for the accessibility of the VMs on the list to change before restarts for those VMs are attempted. The VM loop can then end block and failover orchestration component can return to block to iterate through the loop again if needed to handle additional VMs.

Turning now to flowchart depicts a sub flow that can be performed by an independent thread process of failover orchestration component of master HA module X for processing the restart list noted at block of according to an embodiment. In the embodiment of flowchart is performed in the context of a particular VM on the restart list and thus should be repeated for all other VMs on the list .

At block failover orchestration component can first attempt to find a place for restarting the current VM. In other words failover orchestration component can attempt to find an active host system within host cluster that is suitable for hosting execution of the VM. In one embodiment this placement step may take into account the available compute e.g. CPU memory etc. capacity of each active host system and or other factors.

If failover orchestration component cannot find a place for the VM block component can wait for a host capacity change block and can subsequently return to block .

If a place for the VM can be found at block failover orchestration component can initiate the VM restart process on the identified host system not shown . Failover orchestration component can then check whether the restart was successful block . If so flowchart and the overall restart workflow for this VM can end.

If the restart was unsuccessful failover orchestration component can check whether the restart failed due to insufficient capacity at the selected host system block . If so component can wait for a host capacity change as noted above block and can return to block .

However if the restart did not fail due to insufficient host capacity failover orchestration component can perform a series of steps to determine whether the failed restart was caused by a VM object accessibility issue. In particular at block failover orchestration component can check whether the VM is a VSAN VM. If not failover orchestration component can conclude that the failed restart was due to some transient issue on the target host system and thus can wait for a delay period before re attempting the restart process block .

On the other hand if the VM is VSAN VM failover orchestration component can check whether the restart failed because the VM was inaccessible and whether the VM is still currently inaccessible. In one embodiment component can perform this check by e.g. invoking the same query API used at block of block . This second accessibility check is useful because in certain embodiments the accessibility of the VM may change between the first check performed at block and the actual restart of the VM at block .

If the VM is accessible at block failover orchestration component can wait for a delay period block and subsequently return to block . Otherwise failover orchestration component can remove the VM from the restart list and add it to the wait list blocks and . In this way failover orchestration component can put off any further restart attempts for this VM until its stored objects are accessible.

Turning now to flowchart depicts a sub flow that can be performed by an independent thread process of failover orchestration component of master HA module X for processing the wait list noted at block of and block of according to an embodiment. In the embodiment of flowchart is performed in the context of a particular VM on the wait list and thus should be repeated for all other VMs on the list .

At block failover orchestration component can check whether the accessibility of the current VM has changed. Such a change may occur due to e.g. a resolution event that causes the VM and potentially other VMs in the cluster to become available again. In one set of embodiments failover orchestration component can perform this check by waiting for a notification from the VSAN layer that the VM s accessibility has changed. In these embodiments failover orchestration component can register for the notification by e.g. registering for a callback exposed by the VSAN layer.

It should be noted that in some cases where a resolution event occurs the VSAN layer may not become aware that the storage objects of all VMs affected by the event have become accessible at exactly the same moment. Rather it may take some time for the VSAN layer to determine the accessibility of VM the accessibility of VM and so on. Accordingly in certain embodiments the VSAN layer may be configured to issue multiple accessibility changed notifications in response to a resolution event. For instance each successive notification can report the VMs whose storage objects have become accessible since the last notification. The notifications can be issued on a fixed time interval e.g. one every minute or as objects VMs become accessible. With this approach the average time needed to restart VSAN VMs can be reduced.

If failover orchestration component determines that the current VM is not yet accessible at block component can continue to wait for an accessibility change block and return to block .

However if the VM becomes accessible at block failover orchestration component can remove the VM from the wait list and add it to the restart list blocks and . As a result component can subsequently place and restart the VM whose stored state is now fully accessible per the processing of flowchart of .

It should be appreciated that the embodiments described above for modifying the conventional HA failover restart workflow are illustrative and numerous variations and alternatives are possible. For example in some embodiments as part of the accessibility changed notification step described at block of the VSAN layer may not be able to notify master HA module X when a particular VM has become accessible rather the VSAN layer may only be able to notify master HA module X when one or more storage objects of any VM in the cluster have become accessible. To account for this when failover orchestration component of master HA module X receives the notification component can re invoke the accessibility query API for each VM in the wait list in order to identify the specific VM whose storage object s have become accessible.

In addition in some embodiments the accessibility query API exposed by the VSAN layer may not be able to return a definite answer as to whether a VM is accessible instead the API may only be able to indicate whether a VM s namespace object is accessible. In these scenarios the same failover restart workflow shown in may be used but master HA module X may need to attempt more VM restarts since the restart for a given VM will fail if a full copy of the VM s persistent data such as its virtual disks is not accessible .

When a failure splits the host systems in a host cluster into two or more network partitions the HA and VSAN modules in the cluster may or may not observe the same partitions. For example if the HA and VSAN modules share the same management network they will generally observe the same partitions since a failure in the management network will split both groups of modules in the same manner . However if the HA and VSAN modules do not share the same management network as in environment of there may be cases where a network failure splits one group of modules but not the other or splits the two groups of modules in non identical ways. There are generally four partition scenarios to consider 

Some of the scenarios above may require enhancements to the HA protection state persistence and HA failover restart techniques described in Sections 2 and 3 so that those techniques can operate efficiently. These enhancements are detailed in the sub sections that follow.

In this scenario the HA modules observe a partition but the VSAN modules do not. With respect to environment of this may occur if e.g. there is a failure in management network interconnecting HA modules M but there is no failure in management network interconnecting VSAN modules M .

If scenario 1 occurs the group of HA modules in each partition will elect a master HA module such that there are multiple master HA modules one per partition . The master HA module in each partition will be able to read and write the HA protection state for all VMs in the host cluster because the VSAN layer has full visibility of the entire HC OB object store. Accordingly no changes are needed for the HA protection state persistence retrieval flows described in Section 2 to accommodate this scenario.

In terms of HA failover restart each master HA module will in parallel attempt to restart each VM within the master module s partition. In certain embodiments no mechanism may be provided to prevent concurrent VM restart attempts by different master HA modules in different partitions. Nevertheless generally speaking only one instance of each VM will actually power on because VM power on requires in one embodiment an exclusive lock to be held on one of the VM s files and thus only one master HA module will obtain this lock and successfully power on the VM . This means that changes are also not needed for the failover restart workflow described in Section 3 to accommodate this scenario.

In this scenario the VSAN modules observe a partition but the HA modules do not. One example of this scenario is illustrated in which depicts a host cluster of six host systems in which there is a failure in management network interconnecting VSAN modules that splits the cluster into two partitions and but there is no failure in management network interconnecting HA modules . Thus master HA module on host system identified by the M designation is able to communicate with slave HA modules on host systems while the VSAN modules can only communicate with the other VSAN modules in their particular partition.

A. When a master HA module needs to update the metadata for a VM namespace object e.g. in order to persist modify the VM s HA protection state the master HA module s host system may not be able to access the namespace object while the host systems of one or more slave HA modules can. This is because the VM namespace object will only be accessible in the VSAN partition where there is a quorum of storage objects for that VM. For instance in master HA module on host system cannot access the namespace object of VM since the majority of VM s objects are in the partition while slave HA modules on host systems can.

B. When a master HA module attempts to restart a VM the master HA module may not know which specific host systems if any have access to the VM for placement purposes. For instance in if host system which is running VM fails master HA module on host system does not know that VM may only be placed restarted on host systems or since only those host systems can access VM s objects.

C. When the accessibility of a VM changes this change may not be visible to the host system on which the master HA module is running. For instance in if host system fails and then recovers master HA module on host system should be made aware that the accessibility of VM has changed. But since VSAN module is partitioned from the VSAN module master HA module cannot be notified.

D. When a master HA module is newly elected some VMs may be inaccessible to the host system on which the master is running and thus the master HA module may not know that they need to be restarted after a failure.

For issues A and B above at least three solutions are possible 1 the master HA module queries each slave HA module to learn whether that slave HA module can access a particular VM 2 the VSAN module or some other component on each host system publishes the set of VMs that are accessible from that host and this set is forwarded to the master HA module by each slave HA module and 3 the master HA module uses trial and error when attempting to update the metadata of a VM or when trying to restart it.

For issue C above the master HA module can inform the slave HA modules of the VMs that the master is interested in tracking for accessibility change purposes. The slave HA modules can then report back to the master when a given VM becomes accessible to the respective host systems of the slaves.

For issue D above the master HA module can periodically ask each slave HA module to call the GET API to retrieve the HA protection state information for accessible VMs. The master HA module can subsequently compare the information received from the slave HA modules with what it had previously obtained. Based on this comparison if there are any new VMs that are not currently running the master HA module can attempt to restart those VMs.

Starting with block protection state management component of master HA module can attempt to invoke the VSAN SET API on host system for persisting the protection state of a VM e.g. VM in HC OB object store . If this invocation completes successfully at block meaning that host system has access to the VM s namespace object flowchart can end block .

On the other hand if the invocation of the VSAN SET API does not complete successfully at block protection state management component can identify a subset of slave HA modules and can issue a VM accessibility query to the subset in parallel blocks and . This subset could be determined for example by querying the VSAN layer for the hosts that the current host can access over the management network and then excluding these from the subset queried in block . This VM accessibility query can effectively ask the target HA module whether the host system on which it is running has access to the VM.

If a particular slave HA module in the subset sends a response message indicating that its host system can access the VM block protection state management component of master HA module can transmit a command to that slave HA module with instructions to invoke the VSAN SET API from its respective host system block . If this remote invocation is successful block flowchart can end.

However if no slave HA module in the subset indicates that its host system can access the VM at block or if the remote invocation at block is unsuccessful protection state management component of master HA module can check whether all of the slave HA modules have been queried. If not component can select a new subset at block and repeat the subsequent steps of flowchart .

Finally if protection state component determines that all slave HA modules have been queried at block component can add the VM to the wait for accessibility change list described with respect to block and can subsequently return to block .

At block failover orchestration component of master HA module can transmit a list of VMs to be placed and restarted to slave HA modules for the purpose of inquiring about the accessibility of the VMs from their respective host systems.

At block failover orchestration component can receive the requested accessibility information from the slave HA modules. Failover orchestration component can then match the VMs to certain host systems based on the received accessibility information block .

At block failover orchestration component of master HA module can notify slave HA modules of the wait list of VMs on which master HA module is waiting for an accessibility change notification.

In response each slave HA module can query via the accessibility query API discussed with respect to its respective VSAN module for accessibility of the VMs on the list block .

Finally at block each slave HA module can report back to master HA module when a VM on the list has become accessible or is already accessible .

It should be appreciated that are illustrative and various modifications alternatives are possible. For example in the master HA module may not need to attempt to execute the VSAN SET API on the host of each slave HA module. Rather when an attempt fails on a host X the VSAN module of host X can return the IDs of the hosts that the VSAN module can communicate with over VSAN network . The master HA module can then use this information to make only one attempt per partition rather than one attempt per host.

In this scenario the HA and VSAN modules observe identical partitions. With respect to environment of this may occur if e.g. there are failures at identical points in management networks and respectively. Alternatively this scenario may occur if the HA and VSAN modules share the same management network and there is a failure on that shared network.

Scenario 3 is similar to scenario 1 in that the group of HA modules in each partition will elect a master HA module such that there are multiple master HA modules one per partition . The master HA modules in each partition will only be able to read and write the HA protection state for the VMs that are accessible from within the master s partition however each VM should be accessible to one master HA module. Accordingly no changes are needed for the HA protection state persistence retrieval flows described in Section 2 to accommodate this scenario.

In terms of HA failover restart each master HA module will attempt to restart its accessible VMs within its partition. To accomplish this the master HA modules can follow the failover restart workflow described in Section 3 without any specific changes or enhancements.

In this scenario the HA and VSAN modules observe non identical partitions. An example of this scenario is illustrated in which depicts a host cluster of six host systems in which there is a first failure in management network interconnecting VSAN modules that splits the cluster into two partitions and and there is a second failure in management network interconnecting HA modules that splits the cluster into two different partitions and .

If scenario 4 occurs a master HA module that knows whether a given VM is protected may not know whether the VM is accessible within the master s partition. Conversely a master HA module that knows whether a VM is accessible within its partition may not know whether the VM is protected. To process HA protection state updates and restart failed VMs quickly in this scenario the master HA module that is responding to the protection request or knows a VM is protected should retry such operations repeatedly until they succeed.

The embodiments described herein can employ various computer implemented operations involving data stored in computer systems. For example these operations can require physical manipulation of physical quantities usually though not necessarily these quantities take the form of electrical or magnetic signals where they or representations of them are capable of being stored transferred combined compared or otherwise manipulated. Such manipulations are often referred to in terms such as producing identifying determining comparing etc. Any operations described herein that form part of one or more embodiments can be useful machine operations.

Further one or more embodiments can relate to a device or an apparatus for performing the foregoing operations. The apparatus can be specially constructed for specific required purposes or it can be a general purpose computer system selectively activated or configured by program code stored in the computer system. In particular various general purpose machines may be used with computer programs written in accordance with the teachings herein or it may be more convenient to construct a more specialized apparatus to perform the required operations. The various embodiments described herein can be practiced with other computer system configurations including handheld devices microprocessor systems microprocessor based or programmable consumer electronics minicomputers mainframe computers and the like.

Yet further one or more embodiments can be implemented as one or more computer programs or as one or more computer program modules embodied in one or more non transitory computer readable storage media. The term non transitory computer readable storage medium refers to any data storage device that can store data which can thereafter be input to a computer system. The non transitory computer readable media may be based on any existing or subsequently developed technology for embodying computer programs in a manner that enables them to be read by a computer system. Examples of non transitory computer readable media include a hard drive network attached storage NAS read only memory random access memory e.g. a flash memory device a CD Compact Disc e.g. CD ROM CD R CD RW etc. a DVD Digital Versatile Disc a magnetic tape and other optical and non optical data storage devices. The non transitory computer readable media can also be distributed over a network coupled computer system so that the computer readable code is stored and executed in a distributed fashion.

In addition while described virtualization methods have generally assumed that virtual machines present interfaces consistent with a particular hardware system persons of ordinary skill in the art will recognize that the methods described can be used in conjunction with virtualizations that do not correspond directly to any particular hardware system. Virtualization systems in accordance with the various embodiments implemented as hosted embodiments non hosted embodiments or as embodiments that tend to blur distinctions between the two are all envisioned. Furthermore certain virtualization operations can be wholly or partially implemented in hardware.

Many variations modifications additions and improvements are possible regardless the degree of virtualization. The virtualization software can therefore include components of a host console or guest operating system that performs virtualization functions. Plural instances can be provided for components operations or structures described herein as a single instance. Finally boundaries between various components operations and data stores are somewhat arbitrary and particular operations are illustrated in the context of specific illustrative configurations. Other allocations of functionality are envisioned and may fall within the scope of the invention s . In general structures and functionality presented as separate components in exemplary configurations can be implemented as a combined structure or component. Similarly structures and functionality presented as a single component can be implemented as separate components.

As used in the description herein and throughout the claims that follow a an and the includes plural references unless the context clearly dictates otherwise. Also as used in the description herein and throughout the claims that follow the meaning of in includes in and on unless the context clearly dictates otherwise.

The above description illustrates various embodiments along with examples of how aspects of particular embodiments may be implemented. These examples and embodiments should not be deemed to be the only embodiments and are presented to illustrate the flexibility and advantages of particular embodiments as defined by the following claims. Other arrangements embodiments implementations and equivalents can be employed without departing from the scope hereof as defined by the claims.

