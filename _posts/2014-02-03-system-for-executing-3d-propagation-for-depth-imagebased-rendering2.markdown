---

title: System for executing 3D propagation for depth image-based rendering
abstract: A system is disclosed for executing depth image-based rendering of a 3D image by a computer having a processor and that is coupled with one or more color cameras and at least one depth camera. The color cameras and the depth camera are positionable at different arbitrary locations relative to a scene to be rendered. In some examples, the depth camera is a low resolution camera and the color cameras are high resolution. The processor is programmed to propagate depth information from the depth camera to an image plane of each color camera to produce a propagated depth image at each respective color camera, to enhance the propagated depth image at each color camera with the color and propagated depth information thereof to produce corresponding enhanced depth images, and to render a complete, viewable image from one or more enhanced depth images from the color cameras. The processor may be a graphics processing unit.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09654765&OS=09654765&RS=09654765
owner: THE BOARD OF TRUSTEES OF THE UNIVERSITY OF ILLINOIS
number: 09654765
owner_city: Urbana
owner_country: US
publication_date: 20140203
---
This application is a continuation of U.S. patent application Ser. No. 12 948 373 filed Nov. 17 2010 which claims the benefit under 35 U.S.C. 119 e of U.S. Provisional Patent Application No. 61 262 451 filed Nov. 18 2009 which is incorporated herein in its entirety by this reference.

The present disclosure relates generally to 3D image processing and more particularly to a system for executing 3D propagation for depth image based rendering of a 3D colored image with one or more color cameras and at least one depth camera at arbitrary positions.

Image based rendering IBR is the process of synthesizing new virtual views from a set of real views. Obviating the need to create a full geometric 3D model IBR is relatively inexpensive compared to traditional rendering while still providing high photorealism. Because IBR rendering time is independent of the geometrical and physical complexity of the scene being rendered IBR is also extremely useful for efficient rendering of both real scenes and complex synthetic 3D scenes. Therefore IBR has attracted a lot of research interest recently. Its applications can be found in many areas such as 3DTV free viewpoint TV telepresence video conferencing and computer graphics.

Depth IBR DIBR combines 2D color images with per pixel depth information of the scene to synthesize novel views. Depth information can be obtained by stereo match or depth estimation algorithms. These algorithms however are usually complicated inaccurate and inapplicable for real time applications. Conventional DBIR implementations furthermore use images from cameras placed in a 1D or 2D array to create a virtual 3D view. This requires very expensive camera configurations and high processing resources and prevents development of real time DIBR applications.

Thanks to the recent developments of new range sensors that measure time delay between transmission of a light pulse and detection of the reflected signal on an entire frame at once per pixel depth information can be obtained in real time from depth cameras. This makes the DIBR problem less computationally intense and more robust than other techniques. Furthermore it helps significantly reduce the number of necessary cameras.

Some approaches for solving DIBR have been proposed in professional literature. McMillan with his warping method maps a point in an image to a corresponding point in another image at a different view as long as its depth value is known. However this work considers only single views and did not take advantage of multiple views. Furthermore warping is only the first step of the synthesis work. An additional problem is how to deal with newly exposed areas holes appearing in the warped image which will be discussed in more detail later. Some approaches to handle this problem have also been proposed. However these approaches consider only the 1D case where the virtual camera is forced to be on the same line with real cameras and assumed that depth images are given in the same views with color images. This assumption may not be appropriate because not all depth cameras provide color information. Furthermore standard color cameras are much cheaper and provide much higher color resolution than depth cameras. So the combination of a few depth cameras and many color cameras may be more feasible as will be explored in more detail later. With such a configuration the depth and color camera views will necessarily be different.

Another approach which focuses on signal processing techniques is a one dimensional 1D propagation algorithm developed in part by the assignees of the present application. H. T. Nguyen and M. N. Do Proceedings of IEEE International Conference on Acoustics Speech and Signal Processing ICASSP March 2005. Using depth information surface points that correspond to pixels in the real images are reconstructed and re projected onto the virtual view. Therefore the real pixels are said to be propagated to the virtual image plane. Again it is implied in the Nguyen and Do reference that color cameras and the depth or range camera must have the same resolution the same location and only the 1D case is considered.

Based on the above discussed practical considerations and challenges presented by the same this disclosure considers a generalized 3D case but with separate locations for the color and depth cameras. A disclosed system is adapted to execute a new occlusion removal method to include a depth color bilateral filter and disocclusion filling to improve rendering quality. The proposed 3D propagation algorithm for solving depth image based rendering DIBR combines images from one or more color and depth cameras at arbitrary positions in 3D space relative to a scene to be rendered and efficiently renders 3D colored images at arbitrary virtual views by propagating available depth information from depth cameras to color cameras and then available depth and color information from color cameras to the virtual views. This can be accomplished with three main steps 1 depth propagation 2 color based depth filling and enhancement and 3 rendering. Considered is the case when only low resolution depth images are obtained. Proposed is a color based depth filling and enhancement technique for enhancing depth imaging quality using high quality color images that significantly improves the rendering quality.

Herein low resolution refers to the situation where the resolution of the depth camera is lower or much lower than that of the colored camera s . High resolution refers to situation where the resolution of the depth camera is the same or higher than that of the color camera s .

Also described is the abundant but irregular parallelism of the proposed 3D algorithm based on which it may be mapped onto massively parallel architectures such as general purpose graphics processing units GPGPUs . A preliminary GPU based implementation of the system achieves 277 times faster than the sequential implementation. Experimental results show that the proposed parallelized algorithm running on a GPU provides excellent rendering quality while staying within computational bounds for real time applications.

The major features of the proposed 3D propagation system and corresponding algorithm and their respective benefits include but are not limited to that the system 1 allows adaptable combination of high resolution color cameras with low resolution depth cameras which is cheaper than a full fixed array of cameras 2 cameras can be set up arbitrarily in 3D space 3 uses color contrasts to improve edge rendering which provides a better interpolated 3D view without requiring processing hungry high depth detail and 4 allows massively parallel processing which provides faster processing which once mapped to PGPUs the proposed DIBR should be able to run in real time.

Most DIBR techniques focus on cases of 1D or 2D arrays of cameras while arbitrary camera configurations in 3D are rarely considered. Moreover these techniques usually assume that depth images are available at the same location with color images. This assumption is true with depth estimation based techniques but impractical for depth camera based techniques because depth cameras generally do not provide color images. Another challenge with depth camera based IBR techniques is that the resolution of depth images from depth cameras is often quite low. Since color cameras are much cheaper than depth cameras the need for using a mixture of several cheap high resolution color cameras and a few low cost low resolution depth cameras becomes significant. That is it would be very expensive to buy one or more high resolution cameras that have integrated depth and color imaging capabilities.

Because the geometric information in DIBR is usually captured in real time from the real physical world instead of from a modeling and synthetic world which also makes DIBR more photorealistic the obtained data suffers from noise and insufficient sampling effects. There is therefore a need to combine image processing techniques with rendering techniques to clean up the noise and fill in where the insufficient sampling leaves anomalies. This combination significantly increases the computations and is infeasible for real time applications without parallelism. Since rendering with full geometric information color and depth has been optimized for general purpose graphics processing units GPGPUs GPGPUs are considered the ideal computing platform and a good choice for DIBR applications. Accordingly the proposed image processing algorithm was developed specifically to be suitable to run on a GPGPU hardware platform. In particular included are techniques that have a high degree of locality e.g. bilateral filtering and that maximize massive parallelism e.g. process pixels independently whenever possible.

As reflected in the proposed 3D propagation system seeks to based on images collected from one or more color cameras and depth or range cameras render a new image at an arbitrary virtual view at a virtual camera . Assume that there are N color cameras and M depth cameras capturing a scene in 3D space. The inputs for the algorithm are a set of color images I x depth images d x and parameters of range and color cameras C x fright arrow over w where Cis camera position of the icamera fis its focal length and right arrow over w is its normalized viewing direction vector which points from Cto the image plane center. The output is a rendered image at virtual view I x . The proposed algorithm is based on the two following basic assumptions i Calibrated cameras the positions focal lengths and viewing direction of all cameras are available ii Lambertian surfaces the color of a point on a surface is constant regardless the angle from which it is viewed.

The proposed algorithm is divided into three main steps 1. Depth image propagation Depth information from each depth camera is propagated to the image plane of every color camera . 2. Color based depth filling and enhancement signal processing techniques are applied to obtain an enhanced complete depth image at each color camera . 3. Rendering Depth and color information from each color camera are propagated to the virtual view then merged and filtered to produce the output image .

At block the system performs depth propagation of depth information from the depth camera s to the color cameras to produce propagated depth images through warping. At block the system performs occlusion removal on the propagated depth images to replace occluded pixels in the colored images with newly interpolated values. At block the system performs depth color bilateral filtering DCBF on the propagated depth images for edge preserving of the colored images by calculating unknown depth pixels at the image plane using color information. At block the system performs direction disocclusion filing on the propagated depth images to fill holes caused by disocclusion at a plurality of epipoles of the colored images during image propagation discussed in more detail later . At block the system performs depth edge enhancement on the propagated depth images to sharpen depth edges surrounding objects in the propagated depth images which results in enhanced depth images .

At each of the image processing steps at blocks through the propagated depth images at each color camera are further enhanced resulting in enhanced depth images at each respective color camera. In some examples the processing steps at blocks through may be performed in a different order. The system then at block renders a 3D colored image from the merging of the enhanced depth images from the color cameras again through warping but this time from a reference of the color cameras to a target or desired view of the virtual camera . The system by rendering the 3D colored image at block may also remove occlusions and process the rendered image with a median filter to fill and denoise the rendered image.

In this section presented is how to propagate depth information from a depth camera to a color camera . The depth camera is considered as the reference view and the color camera is considered as the target or desired view. The 3D warping technique referred to earlier allows the mapping of a point in a reference image to a corresponding point in a desired image at a different view as long as the system knows the depth value of that point. Consider a reference camera C f right arrow over w and a desired camera C f right arrow over w in a 3D Euclidian space with basis vectors right arrow over i right arrow over j right arrow over k .

It is known that each point of an image in 2D space can be mapped one to one with a ray in 3D space that goes through the camera position. Given a 2D image plane with basis vectors right arrow over s right arrow over t and a 3D space right arrow over i right arrow over j right arrow over k the 2D point to 3D ray mapping relation is 

With reference to consider a point X in 3D space right arrow over i right arrow over j right arrow over k . Let right arrow over r and right arrow over x be homogeneous coordinates of X in the reference image plane and the target or desired image plane. Let Pand Pbe mapping matrices of the reference camera and the target camera. It has been proven that the warping equation between right arrow over x and right arrow over x is 

This step fills depth pixels and performs depth image enhancement to prepare for the rendering step and was discussed with reference to blocks through of .

Unknown depth patches or holes represented by black color in are caused by two reasons. First uniform sampling in the reference image becomes non uniform sampling in the target image. This means that in some certain patches there are fewer sample points than in some other patches. Those holes can be filled by the interpolation step using depth color bilateral filtering DCBF introduced at block of . Second holes are also created when occluded areas in the reference image are revealed in the desired image which is a disocclusion problem. It seems to be impossible to fill these holes. However for the present setting the system has full color information at the target view. Therefore these holes can be interpolated based on the color image at the target view through directional disocclusion filing introduced at block in .

In addition as shown in some background sample points the brighter specs visible in the reference depth image should be occluded by the foreground pixels with darker color in the desired depth image but are still visible. That significantly degrades the interpolation quality. Therefore an occlusion removal step is necessary to correct those points before performing further processing steps.

The occlusion removal method presented herein is based on the smoothness of surfaces. If a point or pixel A in Dis locally surrounded by neighboring points or pixels whose depth values are smaller than the depth of A then A is selected to be occluded by the surface composed of those neighbors.

As shown in for every point A a 2w 1 2w 1 window whose center is A is divided into four partitions. If there are at least three of those partitions each of which has at least one point B such that depth A depth B then point A is selected to be occluded and its depth is replaced by a new interpolated value. shows a patch of Dbefore and after occlusion removal step respectively. In our experiment discussed below we choose w 3.

Bilateral filtering is a basic non iterative scheme for edge preserving smoothing. It is a combination of a spatial filter whose weights depend on Euclidian distance between samples and a range filter whose weights depend on differences between values of samples. Bilateral filtering is usually applied only for color images and provides excellent enhancement quality. In this disclosure by integrating known depth and color information the proposed DCBF effectively interpolates unknown depth pixels in Dcaused by non uniform resampling while keeping sharp depth edges. The DCBF is defined as following 

The idea of using color differences as a range filter to interpolate depth value is based on the observation that whenever a depth edge appears there is almost always a corresponding color edge due to color differences between objects or between foreground and background. The DCBF also works well with textured surfaces since it counts only pixels on that surface which have similar color to the interpolated pixel. If surfaces have the same color color does not give any new information and the DCBF works as a basic interpolation scheme such as bilinear or bicubic. show parts of the obtained depth image after the DCBF step for the low resolution depth case. The black areas caused by disocclusion are handled in section I B3.

In practice most depth cameras such as from Canesta of Sunnyvale Calif. or Prime Sense of Tel Aviv Israel provide depth images with lower resolution than that of the color images. In the presently disclosed methods the available depth information is first propagated to the color camera. Then the DCBF step is proceeded to calculate unknown depth pixels at the color image plane based on color information from the high resolution color image. As discussed earlier an advantage of the DCBF filter is its ability to combine color and depth information for edge preserving interpolation. Therefore the proposed algorithm can work well with different resolution settings of the cameras which may occur more frequently when using cameras from different manufacturers.

In order to fill holes caused by disocclusion the DCBF can also be used but it needs to follow a specific direction. Otherwise filtering is performed from all directions incorrect depth values may be obtained. As described in filling from right to left causes incorrect interpolating depths because the holes should be filled with the background s depth whereas all neighboring known depth values belong to the foreground. It was observed that in the 2D image plane the disocclusion holes always appear toward the projection of the reference camera position onto the target or desired image plane e.g. the projection originates from an epipole of the reference camera view. So a relationship must exist between the disocclusion holes and the camera position.

With reference to an epipole is the point of intersection of the line joining the optical centers e.g. the baseline with the image plane. Thus the epipole is the image in one camera of the optical center of the other camera. The epipolar plane is the plane defined by a 3D point M and the optical centers C and C . The epipolar line is the straight line of intersection of the epipolar plane with the image plane. The epipolar line is thus the image in one camera of a ray through the optical center and image point in the other camera. All epipolar lines intersect at the epipole.

With reference to D is the center of the depth camera while C1 and C2 are the centers of two color cameras . This provides two pairs of cameras C1 D and C2 D. Each star is an epipole for each camera pair. Each square and is a cross section view of the image plane at the epipole line associated with the epipole for each respective camera pair. The clear circle is an arbitrary object in a scene being photographed. The black area extended behind the arbitrary object is the disocclusion hole. The disocclusion hole is the area that cannot be seen from the viewpoint of D because it is occluded by the circle but that can be seen from the viewpoint of C1 or C2 because the viewpoint is different from points C1 and C2. At viewpoints C1 and C2 therefore there is missing depth information in the black areas that need to be filled with some value.

With reference to the black area or disocclusion hole always appears toward the epipole. So to fill the disocclusion holes the filling directions should be along the lines of arrows and respectively for enhancing images at cameras C1 D and C2 D. In other words the filling directions can be decided correctly based on drawing a line from the epipole for each respective color camera to the center of the target depth image.

More specifically the epipole right arrow over e can be computed as follows 4 right arrow over 5 where Cand Care positions of the reference and target views and Pis the mapping matrix of the target view. Then the filling direction is a vector pointing from the epipole to the center of the target depth image. For example if the epipole lies in the top left quadrant of the image the filling should start from the top left corner such as shown in the left half of . And likewise if the epipole lies in the top right quadrant of the image the filling should start from the top right corner such as shown in the right half of . are respectively indicative of a propagated depth image processed by non directional bilateral filling and by directional bilateral filling. Note the improvement in the features of that makes the objects look more like the real objects. show parts of the complete propagated depth image for the low resolution depth case after filling disocclusion areas.

Even though the DCBF preserves edges while filtering it still cannot provide truly sharp depth edges. Tiny blurring depth edges around objects can be seen in or . Sharp depth edges are extremely important for the rendering step because at some rendered viewpoint tiny blurring edges may blow up to a large smearing region in the rendered color image which is not visually pleasing. Therefore enhancing depth edges is a task that should be performed before rendering.

The proposed depth edge enhancement technique may include two steps. First the system detects depth edge gradients with Sobel operators in vertical horizontal and two diagonal directions. To diminish the effect of noise the depth image needs to be slightly smoothed first. The DCBF can also be applied here. Then the system may classify pixels with significant edge gradients as adjustable depth pixels and the rest of pixels are fixed depth pixels. A significant edge gradient may be determined based on edge gradients beyond a predetermined threshold gradient value.

Secondly for each adjustable depth pixel a block based search is applied to find a neighboring fixed depth pixel that best matches in color. Once the best color matched pixel is chosen the depth value of that pixel is copied to that of the adjustable depth pixel. This second step may be iterated through a few times so that most of the adjustable depth pixels are adjusted. The iteration does not slow down the speed too much since the process is purely parallel and can be done extremely fast on the GPU.

With more specificity given a pixel x x x in an image plane I x and D x are color and depth values respectively of pixel x. Let a neighborhood of xN x be defined as 

The following pseudo code of the proposed depth edge enhancement stage may then be applied to complete the depth edge enhancement stages. In the following pseudo code Tand Tare thresholds and determine the search window size and determines the comparison block size.

Experimental results show that the previously discussed depth edge enhancement technique works very well even for the low resolution depth case as shown by comparing with and comparing with and with . The reason is that there is no texture in a depth map so the significant depth gradients only appear around the true depth edges and at the same location with color gradients. Information about color edges in the high quality color images can be used to decide and sharpen the true depth edges. Since the depth values of pixels in a small vicinity of the same object are almost the same directly copying a depth value from one neighboring pixel of the same object can significantly reduced the computations whereas the enhancement quality is still preserved.

Now each color camera has both depth and color information. The last step is propagating this information to the virtual camera . The color cameras become the reference views and the virtual camera becomes the target or desired view. This process is quite similar to the first two parts of the algorithm. First the system propagates depth and color information of each color view into the virtual view using the same technique disclosed in section I A. Then the system performs the occlusion removal technique disclosed in section I B1 at the virtual view. Finally the rendered image is filled and denoised with a 3 3 or other suitably sized median filter as is known in the art of image filtering. Note that most of the unknown color pixels in this step are caused by non uniform resampling since the color cameras are intentionally installed in a way to capture the whole scene from different views and therefore reduce as much as possible the holes caused by disocclusion. The complete rendered image is shown in .

The computer also may be coupled with shared device memory both static and dynamic other system storage memory and a plurality of JO ports as required to connect to the plurality of color cameras and the at least one depth camera . The computer and the system are further coupled with or otherwise include other hardware and or software as would be evident to one skilled in the art of image processing particularly with regards to the hardware and software disclosed herein. As discussed above the ultimate output of the system includes an arbitrary virtual view at a virtual camera which can include a camera or another display device such as a monitor computer or TV screen.

Images captured at various positions by the plurality of colored cameras and the at least one depth or range camera are processed by the GPU which may be run by the GPGPU as well as by the CUDA . General purpose computing on graphics processing units GPGPU also referred to as GPGP and to a lesser extent GP is the technique of using a GPU which typically handles computation only for computer graphics to perform computation in applications traditionally handled by the CPU. It is made possible by the addition of programmable stages and higher precision arithmetic to the rendering pipelines which allows software developers to use stream processing on non graphics data. These pipelines enable mapping to massively parallel architectures such as GPGPU to allow real time rendering of at least 76 times faster than serial rendering even on a mediocre graphics card.

In November 2006 Nvidia of Santa Clara Calif. launched CUDA a software development kit SDK and application programming interface API that allows a programmer to use the C programming language to code algorithms for execution on Geforce 8 series GPUs. AMD of Sunnyvale Calif. offers a similar SDK for their ATI based GPUs and that SDK and technology is called Stream SDK formerly CTM Close to Metal designed to compete directly with Nvidia s CUDA. AMD has also announced the AMD FireStream product line combining CPU and GPU technology on one chip . Compared for example to traditional floating point accelerators such as the 64 bit CSX700 boards from ClearSpeed that are used in today s supercomputers current top end GPUs from Nvidia and AMD emphasize single precision 32 bit computation as double precision 64 bit computation executes much more slowly. Any of these SDKs or improvement thereto may be employed for execution by the GPUs of the present disclosure.

With more particularity CUDA or AMD s equivalent is a parallel programming model and software environment providing general purpose programming on the GPUs . At the hardware level the GPU device is a collection of multiprocessors each consisting of eight scalar processor cores instruction unit on chip shared memory and texture and constant memory caches. Every core has a large set of local 32 bit registers but no cache. The multiprocessors follow the SIMD architecture e.g. they concurrently execute the same program instruction on different data. Communication among multiprocessors is realized through the shared device memory that is accessible for every processor core.

On the software side the CUDA programming model extends the standard C C programming language with a set of parallel programming supporting primitives. A CUDA program consists of a host code running on the CPU and a device code running on the GPU . The device code is structured into so called kernels. A kernel executes the same scalar sequential program in many data independent parallel threads. Within the kernel threads are organized into thread blocks forming a grid of one or more blocks. Each thread is given a unique index within its block threadIdx and each block is given a unique index blockIdx within the grid. The threads of a single block are guaranteed to be executed on the same multiprocessor thus they can easily access data stored in the shared memory of the multiprocessor. The programmer specifies both the number of blocks and number of threads per block to be created before a kernel is launched. These values are available to the kernel as gridDim and blockDim values respectively.

Using CUDA to accelerate the computation is easily exemplified on a vector summation problem. Suppose two vectors of length n to be summed. In the standard imperative programming language a programmer would use a for loop to sum individual vector elements successively. Using CUDA however the vector elements can be summed concurrently in a single kernel call populated with n threads each responsible for summation of a single pair of vector elements at the position given by the thread index.

One of the advantages of the proposed 3D algorithm is that it can be mapped onto data parallel architectures such as modern graphics processing units GPUs as disclosed with respect to the system or . In this section we briefly describe the parallelism of each processing step of the proposed 3D algorithm and the high level mapping onto the Nvidia CUDA architecture for GPU based computing within the system or .

The depth propagation occlusion removal and DCBF steps are purely parallel as each pixel in the desired view can be computed independently. Copying the depth values in the reference view to appropriate pixels in the target view is more complex from a parallelism perspective since at some pixels this is not a one to one mapping. This operation requires some form of synchronization to prevent concurrent writes to the same pixel and can be accomplished with the use of atomic memory operations or alternatively with the use of Z buffering hardware available on modern GPUs.

The disocclusion filling step in section I B3 also has a sequential component since calculating unknown depth information is dependent on previously interpolated values. However this dependence exists only on 1D lines emanating from the epipole and thus the problem can be expressed as a parallel set of 1D filters. First find the epipole position and categorize it into one of eight following subsets top bottom left right top left top right bottom left or bottom right corresponding to eight sets of parallel lines every 45 degree angle. The parallel lines in each set need to pass through all pixels in the depth image. For each set of parallel lines pixel coordinates of each line can be pre computed and stored in a lookup table. The 1D DCBF is performed with each line proceeding in parallel which can be easily mapped onto the GPU architecture.

The depth edge enhancement step described in Section I B4 is a series of independent window based operators Sobel operators and window based searches for the best match and hence is naturally parallel. The final rendering step is quite similar to the first and second part of the algorithm except for the inclusion of a median filter. The median filter however is another window based operator and hence is data parallel in nature.

In order to check the efficiency of the parallelism the system was configured to implement the depth propagation occlusion removal and DCBF steps in two modes sequential mode and parallel mode. The experiment was run on the platform of Intel Core2 Duo E8400 3.0 GHz and an Nvidia GeForce 9800GT GPU with 112 processing cores. In sequential mode the code was run only on the CPU and in parallel mode the code ran also on the GPU. The result in Table I shows that the parallel mode is about 277 times faster than the sequential mode. The speedup is mainly attributable to the higher degree of parallelism in parallel mode and partly due to the increased efficiency brought about by the massive threading support of the GPGPU architecture. Note that the running time of the algorithm mainly depends on the image resolution not on the complexity of the scene. Therefore the obtained result can be approximated for other examples of the same resolution.

Regarding the parallel scalability of the proposed algorithm experiments show that there is ample data parallelism to take advantage of the heavily threaded 128 core modern GPU architecture. The proposed technique scales further with image size and higher resolution images will create additional parallel work for future data parallel architectures that support yet higher degrees of parallelism. Furthermore with the use of additional cameras the data parallel computational load increases further yet creating additional work that can be gainfully accelerated on future data parallel architectures. In the case of additional cameras two approaches can be used. In one approach the problem can be simplified into the case of using two nearest color cameras on the left and right with one nearest depth camera and ignore other cameras. This approach is appropriate if the computational load is too high for real time operation. In a second alternative data from every camera can be used to improve rendering quality. Note that processing each color view depth propagation filling and enhancement is independent and suitable for parallelism.

For the experiments we adopted a synthesis configuration with two color cameras and one depth camera. The color input images are shown in . For the high resolution depth case the resolution is 800 600. The window size for occlusion removal step in Section I A is 7 7. Parameters for the DCBF in Section I B2 are set as following 3 0.01 and kernel window size 11 11. The enhanced propagated depth image of the left color view and the complete rendered image of a virtual view are shown in . The black area at the bottom right of the rendered image is due to the fact that there is completely no information both depth and color from the input images corresponding to those areas.

For the low resolution depth case the depth image is downsampled to the resolution of 160 120 thus keeping only 4 of the depth information from the high resolution case . Some intermediate results are shown in . From the experimental results we see that the proposed color based depth filling and enhancement technique not only fills and preserves the depth edges but also corrects the depth edges see . The enhanced propagated depth of the left color view and the complete rendered image shown in are almost the same as in the high resolution depth case. It can be noted that the information contained in a depth image is not as much as in a color image. So if a sufficiently sparse set of depth samples and true depth edges are given the high resolution version of the depth image can be completely restored.

In the foregoing description numerous specific details of programming software modules user selections database queries database structures etc. are provided for a thorough understanding of various embodiments of the systems and methods disclosed herein. However the disclosed system and methods can be practiced with other methods components materials etc. or can be practiced without one or more of the specific details.

In some cases well known structures materials or operations are not shown or described in detail. Furthermore the described features structures or characteristics may be combined in any suitable manner in one or more embodiments. The components of the embodiments as generally described and illustrated in the Figures herein could be arranged and designed in a wide variety of different configurations. The order of the steps or actions of the methods described in connection with the disclosed embodiments may be changed as would be apparent to those skilled in the art. Thus any order appearing in the Figures such as in flow charts or in the Detailed Description is for illustrative purposes only and is not meant to imply a required order.

Several aspects of the embodiments described are illustrated as software modules or components. As used herein a software module or component may include any type of computer instruction or computer executable code located within a memory device and or transmitted as electronic signals over a system bus or wired or wireless network. A software module may for instance include one or more physical or logical blocks of computer instructions which may be organized as a routine program object component data structure etc. that performs one or more tasks or implements particular abstract data types.

In certain embodiments a particular software module may include disparate instructions stored in different locations of a memory device which together implement the described functionality of the module. Indeed a module may include a single instruction or many instructions and it may be distributed over several different code segments among different programs and across several memory devices. Some embodiments may be practiced in a distributed computing environment where tasks are performed by a remote processing device linked through a communications network. In a distributed computing environment software modules may be located in local and or remote memory storage devices.

Various modifications changes and variations apparent to those of skill in the art may be made in the arrangement operation and details of the methods and systems disclosed. The embodiments may include various steps which may be embodied in machine or computer executable instructions to be executed by a general purpose or special purpose computer or other electronic device . Alternatively the steps may be performed by hardware components that contain specific logic for performing the steps or by any combination of hardware software and or firmware. Embodiments may also be provided as a computer program product including a machine or computer readable medium having stored thereon instructions that may be used to program a computer or other electronic device to perform processes described herein. The machine readable medium may include but is not limited to floppy diskettes optical disks CD ROMs DVD ROMs ROMs RAMs EPROMs EEPROMs magnetic or other type of media machine readable medium suitable for storing electronic instructions. For example instructions for performing described processes may be transferred from a remote computer e.g. a server to a requesting computer e.g. a client by way of data signals embodied in a carrier wave or other propagation medium via a communication link e.g. network connection .

