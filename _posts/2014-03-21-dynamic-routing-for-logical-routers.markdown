---

title: Dynamic routing for logical routers
abstract: Some embodiments provide a method for a network controller that manages a first logical router of a logical network that is implemented across several managed network elements. The method receives input data specifying a first route for a second logical router. Based on a connection between the first logical router and a second logical router in the logical network, the method dynamically generates a second route for the first logical router based on the first route. The method distributes data to implement the first logical router, including the second route, to a set of the managed network elements.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09503321&OS=09503321&RS=09503321
owner: NICIRA, INC.
number: 09503321
owner_city: Palo Alto
owner_country: US
publication_date: 20140321
---
In traditional physical networking routes come in three types connected static and dynamic. Connected routes are those determined automatically based on local interface information. When an interface has an address configured in a subnet then the router has a directly connected route to that subnet. Static routes are those manually configured at the router and dynamic routes are learned from other routers via routing protocols e.g. BGP OSPF IGP etc. . As this may result in a router being presented with multiple routes for the same IP address routers perform various processing techniques in order to choose between these routes.

Virtualized networks may also have routers referred to as logical routers. Previous implementations of logical routers have only used connected routes however generated based on the IP prefix configured on the port of the logical router. Adding different types of routes to logical routers would pose the problem of requiring additional processing techniques for the logical routers which may not be easily performed by the software forwarding elements often used to implement such logical routers.

Some embodiments provide a network control system that enables the connection of logical routers to each other and the propagation of routes between the logical routers. In some embodiments the logical routers are managed by one or more network controllers which receive input to define the logical routers and compute additional route information for the logical routers. The computation of additional route information may include the propagation of routes specified for one logical router to a different logical router. In order for a logical router to be implemented in a physical network managed by a network controller of some embodiments the network controller generates a routing table for the logical router and distributes the routing table to various elements in the network that implement the logical router. In some embodiments the network controller distributes this routing table including the dynamic route information as i flow entries distributed to managed forwarding elements and ii data tuples defining a routing table for a virtualized container e.g. a namespace that operates as a L3 gateway for communicating with external networks.

In some embodiments the network control system permits several different types of logical routers which may have different predefined functionalities. Some embodiments arrange these different types of logical routers hierarchically. For example in some embodiments a first type of logical router connects to logical switches within a logical network while a second type of logical router provides connections between the shared virtualized infrastructure within which the logical routers are implemented and other networks external to the shared virtualized infrastructure.

Specifically some embodiments enable tenant logical routers and provider logical routers for implementation within a virtualized network. The provider logical routers of some embodiments are managed by a datacenter provider to handle traffic in and out of a datacenter e.g. a multi tenant datacenter within which various tenant logical networks are implemented. These provider logical routers in some embodiments may have connections to multiple tenant logical routers as well as connections to external networks that are implemented in gateways i.e. host machines that have a physical connection to routers in the external network . The tenant logical routers of some embodiments provide logical routing functionality to a single tenant logical network allowing the tenant to connect multiple logical switches to which the tenant machines e.g. virtual machines attach . The tenant logical routers in some embodiments may also connect to a provider logical router in order to receive traffic from and send traffic to external hosts. These restrictions on logical router functionality enable the datacenter administrator to manage via the configuration of a provider logical router the handling of traffic entering and exiting the datacenter.

In order to enable the connection of logical routers to each other the network controllers enable dynamic routing between connected logical routers. In the general case when a first logical router connects to a second logical router the network controller automatically propagates routes from the first logical router to the second logical router and vice versa. Thus if the first logical router stores a connected route that routes network addresses in a particular subnet to a particular port of the logical router the network controllers automatically populate the second logical router with a new route specifying the first logical router i.e. a specific logical port of the first logical router as a next hop for network addresses in the particular subnet. Similarly connected routes for subnets attached to the second logical router are dynamically propagated to the first logical router as dynamic routes that specify the second logical router as a next hop.

In some embodiments the routes that a network controller dynamically propagates include connected routes as well as manually entered static routes. The connected routes described above may be automatically generated for a logical router based on the configuration of the logical router i.e. based on the attachment of a logical port to a particular subnet . The static routes in some embodiments are received by the network controller after manual input by an administrator of the logical network to which the logical router belongs. The static routes might specify for a particular range of network addresses a specific next hop address to which to send the packets. As an example if a logical router has multiple connections to other logical routers or connections to a physical network with multiple physical routers the administrator might want to specify which of these routers should be the next hop for a particular range of network addresses.

The network controllers of some embodiments store connections between logical routers as part of the configuration data for the logical routers. Thus when configuration state routing information for a first logical router is received the network controller identifies whether to propagate this information to any other logical routers as dynamic routes. For the case of provider and tenant logical routers some embodiments place restrictions on the routes that are dynamically propagated between logical routers. Specifically when a tenant logical router connects to a provider logical router some embodiments dynamically propagate the connected routes of the tenant logical router to the provider logical router such that the provider logical router will send packets to the subnets specified by the connected routes to that tenant logical router. However rather than dynamically propagating routes specifying information about the various other tenant networks that connect to the provider logical router the network controller only propagates a dynamic default route to the tenant logical router that sends all packets not otherwise routed by more specific routes to the provider logical router.

In some embodiments different logical routers may have different master network controllers that perform state computation for the logical routers. That is a master network controller for a particular logical router takes the input configuration state for the logical router and generates output state. The master network controller generates the flow entries and or other data tuples used to implement the logical router and distributes this data to the network elements in order for those network elements to implement the logical router. In some embodiments the input configuration state for the logical routers is shared between network controllers in a network control system and routes are propagated dynamically to a particular logical router only by the master network controller for the particular logical router.

The preceding Summary is intended to serve as a brief introduction to some embodiments of the invention. It is not meant to be an introduction or overview of all inventive subject matter disclosed in this document. The Detailed Description that follows and the Drawings that are referred to in the Detailed Description will further describe the embodiments described in the Summary as well as other embodiments. Accordingly to understand all the embodiments described by this document a full review of the Summary Detailed Description and the Drawings is needed. Moreover the claimed subject matters are not to be limited by the illustrative details in the Summary Detailed Description and the Drawing but rather are to be defined by the appended claims because the claimed subject matters can be embodied in other specific forms without departing from the spirit of the subject matters.

In the following detailed description of the invention numerous details examples and embodiments of the invention are set forth and described. However it will be clear and apparent to one skilled in the art that the invention is not limited to the embodiments set forth and that the invention may be practiced without some of the specific details and examples discussed.

Some embodiments provide a network control system that enables the connection of logical routers to each other and the propagation of routes between the logical routers. In some embodiments the logical routers are managed by one or more network controllers which receive input to define the logical routers and compute additional route information for the logical routers. The computation of additional route information may include the propagation of routes specified for one logical router to a different logical router. In order for a logical router to be implemented in a physical network managed by a network controller of some embodiments the network controller generates a routing table for the logical router and distributes the routing table to various elements in the network that implement the logical router. In some embodiments the network controller distributes this routing table including the dynamic route information as i flow entries distributed to managed forwarding elements and ii data tuples defining a routing table for a virtualized container e.g. a namespace that operates as a L3 gateway for communicating with external networks.

In some embodiments the network control system permits several different types of logical routers which may have different predefined functionalities. Some embodiments arrange these different types of logical routers hierarchically. For example in some embodiments a first type of logical router connects to logical switches within a logical network while a second type of logical router provides connections between the shared virtualized infrastructure within which the logical routers are implemented and other networks external to the shared virtualized infrastructure.

Specifically some embodiments enable tenant logical routers and provider logical routers for implementation within a virtualized network. The provider logical routers of some embodiments are managed by a datacenter provider to handle traffic in and out of a datacenter e.g. a multi tenant datacenter within which various tenant logical networks are implemented. These provider logical routers in some embodiments may have connections to multiple tenant logical routers as well as connections to external networks that are implemented in gateways i.e. host machines that have a physical connection to routers in the external network . The tenant logical routers of some embodiments provide logical routing functionality to a single tenant logical network allowing the tenant to connect multiple logical switches to which the tenant machines e.g. virtual machines attach . The tenant logical routers in some embodiments may also connect to a provider logical router in order to receive traffic from and send traffic to external hosts. These restrictions on logical router functionality enable the datacenter administrator to manage via the configuration of a provider logical router the handling of traffic entering and exiting the datacenter.

In some embodiments the network controller is one of several controllers that manages numerous managed forwarding elements that implement multiple logical networks across numerous host machines. For example a logical network might include several logical switches that attach to a logical router with numerous virtual machines VMs attached to the logical switches. The VMs reside on numerous host machines possibly alongside VMs of other logical networks. A managed forwarding element MFE operates on each host machine e.g. as a software forwarding element residing in the virtualization software of the host machine in order to process packets sent to and received from the VMs on that host machine. In some embodiments the MFE on a particular host machine stores information in order to implement the logical forwarding elements for the various different logical networks that have VMs residing on the host machine.

The network controller may manage a particular one or several of these logical forwarding elements or logical networks as a whole and therefore stores information in the state storage about the logical forwarding elements. In some embodiments the network controller receives configuration information defining the logical forwarding elements that it manages and computes additional information for distribution to the MFEs at the host machines in order to implement the logical network. The state storage stores both configuration state and computed state information for all of the logical forwarding elements managed by the controller . In addition in some embodiments other controllers share configuration state information with the network controller for other logical networks that are not managed by the controller . However in some such embodiments the controllers do not share computed state information and each controller only computes state for the logical networks that it manages.

The input interface is an application programming interface API in some embodiments through which the network controller receives configuration information e.g. configuration of logical ports of a logical router static routes for a logical router etc. . The configuration information may be input by an administrator logging into the network controller directly or through a management application that translates administrator entered information into API commands to send to the controller. Upon receiving this information the input interface stores the configuration data into the state storage . In some embodiments each logical forwarding element e.g. the logical router is stored as an object and the routes are stored as objects which are owned by the logical router object. To define a configured route in the state storage some embodiments store the type of route e.g. connected static dynamic the network address or range of addresses governed by the route a destination e.g. a next hop address a logical port a drop action for packets having a network address in the range governed by the route and a priority for the route.

The table mapping engine performs state calculations for logical forwarding elements managed by the controller in some embodiments. These state calculations may include generating flow entries to implement the logical forwarding elements generating data tuples for logical services and routing tables for L3 gateways etc. In some embodiments the table mapping engine is implemented in a table mapping language that performs join operations between sets of tables such as nLog or datalog. In addition the table mapping engine of some embodiments generates dynamic routes based on connections with other logical routers and the routes stored by the other logical routers e.g. the static and connected routes . When the table mapping engine of some embodiments receives a set of routes for a logical router that includes one or more dynamic or static routes i.e. that includes routes other than those defined automatically based on the logical ports of the logical router the table mapping engine utilizes the route processing engine to translate the input set of routes into an output set of routes.

The route processing engine of some embodiments receives a set of routes from the table mapping engine e.g. routes automatically generated by the table mapping engine or network controller API based on the subnets to which the logical ports of the logical router connect static routes input through the network controller API and performs a recursive traversal process on the routes in order to identify a final logical destination for each network address range routed by the logical router. When multiple input routes provide contradictory information for a particular network address or range of addresses the route processing engine of some embodiments determines which route has a higher priority. The route processing engine of some embodiments is described in further detail in U.S. patent application Ser. No. 14 214 545 filed Mar. 14 2014 and now published as U.S. Patent Publication 2015 0263952 which is incorporated herein by reference.

Upon receiving the output set of routes from the route processing engine the table mapping engine of some embodiments generates the information to distribute to the network elements e.g. managed forwarding elements and managed gateways residing on the host machines in order for the network elements to implement the logical router. This data may include flow entries sent to the managed forwarding elements specifying e.g. to forward packets with certain network addresses to certain logical ports as well as routing table information for the gateways e.g. data tuples defining a routing table for an IP stack operating in a namespace . In addition to flow entries that implement the logical routing table specifying to forward packets to a particular logical port the table mapping engine of some embodiments also generates flow entries that map the logical port to physical interfaces so that packets can be sent across the physical managed network between managed forwarding elements.

The controller distributes the data for the logical router and other data for e.g. other logical forwarding elements such as logical switches that attach to the logical router generated by the table mapping engine to the host machines via the state distribution interface . In some embodiments the controller distributes the data through a hierarchy of other network controllers. For instance in some embodiments each logical network or each logical forwarding element is managed by a particular controller which may also manage other logical networks and each host machine is managed by a particular controller which may also manage other host machines . The controller computes the state e.g. flow entries for logical networks that it manages and distributes this data to the various controllers that manage the host machines implementing those logical networks. In other embodiments the state distribution interface interfaces directly with the host machines to distribute the data.

The state sharing interface of some embodiments allows the controller to share input configuration state information with other controllers that manage the various managed forwarding elements of the network. In the hierarchical network control system mentioned above the state sharing interface may be the same as the state distribution interface i.e. the controller controller interface . In some embodiments when the controller receives input configuration state through the API or receives configuration changes sent upwards from the host machine the controller shares this input state information with the other controllers so that the other controller can compute output state for the logical forwarding elements that they manage. Some embodiments share the input configuration state but do not share the output computed state.

An example operation of the network controller will now be described. In some embodiments a user inputs a configuration for a logical network which may include several logical switches connected to a logical router. Each logical switch connects to a logical port of the logical router and each logical port is assigned a subnet i.e. a range of network addresses . In addition at least one of the logical router ports connects to another logical router. The network controller receives the configuration data including the connection between logical routers through the input interface .

Based on the received configuration data the input interface stores configuration state in the state storage . With respect to the logical router the input interface stores 1 a connected route for each logical port for routing packets with network addresses in the range specified for the logical port to that logical port 2 any static routes specified separately in the configuration data and 3 an indication of the connection with the other logical router. In some embodiments the input interface also automatically defines a low priority default route for handling packets sent to network addresses for which routes are not otherwise defined e.g. to a logical gateway port . In other embodiments such a default route is only defined if input by a user.

Upon detecting the change in the configuration state stored in the state storage the table mapping engine begins generating new data tuples for distribution to the host machines in order to implement the logical network. In order to implement the connection between the logical routers the table mapping engine of some embodiments automatically generates dynamic routes for the input logical router based on input configuration data for the connected logical router. The connected logical router stores a set of routes e.g. its own connected routes and static routes which may have been input through the controller or through one of the other controller and shared with the controller .

The table mapping engine propagates these routes to the input logical router using the address of the connected logical router port as a next hop network address. For instance if the connected logical router stores a connected route that routes network addresses in a particular subnet to a particular port of the logical router the table mapping engine automatically generates a new route for the input logical router that specifies the connected logical router i.e. the address of a specific logical port of the connected logical router as a next hop for network addresses in the particular subnet. Some embodiments also propagate manually entered static routes of one logical router to another logical router.

Similarly connected routes for subnets attached to the input logical router as well as static routes of the input logical router are dynamically propagated to the connected logical router as dynamic routes that specify the input logical router as a next hop either by the table mapping engine or by the table mapping engine of a different network controller . In some embodiments different logical routers may have different master network controllers that perform state computation for the logical routers. That is a master network controller for a particular logical forwarding element takes the input configuration state for the logical forwarding element and generates the output state e.g. the flow entries and or other data tuples used to implement the logical forwarding element . Thus the dynamic propagation of routes from a first logical router to a second logical router might be performed by the controller while the propagation of routes from the second logical router to the first logical router might be performed by one of the other controllers .

For the case of provider and tenant logical routers some embodiments place restrictions on the routes that are dynamically propagated between logical routers. Specifically when a tenant logical router connects to a provider logical router some embodiments dynamically propagate the connected routes of the tenant logical router to the provider logical router such that the provider logical router will send packets to the subnets specified by the connected routes to that tenant logical router. However rather than dynamically propagating routes specifying information about the various other tenant networks that connect to the provider logical router the network controller only propagates a dynamic default route to the tenant logical router that sends all packets not otherwise routed by more specific routes to the provider logical router.

In the example operation of controller because dynamic routes and or static routes are defined for the input logical router the table mapping engine offloads the route traversal to the route processing engine . Specifically the table mapping engine sends to the route processing engine an input set of routes i.e. those defined by the configuration state including dynamic routes .

The route processing engine generates an output set of routes from the received set of input routes. Specifically the route processing engine identifies routes that are not in use e.g. lower priority routes that are superseded by higher priority routes for the same set of network addresses and recursively traverses the set of routes to identify a final action for each set of network addresses e.g. a drop packet action a final output port to which to send packets . The route processing engine returns the final route information to the table mapping engine .

The table mapping engine uses the final route information to generate flow entries and or data tuples defining the implementation of the logical router for the host machines . The table mapping engine provides these generated data tuples to the state distribution interface for distribution to the host machines e.g. directly to the host machines through a hierarchical network control system etc. .

The above description introduces the network controller of some embodiments for managing logical routers with static routing. Several more detailed embodiments are described below. First Section I introduces the implementation and configuration of logical networks via a network control system of some embodiments. Section II then describes the different classes of logical routers of some embodiments and Section III describes the dynamic propagation of routes between logical routers in some embodiments. Finally Section IV describes an electronic system with which some embodiments of the invention are implemented.

In some embodiments the network controllers e.g. the controller described above by reference to are part of a network control system used to manage numerous logical networks implemented in a physical managed network e.g. a private datacenter such as an enterprise site a public datacenter etc. . In such a managed network different tenants configure different logical networks which the network control system implements in a virtualized fashion over the same physical network while maintaining isolation between the logical networks. In addition some such managed networks use a provider logical router to manage connections between the virtual machines in the logical network and external hosts i.e. hosts that are not part of the tenant logical networks and are located outside the managed network . The tenants configure their logical network to connect to a provider logical router in order to send traffic to and receive traffic from such external hosts.

The datacenter provider logical router in addition to connecting to the logical routers and also includes two ports connecting to an external network or to separate external networks . In various examples the datacenter provider logical router may have only one port or numerous ports connecting to external networks.

In some embodiments each logical network is an abstract conception of a network generated by an administrator e.g. by each of the tenants and the logical network is implemented in a virtualized distributed manner in a managed physical infrastructure e.g. in a multi tenant datacenter . That is the virtual machines that connect to the logical switches may reside on various different host machines within the infrastructure and physical managed forwarding elements e.g. software virtual switches operating on these host machines implement some or all of the logical forwarding elements logical switches logical routers etc. . Thus the same host machine may host VMs from both of the logical networks and and the MFEs on these host machines would implement the logical forwarding elements as well as the logical forwarding elements .

A tenant logical router in some embodiments connects a set of logical switches to which virtual machines logically attach. Each logical switch or each logical port of the logical router to which a logical switch attaches represents a particular set of IP addresses i.e. a subnet and is implemented in the managed network across a set of managed forwarding elements MFEs to which the virtual machines physically connect e.g. through virtual interfaces . In some embodiments some logical routers are implemented in a centralized manner e.g. in one or more redundant gateways rather than distributed across the MFEs with the logical switches. In other embodiments the logical routers are implemented in a distributed fashion as well by the MFEs that connect to the virtual machines. Some embodiments specifically require that both tenant and provider logical routers be implemented in a distributed manner.

For a provider logical router which also connects to the external network via one or more ports the connections to the external network are implemented through the use of one or more gateways. The gateways in some embodiments are responsible for both sending data traffic from the managed network to the external unmanaged physical network and processing traffic sent from the external network into the managed network.

In addition to the virtual machines each of the hosts operates a managed forwarding element MFE . In some embodiments this MFE is a software virtual switch that operates within the virtualization software of the host e.g. Open vSwitch or another software forwarding element . Because the logical routers and are distributed the MFEs implement both the logical switches and as well as the logical routers and .

As shown because VMs from both the logical networks and reside on the first host the MFE implements i the logical switches to which these VMs connect ii other logical switches of these two logical networks iii the logical routers of these two logical networks and iv the provider logical router. On the other hand the second host only hosts a VM from the first logical network and therefore the MFE implements the logical forwarding elements of this logical network as well as the provider logical router . Implementing all of these logical forwarding elements in the MFE at the host enables first hop processing in some embodiments in which most or all of the logical forwarding element processing for a packet is performed at the first MFE that receives the packet. Thus a packet sent from VM to VM would be processed at the MFE through the logical switch to logical router to logical switch . The MFE would identify the logical egress port of logical switch for the packet as the port to which VM attaches and map this egress port to a tunnel to the MFE at host . For a packet sent from VM in logical network to VM in logical network which the sender may not realize is in a logical network hosted on the same virtualized infrastructure the MFE would process the packet through the logical switch to the logical router to the provider logical router then into the logical network through the logical router and then the logical switch at which point the packet would be sent through a tunnel to the MFE .

For traffic sent to an external destination i.e. not in either of the logical networks or the MFE identifies a logical egress port of the logical router as one of the ports that connects to the external network . The MFE then sends this traffic to one of the gateway hosts or depending on which port the external destination maps to i.e. depending on the routing table of the provider logical router . In some embodiments each of the gateway host machines and host a virtualized container e.g. a namespace and that has the ability to store a routing table and e.g. the rest of a network stack . These virtualized containers each correspond to a particular port of the provider logical router that connects to the external network handle traffic sent out of the managed network via that port or entering the network via that port.

In addition each of the virtualized containers and operates a route advertisement application e.g. a BGP daemon . The route advertisement application of some embodiments uses a dynamic routing protocol to advertise routes to external routers i.e. for the subnets of the logical ports of the tenant logical routers connected to the provider logical router in order to attract traffic for the network addresses specified by those routes. The route advertisement application of some embodiments is described in greater detail in the U.S. patent application Ser. No. 14 214 561 filed Mar. 14 2014 and now published as U.S. Patent Publication 2015 0263946 which is incorporated herein by reference.

The gateway host machines and in some embodiments also operate MFEs and . These MFEs perform first hop processing for packets received at the gateways from the external network . For example when a packet is received from a physical router the MFE first sends the packet to the appropriate namespace as multiple logical routers may have gateways operating on the gateway host machine based on a destination MAC address of the packet which performs its ingress processing and sends the packet back to the MFE. At this point the packet enters the logical network and the MFE performs logical network processing through the provider logical router the appropriate tenant logical router and the appropriate logical switch in order to identify a destination logical egress port then tunnels the packet to the appropriate MFE for delivery to the destination VM.

As described above these MFEs and gateways are provisioned in some embodiments by a network control system. One or more network controllers in the network control system receive the network configuration input by a user administrator and convert this information into flow entries and or data tuples that can be used by the MFEs and gateway host machines and distributes the data tuples to the host machines.

In some embodiments each of the controllers in a network control system is a computer e.g. with an x86 based processor with the capability to function as an input translation controller logical controller and or physical controller. Alternatively in some embodiments a given controller may only have the functionality to operate as a particular one of the types of controller e.g. as a physical controller . In addition different combinations of controllers may run in the same physical machine. For instance the input translation controller and the logical controller may run in the same computing device with which a data center management application interacts or with which an administrator interacts directly .

The input translation controller of some embodiments includes an input translation application that translates network configuration information received from a user. While shown as receiving the information directly from the user in in some embodiments a user interacts with a data center management application which in turn passes the network configuration information to the input translation controller.

For example a user may specify a network topology such as the logical network or shown in . For each of the logical switches the user specifies the machines that connect to the logical switch i.e. to which logical ports of the logical switch the VMs are assigned . The user may also specify which logical switches attach to any logical routers one or more logical ports of the logical router for connection to external networks or to other logical routers e.g. to a provider logical router and any configuration details for the logical router. For instance some embodiments enable the user to specify policies for the logical router. The input translation controller translates the received network topology into logical control plane data that describes the network topology as a set of data tuples in some embodiments. For example an entry might state that a particular MAC address A is located at a first logical port X of a particular logical switch that a tenant logical router Q is located at a second logical port Y of the particular logical switch or that a logical port G of the tenant logical router Q connects to a provider logical router.

In some embodiments each logical forwarding element e.g. each logical router logical switch etc. is governed by a particular logical controller e.g. logical controller . The logical controller of some embodiments translates the logical control plane data that defines the logical network and the logical forwarding elements e.g. logical routers logical switches that make up the logical network into logical forwarding plane data and the logical forwarding plane data into physical control plane data. The logical forwarding plane data in some embodiments consists of flow entries described at a logical level. For the MAC address A at logical port X logical forwarding plane data might include a flow entry specifying that if the destination of a packet matches MAC A to forward the packet to port X. The port of the logical router Q will also have a MAC address and similar flow entries are created for forwarding packets with this MAC address to port Y of the logical switch. Similarly for a logical router with a port K associated with a range of IP addresses C1 C24 the logical forwarding plane data might include a flow entry specifying that if the destination of a packet matches IP C1 C24 to forward the packet to port K.

In some embodiments the logical controller translates the logical forwarding plane data into universal physical control plane data. The universal physical control plane data enables the network control system of some embodiments to scale even when the network includes a large number of managed forwarding elements e.g. hundreds thousands to implement a logical forwarding element and when the network implements a large number of logical networks. The universal physical control plane abstracts common characteristics of different MFEs in order to express physical control plane data without considering differences in the MFEs and or location specifics of the MFEs.

As stated the logical controller of some embodiments translates logical control plane data into logical forwarding plane data e.g. logical flow entries that include a match over logical network parameters such as logical addresses logical ingress ports etc. then translates the logical forwarding plane data into universal physical control plane data. In some embodiments the logical controller application stack includes a control application for performing the first translation and a virtualization application for performing the second translation. Both of these applications in some embodiments use a rules engine for mapping a first set of tables into a second set of tables. That is the different data planes are represented as tables e.g. nLog tables and the controller applications use a table mapping engine e.g. an nLog engine to translate between the planes e.g. by applying join operations on the tables . The input and output tables in some embodiments store sets of data tuples that define the different planes of data.

In some embodiments the logical router processing entails recursive route traversal processes and various types of error checking that are not optimally performed by the table mapping engine. Specifically the configuration data for a logical router includes a set of input routes analogous to the routing information base of a physical router that must be narrowed to a set of output routes used to implement the routing table of the logical router analogous to the forwarding information base of a physical router . In some embodiments this set of output routes is part of the logical control plane data. In order to generate this logical control plane data for the logical router the table mapping engine of some embodiments offloads the route processing to a separate module in the logical controller implemented in a language better suited to such recursive and error checking generation actions. The route processing engine returns a set of output routes that the table mapping engine incorporates into its generation of logical forwarding plane entries.

Each of the physical controllers and is a master of one or more managed forwarding elements e.g. located within host machines . In this example each of the two physical controllers is a master of two managed forwarding elements located at the VM host machines . Furthermore the physical controller is a master of two gateway hosts and which host MFEs as well as the active and standby logical routers for a particular logical network. In some embodiments the active and standby hosts for a logical router are managed by the same physical controller as in this figure while in other embodiments separate physical controllers manage the different gateway hosts of a logical network.

In some embodiments a physical controller receives the universal physical control plane data for a logical network and translates this data into customized physical control plane data for the particular MFEs that the physical controller manages and which require data for the particular logical network. In other embodiments the physical controller passes the appropriate universal physical control plane data to the MFEs which have the ability e.g. in the form of a chassis controller running on the host machine to perform this conversion themselves.

The universal physical control plane to customized physical control plane translation involves a customization of various data in the flow entries. For the first example noted above the universal physical control plane would involve several flow entries i.e. several data tuples . The first entry states that if a packet matches the particular logical data path set e.g. based on the packet being received at a particular physical ingress port and the destination address matches MAC A then forward the packet to logical port X. This entry will be the same in the universal and customized physical control planes in some embodiments. Additional entries are generated to match a physical ingress port e.g. a virtual interface of the host machine to the logical ingress port X for packets received from the VM having MAC A as well as to match a destination logical port X to the physical egress port of the physical MFE e.g. again the virtual interface of the host machine . However these physical ingress and egress ports are specific to the host machine on which the MFE operates. As such the universal physical control plane entries include abstract physical ports while the customized physical control plane entries include the actual physical interfaces which in many cases are virtual interfaces of the specific MFEs.

In some embodiments as shown the gateway hosts also operate managed forwarding elements e.g. using the same packet processing virtual switching software as the VM hosts . These MFEs also receive physical control plane data from the physical controller that enables the MFEs to implement the logical forwarding elements. In addition some embodiments distribute the routing table data to the namespaces operating in the gateway hosts through the hierarchical network control system. The logical controller that manages the logical network selects the gateway host for the logical router in some embodiments e.g. using a load balancing algorithm that spreads the logical routers for various logical networks across a set of hosts .

The logical controller identifies the physical controller that manages the selected gateway host and distributes the routing table as well as any other information used for layer processing such as firewall information NAT etc. to the identified physical controller. In some embodiments the routing table is distributed as a set of data tuples. The physical controller then distribute these data tuples to the gateway host . The gateway hosts convert these data tuples into a routing table for use by a container e.g. a VM a namespace that operates on the gateway host as a logical router or L3 gateway.

The above describes the hierarchical network control system of some embodiments although the network control system of other embodiments includes only a single controller or a controller cluster with one active and one or more standby controllers . Some other embodiments include a cluster of network controllers that operate with each logical forwarding element or logical network assigned to a master controller and each managed forwarding elements assigned to a master controller but without the hierarchical arrangement shown in .

On the left side the input translation controller receives a network configuration through an API which is converted into logical control plane data. This network configuration data includes a logical topology such as that shown for logical network in . In some embodiments the network configuration may also include the specification of one or more static routes for a logical router and the connection of one logical router to another e.g. the tenant logical router to provider logical router connection . The network configuration specifies the various ports of the logical forwarding elements. In some embodiments each logical switch port is assigned a MAC address and an IP address and each logical router port is assigned a MAC address and an IP address and is associated with a particular subnet to which the IP address belongs . Some embodiments require that two logical router ports that connect to each other must be associated with the same subnet of at least a threshold specificity.

As shown the logical control plane data is converted by the logical controller specifically by a control application of the logical controller to logical forwarding plane data and then subsequently by a virtualization application of the logical controller to universal physical control plane data. In some embodiments these conversions generate a flow entry at the logical forwarding plane or a data tuple that defines a flow entry then add a match over the logical data path set e.g. the logical switch or router at the universal physical control plane. The universal physical control plane also includes additional flow entries or data tuples for mapping generic physical ingress ports i.e. a generic abstraction of a port not specific to any particular MFE to logical ingress ports as well as for mapping logical egress ports to generic physical egress ports. For instance for forwarding a packet to a logical router the flow entries at the universal physical control plane for a logical switch would include a forwarding decision to send a packet to the logical port to which the logical router connects when the destination MAC address matches that of the logical router port.

Similar to the examples in the previous paragraph for the logical switch the logical router flow entries identify a logical egress port based on a match over i the logical router pipeline i.e. that the packet has been forwarded to the logical router and ii the IP address. The mapping of IP address to logical port in some embodiments is based on the routing table generated at the logical controller for the logical router. For packets forwarded to a logical router port that faces an external network e.g. a port of a provider logical router the universal physical control plane additionally includes entries for mapping the logical egress port to a destination gateway and encapsulating the packet in a tunnel to the gateway host.

The physical controller one of the several physical controllers in the hierarchical network control system as shown translates the universal physical control plane data into customized physical control plane data for the particular MFEs that it manages at hosts and . This conversion involves substituting specific data e.g. specific physical ports for the generic abstractions in the universal physical control plane data. For instance in the example of the above paragraph the port integration entries are configured to specify the physical layer port appropriate for the particular L3 gateway connection e.g. an actual physical port and tunnel encapsulation information for the particular host machine on which the MFE operates .

The MFE at host one of several MFEs managed by the physical controller performs a translation of the customized physical control plane data into physical forwarding plane data in some embodiments. The physical forwarding plane data in some embodiments are the flow entries stored within a MFE e.g. within a software virtual switch such as Open vSwitch against which the MFE actually matches received packets. In addition the MFE at the gateway host performs such a translation in order to forward packets between the namespace and the other network entities e.g. VMs .

The right side of illustrates data propagated to the gateway hosts e.g. host to implement a logical router either a centralized logical router or a L3 gateway for a distributed logical router rather than to the MFEs. As shown the logical controller receives an input set of routes generates an output set of routes and then translates the output set of routes into routing data tuples from these routes.

In some embodiments the input set of routes is generated by either the logical controller or the input translation controller from the network configuration input by the user e.g. the administrator . When a user designs the logical network such as network each logical switch has an associated IP subnet. From this the logical controller automatically generates routes to each of these logical router ports that attach to the logical switches e.g. if IP 10.0.0.0 24 send to Port J . In addition when a tenant logical router connects to a provider logical router the logical controller of some embodiments generates a low priority default route to send packets to the provider logical router when the packets do not match any other routes. For a provider logical router the logical controller of some embodiments generates a low priority default route to send packets to one of the ports connecting to the external network when the packets do not match any other routes. The logical controller may have data in some embodiments that identifies a physical router in the external network so that the default route sends packets to the identified router.

In addition in some embodiments the logical controller that manages a first logical router generates dynamic routes for the first logical router when the first logical router connects to a second logical router. In some embodiments for each connected route of the second logical router the master controller for the first logical router generates a dynamic route specifying for the first logical router to logically forward packets with a destination address matching the prefix of the connected route to the second logical router. Some embodiments also propagate connected routes similarly. In the specific case of a tenant logical router TLR connecting to a provider logical router PLR some embodiments dynamically propagate to the PLR the connected routes of the TLR for each port to which a logical switch attaches. However in order to maintain isolation between the multiple logical networks that may attach to a PLR the PLR routes are not propagated to the TLR. Instead only a dynamic default route is propagated to the TLR which sends all otherwise unrouted packets to the PLR.

Before calculating the flow entries or the routing data tuples for the logical router the logical controller of some embodiments generates an output set of routes based on the input set of routes. In some embodiments a route processor in the logical controller recursively traverses the set of input routes to identify final output actions for each set of network addresses.

For a distributed logical router much of the output set of routes is specified as part of the logical control plane data and converted into physical control plane data as shown on the left side of . In this case the routing data tuples for distribution to the L3 gateways will still include the routes to the external physical router s as well as routes for processing incoming packets received via the connection with these external routers.

In addition to the routes themselves the logical controller also generates a set of data tuples that defines the logical routers. For instance when a logical router is created the logical controller of some embodiments selects at least one gateway host then creates a new data tuple i.e. a record that specifies the new namespace or other container on the host for the logical router. In addition some embodiments specify in the data tuple that routing is enabled for the namespace as opposed to or in addition to other services such as DHCP .

Once the logical controller creates the data tuples and identifies the gateway host or hosts that will receive the data tuples the logical controller then identifies the physical controller that manages the gateway host. As mentioned like the VM hosts each of the gateway hosts has an assigned master physical controller. In the example of the gateway host is managed by the physical controller so the other physical controller does not receive the logical router data tuples. In order to supply the logical router configuration data to the gateway hosts the logical controller of some embodiments pushes the data to the physical controller . In other embodiments the physical controllers request the configuration data e.g. in response to a signal that the configuration data is available from the logical controller.

The physical controller passes the data to the gateway host much as it passes the physical control plane data. In some embodiments the routing data tuples are sent to a database running on the host that is part of the software associated with the MFE and used to configure certain aspects of the MFE e.g. its port information and other non flow entry configuration data . Some embodiments use a first protocol e.g. OpenFlow to pass the flow entries for the MFE to the hosts while using a second protocol e.g. OVSDB to pass the configuration and routing table data to the hosts. The namespace or other container implementing the logical router retrieves the appropriate information from the database on its host or has the appropriate information passed to it. In some embodiments a process on the gateway host translates the data tuples stored in the database into a routing table and other network stack data e.g. a standard Linux network stack including a routing table for the namespace.

The above description describes the conversion by the network control system of the network configuration into a set of physical forwarding plane flow entries that the physical controller passes to the host e.g. via a protocol such as OpenFlow . In other embodiments however the data for defining flow entries is passed in other forms such as more abstract data tuples and the MFEs or processes running on the hosts with the MFEs convert these data tuples into flow entries for use in processing data traffic.

As indicated above the network control system e.g. the network controllers of some embodiments enables the connection of different types of logical routers. These different types of logical routers may be defined in the network control system of some embodiments as different classes of objects or as the same type of objects with different values for a router type parameter. In some embodiments the different types of logical routers are arranged hierarchically. For instance some embodiments include a first type of logical router to which logical switches may attach and a second type of logical router that may connect to external physical networks. In addition some embodiments may include one or more additional types of logical routers e.g. for placement between these first two logical router types.

In some embodiments the network control system provides the option of tenant logical routers and provider logical routers. TLRs are those that are part of a tenant logical network and to which the tenant that owns the logical network may attach logical switches via the network control system interface . In some embodiments TLRs may not have gateway ports that is they may not directly attach to external networks. PLRs are those that enable the tenant logical networks to reach the external networks. That is the PLRs accept TLR connections and may have one or more gateway ports attaching to external networks. Packets sent from a tenant VM will be first processed by a TLR pipeline after logical switch processing then sent to the PLR logical pipeline for processing if destined for an external network address. The PLR logical pipeline identifies a gateway port as the egress port for the packet and sends the packet to the gateway host machine associated with that gateway port.

When a TLR is connected to a PLR some embodiments automatically propagate dynamic routes to the PLR such that the PLR routes packets to the TLR for the various subnets served by the TLR and automatically propagate a dynamic default route to the TLR that causes the TLR to send packets to the PLR when not otherwise routed by a more specific or higher priority route. When this information is generated the network control system of some embodiments generates from the connected dynamic and static routes flow entries and data tuples used to implement the logical router by the network elements managed by the network control system.

The API of some embodiments provides an interface through which the controller receives configuration state data for one or more logical networks. In some embodiments the API represents a set of methods that may be used to create modify delete query etc. logical network data in the state storage . In some embodiments a network administrator may access the controller through a direct connection e.g. by manually inputting the API calls or through a cloud management application. In the case of a cloud management application in some embodiments the administrator configures a logical network through a graphical interface or other intuitive interface of the application which translates the data received from the user into the appropriate API calls to the controller .

Some such API methods for the API include methods to create a logical router create logical router ports create a logical switch attach a logical switch to a logical router e.g. to a TLR connect a TLR port to a PLR port attach a logical port of a logical router e.g. a PLR to an external network create a static route modify a static route remove a static route query a logical router for its routes etc. These various methods in some embodiments enable the administrator to access or modify configuration state data stored in the state storage . While the API enables an administrator to query dynamic routes in some embodiments dynamic routes may not be input through the API as the routes would then be static routes . In addition the connection of two logical routers e.g. a PLR and a TLR causes the network controller to generate dynamic routes as explained below.

In some embodiments the state storage stores a set of objects or other data structures that define logical networks managed by the controller . Based on commands received by the API the controller creates objects in the state storage . The state storage of some embodiments is a network information base NIB described in detail in U.S. Patent Publication No. 2013 0058356 which is incorporated herein by reference although other embodiments use different storages for storing state information at the network controllers. In addition to storing the configuration state received through the API the state storage of some embodiments also stores computed state calculated by the state computation module .

Furthermore the network controller may receive state information from i other network controllers and ii network elements e.g. MFEs through interfaces with these entities not shown . In some embodiments other logical controllers may receive configuration state through their APIs and share the configuration state information with the controller e.g. through an RPC channel . In some such embodiments the network controllers only share configuration state and do not share computed state. This enables an administrator to input configuration data for a particular logical forwarding element or logical network into any of the network controllers as the configuration data will be shared with the network controller that manages the particular logical forwarding element s and only that network controller will compute state for the logical network based on the input configuration state. The managed forwarding elements and namespaces hosting gateways may also provide state information e.g. regarding links that are down the amount of traffic processed for particular logical networks etc. to their managing physical controller e.g. through one of the channels used to provision the MFE or namespace . When a physical controller receives this information in some embodiments the controller identifies the appropriate logical controller to which to provide the data which is then stored in the state storage of the controller e.g. storage .

The state computation module or set of modules of some embodiments allows the controller to compute additional state information based on i the configuration state received through the API ii previously computed state e.g. in a series of computations and iii state propagated upwards to the network controller from the physical managed forwarding elements. In some embodiments the state computation module is a table mapping engine e.g. the table mapping engine described above by reference to . The state computation module may also include a route processing engine for recursively traversing configuration state routes to generate computed state routes. In some embodiments the state computation module generates dynamic routes based on a PLR TLR connection and stores these routes as input routes for the respective PLR and TLR.

The operation of the controller to receive configuration state including the attachment of a TLR to a PLR and process that configuration state will now be described by reference to . conceptually illustrates the receipt of a logical network configuration by the controller . As shown the API receives a configuration for a logical network through one or more API calls. The logical network as illustrated includes two logical switches that attach to a logical router which also connects to an external network e.g. through a L3 gateway port . The logical switches include several ports to which VMs attach. In addition the logical network configuration indicates that one of the logical router ports connecting to a first logical switch is assigned the subnet 11.0.0.0 24 and the other logical router port connecting to a second logical switch is assigned the subnet 10.0.0.0 24. The logical router a TLR does not have a connection to an external network.

Though shown as a single set of data in some embodiments the API receives separate commands to create each of the logical forwarding elements logical switch A logical switch B and the logical router . In addition in some embodiments the attachment of each logical switch to the logical router is a separate command as is the attachment of each VM to the logical switch.

As illustrated the TLR object of some embodiments includes its list of ports which attach to the logical switches A and B. The TLR object may specify whether the logical router is implemented in distributed or centralized fashion in some embodiments as well as the type of logical router i.e. TLR or by setting a flag that indicates either tenant or provider . In addition the API creates a set of routes as part of the logical router object. In some embodiments each of the routes is also an object owned by the logical router object stored in the state storage . As shown the set of routes includes routes automatically generated by the API based on the logical network configuration . Specifically for each of the logical ports connecting to a logical switch L3 gateway etc. the API generates a connected high priority route for the network address range e.g. IP addresses associated with that port. While shown as being performed by the API in this figure in some embodiments the API simply creates the ports and connections in the state storage and the state computation module generates the connected routes based on the network address ranges specified for the ports.

In this case one of the routes is for the port to which logical switch A attaches. This route routes network addresses that fall in the range given in Classless Inter Domain Routing CIDR format 11.0.0.0 24 to the logical output port X. In addition the route object of some embodiments specifies the type of route e.g. connected because the route is based on a specific logical port of the TLR the action to take for packets with destination IP addresses in the prefix range accept in this case though other routes may specify to drop packets and the priority of the route. In some embodiments connected routes are given the highest priority with static routes a lower priority though this may be manually input in some embodiments and dynamic routes a still lower priority. In addition to the route the set of routes also includes a similarly structured route to send IP addresses in the range 10.0.0.0 24 to logical output port Y.

Though not shown in this figure the state computation module of some embodiments identifies that the configuration state stored in the state storage has changed and subsequently retrieves this data in order to compute state information for distribution to the managed forwarding elements. The state computation module generates flow entries and or data tuples to distribute to the managed forwarding elements and namespaces and distributes this computed information e.g. via a set of physical controllers . The state computation module also stores the computed state back into the state storage .

Because in this case the configuration input to attach the TLR to the PLR is correctly specified the API stores this connection between the ports in both the tenant logical router object as well as a provider logical router object which already exists in the state storage as the provider logical router would have been created previously by an administrator of the managed network that is managed by the controller . For the TLR the API stores a new logical port with the IP address 18.0.0.1 on the subnet 18.0.0.0 28. As such either the API or the state computation module generates a connected route for all packets on the subnet 18.0.0.0 28 to be sent to the new logical port. In addition the API stores e.g. as a property of the logical port the connection to the PLR port with IP address 18.0.0.2. For the PLR object the API stores a new logical port if the port was not already created on the PLR and similarly generates a connected route to the subnet . Furthermore the API stores the connection to the TLR port having IP address 18.0.0.1

As a result of the connection the state computation module e.g. the table mapping engine component identifies the updates to the TLR object and retrieves the connection data for the TLR. As a result the state computation module generates a new dynamic default route for the TLR and stores this information in the input routing table of the TLR . As shown the dynamic default route sends all packets using the prefix 0.0.0.0 0 to the IP address of the PLR 18.0.0.2. Some embodiments set a low priority e.g. using a high number for this route. This allows the user to specify a static default route that will have the same specificity 0.0.0.0 0 but a higher priority for outgoing packets. For instance a user might use a direct host return port to send outgoing packets from the logical network directly to a physical router outside the managed network rather than using the PLR gateways which are a potential bottleneck . Such direct host return ports of some embodiments are described in U.S. patent application Ser. No. 14 068 658 filed Oct. 31 2013 and now published as U.S. Patent Publication 2015 0103838 which is incorporated herein by reference.

This figure assumes that the controller is the master controller for both the TLR and the PLR. If on the other hand a different controller is the master for the PLR then the state computation module would not compute the dynamic routes for the PLR. Instead as the controller would have shared this data with other controllers in the network control system including the master controller for the PLR the PLR master controller would compute the dynamic routes for the PLR and perform the subsequent route processing for the PLR to identify the output routing table for the PLR .

As shown the process begins by receiving at a configuration for a new logical router for a particular tenant network. In some embodiments the controller receives this configuration data through an API call to create a new logical router as well as additional calls to configure ports of the logical router e.g. with MAC addresses IP addresses and subnets attach logical switches to some of the new ports and or connect a port to an existing provider logical router. Some embodiments specify the existence of one or more provider logical routers to tenants. For example through the API some embodiments allow tenant administrators to query for PLRs available to accept connections with the TLR.

Based on the configuration information the process generates at a connected route for each port of the new logical router using the specified network address prefix of the subnet to which the port is connected. Thus in the above example the logical port X identified at the network controller by e.g. a UUID is associated with the subnet 11.0.0.0 24 and therefore the controller generates a connected route to send packets for this subnet to the port X of the logical router. The process generates similar routes for each of the logical ports of the new TLR.

Next the process determines at whether any connection to a PLR is specified for any of the ports of the TLR. When no such connection is specified the process ends. In some embodiments if the datacenter tenant wants the logical network to receive packets from external networks then a connection to a PLR is required i.e. because tenants may not have connections to L3 gateways . However a tenant administrator might initially specify the logical router then attach the TLR to a PLR at a later time at which point the remainder of process would be performed .

When the specified logical router includes a connection to a PLR the process identifies at the prefix and network address of the TLR port and PLR ports that are connected. This process in some embodiments is performed by the API upon receiving the specification of the connection. That is some embodiments perform operation prior to operation when a TLR to PLR connection is specified as part of the received configuration data.

The process determines at whether the ports meet a set of specified requirements for the PLR TLR connection. For example some embodiments require that the ports have different IP addresses that the ports have the same prefix assigned and that this prefix not overly specific. For instance some embodiments require that the prefix be at least 30 i.e. that the prefix have three bits available for IP addresses. Other embodiments do not place any such requirements on the size of the subnet. When the ports do not meet the connection requirements the process returns at an error for an invalid connection. Some embodiments return an error message to the source of the command e.g. a management application an administrator computer etc. that attempted to create the PLR TLR connection. The process then ends.

Next the process generates at a dynamic default route for the TLR that specifies the network address of the PLR port as the next hop address. That is the route sends all packets that do not match any other routes at the TLR using the prefix 0.0.0.0 0 to the PLR. As the TLR already includes a connected route to output packets for the prefix containing this next hop address to the TLR port connected to the PLR then route processing by the network controller will resolve this route to a default route to output all otherwise unrouted packets to the logical port that connects to the PLR.

In addition for the PLR the process generates at a dynamic route for each prefix associated with the other ports of the TLR. The process then ends. These dynamic routes specify the connected port of the TLR as the next hop address. The route in the above example is such a route. Because the TLR has a connected route for the prefix 11.0.0.0 24 the PLR receives a dynamic route for this prefix with the next hop IP address 18.0.0.1. Whenever the administrator for the TLR adds a new logical port the controller generates a new connected route for the TLR and subsequently a new dynamic route for the connected PLR. As the PLR will already have a connected route to output packets for the prefix containing the next hop address of the dynamic routes to the PLR port connected to the TLR then route processing by the network controller will resolve the dynamic routes of the PLR to output packets sent to network addresses in the TLR subnets to the logical port that connects to the TLR.

Though not shown in these figures as described above by reference to e.g. the network controllers in addition to generating routes storing the routes and performing route processing for the logical routers also generates and distributes data to the various managed network elements that implement the logical networks within a managed network. These network elements include managed forwarding elements MFEs and managed L3 gateways in some embodiments. As the TLRs are implemented in distributed fashion in some embodiments the controller or set of controllers generates flow entries for the appropriate MFEs i.e. those at the host machines on which the VMs of the tenant network reside in order for the MFEs to implement the TLR and the tenant logical switches . When the TLR is connected to a PLR the controller or set of controllers generates flow entries for the PLR for those MFEs as well. The PLR flow entries are distributed to host machines at which VMs of other logical networks connected to the PLR reside. In addition the network controller generates data tuples to define and configure one or more L3 gateways for the PLR e.g. one L3 gateway for each port of the PLR that connects to an external network.

The flow entries for the MFEs follow a match action format in some embodiments. That is each flow entry specifies i a set of match conditions e.g. source and or destination MAC addresses source and or destination IP addresses logical context data such as ingress or egress ports of specific logical forwarding elements etc. and ii a set of actions to perform on the packet e.g. write data to a register resubmit packet encapsulate the packet in a specific tunnel header etc. .

In some embodiments the network control system distributes flow entries to the MFEs in order for the MFEs to perform first hop processing. For first hop processing the first MFE that receives a packet e.g. the MFE on the same host as the VM that sent the packet or the MFE on a gateway host for incoming packets performs as much of the logical pipeline as possible for a packet before sending the packet out over the managed network. For traffic between VMs in a logical network the first hop MFE typically identifies a logical egress port of a logical switch that corresponds to the destination address. If the two VMs are on different logical switches then the first hop MFE performs the logical processing pipelines for the source logical switch logical router e.g. a TLR and the destination logical switch.

The first example shown in illustrates a packet sent by VM to an external destination. As shown VM initially sends a packet to the MFE e.g. through a VNIC . Both the VM and the MFE are located at the host . The packet includes a payload source and destination MAC addresses source and destination IP addresses and a time to live TTL counter. As sent by the VM the source MAC and IP addresses are those of VM the destination MAC address is that of the TLR to which the logical switch attaches the default gateway for the VM and the destination IP address is an external address located outside the logical network.

At the MFE the first stage of processing performed on the packet is ingress context mapping . The ingress context mapping stage represents one or more flow entries that identify a logical context for the packet . In some embodiments the ingress context mapping flow entry identifies a particular logical port of the logical switch based on the physical ingress port through which the packet was received e.g. the VNIC by which the VM attaches to the MFE .

While processing a packet the MFE of some embodiments stores the packet data in registers. That is up on receiving a packet the various components of the packet are written into register fields for access during processing. The MFEs repeatedly match the packet to a flow entry perform the action s specified by the flow entry then resubmit the packet through a dispatch port for additional processing while keeping the data in registers associated with the packet. In the case of the ingress context mapping the flow entry specifies to write the identified logical ingress port of the logical switch into a register.

Based on the ingress context mapping the MFE then performs the logical L2 processing pipeline of the logical switch . In some embodiments this processing pipeline involves an ingress ACL e.g. to ensure that the source MAC and or IP address of the packet match the logical port specified by the ingress context mapping entry a logical forwarding decision and an egress ACL entry. In some embodiments the logical forwarding decision for an L2 pipeline uses the destination MAC address to identify a logical egress port of the logical switch and the egress ACL entry determines whether to allow the packet through the identified egress port e.g. based on the destination MAC and or IP address . This egress port connects to a logical port of the TLR and therefore another flow entry specifies the TLR ingress port in a register field for the packet as shown on the right side of the figure. At this point only the logical context of the packet has been modified as the MAC addresses IP addresses and TTL fields remain unchanged.

Based on the specified TLR ingress port the MFE next performs the logical L3 processing pipeline of the tenant logical router . In some embodiments like the logical switch processing the L3 processing pipeline involves an ingress ACL entry a logical forwarding decision and an egress ACL entry. In some embodiments the ingress ACL entry ensures that the source MAC and or IP addresses of the packet are allowed for the ingress port. The flow entries for the logical forwarding decision implement the TLR routing table as determined by the network controller. That is the logical forwarding flow entries stored by the MFE include entries for each of the connected routes to the different logical switches as well as the default route to the PLR. In this case because the destination IP address is an external address i.e. not in either of the subnets associated with the logical switches and the packet matches a flow entry specifying the TLR logical port that connects to the PLR as the logical egress port. As this egress port connects to the logical port of the PLR another flow entry specifies the PLR ingress port in a register field for the packet as shown on the right side of the figure.

In addition as part of the logical forwarding flow entry or entries the MFE modifies the packet during the processing stage . Specifically as the packet has now been processed by a router the MFE decrements the TTL by one. In this example the packet initially has a TTL of 64 which is decremented once by each router that processes the packet. Different embodiments may use different initial TTLs e.g. 255 100 etc. . In addition as part of routing the packet the processing pipeline changes the source and destination MAC addresses of the packet. Specifically the source address is now that of the TLR egress port and the destination MAC is now that of the PLR ingress port. The TLR flow entries indicate that the next hop IP address for the utilized route is that of the PLR ingress port. Rather than using Address Resolution Protocol ARP because the logical network is known to the controller that generates the flow entries the ARP response with the MAC address to be used as the destination MAC for the packet can be directly embedded in the flow entry to begin with in some embodiments.

Next based on the specified PLR ingress port the MFE performs the logical L3 processing pipeline of the provider logical router . In some embodiments like the TLR processing the PLR processing pipeline involves an ingress ACL entry a logical forwarding decision and an egress ACL entry. Much like with the TLR the ingress and egress ACL ensure that the packet should be allowed in the identified ingress port of the PLR and out of the egress port identified by the logical forwarding entry. The logical forwarding entries implement the routing table of the PLR as determined by the network controller. Thus the PLR flow entries include entries for forwarding packets with destination IP addresses in the logical switch subnets to the PLR port that connects to the appropriate TLR for the logical switch subnet based on the dynamic routes propagated to the PLR as well as routes for sending other packets to one or more gateway ports of the TLR. In this case because the destination IP address is an external address the PLR does not route the packet to any of the TLRs to which it connects. Instead based on either its default route static routes input for the PLR through the API or dynamic routes learned from external physical routers through route advertisement protocols e.g. BGP OSPF etc. the PLR forwarding entries identify one of the gateway ports of the PLR as the logical egress port.

In addition as with the TLR the MFE modifies the packet during the processing stage . Specifically the TTL has been decremented again such that it now has a value of 62. Even though only one physical forwarding element the MFE has processed the packet its TTL has been decremented twice during that processing. In addition the PLR has modified the source and destination MAC for the packet again e.g. by modifying register values . The source MAC is now that of the L3 gateway port for the logical router and the destination MAC is the L3 gateway itself which in some embodiments is assigned a MAC separate from the logical port . Using the L3 gateway MAC for the destination address ensures that when the MFE at the gateway host machine receives the packet that MFE will send the packet to the appropriate namespace for gateway processing. Again rather than the MFE determining this MAC address through ARP the network controller directly embeds the destination MAC address information into the flow entries.

Finally at this point the egress port identified based on the logical forwarding portion of the PLR processing does not map to another logical forwarding element flow table implemented by the MFE at the host . Instead the packet now matches an egress context mapping flow entry that maps the packet to a physical destination i.e. a particular gateway host machine . In addition a physical forwarding flow entry which may be part of the egress context mapping or a separate flow entry specifies the actual tunnel information to use in order to reach the MFE at this gateway host machine. As such before sending the packet out of the host and into the managed network the MFE encapsulates the packet in a tunnel e.g. using STT GRE or another tunnel encapsulation technique . This is shown in the figure by the tunnel source and destination IP addresses. The logical context is also maintained on the packet e.g. stored within a particular field of the tunnel header for use by the MFE at the gateway host machine.

The MFE sends the encapsulated packet out into the managed network to the gateway host machine. In some embodiments the MFE at the gateway host machine forwards the packet to the namespace that implements the L3 gateway having removed the tunnel encapsulation and logical context data. The namespace performs its processing to route the packet for delivery to a particular physical router and then sends the packet back to the MFE which handles the output of the packet onto the connection with the physical network.

At the MFE the first three stages ingress context mapping logical L2 processing for the logical switch and logical router processing for the TLR are the same as in the example of . For processing by the TLR the destination IP of VM is treated the same as an external IP as it is not on any of the subnets connected to the TLR . After the pipeline the TTL has been decremented to 63 the source MAC is that of the TLR egress port and the destination MAC is that of the PLR ingress port.

The L3 processing pipeline of some embodiments for the PLR involves an ingress ACL logical forwarding decision and an egress ACL. However unlike in the previous example this time the packet matches a flow entry implementing a dynamic route with the TLR of the logical network as the next hop. The IP address of VM the destination address of the packet is in the subnet associated with the TLR port connecting to the logical switch and therefore the packet matches the flow entry implementing the dynamic route propagated to the PLR for this subnet. As such in addition to decrementing the TTL to the PLR processing sets the source MAC of the packet to that of the PLR port connected to the TLR and sets the destination MAC of the packet to that of the TLR port connected to that PLR port. The processing subsequently identifies this TLR port as the new ingress port stored with the logical context of the packet.

The MFE then performs the logical L3 processing stage for the TLR based on this logical ingress context. As with the previously described logical router pipelines the L3 processing involves an ingress ACL logical forwarding and an egress ACL. In this case the logical forwarding decision identifies the destination IP address of VM as belonging to a subnet for which a flow entry implements a connected route to the logical router port to which the logical switch attaches . Based on this IP address the logical forwarding entry or entries specifies to identify the egress port of the logical router decrement the TTL to and modify the source and destination MAC addresses. The source MAC address is changed to that of the TLR port attached to the logical switch and the destination MAC address is changed to that of VM . Rather than sending an actual ARP request to VM to determine its MAC address in some embodiments the MFE already stores this information.

Finally the MFE performs logical L2 processing for the logical switch . Here the only changes made to the packet are to identify a logical egress port of the logical switch based on the destination MAC address that corresponds to VM and write this information into a register. At the egress context mapping stage the MFE maps this egress port to a physical destination the attached VM and subsequently delivers the packet without a logical context to the destination VM.

The above section described the use of provider logical routers and tenant logical routers which exemplified one specific example of a network controller performing logical dynamic routing. In some embodiments the only types of network controller connections allowed are those between PLRs and TLRs as described above. However the network control system of some embodiments provides users with the ability to create multiple logical routers as part of a single logical network and connect these routers to each other. This enables the administrator of the tenant network to assign different policies to different logical routers. For example for packets from a first logical switch the tenant might want to use policy routing to route packets based on size while for packets from second and third logical switches the tenant might not care about the size of the packets. Connecting these logical switches to two different logical routers allows the tenant to easily implement the different policies at the different logical routers.

The API provides an interface through which the controller receives configuration state data for one or more logical forwarding elements. As described above by reference to in some embodiments the API represents a set of methods that may be used to create modify delete query etc. logical network data in the state storage . In some embodiments a network administrator may access the controller through a direct connection e.g. by manually inputting the API calls or through a cloud management application. Some such API methods for the API include methods to create a logical router create logical router ports create a logical switch attach a logical switch to a logical router attach a logical port to an external network attach logical router ports of two routers to each other create a static route query a logical router for its routes etc. These various methods in some embodiments enable the administrator to access or modify configuration state data for logical routers stored in the state storage .

In some embodiments the state storage stores a set of objects that define logical forwarding elements managed by the controller as well as configuration state for logical forwarding elements managed by other network controllers. Based on commands received by the API the controller creates modifies and deletes objects in the state storage . In addition to storing the configuration state received through the API the state storage of some embodiments also stores computed state calculated by the state computation module e.g. dynamic input routes generated by the table mapping engine output routes generated by the route processing engine etc. . Furthermore the network controller may receive state information from other network controllers and network elements e.g. MFEs gateways operating in namespaces as described above by reference to .

The table mapping engine performs state calculations for logical networks managed by the controller . As shown the table mapping engine includes a flow generation module and a configuration data generation module both of which generate data for distribution to the managed forwarding elements and L3 gateways. In some embodiments both of these modules use similar input data tuples to generate output data tuples but generate different data for distribution to the various network elements. In addition the table mapping engine performs dynamic routing for connected logical routers in some embodiments. The table mapping engine of some embodiments uses database join operations to generate the data tuples describing dynamic routes and then stores these dynamic routes as objects in the state storage . For example for a first logical router connected to a second logical router that itself has a connected route the table mapping engine of some embodiments would perform an operation to join the prefix of the connected route with the port of the second logical router as a next hop IP address to create a dynamic route for the first logical router.

The flow generation module generates data for the managed forwarding elements to use in processing packets. Specifically in some embodiments the flow generation module generates flow entries that take a match action format. That is each flow entry specifies a condition or set of conditions for a packet to match and an action or set of actions for a managed forwarding element to apply to the packet when the packet matches all of the conditions. For instance one of many flow entries used to implement a logical router might specify that if a packet i has been assigned to the logical router and ii has a destination IP address in a particular range e.g. 10.0.0.0 24 then take the actions of i writing a particular logical egress port into a register for the packet and ii resubmit the packet to the managed forwarding element for further processing. In some embodiments the flow generation module generates the flow entries by performing table mapping operations e.g. join operations on the data stored in the state storage as well as information received from the route processing engine . In some embodiments the flow generation module of the table mapping engine outputs data for distribution via the OpenFlow protocol.

The configuration data generator generates data for both the managed forwarding elements as well as the namespaces in which logical routers and L3 gateways operate in some embodiments. For the managed forwarding elements the configuration data may include port and or tunnel configuration among other data. Whereas the MFEs receive packet processing data as flow entries however the namespaces that implement L3 gateways for distributed logical routers receive packet processing instructions in the format of data tuples distributed in the same manner as configuration data. For instance for a namespace the gateway host machine on which the namespace resides receives the definition of the namespace as a data tuple generated by the configuration data generator and receives its routing table and other network stack configuration in this format as well in some embodiments. As with the flow generation module the configuration data generator of some embodiments generates configuration data by performing table mapping operations e.g. join operations on the data stored in the state storage as well as information received from the route processing engine . In some embodiments the configuration data generator outputs data for distribution via the OVSDB protocol.

The route processing engine of some embodiments receives a set of input routes from the table mapping engine e.g. routes automatically generated based on the subnets to which the logical ports of the logical router connect static routes received through the API dynamic routes generated by the table mapping engine based on logical router connections and performs a recursive traversal process on the routes in order to identify a final logical destination for each network address range routed by the logical router. When multiple input routes provide contradictory information for a particular network address or range of addresses the route processing engine of some embodiments determines which route has a higher priority. Some input routes may provide a next hop address rather than output port for a route. In these cases the route processing engine recursively traverses the set of input routes until reaching a route specifying either a destination output port or a drop packet action. The route processing engine returns the set of output routes with final actions e.g. drop packet send to particular output port specified for each route. In some embodiments the route processing engine is implemented in a language different from the table mapping engine e.g. C .

The controller distributes the flow entries and configuration data for the logical router and other data for e.g. other logical forwarding elements such as the logical switches of the logical network generated by the table mapping engine to host machines via the state distribution interface . The host machines shown in the figure include a first machine for hosting VMs and a second machine for hosting namespaces to implement L3 gateways. Both of the host machines and include managed forwarding elements for processing packets e.g. OVS while the gateway host also includes the namespaces for the L3 gateways.

In some embodiments the controller distributes the data through a hierarchy of other network controllers as shown above in . In such embodiments the state distribution interface is an interface with other controllers that act as intermediaries for the distribution of data to the host machines and possibly perform additional translation of the data tuples . In some embodiments the controller uses a Remote Procedure Call RPC channel to communicate with other controllers.

In other embodiments the controller interfaces directly with the host machines and as well as numerous other host machines to distribute the data. In some such embodiments the controller uses two channels for communication with each host machine a first channel e.g. OpenFlow for distributing the flow entries generated by the flow entry generation module for use by the managed forwarding elements and a second channel e.g. OVSDB for distributing the configuration data generated by the configuration data generator .

The data flow through the network controller during its operation to process logical router information will now be described. includes several encircled numbers which indicate the flow of different data into through and out of the network controller . One of ordinary skill in the art will recognize that the controllers of some embodiments will process data other that that which is shown and that the data flow in this figure is meant to represent the operations performed and data transferred specifically relating to the two logical routers managed by the network controller .

As shown by the encircled the API receives a command to create or modify the configuration of the logical router . Specifically in this example the command modifies the routes stored for the logical router . The command could be the creation of a new static route the creation of a new port for the logical router the modification of the subnet to which a logical port connects etc.

As a result shown by the encircled the API modifies the data structure stored for the logical router e.g. an object such as a C object in the state storage . The figure illustratively shows the logical router data structure as storing a RIB set of input routes and FIB set of output routes . While some embodiments use such a structure other embodiments store data structures e.g. objects for each input route owned by the logical router. After processing in some such embodiments the logical router also stores a status data structure e.g. object for each route. Other such embodiments modify the route data structure after processing to include the status data.

When the configuration state of the logical router data structure is modified the table mapping engine retrieves the state of the logical router as shown by the encircled in the figure. The table mapping engine performs several operations based on this new configuration data. Because the controller is the master controller for the second logical router the controller generates a new dynamic route for the logical router and modifies the data structure stored for the logical router in the stage storage as shown by the encircled . The table mapping engine modifies the input set of routes for the data structure by e.g. creating a new route object for the generated dynamic route.

In addition to generating the dynamic route for the second logical router the table mapping engine processes the new input route for the first logical router . Rather than computing state e.g. flow entries etc. for the first logical router the table mapping engine passes the route data to the route processing engine as shown by the encircled .

The route processing engine performs a route selection and traversal operation in order to identify the output routing table for the logical router. In some embodiments the route processing engine takes as input each new or modified route for the logical router and outputs status data for each route. For instance for a new static route that specifies a next hop IP address the route processing engine determines whether to use the new route and if in use a final output port for the route or a final action of blackhole i.e. drop packets for the route. The route processing engine returns the output set of routes to the table mapping engine as shown by the encircled .

At this point the table mapping engine performs several actions. The output routing data computed by the route processing engine is stored in the logical router data structure as shown by the encircled . This figure conceptually illustrates this data as being stored in the FIB. The conceptual RIB for input configuration data and FIB for output data represent analogies to the RIB to FIB conversion performed by physical routers in traditional networks.

The table mapping engine also generates both flow entries and configuration data using the output routing data provided by the route processing engine . A route specifying that a particular network address range is routed to a particular logical port will be encoded as a match on the destination address over the network address range and an action to send the packet to the logical port with a particular next hop IP address in some cases simply encoding the MAC address to which the next hop IP address corresponds into the flow entry to avoid the need for an ARP request . In some cases the logical router also specifies other data e.g. routing policies etc. which the table mapping engine encodes in flow entries as well. Because the MFEs operate on both the VM hosts and the gateway hosts the table mapping engine distributes the flow entries to both the host and the host through the state distribution interface as shown by the encircled though at least some of the flow entries distributed will be different between the two hosts .

In addition because the first logical router connects to an external network this router has a L3 gateway implemented in a namespace on the host . As such the table mapping engine uses the output routing data from the route processing engine to generate configuration data for the namespace. This configuration data in some embodiments i defines the existence of the namespace and ii provides configuration information for the network stack in the namespace including the routing table. Thus the output routing data from the route processing engine is used to generate a set of data tuples defining a routing table for the namespace that implements a L3 gateway for the logical router. This data is distributed to the gateway host through the state distribution interface as shown by the encircled . As described above both the flow entry data tuples and the configuration data tuples may be distributed through a hierarchy of network controllers rather than directly from the controller to the host machines and e.g. through two different network controllers that manage the two different host machines and .

Because the table mapping engine generated a new input route for the second logical router the dynamic route stored as shown by the encircled new route traversal and data tuple generation and distribution is required for this logical router as well. As such shown by the encircled the table mapping engine retrieves the input state from the logical router object . While no new routes are present that require the propagation of any dynamic routes to the first logical router the other operations described for the first logical router are performed by the network controller .

The table mapping engine sends the input route data to the route processing engine shown by the encircled which returns the output route data shown by the encircled . At this point the table mapping engine stores the output routing data in the data structure as shown by the encircled . In addition the flow generation module of the table mapping engine generates new flow entries for the second logical router and distributes these to the host machines and shown by the encircled . Although the second logical router does not have any gateway ports packets could arrive at the L3 gateway on the host from an external network with a destination address of either VM and VM . In this case in order for the MFE at the gateway host to perform first hop processing it requires the flow entries for the second logical router . However no additional configuration data for a L3 gateway implemented in a namespace is distributed to any gateway host as no gateways are present for this logical router.

Although the operations pertaining to implementing the first logical router are shown as completely preceding the operations pertaining to the second logical router other than the dynamic route propagation one of ordinary skill in the art will recognize that these operations may be performed simultaneously or in overlapping fashion. For instance the table mapping engine might process the output data from the route processing engine in order to generate flow entries and configuration data for the first logical router while the route processing engine performs route traversal for the second logical router.

Whereas illustrates the case in which the controller is the master of both logical routers conceptually illustrates two network controllers and which are respectively the master controllers for the first and second logical routers and as well as the data flow through the controllers upon receipt at the first controller of a new route for the first logical router . The first network controller includes an API a state storage a table mapping engine a route processing engine a state distribution interface and a controller to controller interface . Similarly the second network controller includes an API a state storage a table mapping engine a route processing engine a state distribution interface and a controller to controller interface .

Most of the illustrated components perform the same functions as those described above for the network controller in . The controller to controller interfaces and enable the exchange of configuration state data between the controllers and . In some embodiments controllers exchange only configuration state i.e. state received through the API and do not exchange computed state e.g. dynamic routes or other information generated by the table mapping engine flow entries and configuration data etc. . The controller controller interfaces and in some embodiments are RPC interfaces for communicating over a RPC channel.

The data flow through the network controllers and begins in a similar way to the operations shown for . As shown by the encircled and the API of the first controller receives new route information e.g. a static route a new logical port for the first logical router and stores this information in the input route information of the logical router object .

Because this information is new configuration state data the controller automatically shares the change to the logical router object with the controller through a communication over the controller controller interfaces and shown by the encircled . As a result the newly received data for the logical router is stored in the copy of the logical router object stored in the state storage .

At the first controller the table mapping engine retrieves the updated data from the input set of routes for the first logical router from the state storage shown by the encircled but does not compute any dynamic routes for the second logical router as this controller is not the master of the second logical router. Instead the table mapping engine performs the processing for the first logical router sending the data to the route processing engine and receiving the output route information back shown by the encircled and then storing this data in the logical router object shown by the encircled and generates flow entry data and configuration data for distribution to the host machines and shown by the encircled and .

Correspondingly at the second controller the table mapping engine retrieves the updated data from the input set of routes for the first logical router as well shown by the encircled . Although the table mapping engine does not perform any route processing or flow generation for the first router the table mapping engine does propagate the new route to the input routing table of the second logical router and store this in the object for the second logical router . shown by the encircled 

At this point as the input routing data for the second logical router has changed the table mapping engine retrieves this data from the state storage shown by the encircled . The table mapping engine sends this input routing data to the route processing engine and receives output routing data in return as shown by the encircled and . Finally the table mapping engine stores this data in the logical router object shown by the encircled and generates and distributes flow entries for the second logical router shown by the encircled . For the reasons described above by reference to the controller distributes flow entries to both the appropriate VM hosts and gateway hosts .

The above figures conceptually illustrate the data flow through the network controller or controllers to perform dynamic routing and other logical router related operations . conceptually illustrates a process performed by a network controller of some embodiments to process a new input route for a logical router received at the controller. Various different operations of the process may be performed by different modules of the network controller e.g. the API the table mapping engine etc. .

As shown the process begins at by receiving input routes for a particular logical router. These input routes may be received through the API at the network controller e.g. via a communication from a cloud management application or a direct communication from a network administrator logging into the controller or via a state sharing mechanism from another network controller e.g. RPC channel in the same network control system e.g. managing the same datacenter . The received set of input routes might be one or more static route configurations the configuration of one or more logical ports for the logical router e.g. to create a new logical port to change the interface to which a logical port connects etc. that causes the automatic creation of new connected routes etc.

The process then determines at whether the new route was received through the API or whether the route was received via the state sharing mechanism from another controller. When the route is received through the API the process distributes at the route to other network controllers with which the controller shares configuration state data. In some embodiments network controllers in a cluster operate in a fully connected mesh and therefore when one controller receives configuration state through its API that controller shares the data with each of the other controllers in the cluster. As such if a first controller receives configuration data from a second controller then the first controller need not re share the configuration state. For controllers interconnected differently in a network control system different variations on the process may be used in order to ensure that all controllers receive the input configuration state.

After sharing the input data as required the process determines at whether the controller that performs the process is the master of the particular logical router. In some embodiments this operation is performed by the table mapping engine of the controller. When the table mapping engine retrieves new configuration data the engine performs an operation to determine whether the configuration data is for a logical forwarding element identified e.g. by its UUID for which the controller is a master. When the controller is not the master of the particular logical router the process proceeds to operation described below.

When the controller is a master of the particular logical router for which the input configuration data is received the process calculates at output routes for the logical router. In some embodiments as shown in the previous the output routes are calculated by the route processing engine which performs a recursive route traversal operation to identify the output routes. For instance if a new static route to a particular next hop network address is received the route processing engine identifies either an output logical port for the route or a drop action by traversing through the other routes specified for the particular logical router.

After calculating the output routes the process distributes at the output routes to the network elements that implement the particular logical router. As described above in some embodiments this entails computing data tuples for both flow entries to provide to the managed forwarding elements at the VM host machines and the gateway host machines and configuration data to provide to the gateway host machines for the namespaces operating as L3 gateways on those host machines when the particular logical router has one or more L3 gateways . After computing the data tuples the controller distributes the data either directly to the host machines or through a set of network controllers e.g. the hierarchical network control system described above by reference to .

Next the process determines at whether the controller performing the process is a master of any other logical routers to which the particular logical router connects. As shown in two logical routers may be connected within a logical network. In some cases different controllers will be the masters of the different logical routers within a logical network or a PLR and TLR will have different master controllers. When either the particular logical router does not connect to any other logical routers in which case no dynamic routing needs to be performed or the controller is not the master of any logical routers to which the particular logical router connects the process ends without performing any dynamic routing.

When the controller is the master for at least one logical router that connects to the particular logical router for which the new route information was received the process dynamically propagates at the new route information to each of these other logical routers for which it is the master . In some cases the particular logical router might connect to multiple other logical routers to which it propagates routes but the present controller is only the master of one of these routers in which case it handles the propagation of the routes to that logical router while a different controller handles the propagation of the routes to another logical router.

For a second logical router connected to the particular logical router when the new route information for the particular logical router is a static route that lists a next hop address other than the address of the port of the second logical router to which the particular logical router connects the process propagates the static route to the second logical router. If the new route is a connected route for a prefix other than that to which both of the logical routers connect in order to connect with each other then the process propagates the dynamic route to the second logical router. For either of these cases the next hop network address for the dynamic route is that of the port of the particular logical router to which the second logical router connects. In addition when propagating a static route some embodiments provide additional route length information for use in performing route traversal. In traditional BGP advertised routes may include a number of the autonomous systems that a packet will travel through when following a particular route. Similarly when dynamically propagating routes some embodiments provide at least an indication as to whether the route being propagated is a connected route or a static route that sends packets either to a L3 gateway or another logical router. In the latter case the logical router that receives the dynamic routes may have a different more direct way of reaching the routed prefix.

After propagating the routes to the other logical routers the process calculates at updated output routes for the other logical routers. In some embodiments as shown in the previous the output routes are calculated by the route processing engine which performs a recursive route traversal operation to identify the output routes. For instance when a route is dynamically propagated to a logical router the route processing engine identifies whether to use the dynamic route and if using the route then identifies either an output logical port for the route or a drop action by traversing through the other routes specified for the particular logical router.

After calculating the output routes the process distributes at the output routes to the network elements that implement the particular logical router. As described above in some embodiments this entails computing data tuples for both flow entries to provide to the managed forwarding elements at the VM host machines and the gateway host machines and configuration data to provide to the gateway host machines for the namespaces operating as L3 gateways on those host machines when the particular logical router has one or more L3 gateways . After computing the data tuples the controller distributes the data either directly to the host machines or through a set of network controllers e.g. the hierarchical network control system described above by reference to . The process then ends.

As mentioned conceptually illustrates a logical network which includes three logical routers all of which connect on the subnet 18.0.0.0 28. The first logical router has a port W on this subnet with an IP address of 18.0.0.1 the second logical router has a port T on this subnet with an IP address of 18.0.0.3 and the third logical router has a port Q on this subnet with an IP address of 18.0.0.2. In addition the first logical router has two logical switches attached a first logical switch on the subnet 10.0.0.0 24 attached to its port X and a second logical switch on the subnet 11.0.0.0 24 attached to its port Y. The second logical router has a third logical switch on the subnet 12.0.0.0 24 attached to its port V and the third logical router has a fourth logical switch on the subnet 13.0.0.0 24 attached to its port P.

Finally the first and third logical routers and each have one gateway port connecting to an external physical router. The first logical router has a port Z on the subnet 20.0.0.0 28 with an IP address of 20.0.0.1 which connects to an interface of a physical router with an IP address of 20.0.0.2. The second logical router has a port S on the subnet 22.0.0.0 28 with an IP address of 22.0.0.1 which connects to an interface of a physical router with an IP address of 22.0.0.2.

As shown the first routing table includes four connected routes for the four logical ports of the first router . Thus for example the first connected route specifies to send all packets with addresses matching the prefix 10.0.0.0 24 to the logical port X while the fourth connected route specifies to send all packets with addresses matching the prefix 20.0.0.0 28 to the logical port Z i.e. onto the external network . The second routing table includes only two connected routes and as the second logical router only has two logical ports T and V. The third routing table has three connected routes for its three logical ports one with a logical switch attached a second connecting to the subnet with the other routers and a third connecting to the external network.

Each of the routing tables also includes a static default route. The first routing table includes a static route that specifies to send all packets the prefix 0.0.0.0 0 to the next hop 20.0.0.2 which is the physical router interface to which the gateway for the logical router connects on the physical network 20.0.0.0 28. Similarly the third routing table includes a route that specifies to send all packets to the next hop 22.0.0.2 which is the physical router interface to which the gateway for the logical router connects on the physical network 22.0.0.0 28. The routing table includes a static default route that specifies 18.0.0.1 the port W of the first logical router as the next hop IP for all otherwise unmatched packets. Thus packets sent from VM and VM will be logically routed through the first logical router rather than the third logical router unless a more specific route is present.

Finally the routing tables and each include several dynamic routes. The first routing table for the first logical router includes three dynamic routes a first route propagated from the logical router for the prefix 12.0.0.0 24 based on one of its connected route with a next hop address of 18.0.0.3 and second and third dynamic routes and propagated from the third logical router for the prefixes 13.0.0.0 24 and 22.0.0.28 based on its two connected routes both with a next hop address of 18.0.0.2. Neither of the default routes or are propagated to the logical router as the routing table already includes a default route for the prefix 0.0.0.0 0 some embodiments do propagate this route but it would not be in use because of a higher priority static route already present in the routing table . Furthermore connected routes for the subnet connecting the routers 18.0.0.0 28 are not propagated.

Similarly the second routing table for the second logical router includes five dynamic routes. The dynamic routes and correspond to the connected routes of the first logical router while the dynamic routes and correspond to the connected routes of the third logical router . For the third logical router the third routing table has four dynamic routes. The dynamic routes correspond to the connected routes of the first logical router while the dynamic route corresponds to the connected route of the second logical router .

Shortly after configuring the logical network as shown in and inputting the default routes shown in the network control system would have generated the routing tables and and subsequently performed route processing generated flow entries and gateway configuration data and distributed this data to the managed forwarding elements and gateways in order to implement the logical network in a managed network.

As shown at the master controller for each of the other logical routers and this static route is propagated to the routing tables and other logical routers. Thus the routing table for the second logical router receives a new dynamic route that specifies to send packets matching the prefix 23.0.0.0 24 to the next hop address 18.0.0.1 the port W of logical router . The routing table for the third logical router receives a similar dynamic route that also specifies to send packets matching the prefix 23.0.0.0 24 to the next hop address 18.0.0.1. As this route is more specific than its default route out the gateway port when VM or VM sends a packet to an address in the subnet 23.0.0.0 24 the managed forwarding element that receives the packet will process the packet through the pipelines for both the third logical router and the first logical router and send the packet to the L3 gateway for the first logical router .

In a new static route is received at one of the network controllers for the third logical router . The static route specifies to send packets matching the subnet 27.0.0.0 24 to the next hop 18.0.0.1 i.e. the port W of the logical router . This causes packets sent from VM or VM to addresses in this subnet to be processed by the first logical router and sent out the L3 gateway for the first logical router rather than out the L3 gateway of the third logical router . In addition the master controller of the second logical router propagates this route to create a new dynamic route in the routing table which specifies a next hop IP address of 18.0.0.2 port Q of the third logical router . Thus a packet sent from VM or VM to an address in the subnet 27.0.0.0 24 would be processed by all three logical routers at the first hop MFE before being sent out the L3 gateway of the first logical router.

In some embodiments however the dynamic route propagation mechanism recognize that the second logical router is on the same subnet as the first logical router and third logical router and specify for the second logical router to send packets directly to the first logical router. Alternatively just as the route is not dynamically propagated to the routing table of the first logical router because the next hop IP address corresponds to the port of that logical router the route is also not propagated to the routing table of the second logical router because the next hop IP is on the same subnet 18.0.0.0 28 as the port T of the second logical router.

Many of the above described features and applications are implemented as software processes that are specified as a set of instructions recorded on a computer readable storage medium also referred to as computer readable medium . When these instructions are executed by one or more processing unit s e.g. one or more processors cores of processors or other processing units they cause the processing unit s to perform the actions indicated in the instructions. Examples of computer readable media include but are not limited to CD ROMs flash drives RAM chips hard drives EPROMs etc. The computer readable media does not include carrier waves and electronic signals passing wirelessly or over wired connections.

In this specification the term software is meant to include firmware residing in read only memory or applications stored in magnetic storage which can be read into memory for processing by a processor. Also in some embodiments multiple software inventions can be implemented as sub parts of a larger program while remaining distinct software inventions. In some embodiments multiple software inventions can also be implemented as separate programs. Finally any combination of separate programs that together implement a software invention described here is within the scope of the invention. In some embodiments the software programs when installed to operate on one or more electronic systems define one or more specific machine implementations that execute and perform the operations of the software programs.

The bus collectively represents all system peripheral and chipset buses that communicatively connect the numerous internal devices of the electronic system . For instance the bus communicatively connects the processing unit s with the read only memory the system memory and the permanent storage device .

From these various memory units the processing unit s retrieve instructions to execute and data to process in order to execute the processes of the invention. The processing unit s may be a single processor or a multi core processor in different embodiments.

The read only memory ROM stores static data and instructions that are needed by the processing unit s and other modules of the electronic system. The permanent storage device on the other hand is a read and write memory device. This device is a non volatile memory unit that stores instructions and data even when the electronic system is off. Some embodiments of the invention use a mass storage device such as a magnetic or optical disk and its corresponding disk drive as the permanent storage device .

Other embodiments use a removable storage device such as a floppy disk flash drive etc. as the permanent storage device. Like the permanent storage device the system memory is a read and write memory device. However unlike storage device the system memory is a volatile read and write memory such a random access memory. The system memory stores some of the instructions and data that the processor needs at runtime. In some embodiments the invention s processes are stored in the system memory the permanent storage device and or the read only memory . From these various memory units the processing unit s retrieve instructions to execute and data to process in order to execute the processes of some embodiments.

The bus also connects to the input and output devices and . The input devices enable the user to communicate information and select commands to the electronic system. The input devices include alphanumeric keyboards and pointing devices also called cursor control devices . The output devices display images generated by the electronic system. The output devices include printers and display devices such as cathode ray tubes CRT or liquid crystal displays LCD . Some embodiments include devices such as a touchscreen that function as both input and output devices.

Finally as shown in bus also couples electronic system to a network through a network adapter not shown . In this manner the computer can be a part of a network of computers such as a local area network LAN a wide area network WAN or an Intranet or a network of networks such as the Internet. Any or all components of electronic system may be used in conjunction with the invention.

Some embodiments include electronic components such as microprocessors storage and memory that store computer program instructions in a machine readable or computer readable medium alternatively referred to as computer readable storage media machine readable media or machine readable storage media . Some examples of such computer readable media include RAM ROM read only compact discs CD ROM recordable compact discs CD R rewritable compact discs CD RW read only digital versatile discs e.g. DVD ROM dual layer DVD ROM a variety of recordable rewritable DVDs e.g. DVD RAM DVD RW DVD RW etc. flash memory e.g. SD cards mini SD cards micro SD cards etc. magnetic and or solid state hard drives read only and recordable Blu Ray discs ultra density optical discs any other optical or magnetic media and floppy disks. The computer readable media may store a computer program that is executable by at least one processing unit and includes sets of instructions for performing various operations. Examples of computer programs or computer code include machine code such as is produced by a compiler and files including higher level code that are executed by a computer an electronic component or a microprocessor using an interpreter.

While the above discussion primarily refers to microprocessor or multi core processors that execute software some embodiments are performed by one or more integrated circuits such as application specific integrated circuits ASICs or field programmable gate arrays FPGAs . In some embodiments such integrated circuits execute instructions that are stored on the circuit itself.

As used in this specification the terms computer server processor and memory all refer to electronic or other technological devices. These terms exclude people or groups of people. For the purposes of the specification the terms display or displaying means displaying on an electronic device. As used in this specification the terms computer readable medium computer readable media and machine readable medium are entirely restricted to tangible physical objects that store information in a form that is readable by a computer. These terms exclude any wireless signals wired download signals and any other ephemeral signals.

While the invention has been described with reference to numerous specific details one of ordinary skill in the art will recognize that the invention can be embodied in other specific forms without departing from the spirit of the invention. In addition a number of the figures including conceptually illustrate processes. The specific operations of these processes may not be performed in the exact order shown and described. The specific operations may not be performed in one continuous series of operations and different specific operations may be performed in different embodiments. Furthermore the process could be implemented using several sub processes or as part of a larger macro process. Thus one of ordinary skill in the art would understand that the invention is not to be limited by the foregoing illustrative details but rather is to be defined by the appended claims.

