---

title: System for managing and scheduling containers
abstract: A system and method for a container service that obtains a software image of a software container that has been configured to be executed within a computer system instance registered to a cluster by one or more processors. The container service is configured to receive a request to launch the software image in accordance with a task definition, wherein the task definition specifies an allocation of resources for the software container. The container service may then determine, according to a placement scheme, a subset of a set of container instances registered to the cluster in which to launch the software image in accordance with the task definition. Upon determining the subset of the set of container instances, the container service may launch the software image as one or more running software containers in the set of container instances in accordance with the task definition.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09256467&OS=09256467&RS=09256467
owner: Amazon Technologies, Inc.
number: 09256467
owner_city: Seattle
owner_country: US
publication_date: 20141111
---
Companies and individuals seeking to reduce the expense and overhead associated with maintaining their own computing resources have turned instead to purchasing remote computing services such as remote program execution over multiple virtual machine instances and remote data storage offered by computing resource service providers to customers. Such remote computing services are often configurable and scalable to meet various computing needs of the customers of the computing resource service provider. However it can be difficult for a customer to manage various applications running on multiple virtual machine instances and such applications may not be portable to other computing systems or scalable to meet an increased need for resources.

In the following description various embodiments will be described. For purposes of explanation specific configurations and details are set forth in order to provide a thorough understanding of the embodiments. However it will also be apparent to one skilled in the art that the embodiments may be practiced without the specific details. Furthermore well known features may be omitted or simplified in order not to obscure the embodiment being described.

Techniques described and suggested include systems and methods for creating clusters of software container instances for running software containers for customers of a computing resource service provider. The software container instances may be virtual machine instances configured to support containerization and the software container instances may be registered or deregistered from the cluster as needed to fit the needs of the customer. Each software container instance may contain a container agent which is an application configured to when executed by one or more processors of a computer system perform various tasks with software containers and the software container instances including registering container instances deregistering container instances starting tasks stopping tasks providing task statuses signaling task heartbeats and reporting task events. Software containers may include data and one or more processes that when executed in a computer system instance configured to support containerization may be isolated from other processes running within the same computer system instance.

The software containers may be configured to run as tasks according to a task definition and the task definition may be stored as a task definition file. The task definition file may describe one or more software containers that are assigned to start as a group. Software images of the software containers which may represent an entire copy of the particular state of the software container at the time the software image was generated that have been configured to execute within the software container instances may be provided to the computing resource service provider or their locations specified in the task definition. The task definition may also specify resource requirements relationships between containers network ports used and shared resources.

Upon receiving a request to start the tasks of the task definition a scheduler may determine according to a placement scheme which software container instances within the cluster to run the tasks. In some cases the computing resource service provider may provide a multitenant scheduler for determining where to run software containers and in some cases the computing resource service provider may allow customers to provide and configure their own schedulers to customize how the scheduler operates. In some cases the scheduler may be configured use a random selection scheme to randomly according to some stochastic distribution scheme or in a round robin fashion select a container instance to host a given software container taking into account the resource requirements specified in the task definition. The container agents may be configured to start the tasks within their respective software container instances. In some embodiments a telemetry agent may be configured to collect data such as control group metrics and log event information about the running software containers and report that data to a telemetry service. The telemetry service may allow the customers to monitor the states of software containers within their container instances and customers may specify via the telemetry service actions to perform in response to various container events and criteria for raising alerts based on the collected data.

The described and suggested techniques improve the field of computing and specifically the fields of virtualization and containerization by providing a set of multitenant containerization services for managing software containers. Additionally the described and suggested techniques improve the functioning of computer systems by allowing resources of computing systems to be efficiently allocated and isolating software applications such that they do not compete with resources between each other. Moreover the described and suggested techniques offer meaningful advantages over general virtualization systems by improving portability providing multitenant services and auto scaling resources as demand increases or decreases.

The customer may be an individual associated with a customer account of the container service or may be an individual system resource computing device or other entity authorized to act on behalf of the individual associated with the customer account. Furthermore the resources of the customer may be made available to other users. For example the customer may utilize the container instance to provide an online marketplace for other users and these other users may then buy and or sell goods and services through the online marketplace provided by the customer through the container service. The customer may communicate with the container service of the computing resource service provider through the network which may be a communication network such as the Internet an intranet an internet service provider ISP network and or some other such network as described below.

In some examples a task definition or task definition file may refer to file specifying a set of linked containers i.e. a set of containers that when running on a host computing system are associated with each other that are assigned to start as a group. The task definition file may further specify disk and network locations that the containers are able to share on a single physical machine. The task definition file may then be utilized for launching the set of containers . In some implementations the task definition file may define and link containers spread across multiple physical machines. One task definition file may contain and schedule many tasks. In some examples a task may refer to an instantiation of a task definition file and may consist of one or more containers . Tasks may be modified by applying a new task definition to the task.

The task definition file may contain all the information needed to place containers in a cluster and the cluster may be managed through application programming interface calls.

The example task definition specifies that a first task entitled db has a software image located at the path forest postgresql. The first task is allocated processing shares of 1 000 and 1 gigabyte of memory and the first task uses port . Similarly the task definition also specifies that a second task entitled web has a software image located at the path hub.web.com rails latest. The second task is allocated processing shares of 1 000 and 1 gigabyte of memory and the second task uses ports 8000 8000. The task definition notes that the second task web is allowed to link to the first task db . Note that while some units used in this example are given as a fixed number such as the processing capacity given as a fixed number of central processing unit shares it is contemplated that other units and other types of values e.g. percentages of total processing capacity percentages of total memory could be used instead to allow for dynamic resource allocation.

The container service may be a service provided by the computing resource service provider to allow the customer to execute the containers within the cluster . The container service may be similar to the container service described in conjunction with . The computing resource service provider may be a computing resource service provider similar to the computing resource service provider described in conjunction with and may provide one or more computing resource services to its customers individually or as a combination of services of a distributed computer system. The one or more computing resource services of the computing resource service provider may be accessible over the network and may include services such as virtual computer system services block level data storage services cryptography services on demand data storage services notification services authentication services policy management services task services and or other such services. Not all embodiments described include all of the services described and additional services may be provided in addition to or as an alternative to services explicitly described.

As one example the computing resource service provider may be an entity that provides computing resources such as data storage services virtual computer system services and or web services. Customers of the computing resource service provider may communicate with one or more of the services via an interface which may be a web services interface or any other type of customer interface. Each service provided by a computing resource service provider may have its own interface and subsets of the services may have corresponding individual interfaces in addition to or as an alternative to a common interface.

In some examples a container instance may refer to a computer system instance virtual or non virtual such as a physical computer system running an operating system that is configured to launch and run software containers. Thus the container instance may be configured to run the containers in accordance with the task definition file provided by the customer . One or more container instances may comprise a cluster. In some examples cluster may refer to set of one or more container instances that have been registered to i.e. as being associated with the cluster. Thus the container instance may be one of many different container instances registered to the cluster and the other container instances of the cluster may be configured to run the same or different types of containers as the containers . The container instances within the cluster may be of different instance types or of the same instance type and the customer may have more than one cluster. Thus the customer may launch one or more clusters and then manage user and application isolation of the containers within each cluster through application programming interface calls. Further detail about clusters may be found in the description of .

A container also referred to as a software container or isolated user space instance may be a lightweight virtualization instance running under a computer system instance that includes programs data and system libraries. When the container is run the running program i.e. the process is isolated from other processes running in the same computer system instance. Thus the containers may each run on an operating system e.g. using memory CPU and storage allocated by the operating system an operating system of the container instance and execute in isolation from each other e.g. each container may have in isolated view of the file system of the operating system . Each of the containers may have its own namespace and applications running within the containers are isolated by only having access to resources available within the container namespace. Thus containers may be an effective way to run one or more single applications within their own namespace. A container encapsulation system allows one or more containers to run within a single operating instance without overhead associated with starting and maintaining virtual machines for running separate user space instances. An example container encapsulation system is the Docker container engine.

The containers may be launched to have only specified resources from resources allocated to the container instance that is a container may be launched to have a certain amount of memory and to not utilize more than a specified amount of processing power. The resource allocation for the containers may be specified in the task definition file . Multiple containers may run simultaneously on a single host computer or host container instance and the resources of the host can be allocated efficiently between the containers. In some embodiments a host may support running containers in container instances from only one customer. In other embodiments a single host may allow multiple customers to have container instances running on the host. In the latter case the container service may provide security to ensure that the customers are unable to access containers clusters or container instances of the others.

Different types of tasks may have different resource requirements and may have different lifespans. Thus the containers may be dynamically scheduled to run by a scheduler service in the container service independent of an underlying operating system of the container instance and as such the underlying operating system of the container instance may be very basic. Alternatively the containers may be scheduled to run by a scheduler installed within the container instance of the cluster .

The other services may be services such as services and of the computing resource service provider described in conjunction with . Likewise the other resources may include resources that can be shared between virtualized instances such as a storage volume of a block level data storage service.

Each of the container instances may be configured to contain a pair of agents the container agent and a telemetry agent that may allow containers to be managed may provide cluster state information and may enable logging and telemetry data collection. The container service may be comprised of a set of services including a template service one or more schedulers a router service and a telemetry service . Because the container service may be configured as a multitenant service i.e. the resources of the container service may serve multiple customers concurrently and because the services provided by the container service exist outside and separate from the container instances the customers need not install individual software applications within their respective container instances to provide the functionality afforded by the services. The customers may utilize the functionality provided by these services by making web service application programming interface function calls through the front end service which may be configured to receive requests from the customers and forward the requests to the appropriate service such as the appropriate container manager backend service .

As noted a customer may launch one or more clusters and then manage user and application isolation within each cluster through the front end service . For example the customer may specify that instances should comprise a first cluster and instances should comprise a second cluster. Thereafter when the customer submits a launch request for a container the customer may specify whether the container should be launched in the first cluster or the second cluster.

Each cluster may have a cluster identifier ID that uniquely distinguishes the clusters from each other. Clusters may be specified by referring to their respective cluster ID. A customer may use different clusters for different types of tasks. For example a customer may use a first cluster for launch services and may use a second cluster for executing batch jobs. The instances in the first cluster may have been optimized for running a server and the instances of the second server may have been optimized for submitting short lived transient jobs like batch jobs.

Available application programming interface function calls for clusters may include CreateCluster DescribeCluster ListClusters and DeleteCluster. The CreateCluster application programming interface call may cause a cluster to be created. In some cases after the cluster is created the front end service may provide the customer with the cluster ID of the newly created cluster. The cluster ID may allow the customer to register container instances with the cluster for example the customer may cause a container instance be registered to a cluster by passing the cluster ID as a parameter to a RegisterContainerInstance application programming interface call. Additionally or alternatively the customer may specify one or more existing container instances belonging to the customer as parameters to the CreateCluster application programming interface call to cause the one or more existing container instances to be registered to the cluster in concurrence with creating the cluster. Similarly in some cases the customer may specify in parameters of the CreateCluster application programming interface call a quantity of container instances to be created and registered to the cluster upon creation of the cluster. The CreateCluster application programming interface function call is further illustrated in .

Once a cluster is created and running the DescribeCluster application programming interface call may be used by the customers or applications to request information about a cluster. Information returned in response to the DescribeCluster application programming interface call may include a list of what applications are running in the cluster resources available to the cluster and their types. Information returned from the DescribeCluster application programming interface call may then be used as parameters for a StartTask application programming interface call to launch a task within the described cluster.

The scheduler may be configured to launch tasks within the described cluster. Alternatively the customers may implement their own scheduler rather than the scheduler and use the information retrieved by the DescribeCluster application programming interface call in conjunction with the placement logic of the customer scheduler to make placement decisions for scheduled tasks. The customer may design and or upload a placement algorithm to be used by his her own scheduler or may select from a list of possible placement algorithms provided to the customer by the computing resource service provider. The container service may determine based on available resources whether to accept or reject placement decisions made by the customer scheduler.

The scheduler may be a multitenant service configured to optimally schedule a set of tasks to run with a cluster. In this way the customers need not choose where the tasks should be executed. The placement scheme of the scheduler may be configured to distribute tasks evenly over the cluster e.g. round robin fashion stochastic distribution scheme etc. and may be configured to distribute tasks based on a current or projected resource consumption by the cluster in order to make the most efficient use of available resources. The scheduler may obtain cluster manager metadata and other information about the availability of the container instances in a cluster via the container manager backend services . The cluster manager metadata and other information may include data about the current state of the container instances assigned to the cluster available resources within the container instances containers running within the container instances and other information usable by the scheduler to make placement decisions. The DescribeCluster application programming interface call may cause the cluster manager backend service to provide the cluster metadata for the specified cluster.

The scheduler may distribute tasks based at least in part on security rules security groups and or security roles have affinity and anti affinity. For example tasks within a certain security group may be configured to be launched on the same container instance whenever possible or practicable or conversely may be scheduled to be launched in separate container instances. As another example tasks may be assigned to certain security groups or designated to have certain security roles and based on the particular security group assigned or security role designated a task may be assigned to run in a specific cluster or specific cluster instances or may have priority for resources over other tasks. Task distribution based on affinity anti affinity include assigning tags indicating relationships between containers container instances and tasks. As an example of distribution based at least in part on affinity two container instances may be tagged as general and one container instance may be tagged as database. In a task definition a task could be specified with a general launch constraint providing affinity that the task is to be launched into one or more of the container instances tagged as general. As an example of task distribution of anti affinity certain container instances may be tagged as secure credit card processing and certain other container instance may be tagged as http website. Because http website containers may be serving traffic from untrusted outside entities it may be desirable not to share such containers with credit card processing tasks. Thus a task definition be configured to indicate that http website is anti affinity to credit card processing tasks to ensure that credit card processing containers are not launched in http website tagged container instances. In this example the scheduler can be configured to parse any tags in a task definition and use it to filter out container instances that do not meet the criteria associated with the tag. Next the scheduler can select a container instance from the filtered set to host the container.

In some embodiments the scheduler may communicate directly with the container agents to launch tasks. In other embodiments the scheduler may communicate to the agent communication services which may then relay the communication to the container agent . In still other embodiments the scheduler may communicate placement decisions to the container manager backend services which may relay communicate directly to the container agent or to the agent communication services . As noted in some embodiments the customers may alternatively implement their own schedulers for task management rather than using the scheduler .

In some cases containers may be configured to compete for resources and in cases of conflict containers associated with certain security roles may have priority over other containers. Note that in all cases the placement scheme may take the available resources in the container instances and the resource requirements of the container into account. For example a container instance may not be selected for particular tasks if the particular tasks have resource requirements exceeding the available resources in the container instance. In some implementations however the container instance may still be selected but launching of the particular tasks may be delayed until enough resources become available within the container instance to support running the particular tasks.

Containers may also be configured to utilize other resources provided by the computing resource service provider. For example each container may be configured with a virtual network interface to allow the container to communicate through the virtual network to other containers and or other resources of the computing resource service provider. Likewise security credentials may be assigned on a per container basis so that containers have their own security credentials to access the other resources provided by the computing resource service provider. Additionally the container service may be configured to integrate with a load balancing service to load balance workloads directed at the containers. For example a customer may present a list of internet protocol addresses associated with virtual networks of the customer s containers and or customer instances to a load balancer of the load balancing service and instruct the load balancer to balance workloads between the internet protocol addresses. In addition the container service may be integrated with an auto scaling service to allow resources utilized by the containers to be dynamically adjusted as needed.

For example a customer may implement multiple schedulers for a single cluster and the multiple schedulers determine for which instances within the cluster to launch containers. The multiple schedulers may compete with each other according to a placement scheme such as by using an optimistic concurrency control method for determining where to launch containers. For example each scheduler using optimistic concurrency may assume that in general containers will not interfere with each other and in a situation where one container has modified data that another container has read the reading container may restart and re read the modified data.

In some cases a scheduler may be configured to maximize available resources. For example a cluster may be utilized primarily for running long running services and a need arises for running one or more short lived batch jobs. In such an example a scheduler configured to maximize available resources may look for an instance with just enough processing power to support the batch jobs as a best fit location for the batch jobs. Alternatively the scheduler may look for an instance utilizing the least of its processing power and schedule the batch jobs with this instance.

In some implementations a meta scheduler may be configured to run hierarchically on top of one or more schedulers and the meta scheduler may decide where to launch the batch jobs. For example in an implementation there may be two types of tasks short term batch job tasks and long term services. The two types of tasks may have different placement rules such as it may be desirable to distribute the service tasks evenly among the available container instances in one or more specified clusters whereas the batch job tasks may be configured to run in any available container instance with space processing cycles. In some cases a different scheduler may be assigned to each set of placement rules. However in cases where such placement rules are competing or in conflict with each other or in cases where an appropriate scheduler must be determined and assigned to a particular task the meta scheduler may determine which competing task should prevail synchronize information between schedulers or determine the appropriate scheduler for placing a task.

The cluster may be managed through application programming interface calls made to the front end service . For example an application programming interface call could be made to request a list of what clusters are available and what containers may be running on which clusters. The ListClusters application programming interface call may list all clusters to which a customer has access. The DeleteCluster application programming interface call may delete a specified cluster and is further described in reference to .

Available application programming interface function calls for container instances may include RegisterContainerInstance DescribeContainerInstance and DeregisterContainerInstance. A RegisterContainerInstance application programming interface call may be used by a container agent of a container instance to register the container instance with a cluster manager once the container instance is instantiated. In some implementations the cluster manager is comprised of metadata about the clusters e.g. the grouping of container instances . In other implementation the cluster manager may comprise configuration management software installed by a customer or made available by the computing resource service provider. The scheduler may then refer to the container instances registered with the cluster manager when determining into which container instance the containers should be launched. The RegisterContainerInstance is further described in reference to . The DescribeContainerInstance application programming interface call may return information about the container instance including resources available to the container instance and running tasks within the container instance.

Because containers may be run in any available container instance with sufficient resources in the cluster containers may be scaled up or down within the cluster as needed provided enough container instances are available. If the number of container instances in a cluster is insufficient additional container instances may be created and registered to the cluster through the cluster manager. If an overabundance of container instances exist in the cluster some container instances may be deregistered from the cluster via the cluster manager. The DeregisterContainerInstance application programming interface call may be used to deregister from a specified cluster a container instance that was registered using the RegisterContainerInstance application programming interface call. The DeregisterContainerInstance is further described concerning .

Available application programming interface function calls for task definitions may include RegisterTaskDefinition DescribeTaskDefinition ListTaskDefinitions and DeregisterTaskDefinition. The RegisterTaskDefinition application programming interface call may be used to register a task definition for launching tasks and is further described in conjunction with . The DescribeTaskDefinition application programming interface call may be used to return information about the task definition including the contents of the task definition file. The ListTaskDefinitions application programming interface call may return a list of task definitions available to a customer or may return a list of task definitions associated with a particular cluster ID or container instance ID. The DeregisterTaskDefinition application programming interface call may deregister a task definition that was registered using the RegisterTaskDefinition application programming interface call and is further described concerning .

Application programming interface function calls for tasks may include StartTask DescribeTask ListTasks and StopTask. A customer may pass a task definition file or their respective identifiers as parameters to the StartTask application programming interface call and may further specify a container instance or cluster to launch one or more tasks within the container instance or cluster. For example the customer may have obtained one or more container instance IDs of a cluster in response to a DescribeCluster application programming interface call and may specify to execute a task definition file on the one or more identified container instances. If a customer is running tasks directly they may call the StartTask application programming interface and specify the container instance to launch into. Alternatively the scheduler may use the StartTask application programming interface to launch tasks. In addition the customers may configure their own scheduler to use the StartTask application programming interface to launch tasks.

The scheduler or customer installed scheduler may also be configured to start tasks within a task definition file and determine where to place the tasks within the cluster. For example the scheduler may determine to distribute tasks evenly between the container instances of the cluster distribute tasks in some chi squared distribution or may distribute tasks among container instances of a cluster according to some other heuristic or set of constraints. Parameters passed to the StartTask application programming interface call may specify such as by passing a task definition file outlining multiple tasks that multiple tasks should be started. In some implementations the customer may rather than specify a single cluster or container instance specify a list of clusters or container instances and the scheduler may determine in which of the container instances to execute the tasks. The StartTask application programming interface call is described in further detail in reference to .

The DescribeTask application programming interface call may be used to return information about a running tasks from the container agents including information about allocated resources age and running state. The ListTasks application programming interface call may be used to return a list of all tasks running or a list of all tasks running in a specified cluster or container instance. The StopTask application programming interface call may be used to command the container agents to stop specified running tasks started using the StartTask application programming interface call. The StopTask application programming interface call is further described concerning .

The container service may interact with an authentication system of the computing resource service provider such as the authentication system of to authenticate application programming interface calls made to the front end service . In some embodiments separate security groups and security roles may be configured and assigned to different containers on a single host. The container service may also be configured to launch containers and container instances within a virtual private cloud.

In some embodiments the containers may be configured to attach to other services of the computing resource service provider such as block level data storage services and or on demand data storage services. In some cases containers may be configured to share attached services with other containers. As an example a container instance of a customer may be mapped to a block level storage volume of a block level storage service of a computing resource service provider and the containers within the instance may each be configured to be able to read from and or write to the block level storage volume. In some examples the block level storage volume may be shared between multiple container instances such as all container instances within a cluster or multiple clusters such that container instances within the cluster or clusters may all be able to share the block level storage volume with their running containers. As another example a container instance may be mapped to 20 different block level storage volumes but only two of the block level storage volumes are specified for the containers.

The template service may be configured to allow the customers to define a task definition for their containers. In some examples a task definition may refer to a script or set of metadata that may define a group of containers e.g. the number of containers their types their components their relationships to other containers information describing associated instances and other metadata. Task definitions may also specify that groups of containers are to be launched in coordination. The template service may receive task definitions from the customers store the task definitions in the database and allow the customers to create view update delete and otherwise manage their task definitions.

The template service may grant the customers the ability to define a task definition. The template service may allow customers with the ability to provide the task definition by uploading a task definition file or may provide the task definition by allowing customers to select from various options and or change default settings to dynamically create a task definition file. The template service may allow customers to register a task definition. The template service may also provide an editing interface for editing currently registered task definitions. The template service may register the task definitions at least in part by providing the task definitions to the container manager backed service to be stored in in the database .

In some examples a fleet may refer to a set of computer systems virtual or physical running instances such as the container instances of the present disclosure or other applications of the container service . A fleet may be subdivided into sub fleets and each sub fleet may be supported by a container manager back end service and an agent communication service dedicated to that sub fleet. The agent communication services may be configured to communicate with the container agents and telemetry agents running on container instances within the sub fleet.

The container manager backend services may be configured to provide other management services and resources to the sub fleet on the backend such as the cluster management software or cluster manager metadata described in the present disclosure. The container manager backend services may be configured to receive task definitions from the template service store the task definitions in the database receive cluster manager metadata from container instances or the agent communication services and provide task definition information and the cluster manager metadata to the scheduler or a customer installed scheduler upon request. The container manager backend services may be configured to provided information about a specified cluster such as cluster manager metadata in response to a DescribeCluster application programming interface call.

The agent communication services and container manager backends may be implemented on separate computer systems within the sub fleet separate virtual machine instances within the sub fleet may share the same computer systems and or virtual machine instances within the sub fleet or may run on computer systems separate from but in communication with their respective sub fleet. There may be multiple container instances per sub fleet. In some cases each sub fleet may represent a single cluster. In other cases clusters may span multiple sub fleets. In still other cases each sub fleet may host more than one cluster. The router service may be configured to route requests from the front end service to the appropriate sub fleet. In some embodiments the router service may route requests to a single sub fleet. In other embodiments the router service may route requests between multiple sub fleets.

The telemetry service may be configured to aggregate control group metrics e.g. information about the processes running within the containers and container logs and provide the aggregated metrics and logs to a resource monitoring service to allow the customers to monitor resource utilization such as processor storage and network usage of their respective container instances. Control group metrics include information such as the amount of memory used by processes of the containers number of times that a process triggered a page fault central processing unit usage by the processes of the containers time during which the central processing units were executing system calls on behalf of processes of the containers number of reads and writes by the processes of the containers and number of input output operations queued for the processes of the containers.

The container manager backend services may be configured to receive placement requests from the customers for their containers through the front end service and may ensure that the requested resources are available for the containers. The container manager backend services may then write the desired container state to the database . In some implementations the container manager backend services is responsible for the cluster manager metadata which may be stored in the database and provided to the scheduler or customer installed scheduler when requested. Additionally the container manager backend services may receive information from individual container agents regularly such as information related to life cycle events and heartbeats e.g. periodic signals sent by the container agents to indicate normal operation . In some cases this information may be communicated to the appropriate component or entity through the front end service . In some implementations the container agents communicate this information through the agent communication services which may then communicate the information directly to the container manager backend services or in other implementations stores the information in the database whereupon the container manager backend services can read the information.

The database may be a data store located within the distributed computing system of the container service or may be a data store of a different service of a computing resource service provider such as a relational database service. In some embodiments the database may be a set of distributed databases that share a transaction log. The agent communication services may be configured to track the status of all agents in a cluster and may push run commands and state transitions no its respective instance. In some embodiments communication by other components of the container service with containers and the container instances is performed through the agent communication services . Each fleet may have at least one agent communication service which relays the messages between the container agents of the fleet.

The container agents may be software applications configured to run in instances owned by the customers and may act as an interface between their respective container instances and other services and entities such as the container manager backend services . For examples the container agents may act as intermediaries between the running tasks of their respective container instances and other entities and services such that all communication to or from a container passes through the container agent. In this manner the container agent may be configured to interpret and translate commands between the container and a particular container encapsulation system running with the container service . This may allow changes to be made to the particular container encapsulation system without requiring updates to be made to the tasks or task definitions i.e. only the container agents may need to be updated to reflect the changes to the particular encapsulation system.

Thus each of the container instances may have a respective container agent running within it that communicates with a respective container manager backend service. The container agent may itself be a container configured to monitor its respective container instance and may provide information to the system usable to launch containers track containers and monitor cluster state. The container agent may also perform functions of registering and deregistering its respective container instance starting and stopping tasks within its respective container instance. The container agent may also be configured to respond to requests to describe its respective container instance requests to list tasks running in its respective container instance and requests to describe tasks running in its respective container instance. The container agents may be configured to monitor the health of the containers within the respective container instances e.g. report heartbeats signaling that the container instance is operating report lifespans of containers and report container statuses and occurrences of container errors and may further be configured to perform actions based on the occurrence of certain events. For example if a container agent detects that a container has encountered an error and ceased operation the container agent may automatically cause a new container to be generated to replace the malfunctioning container. In other embodiments the scheduler may take certain actions in response to events reported to it by the container agents . In the above example it may be the scheduler that causes a new container to be generated to replace a malfunctioning container. The customer owner of the container may specify conditions events and actions for the scheduler and or container agent. For example the customer may specify if the customer s containers cease operations such as due to an error or power outage that the scheduler or container agent is not to generate replacement containers for the inoperative containers. Instead the customer may specify that the scheduler or container agent is to notify e.g. by changing a status indicator providing an e mail message etc. the customer of the occurrence of the problem.

The container agents and or the telemetry agents may be configured to launch automatically when their respective container instances are instantiated. If a new container encapsulation system is implemented by the container service the only changes required for the container instances and containers may be for new container agents configured to be compatible with the new container encapsulation system to be created and the container agents to be swapped for the new container agents. In such a case the customers should be able to use the same application programming interfaces with the container service and the new container agents should be configured to support the same application programming interfaces without the customers being aware of the change to the new encapsulation system.

The container agents may be polled by the container encapsulation system to communicate information to the container encapsulation system. The container agent may register or deregister container instances receive instructions from the container manager backend services and ensure that a telemetry agent has been started and is running. The container agent may also enable updates to containers in the container instances and monitor the state of containers running in the container instances via an event stream.

The telemetry agent may be configured to collect telemetry data such as a set of control group metrics and container encapsulation system logs and provide such telemetry data to the telemetry service . The telemetry service may be configured to aggregate data received from the telemetry agent for a resource monitoring service of a computing resource service provider which in turn may be configured to trigger an alarm or take some other action based on the aggregated data. For example if the telemetry agent communicates a log indicating an error state of a container to the telemetry service the telemetry service may provide the error state to the resource monitoring service which react by triggering an alarm notifying the customer that the container has experienced an error. As another example the resource monitoring service may trigger an alarm if one of the metrics e.g. central processing unit usage by processes of a container provided by the telemetry service exceeds a threshold. Note in some implementations the telemetry service may be configurable to specify the alarm conditions and thresholds. Examples of triggering an alarm include providing a text message to the customer owner of the container e mailing the customer owner of the container and or displaying a visual indicator e.g. a red icon popup window etc. on an interface displaying container statuses.

The container service may also allow data volumes to be linked to containers. Such data volumes may be designated directories within a container and may be shared with one or more other containers that may bypass the default file system of the container instance. In this manner data may be stored persistently and shared among other containers within the container instance. The data volume may be configured through entries in the task definition file. In some implementations creation and selection of one or more data volumes for a container may be achieved through a user interface configured for that purpose that communicates to the front end service . The container service may utilize other data storage services such as the on demand data storage service or the block level data storage service of of the computing resource service provider for providing the storage for the data volume. In other implementations the data volumes may utilize the native storage of the container instance for the data volume.

The container service may be integrated with other services of a computing resource service provider. For example the container instances may be tagged and or assigned to an auto scaling group of an auto scaling service of the computing resource service provider. In this manner the auto scaling service may monitor resource usage by the container instances and may dynamically adjust allocate resources as needed such as a sudden increase in resource demand by the container instances. Likewise the container service may integrate with a load balancer service of the computing resource service provider. For example the load balancer service may distribute traffic to the containers or container instances in order to balance the workload between the container instances.

As an example a customer may operate a website using container instances assigned to an auto scaling group. The website may receive requests from multiple users over the Internet and a load balancer of the load balancer service may distribute the requests to the container instances according to a load balancing distribution scheme. The load balancer service may be a computer system or virtual computer system configured to distribute the requests to the container instances assigned to the load balancer in order to optimize resource utilization and or avoid overloading any particular host computer. For example the load balancer may include physical hardware connected to a server rack or otherwise included in a data center. In another example the load balancer may include one or more virtual machines supported by a host computer. At the same time the auto scaling service may detect whether more or fewer resources are needed by the container instances due to the incoming requests and may allocate more or fewer resources to the container instances as needed.

Virtualization layers in the system hardware enables the system hardware to be used to provide computational resources upon which one or more container instances may operate. The virtualization layer may be any device software or firmware used for providing a virtual computing platform for the container instances . The virtualization layers executing on the hosts enables the set of system hardware to be used to provide computational resources necessary to support the container instances . Furthermore the physical host may host multiple virtualization layers of the same or different types on the same system hardware . Each container instance may include various virtual computer components such as one or more virtual processors virtual memory and virtual storage. The container instances may be provided to the customers of the computing resource service provider and the customers may run an operating system and applications on each of the container instances . An example of a virtualization layer includes a hypervisor.

Requests may be received by a request interface such as the front end service of operated by the computing resource service provider . The request interface may direct the request to the appropriate container instance. Each container instance may include one or more software agents . The software agents may be configured to allow the customers to manage their respective containers and container instances. The software agents may be further configured to perform logging of events and gather telemetry data related to the containers and container instances . Examples of such software agents are the container agents and the telemetry agents described in conjunction with .

The operating systems may be any operating systems suitable for running within the container instances and that provide isolation technology that enable containerization schemes to isolate virtualization instances such as the containers from other processes running under the operating system . Examples of such operating systems include various implementations of Linux operating systems that support resource isolation features in the Linux kernel. As noted the containers may be virtualized instances within the operating systems launched from application images in accordance with one or more task definitions and may be allocated resources from their respective container instances .

The containers similar to containers discussed elsewhere in the present disclosure may be running virtualization instances also referred to as tasks of varying lifespans e.g. short term batch jobs long term background processes etc. that have been isolated from other processes within the container instance. Metrics about the containers may be gathered by the telemetry agent aggregated and provided to the telemetry service . The container agent acts as a go between between the containers and resources services and other entities outside the namespace of the containers . In some implementations the containers may be configured to share external resources such as block level data storage volumes. In some of these implementations access to and communication with the shared external resources by the containers may be made through the container agent . In other implementations the container instance or operating system of the container instance may support allowing the containers to access or communicate with the shared resources without going through the container agent .

The cluster manager may be metadata or cluster management software that is configured with a set of rules for determining within which container instance that containers should be launched. For example when the container instance is instantiated its container agent may update the cluster manager with information indicating that the container instance is available for hosting containers for a customer. Thereafter if a request to launch a container is received through the front end service from the customer or from a scheduler a scheduler may refer to the cluster manager to select the container instance and tell the container agent of the container instance to launch the container. The cluster manager may also be configured to specify what actions should be taken in response to certain types of container events. For example if one of the containers malfunctions and or ceases operation the container agent may reference the cluster manager to determine whether to re launch the malfunctioning or inoperative container.

Note in some embodiments the scheduler may communicate with the agent communication service or directly to the container agent to instruct the container agent to launch tasks. In other embodiments the scheduler communicates the placement and launching instructions to the container manager backend which subsequently causes the containers to be launched according to the placement orders from the scheduler such as by communicating the launch instructions to the container agent directly or through the agent communication service. The cluster manager may be multitenant e.g. the cluster manager may be configured with information for managing clusters of multiple customers of the computing resource service provider.

As noted the container manager backend may be configured to provide an environment for other processes supporting the containers and container instances in the particular sub fleet such as the cluster manager . The container agent may be configured to provide lifecycle and health information about the containers being monitored by the container agent to the cluster manager . In some implementations the container agent collects metrics and log information and passes this information to the telemetry agent or directly to a telemetry service. As well in some implementations the container agent may update the cluster manager through the agent communication service . In some embodiments communication between the container agent and outside components may be a one way communication from the container agent to the agent communication service and the outside components. In other embodiments communication between the container agent and the agent communication service is bi directional. In other implementations the container agent may directly update cluster manager without going through the agent communication service . Note that the container agent may be a software container that is launched when the container instance is created and in other cases the container agent may be a process running under the operating system of the container instance in communication with the containers .

As noted the telemetry agent may be configured to gather metrics and log information about the containers running within the container instance and pass the gathered metrics and log information to the telemetry service . In some implementations the telemetry agent may be a separate container of its own that launches when the container instance is created. In other implementations the telemetry agent may be a process running under the operating system of the container instance and configured to receive metrics and log information directly through the containers or through the container agent . Note that not all implementations of the present disclosure require a telemetry agent and or telemetry service .

The container instance may include a container agent . The container agent as noted may be a separate running container configured to interface between the containers A C and entities external to the container instance . The amount of resources to be allocated to the containers A C may be specified within the task definition. A scheduler may determine the container instance in which to launch the containers A C based on as has been described in the present specification a placement scheme and or available resources within a set of container instances of which the container instance is a member. Thereupon the scheduler may notify the container instance or the container agent to allocate the amount of resources specified by the task definition to the containers A C and the container agent may allocate the resources to the containers A C as directed by the scheduler. Additionally once the resources are allocated for each of the containers A C the scheduler may notify the container instance or the container agent to launch each of the containers A C as running containers i.e. tasks within their own namespace and with exclusive use of their respectively allocated resources. Upon being notified the container agent may launch the containers A C within the container instance as directed by the scheduler.

In a computing resource service provider receives a software image from a customer of the computing resource service provider. As noted a software image may refer to data representing the entire state of a software application at the time it was imaged such that the software application may be restored to this point by restoring launching the software image. In some cases the operations in may be to obtain an executable installation or application file rather than an image. Note in some cases receiving the software image includes having a software image accessible to the computing resource service provider such as through a resource location specified in a task definition file. In some cases the software image may be stored in a data store such as the database of of the computing resource service provider.

In the computing resource service provider may receive a task definition describing the tasks containers to run within one or more instances. The task definition may be in the form of a task definition file or may be generated by the computing resource service provider in response to options selected by the customer through a user interface. The task definition may specify information such as a description of the tasks containers locations of where to find images for the tasks containers amounts of resources allocated to the tasks containers shared resources relationships between other containers and other information. As noted the task definition may be stored in a data store such as the database of of the computing resource service provider.

In the container instances may be instantiated. As has been noted container instance may be virtual machine instances that support containerization. In some cases instantiating the container instances includes launching a container agent within the container instance. The container agent may run under an operating system of the container instance and the container agent itself may be a container.

In one or more clusters may be created. As has been noted a cluster may be a logical grouping of one or more container instances. Clusters may be used for various purposes including separating tasks by function grouping different types of container instances together and grouping container instances based on locations of the physical host machines within the computing resource service provider infrastructure. Once a cluster is created one or more container instances may be registered via the cluster manager as being members of the cluster. As has been noted in some cases container instances may be members of more than one cluster. Note that operations may occur together or in any order. For example a cluster may be created before container instances and as the container instances are instantiated the container agents within the container instances may automatically cause the container instance to be registered with the cluster.

In the computing resource service provider provides a cluster or cluster ID to the customer or other entity that may be responsible for requesting tasks to be launched. In some cases this other entity may be a scheduler such as the scheduler of provided by the container service or a scheduler installed by the customer. The cluster ID may be provided in response to a request to create the cluster in . At other times the cluster ID may be provided to a requesting entity that made a ListClusters application programming interface call.

In a DescribeCluster application programming interface call may be received from a customer or other entity responsible for requesting tasks to be launched e.g. a scheduler specifying a cluster e.g. by passing the cluster ID of as a parameter to describe. In response to the DescribeCluster application programming interface call the computing resource service provider in may provide information about the cluster including the number and IDs of container instances within the cluster resources available within the cluster resources being used in the cluster running tasks etc. This information may be used by the customer or scheduler to determine where e.g. which container instance particular tasks should be launched e.g. to achieve certain performance goals to balance load etc. .

In the computing resource service provider receives a request to run one or more tasks in accordance with information specified in the task definition obtained in . In some cases the request may specify which container instances in which to run the one or more tasks. In other cases the request may specify which cluster in which to run the one or more tasks and a scheduler such as the scheduler of may determine into which container instance the tasks should be launched according to a placement scheme. Placement schemes may include round robin placement schemes stochastically assigned placement or heuristics based on resource usage or other criteria.

Once the target location of the tasks is determined e.g. after determining instance IDs of container instance in which to launch the tasks in the tasks corresponding to software images of may be launched into one or more container instances in accordance with the task definition of . In some cases the launching of the tasks may be performed by the container agents running in the determined container instances. The dotted line between and represent that the operations below the dotted line may not be integral to the launching of tasks in a cluster but provide information about the running tasks launched in .

In a request to describe the cluster may be received similar to the request in . In the computing resource service provider may query one or more container agents of the container instances running tasks for status information about the running tasks in the container instances. In the computing resource service provider may provide the information describing the cluster in response to the request of similar to the operations of . However differentiated from the information should now reflect the running tasks and or effect of the resources allocated to the running tasks launched into the cluster . Note that one or more of the operations performed in may be performed in various orders and combinations including in parallel.

In a computing resource service provider receives an application programming interface call from a customer or other entity to create a cluster. Creating a cluster may include generating a cluster ID for the cluster storing the cluster ID in a database along with other metadata about the cluster. In some cases the requestor may indicate through the CreateCluster application programming interface call to register container instances with the cluster upon creation. In such cases specified container instances e.g. one or more container instance IDs passed as parameters in the application programming interface call or available container instances may be registered in association with the cluster ID during cluster creation. In other such cases if existing container instances are not specified and no container instances are available new container instances may be instantiated and registered to the cluster during cluster creation. Note that in some implementations the computing resource service provider may provide a customer with a default cluster in such a case the customer need not utilize the CreateCluster application programming interface and may just register container instances to the default cluster unless additional clusters are required or if the default cluster is deleted.

In the computing resource service provider determines whether the requestor has sufficient privileges to have the request fulfilled. For example the requestor may have provided an identity and proof of possession of credentials as parameters with the CreateCluster application programming interface call. For example the requestor may supply as proof of possession of credentials corresponding to the identity information sufficient to prove access to the credentials such as a password a cryptographic hash digest of the password cryptographic digital signature generated by a signing encryption key or other secret key verifiable by the computing resource service provider for authorizing the identity of the requestor. The computing resource service provider may provide that identity and proof to an authentication service such as the authentication system of which may then verify the identity and proof of credentials of the requestor. Once the identity of the requestor is verified the computing resource service provider or authentication service may determine whether a security policy and or role associated with the identity grants sufficient privileges to allow the request to be fulfilled.

If the computing resource service provider was unable to determine the identity e.g. not found in a security database proof of credentials were insufficient to prove the identity e.g. wrong password or identity of the requestor could otherwise not be confirmed the system performing the process may not proceed further in the process . The system performing the process may respond to a failure in authentication with an error message to the requestor and or register the authentication failure in a security log. Otherwise if the requestor is authenticated and determined to have sufficient privileges to have the CreateCluster request fulfilled the system performing the process may proceed to .

In the system performing the process may store the cluster ID and cluster metadata in a data store such as the database described in conjunction with . As noted in some cases the cluster may be metadata stored in association with one or more container instances e.g. container instances registered to the cluster . A scheduler may reference a cluster manager to determining which container instance in a cluster in which to launch a container. In some examples the cluster manager refers at least in part to the cluster metadata. In other examples the cluster manager may be a software application or service that manages the cluster metadata and is in communication with the scheduler. Note that in some implementations storing the cluster metadata in the data store may cause the cluster ID to be generated. The cluster metadata may include information such as geographic region of the hosts of the container instances in the cluster number of container instances assigned to or permitted to be assigned to the cluster and date time of the creation of the cluster.

Once stored in the system performing the process may notify the requestor that creation of the cluster has been successful. In some cases this notification may include additional information metadata about the cluster including IDs of any container instances registered to the cluster and available resources e.g. memory and or processing capabilities . Thereafter the requestor if authenticated may utilize the DescribeCluster application programming interface call passing the cluster ID as a parameter and the computing resource service provider may respond by providing information including metadata information a list of container instances registered to the specified cluster a list of tasks running within the cluster container instances available resources in the cluster resources being used in the cluster metrics about the container instances of the cluster collected by a telemetry service such as the telemetry service of task definitions associated with the cluster or cluster container instances and other state information about the cluster.

In a computing resource service provider receives an application programming interface call to register a container instance. In some cases this request may be received from a customer or other entity external to the container service. In some of these cases the request is received through the front end service and communicated to the container agent which registers the container instance. In other cases this request may be made from a container agent to the front end service. For example when a container instance is instantiated with the container agent the container agent may automatically seek to register the container instance with a specified cluster or default cluster. Registering a container instance may include associating an existing virtual machine instance that has been specifically configured to support containerization or associating an existing virtual machine instance that has support for containerization with a particular cluster. This association may be stored in a database such as the database of as a relationship between the ID of the instance and the cluster ID. In such a case the requestor may provide the instance ID or other identifying property and the cluster ID as parameters in the RegisterContainerInstance function call.

In some cases the requestor may not specify a particular existing container instance and in such a case the computing resource service provider may select a container from a set of pre instantiated container instances suitable for containerization. In other cases if a suitable pre instantiated container instance is unavailable the computing resource service provider may instantiate a new container instance suitable for container instance and register the newly instantiated container instance with the specified cluster. In some implementations the RegisterContainerInstance application programming interface call may be used to register an existing container instance with a different cluster than a cluster to which it is currently registered. In such a case the requestor may specify the container instance ID and the cluster ID as parameters of the application programming interface call.

In the computing resource service provider determines whether the requestor has sufficient privileges to have the request fulfilled. For example the requestor may have provided an identity and proof of possession of credentials as parameters with the RegisterContainerInstance application programming interface call. For example the requestor may supply as proof of possession of credentials corresponding to the identity information sufficient to prove access to the credentials such as a password a cryptographic hash digest of the password cryptographic digital signature generated by a signing encryption key or other secret key verifiable by the computing resource service provider for authorizing the identity of the requestor. The computing resource service provider may provide that identity and proof to an authentication service such as the authentication system of which may then verify the identity and proof of credentials of the requestor. Once the identity of the requestor is verified the computing resource service provider or authentication service may determine whether a security policy and or role associated with the identity grants sufficient privileges to allow the request to be fulfilled.

If the computing resource service provider was unable to determine the identity e.g. not found in a security database proof of credentials were insufficient to prove the identity e.g. wrong password or identity of the requestor could otherwise not be confirmed the system performing the process may not proceed further in the process . The system performing the process may respond to a failure in authentication with an error message to the requestor and or register the authentication failure in a security log. Otherwise if the requestor is authenticated and determined to have sufficient privileges to have the RegisterContainerInstance request fulfilled the system performing the process may proceed to .

In the system performing the process may attempt to look up the container instance ID if specified in the relevant database such as database . If the container instance ID is found and determined to already be associated with a different cluster ID in some cases the system performing the process may in deregister the container instance from the cluster it is currently registered to and register the specified cluster. In some of these cases this deregistration reregistration may be performed automatically after the requestor is authenticated. In other cases the system performing the process may prompt the requestor for confirmation to deregister the container instance from the cluster it is currently registered to before the system performs the deregister reregister operation. In still other implementations container instances may be registered to multiple clusters simultaneously and in such implementations an already registered container instance may not be deregistered from the cluster with which it is currently registered.

If the container instance has not already been registered to a cluster has been deregistered from a cluster or if the implementation supports container instance registration with multiple clusters the system performing the process may store the container instance ID in the database in association with the specified cluster. The system performing the process may then notify the requestor that registering the container instance with the specified cluster has been successful and in some cases particularly cases where a new container instance was created the system may include the container instance ID in the response. In a subsequent DescribeContainerInstance application programming interface call the computing resource service provider may provide the requestor if authenticated with information about the container instance including available resources tasks running within the container instance and with which cluster or clusters the container instance is registered.

In a computing resource service provider receives an application programming interface call from a customer or other entity to register a task. Registering a task may include storing a task definition or task definition file in a database. In some implementations the requestor need not create and upload a separate task definition but may be able to select various options from a user interface provided by the computing resource service provider and the computing resource service provider may dynamically generate a task definition based on the selected options. In some cases the computing resource service provider may provide a text entry interface to allow the customer to type or cut paste a task definition through the interface. In some cases the request to register a task may be accompanied by one or more software images corresponding to software images specified in the task definition. In some implementations the request may specify one or more cluster IDs to associate with the task definition. In other implementations the request may specify one or more container instance IDs to associate with the task definition. In still other implementations the task definition may be registered independently of any specified cluster or container instance.

In the computing resource service provider determines whether the requestor has sufficient privileges to have the request fulfilled. For example the requestor may have provided an identity and proof of possession of credentials as parameters with the RegisterTaskDefinition application programming interface call. For example the requestor may supply as proof of possession of credentials corresponding to the identity information sufficient to prove access to the credentials such as a password a cryptographic hash digest of the password cryptographic digital signature generated by a signing encryption key or other secret key verifiable by the computing resource service provider for authorizing the identity of the requestor. The computing resource service provider may provide that identity and proof to an authentication service such as the authentication system of which may then verify the identity and proof of credentials of the requestor. Once the identity of the requestor is verified the computing resource service provider or authentication service may determine whether a security policy and or role associated with the identity grants sufficient privileges to allow the request to be fulfilled.

If the computing resource service provider was unable to determine the identity e.g. not found in a security database proof of credentials were insufficient to prove the identity e.g. wrong password or identity of the requestor could otherwise not be confirmed the system performing the process may not proceed further in the process . The system performing the process may respond to a failure in authentication with an error message to the requestor and or register the authentication failure in a security log. Otherwise if the requestor is authenticated and determined to have sufficient privileges to have the RegisterTaskDefinition request fulfilled the system performing the process may proceed to .

In the system performing the process may store the task definition in a data store such as the database described in conjunction with . In some implementations the task definition may be registered in association with a customer ID of an account of the computing resource service provider. In other implementations the task definition may be registered to an identity of a user who obtained the task definition from a website a computing resource service provider or the computing resource service provider e.g. a purchaser of a task definition from an online marketplace . In still other implementations the task definition may be stored and linked to one or more cluster IDs and or container instance IDs. Once stored a task definition ID unique to the task definition may be generated by the computing resource service provider or by the database. Note as well that in some implementations the task definition ID may be provided with the RegisterTaskDefinition application programming interface call in order to update a task definition already stored in the database. That is the requestor may submit an updated task definition and an existing task definition ID as parameters of the RegisterTaskDefinition application programming interface call and the computing resource service provider may locate the task definition entry corresponding to the existing task definition ID and update the task definition corresponding to the task definition ID with the updated task definition.

Thus in the system performing the process may provide that task definition ID to the requestor both as confirmation of the success of registering the task definition and also as a way for the requestor to specify the task definition stored in the database at a later date. In some cases one or more software images may accompany the RegisterTaskDefinition application programming interface call and these software images may be stored in the same or different data store as the task definition. In some cases these software images may be referred to by the task definition ID whereas in other cases the software images may receive software image IDs which may be provided to the requestor along with the task definition ID of the stored task definition.

In a computing resource service provider receives an application programming interface call to start a task. In some cases this application programming interface call may be received through the front end service from a customer or other entity external to the container service. In other cases this application programming interface call may be made by a container agent in response to a communication from a scheduler to start a task. The StartTask application programming interface may be used to start running a software container from for example an image of a software container or from one or more executable application files. The StartTask application programming interface call may include a parameter for one or more task definition IDs for task definitions stored in a data store such as the database . In some cases the task definition itself rather than a task definition ID may be passed as a parameter to the StartTask application programming interface call. In other cases parameters indicating locations e.g. uniform resource locator network resource locator file path etc. of one or more task definition files may be passed as parameters to the StartTask application programming interface. In still other cases the container service may be configured to start tasks without a separate program definition for example the StartTask application programming interface may have default parameters and any information for starting the task that is required from the requestor or which deviates from the defaults may be passed as parameters to the StartTask application programming interface call.

The StartTask application programming interface may also accept as a parameter one or more image IDs for software images representing the base container images. In some cases a parameter indicating a location of an image file may be passed instead of an image file. In still other cases the task definition may contain information for locating the container software to start running as the task. The requestor may also specify one or more cluster IDs as parameters to the StartTask application programming interface call to indicate into which clusters the task or tasks should be started. Similarly the requestor may specify one or more container instance IDs as parameters to the StartTask application programming interface call to indicate into which container instances the task or tasks should be started. The IDs of clusters may be obtained through a ListClusters application programming interface call. The IDs of registered container instances may be obtained from a DescribeCluster application programming interface call.

In the computing resource service provider determines whether the requestor has sufficient privileges to have the request fulfilled. For example the requestor may have provided an identity and proof of possession of credentials as parameters with the StartTask application programming interface call. For example the requestor may supply as proof of possession of credentials corresponding to the identity information sufficient to prove access to the credentials such as a password a cryptographic hash digest of the password cryptographic digital signature generated by a signing encryption key or other secret key verifiable by the computing resource service provider for authorizing the identity of the requestor. The computing resource service provider may provide that identity and proof to an authentication service such as the authentication system of which may then verify the identity and proof of credentials of the requestor. Once the identity of the requestor is verified the computing resource service provider or authentication service may determine whether a security policy and or role associated with the identity grants sufficient privileges to allow the request to be fulfilled.

If the computing resource service provider was unable to determine the identity e.g. not found in a security database proof of credentials were insufficient to prove the identity e.g. wrong password or identity of the requestor could otherwise not be confirmed the system performing the process may not proceed further in the process . The system performing the process may respond to a failure in authentication with an error message to the requestor and or register the authentication failure in a security log. Otherwise if the requestor is authenticated and determined to have sufficient privileges to have the StartTask request fulfilled the system performing the process may proceed to .

In the system performing the process may determine the container instance into which to start the task. In some embodiments a scheduler of the container service such as the scheduler of may determine into which container instance of specified clusters and or list of container instances the task or tasks should be started according to a placement scheme. The placement scheme may be any of a variety of placement schemes including round robin pseudo random stochastic placement scheme or a placement scheme that takes container states e.g. current amount of available resources etc. into account. In some implementations the customer of the account may have installed a scheduler application within a virtual machine instance. In such implementations the StartTask application programming interface call may receive as a parameter an ID or resource locator indicating the location of the installed scheduler and the process may further include the system performing the process communicating with the installed scheduler to determine into which container instance the task or tasks should be launched. In some cases the specified task may already be started. In such cases the system performing the process may stop the running task and restart the task. In some of these cases the system performing the process may notify the requestor that the task is already started and may request confirmation whether to stop the running task and restart the task.

In some situations the system performing the process may determine that insufficient resources are available in the cluster or list of container instance to launch the task or tasks. For example the task definition file may have specified more memory to be allocated to the software containers than is currently available in the container instances of a specified cluster. In some of these cases the system performing the process may wait for a predetermined period of time e.g. 10 milliseconds and try again to start the tasks repeating until the system is able to start the tasks or until a threshold number of attempts have occurred.

If the system performing the process is unable to start the tasks the system may notify the requestor that the tasks were unable to be started due to the particular problem encountered. In some implementations if insufficient resources are available to start the task the container service may interact with an auto scaling service to generate additional resources to support starting the task. For example the system performing the process may instantiate and register one or more new container instances with the cluster and start the task or tasks in the new container instances. As another example the system performing the process may take a snapshot of a container instance restart the container instance with upgraded resources e.g. processing capability and memory and restore the container instance from the snapshot.

In the system performing the process may launch the container image or specified application into the determined container instance. As noted launching the container image or specified application into the container instance may include generating one or more task IDs for the tasks storing the task IDs in a data store such as database of in association with the determined container instance ID specifying a separate namespace for a container allocating specified resources to the container and restoring the specified container image and or specified application into the container namespace. This may be performed for each container defined in a task definition. The task definition may also indicate linkages between containers for example a container configured to serve as a web service may be linked to a container configured to serve as a database and the two containers may be configured to communicate with each other when they are launched. Likewise in some implementations containers may share specified resources of other services of the computing resource service provider. For example the task definition file may specify that two containers share a particular block level data storage volume.

Once launched the system performing the process may notify the requestor that the task or tasks have been successfully launched. In some cases this may be a matter of changing an icon of a monitoring interface to indicate that a task has been started and the task is now running within a particular cluster or instance. In other cases this may be a matter of logging the event. In still other cases the requestor may be notified by receiving a valid task ID from the system that corresponds to the running task. The list of running tasks may be obtained from a ListTasks application programming interface call. Data about the task such as health of the container age of the container and resource usage by the container may be obtained by using a DescribeTask application programming interface and specifying a task ID.

In a computing resource service provider receives an application programming interface call to stop a running task. In some cases the application programming interface call may be received from a customer or other entity external to the container service to the front end service. In other cases the container agent may make the application programming interface call in response to a communication from a scheduler to stop a task. The StopTask may receive as a parameter one or more task IDs of running tasks. In the computing resource service provider determines whether the requestor has sufficient privileges to have the request fulfilled. For example the requestor may have provided an identity and proof of possession of credentials as parameters with the StopTask application programming interface call. For example the requestor may supply as proof of possession of credentials corresponding to the identity information sufficient to prove access to the credentials such as a password a cryptographic hash digest of the password cryptographic digital signature generated by a signing encryption key or other secret key verifiable by the computing resource service provider for authorizing the identity of the requestor. The computing resource service provider may provide that identity and proof to an authentication service such as the authentication system of which may then verify the identity and proof of credentials of the requestor. Once the identity of the requestor is verified the computing resource service provider or authentication service may determine whether a security policy and or role associated with the identity grants sufficient privileges to allow the request to be fulfilled.

If the computing resource service provider was unable to determine the identity e.g. not found in a security database proof of credentials were insufficient to prove the identity e.g. wrong password or identity of the requestor could otherwise not be confirmed the system performing the process may not proceed further in the process . The system performing the process may respond to a failure in authentication with an error message to the requestor and or register the authentication failure in a security log. Otherwise if the requestor is authenticated and determined to have sufficient privileges to have the StopTask request fulfilled the system performing the process may proceed to .

In the specified running task or tasks may be stopped and the resources previously allocated to the task or tasks may be freed garbage collected and added to the available resources of the host container instance. In some cases the task ID may be disabled archived or removed from the data store. In other cases a status of the task may be updated to Stopped and this change in status may be reflected in any interface for viewing the status of running tasks. The requestor may also be notified that the task associated with the task ID has been successfully stopped.

In a computing resource service provider receives an application programming interface call to deregister a task. In some cases this request may be received from a customer or other entity to the container service. Deregistering a task may include disabling archiving or removing a task from a data store that has been registered according to the process for registering a task. Thus the task definition ID generated in the process may be provided with the DeregisterTaskDefinition application programming interface call to identify which task is to be deregistered.

In the computing resource service provider determines whether the requestor has sufficient privileges to have the request fulfilled. For example the requestor may have provided an identity and proof of possession of credentials as parameters with the DeregisterTaskDefinition application programming interface call. For example the requestor may supply as proof of possession of credentials corresponding to the identity information sufficient to prove access to the credentials such as a password a cryptographic hash digest of the password cryptographic digital signature generated by a signing encryption key or other secret key verifiable by the computing resource service provider for authorizing the identity of the requestor. The computing resource service provider may provide that identity and proof to an authentication service such as the authentication system of which may then verify the identity and proof of credentials of the requestor. Once the identity of the requestor is verified the computing resource service provider or authentication service may determine whether a security policy and or role associated with the identity grants sufficient privileges to allow the request to be fulfilled.

If the computing resource service provider was unable to determine the identity e.g. not found in a security database proof of credentials were insufficient to prove the identity e.g. wrong password or identity of the requestor could otherwise not be confirmed the system performing the process may not proceed further in the process . The system performing the process may respond to a failure in authentication with an error message to the requestor and or register the authentication failure in a security log. Otherwise if the requestor is authenticated and determined to have sufficient privileges to have the DeregisterTaskDefinition request fulfilled the system performing the process may proceed to .

In the system performing the process may disable archive or remove the task definition from the data store where the task definition was stored in . Any data entries indicating a relationship between the task definition and a cluster container instance software image or other container may be disabled archived or removed as well. In some cases if the task or tasks associated with the task definition are running the tasks may be stopped in accordance with the process for stopping tasks. Once the task is deregistered the requestor may be notified of the successful deregistration of the task.

In a computing resource service provider receives an application programming interface call from a customer or other entity to deregister a container instance. Deregistering a container instance may include disassociating a container instance registered in accordance with the process for registering a container instance from a specified cluster. This association may be found in a database such as the database of as a relationship between an ID of the registered container instance passed as parameters of the DeregisterContainerInstance application programming interface call and a cluster ID. In cases this request may be received from the front end service and communicated with the container agent which deregisters its respective container instance. In other cases this request may be made from a container agent to the front end service. For example when a container instance is to be deprovisioned removed from a cluster and transferred to a different cluster the container agent may automatically seek to deregister the container instance from the cluster to which it is currently registered. In some of these cases the requestor may specify an instance ID of the registered container as may be revealed by a DescribeCluster application programming interface call and the system performing the process may deregister the specified container instance from all clusters to which it is registered. In some cases if the container instance is registered to multiple clusters the system performing the process may prompt the requestor for confirmation of which cluster or clusters from which the container instance should be deregistered. In other cases both an instance ID and a cluster ID may be passed as parameters to the DeregisterContainerInstance application programming interface call and the container instance may be deregistered assuming the requestor has sufficient privileges as determined in from the specified cluster.

In the computing resource service provider determines whether the requestor has sufficient privileges to have the request fulfilled. For example the requestor may have provided an identity and proof of possession of credentials as parameters with the DeregisterContainerInstance application programming interface call. For example the requestor may supply as proof of possession of credentials corresponding to the identity information sufficient to prove access to the credentials such as a password a cryptographic hash digest of the password cryptographic digital signature generated by a signing encryption key or other secret key verifiable by the computing resource service provider for authorizing the identity of the requestor. The computing resource service provider may provide that identity and proof to an authentication service such as the authentication system of which may then verify the identity and proof of credentials of the requestor. Once the identity of the requestor is verified the computing resource service provider or authentication service may determine whether a security policy and or role associated with the identity grants sufficient privileges to allow the request to be fulfilled.

If the computing resource service provider was unable to determine the identity e.g. not found in a security database proof of credentials were insufficient to prove the identity e.g. wrong password or identity of the requestor could otherwise not be confirmed the system performing the process may not proceed further in the process . The system performing the process may respond to a failure in authentication with an error message to the requestor and or register the authentication failure in a security log. Otherwise if the requestor is authenticated and determined to have sufficient privileges to have the RegisterContainerInstance request fulfilled the system performing the process may proceed to .

In the system performing the process may look up the specified instance ID in the data store where the container instance was registered. Note that tasks running within the specified container instance may be stopped in accordance with the process for stopping tasks before the container instance is deregistered. In some cases if tasks are running in the specified container instance the system performing the process may request confirmation from the requestor to stop the running tasks before the system will deregister the container instance. As noted in some implementations a container instance may be registered to multiple clusters and the requestor may specify that the container instance only be deregistered from a subset of the clusters. In such implementations if tasks associated with the specified subset of the clusters are running within the container instance and confirmed by the requestor to be stopped only the tasks associated with the specified subset of the clusters may be stopped and the remainder of running tasks may left alone.

In after the instance ID has been found in the database and relevant tasks have been stopped the instance ID may be disabled archived or removed from the data store. In some cases a status of the instance may be updated to Deregistered and this change in status may be reflected in any interface for viewing the status of the cluster. If the container instance is registered to multiple clusters the specified container instance may be disassociated from only the specified subset of the clusters and the container instance may remain registered to the remainder of the clusters. Once deregistered the requestor may also be notified that the container instance associated with the instance has been successfully deregistered in the cluster or clusters from which it was deregistered.

Container instances that are not registered to any clusters may be deprovisioned and any resources allocated to the container instances may be freed in some implementations of the present disclosure. Alternatively the container instances may be added to a queue of instances available to be registered with a cluster. In this manner when a container instance is needed for registration to a cluster in accordance with the process for registering container instances a container instance may be selected from the available container instances without the overhead of provisioning a new container instance for registration.

In a computing resource service provider receives an application programming interface call from a customer or other entity to delete a cluster. The cluster to be deleted may be a default cluster provided by the computing resource service provider or may be a cluster created in accordance with the process for creating a cluster. The DeleteCluster application programming interface may receive as a parameter a cluster ID for the cluster to be deleted. The cluster ID and cluster metadata may consequently be stored in a data store such as the database of . This cluster ID may have been provided to the requestor when the cluster was created or may be determined from a ListClusters application programming interface call.

In the computing resource service provider determines whether the requestor has sufficient privileges to have the request fulfilled. For example the requestor may have provided an identity and proof of possession of credentials as parameters with the DeleteCluster application programming interface call. For example the requestor may supply as proof of possession of credentials corresponding to the identity information sufficient to prove access to the credentials such as a password a cryptographic hash digest of the password cryptographic digital signature generated by a signing encryption key or other secret key verifiable by the computing resource service provider for authorizing the identity of the requestor. The computing resource service provider may provide that identity and proof to an authentication service such as the authentication system of which may then verify the identity and proof of credentials of the requestor. Once the identity of the requestor is verified the computing resource service provider or authentication service may determine whether a security policy and or role associated with the identity grants sufficient privileges to allow the request to be fulfilled.

If the computing resource service provider was unable to determine the identity e.g. not found in a security database proof of credentials were insufficient to prove the identity e.g. wrong password or identity of the requestor could otherwise not be confirmed the system performing the process may not proceed further in the process . The system performing the process may respond to a failure in authentication with an error message to the requestor and or register the authentication failure in a security log. Otherwise if the requestor is authenticated and determined to have sufficient privileges to have the DeleteCluster request fulfilled the system performing the process may proceed to .

In if any container instances are registered to the specified cluster each of the container instances may be deregistered from the cluster in accordance with the process for deregistering container instances. In the entry or entries for the cluster associated with the specified cluster ID may be located in the data store and disabled archived or removed from the data store. If the cluster is disabled or archived the metadata about the cluster may be preserved and the cluster may be reactivated by an entity with authority to do so. The requestor may then be notified that the cluster has been successfully deleted and that container instances can no longer be registered to the cluster ID. For example an interface displaying the statutes of clusters may be updated to reflect Deleted for the specified cluster and the cluster may no longer be reflected in ListClusters application programming interface call response.

The computing resource service provider may provide various computing resource services to its customers. The services provided by the computing resource service provider in this example include a virtual computer system service a block level data storage service a cryptography service an on demand data storage service a notification service an authentication system a policy management service a container service and one or more other services . It is noted that not all embodiments described include the services described with reference to and additional services may be provided in addition to or as an alternative to services explicitly described. As described each of the services may include one or more web service interfaces that enable the customer to submit appropriately configured API calls to the various services through web service requests. In addition each of the services may include one or more service interfaces that enable the services to access each other e.g. to enable a virtual computer system of the virtual computer system service to store data in or retrieve data from the on demand data storage service and or to access one or more block level data storage devices provided by the block level data storage service .

The virtual computer system service may be a collection of computing resources configured to instantiate virtual machine instances on behalf of the customer . The customer may interact with the virtual computer system service via appropriately configured and authenticated API calls to provision and operate virtual computer systems that are instantiated on physical computing devices hosted and operated by the computing resource service provider . The virtual computer systems may be used for various purposes such as to operate as servers supporting a website to operate business applications or generally to serve as computing power for the customer. Other applications for the virtual computer systems may be to support database applications electronic commerce applications business applications and or other applications. Although the virtual computer system service is shown in any other computer system or computer system service may be utilized in the computing resource service provider such as a computer system or computer system service that does not employ virtualization or instantiation and instead provisions computing resources on dedicated or shared computers servers and or other physical devices.

The block level data storage service may comprise one or more computing resources that collectively operate to store data for a customer using block level storage devices and or virtualizations thereof . The block level storage devices of the block level data storage service may for instance be operationally attached to virtual computer systems provided by the virtual computer system service to serve as logical units e.g. virtual drives for the computer systems. A block level storage device may enable the persistent storage of data used generated by a corresponding virtual computer system where the virtual computer system service may only provide ephemeral data storage.

The computing resource service provider also includes a cryptography service . The cryptography service may utilize one or more storage services of the computing resource service provider to store keys of the customers in encrypted form whereby the keys may be usable to decrypt customer keys accessible only to particular devices of the cryptography service .

The computing resource service provider further includes an on demand data storage service . The on demand data storage service may be a collection of computing resources configured to synchronously process requests to store and or access data. The on demand data storage service may operate using computing resources e.g. databases that enable the on demand data storage service to locate and retrieve data quickly to allow data to be provided in responses to requests for the data. For example the on demand data storage service may maintain stored data in a manner such that when a request for a data object is retrieved the data object can be provided or streaming of the data object can be initiated in a response to the request. As noted data stored in the on demand data storage service may be organized into data objects. The data objects may have arbitrary sizes except perhaps for certain constraints on size. Thus the on demand data storage service may store numerous data objects of varying sizes. The on demand data storage service may operate as a key value store that associates data objects with identifiers of the data objects that may be used by the customer to retrieve or perform other operations in connection with the data objects stored by the on demand data storage service .

In the environment illustrated in a notification service is included. The notification service may comprise a collection of computing resources collectively configured to provide a web service or other interface and browser based management console. The management console can be used to configure topics for which customers seek to receive notifications configure applications or people subscribe clients to the topics publish messages or configure delivery of the messages over clients protocol of choice i.e. hypertext transfer protocol HTTP e mail and short message service SMS among others . The notification service may provide notifications to clients using a push mechanism without the need to check periodically or poll for new information and updates. The notification service may further be used for various purposes such as monitoring applications executing in the virtual computer system service workflow systems time sensitive information updates mobile applications and many others.

As illustrated in the computing resource service provider in various embodiments includes an authentication system and a policy management service . The authentication system in an embodiment is a computer system i.e. collection of computing resources configured to perform operations involved in authentication of users of the customer. For instance one of the services and may provide information from a user to the authentication system to receive information in return that indicates whether the user requests are authentic.

The policy management service in an embodiment is a computer system configured to manage policies on behalf of customers such as customer of the computing resource service provider . The policy management service may include an interface that enables customers to submit requests related to the management of policy. Such requests for instance may be requests to add delete change or otherwise modify policy for a customer or for other administrative actions such as providing an inventory of existing policies and the like.

The computing resource service provider in various embodiments is also equipped with a container service . The container service is configured to create and manage software containers and container instances for the customers of the computing resource service provider in the manner described for the container service of . The container service may be configured to use other resources of the computing resource service provider such as the block level data storage service . For example the container service may allow tasks running within container instances to share one or more specified block level data storage volumes.

The computing resource service provider additionally maintains one or more other services based at least in part on the needs of its customers . For instance the computing resource service provider may maintain a database service for its customers . A database service may be a collection of computing resources that collectively operate to run one or more databases for one or more customers . The customer may operate and manage a database from the database service by utilizing appropriately configured API calls. This in turn may allow a customer to maintain and potentially scale the operations in the database. Other services include but are not limited to object level archival data storage services and services that manage and or monitor other services.

Note that unless otherwise specified use of expressions regarding executable instructions also referred to as code applications agents etc. performing operations that instructions do not ordinarily perform unaided e.g. transmission of data calculations etc. in the context of describing disclosed embodiments denote that the instructions are being executed by a machine thereby causing the machine to perform the specified operations.

The illustrative environment includes an application server and a data store . It should be understood that there could be several application servers layers or other elements processes or components which may be chained or otherwise configured which can interact to perform tasks such as obtaining data from an appropriate data store. Servers as used may be implemented in various ways such as hardware devices or virtual computer systems. In some contexts servers may refer to a programming module being executed on a computer system. As used unless otherwise stated or clear from context the term data store refers to any device or combination of devices capable of storing accessing and retrieving data which may include any combination and number of data servers databases data storage devices and data storage media in any standard distributed virtual or clustered environment. The application server can include any appropriate hardware software and firmware for integrating with the data store as needed to execute aspects of one or more applications for the client device handling some or all of the data access and business logic for an application. The application server may provide access control services in cooperation with the data store and is able to generate content including text graphics audio video and or other content usable to be provided to the user which may be served to the user by the web server in the form of HyperText Markup Language HTML Extensible Markup Language XML JavaScript Cascading Style Sheets CSS or another appropriate client side structured language. Content transferred to a client device may be processed by the client device to provide the content in one or more forms including forms that are perceptible to the user audibly visually and or through other senses including touch taste and or smell. The handling of all requests and responses as well as the delivery of content between the electronic client device and the application server can be handled by the web server using PHP Hypertext Preprocessor PHP Python Ruby Perl Java HTML XML or another appropriate server side structured language in this example. It should be understood that the web and application servers are not required and are merely example components as structured code discussed can be executed on any appropriate device or host machine as discussed elsewhere. Further operations described as being performed by a single device may unless otherwise clear from context be performed collectively by multiple devices which may form a distributed and or virtual system.

The data store can include several separate data tables databases data documents dynamic data storage schemes and or other data storage mechanisms and media for storing data relating to a particular aspect of the present disclosure. For example the data store illustrated may include mechanisms for storing production data and user information which can be used to serve content for the production side. The data store also is shown to include a mechanism for storing log data which can be used for reporting analysis or other purposes. It should be understood that there can be many other aspects that may need to be stored in the data store such as page image information and access rights information which can be stored in any of the above listed mechanisms as appropriate or in additional mechanisms in the data store . The data store is operable through logic associated therewith to receive instructions from the application server and obtain update or otherwise process data in response thereto. The application server may provide static dynamic or a combination of static and dynamic data in response to the received instructions. Dynamic data such as data used in web logs blogs shopping applications news services and other applications may be generated by server side structured languages as described or may be provided by a content management system CMS operating on or under the control of the application server. In one example a user through a device operated by the user might submit a search request for a certain type of item. In this case the data store might access the user information to verify the identity of the user and can access the catalog detail information to obtain information about items of that type. The information then can be returned to the user such as in a results listing on a web page that the user is able to view via a browser on the electronic client device . Information for a particular item of interest can be viewed in a dedicated page or window of the browser. It should be noted however that embodiments of the present disclosure are not necessarily limited to the context of web pages but may be more generally applicable to processing requests in general where the requests are not necessarily requests for content.

Each server typically will include an operating system that provides executable program instructions for the general administration and operation of that server and typically will include a computer readable storage medium e.g. a hard disk random access memory read only memory etc. storing instructions that when executed by a processor of the server allow the server to perform its intended functions. Suitable implementations for the operating system and general functionality of the servers are known or commercially available and are readily implemented by persons having ordinary skill in the art particularly in light of the disclosure.

The environment in one embodiment is a distributed and or virtual computing environment utilizing several computer systems and components that are interconnected via communication links using one or more computer networks or direct connections. However it will be appreciated by those of ordinary skill in the art that such a system could operate equally well in a system having fewer or a greater number of components than are illustrated in . Thus the depiction of the example environment in should be taken as being illustrative in nature and not limiting to the scope of the disclosure.

The various embodiments further can be implemented in a wide variety of operating environments which in some cases can include one or more user computers computing devices or processing devices that can be used to operate any of a number of applications. User or client devices can include any of a number of general purpose personal computers such as desktop laptop or tablet computers running a standard operating system as well as cellular wireless and handheld devices running mobile software and capable of supporting a number of networking and messaging protocols. Such a system also can include a number of workstations running any of a variety of commercially available operating systems and other known applications for purposes such as development and database management. These devices also can include other electronic devices such as dummy terminals thin clients gaming systems and other devices capable of communicating via a network. These devices also can include virtual devices such as virtual machines hypervisors and other virtual devices capable of communicating via a network.

Various embodiments of the present disclosure utilize a network that would be familiar to those skilled in the art for supporting communications using any of a variety of commercially available protocols such as Transmission Control Protocol Internet Protocol TCP IP User Datagram Protocol UDP protocols operating in various layers of the Open System Interconnection OSI model File Transfer Protocol FTP Universal Plug and Play UpnP Network File System NFS Common Internet File System CIFS and AppleTalk. The network can be for example a local area network a wide area network a virtual private network the Internet an intranet an extranet a public switched telephone network an infrared network a wireless network a satellite network and any combination thereof.

In embodiments utilizing a web server the web server can run any of a variety of server or mid tier applications including Hypertext Transfer Protocol HTTP servers FTP servers Common Gateway Interface CGI servers data servers Java servers Apache servers and business application servers. The server s also may be capable of executing programs or scripts in response to requests from user devices such as by executing one or more web applications that may be implemented as one or more scripts or programs written in any programming language such as Java C C or C or any scripting language such as Ruby PHP Perl Python or TCL as well as combinations thereof. The server s may also include database servers including those commercially available from Oracle Microsoft Sybase and IBM as well as open source servers such as MySQL Postgres SQLite MongoDB and any other server capable of storing retrieving and accessing structured or unstructured data. Database servers may include table based servers document based servers unstructured servers relational servers non relational servers or combinations of these and or other database servers.

The environment can include a variety of data stores and other memory and storage media as discussed above. These can reside in a variety of locations such as on a storage medium local to and or resident in one or more of the computers or remote from any or all of the computers across the network. In a particular set of embodiments the information may reside in a storage area network SAN familiar to those skilled in the art. Similarly any necessary files for performing the functions attributed to the computers servers or other network devices may be stored locally and or remotely as appropriate. Where a system includes computerized devices each such device can include hardware elements that may be electrically coupled via a bus the elements including for example a central processing unit CPU or processor an input device e.g. a mouse keyboard controller touch screen or keypad and an output device e.g. a display device printer or speaker . Such a system may also include one or more storage devices such as disk drives optical storage devices and solid state storage devices such as random access memory RAM or read only memory ROM as well as removable media devices memory cards flash cards etc.

Such devices also can include a computer readable storage media reader a communications device e.g. a modem a wireless or wired network card an infrared communication device etc. and working memory as described above. The computer readable storage media reader can be connected with or configured to receive a computer readable storage medium representing remote local fixed and or removable storage devices as well as storage media for temporarily and or more permanently containing storing transmitting and retrieving computer readable information. The system and various devices also typically will include a number of software applications modules services or other elements located within a working memory device including an operating system and application programs such as a client application or web browser. It should be appreciated that alternate embodiments may have numerous variations from that described above. For example customized hardware might also be used and or particular elements might be implemented in hardware software including portable software such as applets or both. Further connection to other computing devices such as network input output devices may be employed.

Storage media and computer readable media for containing code or portions of code can include any appropriate media known or used in the art including storage media and communication media such as volatile and non volatile removable and non removable media implemented in any method or technology for storage and or transmission of information such as computer readable instructions data structures program modules or other data including RAM ROM Electrically Erasable Programmable Read Only Memory EEPROM flash memory or other memory technology Compact Disc Read Only Memory CD ROM digital versatile disk DVD or other optical storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium which can be used to store the desired information and which can be accessed by the system device. Based on the disclosure and teachings provided a person of ordinary skill in the art will appreciate other ways and or methods to implement the various embodiments.

The specification and drawings are accordingly to be regarded in an illustrative rather than a restrictive sense. However it will be evident that various modifications and changes may be made thereunto without departing from the broader spirit and scope of the invention as set forth in the claims.

Other variations are within the spirit of the present disclosure. Thus while the techniques are susceptible to various modifications and alternative constructions certain illustrated embodiments thereof are shown in the drawings and have been described above in detail. It should be understood however that there is no intention to limit the invention to the specific form or forms disclosed but on the contrary the intention is to cover all modifications alternative constructions and equivalents falling within the spirit and scope of the invention as defined in the appended claims.

The use of the terms a an and the and similar referents in the context of describing the embodiments especially in the context of the following claims are to be construed to cover both the singular and the plural unless otherwise indicated or clearly contradicted by context. The terms comprising having including and containing are to be construed as open ended terms i.e. meaning including but not limited to unless otherwise noted. The term connected when unmodified and referring to physical connections is to be construed as partly or wholly contained within attached to or joined together even if there is something intervening. Recitation of ranges of values are merely intended to serve as a shorthand method of referring individually to each separate value falling within the range unless otherwise indicated and each separate value is incorporated into the specification as if it were individually recited. The use of the term set e.g. a set of items or subset unless otherwise noted or contradicted by context is to be construed as a nonempty collection comprising one or more members. Further unless otherwise noted or contradicted by context the term subset of a corresponding set does not necessarily denote a proper subset of the corresponding set but the subset and the corresponding set may be equal.

Conjunctive language such as phrases of the form at least one of A B and C or at least one of A B and C is understood with the context as used in general to present that an item term etc. may be either A or B or C or any nonempty subset of the set of A and B and C unless specifically stated otherwise or otherwise clearly contradicted by context. For instance in the illustrative example of a set having three members the conjunctive phrases at least one of A B and C and at least one of A B and C refer to any of the following sets A B C A B A C B C A B C. Thus such conjunctive language is not generally intended to imply that certain embodiments require at least one of A at least one of B and at least one of C each to be present.

Operations of processes described can be performed in any suitable order unless otherwise indicated or otherwise clearly contradicted by context. Processes described or variations and or combinations thereof may be performed under the control of one or more computer systems configured with executable instructions and may be implemented as code e.g. executable instructions one or more computer programs or one or more applications executing collectively on one or more processors by hardware or combinations thereof. The code may be stored on a computer readable storage medium for example in the form of a computer program comprising instructions executable by one or more processors. The computer readable storage medium may be non transitory.

The use of any examples or exemplary language e.g. such as provided is intended merely to better illuminate embodiments of the invention and does not pose a limitation on the scope of the invention unless otherwise claimed. No language in the specification should be construed as indicating any non claimed element as essential to the practice of the invention.

Embodiments of this disclosure are described including the best mode known to the inventors for carrying out the invention. Variations of those embodiments may become apparent to those of ordinary skill in the art upon reading the foregoing description. The inventors expect skilled artisans to employ such variations as appropriate and the inventors intend for embodiments of the present disclosure to be practiced otherwise than as specifically described. Accordingly the scope of the present disclosure includes all modifications and equivalents of the subject matter recited in the claims appended hereto as permitted by applicable law. Moreover any combination of the above described elements in all possible variations thereof is encompassed by the scope of the present disclosure unless otherwise indicated or otherwise clearly contradicted by context.

All references including publications patent applications and patents cited are hereby incorporated by reference to the same extent as if each reference were individually and specifically indicated to be incorporated by reference and were set forth in its entirety.

