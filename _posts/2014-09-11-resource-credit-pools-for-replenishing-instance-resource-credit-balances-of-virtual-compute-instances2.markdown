---

title: Resource credit pools for replenishing instance resource credit balances of virtual compute instances
abstract: A provider network may implement resource credit pools to replenish resource credit balances for virtual compute instances. A resource credit pool may be maintained that makes resource credits available to virtual compute instances authorized to obtain resource credits from the resource credit pool. Resource credits from the resource credit pool may be applicable to increase utilization of physical computer resource for a virtual compute instance. In response to a resource credit request for an authorized virtual compute instance, a number of resource credits to add to an individual resource credit balance for the authorized virtual compute instance may be determined. A response may be sent indicating the number of resource credits to add to the individual resource credit balance and the resource credit pool may be updated to remove the number of resource credits from the resource credit pool.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09626210&OS=09626210&RS=09626210
owner: Amazon Technologies, Inc.
number: 09626210
owner_city: Reno
owner_country: US
publication_date: 20140911
---
The advent of virtualization technologies for commodity hardware has provided benefits with respect to managing large scale computing resources for many customers with diverse needs allowing various computing resources to be efficiently and securely shared by multiple customers. For example virtualization technologies may allow a single physical computing machine to be shared among multiple users by providing each user with one or more virtual machines hosted by the single physical computing machine with each such virtual machine being a software simulation acting as a distinct logical computing system that provides users with the illusion that they are the sole operators and administrators of a given hardware computing resource while also providing application isolation and security among the various virtual machines. As another example virtualization technologies may allow data storage hardware to be shared among multiple users by providing each user with a virtualized data store which may be distributed across multiple data storage devices with each such virtualized data store acting as a distinct logical data store that provides users with the illusion that they are the sole operators and administrators of the data storage resource.

Virtualization technologies may be leveraged to create many different types of services or perform different functions for client systems or devices. For example virtual machines may be used to implement a network based service for external customers such as an e commerce platform. Virtual machines may also be used to implement a service or tool for internal customers such as information technology IT service implemented as part of an internal network for a corporation. Utilizing these virtual resources efficiently however may require flexible utilization options for many different types of virtual resource workloads. In some environments multiple virtual machines may be hosted together on a single host creating the possibility for contention and conflicts when utilizing different virtual computing resources that may rely upon the same physical computer resources.

While embodiments are described herein by way of example for several embodiments and illustrative drawings those skilled in the art will recognize that the embodiments are not limited to the embodiments or drawings described. It should be understood that the drawings and detailed description thereto are not intended to limit embodiments to the particular form disclosed but on the contrary the intention is to cover all modifications equivalents and alternatives falling within the spirit and scope as defined by the appended claims. The headings used herein are for organizational purposes only and are not meant to be used to limit the scope of the description or the claims. As used throughout this application the word may is used in a permissive sense i.e. meaning having the potential to rather than the mandatory sense i.e. meaning must . The words include including and includes indicate open ended relationships and therefore mean including but not limited to. Similarly the words have having and has also indicate open ended relationships and thus mean having but not limited to. The terms first second third and so forth as used herein are used as labels for nouns that they precede and do not imply any type of ordering e.g. spatial temporal logical etc. unless such an ordering is otherwise explicitly indicated.

Various components may be described as configured to perform a task or tasks. In such contexts configured to is a broad recitation generally meaning having structure that performs the task or tasks during operation. As such the component can be configured to perform the task even when the component is not currently performing that task e.g. a computer system may be configured to perform operations even when the operations are not currently being performed . In some contexts configured to may be a broad recitation of structure generally meaning having circuitry that performs the task or tasks during operation. As such the component can be configured to perform the task even when the component is not currently on. In general the circuitry that forms the structure corresponding to configured to may include hardware circuits.

Various components may be described as performing a task or tasks for convenience in the description. Such descriptions should be interpreted as including the phrase configured to. Reciting a component that is configured to perform one or more tasks is expressly intended not to invoke 35 U.S.C. 112 paragraph six interpretation for that component.

 Based On. As used herein this term is used to describe one or more factors that affect a determination. This term does not foreclose additional factors that may affect a determination. That is a determination may be solely based on those factors or based at least in part on those factors. Consider the phrase determine A based on B. While B may be a factor that affects the determination of A such a phrase does not foreclose the determination of A from also being based on C. In other instances A may be determined based solely on B.

The scope of the present disclosure includes any feature or combination of features disclosed herein either explicitly or implicitly or any generalization thereof whether or not it mitigates any or all of the problems addressed herein. Accordingly new claims may be formulated during prosecution of this application or an application claiming priority thereto to any such combination of features. In particular with reference to the appended claims features from dependent claims may be combined with those of the independent claims and features from respective independent claims may be combined in any appropriate manner and not merely in the specific combinations enumerated in the appended claims.

The systems and methods described herein may implement resource credit pools for replenishing individual resource credit balances of virtual compute instances according to some embodiments. Different clients implementing virtual computing resources have different resource demands. For example some clients workloads are not predictable and may not utilize fixed resources efficiently. Virtual compute instances implementing resource credits for scheduling virtual computing resources may provide dynamic utilization of resources creating flexible high performance without wasting unutilized fixed resources. Resource credits may be accumulated for individual virtual compute instances and maintained as part of an individual resource credit balance. When a virtual compute instance needs to perform work at high performance the resource credits may be applied to the work effectively providing full utilization of underlying physical resources for the duration of the resource credits. When a virtual compute instance is using less than its share of resources e.g. little or no work is being performed credits may be accumulated and used for a subsequent task. Resources may in various embodiments be any virtualized computer resource that is implemented or performed by a managed physical computer resource including but not limited to processing resources communication or networking resources and storage resources.

While scheduling utilization of physical computer resources according to individual resource credit balances may allow individual virtual compute instances to handle some bursts or large changes in instance workloads the workload that may be directed to any one particular instance may be difficult to predict. If for instance a group of instances is used to provide some kind of service for which different instances may randomly experience burst workloads the overall workload of many instances may be relatively low. Yet a few instances may receive workloads that may even be in excess of the burst capacity handled by individual resource credit balances. Instead of trying to predict which particular instances may receive such high workloads a resource credit pool may be implemented to provide additional resource credits to one or more instances in a group of virtual compute instances. The aggregate workload for a large group of instances may be more easily determined based on various statistical techniques . Thus the resource credit pool may be filled with sufficient resource credits to process the aggregate workload in a more cost effective manner.

Consuming resource credits a virtual compute instance may utilize sufficient resources e.g. CPU cores network interface card functions etc. to obtain high performance when needed. However to perform some work requests the individual resource credit balance may be insufficient to complete the work requests at a high performance level. For example if no resource credits are available when performing a work request a baseline utilization guarantee may still be applied to perform the work request. A provider network may implement a resource credit pool which may replenish resource credits to individual resource credit balances . For example resource credit requests may be made to the resource credit pool to obtain additional resource credits when it may be determined that additional resource credits are need to complete one or more work requests for a virtual compute instance. The utilization of underlying physical resources when credits are applied such as when credits obtained from resource credit pool are applied may trigger migration events for some virtualization hosts as described below with regard to which may migrate virtual compute instances from one virtualization host to another in order to provide capacity to apply the additional resource credits for the virtual compute instance s work requests.

Different resource credit pools may correspond to different types of physical computer resources. In some embodiments virtual compute instances may be authorized to access multiple different resource credit pools corresponding to different physical computer resources. Resource credit pools may also be linked to a single user or payment account from which funds may be drawn to obtain additional resource credit s to replenish the resource credit pool. Different replenishment policies for resource credit pool may be implemented providing automated or manually requested replenishment.

Please note that previous descriptions are not intended to be limiting but are merely provided as an example of a resource credit pool for replenishing individual resource credit balances of virtual compute instances. Accumulation rates initial balances and balances limits may all be different as may be the various amounts in which resource credits may be used.

This specification next includes a general description of a provider network which may implement resource credit pools for replenishing individual resource credit balances of virtual compute instances. Then various examples of a provider network are discussed including different components modules or arrangements of components module that may be employed as part of the provider network. A number of different methods and techniques to implement a resource credit pool for replenishing individual resource credit balances are then discussed some of which are illustrated in accompanying flowcharts. Finally a description of an example computing system upon which the various components modules systems devices and or nodes may be implemented is provided. Various examples are provided throughout the specification.

In various embodiments provider network may implement a control plane in order to manage the computing resource offerings provided to clients by provider network . Control plane may implement various different components to manage the computing resource offerings. Control plane may be implemented across a variety of servers nodes or other computing systems or devices such as computing system described below with regard to . It is noted that where one or more instances of a given component may exist reference to that component herein may be made in either the singular or the plural. However usage of either form is not intended to preclude the other.

In at least some embodiments control plane may implement interface . Interface may be configured to process incoming requests received via network and direct them to the appropriate component for further processing. In at least some embodiments interface may be a network based interface and may be implemented as a graphical interface e.g. as part of an administration control panel or web site and or as a programmatic interface e.g. handling various Application Programming Interface API commands . In various embodiments interface may be implemented as part of a front end module or component dispatching requests to the various other components such as resource management reservation management resource credit pool management and resource monitoring . Clients may in various embodiments may not directly provision launch or configure resources but may send requests to control plane such that the illustrated components or other components functions or services not illustrated may perform the requested actions. discussed below provides various examples of requests that may be processed via interface .

Control plane may implement resource management module to manage the access to capacity of mappings to and other control or direction of computing resources offered by provider network. In at least some embodiments resource management module may provide both a direct sell and 3party resell market for capacity reservations e.g. reserved compute instances . For example resource management module may allow clients via interface to learn about select purchase access to and or reserve capacity for computing resources either from an initial sale marketplace or a resale marketplace via a web page or via an API. For example resource management component may via interface provide a listings of different available compute instance types each with a different credit accumulation rate. Additionally in some embodiments resource management module may be configured to offer credits for purchase in addition to credits provided via the credit accumulation rate for an instance type for a specified purchase amount or scheme e.g. lump sum additional periodic payments etc. . For example resource management module may be configured to receive a credit purchase request e.g. an API request and credit the resource credit pool with the purchased credits. Similarly resource management module may be configured to handle a request to reconfigure an instance such as increase a credit accumulation rate for a particular instance. Resource management may also offer and or implement a flexible set of resource reservation control and access interfaces for clients via interface . For example resource management module may provide credentials or permissions to clients such that compute instance control operations interactions between clients and in use computing resources may be performed. In at least some embodiments resource management modules may be configured to perform various migrations of virtual compute instances from one virtualization host to another in response to detecting migration events as discussed below with regard to .

In various embodiments reservation management module may be configured to handle the various pricing schemes of instances at least for the initial sale marketplace in various embodiments. For example network based virtual computing service may support several different purchasing modes which may also be referred to herein as reservation modes in some embodiments for example term reservations i.e. reserved compute instances on demand resource allocation or spot price based resource allocation. Using the long term reservation mode a client may make a low one time upfront payment for a compute instance or other computing resource reserve it for a specified duration such as a one or three year term and pay a low hourly rate for the instance the client would be assured of having the reserved instance available for the term of the reservation. Using on demand mode a client could pay for capacity by the hour or some appropriate time unit without any long term commitments or upfront payments. In the spot price mode a client could specify the maximum price per unit time that it is willing to pay for a particular type of compute instance or other computing resource and if the client s maximum price exceeded a dynamic spot price determined at least in part by supply and demand that type of resource would be provided to the client.

During periods when the supply of the requested resource type exceeded the demand the spot price may become significantly lower than the price for on demand mode. In some implementations if the spot price increases beyond the maximum bid specified by a client a resource allocation may be interrupted i.e. a resource instance that was previously allocated to the client may be reclaimed by the resource management module and may be allocated to some other client that is willing to pay a higher price. Resource capacity reservations may also update control plane data store to reflect changes in ownership client use client accounts or other resource information.

In various embodiments control plane may implement resource credit pool management . Resource credit pool management may in various embodiments be configured to manage and handle requests to create configure add instances or remove instances or any other management operation as part of providing resource credit pools. Resource credit pool management may store resource credit pool balances authorized instances or any other information in control plane data store . Resource credit pool management may in various embodiments handle resource credit requests determine the number of resource credits to provide send responses to add credits or deny the resource request and update the resource credit pool based on replenishment actions to individual resource credit balances or acquisitions of new resource credits for the resource credit pool. Resource credit pool management may request resource migrations from resource management module and perform evaluations of virtualization hosts to detect migration events.

In various embodiments control plane may implement resource monitoring module . Resource monitoring module may track the consumption of various computing instances e.g. resource credit balances resource credit consumption consumed for different virtual computer resources clients user accounts and or specific instances. In at least some embodiments resource monitoring module may implement various administrative actions to stop heal manage or otherwise respond to various different scenarios in the fleet of virtualization hosts and instances . Resource monitoring module may also provide access to various metric data for client s as well as manage client configured alarms. Information collected by monitoring module may be used to detect migration events for virtualization hosts in some embodiments.

In various embodiments control plane may implement a billing management module not illustrated . The billing management module may be configured to detect billing events e.g. specific dates times usages requests for bill or any other cause to generate a bill for a particular user account or payment account linked to user accounts . In response to detecting the billing event billing management module may be configured to generate a bill for a user account or payment account linked to user accounts.

A virtual compute instance may for example comprise one or more servers with a specified computational capacity which may be specified by indicating the type and number of CPUs the main memory size and so on and a specified software stack e.g. a particular version of an operating system which may in turn run on top of a hypervisor . A number of different types of computing devices may be used singly or in combination to implement the compute instances of provider network in different embodiments including general purpose or special purpose computer servers storage devices network devices and the like. In some embodiments instance clients or other any other user may be configured and or authorized to direct network traffic to a compute instance .

Compute instances may operate or implement a variety of different platforms such as application server instances Java virtual machines JVMs general purpose or special purpose operating systems platforms that support various interpreted or compiled programming languages such as Ruby Perl Python C C and the like or high performance computing platforms suitable for performing client applications without for example requiring the client to access an instance . There may be various different types of compute instances. In at least some embodiments there may be compute instances that implement rolling resource credit balances for scheduling virtual computer resource operations. This type of instance may perform based on resource credits where resource credits represent time an instance can spend on a physical resource doing work e.g. processing time on a physical CPU time utilizing a network communication channel etc. . The more resource credits an instance has for computer resources the more time it may spend on the physical resources executing work increasing performance . Resource credits may be provided at launch of an instance and may be defined as utilization time e.g. CPU time such as CPU minutes which may represent the time an instance s virtual resources can spend on underlying physical resources performing a task.

In various embodiments resource credits may represent time or utilization of resources in excess of a baseline utilization guarantee. For example a compute instance may have a baseline utilization guarantee of 10 for a resource and thus resource credits may increase the utilization for the resource above 10 . Even if no resource credits remain utilization may still be granted to the compute instance at the 10 baseline. Credit consumption may only happen when the instance needs the physical resources to perform the work above the baseline performance. In some embodiments credits may be refreshed or accumulated to the resource credit balance whether or not a compute instance submits work requests that consume the baseline utilization guarantee of the resource.

Different types of compute instances may be offered. Different compute instances may have a particular number of virtual CPU cores memory cache storage networking as well as any other performance characteristic. Configurations of compute instances may also include their location in a particular data center availability zone geographic location etc. . . . and in the case of reserved compute instances reservation term length. Different compute instances may have different resource credit accumulation rates for different virtual resources which may be a number of resource credits that accumulate to the current balance of resource credits maintained for a compute instance. For example one type of compute instance may accumulate 6 credits per hour for one virtual computer resource while another type of compute instance may accumulate 24 credits per hour for the same type of computer resource in some embodiments. In another example the resource credit accumulation rate for one resource e.g. CPU may be different than the resource credit accumulation rate for a different computer resource e.g. networking channel for the same virtual compute instance. In some embodiments multiple different resource credit balances may be maintained for a virtual compute instance for the multiple different physical resources used by the virtual compute instances. A baseline performance guarantee may also be implemented for each of the computer resources which may be different for each virtual computer resource as well as for the different instance types.

Baseline performance guarantees may be included along with the resource credit accumulation rates in some embodiments. Thus in one example an instance type may include a specific resource credit accumulation rate and guaranteed baseline performance for processing and another specific resource credit accumulation rate and guaranteed baseline performance rate for networking channels. In this way provider network may offer many different types of instances with different combinations of resource credit accumulation rates and baseline guarantees for different virtual computer resources. These different configurations may be priced differently according to the resource credit accumulation rates and baseline performance rates in addition to the various physical and or virtual capabilities. In some embodiments a virtual compute instance may be reserved and or utilized for an hourly price. While a long term reserved instance configuration may utilize a different pricing scheme but still include the credit accumulation rates and baseline performance guarantees.

As illustrated in a virtualization host such as virtualization hosts through may implement and or manage multiple compute instances in some embodiments and may be one or more computing devices such as computing system described below with regard to . A virtualization host may include a virtualization management module such as virtualization management modules through capable of instantiating and managing a number of different client accessible virtual machines or compute instances . The virtualization management module may include for example a hypervisor and an administrative instance of an operating system which may be termed a domain zero or dom0 operating system in some implementations. The dom0 operating system may not be accessible by clients on whose behalf the compute instances run but may instead be responsible for various administrative or control plane operations of the network provider including handling the network traffic directed to or from the compute instances .

Client s may encompass any type of client configurable to submit requests to network based virtual computing service . For example a given client may include a suitable version of a web browser or may include a plug in module or other type of code module configured to execute as an extension to or within an execution environment provided by a web browser. Alternatively a client may encompass an application such as a dashboard application or user interface thereof a media application an office application or any other application that may make use of compute instances to perform various operations. In some embodiments such an application may include sufficient protocol support e.g. for a suitable version of Hypertext Transfer Protocol HTTP for generating and processing network based services requests without necessarily implementing full browser support for all types of network based data. In some embodiments clients may be configured to generate network based services requests according to a Representational State Transfer REST style network based services architecture a document or message based network based services architecture or another suitable network based services architecture. In some embodiments a client e.g. a computational client may be configured to provide access to a compute instance in a manner that is transparent to applications implement on the client utilizing computational resources provided by the compute instance .

Clients may convey network based services requests to network based virtual computing service via network . In various embodiments network may encompass any suitable combination of networking hardware and protocols necessary to establish network based communications between clients and network based virtual computing service . For example a network may generally encompass the various telecommunications networks and service providers that collectively implement the Internet. A network may also include private networks such as local area networks LANs or wide area networks WANs as well as public or private wireless networks. For example both a given client and network based virtual computing service may be respectively provisioned within enterprises having their own internal networks. In such an embodiment a network may include the hardware e.g. modems routers switches load balancers proxy servers etc. and software e.g. protocol stacks accounting software firewall security software etc. necessary to establish a networking link between given client and the Internet as well as between the Internet and network based virtual computing service . It is noted that in some embodiments clients may communicate with network based virtual computing service using a private network rather than the public Internet.

In virtualization management module may implement resource credit balance scheduler . Resource credit balance scheduler may act as a meta scheduler managing tracking applying deducting and or otherwise handling all individual resource credit balances for each of compute instances for the different respective physical resources . In various embodiments resource credit balance scheduler may be configured to receive virtual resource work requests from computes instances. Each work request may be directed toward the virtual computer resource corresponding to the compute instance that submitted the work. For each request resource credit balance scheduler may be configured to determine a current resource credit balance for the requesting compute instance and generate scheduling instructions to apply resource credits when performing the work request. In some embodiments resource credit balance scheduler may perform or direct the performance of the scheduling instructions directing or sending the work request to the underlying physical computing resources to be performed as illustrated by the arrow between and . For example in some embodiments different hardware queues may be implemented and resource credit balance scheduler may be used to place tasks for performing work requests in the queues according to the applied resource credits e.g. queuing tasks according to the amount of time of applied resource credits . However in some embodiments the resource scheduling instructions may be sent to virtual compute resource scheduler which may be a scheduler for the physical resources such as CPU s implemented at virtualization host .

In some embodiments in response to receiving the scheduling instructions virtual compute resource scheduler may provide physical scheduling instructions for work requests to physical computing resources such as physical CPU s in various embodiments. In at least some embodiments virtual compute resource scheduler may be a credit based scheduler for one or more CPUs.

Rolling resource credit balance scheduler may also report credit balance and usage metrics to along with any other host metrics health information etc. to resource monitoring module .

In some instances the individual resource credit balances may be insufficient to complete work requests . As described below with regard to credit requests may be sent via credit pool agent which handles communications between virtualization host and resource credit manager to request a number of resource credits from a particular resource credit pool. Resource credit manager may send a response authorizing additional resource credits to credit pool agent which in turn may inform the scheduler of the additional resource credits . In some embodiments scheduling instructions which may apply the additionally granted credits to an individual resource account according to a schedule or in response to events such as the completion of a migration for applying additional resource credits may be enforced.

Resource credit pools may be offered to clients of a provider network in order to allow resource utilization to be purchased for more predictable requirements than individual instance requirements. illustrates interactions between a client and a provider network that implements resource credit pools for replenishing instance resource credit balances according to some embodiments. Client similar to client s in may interact with control plane via interface . As noted above interface may be implemented as a graphical user interface e.g. at a network based site or programmatically e.g. an API .

Client may submit a request to create a resource credit pool to control plane . Creation request may indicate the type of physical computer resource for the resource credit pool to maintain resource credits. The resource credit pool creation request may also include a replenishment policy e.g. on demand periodic refill manual refill . Replenishment policies for individual resource credit balances may also be included. A separate request to configure these replenishment policies or change these replenishment policies may also be sent. The creation request may also identify the virtual compute instances authorized to obtain resource credits from the resource credit pool e.g. including a list of instance identifiers a zone region or other indication of instances that are authorized . Requests to add compute instances to those authorized to replenish credits from the resource credit pool may be sent as well as requests to remove authorization for particular compute instances.

While some replenishment policies or schemes for resource credit pools may provide for mechanisms to automatically acquire more resource credits for a resource credit pool requests to add resource credits to the resource credit pool may also be sent. As the purchase price for different types of resource credits may vary in some embodiments requests for pricing information may be sent to obtain resource credit pricing when making purchasing decisions.

In response to detecting the migration event control plane may select one or more instances to migrate to destination virtualization hosts. As illustrated in instance and instance may be selected for instance migration . Instances may be selected for migration for various reasons. For example typical utilization of the physical computer resource by instances and may offset the increased utilization provided by the additional resource credits for instance . Destination virtualization hosts and may be selected to host instances and respectively. discussed below provides further examples and techniques for selecting instances and destination virtualization hosts for migration.

Control plane may send a response authorizing a number of resource credits to be added to the local credit balance for instance . The response may include a scheduling instruction which may allow only a portion resource credits to be applied until instances and are migrated to virtualization hosts and . Control plane may also direct the instance migration performing various operations to re instantiate instances and at virtualization hosts and . For example control plane may provision a replica instance on virtualization host of instance synchronize the state of the two instances and redirect traffic to virtualization host to the new instance acting as instance . The individual resource credit balances for instances and may also be replicated to virtualization host and . Migration may be performed in such a way as to be transparent to a user or client of instances and which as the virtualization hosts may be multi tenant utilization changes due to resource credit requests may be hidden from view . Once migration is complete virtualization host may make the physical computer resources utilized by instances and available to other instances.

Control plane may send a response authorizing a number of credits to be added to the individual resource credit pool for instance . The response may include a scheduling instruction which may allow only a portion resource credits to be applied until instance is migrated to virtualization host . Control plane may also direct the instance migration performing various operations to re instantiate instance at virtualization host . For example control plane may provision a replica instance on virtualization host of instance synchronize the state of the two instances and redirect traffic to virtualization host to the new instance acting as instance . The individual resource credit balances for instance may also be replicated to virtualization host . Migration may be performed in such a way as to be transparent to a user or client of instance . Once migration is complete virtualization host may make the physical computer resources utilized by instance available to other instances.

The examples of implementing resource credit pools for replenishing individual resource credit balances discussed above with regard to have been given in regard to virtual compute instances offered by a provider network. Various other types or configurations of virtual compute instances and or a provider network may implement these techniques which may or may not be offered as part of a network based service. is a high level flowchart illustrating various methods and techniques for implementing resource credit pools for replenishing resource credit balances of virtual compute instances according to some embodiments. These techniques may be implemented using various components of provider network as described above with regard to or other system or service providing virtual computing instances.

As indicated at a resource credit pool of resource credits may be maintained to replenish individual resource credit balances of authorized compute instances in various embodiments. Resource credit pools as discussed above with regard to may pertain to a particular type of physical computer resource e.g. processing network I O or storage . Accordingly in some embodiments multiple different resource credit pools may be accessible to a virtual compute instance corresponding to different physical computer resources that the virtual compute instances utilizes to perform work requests. The resource credits in the resource credit pool may be individually applicable to increase utilization of the physical computer resource for the virtual compute instance for which the resource credits are applied.

One or multiple different virtual compute instances may be authorized to obtain resource credits from the resource credit pool. As illustrated above in virtual compute instances may be added or removed from the group of virtual compute instances authorized to obtain resource credits. Various enforcement mechanisms e.g. an access list of authorized instances may be implemented to ensure that only authorized instances obtain resource credits from a resource credit pool. In some embodiments a common set of virtual compute instances may be authorized to obtain resource credits from multiple different resource credit pools e.g. a pool for networking a pool for processing a pool for I O etc. while in other embodiments the authorized virtual compute instances may vary from one resource credit pool to another.

Resource credit pools may be replenished in various ways by obtaining more resource credits from a provider network. A provider network may offer resource credits for purchase either individually or in batches of resource credits. Resource credit pools may be refilled in automated fashion as discussed below with regard to either on demand or according to a scheduled or periodic refill rate. In some embodiments resource credits may be purchased or added on demand from instances authorized to access the resource credit pool. In at least some embodiments resource credit pools may authorize access to any virtual compute instance of a provider network. Resource credits may also be manually purchased by submitting a purchase request for resource credits as illustrated above in to refill a resource credit pool. Resource credit pricing may be determined according to a fixed pricing scheme such as price per individual resource credit which may also be discounted as larger numbers of resource credits are purchased. In some embodiments resource credit pricing may be determined according to market or otherwise variable rate.

As indicated at a resource credit request may be received for an authorized virtual compute instance to replenish the individual resource credit balance for the authorized virtual compute instance in various embodiments. The resource credit request may specify a number of resource credits in some embodiments. In response to the resource credit request a number of resource credits to add to the individual resource credit balance for the authorized compute instance may be determined as indicated at . The number of resource credits may be the same as a requested number of resource credits. While in some embodiments resource credits may be replenished to individual resource credit balances according to an individual resource credit replenishment scheme e.g. providing a pre determined number of resource credits to virtual compute instance in response to a request .

As indicated at a response may be sent indicating the number of resource credits to be added to the individual resource credit balance for the authorized compute instance. In at least some embodiments the response may include a scheduling instruction or other information directing the addition or application of the resource credits. As described above with regard to and below with regard to the virtualization host implementing the virtual compute instance may add the resource credits to the individual resource credit balance and apply them to work requests utilizing the underlying physical computer resource. The resource credit pool may be updated to remove the number of resource credits from the resource credit pool as indicated at in various embodiments.

Credit based scheduling for virtual compute instances may allow virtual compute instances to handle workloads that are irregular or unpredictable. For multiple virtual compute instances located on the same virtualization host credit based scheduling distributes utilization of underlying physical resources according the individual resource credit balances for the instances. Some increased utilization for a virtual compute instance may exceed the capacity or capability of a virtualization host to provide or without reducing the performance of other virtual compute instances located at the virtualization host . When an individual resource credit balance for a virtual compute instance is replenished it may be that the virtualization host is unable to meet the various performance commitments of the virtual compute instances located at the virtualization host. In such scenarios migrating one or more virtual compute instances to another virtualization host may allow for the additional resource credits added to an individual resource credit balance to be applied. is high level flowchart illustrating various methods and techniques for migrating instances in a provider network as part of replenishing instance resource credit balances from a resource credit pool according to some embodiments.

Similar to the description above with regard to a resource credit request may be received for an authorized virtual compute instance to replenish an individual resource credit balance for the authorized virtual compute instance from a resource credit pool. The increase in utilization provided by applying the additional resource credits may cause the overall utilization of the underlying physical resource to exceed the capacity of the underlying physical resource. Due to the unpredictably at which resource credits may be obtained and applied at individual virtual compute instances virtualization hosts may be evaluated and or monitored. A virtualization host implementing an authorized virtual compute instance that receives additional resource credits from a resource credit pool may be evaluated to detect a migration event for the virtualization host as indicated at as a result of replenishing the individual resource credit balance. For instance in at least some embodiments credit usage and other information or performance statistics for the virtualization host and the instances located on the virtualization host including the virtual compute instance for which the resource credits are requested as well as other virtual compute instances may be collected as illustrated in . The virtualization host may be evaluated based on current utilization of the underlying physical resource for the requested resource credits as well as historical utilization trends based on the virtual compute instances located on the virtualization host. The evaluation may also include adding the increase in utilization of the physical resource when the requested resource credits are applied to determine whether the utilization increase exceeds the capabilities of the virtualization host triggering a migration event. If for instance the evaluation determines that based on historical trends and current utilization information a physical resource e.g. one or more central processing units is at 80 utilization and the requested resource credits provide a 30 increase to the utilization of the physical resource then a migration event may be detected as the estimated utilization 110 exceeds the capacity of the virtualization host. Note that various other thresholds lower than 100 capacity may trigger migration detection events especially as workloads for virtual compute instances may occur in bursts . For example in some embodiments migration events may be triggered when utilization or workload for an underlying physical resource exceeds a threshold above which slows or impacts the performance of a virtual compute instance in violation of a service level agreement.

If a migration even is not detected as indicated by the negative exit from then replenishment of the individual resource credit balance for the authorized virtual compute instance may be completed as indicated at as discussed above with regard to . However if a migration event is detected as indicated by the positive exit from then a migration of one or more virtual compute instances located on the virtualization host may be performed so that the utilization capacity for the replenished virtual compute instance exists. Many different methods for selecting virtual compute instance s to migrate from a virtualization host as well as a destination virtualization host may be implemented. in some embodiments one or more virtual compute instances implemented on the virtualization host may be selected to migrate so as to provide the utilization capacity. For instance if an originating virtualization host has high CPU utilization and low memory utilization then it may be desirable to locate a virtualization host with a reverse utilization a low CPU high memory instance.

Selecting virtual compute instances to migrate may depend upon various factors. For example migration burden or workload may be assessed for the virtual compute instances in some embodiments. Migrating a larger virtual compute instance by resource utilization and or workload may be for instance more difficult or costly to perform. If the movement of multiple smaller virtual compute instances achieves the same effect then in some cases multiple virtual compute instances may be moved. In addition to the cost of migrating virtual compute instances the impact of migration on the operation of virtual compute instances may be assessed. For example the performance of the various virtual compute instances on a virtualization host may be subject to respective service level agreements SLAs . If a migration operation may cause a virtual compute instance to violate an SLA then the virtual compute instance may be less likely to be selected for migration. As noted above in various embodiments virtualization hosts may be multi tenant hosting virtual compute instances for different clients. Thus the impact of a resource credit performance on those virtual compute instances that did not request the resource credits may be minimized when selecting instances to migrate. Similarly the impact or effect of performing a migration may be examined upon the one or more virtualization hosts selected as destinations for virtual compute instances as discussed below . For example virtualization hosts in a provider network may be analyzed to determine whether utilization capacity exists to perform migration and host one or more of the instances selected for migration. Thus a possible destination virtualization host may be evaluated based on current utilization of underlying physical resources utilized by selected compute instances of migration as well as historical utilization trends based on the virtual compute instances located on the possible destination virtualization host. The analysis may also include adding the increase in utilization of the physical resources of the one or more of the selected instances to be hosted to determine whether the hosting one or more of the selected instances exceeds the capabilities of the virtualization host or negatively impacts the performance of currently hosted instances in violation of an SLA . Please note that while resource credits obtained from the resource credit pool may increase utilization of one physical resource implemented at virtualization host the utilization of many different physical computer resources at the virtualization host may also be considered when selecting virtual compute instances to migrate and destination virtualization hosts.

In some embodiments a placement technique for migrating instances may be implemented to balance utilization of resources across the virtualization hosts of a provider network. One such technique is described below with regard to elements through . As indicated at a set of candidate destination virtualization hosts may be selected for consideration when performing a migration of a virtual compute instance from the virtualization host. A provider network may for instance implement large numbers of virtualization hosts distributed across multiple data centers. It may be computationally less expensive to reduce the number hosts considered for hosting a migrated virtual compute instance. For example a set of virtualization hosts may be randomly selected. Some biases may be included when performing the selection such as those virtualization hosts that have unbalanced utilization among different underlying physical computer resources as well as those virtualization hosts that are similarly located as the originating host for the migrated instance.

As indicated at the virtual compute instances located on the virtualization host for which the migration event is detected may be scored for migration in some embodiments. For example a score may be calculated by calculating the standard deviation of the mean of the utilization percentages of resources for the virtualization host and then may determine scores for each instance according to how much a migration of the instance from the virtualization host reduces the standard deviation. A similar calculation may be performed for each candidate destination virtualization host. As indicated at the set of candidate virtualization hosts may be scored individually for each of the virtual compute instances located on the host for which a migration event has been detected. Thus if the virtualization host implements for 4instances as in then a candidate destination virtualization host may be scored for each of the 4 instances. A score for a candidate virtualization host may be determined in various ways. For example similar to the calculation above for the virtual compute instances on the originating host the standard deviation of the mean of the utilization percentages of resources for the candidate destination virtualization host may be determined and then a score may be determined for each instance added to the host according to how much a migration of the instance from the virtualization host reduces improves the standard deviation. As indicated at based at least in part on the scores determined at and one or more instances may be selected to migrate to a particular destination virtualization host. For example the instances and destinations based on determining which migrations improve the standard deviations of utilization of physical computer resources at the originating and destination hosts respectively. In some embodiments minimum improvement thresholds or criteria may be implemented such that a new set of candidate destination virtualization hosts may be selected if a migration does not satisfy the criteria.

As indicated at migration of the virtual compute instances from the virtualization host to a selected destination virtualization host may be directed in various embodiments. Migration operations may provide a live migration experience in some embodiments. Thus users clients or other systems that interact with the migrated instances may experience little or no impact e.g. downtime as a result of the migration. For example migration may include provisioning and configuring a destination virtual compute instance based on the selected virtual compute instance for migration. Operation at the destination virtual compute instance may be started and may be synchronized with the currently operating virtual compute instance selected for migration in some embodiments. For instance tasks operations or other functions performed at the selected virtual compute instance may be replicated at the destination virtual compute instance. A stream of messages or indications of these tasks may be sent from the selected virtual compute instance to the destination virtual compute instance so that they may be replicated for example. Access to other computing resources e.g. a data volume or systems that are utilized by the selected virtual compute instance may be provided to the destination virtual compute instance in order to replicate or be aware of the current state of operations at the selected virtual compute instance in some embodiments. Individual resource credit balances for the virtual compute instance may be transferred to the destination virtualization host. Once synchronized in some embodiments requests for the selected virtual compute instance may be directed to the destination virtual compute instance. For example a network endpoint or other network traffic component may be modified or programmed to now direct traffic for the selected virtual compute instance to the destination virtual compute instance. Operation of the selected virtual compute instance that is currently operating may then be stopped allowing the virtualization host to use physical computer resources once used by the selected virtual compute instance.

As indicated at a response to replenish the individual resource credit balance for the authorized virtual compute instance may be sent that is configured according to the migration performed in element above in various embodiments. As the migration of one or more virtual compute instances may not occur instantaneously resources freed by migrating the virtual compute instances or new resources acquired in scenarios where the requesting virtual compute instance is moved to a different virtualization host may also not be fully available until the completion of the migration. Thus in some embodiments a scheduling instruction or other indication may be included in responses sent to replenish individual resource credit balances indicating how and or when the additional resource credits may be consumed. For example if 20 new resource credits are to be added the scheduling instruction may indicate that 10 resource credits may be immediately available while the remaining 10 resource credits may not be applied until the migration is complete. In some embodiments the response may be sent to a current and destination virtualization host if the virtual compute instance is itself migrating in response to replenishing the individual resource credit balance.

The large computing resources of a provider network may allow for increased utilization of computing resources via resource credits in a manner that makes the resource credits that may be added to or included in a resource credit pool appear unlimited to a customer of a provider network that implements resource credit pools. In this way resource credits may be acquired for a resource credit pool in manner commensurate with the type of work performed by the virtual compute instances that replenish resource credits from the resource credit pool. For example virtual compute instances may perform work that provides revenue or otherwise adds value as a result of performance. Therefore in such a scenario a replenishment scheme or technique for acquiring additional resource credits may provide automatic resource credit acquisitions as needed. In another example virtual compute instances may perform work that is a cost to be constrained or budgeted for e.g. support functions such as Information Technology IT services . In this scenario scheduled or manual resource credit acquisitions so as to remain within constraints for performing the work may be implemented as part of a replenishment scheme or technique. is a high level flowchart illustrating various methods and techniques replenishing a resource credit pool according to some embodiments.

As indicated at available resource credits in a resource credit pool may be monitored in various embodiments. A resource credit pool balance or other indicator of resource credits may be maintained and or updated in response to resource credit acquisitions or deductions for replenishing individual resource credit balances. The available resource credits may in some embodiments be compared to a replenishment threshold as indicated at . If as indicated by the negative exit from the available resource credits are above the replenishment threshold then monitoring of the available resource credits may continue. However if the available resource credits are below the replenishment threshold as indicated by the positive exit from then a replenishment action may be necessary for the resource credit pool.

A replenishment policy or scheme may be implemented for a resource credit pool in various embodiments. As illustrated above in the replenishment policy may be configured at the creation of or during the existence of the resource credit pool. The replenishment policy for the resource credit pool may provide various instructions or actions to take as part of replenishing the resource credit pool. For example the replenishment policy may indicate the replenishment threshold or other triggering event that determines when new resource credits should be acquired for the resource credit pool. In some embodiments the replenishment policy may indicate pricing limits or spending limits which may be determinative as to the number resource credits acquired. The replenishment policy may in some embodiments describe a schedule e.g. monthly weekly or daily of resource credit acquisitions or indicate that resource credit acquisitions are manually performed while other replenishment policies may refill the resource credit pool on demand from authorized virtual compute instances.

As indicated at in some embodiments the replenishment policy for the resource credit pool may provide for an automated replenishment of resource credits. As indicated at resource credits may be obtained from the provider network to replenish the resource credit pool according to the automated replenishment policy in some embodiments. For example as noted above the replenishment policy may describe a fixed number of resource credits to purchase or fixed amount of purchasing funds. In some embodiments the number of resource credits may be determined based on the replenishment threshold e.g. how many resource credits to be acquired in order to exceed the replenishment threshold . In some embodiments resource credits may be acquired at a pre determined price. In at least some embodiments a provider network may offer resource credits for purchase to replenishing resource credit pools according to a market value for the resource credits. Thus prices for resource credits may vary e.g. depending on the type of underlying physical computer resource . When obtaining resource credits the current credit price may be determined as illustrated above in .

If as indicated by the negative exit from the replenishment policy for the resource credit pool is not automated then a low resource credit notification for the resource credit pool may be sent e.g. to a client associated with a user account as indicated at .

As discussed above with regard to resource credit balances may be applied to perform work requests at the type of underlying physical computer resource to which the resource credits correspond. Thus work requests to utilize processing resources for instance may be performed by applying processing resource credits from the processing resource credit balance of a virtual compute instance requesting the work. In some embodiments a credit based scheduler or other component of a virtualization host or other system implementing a virtual compute instance may be configured to perform work requests in such a manner.

While in some embodiments resource credit balances for compute instances may be replenished according to a periodic refill rate and or carrying over unused resource credits some workloads or numbers of work requests for a virtual compute instance may be sufficient to exhaust the individual resource credit balance for the instance and type of physical computer resource . Virtualization hosts or other system implementing a virtual compute instance may be able to determine when a resource credit balance needs replenishment from a resource credit pool. is a high level flowchart illustrating various methods and techniques requesting resource credits from a resource credit pool for a particular instance according to some embodiments.

As indicated at an individual resource credit balance for a virtual compute instance implemented at a virtualization host may be maintained. As resource credits are expended or added a table entry or other set of metadata describing resource credit balances may be updated for example. In at least some embodiments multiple individual resource credit balances for different types of physical computer resources may be maintained e.g. processing network I O or storage . As virtualization hosts may also implement other virtual compute instances other individual resource credit balances for those other virtual compute instances may also be maintained in some embodiments.

Work requests may be received and or instigated at the virtual compute instance. These work requests may requests to perform a certain amount of processing data transfer over a network or any other utilization of a physical computer resource implemented at the virtualization host. For some work requests resource credits maintained in the individual resource credit balance may be sufficient to perform the work request. However in some cases work requests for a virtual compute instance may exceed the individual resource credit balance. As indicated at a number of resource credits to perform work request s at the virtual compute in addition to the available resource credits in the individual resource credit balance may be determined in various embodiments. For example if the work request s utilize a resource for a certain duration or size of operation e.g. sending network packets over a network to multiple destinations at a certain frequency the amount of resource credits to operate a full utilization of the physical computer resource until completion of the work request s may be determined by calculating the number of resource credits necessary to provide utilization of the physical computer resource for the duration of or amount of work in the work requests. If for instance an application running on a virtual compute instance needs to perform 500 I O operations per second IOPS then a corresponding number of I O resource credits to provide utilization of the physical I O channel that achieve 500 IOPS may be calculated based on the utilization value of individual I O resource credits.

Once the number of additional resource credits for performing the work requests is determined a resource credit request may be sent to obtain the number of additional resource credits from a resource credit pool as indicated at in various embodiments. In various embodiments authorization and or identification credentials may be included in the resource credit request. Other information may also be included such as the individual resource credit balance for the virtual compute instance which may be used for example to prioritize replenishment requests in some embodiments. The request may be formatted according to an API or other protocol for resource credit pool manager or other system or device that manages the resource credit pool.

A response may be received in various embodiments to add at least one resource credit to update the individual resource credit balance as indicated at . For example although 10 resource credits may have been requested the response may only indicate to add 5 resource credits if for example the resource credit pool manager implements prioritization or replenishment schemes for replenishing individual resource credit balances . In some embodiments as noted above with regard to the response may include a scheduling or other application instruction for the credits e.g. a rate or event in which some or all of the additional resource credits may be added to a resource credit pool such as at the completion of a migration operation . The updated individual resource credit balance may be applied to perform the work requests as indicated at in various embodiments. With the resource credits added to the resource credit balance for the virtual compute instance for instance the credit based scheduler or other component that applies enforces physical computer resource utilization according to the individual resource credit balances may consider apply the additional resource credits when determining utilization of the physical computer resource for the virtual compute instance.

The methods described herein may in various embodiments be implemented by any combination of hardware and software. For example in one embodiment the methods may be implemented by a computer system e.g. a computer system as in that includes one or more processors executing program instructions stored on a computer readable storage medium coupled to the processors. The program instructions may be configured to implement the functionality described herein e.g. the functionality of various servers and other components that implement the network based virtual computing resource provider described herein . The various methods as illustrated in the figures and described herein represent example embodiments of methods. The order of any method may be changed and various elements may be added reordered combined omitted modified etc.

Embodiments of resource credit pools for replenishing resource credit balances of virtual compute instances as described herein may be executed on one or more computer systems which may interact with various other devices. is a block diagram illustrating an example computer system according to various embodiments. For example computer system may be configured to implement nodes of a distributed system and or a client in different embodiments. Computer system may be any of various types of devices including but not limited to a personal computer system desktop computer laptop or notebook computer mainframe computer system handheld computer workstation network computer a consumer device application server storage device telephone mobile telephone or in general any type of computing device.

Computer system includes one or more processors any of which may include multiple cores which may be single or multi threaded coupled to a system memory via an input output I O interface . Computer system further includes a network interface coupled to I O interface . In various embodiments computer system may be a uniprocessor system including one processor or a multiprocessor system including several processors e.g. two four eight or another suitable number . Processors may be any suitable processors capable of executing instructions. For example in various embodiments processors may be general purpose or embedded processors implementing any of a variety of instruction set architectures ISAs such as the PowerPC SPARC or MIPS ISAs or any other suitable ISA. In multiprocessor systems each of processors may commonly but not necessarily implement the same ISA. The computer system also includes one or more network communication devices e.g. network interface for communicating with other systems and or components over a communications network e.g. Internet LAN etc. . For example a client application executing on system may use network interface to communicate with a server application executing on a single server or on a cluster of servers that implement one or more of the components of the provider network described herein. In another example an instance of a server application executing on computer system may use network interface to communicate with other instances of the server application or another server application that may be implemented on other computer systems e.g. computer systems .

In the illustrated embodiment computer system also includes one or more persistent storage devices and or one or more I O devices . In various embodiments persistent storage devices may correspond to disk drives tape drives solid state memory other mass storage devices or any other persistent storage device. Computer system or a distributed application or operating system operating thereon may store instructions and or data in persistent storage devices as desired and may retrieve the stored instruction and or data as needed. For example in some embodiments computer system may host a storage system server node and persistent storage may include the SSDs attached to that server node.

Computer system includes one or more system memories that are configured to store instructions and data accessible by processor s . In various embodiments system memories may be implemented using any suitable memory technology e.g. one or more of cache static random access memory SRAM DRAM RDRAM EDO RAM DDR 10 RAM synchronous dynamic RAM SDRAM Rambus RAM EEPROM non volatile Flash type memory or any other type of memory . System memory may contain program instructions that are executable by processor s to implement the methods and techniques described herein. In various embodiments program instructions may be encoded in platform native binary any interpreted language such as Java byte code or in any other language such as C C Java etc. or in any combination thereof. For example in the illustrated embodiment program instructions include program instructions executable to implement the functionality of a provider network or a virtualization host in different embodiments. In some embodiments program instructions may implement multiple separate clients server nodes and or other components.

In some embodiments program instructions may include instructions executable to implement an operating system not shown which may be any of various operating systems such as UNIX LINUX Solaris MacOS Windows etc. Any or all of program instructions may be provided as a computer program product or software that may include a non transitory computer readable storage medium having stored thereon instructions which may be used to program a computer system or other electronic devices to perform a process according to various embodiments. A non transitory computer readable storage medium may include any mechanism for storing information in a form e.g. software processing application readable by a machine e.g. a computer . Generally speaking a non transitory computer accessible medium may include computer readable storage media or memory media such as magnetic or optical media e.g. disk or DVD CD ROM coupled to computer system via I O interface . A non transitory computer readable storage medium may also include any volatile or non volatile media such as RAM e.g. SDRAM DDR SDRAM RDRAM SRAM etc. ROM etc. that may be included in some embodiments of computer system as system memory or another type of memory. In other embodiments program instructions may be communicated using optical acoustical or other form of propagated signal e.g. carrier waves infrared signals digital signals etc. conveyed via a communication medium such as a network and or a wireless link such as may be implemented via network interface .

In some embodiments system memory may include data store which may be configured as described herein. In general system memory e.g. data store within system memory persistent storage and or remote storage may store data blocks replicas of data blocks metadata associated with data blocks and or their state configuration information and or any other information usable in implementing the methods and techniques described herein.

In one embodiment I O interface may be configured to coordinate I O traffic between processor system memory and any peripheral devices in the system including through network interface or other peripheral interfaces. In some embodiments I O interface may perform any necessary protocol timing or other data transformations to convert data signals from one component e.g. system memory into a format suitable for use by another component e.g. processor . In some embodiments I O interface may include support for devices attached through various types of peripheral buses such as a variant of the Peripheral Component Interconnect PCI bus standard or the Universal Serial Bus USB standard for example. In some embodiments the function of I O interface may be split into two or more separate components such as a north bridge and a south bridge for example. Also in some embodiments some or all of the functionality of I O interface such as an interface to system memory may be incorporated directly into processor .

Network interface may be configured to allow data to be exchanged between computer system and other devices attached to a network such as other computer systems which may implement one or more storage system server nodes database engine head nodes and or clients of the database systems described herein for example. In addition network interface may be configured to allow communication between computer system and various I O devices and or remote storage . Input output devices may in some embodiments include one or more display terminals keyboards keypads touchpads scanning devices voice or optical recognition devices or any other devices suitable for entering or retrieving data by one or more computer systems . Multiple input output devices may be present in computer system or may be distributed on various nodes of a distributed system that includes computer system . In some embodiments similar input output devices may be separate from computer system and may interact with one or more nodes of a distributed system that includes computer system through a wired or wireless connection such as over network interface . Network interface may commonly support one or more wireless networking protocols e.g. Wi Fi IEEE 802.11 or another wireless networking standard . However in various embodiments network interface may support communication via any suitable wired or wireless general data networks such as other types of Ethernet networks for example. Additionally network interface may support communication via telecommunications telephony networks such as analog voice networks or digital fiber communications networks via storage area networks such as Fibre Channel SANs or via any other suitable type of network and or protocol. In various embodiments computer system may include more fewer or different components than those illustrated in e.g. displays video cards audio cards peripheral devices other network interfaces such as an ATM interface an Ethernet interface a Frame Relay interface etc. 

It is noted that any of the distributed system embodiments described herein or any of their components may be implemented as one or more network based services. For example a compute cluster within a computing service may present computing services and or other types of services that employ the distributed computing systems described herein to clients as network based services. In some embodiments a network based service may be implemented by a software and or hardware system designed to support interoperable machine to machine interaction over a network. A network based service may have an interface described in a machine processable format such as the Web Services Description Language WSDL . Other systems may interact with the network based service in a manner prescribed by the description of the network based service s interface. For example the network based service may define various operations that other systems may invoke and may define a particular application programming interface API to which other systems may be expected to conform when requesting the various operations. though

In various embodiments a network based service may be requested or invoked through the use of a message that includes parameters and or data associated with the network based services request. Such a message may be formatted according to a particular markup language such as Extensible Markup Language XML and or may be encapsulated using a protocol such as Simple Object Access Protocol SOAP . To perform a network based services request a network based services client may assemble a message including the request and convey the message to an addressable endpoint e.g. a Uniform Resource Locator URL corresponding to the network based service using an Internet based application layer transfer protocol such as Hypertext Transfer Protocol HTTP .

In some embodiments network based services may be implemented using Representational State Transfer RESTful techniques rather than message based techniques. For example a network based service implemented according to a RESTful technique may be invoked through parameters included within an HTTP method such as PUT GET or DELETE rather than encapsulated within a SOAP message.

Although the embodiments above have been described in considerable detail numerous variations and modifications may be made as would become apparent to those skilled in the art once the above disclosure is fully appreciated. It is intended that the following claims be interpreted to embrace all such modifications and changes and accordingly the above description to be regarded in an illustrative rather than a restrictive sense.

