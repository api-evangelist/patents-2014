---

title: Modifying one or more session parameters for a coordinated display session between a plurality of proximate client devices based upon eye movements of a viewing population
abstract: In an embodiment, a control device configures session parameters (e.g., related to an audio component, a video component, an eye tracking component, etc.) for a coordinated display session. The control devices maps, for proximate client devices registered as presentation devices for the coordinated display session, a different portion of visual data for the coordinated display session to respective display screens, and delivers the mapped portions of the visual data to the proximate client devices for presentation by the respective display screens during the coordinated display session. The control device obtains eye movement monitoring feedback from a set of eye tracking devices, the eye movement monitoring feedback characterizing eye movements of a viewing population of the coordinated display session. The control device modifies the session parameters associated with the coordinated display session based on the eye movement monitoring feedback.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09047042&OS=09047042&RS=09047042
owner: QUALCOMM Incorporated
number: 09047042
owner_city: San Diego
owner_country: US
publication_date: 20140417
---
The present application for patent claims priority to Provisional Application No. 61 813 891 entitled COORDINATING A DISPLAY FUNCTION BETWEEN A PLURALITY OF PROXIMATE CLIENT DEVICES filed Apr. 19 2013 by the same inventors as the subject application assigned to the assignee hereof and hereby expressly incorporated by reference herein in its entirety.

Embodiments of the invention relate to modifying one or more session parameters for a coordinated display function between a plurality of proximate client devices based upon eye movements of a viewing population.

Wireless communication systems have developed through various generations including a first generation analog wireless phone service 1G a second generation 2G digital wireless phone service including interim 2.5G and 2.75G networks and a third generation 3G high speed data Internet capable wireless service. There are presently many different types of wireless communication systems in use including Cellular and Personal Communications Service PCS systems. Examples of known cellular systems include the cellular Analog Advanced Mobile Phone System AMPS and digital cellular systems based on Code Division Multiple Access CDMA Frequency Division Multiple Access FDMA Time Division Multiple Access TDMA the Global System for Mobile access GSM variation of TDMA and newer hybrid digital communication systems using both TDMA and CDMA technologies.

It is typical for client devices e.g. laptops desktops tablet computers cell phones etc. to be provisioned with one or more display screens. However during playback of visual data e.g. image data video data etc. each client device is usually limited to outputting the visual data via its own display screen s . Even where one client device forwards the visual data to another client device the output of the visual data is typically constrained to the set of display screens connected to one particular client device.

For example if a group of users have access to multiple display devices e.g. iPhones Android phones iPads etc. and the group of users wants to display a big image or video the group of users is must typically use the display device with the biggest display screen. For example if the group of users collectively has four 4 smart phones and three 3 tablet computers the group of users will probably select one of the tablet computers for displaying the video or image. As will be appreciated many of the available display screens go unused in this scenario.

In an embodiment a control device configures session parameters e.g. related to an audio component a video component an eye tracking component etc. for a coordinated display session. The control devices maps for proximate client devices registered as presentation devices for the coordinated display session a different portion of visual data for the coordinated display session to respective display screens and delivers the mapped portions of the visual data to the proximate client devices for presentation by the respective display screens during the coordinated display session. The control device obtains eye movement monitoring feedback from a set of eye tracking devices the eye movement monitoring feedback characterizing eye movements of a viewing population of the coordinated display session. The control device modifies the session parameters associated with the coordinated display session based on the eye movement monitoring feedback.

Aspects of the invention are disclosed in the following description and related drawings directed to specific embodiments of the invention. Alternate embodiments may be devised without departing from the scope of the invention. Additionally well known elements of the invention will not be described in detail or will be omitted so as not to obscure the relevant details of the invention.

The words exemplary and or example are used herein to mean serving as an example instance or illustration. Any embodiment described herein as exemplary and or example is not necessarily to be construed as preferred or advantageous over other embodiments. Likewise the term embodiments of the invention does not require that all embodiments of the invention include the discussed feature advantage or mode of operation.

Further many embodiments are described in terms of sequences of actions to be performed by for example elements of a computing device. It will be recognized that various actions described herein can be performed by specific circuits e.g. application specific integrated circuits ASICs by program instructions being executed by one or more processors or by a combination of both. Additionally these sequence of actions described herein can be considered to be embodied entirely within any form of computer readable storage medium having stored therein a corresponding set of computer instructions that upon execution would cause an associated processor to perform the functionality described herein. Thus the various aspects of the invention may be embodied in a number of different forms all of which have been contemplated to be within the scope of the claimed subject matter. In addition for each of the embodiments described herein the corresponding form of any such embodiments may be described herein as for example logic configured to perform the described action.

A client device referred to herein as a user equipment UE may be mobile or stationary and may communicate with a radio access network RAN . As used herein the term UE may be referred to interchangeably as an access terminal or AT a wireless device a subscriber device a subscriber terminal a subscriber station a user terminal or UT a mobile terminal a mobile station and variations thereof. Generally UEs can communicate with a core network via the RAN and through the core network the UEs can be connected with external networks such as the Internet. Of course other mechanisms of connecting to the core network and or the Internet are also possible for the UEs such as over wired access networks WiFi networks e.g. based on IEEE 802.11 etc. and so on. UEs can be embodied by any of a number of types of devices including but not limited to PC cards compact flash devices external or internal modems wireless or wireline phones and so on. A communication link through which UEs can send signals to the RAN is called an uplink channel e.g. a reverse traffic channel a reverse control channel an access channel etc. . A communication link through which the RAN can send signals to UEs is called a downlink or forward link channel e.g. a paging channel a control channel a broadcast channel a forward traffic channel etc. . As used herein the term traffic channel TCH can refer to either an uplink reverse or downlink forward traffic channel.

Referring to UEs . . . N are configured to communicate with an access network e.g. the RAN an access point etc. over a physical communications interface or layer shown in as air interfaces and or a direct wired connection. The air interfaces and can comply with a given cellular communications protocol e.g. CDMA EVDO eHRPD GSM EDGE W CDMA LTE etc. while the air interface can comply with a wireless IP protocol e.g. IEEE 802.11 . The RAN includes a plurality of access points that serve UEs over air interfaces such as the air interfaces and . The access points in the RAN can be referred to as access nodes or ANs access points or APs base stations or BSs Node Bs eNode Bs and so on. These access points can be terrestrial access points or ground stations or satellite access points. The RAN is configured to connect to a core network that can perform a variety of functions including bridging circuit switched CS calls between UEs served by the RAN and other UEs served by the RAN or a different RAN altogether and can also mediate an exchange of packet switched PS data with external networks such as Internet . The Internet includes a number of routing agents and processing agents not shown in for the sake of convenience . In UE N is shown as connecting to the Internet directly i.e. separate from the core network such as over an Ethernet connection of WiFi or 802.11 based network . The Internet can thereby function to bridge packet switched data communications between UE N and UEs . . . N via the core network . Also shown in is the access point that is separate from the RAN . The access point may be connected to the Internet independent of the core network e.g. via an optical communication system such as FiOS a cable modem etc. . The air interface may serve UE or UE over a local wireless connection such as IEEE 802.11 in an example. UE N is shown as a desktop computer with a wired connection to the Internet such as a direct connection to a modem or router which can correspond to the access point itself in an example e.g. for a WiFi router with both wired and wireless connectivity .

Referring to a server is shown as connected to the Internet the core network or both. The server can be implemented as a plurality of structurally separate servers or alternately may correspond to a single server. As will be described below in more detail the server is configured to support one or more communication services e.g. Voice over Internet Protocol VoIP sessions Push to Talk PTT sessions group communication sessions social networking services etc. for UEs that can connect to the server via the core network and or the Internet and or to provide content e.g. web page downloads to the UEs.

While internal components of UEs such as the UEs A and B can be embodied with different hardware configurations a basic high level UE configuration for internal hardware components is shown as platform in . The platform can receive and execute software applications data and or commands transmitted from the RAN that may ultimately come from the core network the Internet and or other remote servers and networks e.g. application server web URLs etc. . The platform can also independently execute locally stored applications without RAN interaction. The platform can include a transceiver operably coupled to an application specific integrated circuit ASIC or other processor microprocessor logic circuit or other data processing device. The ASIC or other processor executes the application programming interface API layer that interfaces with any resident programs in the memory of the wireless device. The memory can be comprised of read only or random access memory RAM and ROM EEPROM flash cards or any memory common to computer platforms. The platform also can include a local database that can store applications not actively used in memory as well as other data. The local database is typically a flash memory cell but can be any secondary storage device as known in the art such as magnetic media EEPROM optical media tape soft or hard disk or the like.

Accordingly an embodiment of the invention can include a UE e.g. UE A B etc. including the ability to perform the functions described herein. As will be appreciated by those skilled in the art the various logic elements can be embodied in discrete elements software modules executed on a processor or any combination of software and hardware to achieve the functionality disclosed herein. For example ASIC memory API and local database may all be used cooperatively to load store and execute the various functions disclosed herein and thus the logic to perform these functions may be distributed over various elements. Alternatively the functionality could be incorporated into one discrete component. Therefore the features of the UEs A and B in are to be considered merely illustrative and the invention is not limited to the illustrated features or arrangement.

The wireless communication between the UEs A and or B and the RAN can be based on different technologies such as CDMA W CDMA time division multiple access TDMA frequency division multiple access FDMA Orthogonal Frequency Division Multiplexing OFDM GSM or other protocols that may be used in a wireless communications network or a data communications network. As discussed in the foregoing and known in the art voice transmission and or data can be transmitted to the UEs from the RAN using a variety of networks and configurations. Accordingly the illustrations provided herein are not intended to limit the embodiments of the invention and are merely to aid in the description of aspects of embodiments of the invention.

Referring to the communication device includes logic configured to receive and or transmit information . In an example if the communication device corresponds to a wireless communications device e.g. UE A or B AP a BS Node B or eNodeB in the RAN etc. the logic configured to receive and or transmit information can include a wireless communications interface e.g. Bluetooth WiFi 2G CDMA W CDMA 3G 4G LTE etc. such as a wireless transceiver and associated hardware e.g. an RF antenna a MODEM a modulator and or demodulator etc. . In another example the logic configured to receive and or transmit information can correspond to a wired communications interface e.g. a serial connection a USB or Firewire connection an Ethernet connection through which the Internet can be accessed etc. . Thus if the communication device corresponds to some type of network based server e.g. server etc. the logic configured to receive and or transmit information can correspond to an Ethernet card in an example that connects the network based server to other communication entities via an Ethernet protocol. In a further example the logic configured to receive and or transmit information can include sensory or measurement hardware by which the communication device can monitor its local environment e.g. an accelerometer a temperature sensor a light sensor an antenna for monitoring local RF signals etc. . The logic configured to receive and or transmit information can also include software that when executed permits the associated hardware of the logic configured to receive and or transmit information to perform its reception and or transmission function s . However the logic configured to receive and or transmit information does not correspond to software alone and the logic configured to receive and or transmit information relies at least in part upon hardware to achieve its functionality.

Referring to the communication device further includes logic configured to process information . In an example the logic configured to process information can include at least a processor. Example implementations of the type of processing that can be performed by the logic configured to process information includes but is not limited to performing determinations establishing connections making selections between different information options performing evaluations related to data interacting with sensors coupled to the communication device to perform measurement operations converting information from one format to another e.g. between different protocols such as .wmv to .avi etc. and so on. For example the processor included in the logic configured to process information can correspond to a general purpose processor a digital signal processor DSP an ASIC a field programmable gate array FPGA or other programmable logic device discrete gate or transistor logic discrete hardware components or any combination thereof designed to perform the functions described herein. A general purpose processor may be a microprocessor but in the alternative the processor may be any conventional processor controller microcontroller or state machine. A processor may also be implemented as a combination of computing devices e.g. a combination of a DSP and a microprocessor a plurality of microprocessors one or more microprocessors in conjunction with a DSP core or any other such configuration. The logic configured to process information can also include software that when executed permits the associated hardware of the logic configured to process information to perform its processing function s . However the logic configured to process information does not correspond to software alone and the logic configured to process information relies at least in part upon hardware to achieve its functionality.

Referring to the communication device further includes logic configured to store information . In an example the logic configured to store information can include at least a non transitory memory and associated hardware e.g. a memory controller etc. . For example the non transitory memory included in the logic configured to store information can correspond to RAM memory flash memory ROM memory EPROM memory EEPROM memory registers hard disk a removable disk a CD ROM or any other form of storage medium known in the art. The logic configured to store information can also include software that when executed permits the associated hardware of the logic configured to store information to perform its storage function s . However the logic configured to store information does not correspond to software alone and the logic configured to store information relies at least in part upon hardware to achieve its functionality.

Referring to the communication device further optionally includes logic configured to present information . In an example the logic configured to present information can include at least an output device and associated hardware. For example the output device can include a video output device e.g. a display screen a port that can carry video information such as USB HDMI etc. an audio output device e.g. speakers a port that can carry audio information such as a microphone jack USB HDMI etc. a vibration device and or any other device by which information can be formatted for output or actually outputted by a user or operator of the communication device . For example if the communication device corresponds to UE A or UE B as shown in the logic configured to present information can include the display A of UE A or the touchscreen display B of UE B. In a further example the logic configured to present information can be omitted for certain communication devices such as network communication devices that do not have a local user e.g. network switches or routers remote servers such as the server etc. . The logic configured to present information can also include software that when executed permits the associated hardware of the logic configured to present information to perform its presentation function s . However the logic configured to present information does not correspond to software alone and the logic configured to present information relies at least in part upon hardware to achieve its functionality.

Referring to the communication device further optionally includes logic configured to receive local user input . In an example the logic configured to receive local user input can include at least a user input device and associated hardware. For example the user input device can include buttons a touchscreen display a keyboard a camera an audio input device e.g. a microphone or a port that can carry audio information such as a microphone jack etc. and or any other device by which information can be received from a user or operator of the communication device . For example if the communication device corresponds to UE A or UE B as shown in the logic configured to receive local user input can include the keypad A any of the buttons A or B through B the touchscreen display B etc. In a further example the logic configured to receive local user input can be omitted for certain communication devices such as network communication devices that do not have a local user e.g. network switches or routers remote servers such as the server etc. . The logic configured to receive local user input can also include software that when executed permits the associated hardware of the logic configured to receive local user input to perform its input reception function s . However the logic configured to receive local user input does not correspond to software alone and the logic configured to receive local user input relies at least in part upon hardware to achieve its functionality.

Referring to while the configured logics of through are shown as separate or distinct blocks in it will be appreciated that the hardware and or software by which the respective configured logic performs its functionality can overlap in part. For example any software used to facilitate the functionality of the configured logics of through can be stored in the non transitory memory associated with the logic configured to store information such that the configured logics of through each performs their functionality i.e. in this case software execution based in part upon the operation of software stored by the logic configured to store information . Likewise hardware that is directly associated with one of the configured logics can be borrowed or used by other configured logics from time to time. For example the processor of the logic configured to process information can format data into an appropriate format before being transmitted by the logic configured to receive and or transmit information such that the logic configured to receive and or transmit information performs its functionality i.e. in this case transmission of data based in part upon the operation of hardware i.e. the processor associated with the logic configured to process information .

Generally unless stated otherwise explicitly the phrase logic configured to as used throughout this disclosure is intended to invoke an embodiment that is at least partially implemented with hardware and is not intended to map to software only implementations that are independent of hardware. Also it will be appreciated that the configured logic or logic configured to in the various blocks are not limited to specific logic gates or elements but generally refer to the ability to perform the functionality described herein either via hardware or a combination of hardware and software . Thus the configured logics or logic configured to as illustrated in the various blocks are not necessarily implemented as logic gates or logic elements despite sharing the word logic. Other interactions or cooperation between the logic in the various blocks will become clear to one of ordinary skill in the art from a review of the embodiments described below in more detail.

The various embodiments may be implemented on any of a variety of commercially available server devices such as server illustrated in . In an example the server may correspond to one example configuration of the application server described above. In the server includes a processor coupled to volatile memory and a large capacity nonvolatile memory such as a disk drive . The server may also include a floppy disc drive compact disc CD or DVD disc drive coupled to the processor . The server may also include network access ports coupled to the processor for establishing data connections with a network such as a local area network coupled to other broadcast system computers and servers or to the Internet. In context with it will be appreciated that the server of illustrates one example implementation of the communication device whereby the logic configured to transmit and or receive information corresponds to the network access ports used by the server to communicate with the network the logic configured to process information corresponds to the processor and the logic configuration to store information corresponds to any combination of the volatile memory the disk drive and or the disc drive . The optional logic configured to present information and the optional logic configured to receive local user input are not shown explicitly in and may or may not be included therein. Thus helps to demonstrate that the communication device may be implemented as a server in addition to a UE implementation as in .

It is typical for client devices e.g. laptops desktops tablet computers cell phones etc. to be provisioned with one or more display screens. However during playback of visual data e.g. image data video data etc. each client device is usually limited to outputting the visual data via its own display screen s . Even where one client device forwards the visual data to another client device the output of the visual data is typically constrained to the set of display screens connected to one particular client device.

For example if a group of users have access to multiple display devices e.g. iPhones Android phones iPads etc. and the group of users wants to display a big image or video the group of users is must typically use the display device with the biggest display screen. For example if the group of users collectively has four 4 smart phones and three 3 tablet computers the group of users will probably select one of the tablet computers for displaying the video or image due to their larger display screen area. As will be appreciated many of the available display screens go unused in this scenario.

Embodiments of the invention are directed to methods for quick utilization of aggregate display technique for ad hoc aggregated displays based upon dynamically discovering relative position and orientation information pertaining to individual displays in the ad hoc created display group. More specifically embodiments are directed to client applications configured for execution on a set of proximate client devices for implementing a coordinated display session and a master application running on a control device for managing the coordinated display session e.g. a central server one of the proximate client devices that is engaged in the coordinated display session or another proximate client device that is not engaged in the coordinated display function . For example modern mobile devices e.g. large display smartphones tablets etc. can be kept or held adjacent to each other to form a large aggregate display screen. The master application can utilize this large aggregate display screen to facilitate a group render function where visual data spans across the large aggregate display screen as if it were a single display screen as will be explained in more detail below.

Referring to the master application on the control device detects and then connects to client device client device reports its display capability information and the master application registers client device to a coordinated display group . The display capability information reported at can include display screen size information display screen resolution information display screen brightness capabilities display screen frame rate capabilities and or any other device capability information relevant to the manner in which visual data is output by client device . The coordinated display group to which client device is registered at corresponds to a group of proximate client devices that have some degree of display capability. The coordinated display group can include mobile devices such as tablet computers or cell phones but can also include one or more relatively fixed display devices such as TV sets or computer monitors. The master application on the control device also detects and then connects to client devices . . . N e.g. where N is greater than or equal to 2 and client devices . . . N each report their respective display capability information . In an example one or more proximity conditions for registration of a particular client device into the coordinated display group can be considered before the master application registers a new client device into the coordinated display group. For example a proximity condition could be that the new client device be no more than a first proximity threshold e.g. 5 feet 10 feet etc. from any other client device in the coordinated display group that the new client device be no more than a second proximity threshold e.g. 8 inches etc. from at least one other client device in the coordinated display group that the new client device be no more than a third proximity threshold e.g. 3 feet etc. from an average distance between the new client device and each currently registered client device in the coordinated display group and so on. If the master application determines that the one or more proximity conditions are not satisfied the master application does not register the new client device into the coordinated display group and optionally forms a new coordinated display group including the new client device. Also client devices in the new coordinated display group can overlap at least in part with a preexisting coordinated display group although if both coordinated display groups are presenting visual data at the same time an overlapping client device would need to be aligned with only one of the coordinated display groups at any given time in at least one embodiment.

In the embodiment of assume that client devices . . . N are proximate to each other and that the master application recognizes their relative proximity to each other. For example client devices . . . N may each be connected to the same wireless access point client devices . . . N may be in range of each other via Bluetooth or WiFi client devices . . . N may have position determining systems e.g. GPS etc. that indicate a close geographical relationship and so on and this information can be conveyed to the master application on the control device. For instance if the control device is itself a proximate client device the proximity of client devices . . . N can be inferred by detection of the respective client devices via a local wireless protocol e.g. WiFi Bluetooth etc. . In another example if the control device is a remote server client devices . . . N may detect each other and report their detections to the remote server or client devices . . . N may simply report their individual locations from which the remote server compares to infer client device proximity. In any case based on the recognition that client devices . . . N are proximate at the master application on the control device registers client devices . . . N to the coordinated display group.

At some later point in time the master application identifies visual data to be displayed in proximity to the client devices . . . N by the coordinated display group via a coordinated display session . For example at a user of one or more of the client devices . . . N may desire to output a video via an aggregated display screen that leverages the display screens on two or more of the client devices . . . N. In a further example while not shown in the identification of the visual data e.g. a movie or other video etc. can be what triggers formation of the coordinated display group between .

In response to the determination to implement the coordinated display session via the coordinated display group at the master application receives synchronization information that indicates current relative orientation and position data for each of client devices . . . N . Examples of how the master application can obtain the synchronization information at are described below in more detail with respect to .

After obtaining the synchronization information at the master application maps a different portion of the visual data to a respective display screen of client devices . . . N based on i the display capability information of client devices . . . N as reported at and and ii the synchronization information received at . In an example client devices . . . N may correspond to less than all of the client devices that previously registered to the coordinated display group between . For example one or more registered client devices may be out of position or have moved out of proximity so as to fail to satisfy a proximity condition e.g. see one or more registered client devices may have improper or deficient orientation e.g. see or one or more registered client devices may have lost the capacity to output visual data e.g. see and in any of these cases those particular client devices can be omitted or excluded from the visual mapping of despite having previously registered to the coordinated display group by virtue of proximity and display capability. For example each mapped portion of visual data can correspond to a different non overlapping portion of the visual data that when concurrently output by the display screen on the corresponding client device functions to substantially reconstruct the visual data across the display screens of client devices . . . N. After generating the mapped portions of the visual data at the master application delivers the mapped portions of the visual data to client devices . . . N and client devices . . . N each present their respective mapped portions of the visual data and .

As will be appreciated if the control device corresponds to one of client devices . . . N in the embodiment of the detection of or can be omitted and the delivery of can correspond to the master application passing the mapped portion of visual data to the client application on the same client device e.g. as opposed to an over the air transmission via Bluetooth or WiFi or an TCP IP transmission . Also the delivery of the visual data at can be a direct transmission from the control device to client devices . . . N or alternatively the control device can direct some other device such as one of the client devices . . . N to relay the visual data to the client devices e.g. the control device can convey instructions for some other entity to stream the visual data instead of streaming the visual data itself . Further it is possible that the visual data is already stored on one or more of client devices . . . N e.g. a video file or power point presentation may already be locally stored on one or more of the client devices . In this case instead of streaming raw visual data from the control device to the client devices . . . N the control device can instead send control information to one or more client devices which asks the target client devices to render the mapped portion of the visual data thereon. For example if the visual data is a video and client device is mapped to an upper left portion of the video at the coordinates of the upper left portion can be conveyed to client device with client device being expected to render the upper left portion based on the conveyed coordinates in conjunction with its locally stored copy of the video.

In context with of the synchronization information that indicates the current relative orientation and position data for the tablet computers . . . in can be obtained by the master application in a number of ways. In one embodiment of the invention the tablet computers . . . which are assumed to be provisioned with touch screen capability are temporarily asked by the master application to enter into a synchronization learn mode. While in the synchronization learn mode a user is prompted to swipe his her finger over the aggregate display screen that is generated by tablet computers . . . . For example illustrates an example whereby the user starts by swiping the top row of tablet computers in a left to right motion and then returns to swipe the next row of tablet computers in another left to right motion. illustrates another example where the user starts by swiping the left column of tablet computers in a top to bottom motion and then returns to swipe the next column of tablet computers in another top to bottom motion and so on. In or timing characteristics such as the speed of the user s finger in conjunction with the arcs of movement can be used to estimate the relative distance and orientations of the respective tablet computers . . . . For example in if a 0.2 second gap occurs from when the user s finger leaves tablet computer before it reaches tablet computer and a 0.3 second gap occurs from when the user s finger leaves tablet computer before it reaches tablet computer assuming the user s swipe speed is relatively constant then tablet computers are closer together than tablet computers . Thus the left to right swipes or horizontal swipes from can be used to estimate how far apart the tablet computers are from each other horizontally while the top to bottom swipes or vertical swipes from can be used similarly to estimate how far apart the tablet computers are from each other vertically. As will be appreciated accurately synchronizing the tablet computers . . . using swipe detection as described above relies upon clock synchronization between tablet computers . . . . Accordingly to facilitate this function each of tablet computers . . . can report their precise system time during registration with the master application at or of in an example.

Further the swipes from are merely one manner by which the relative position and orientation data for tablet computers . . . from can be ascertained. Another option is manual entry e.g. a control screen displayed by the master application permits an operator to manually recreate the relative positions on tablet computers . . . via a virtual interface .

Yet another option is that a picture of the coordinated display group can be snapped by some other camera device reported to the master application and then analyzed to identify where tablet computers . . . are relative to each other. In a further example to facilitate the picture based synchronization for the relative position and orientation of the coordinated display group the master application can deliver a unique image e.g. a number a color a QR Code etc. to display while the camera device snaps the picture of the coordinated display group. The master can then identify the relative position and orientation data based upon detection of the unique images in the snapped image. illustrates an example whereby the unique images to facilitate the picture based synchronization for the relative position and orientation of the coordinated display group correspond to numbers and illustrate an example whereby the unique images to facilitate the picture based synchronization for the relative position and orientation of the coordinated display group correspond to QR codes or barcodes . In either or it will be appreciated that the mapping of of can include identifying each of the unique images in the picture associating each unique identified image in the picture with a corresponding one of the plurality of proximate client devices and then determining the current relative orientation and position data for each of the plurality of proximate client devices based upon the association.

It will be appreciated that requiring a user to swipe his her finger across the display screens of the coordinated display group can become impractical for medium or large aggregate screen sizes or for coordinated display groups that include some client devices without touch screen capability as illustrated in for example. In two cell phones and and two tablet computers and are used in conjunction to extend the range of a television set . In this case the television set probably does not have access to touch screen capability and even if it did it would be relatively difficult for the user to swipe his her finger across the entire screen area of the television set .

In these cases another option is to strobe a light beam or sound wave across the coordinated display group and then gauge the relative positions and orientations of its constituent client devices based on differences in timing and or angle of detection relative to the strobe. In the sound wave example for a medium size display e.g. with an aggregate size of a few feet across as shown in a sound source audible or ultrasound can be used and moved in front of the coordinated display group in specified patterns. By capturing the time when each client device recorded the peak amplitude sound and knowing the pattern the relative positions and orientations of the client devices can be derived by the master application. In the light beam example for medium to large size aggregates the devices can be pointed by a simple Laser pointer in an example. With front facing cameras on the target client devices of the coordinated display group the time stamp when direct light beam was captured can be recorded by the respective devices and reported to the master application and knowing the timing pattern of detections the relative positions and orientations of the client devices can be derived. For a very large number of devices the laser beam can be replaced by a strong directional light beam.

For a very large aggregate display e.g. thousands of client devices held by users in a stadium the users can be asked to take a picture of a fixed object e.g. a three dimensional object that is present in each user s view while being relatively close to the respective users. For example in a large number of users in a stadium can fixate on an object present in a central location of the stadium such as a stage. Each user to object relationship has a unique combination of distance and or orientation based on the different elevations and or positions of the seats in the stadium . Accordingly each user can snap a picture of the object via his her client device and then send the picture to the master application on the control device which can identify the relative position of each device by image processing and then virtually reconstruct the effective aggregated display screen of the coordinated display group. For example using the coordinated display group aspects described with respect to above thousands of client devices at the United States seating section at the Olympics can be formed into a coordinated display group for collectively reconstructing an image of the U.S. Flag which becomes visible to Olympic participants in the stadium as they compete in their respective events.

In conjunction with registering client devices X . . . Z the master application receives updated synchronization information that indicates current relative orientation and position data for each of client devices . . . N with respect to client devices X . . . Z A e.g. similar to of . For example as shown in the updated synchronization information received at A can be configured to indicate that tablet computers and are positioned in top and bottom rows respectively on the right of the aggregated display screen area.

After obtaining the updated synchronization information at A the master application updates the mapping of the visual data based on i the display capability information of client devices . . . N and X . . . Z as reported at and A and ii the updated synchronization information received at A in order to incorporate the respective display screens of client devices X . . . Z into the aggregated display screen area A. In context with because the incorporation of tablet computers and increases the horizontal portion of the aspect ratio of the aggregated display screen area i.e. the aggregated display screen area is essentially stretched wider the updated mapping at A for the example shown in would stretch the visual data across more horizontal distance which causes each of tablet computers . . . to show a slightly different portion of the visual data as compared to before tablet computers and were taken into effect by via the mapping. After generating the updated mapped portions of the visual data at A the master application delivers the updated mapped portions of the visual data to client devices . . . N and X . . . Z A and client devices . . . N and X . . . Z each present their respective updated mapped portions of the visual data A and A.

Later during the coordinated display session the master application determines to remove one or more client devices from the coordinated display session A. For convenience of explanation assume that the master application determines at A to remove client devices and from the coordinated display group while permitting client devices . . . N to remain in the coordinated display group. The determination of A can be reached in a variety of different ways. For example users of client devices and may physically move client devices and away from the aggregated display screen area client devices and may experience a low battery condition even if they are not moved and so on.

In conjunction with removing client devices and from the coordinated display group the master application obtains updated synchronization information that indicates current relative orientation and position data for each of client devices . . . N A e.g. similar to of . In the updated synchronization information obtained at A can be obtained by actively checking the current relative orientation and position of client devices . . . N or alternatively the master application can simply update the previous synchronization information by removing client devices and from the aggregated display screen area without actively double checking whether the positions or orientations of client devices . . . N have changed. For example as shown in the updated synchronization information received at A can be configured to indicate that tablet computers and are removed from the aggregated display screen area altogether and that tablet computer has been moved to tablet computer s old position in the aggregated display screen area such that tablet computers and no longer satisfy a proximity condition for the coordinated display session. In another example as shown in the updated synchronization information received at A can be configured to indicate that tablet computer no longer satisfies an orientation condition for the coordinated display session e.g. a user may flip tablet over so the display screen of tablet computer is no longer oriented in alignment with the other tablet computers . In another example as shown in the updated synchronization information received at A can be configured to indicate that tablet computer has lost its display capability and thereby no longer satisfies a display capability condition for the coordinated display session e.g. tablet computer may go into low power mode and cut off power to its display screen .

After obtaining the updated synchronization information at A the master application updates the mapping of the visual data based on i the display capability information of client devices . . . N as reported at and ii the updated synchronization information obtained at A in order to adapt the aggregated display screen area based on the departure of client devices and A. In context with because the exit of tablet computers and and the re shuffling of tablet computer to tablet computer s old position decreases the horizontal portion of the aspect ratio of the aggregated display screen area i.e. the aggregated display screen area is essentially crunched together the updated mapping at A for the example shown in would crunch or condense the visual data within less horizontal distance which causes each of tablet computers . . . to show a slightly different portion of the visual data as compared to before tablet computers and were removed. In context with or E because tablet computer can no longer be relied upon to present its mapped portion of the visual data the master application can update the mapping so as to exclude tablet computer from the coordinated display session although tablet computer could remain registered to the coordinated display group itself for re entry into the coordinated display session once its orientation and or display capability recovers . At this point the master application can either also remove tablet computer from the coordinated display session and crunch the visual data for presentation by tablet computers . . . and . . . as shown in . Alternatively the master application can omit tablet computer from the coordinated display session while retaining tablet computer in which case the resultant presentation simply omits an upper left portion of the visual data as shown in . After generating the updated mapped portions of the visual data at A the master application delivers the updated mapped portions of the visual data to client devices . . . N A and client devices . . . N each present their respective updated mapped portions of the visual data A and A.

In the embodiments described above with respect to the control device that runs the master application which manages or arbitrates the coordinated display session is described under the assumption that the identity of the control device does not change during the coordinated display session. In another embodiment the master application function can be transferred between devices during the coordinated display session. For example in the embodiment of described below the transfer of the master application function is described as occurring from client device to client device where both client devices and are display participants coupled to display screens that form part of the aggregate display screen area for the coordinated display session. However in other embodiments the master application function does not need to be transferred between two client devices that are display participants in the coordinated display session and instead could be transferred between a remote server to a local client device between local client devices that are not coupled to display screens that form part of the aggregate display screen area for the coordinated display session and so on.

Later during the coordinated display session the master application determines to transition the master application function from client device to a different device A. In the embodiment of the master application specifically determines to transfer the master application function to another client device that is outputting part of the visual data during the coordinated display session but in other embodiments the master application function could be transferred to a remote server or to a client device that is not outputting the visual data. The determination of A can be reached in a variety of different ways. For example a user of client device may physically move client device away from the aggregated display screen area to trigger the management or master application transfer decision e.g. as in where tablet computer moves away from the aggregate display screen area which triggers a transition of the master application function to tablet computer . Alternatively the master application function transition determination of A can be reached based on other triggering conditions such as client device experiencing a low battery condition.

After determining to transition the master application function away from client device at A client device negotiates with client devices . . . N in order to identify a target client device for the master application function transfer A. For convenience of explanation in the embodiment of assume that the negotiation of A results in the master application determining to transition the master application function from client device to client device such that client device will become the new control device running the master application. Under this assumption client device stops running the master application at A and client device starts running the master application at A. In an example if client device remains part of the coordinated display session after A the client device can begin or continue execution of a client application which handles client device s display function for the coordinated display session and interacts with the master application that is now running on client device . Client device on the other hand does not necessarily stop running its corresponding client application in conjunction with its ascension to control device status because it is possible that the master application on client device will handle the visual data mapping and distribution aspects while the client application on client device will separately handle the display function for the coordinated display session i.e. client device runs both the master application and client application for the coordinated display session because client device is both the control device and an active display participant of the aggregate display screen area after A .

In conjunction with transitioning the master application function from client device to client device the master application now on client device obtains updated synchronization information that indicates current relative orientation and position data for each of client devices . . . N A e.g. similar to of . In the updated synchronization information obtained at A can be obtained by actively checking the current relative orientation and position of client devices . . . N. For example as shown in the updated synchronization information received at A can be configured to indicate that tablet computer is removed from the aggregated display screen area.

After obtaining the updated synchronization information at A the master application updates the mapping of the visual data based on i the display capability information of client devices . . . N as reported at and ii the updated synchronization information obtained at A in order to adapt to any changes to the aggregated display screen area A. In context with because the exit of tablet computer skews the aspect ratio of the aggregated display screen area i.e. the aggregated display screen area is no longer an approximate rectangle the updated mapping at A for the example shown in could omit tablet computer from the aggregate display screen area while keeping tablet computer in the coordinated display group so that the rectangle of tablet computers and could be used to output a modified version of the visual data or the mapping could be updated in some other way. After generating the updated mapped portions of the visual data at A the master application delivers the updated mapped portions of the visual data to client devices and . . . N A and client devices . . . N each present their respective updated mapped portions of the visual data A A and A. In the embodiment of A is optional because the transition of the master application function away from client device may have been triggered by client device being removed from proximity to the coordinated display session e.g. see although it is possible that the client device could have maintained its status as control device even if it were removed as a display participant of the coordinated display session.

Referring to A through A substantially correspond to through of except that client devices . . . N report their respective audio output capability information in addition to their display capability information at A and A. For example client device may report the amount of decibels that its speakers are capable of outputting client device may report that it is incapable of outputting audio data e.g. no speakers it is currently muted etc. and so on. Then in addition to mapping the visual data to different client devices in A the master application can also map different portions of associated audio data to one or more of the client devices . . . N based on i the audio output capability information reported at A and or A and ii the synchronization information from A. The master application then delivers the mapped portions of the visual data to client devices . . . N at A which are presented at A A and the master application also delivers the mapped portions of the audio data to one or more of client devices . . . N at A which are output at A A.

In a further example the target client devices to which the audio data is mapped can be based in part upon the content of the visual data that is being presented. For example the aggregate display screen area in is presenting an image or video of a bicycle. If the visual data is accompanied by a honking sound the master application can determine that the bicycle s horn is being presented by tablet computer so the sound of the bike horn should emanate from tablet computer if possible . In other examples during a sports themed video a ball may be moving across the aggregate display screen area and the motion of the ball may be accompanied with some type of sound. The audio data for the sound can be tracked with the ball e.g. based on analysis of motion vectors between successive I frames etc. so that whichever client devices are currently displaying the ball are the target client devices for the associated audio output of the sound. In this example viewers of the aggregate display screen area would be able to hear the ball whizzing past as it moves between the display screens.

Referring to the master application configures a set of audio parameters for an audio component of the coordinated display session to accommodate either a default audio focus or no audio focus . For example at each client device in the coordinated display session that is capable of outputting audio may be asked to output the same respective audio component at the same volume level without any audio focus being oriented upon any particular subset of the client devices. In another example at the set of audio parameters can include audio orientation whereby client devices in the coordinated display session may have their respective mapped audio portions configured based on their relative location in the coordinated display session. An example of this type of scenario is discussed above and shown in whereby a right most client device is configured as the Right speaker a left most client device is configured as the Left speaker and a client device in the middle is configured as the Center speaker.

The set of audio parameters configured at can relate to any audio characteristic associated with the coordinated display session e.g. which client devices are asked to output audio for the session the volume level or amplitude at which one or more of the client devices are asked to output audio for the session settings such as bass treble and or fidelity associated with audio to be output by one or more of the client devices an audio orientation for the session such as 2.1 surround sound or 5.1 surround sound etc. . In another example the set of audio parameters can include how an equalizer function is applied to audio to be output for the coordinated display session e.g. how the audio is processed through an enhancing or attenuation de emphasizing equalizer function . In the equalizer function example if motion vectors e.g. see or object focus e.g. see trigger an audio focus transition an enhancing equalizer function can be used to enhance the sound on the device s that are targets for the audio focus whereas one or more non target devices e.g. devices depicting frame portions with low motion or low focus can have inverse functions viz. inverse equalizer processing applied to their respective audio portions. Accordingly while the examples below primarily relate to the set of audio parameters being volume level or audio orientation this is primarily to simplify the disclosure and other embodiments can relate to parameters for any type of audio characteristic being included among the set of audio parameters configured at and or reconfigured at .

The configuration of the set of audio parameters can occur at the beginning of the coordinated display session in an example and or at the start point of audio for the coordinated display session. While not shown explicitly in the set of audio parameters configured at can change without the audio focus being changed based on video content analysis. For example assume that the set of audio parameters is configured as shown in and that two additional tablet computers are added to the right of tablet computers and respectively. At this point the two new tablet computers become the right most client devices in the coordinated display session which can trigger a transfer of the Right speaker function to one or both of the new tablet computers and also a modification to how the video portions are being mapped so that the video being presented is stretched over the wider display area .

After the set of audio parameters is configured at assume that the coordinated display session continues for a period of time with the audio component being output in accordance with the configured set of audio parameters. During the coordinated display session the master application evaluates video content data within one or more mapped portions of the video content . For example the evaluated video content data can include one or more motion vectors e.g. see and or object focus data e.g. see . Based on the evaluation of the video content data from the master application identifies one or more of the mapped video portions as target s for the audio focus of the coordinated display session . For example a first subset of two high motion mapped video portions can be targeted for the audio focus or a second subset of mapped video portions including a block of high focus objects can be targeted for the audio focus and so on. After identifying the target s for the audio focus in the master application determines whether to transition the audio focus for the coordinated display session . For example if a current configuration of the set of the audio parameters is not focused upon the mapped video portion s identified as target s for the audio focus the master application may determine to transition the audio focus at . Of course if the current configuration of the set of the audio parameters is already focused upon the mapped video portion s identified as target s for the audio focus then no change to the audio focus may be necessary. If the master application determines not to transition the audio focus at the coordinated display session continues using the current set of audio parameters and the process returns to where the master application continues to evaluate the video content data within one or more mapped portions of the video content for the coordinated display session. Otherwise if the master application determines to transition the audio focus at the process advances to . At the master application reconfigures the set of audio parameters so as to transition the audio focus for the coordinated display session to the identified mapped video portion s from based on the video content evaluation e.g. by adjusting volume levels being output by one or more of the client devices in the session changing an audio configuration for the session e.g. 2.1 to 5.1 surround sound or vice versa modifying how enhancing or de emphasizing equalizer functions are applied to audio being mapped to one or more client devices in the session etc. . After reconfiguring the set of audio parameters at the coordinated display session continues using the reconfigured set of audio parameters and the process returns to where the master application continues to evaluate the video content data within one or more mapped portions of the video content for the coordinated display session. So to put into context with the set of audio parameters reconfigured at can be used to update the mapped portions of the audio data from A A during the coordinated display session.

For example at video frame motion vectors that correspond to the video being collectively output by the client devices participating in the coordinated display session are measured in real time by the master application. The video frame motion vectors can then be analyzed to detect an object or objects with the highest relative motion vector . Then the audio focus can shift to focus on the identified high motion object or objects by reconfiguring the set of audio parameters so that a client device outputting the mapped video portion with the detected object s outputs audio at a higher relative volume and or amplification by temporarily muting or lowering the volume output by other client devices and so on. In a specific example a given client device outputting the mapped video portion with the detected object s can have its volume raised 50 each adjacent client device to the given client device can have their respective volume raised 25 each client device that is two screens or two positions away from the given client device can play at a normal or default volume level and each other client device can have their respective volume temporarily muted or lowered by some percentage.

For example at video frames from the video being collectively output by the client devices participating in the coordinated display session are measured in real time by the master application. The video frames can then be analyzed to detect an object or objects with the highest relative object focus . Then the audio focus can shift to focus on the identified in focus object or objects by reconfiguring the set of audio parameters so that a client device outputting the mapped video portion with the detected object s outputs audio at a higher relative volume and or amplification by temporarily muting or lowering the volume output by other client devices and so on. In a specific example a given client device outputting the mapped video portion with the detected object s can have its volume raised 50 each adjacent client device to the given client device can have their respective volume raised 25 each client device that is two screens or two positions away from the given client device can play at a normal or default volume level and each other client device can have their respective volume temporarily muted or lowered by some percentage.

Further while are described as separate implementations of the process of in another example the processes of can be executed in parallel. For example consider a series of video frames with multiple objects e.g. subjects playing soccer wherein the soccer ball is moving from one subject to another. In accordance with the master application can evaluate the series of video frames to detect high motion in the mapped video portions that depict the soccer ball as it shifts from one player to another and thereby determine these mapped video portions as targets for the audio focus. However further consider that the object focus can also shift as the soccer ball is passed around because the soccer ball is probably the most relevant object in the series of video frames and a camera is likely to attempt to focus on the soccer ball and its immediate vicinity. In accordance with the master application can evaluate the series of video frames to detect one or more in focus objects e.g. the soccer ball and one or more nearby objects to it such as one of the soccer players a soccer goal etc. in the mapped video portions and thereby determine these mapped video portions as targets for the audio focus. So the combination of mapped video portions that qualify for audio focus targets in accordance with parallel execution of both and can be used as an expanded or enhanced audio focus in at least one example with the expanded or enhanced audio focus being yet another example of execution of .

Further as the coordinated display session is implemented the process of e.g. or the parallel processes of can be execute on a frame by frame basis e.g. for each video frame presented during the coordinated display session or alternatively can be implemented more selectively for specific video frames. For example the granularity of executing a frame specific audio focus procedure as in or can range from every video frame to every Nth video frame. Generally executing a frame specific audio focus procedure as in or more frequently will result in a more immersive audio experience whereas executing the frame specific audio focus procedure as in or less frequently will conserve resources e.g. power resources memory resources processing resources etc. .

Further while the client devices shown in as participating in the audio output component for the coordinated display session are also video output participants that are each receiving and outputting a video component of the coordinated display session the audio presentation devices for the coordinated display session do not necessarily need to double as video presentation devices for the coordinated display session. For example a first proximity threshold to qualify as a video presentation device for the coordinated display session may be less than a second proximity threshold to qualify as an audio presentation device for the coordinated display session e.g. because proximally disparate video sources lose their relevance to the coordinated display session more quickly than proximally disparate audio sources . So tablet computers and could remain audio presentation devices in the coordinated display session in for a period of time after tablet computers and are removed as video presentation devices in the coordinated display session. In another example an orientation requirement for video presentation is less relevant for audio presentation. So tablet computer could remain an audio presentation device in the coordinated display session in after it is flipped over because improper screen orientation does not obviate an audio function.

While are directed to selectively transitioning an audio focus for the coordinated display session based on a video content analysis of the respective mapped video portions being delivered and output by the proximate client devices participating in the coordinated display session are directed to selectively modifying one or more session parameters e.g. audio and or non audio parameters based upon eye movement monitoring feedback related to a viewing population of the coordinated display session in accordance with embodiments of the invention. The process of can execute in conjunction with any of the coordinated display sessions described above.

Referring to a set of eye tracking devices is designated for tracking eye movement of a viewing population e.g. one or more proximate viewers of a coordinated display session . The set of eye tracking devices e.g. a set of front mounted cameras provisioned on at least one of the proximate client devices participating in the coordinated display session can be designated by the master application e.g. in response proximity detection of one or more viewers or alternatively the set of eye tracking devices can be independently selected or self selecting e.g. each proximate client device participating as a video presentation device for the coordinated display session independently detects whether any viewers are proximate and if so attempts to track the eye movements of the proximate viewers . At the master application optionally configures a set of audio parameters e.g. volume levels an audio configuration for the session e.g. 2.1 to 5.1 surround sound how an enhancing or de emphasizing equalizer functions are applied to the audio being mapped to different client devices in the session etc. for an audio component of the coordinated display session e.g. similar to of . In is optional because the coordinated display session may not have an audio component e.g. video only . After assume that the coordinated display session continues for a period of time during which the master application obtains eye movement monitoring feedback from the designated set of eye tracking devices .

At in a first embodiment assume that the set of eye tracking devices corresponds to a single master eye tracking device that is responsible for tracking the eye movements of each viewer in the viewing population. In this case the master eye tracking device can execute a baselining operation which establishes the central eye position on the horizontal axis and vertical axis. The baselining operation could be triggered as a dedicated calibration step moment time window during setup of the coordinated display session irrespective of where the viewing population is expected to be looking at that particular time. Alternatively the baselining operation can be triggered in association with a prompt that is expected to draw the gazes of the viewing population. For example a play start touch screen option may be output by one of the video presentation devices in the viewing population such as the device designated as the master eye tracking device. In this case when a viewer presses the play start button being displayed on the master eye tracking device the viewer can reasonably be expected to be looking at the play start button which can assist in eye tracking calibration. Eye movement along the horizontal axis up down and vertical axis left right can thereafter be measured by the master eye tracking device and conveyed back to the master application as the eye movement monitoring feedback at . In a further example a max threshold of eye movement can be established beyond which the eye tracking deviations would be ignored e.g. either omitted from the eye movement monitoring feedback by the master eye tracking device or included in the eye movement monitoring feedback by the master eye tracking device and then discarded by the master application . For example the max threshold can include max values for horizontal and vertical movement delta from the baseline whereby the delta is the angular deviation for the stare relative to the baseline.

At in a second embodiment instead of designing at single master eye tracking device a distributed eye tracking solution can be implemented. In this case two or more client devices e.g. potentially all of the video presentation devices participating in the coordinated display session are designated to perform eye tracking and the two or more designated eye tracking devices establish the horizontal and vertical deviation of the viewer s stare gaze from the principal and perpendicular axis. Each of the two or more designated eye tracking devices independently acts on the deviation measures therein and attenuates or amplifies the audio stream. In an example in the distributed eye tracking mode if there is a 3 3 array not shown of video presentation devices and the viewer is looking at the top right device other devices would measure horizontal and vertical axis stare gaze deviation increasing from right to left as well as from top to bottom. In another example in the distributed eye tracking mode if there is a 2 4 array of video presentation devices and the viewer is looking at the top right device e.g. see Viewer in other devices would measure horizontal and vertical axis stare gaze deviation increasing from right to left as well as from top to bottom.

After obtaining the eye movement monitoring feedback from the designated set of eye tracking devices at the master application determines whether to modify one or more session parameters associated with the coordinated display session . If the master application determines not to modify the one or more session parameters at the coordinated display session continues using the current session parameter configuration and the process returns to where the master application continues to obtain eye movement monitoring feedback from the designated set of eye tracking devices. Otherwise if the master application determines to modify the one or more session parameters at the process advances to . At the master application modifies the one or more session parameters associated with the coordinated display session based on the eye movement monitoring feedback after which the coordinated display session continues using the modified session parameters and the process returns to where the master application continues to obtain eye movement monitoring feedback from the designated set of eye tracking devices.

After obtaining the eye movement monitoring feedback from the designated set of eye tracking devices at the master application determines whether to modify an audio component e.g. the set of audio parameters previously configured at of the coordinated display session based on the eye movement monitoring feedback . If the master application determines not to modify the audio component of the coordinated display session at the coordinated display session does not modify the audio component and instead continues using the current set of audio parameters and then advances to . Otherwise if the master application determines to modify the audio component at the master application modifies the audio component by reconfiguring the set of audio parameters based on the eye movement monitoring feedback from e.g. by adjusting volume levels being output by one or more of the client devices in the session changing an audio orientation for the session modifying how enhancing or de emphasizing equalizer functions are applied to audio being mapped to one or more client devices in the session etc. and then advances to . Examples of how the audio component can be modified based on the eye movement monitoring feedback are provided below in more detail.

At the master application determines whether to modify an eye tracking component of the coordinated display session based on the eye movement monitoring feedback from . The eye tracking component relates to any parameter associated with how the eye movement monitoring feedback is obtained. For example at the master application can determine whether to modify how client devices are allocated to the set of eye tracking devices the master application may determine whether to ask the set of eye tracking devices to initiate a calibration or baselining procedure the master application may determine whether to toggle eye tracking off or on for the coordinated display session the master application can determine whether a priority viewer has been detected in the viewing population and if so order the set of eye tracking devices to focus on the priority viewer and so on. If the master application determines not to modify the eye tracking component of the coordinated display session at the coordinated display session continues without modifying the eye tracking component and then advances to . Otherwise if the master application determines to modify the eye tracking component at the master application modifies the eye tracking component based on the eye movement monitoring feedback from and then advances to . Examples of how the eye tracking component can be modified based on the eye movement monitoring feedback are provided below in more detail.

At the master application determines whether to modify a video component associated with the coordinated display session based on the eye movement monitoring feedback from . For example at the master application can determine whether to expand a particular mapped video portion so that a bigger version of the particular mapped version is displayed across multiple or even all of the video presentation devices participating in the coordinated display session e.g. a full screen mode or zoomed in mode . In another example at the master application can determine whether to duplicate a particular mapped video portion so that a same sized version of the particular mapped version is displayed across multiple or even all of the video presentation devices participating in the coordinated display session e.g. a screen copy or multi view mode . If the master application determines not to modify the video component for the coordinated display session at the coordinated display session continues without modifying the video component and the process returns to where the master application continues to obtain eye movement monitoring feedback e.g. potentially in a modified form if the eye tracking component is modified at or even stopped altogether if the eye tracking component modification toggles eye tracking to an off mode or disabled mode . Otherwise if the master application determines to modify the video component for the coordinated display session at the master application modifies the video component based on the eye movement monitoring feedback from . After the process returns to where the master application continues to obtain eye movement monitoring feedback e.g. potentially in a modified form if the eye tracking component is modified at or even stopped altogether if the eye tracking component modification toggles eye tracking to an off mode or disabled mode . Additional examples of how the video component can be modified based on the eye movement monitoring feedback are provided below in more detail.

In the embodiment of the master application evaluates whether the session parameters in a particular order e.g. audio then eye tracking then video . However it will be appreciated that this order is merely exemplary and alternative implementations of the process of can be implemented in any order or in parallel with each other. Also while focuses on a particular implementation whereby the session parameter types being evaluated for potential modification based on the eye movement monitoring feedback include audio eye tracking and video components it will be appreciated that other embodiments can be directed to fewer session parameter types e.g. only the audio component or the video component and the eye tracking component but not the audio component etc. and or additional session parameter types. Further it will be appreciated that collectively corresponds to an example implementation of of .

Table 1 below illustrates a variety of implementation examples whereby different session parameters e.g. the audio component the eye tracking component the video component etc. are modified at of in different ways based on different types of eye movement monitoring feedback. In particular Table 1 is configured under the assumption that the coordinated display session is being displayed by a grid or array of video presentation devices in a 2 4 arrangement with four 4 columns and two 2 rows as shown in B C D etc. For the sake of simplicity Table 1 refers to the video presentation device grid positions by the numbers from these FIGS with a top right presentation device corresponding to Screen or mapped video portion the bottom right presentation device corresponding to Screen or mapped video portion and so on. It will be appreciated that the embodiments of are compatible with any grid arrangement and the example 2 4 grid arrangement is used herein primarily for convenience of explanation in view of its familiarity from the description of other embodiments of the invention.

As will be appreciated from a review of examples provided in Table 1 above different types of monitoring feedback can trigger different session parameter changes.

Referring to Example 1A from Table 1 a viewing population with a single viewer Viewer being actively eye tracked or monitored by the set of eye tracking devices is detected as looking at Screen e.g. for more than a nominal threshold period of time etc. for a coordinated display session with a session state that is characterized by a single video audio source or feed being collectively output by the coordinated display group by Screens . . . . In Example 1A an example session parameter modification that can be triggered by the eye movement monitoring feedback is to increase the relative speaker volume being output by Screen more specifically by an audio output device coupled to the video presentation device with Screen and or by other screens in proximity to Screen more specifically by other audio output devices coupled to the video presentation devices with the other screens in proximity to Screen . As used herein referring to a screen in context with audio output will be recognized as referring to an audio output device that is coupled to or associated with that particular screen. For example in assume that an initial audio configuration state for the coordinated display session is that Screens . . . each output 25 speaker volume. Under this assumption an example audio configuration state after the audio component modification can increase the speaker volume for Screen to 100 can increase the speaker volume for each adjacent screen of Screen i.e. Screens and . . . to 50 and to mute or reduce speaker volume to 0 to each other screen i.e. Screens and .

Referring to Example 1B from Table 1 assume that the session modification from Example 1A has already occurred and the audio component for the coordinated display session has been updated based on Viewer being detected as looking at Screen . Now in Example 1B at some later point in time assume that Viewer is detected by the set of eye tracking devices as either looking away from Screen e.g. for more than a threshold period of time so that minor eye deviations such as blinking by Viewer will not trigger an audio component modification for the coordinated display session or physically moving out of range of the set of eye tracking devices. In this case the session parameter modification is to revert the audio configuration to previous settings and or a previous audio configuration state. For example the speaker volume for each of Screens . . . can be returned to 25 . In another example the previous audio configuration state could be configured different for example as 2.1 pseudo surround sound 5.1 pseudo surround sound or some other static playout mode that is not dictated by eye tracking.

Referring to Example 1C from Table 1 similar to Example 1A a viewing population with a single viewer Viewer being actively eye tracked or monitored by the set of eye tracking devices is detected as looking at Screen e.g. for more than a nominal threshold period of time etc. for a coordinated display session with a session state that is characterized by a single video audio source or feed being collectively output by the coordinated display group by Screens . . . . In Example 1C an example session parameter modification that can be triggered by the eye movement monitoring feedback is to apply an enhancing equalizer function to audio being output by Screen more specifically by an audio output device coupled to the video presentation device with Screen and or by other screens in proximity to Screen more specifically by other audio output devices coupled to the video presentation devices with the other screens in proximity to Screen such as adjacent Screens and . Also a de emphasizing or inverse equalizer function can be applied to audio being output by one or more screens that are not in proximity to Screen e.g. Screens and which are not adjacent to Screen or even the adjacent Screens and . In one example the enhancing equalizer function is applied to Screen while Screens and do not have their audio modified. In another example the enhancing equalizer function is applied to Screens . . . and . . . e.g. Screen plus adjacent screens while Screens and do not have their audio modified. In another example the enhancing equalizer function is applied to Screen only Screens and . . . do not have their audio modified and a de emphasizing or inverse equalizer function is applied to Screens and . It will be appreciated that while the audio component modifications in other examples from Table 1 pertain primarily to volume levels and or audio configuration any of these examples could be implemented with respect to modifications to other audio parameter types e.g. equalizer functions treble bass and or fidelity modifications etc. in other scenarios based on similar feedback.

Referring to Example 2A from Table 1 a viewing population with multiple viewers Viewers . . . being actively eye tracked or monitored by the set of eye tracking devices is detected with Viewers . . . looking at Screen Viewer looking at Screen and Viewer looking at Screen for a coordinated display session with a session state that is characterized by a single video audio source or feed being collectively output by the coordinated display group by Screens . . . . In each case some nominal threshold of time of eye to screen contact can be required before any particular viewer qualifies as looking at that particular screen. In Example 2A an example session parameter modification that can be triggered by the eye movement monitoring feedback is to stop eye tracking so long as multiple viewers are present and to transition the audio configuration state to a default audio configuration state e.g. the all 25 speaker volume state 2.1 pseud surround sound 5.1 pseudo surround sound . Example 2A from Table 1 is not expressly illustrated in the FIGS. Basically in Example 2A from Table 1 the master application assumes that it will be difficult to track eye movement from a large viewing population so as to provide relevant eye movement based audio to the entire viewing population and thereby decides to supply the viewing population with basic or default audio.

Referring to Example 2B from Table 1 a viewing population with multiple viewers Viewers . . . being actively eye tracked or monitored by the set of eye tracking devices is detected with Viewers . . . looking at Screen Viewer looking at Screen and Viewer looking at Screen for a coordinated display session with a session state that is characterized by a single video audio source or feed being collectively output by the coordinated display group by Screens . . . . In each case some nominal threshold of time of eye to screen contact can be required before any particular viewer qualifies as looking at that particular screen. In Example 2B an example session parameter modification that can be triggered by the eye movement monitoring feedback is to have each eye tracking device in the set of eye tracking devices monitor eye movements for each viewer in its respective range and to selectively increase the relative speaker volume being output by each screen being watched by a threshold number of viewers e.g. etc. and screens in proximity to one of the watched screens. For example in assume that the threshold number of viewers is 1 and that an initial audio configuration state for the coordinated display session is that Screens . . . each output 25 speaker volume. Under this assumption an example audio configuration state after the audio component modification can increase the speaker volume for each watched screen i.e. Screens and to 100 to increase the speaker volume for each adjacent screen i.e. Screens and of any watched screen to 50 and to mute or reduce speaker volume to 0 to each other screen i.e. in this case there are no muted screens .

Referring to Example 2C from Table 1 a viewing population with multiple viewers Viewers . . . being actively eye tracked or monitored by the set of eye tracking devices is detected with Viewers . . . looking at Screen Viewer looking at Screen and Viewer looking at Screen for a coordinated display session with a session state that is characterized by a single video audio source or feed being collectively output by the coordinated display group by Screens . . . . In each case some nominal threshold of time of eye to screen contact can be required before any particular viewer qualifies as looking at that particular screen. In Example 2C an example session parameter modification that can be triggered by the eye movement monitoring feedback is to calculate a weighted score for each screen based on screen specific viewing metrics and then to configure a target audio configuration state for the coordinated display session based on the screen specific viewing metrics. For example the screen specific viewing metrics can include i a number of viewers watching each screen ii a proximity of a non watched screen from a watched screen iii a number of watched screens to which a non watched screen is adjacent iv a duration that one or more viewers have been watching a particular screen e.g. an average duration that viewers historically watch a particular screen compared with other screens etc. and or v any combination thereof.

For example in Screen has the highest number of viewers i.e. 3 compared with Screens and which each have a single viewer. So a viewer quantity weighting component favors Screen over Screens and and the viewing quantity weighting component favors any of Screens or over the non watched screens i.e. Screens and . Further assume that Viewer has been watching Screen for the longest time so that an average viewing duration weighting component favors Screen . Next for the non watched screens Screens and are adjacent to a single watched screen i.e. Screen Screens and are adjacent to two watched screens i.e. Screens and and Screens and respectively and Screen is adjacent to three watched screens i.e. Screens and . So an adjacent watched screen weighting component favors Screens and over Screens and and further favors Screen over Screens or . The various weighting components can be allocated different weight factors based upon the implementation so the various eye movement monitoring feedback can be used to produce a weighted score for each screen which can then be mapped to a corresponding audio configuration state for the coordinated display session. Using the assumptions from above one example of a resultant audio configuration state based on the weighted scores is shown in whereby Screens and output at 50 e.g. because Screens and are only adjacent to a single watched screen and have no other contribution to their weighting score Screens and output at 62 e.g. because Screens and are adjacent to two watched screens which is more than Screens and Screen outputs at 68 e.g. because Screen is adjacent to three watched screens which is more than any other non watched screen Screen outputs at 75 e.g. because Screen has a single viewer Screen outputs at 85 e.g. because Screen has a single viewer but the single viewer has been watching for a relatively long time and Screen outputs at 100 e.g. due to its high number of viewers .

Referring to Example 3A from Table 1 a viewing population with a single viewer Viewer being actively eye tracked or monitored by the set of eye tracking devices is detected as looking at Screen e.g. for more than a nominal threshold period of time etc. for a coordinated display session with a session state that is characterized by a single video source or feed which may optionally include audio being collectively output by the coordinated display group by Screens . . . . In Example 3A an example session parameter modification that can be triggered by the eye movement monitoring feedback is to zoom in or blow up the mapped video portion being output by the screen being watched by the viewer. For example in assume that the single video source which may optionally include audio is displaying a scene related to a soccer game and that Viewer is watching Screen which depicts a goalie guarding one of the soccer goals. As shown in the mapped video portion being shown in Screen can be blown up and spread across the entire coordinated display session via each of Screens . . . based upon the detection Viewer watching and presumably being particularly interested in Screen . Because the zoom in feature is a somewhat dramatic visual effect the above noted zoom in video component modification can be conditioned upon Viewer being locked onto the associated screen for more than a threshold period of time t e.g. 5 seconds 10 seconds etc. that will generally be longer than the threshold that is used merely to verify that a particular viewer is actually watching a screen. Although this condition can be overridden if desired. Also while not shown explicitly in the single video feed can potentially include audio in which case it is possible that Screen is associated with some type of unique audio component. If so the audio component specific to Screen can be blown up such that each of Screens . . . output the Screen specific audio or some version of it such as a surround sound effect being applied to the audio previously output by Screen . However this could also potentially be confusing to Viewer so it is also possible that the audio component can remain unchanged while the coordinated display session is zoomed in on Screen . Further it will be appreciated that when Screen is blown up across Screens . . . Viewer may look around at the different screens without necessarily triggering a return to the prior non zoomed state. In fact the zoom in feature could be iterative such that Viewer could lock his her eyes onto one of the Screen zoom in screens and cause yet another zoom in. In a further example one or more zoom out triggers could also be implemented e.g. Viewer looking away from all screens Viewer leaving the range of the set of eye tracking devices Viewer making some type of pre defined motion such as a hand wave or hand gesture etc. which undue one or all zoom in operations. Further the zoom in does not necessarily need to be limited to Screen . Rather when Screen is blown up Screen can be supplemented some visual data that is adjacent to Screen s mapped video portion e.g. to comply with an aspect ratio of the coordinated display session which can be different than the aspect ratio of Screen by itself to limit a degree of the zoom to maintain a threshold video resolution during the zoom etc. .

Referring to Example 3B from Table 1 unlike Examples 1A 3A the coordinated display session has a session state that is characterized by multiple video sources or feeds each of which may optionally include audio being collectively output by the coordinated display group by Screens . . . . As shown in the mapped video portion being output by Screen corresponds to Feed the mapped video portion being output by Screen corresponds to Feed and so on. In Example 3B a viewing population with multiple viewers Viewers . . . being actively eye tracked or monitored by the set of eye tracking devices is detected with Viewer watching Feed on Screen Viewer watching Feed on Screen and Viewer watching Feed on Screen . In particular while Viewers . . . each watch their respective screens for at least a nominal threshold of time e.g. 0.5 seconds 1 second etc. assume that Viewer is detected as watching Feed on Screen for a longer threshold of time t e.g. 20 seconds 40 seconds 2 minutes etc. . For example Feeds . . . may correspond to different sports games and Feed may have captured the interest of Viewer by displaying a particularly exciting sports moment or Feeds . . . may correspond to security camera feeds being watched by Viewers . . . i.e. security personnel and Viewer may have detected suspicious activity in Feed that captured his her interest.

In Example 3B an example session parameter modification that can be triggered by the eye movement monitoring feedback is to duplicate the mapped video portion being output by the screen being watched by any viewer for more than tonto one or more other screens temporarily blocking other feeds that were previously mapped to those screens. For example based on Viewer staring at Feed on Screen for more than tas shown in Feed can temporarily be duplicated and displayed in redundant fashion on each of Screens . . . as shown in . While not shown explicitly in one or more of Feeds . . . can potentially include audio. In an example to avoid simultaneous audio feeds being output audio output can be restricted to the feed duplication scenario such that Feed s audio feed is output when Feed is duplicated across Screens . . . but no audio feeds are output during a non duplication scenario. Further one or more duplication ending triggers could also be implemented e.g. Viewers or look away from all screens Viewers or leaving the range of the set of eye tracking devices Viewers or making some type of pre defined motion such as a hand wave or hand gesture etc. which undue the duplication of Feed across Screens . . . . Further Feed does not necessarily need to take over all of Screens . . . . In another example not shown explicitly Feed can be duplicated only on other screens that are currently being watched by the other viewers i.e. Screens and . In this case Viewer looking away from Screen could function to change Feed back to Feed on Screen while Feed is maintained on Screen so long as Viewer is still staring at Screen while Feed is being displayed.

Referring to Example 4A from Table 1 similar to Example 3B the coordinated display session has a session state that is characterized by multiple video sources or feeds each of which may optionally include audio being collectively output by the coordinated display group by Screens . . . . As shown in the mapped video portion being output by Screen corresponds to Feed the mapped video portion being output by Screen corresponds to Feed and so on. In Example 4A a viewing population with multiple viewers Viewers . . . being actively eye tracked or monitored by the set of eye tracking devices is detected with Viewer watching Feed on Screen Viewer watching Feed on Screen and Viewer having a history of alternating between Feeds and on Screens and for more than a time threshold t e.g. Viewer watches Feed for 10 seconds then Feed for 19 seconds then Feed for 18 seconds then Feed for 20 seconds then Feed again for 8 seconds and so on so it is clear that Viewer keeps returning to these four particular feeds habitually .

In Example 4A an example session parameter modification that can be triggered by the eye movement monitoring feedback is to zoom out or merge multiple mapped video portions being viewed habitually by a particular viewer over time i.e. more than t so as to produce a merged feed that is output by at least one of the habitually viewed screens. So it is possible that each feed being viewed habitually by Viewer can be updated to output the merged feed or alternatively that only a few or even one of the habitually viewed screens is affected. illustrates an example whereby the merged feed which includes Feeds and is mapped to each of Screens and . Also it is possible that one or more of Feeds and has an audio component. If so the respective audio components can either be merged for output by each screen receiving the merged frame or alternatively the audio components can simply be muted so as not to confuse Viewer .

Referring to Example 4B from Table 1 the coordinated display session has a session state that is characterized by a single video source or feed which may optionally include audio being collectively output by the coordinated display group by Screens . . . . In Example 4B a viewing population with a single viewer Viewer being actively eye tracked or monitored by the set of eye tracking devices is detected with Viewer having a history of alternating between Screens and for more than the time threshold t e.g. Viewer watches Screen for 10 seconds then Screen for 19 seconds then Screen for 18 seconds then Screen for 20 seconds then Screen again for 8 seconds and so on so it is clear that Viewer keeps returning to these four particular screens habitually .

In Example 4B similar to Example 4A an example session parameter modification that can be triggered by the eye movement monitoring feedback is to zoom out or merge multiple mapped video portions being viewed habitually by a particular viewer over time i.e. more than t so as to produce a merged feed that is output by at least one of the habitually viewed screens. So it is possible that each screen being viewed habitually by Viewer can be updated to output the merged feed or alternatively that only a few or even one of the habitually viewed screens is affected. illustrates an example whereby the merged feed which includes the mapped video portions previously delivered to Screens and e.g. a bigger area of the soccer game that is still less than the whole unpartitioned video feed . Also it is possible that one or more of the mapped video portions previously delivered to Screens and has a unique screen specific audio component. If so the respective screen specific audio components can either be merged for output by each screen receiving the merged frame or alternatively the audio components can remain unchanged during the zoom out or merge function.

In accordance with any of the session parameter modifications discussed above with respect to Table 1 and or the modifications can be undone based on detection of certain triggering events e.g. a time threshold expiring a hand gesture more eye movement monitoring feedback etc. .

Those of skill in the art will appreciate that information and signals may be represented using any of a variety of different technologies and techniques. For example data instructions commands information signals bits symbols and chips that may be referenced throughout the above description may be represented by voltages currents electromagnetic waves magnetic fields or particles optical fields or particles or any combination thereof.

Further those of skill in the art will appreciate that the various illustrative logical blocks modules circuits and algorithm steps described in connection with the embodiments disclosed herein may be implemented as electronic hardware computer software or combinations of both. To clearly illustrate this interchangeability of hardware and software various illustrative components blocks modules circuits and steps have been described above generally in terms of their functionality. Whether such functionality is implemented as hardware or software depends upon the particular application and design constraints imposed on the overall system. Skilled artisans may implement the described functionality in varying ways for each particular application but such implementation decisions should not be interpreted as causing a departure from the scope of the present invention.

The various illustrative logical blocks modules and circuits described in connection with the embodiments disclosed herein may be implemented or performed with a general purpose processor a digital signal processor DSP an application specific integrated circuit ASIC a field programmable gate array FPGA or other programmable logic device discrete gate or transistor logic discrete hardware components or any combination thereof designed to perform the functions described herein. A general purpose processor may be a microprocessor but in the alternative the processor may be any conventional processor controller microcontroller or state machine. A processor may also be implemented as a combination of computing devices e.g. a combination of a DSP and a microprocessor a plurality of microprocessors one or more microprocessors in conjunction with a DSP core or any other such configuration.

The methods sequences and or algorithms described in connection with the embodiments disclosed herein may be embodied directly in hardware in a software module executed by a processor or in a combination of the two. A software module may reside in RAM memory flash memory ROM memory EPROM memory EEPROM memory registers hard disk a removable disk a CD ROM or any other form of storage medium known in the art. An exemplary storage medium is coupled to the processor such that the processor can read information from and write information to the storage medium. In the alternative the storage medium may be integral to the processor. The processor and the storage medium may reside in an ASIC. The ASIC may reside in a user terminal e.g. UE . In the alternative the processor and the storage medium may reside as discrete components in a user terminal.

In one or more exemplary embodiments the functions described may be implemented in hardware software firmware or any combination thereof. If implemented in software the functions may be stored on or transmitted over as one or more instructions or code on a computer readable medium. Computer readable media includes both computer storage media and communication media including any medium that facilitates transfer of a computer program from one place to another. A storage media may be any available media that can be accessed by a computer. By way of example and not limitation such computer readable media can comprise RAM ROM EEPROM CD ROM or other optical disk storage magnetic disk storage or other magnetic storage devices or any other medium that can be used to carry or store desired program code in the form of instructions or data structures and that can be accessed by a computer. Also any connection is properly termed a computer readable medium. For example if the software is transmitted from a website server or other remote source using a coaxial cable fiber optic cable twisted pair digital subscriber line DSL or wireless technologies such as infrared radio and microwave then the coaxial cable fiber optic cable twisted pair DSL or wireless technologies such as infrared radio and microwave are included in the definition of medium. Disk and disc as used herein includes compact disc CD laser disc optical disc digital versatile disc DVD floppy disk and blu ray disc where disks usually reproduce data magnetically while discs reproduce data optically with lasers. Combinations of the above should also be included within the scope of computer readable media.

While the foregoing disclosure shows illustrative embodiments of the invention it should be noted that various changes and modifications could be made herein without departing from the scope of the invention as defined by the appended claims. The functions steps and or actions of the method claims in accordance with the embodiments of the invention described herein need not be performed in any particular order. Furthermore although elements of the invention may be described or claimed in the singular the plural is contemplated unless limitation to the singular is explicitly stated.

