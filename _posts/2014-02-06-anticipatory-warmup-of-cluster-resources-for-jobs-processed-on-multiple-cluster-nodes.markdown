---

title: Anticipatory warm-up of cluster resources for jobs processed on multiple cluster nodes
abstract: Systems and methods are disclosed for reducing latency in processing data sets in a distributed fashion. A job-queue operable for queuing data-processing jobs run on multiple nodes in a cluster may be communicatively coupled to a job analyzer. The job analyzer may be operable to read the data-processing jobs and extract information characterizing those jobs in ways that facilitate identification of resources in the cluster serviceable to run the data-processing jobs and/or data to be processed during the running of those jobs. The job analyzer may also be coupled to a resource warmer operable to warm-up a portion of the cluster to be used to run a particular data-processing job prior to the running of the job. In some embodiments, mappers and/or reducers may be extracted from the jobs and converted into compute node identifiers and/or data units identifying blocks for processing, informing the warm-up operations of the resource warmer.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09313133&OS=09313133&RS=09313133
owner: ROBIN SYSTEMS, INC.
number: 09313133
owner_city: San Jose
owner_country: US
publication_date: 20140206
---
This application claims the benefit of and hereby incorporates by reference U.S. Provisional Application Ser. No. 61 876 045 entitled Virtual Hadoop Clusters and filed on Sep. 10 2013.

This invention relates to the handling of data processing jobs and more particularly to the handling of data processing jobs run on partitioned subsets of data in parallel on multiple nodes of a cluster of nodes.

The ability to process large amounts of data within shorter periods of time is growing in importance. For one thing more and more data is being produced as more mobile technologies with larger information sensing capacities are spreading more people interact over the internet via social media and more devices become equipped with smart technologies among other reasons. Some of such sources include web searches email logs internet marketing geospatial data financial data space research healthcare data scientific research and more. Furthermore the world s ability to store data is increasing according to one study for example the world s per capita data storage capacity has been doubling every 40 months since the 1980s.

Not only are larger and larger data sets becoming more common but the processing of such data sets is becoming increasingly important in more areas. Large data sets are frequently involved in several areas of research from meteorology to genetics to many other fields of research requiring complex modeling. The ability to process large amounts of data has also become important in more every day applications from finance marketing e commerce social media and internet searches. However the growing size of data sets that must be processed to support functionalities in these and other areas is often so large that traditional processing approaches are either impractical or simply impossible.

To make possible the processing of large data sets often the presence of multiple chunks that can be processed independently is leveraged to break up the job for parallel processing. Parallel processing can occur on several nodes or machines simultaneously greatly speeding up the underlying job. However sending large amounts of data across a network for processing can introduce its own complexities take time and occupy large amounts of bandwidth within a network. Many other complex problems arise in distributed processing generally such as the details of the parallel and distributed processing itself and the handling of errors during processing.

In the late 1990s and early 2000s in the process of addressing problems associated with indexing the massive amounts of information that its search engine relies on Google noticed several features common to many big data processing problems. As a result it developed a distributed file system the Google File System GFS that provides a framework for breaking up and storing large data sets across physically independent commodity machines interlinked by a network and lends itself to the processing of those large data sets. Additionally Google developed a framework known as the MapReduce framework for processing distributed data sets implemented in two main phases. These main phases comprise a map phase that takes input files with key value pairs and produces intermediate files with new key value pairs and a reduce phase that combines values from common keys in the intermediate files.

In 2003 and 2004 Google published its GFS and MapReduce framework respectively in two papers. These papers together with a lot of collaboration from large corporations and other contributors have led to open source versions of the foregoing system and framework respectively referred to as the Hadoop Distributed File System HDFS and Hadoop MapReduce engine or collectively as simply Hadoop. Whether in terms of Google s version Hadoop or some other version these distributed file systems and MapReduce frameworks have proved a boon to big data processing in such areas as search analytical functions transformations aggregations data mining among others and have become ubiquitous in the field. However additional demands such as those of larger data sets and needs for quicker processing times require additional innovations that can sit atop Hadoop like approaches and potentially other approaches to distributed processing. The following description and claims set forth such innovations.

It will be readily understood that the components of the present invention as generally described and illustrated in the Figures herein can be arranged and designed in a wide variety of different configurations. Thus the following more detailed description of the embodiments of the invention as represented in the Figures is not intended to limit the scope of the invention as claimed but is merely representative of certain examples of presently contemplated embodiments in accordance with the invention. The presently described embodiments will be best understood by reference to the drawings wherein like parts are designated by like numerals throughout.

Although distributing processing duties for large data processing jobs across several nodes can make the running of such jobs possible and or practical getting the data to those nodes can introduce complexities and slow down the jobs. Hadoop and Hadoop like approaches hereinafter Hadoop approaches provide a framework that reduces these complexities by providing frameworks that transparently and intelligently handle the storage of and processing of large data sets in a way that in a sense takes the processors to the data as opposed to taking the data to the processors. However the very strength of these approaches in reducing data handling obscures additional approaches where improvements can be made in terms of the details of the way data is handled.

Hadoop approaches are designed to place processors in close proximity to the data blocks which they process which likely reside on or near those nodes. However such data blocks still need to be loaded into caches at those nodes. Additionally although the distances are usually reduced by Hadoop approaches some data blocks may need to be transferred to the nodes where they are processed. In some instances which extend the capabilities of Hadoop approaches data blocks may even reside outside the cluster where they are processed for example in the cloud. Typical data block sizes involved in Hadoop approaches are 128 Mega Bytes MBs and above indicative of the delays that warm up activities like loading caches and transferring data blocks can produce.

The present application discloses innovations to reduce these latencies. For example a system to reduce latencies may involve a job analyzer which may be implemented as a module as discussed below that is communicatively coupled to a job queue. The job queue may be operable for queuing data processing jobs to be run in a distributed fashion on multiple nodes in a cluster of nodes. The job analyzer may be operable to read one or more data processing jobs in the job queue and or extract characterization information from one or more data processing jobs. The characterization information extracted by the job analyzer may characterize one or more resources in a cluster of nodes for processing such jobs. The one or more resources may be serviceable and or designated to run a given data processing job or pool of data processing jobs. Additionally or in the alternative such characterization information may characterize identify and or facilitate identification of data to be processed by the given data processing job or pool of data processing jobs.

Such a system may also include a resource warmer which may also be implemented as module communicatively coupled to the job analyzer and to the cluster. The resource warmer may be operable to warm up a portion of the cluster to be used to run the given data processing job or pool of data processing jobs. The portion may be identified by the characterization information extracted from the given data processing job or pool of data processing jobs. Once the portion of the cluster has been warmed up the given data processing job or pool of data processing jobs may be run without the latency of the previously accomplished warm up period.

In some examples the system may also include a conversion model communicatively coupled to the job analyzer. The conversion module may be operable to convert characterization information which may comprise a set of mappers and or a set of reducers into a set of compute node identifiers for nodes at which the given data processing job or pool of data processing jobs is to be run and or a set of data units identifying data blocks replicas to be processed during the data processing job. In such examples the resource warmer may receive the set of compute node identifiers and or the set of data units. Furthermore in certain examples the resource warmer may warm up the portion of the cluster by provisioning one or more of the data blocks replicas identified by the set of data units to one or more nodes in the cluster. In some embodiments provisioning data blocks replicas may further involve loading them in one or more caches at the nodes indicated by the compute node identifiers.

To provide a more thorough account of embodiments of the present innovations it is helpful to provide some additional contextual information about approaches to processing large data sets such as Hadoop approaches. Therefore are provided that explain the two key concepts involved in Hadoop approaches. These two concepts are automated distributed filing systems like GFS and HDFS and the MapReduce framework engine.

Referring to an Automated Distributed Filing System ADFS is depicted consistent with examples such as GFS or HDFS as applied in Hadoop approaches. The ADFS may be implemented in software and or firmware and or the like residing at various hardware components with the use of modules as discussed below. The hardware components may comprise commodity hardware and or specially purposed hardware. The various hardware components may provide the infrastructure for various data nodes and a name node to be discussed in greater detail below which comprise a cluster

The ADFS may be configured to receive a large data file or data set and split the large data set into multiple blocks also referred to as data blocks for storage on multiple data nodes thereby increasing the potentially available storage capacity of the ADFS . In some examples the data set may include multiple files that share a logical grouping. The blocks may be fairly large in size for example from tens of megabytes to gigabytes. To provide redundancy in case a data node on which a given block is stored fails and or to provide greater access to the blocks the blocks may be replicated to produce a number of replicas of each block . As used in this application the term block is synonymous with any replica carrying the same data. Although the example depicted in depicts three replicas for each block as can be appreciated any number of ratios of replicas to different blocks may be used. These replicas may then be stored at the various data nodes which may store one or more blocks and or replicas .

The ADFS may be configured for fault tolerance protocols to detect faults and apply one or more recovery routines. To assist in fault tolerance the data nodes and the name node may be configured with a web server. Also the ADFS may be configured to store blocks replicas as close to processing logic on the hardware components as possible so that a data processing job can be run on the blocks pertaining to the data set with minimal block transfers. Also multiple different data sets may be stored on the cluster in this way.

The name node may fill a role as a master server in a master slave architecture with data nodes filling slave roles. Since the name node may manage the namespace for the ADFS the name node may provide awareness or location information of the various locations at which the various blocks replicas are stored. For example the name node may maintain a directory tree of all the blocks replicas in ADFS and may track where the various blocks replicas are stored across the cluster . Furthermore the name node may determine the mapping of blocks replicas to data nodes . Under the direction of the name node the data nodes may perform block creation deletion and replica functions.

Although only seven data nodes are depicted in for purposes of illustration as can be appreciated any number of nodes including numbers in the thousands are possible. As described with respect to an ADFS may provide automation and infrastructure for placing a large data set on several data nodes as blocks replicas spread out on physically different machines nodes and even across data centers in effect taking the processing logic close to the data. In the process the ADFS may set the stage for various approaches to distributed and or parallel processing. For example as discussed below the locational awareness of the ADFS may be leveraged to provide more efficient approaches to distributed and or parallel processing. The following figure is used to provide context relevant to the innovations explained herein of one such approach.

Referring to elements of a typical MapReduce engine are depicted. A MapReduce engine may implement a map phase and a reduce phase described in further detail below. A MapReduce engine may comprise additional phases such as a combination phase and or a shuffle phase also described below between the map phase and the reduce phase .

A master slave architecture as discussed with respect to the ADFS in terms of the relationship between the name node and the data nodes may be extended to the MapReduce engine in terms of a job tracker which also may be implemented as a resource manager and or application master in a master role and one or more task trackers which also may be implemented as node managers in a slave role. Together the job tracker and the name node may comprise a master node and individual parings of task trackers and data nodes may comprise individual slave nodes . In some examples the master node may also be configured with its own data node and task tracker .

Consistent with the concept of distributed parallel processing a data processing job may involve multiple component tasks. The job tracker may schedule and monitor the component tasks coordinating the re execution of a task where there is a failure. The job tracker may be operable to harness the locational awareness provided by the name node to determine the nodes on which various data blocks replicas pertaining to a data processing job reside and which nodes and or machines hardware and or processing logic are nearby.

The job tracker may further leverage such locational awareness to optimize the scheduling of component tasks on available slave nodes to keep the component tasks as close to the underlying data blocks replicas as possible. In the event that the requisite processing logic on a node on which a relevant block replica resides is unavailable the job tracker may select a node on which another replica resides or select a node in the same rack or otherwise geographically proximate to which to transfer the relevant block replica reducing the load on a network backbone. Owing to its monitoring and rescheduling capabilities the job tracker may reschedule a component task that fails.

The component tasks scheduled by the job tracker may involve multiple map tasks and reduce tasks to be carried out on various slave nodes in the cluster . Individual map and reduce tasks may be overseen at the various slave nodes by individual instances of task trackers residing at those nodes . Such task trackers may spawn separate Java Virtual Machines JVM to run their respective tasks and or may provide status updates to the job tracker for example and without limitation via a heartbeat approach. Although only depicts five such nodes for purposes of illustration. However any number of nodes may be involved easily including numbers in the thousands.

During a map phase a first set of slave nodes may be utilized and or dedicated for map tasks. For example a data processing job may involve processing the various blocks replicas that make up a data set . Although the tasks may be run in parallel the processing of each block replica at the various slave nodes makes up an individual map task. To execute a map task a task tracker may apply a mapper to a block replica pertaining to a job being run.

For example the job tracker may assign a first task tracker to apply a first mapper to a first data block pertaining to a data processing job being run by the job tracker . In some examples the job tracker may provide the first mapper to the first task tracker . In other examples the first mapper or a portion thereof may already reside at the slave node at which the task tracker also resides. The first data block may reside at a first data node that also stores several other blocks replicas . The first task tracker may select the appropriate data block from among the other blocks replicas in a storage volume used to maintain the first data node at the first slave node . A storage volume may comprise a disk on hardware supporting a slave node a solid state drive or any other technology for storing data.

Data blocks corresponding to a given data processing job being run by the job tracker in are depicted with boxes having white diagonal stripes on a black background similar to the one labeled . As can be appreciated these blocks may be stored in various locations relative to other blocks replicas pertaining to other data processing jobs and or data sets indicated by alternative shading patterns. Also as depicted with respect to the first data node a single data node may store multiple blocks replicas pertaining to a given data processing job.

A mapper may be executable to treat an input data block replica as a set of key value pairs. A mapper may proceed to process the block replica by producing a set of intermediate key value pairs written in an intermediate record . An illustrative but non limiting example may be provided in the context of search engines and something like the page rank algorithm with respect to which at least in part MapReduce engines were first developed. In such a context a web crawler may collect a large data set that stores web pages searched and links between them. In such an example an ADFS such as GFS may split this data set into many blocks replicas each of which may become an input for a mapper .

A mapper in such an example may view the input block replica as a set of key value pairs where a key may correspond to a source page and a value may correspond to a target page to which a link on the source page points. The mapper may run as a function applied to each key value pair in the process counting the number of pages that link to a given target page. In such an example the target page which corresponds to the value parameter of the input may become the key parameter in a new key value pair produced by the mapper and recorded in an intermediate record . In such example the value parameter of the new key value pair may be the number of links to the target page from various source pages. An intermediate record may therefore be a set on new key value pairs generated by applying a map function to a set of key value pairs in the input block replica . In the example an intermediate record would correspond to a set of new key value pairs comprising a set of target pages paired with counts of links to those target pages.

However after a map phase the results for a data processing job may be scattered across several different intermediate records . The reduce phase may be applied to bring these scattered results together. A shuffle phase may be implemented to facilitate the reduce phase . During the shuffle phase the intermediate records may be shuffled so that intermediate records with common keys e.g. and may be directed across a network which may connect various nodes in a cluster to nodes with the same reducers e.g. and respectively.

The job tracker may schedule a reducer phase to task trackers on slave nodes within a cluster . Data nodes at those slave nodes may receive intermediate records over a network for storage in corresponding storage volumes . Individual task trackers may apply a reducer to the intermediate records stored by the data node at the corresponding slave node . In some examples the job tracker may provide a reducer to a first task tracker . In other examples the reducer or a portion thereof may already reside at the slave node at which the task tracker also resides.

Although a map phase and a reduce phase may run in two distinct phases in some examples they may be overseen by a common task tracker . Conversely a map phase and a reduce phase may be overseen by two distinct task trackers . Even though reducers may not start until all mappers are complete shuffling may begin before all mappers are complete.

Although at least one mapper runs on each node that has data for the job reducers are not constrained to the nodes on which they run by considerations of data locality. Reducers may be assigned to run in the network with flexibility. Hence the job tracker may assign tasks to less loaded task trackers . For example the load may be assigned toward the achievement of an evenly distributed load. One approach to evenly distribute the load may involve assigning a reducer to run on all available nodes if there are enough reducers in the job. Also the intermediate records files mapper output may avoid the overhead associated with replication associated with the data blocks .

A reducer may run on multiple intermediate records to produce an output record . An output record generated by such a reducer may group values associated with common keys to produce combined values. Picking up with the previous example the counts for links to the various target pages in the various intermediate records could for example be summed. In such a way one or more reduce phases may be used to combine the results from several different mappers in different intermediate files to create a result for a data processing job. Due to the way in which individual mappers and or reducers operate at individual nodes the term mapper and or reducer may be used to refer to the nodes at which individual instances of mappers and or reducers are implemented. As can be appreciated a MapReduce engine may compose any different number of map phases and or reduce phases in any different combination where a reduce phase follows one or more map phases to run a given data processing job.

The functions involved in implementing such an ADFS some other distributed filing system a Hadoop engine some other approach for processing distributed data and or the innovations discussed herein may be handled by one or more subsets of modules. With respect to the modules discussed herein aspects of the present invention may take the form of an entirely hardware embodiment an entirely software embodiment including firmware resident software micro code etc. or an embodiment combining software and hardware aspects that may all generally be referred to herein as a module. Furthermore aspects of the presently discussed subject matter may take the form of a computer program product embodied in any tangible medium of expression having computer usable program code embodied in the medium.

With respect to software aspects any combination of one or more computer usable or computer readable media may be utilized. For example a computer readable medium may include one or more of a portable computer diskette a hard disk a random access memory RAM device a read only memory ROM device an erasable programmable read only memory EPROM or Flash memory device a portable compact disc read only memory CDROM an optical storage device and a magnetic storage device. In selected embodiments a computer readable medium may comprise any non transitory medium that may contain store communicate propagate or transport the program for use by or in connection with the instruction execution system apparatus or device.

Computer program code for carrying out operations of the present invention may be written in any combination of one or more programming languages including an object oriented programming language such as Java Smalltalk C or the like and conventional procedural programming languages such as the C programming language or similar programming languages. Aspects of a module and possibly all of the module that are implemented with software may be executed on a micro processor Central Processing Unit CPU and or the like. Any hardware aspects of the module may be implemented to interact with software aspects of a module.

Referring to an implementation of virtual clusters consistent with a Hadoop approach is depicted. Although the virtual clusters depicted in are discussed below in terms of virtual clusters consistent with a Hadoop implementation any type of virtual clusters such as open stack clusters are possible. By adding one or more layers of abstraction the abilities of the Hadoop approach discussed above may be extended through virtualization. Such innovations may provide a configuration platform operable to configure any number of different virtual clusters with any number of data nodes name nodes job trackers and task trackers on a wide variety of physical hardware .

Such additional layers of abstraction may involve a new name space . The new name space may be configured consistent with an abstract file system which may reside with the configuration platform or elsewhere operable to store data blocks replicas in a virtual cluster . Additionally safeguards and or redundancies may be built into this new abstract file system to address problems typical of flash memory such as without limitation data retention problems to allow data blocks replicas to be stored at the physical layer on Solid State Drives SSD implementing such memory. Additional elements and or abstractions provided to facilitate and configure virtual clusters may be implemented on top of an existing Hadoop approach implementation in the user space as opposed to the kernel space of the Hadoop implementation.

The abstractions and or additional elements discussed above may facilitate implementation and or removal of virtual clusters and or additions to and or removal of nodes and or trackers from such virtual clusters on the fly through the configuration platform . Virtualization may facilitate the creation of several nodes on a single physical node . Additionally the flexibility of virtualization may allow Hadoop implementations on more heterogeneous clusters involving SSDs and or Hard Disc Drives HDD in a Storage Area Network SAN for data storage functions communicatively coupled over a physical network and protocols such as Internet Small Computer System Interface iSCSI . Additional details used to explain the implementation of virtual clusters can be found in the provisional application mentioned above U.S. Provisional Application Ser. No. 61 876 045 entitled Virtual Hadoop Clusters and filed on Sep. 10 2013.

Such flexibility can pave the way for implementation of new technologies such as cloud technologies in the Hadoop framework. Indeed such flexibility could literally lead to the use of physical resources anywhere in the world to support virtual hadoop clusters . Such flexibility however may have the potential to increase latencies already involved in running data processing jobs. For example retrieving a data block replica from a SAN or a cloud service may take time. Such latencies required to prepare a cluster for operations may be classified under the category of warm up operations.

Operations to warm up a cluster generate latency issues whether the cluster is a virtual cluster or not. Such warm up operations may include loading a data block replica to a cache for processing which may take significant time for sizes involved in distributed processing such as those from 64 megabytes to gigabytes. Where the requisite block replica does not reside on a node with available processing logic warm up latencies may be incurred in transferring the block replica to the requisite node via a network protocol. Such transfers may involve warming up relevant network resources. Additionally one or more task trackers may require time to spawn one or more JVMs to implement mappers and or reducers which may also need to be provisioned to the requisite nodes . The foregoing is not intended as an exhaustive list of potential warm up operations but is provided only by way of illustration. As can be appreciated there may be several additional warm up operations that may contribute to the latency of running a data processing job. The discussion with respect to the following figures explains innovations to reduce and or remove warm up latencies.

Referring to a job analyzer which may be implemented as a module is depicted. The job analyzer may be implemented outside of a Hadoop implementation such as in the user space while being operable to interface with a job queue . The job queue may be operable for queuing data processing jobs to be run in a distributed fashion on multiple nodes in a cluster of nodes . In some examples the job queue may reside at a job tracker pertaining to a MapReduce layer sitting atop an ADFS layer to which a name node may pertain. Where a job tracker maintains scheduler data structures and allows for a pluggable scheduler the job analyzer enhancement may be provided with the job tracker without having to change a corresponding Hadoop specification.

Being communicatively coupled with the job queue the job analyzer may be operable to read one or more data processing jobs in the job queue . For example the job analyzer may utilize information about a file structure used to store such data processing jobs at the master node to read the data processing jobs . Additionally the job analyzer may be operable to retrieve characterization information from one or more data processing jobs . The characterization information for a given data processing job may characterize one or more resources in a cluster serviceable to run the data processing job and or data units such as one or more data blocks replicas to be processed during the data processing job .

Referring to a job analyzer is depicted as operable to traverse across data processing jobs for purposes of reviewing the data processing jobs in the job queue . By way of example and not limitation the data processing jobs in the job queue may be processed in a First In First Out FIFO manner such that the data processing job on the right side of the queue may be interpreted as both the first data processing job into the job queue and the next data processing job to be processed . As can be appreciated additional approaches apart from FIFO for determining when to run data processing jobs in the queue may be implemented with the disclosure provided herein. Conversely the data processing job on the left may be interpreted as both the last data processing job into the job queue and the last data processing job currently in the queue that will be processed .

The job analyzer is depicted examining the second data processing job in line for processing indicated by the horizontal line pattern. The job analyzer may be operable to extract characterization information from this data processing job . The characterization information may include a set of mappers and or a set of reducers for the data processing job

The master node on which the job queue may reside may pertain to a cluster of nodes . Each node in the cluster may have data storage capability in terms of a data node data processing capability directed by a task tracker and or software for coordinating one or more mapping and or reducing operations within the cluster . The overall data processing system may include a distribution module not depicted operable to divide an input file into multiple splits or blocks and or store at least one copy of each of the multiple splits as data blocks at multiple locations selected from the cluster and or a backend storage device or cloud service. The name node on the master node may receive locational awareness or location information identifying the multiple locations of the data blocks replicas .

As can be appreciated the mappers and or reducers extracted from the data processing job may correspond to individual nodes in the cluster . To illustrate this correspondence the pattern of white diagonal stripes on a black background used to illustrate the set of three mappers is echoed by the same pattern on three slave nodes that correspond to these three mappers . The mappers may correspond to these three slave nodes inasmuch as the mappers may be implemented on these nodes 

Similarly the pattern of vertical stripes used to illustrate the set of two reducers is echoed by the same pattern on two slave nodes that correspond to these two reducers . In some examples the reducers may be implemented on these nodes . As the remaining nodes and in the cluster may not be utilized in the map phase or the reduce phase they are left blank.

In some examples but not in all examples a schedule control module may reside at the job analyzer and or be communicatively coupled to the job analyzer. The schedule control module may be operable to analyze control information such as but not limited to mappers and or reducers extracted from multiple data processing jobs in the job queue . The schedule control module may further be operable to optimize a pool of data processing jobs to be run concurrently. The schedule control module may determine the optimization of data processing jobs to be run concurrently based on data availability. Data unit availability may be defined by a data block replica being available where the data block replica to be processed by one data processing job in the pool is loaded in a cache for processing by a node for another data processing job .

Referring to a conversion module and a resource warmer which may be implemented as a module are depicted in relation to the job analyzer . The resource warmer may be communicatively coupled to the job analyzer and to the cluster . The resource warmer which may be implemented separately from the master node while being communicatively coupled to the master node may be operable to warm up a portion of the cluster to be used to run a data processing job . The portion of the cluster that the resource warmer may warm up may be a portion of the cluster identified from the characterization information extracted from the data processing job by the job analyzer which may be in communication with the resource warmer . The resource warmer may warm up a portion of the cluster for a single data processing job and or multiple data processing jobs from the job queue . Where there are insufficient cache resources to fully warm up the cluster for all of the jobs in the job queue a selection may be made of a subset of the jobs which may or may not be based on a processing order of the jobs in the job queue .

The conversion module may assist in the identification of the portion of the cluster to be warmed up. The conversion module may be communicatively coupled to the job analyzer and or to the resource warmer . Although the conversion module is depicted in as residing at the job analyzer the conversion module may be located elsewhere. The conversion module may be operable to convert the characterization information into a set of compute node identifiers at which a given data processing job is to be run.

The set of compute node identifiers may correspond to nodes at which mapping and or reducing functions for the data processing job may occur. Inasmuch as the set of mappers may correspond to certain slave nodes in the cluster of nodes at which the mappers may be implemented the conversion module may be operable to determine these slave nodes and designate these as compute node identifiers . In some examples the conversion module may be operable to determine a proximate and available node for a mapper where the corresponding data block replica does not reside at a slave node with the requisite processing logic available.

In some embodiments the conversion module may also be operable to determine slave nodes at which the reducers may be implemented and designate these as compute node identifiers . The set of compute node identifiers may carry additional information to indicate whether a given compute node identifier pertains to a map operation a reduce operation or some other processing functionality. The set of compute node identifiers may be limited solely to compute node identifiers designated for map functions solely limited to compute node identifiers designated for reduce functions or compute node identifiers designated solely for both. The set of compute node identifiers may also include compute node identifiers designated for other data processing functionalities.

The conversion module may be operable in the alternative or additionally to convert the characterization information into a set of data unit identifiers indicating blocks replicas to be processed during the given data processing job . Either the conversion module the job analyzer the resource warmer some other module or any combination of the foregoing may be operable to retrieve multiple locations at which a set of data blocks replicas to be processed in accordance with a given data processing job are stored. These multiple locations may be retrieved from the name node based on information in the characterization information such as but not limited to various mappers and or reducers .

The name node may be operable to store location information about locations where data blocks replicas reside within the cluster according to a distributed filing system such as a system consistent with the ADFS described with respect to . The location information may be generated and or stored in the name node by the distributed filing system. Such location information may be provided at one or more of various levels of granularity such as for example a geographic a data center a sector of a data center a rack a machine and or a particular data node at which or in which the data block replica is stored.

As stated above either the conversion module the job analyzer the resource warmer some other module or any combination of the foregoing may be operable to access the location information from the name node . One or more of these entities may also be operable to apply information from the characterization information to the location information to determine where data blocks replicas to be processed during the data processing job reside. These data blocks replicas may be indicated by the characterization information extracted from the data processing job

The job analyzer conversion module resource warmer or some other entity may also be operable to select nearby nodes at which to process the data blocks replicas . Where a data block replica resides at node with the requisite processing logic available to process the block replica that node may be selected. If the requisite processing logic is not available one or more algorithms or routines may be applied to select a proximate node with the requisite processing logic available. Status reports may be served from the various nodes to determine availability of processing logic.

In some examples one or more data blocks replicas making up a portion of the data to be processed during the running of a data processing job or group of data processing jobs scheduled to be run concurrently may reside on a backend storage device such as without limitation those discussed with respect to . Although the backend storage device may be outside the cluster it may also be communicatively coupled to the cluster . The routines and or algorithms discussed above may be employed to select nodes for processing such data blocks replicas within the cluster

The job analyzer may send a message to the resource warmer with characterization information extracted from one or more data processing jobs . The characterization information may include without limitation mappers reducers compute node identifiers and or data unite identifiers . In some examples the resource warmer may receive a set of compute node identifiers and or the set of data units identifier .

As stated the resource warmer may be operable to warm up aspects of the cluster indicated by the characterization information as relevant to processing a given data processing job or group of data processing jobs to be run concurrently. The resource warmer may be operable to warm up these aspects in advance of processing the given data processing job or group of data processing jobs . In this way the resource warmer may contribute to removing latencies associated with performing warm up operations.

Referring to various potential functionalities of the resource warmer are depicted. In some examples the resource warmer may be operable to 1 warm up a portion of the cluster . In the depicted portion only includes a single slave node for purposes of illustration. However the actual portion may include many more aspects for the cluster such as slave nodes and in the cluster

The resource warmer may warm up the portion of the cluster at least in part by 2 provisioning a data block replica to a node in the cluster . In other words the warm up of the aspects of the cluster indicated by the characterization information may involve provisioning the set of data blocks replicas from the multiple locations at which they are stored to caches at the selected slave nodes which may correspond to the compute node identifiers where the data processing job or pool of jobs runs. In some examples where a data block replica does not reside at the node where it is to be processed provisioning the block replica may involve warming up a relevant network resource in the cluster used to relay block replica to the node at which the processing will take place. In some examples the warm up process may involve bringing one or more blocks replicas into the cluster from the backend . In certain examples provisioning a block replica may further involve loading a data block replica to be processed during a data processing job or pool of jobs into a cache for processing logic in the cluster for processing. In certain examples to avoid data transfer over the network by the use of distributed read to warm up a data node cache the resource warmer may be provided with a cache warming Application Programming Interface API . By way of example and not limitation some examples of data node reads may include consistency checks health monitoring and or the like.

The resource warmer may identify and or receive identification of the data blocks replicas in the set to be processed based on the characterization information extracted by the job analyzer . The resource warmer and or the warm up module therein may determine the nodes such as the enlarged slave node in on which the set of data blocks replicas reside and may further select the relevant data block replica from among several data blocks replicas in a storage volume of a relevant data node . In some examples the resource warmer may rely on a data unit identified by the conversion module to identify and or select the appropriate data block replica . In some examples raw mappers and or reducers may be used. In other examples a compute node identifier may be used and or other characterization information. In some examples once a data block replica has been identified the job analyzer conversion module resource warmer or some other entity may consult a name node to determine the location or general location of the data node at which the data block replica resides.

In some examples where multiple mappers and or reducers are stored at a given node information from an extracted mapper and or a compute node identifier indicator may be used to select 3 the appropriate mapper indicated by the diagonal background pattern in to apply at the node . Certain nodes at which mappers may be implemented may 4 provide a status update when the requisite data block replica for a given data processing job has been provisioned and or loaded into a cache to the resource warmer and or the job analyzer. In some examples the status update may indicate the status of other warm up operations such as those discussed above and or only indicate a partial completion of a warm up operation.

The resource warmer may send an update message with status update information to the job analyzer where the job analyzer does not receive such information directly. An advance module which may be termed a process module may reside with the job analyzer and or be communicatively coupled to the job analyzer . The advance module may be operable to receive one or more status updates.

Upon a determination that the portion of the cluster relevant to the data processing job or pool of data processing jobs is warmed up or has reached a level of warm up that satisfies a predetermined threshold depending on the example the advance module may 6 advance or act to advance the relevant data processing job or pool of data processing jobs selected by the scheduling module out of the queue regardless of its or their position with respect to a FIFO scheduling order or some other approach for determining when to run data processing jobs in the queue . In some examples the advance module may be operable to advance 6 the data processing job or pool of data processing jobs for 7 processing upon the warm up of the aspects of the cluster indicated by the characterization information reaching a completion threshold. The advance module may further 7 initiate or act to initiate processing the data processing job or pool of data processing jobs by the cluster

In some examples the running of one or more data processing jobs may be broken into various stages such that after a first warm up and run for a map phase another warm up and run phase may begin. To illustrate this concept certain nodes at which reducers may be implemented are depicted 8 providing a status update with respect to the provisioning and or loading of the requisite intermediate records for a given data processing job . The running of data processing jobs may be optimized accordingly.

As can be appreciated the innovations discussed with respect to the previous figures and to be discussed with respect to the figures that follow may be implemented on a virtual cluster . Such a virtual cluster may include data nodes configured on SSDs . As discussed with respect to such a virtual cluster may use a file system with location awareness and or with redundancies that addresses data retention issues typical of flash memory.

Referring to methods are depicted for warming up a portion of a cluster of nodes in anticipation of running a data processing job or jobs . The flowchart in illustrates the architecture functionality and or operation of possible implementations of systems methods and computer program products according to certain embodiments of the present invention. In this regard each block in the flowcharts may represent a module segment or portion of code which comprises one or more executable instructions for implementing the specified logical function s . It will also be noted that each block of the flowchart illustrations and combinations of blocks in the flowchart illustrations may be implemented by special purpose hardware based systems that perform the specified functions or acts or combinations of special purpose hardware and computer instructions.

Where computer program instructions are involved these computer program instructions may be provided to a processor of a general purpose computer special purpose computer or other programmable data processing apparatus to produce a machine such that the instructions which execute via the processor of the computer or other programmable data processing apparatus create means for implementing the functions acts specified in the flowchart and or block diagram block or blocks.

These computer program instructions may also be stored in a computer readable medium that may direct a computer or other programmable data processing apparatus to function in a particular manner such that the instructions stored in the computer readable medium produce an article of manufacture including instruction means which implement the function act specified in the flowchart and or block diagram block or blocks.

The computer program may also be loaded onto a computer or other programmable data processing apparatus to cause a series of operation steps to be performed on the computer or other programmable apparatus to produce a computer implemented process such that the instructions which execute on the computer or other programmable apparatus provide processes for implementing the functions acts specified in the flowchart and or block diagram block or blocks.

It should also be noted that in some alternative implementations the functions noted in the blocks may occur out of the order noted in the figure. In certain embodiments two blocks shown in succession may in fact be executed substantially concurrently or the blocks may sometimes be executed in the reverse order depending upon the functionality involved. Alternatively certain steps or functions may be omitted if not needed.

The methods may utilize characterization information about a data processing job or jobs to warm up the appropriate portion of a cluster . The method may begin by reading one or more data processing jobs in a job queue . The data processing job may be configured for distributed processing of a set of data blocks replicas on a cluster of nodes . The set of data blocks replicas may be stored at multiple locations either in the cluster and or a backend.

The methods may continue by extracting characterization information from the data processing job or jobs . The characterization information may have various types of information characterizing the data processing job or jobs and or data and or cluster resources involved in running the or jobs . In some examples the methods may proceed by warming up aspects of the cluster indicated by the characterization information as aspects to be utilized in running the data processing job or jobs .

In other examples the warm up step may be proceeded by analyzing the characterization information for identification information. The identification information may identify without limitation a data block replica in the set to be processed during the data processing job jobs . The identification information may include one or more locations at which a data block replica in the set is stored one or more nodes in the cluster serviceable for processing a data block replica in the set and or network resources serviceable to provision a data block replica in the set to a node in the cluster for processing.

In some examples analyzing the characterization information for identification information may further involve converting a set of mappers and or a set of reducers extracted with the characterization information into a set of compute node identifiers serviceable for processing the data processing job or jobs . The analysis step may further involve converting set of mappers and or a set of reducers into a group of data units indicating data blocks replicas on which the data processing job or jobs runs. Additionally the step may involve determining one or more of the multiple locations at which the set of data blocks replicas are stored whether inside the cluster or outside the cluster . These locations may be obtained in some examples from locational awareness information stored in a name node .

The methods may wait to proceed until a determination that the relevant aspects of the cluster are warmed up have warmed up to a predetermined threshold or a sufficient portion of the aspects of the cluster are warmed up. When an affirmative warm up determination has been reached the methods may proceed with running the data processing job or jobs after at least a portion of the aspects of the cluster are warmed up. Running the data processing job or jobs may involve getting the relevant data processing job or jobs from the queue and initiating processing of the data processing job or jobs . At this point in many examples the method may end.

However in additional examples an additional determination may be made as to whether additional aspects of the cluster warmed up previously for the data processing job or jobs may be utilized for a second data processing job or jobs . If they are the second data processing job or jobs which utilize these aspects may be selected from the job queue to be run. In other examples involving the running of multiple data processing jobs concurrently the methods may involve reading multiple data processing jobs in the job queue .

Such methods may further involve extracting characterization information for each of the multiple data processing jobs . This characterization information may be used for identifying overlapping resources in the cluster to be used by more than one data processing job . Based on these overlapping resources a pool of data processing jobs may be selected to be run concurrently in the cluster that may be used to process the pool of data processing jobs . The overlapping resources may be warmed up at which point processing the pool of data processing jobs may begin.

The present invention may be embodied in other specific forms without departing from its spirit or essential characteristics. The described embodiments are to be considered in all respects only as illustrative and not restrictive. The scope of the invention is therefore indicated by the appended claims rather than by the foregoing description. All changes which come within the meaning and range of equivalency of the claims are to be embraced within their scope.

