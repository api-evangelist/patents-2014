---

title: System and method for broadcasting data to multiple hardware forwarding engines
abstract: A method and apparatus of a device that broadcasts data to multiple hardware forwarding engines is described. In an exemplary embodiment, a central processing unit of the device receives the data to broadcast to the plurality of hardware forwarding engines. The device further writes the data to a broadcast log. In addition, the device transmits a signal to one or more co-processors that the data is available to be read, wherein each of the plurality of hardware forwarding corresponds to one of the one or more co-processors. Each of these co-processors reads the data in the broadcast log by receiving the signal that the data is ready to be read from the broadcast log. In addition, each co-processor determines a broadcast log entry for the data for that co-processor. Each co-processor further reads the data from the broadcast log entry via a direct memory access in memory that stores the broadcast log and the plurality of hardware forwarding engines use the data to process network traffic.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09479425&OS=09479425&RS=09479425
owner: Arista Networks, Inc.
number: 09479425
owner_city: Santa Clara
owner_country: US
publication_date: 20140505
---
Applicant claims the benefit of priority of prior provisional application Ser. No. 61 822 259 filed May 10 2013 the entirety of which is incorporated by reference.

This invention relates generally to data networking and more particularly relates to broadcasting data from a central processing unit to multiple hardware forwarding engines.

A network element can include two different planes that are used to process network traffic a data plane and a control plane. The data plane receives processes and forwards network traffic using various configuration data e.g. forwarding security quality of service QoS and other network traffic processing information . For example for each received packet of the network traffic the data plane determines a destination address of that packet looks up the requisite information for that destination in one or more tables stored in the data plane and forwards the packet out the proper outgoing interface. The control plane gathers the configuration data from different sources e.g. locally stored configuration data via a command line interface or other management channel such as Simple Network Management Protocol SNMP and configures the data plane using the configuration data.

The control plane includes a central processing unit CPU that will write configuration data to the data plane. For each write the CPU retrieves the data from memory e.g. Dynamic Random Access Memory DRAM memory that is part of the control plane sends this data to across a bus such as a Peripheral Components Interconnect PCI bus and waits for confirmation that the data is stored in the data plane. This can however be a slow process because the CPU needs to wait for confirmation that the data is stored in the data plane and the travel across the bus between the control and data plane can be slow. Thus the overall process of writing data to the data plane can be a slow process. This is compounded if there are multiple hardware forwarding engines that each uses the same copy of the configuration data. For example updating a network element with 48 hardware forwarding engines by the CPU can take a relatively long time especially for configuration data that can be quickly changing or large such as routing or media access control MAC tables.

A method and apparatus of a device that broadcasts data to multiple hardware forwarding engines is described. In an exemplary embodiment a central processing unit of the device receives the data to broadcast to the plurality of hardware forwarding engines. The device further writes the data to a broadcast log. In addition the device transmits a signal to one or more co processors that the data is available to be read where each of the plurality of hardware forwarding engines corresponds to one of the one or more co processors. Each of these co processors read the data in the broadcast log by receiving the signal that the data is ready to be read from the broadcast log. In addition each co processor determines a broadcast log entry for the data for that co processor. Each co processor further reads the data from the broadcast log entry via a direct memory access in memory that stores the broadcast log and the plurality of hardware forwarding engines use the data to process network traffic.

In another embodiment the device reads data broadcasted from a control plane to a plurality of hardware forwarding engines of a network element. A plurality of co processors receives a signal that indicates that the data is ready to be read from the broadcast log where each of the co processors corresponds to one of the hardware forwarding engines. Each of the co processors determines a broadcast log entry for the data for the co processor. In addition each co processors reads the data from the broadcast log entry via a direct memory access in memory that stores the broadcast log where the plurality of hardware forwarding engines use the data to process network traffic and a central processing unit broadcasts the data to the broadcast log.

A method and apparatus of a device that broadcasts data to multiple hardware forwarding engines is described. In the following description numerous specific details are set forth to provide thorough explanation of embodiments of the present invention. It will be apparent however to one skilled in the art that embodiments of the present invention may be practiced without these specific details. In other instances well known components structures and techniques have not been shown in detail in order not to obscure the understanding of this description.

Reference in the specification to one embodiment or an embodiment means that a particular feature structure or characteristic described in connection with the embodiment can be included in at least one embodiment of the invention. The appearances of the phrase in one embodiment in various places in the specification do not necessarily all refer to the same embodiment.

In the following description and claims the terms coupled and connected along with their derivatives may be used. It should be understood that these terms are not intended as synonyms for each other. Coupled is used to indicate that two or more elements which may or may not be in direct physical or electrical contact with each other co operate or interact with each other. Connected is used to indicate the establishment of communication between two or more elements that are coupled with each other.

The processes depicted in the figures that follow are performed by processing logic that comprises hardware e.g. circuitry dedicated logic etc. software such as is run on a general purpose computer system or a dedicated machine or a combination of both. Although the processes are described below in terms of some sequential operations it should be appreciated that some of the operations described may be performed in different order. Moreover some operations may be performed in parallel rather than sequentially.

The terms server client and device are intended to refer generally to data processing systems rather than specifically to a particular form factor for the server client and or device.

A method and apparatus of a device that broadcasts data to multiple hardware forwarding engines is described. In one embodiment a central processing unit CPU sends configuration data to multiple hardware forwarding engines using a broadcast log. The broadcast log is a log that is used to store data for transfer between the CPU and the co processors. Because the broadcast log is stored in memory e.g. dynamic random access memory DRAM static random access memory SRAM flash memory graphics memory or another type of memory the co processors can directly access the log using a DMA controller. For transfer of data from the control plane to the hardware forwarding engines the CPU writes data into the broadcast log for each hardware forwarding engine. In addition the CPU signals each co processor that corresponds to each of the hardware forwarding engines that there is data in the broadcast log that is available to be read. The co processors can directly access this data in the broadcast log using a DMA controller that is coupled to the DRAM that stores the broadcast log. In one embodiment the data stored in the broadcast log can include a hardware forwarding engine identifier a key value pair of data and or a co processor identifier. In another embodiment the co processor can transfer data from the hardware forwarding engine to the CPU. In this embodiment the co processor receives data from a hardware forwarding engine and writes the data to the broadcast log using the DMA controller. The co processor signals the CPU that this data is available in the broadcast log and the CPU reads this data. In one embodiment the DMA controller can read and or write to the memory that stores that the broadcast log. This DMA controller may reside in the coprocessor or may be a separate component within the switch that the coprocessor can use or otherwise control to read or write the DRAM that stores the broadcast log. In one embodiment there is one or more DMA controllers per coprocessor and the DMA controller resides in the data plane.

The data plane includes multiple switches A C that can each receive process and or forward network traffic. In one embodiment the complex of one coprocessor and the associated forwarding engine s is one switch for the purposes of this invention. There may be multiple switches on a linecard or just one. The switches may all be connected to the CPU using one PCI bus or multiple PCI buses or some other shared bus used to access the DRAM in the switch. In this embodiment the same design applies even if there are no linecards and even if there is a single coprocessor and forwarding engine. In a fixed system with no linecards it is possible that there is still a coprocessor that does this. In another embodiment the coprocessor associated with a switch may reside in the main CPU and be a separate thread of the same CPU or of another CPU in the system that is associated with one or more hardware forwarding engines but reading from the broadcast log. In a further embodiment the coprocessor need not reside on the same linecard as the hardware forwarding engine. In this embodiment the coprocessor has access to main memory e.g. DMA controller and that the coprocessor has the ability to write to and possibly read from the associated hardware forwarding engines. The coprocessor can reside on a different linecard or it could be dedicated coprocessor hardware on the main supervisor controller card or a thread of the main CPU. In another embodiment the coprocessor may reside physically inside the hardware forwarding engine and be a component of that hardware forwarding engine.

In one embodiment each switch A C includes one or more hardware forwarding engines A C co processor A C and ports A C respectively. In one embodiment the hardware forwarding engines A C is a component that forwards data for the network element e.g. routing switching or another type of network forwarding . In one embodiment the co processor A C is a processor for each switch A C that can be used to accelerate various functions of the switch A C. For example and in one embodiment the co processors A C can read and write from broadcast log in the control plane to program the corresponding hardware forwarding engines A C and to push data from the hardware forwarding engines A C to the CPU via the broadcast log . In one embodiment there is one co processor A C for one or more hardware forwarding engines A C. For example and in one embodiment there is one co processor A for three hardware forwarding engines A. Reading and writing with the broadcast log is further described in below. In one embodiment the ports A C are used to receive and transmit network traffic. The ports A C can be the same or different physical media e.g. copper optical wireless and or another physical media . In alternative embodiment one coprocessor can control multiple hardware forwarding engines not illustrated . In a further embodiment the hardware forwarding engines could be implemented in software or a combination of software and hardware. For example and in one embodiment the hardware forwarding engine could be doing the forwarding in software in some sort of programmable processor or network processor on the linecard where the tables memories or data structures that are used by that forwarding engine are written to by the coprocessor.

In one embodiment the control plane gathers the configuration data for the hardware forwarding engines A C from different sources e.g. locally stored configuration data via a command line interface or other management channel e.g. SNMP Simple Object Access Protocol SOAP Representational State Transfer type Application Programming Interface RESTful API Hypertext Transfer Protocol HTTP HTTP over Secure Sockets layer HTTPs Network Configuration Protocol NetConf Secure Shell SSH and or another management protocol and pushes this configuration data to the hardware forwarding engines A C using the broadcast log . In one embodiment the control plane includes central processing unit CPU and DRAM . In this embodiment the CPU is used to process information for the control plane . In particular the CPU writes configuration data for the hardware forwarding engines A C and reads data from the hardware forwarding engines A C using the broadcast log .

In one embodiment the DRAM is memory that is used to stored data for the control plane. In addition the DRAM is shared with the data plane because the co processors A C of the data plane can access the contents of this memory. In one embodiment the DRAM includes the broadcast log and a direct memory access DMA controller . Alternatively the DMA controller is not part of the DRAM is part of the switch A C and is coupled to the DRAM . In one embodiment the broadcast log is a log that enables the CPU to broadcast data to the multiple co processors A B. In this embodiment the CPU writes an entry for each hardware forwarding engine A C in the broadcast log . In one embodiment each entry in the broadcast log can include a hardware forwarding engine identifier the value and a co processor identifier. In one embodiment the value is a key value pair. The broadcast log is further described in below. The DMA controller is a controller coupled to the memory in the DRAM that allows the co processors A B to directly access the DRAM . In one embodiment the DMA controller allows the co processors to directly access the broadcast log without needing the CPU to send the data to each co processor A B.

In alternative embodiment each hardware forwarding engine is assigned a small numeric identifier e.g. 1 to 64 if there are 64 hardware forwarding engines and a log entry carries a target hwfe bit vector to indicate which hardware forwarding engine should have the log entry applied to them. In this embodiment the target hwfe is of at least as many bits as there are possible HWFEs in the system. For example and in one embodiment the ibit position with bit set to 1 indicates that the corresponding entry should be applied to the ihardware forwarding engine by the controlling co processor. Alternatively if the ibit is 0 then coprocessor controlling the ihardware forwarding engine ignores the corresponding entry.

To use a bit vector a coprocessor will learn which hardware forwarding engine s the coprocessor is responsible for and which bits of the bit vector into the HWFEs that the coprocessor controls. In one embodiment this is done by writing that information into the control memory of the coprocessor. Alternative possibilities exist to associate the coprocessor to one or more hardware forwarding engines e.g. the coprocessor automatically discovering that information by reading from the hardware forwarding engines identifying its physical location in the system linecard and chip position using for instance hardware strapping pins etc. .

In this embodiment the bit vector is one compact way to represent the set of hardware forwarding engines targeted by a log entry. In one embodiment the bit vector is of fixed length because the maximum number of for the network element is fixed. Alternatively there can be other ways to represent a set of identifiers e.g. a list of identifiers variable length bit vector or some other way to represent a set of identifiers .

At block process signals the relevant co processor that data is in the broadcast log ready for retrieval. In one embodiment process signals the relevant co processor by writing to a register or memory address inside the coprocessor to indicate that there is more data in the broadcast log that the co processor should go get. In addition or in an alternative embodiment the main CPU can write to an interrupt or mailbox or doorbell register to wake or otherwise interrupt the co processor and cause the co processor to go look in the broadcast log. The signal could also indicate in some way a pointer or sequence number of the address in the memory of the broadcast log. Process may also indicate the address of the last entry in the broadcast log if it is represented as an array or the end may be implied in the data structure of the broadcast log itself if it is a linked list for instance the coprocessor discovers the end of the broadcast log when it reaches the end of a linked list . In one embodiment after signaling the co processor the CPU is available to process other instructions. This is because since the CPU is writing to DRAM the CPU does not need wait to verify that the data is stored in the DRAM. In one embodiment by having the co processors directly read the data in the broadcast log instead of the CPU writing the data to the co processor the process of moving the data from the CPU to the hardware forwarding engines can be greatly sped up. For example and in one embodiment the speed up in this process can be up to is a thousand fold where a transfer of a routing table that took minutes would now take seconds.

In one embodiment each co processor maintains either in its local memory or in main memory e.g. DRAM a sequence number or other indicator like a pointer to indicate to the main CPU how much of the broadcast log it has processed. In this embodiment once all coprocessors have processed to at least broadcast log entry N the associated memory for broadcast log entries smaller than N can be reclaimed and freed or reused. By freeing up the memory for the broadcast log entries the broadcast log will not continually grow because the network element would not know when a coprocessor is done using parts of the broadcast log.

In another embodiment if a hardware forwarding engine and or coprocessor is removed from the system e.g. by linecard removal or component failure there is a way for the main CPU to know that the corresponding hardware is no longer there and to not wait for it for any subsequent entries so that it can free the associated memory. In one embodiment a heartbeat or liveness check indicates that the coprocessor is still there. The CPU considers as complete all outstanding writes and stops waiting for any outstanding reads. In this embodiment a coprocessor and its associated hardware forwarding engines are determined to no longer be present based on the liveness check. In addition the network element may also take action based on an interrupt or error that is delivered to the CPU by hardware within the switch when attempting to access memory on the linecard or in the coprocessor that is no longer accessible when the linecard is removed or the coprocessor or hardware forwarding engine fails.

In one embodiment the network element can handle a hot insertion of a hardware forwarding engine and or co processor. In this embodiment the broadcast log is an ongoing sequence of updates to the plurality of hardware forwarding engines. In one embodiment each of the hardware forwarding engines are identical or substantially similar that have tables that are either identical or that can be programmed by passing them all the same sequence of broadcast log entries. For example and in one embodiment the updates stored in the broadcast log may be a sequence of register writes or a sequence of routing table or hardware MAC table insertions.

In one embodiment a problem is that the network element has been up for 10 weeks and 1 000 000 different writes have already happened. In this embodiment the 1 000 000 updates have been seen processed by the existing coprocessors and updates 1 1 000 000 no longer exist in the broadcast log because the updates in the broadcast log has been reclaimed as the updates have all been used. However a new hardware forwarding engine and possibly a new coprocessor is added into the system and start using the broadcast log. In one embodiment to insert the new hardware forwarding engine and bring this new engine up to date with the other hardware forwarding engines the network element may need to replay a sequence of updates to the new hardware forwarding engine that will get new hardware forwarding engine to the same state as if it had been there all along and had seen the log updates 1 1 000 000. It would be helpful to bring the new hardware forwarding engine up to date without actually storing or playing back all prior 1 000 000 updates. Storing the past updates would use too much memory and playing them all back is too slow so a better approach is useful.

In one embodiment the approach used is to record the writes in the broadcast log into a hardware shadow for each class of hardware forwarding engines. In this embodiment the shadow represents the summarized state that a hardware forwarding engine would be in after applying all previous broadcast log entries. In one embodiment a class of hardware forwarding engine would be a set of hardware forwarding engines that can all be managed by the same sequence of operations in the broadcast log. These might be different instances of the same hardware engine wherein either some or all tables within that hardware engine are identically programmed.

For example and in one embodiment if the broadcast log is a sequence of hardware writes to the hardware forwarding engine represented as address value pairs then the associated hardware shadow is simply a record of pairs that records the latest value for every address that has ever been written. In this example when a new instance of this class of hardware forwarding engine comes online the corresponding hardware shadow is written back to that hardware forwarding engine either by invoking a DMA sequence unrelated to the broadcast log or by putting those writes into the shared broadcast log and targeted to that hardware forwarding engine. Once the initial synchronization has complete that captures the writes to the hardware shadow through broadcast log entry N the corresponding co processor can be directed using mechanisms described already in the description to start reading from the broadcast log at entry N. In another embodiment the DMA sequence containing the writes used to replay the hardware shadow is presented to the coprocessor in the same format used for the shared broadcast log. In this embodiment a special instruction appears at the end of this first broadcast log indicating to the coprocessor that it should start processing log entries in the shared broadcast log starting from log entry N.

In one embodiment the advantage of this approach relative to using a hardware shadow is that multiple identical or sufficiently alike hardware forwarding engines in the same class can be managed with a single shadow and a single log and thus much more efficiently which results in faster update times and uses less memory.

As described above the CPU can write data to the broadcast log that is read by the co processor. In another embodiment the co processor can write data to the broadcast log. In one embodiment the co processor writes this data so as to communicate data gathered by the hardware forwarding engines to the control plane e.g. counter data statistics regarding the network traffic processed alarms dynamic state data or other data gathered or generated by the hardware forwarding engine . is a flow diagram of one embodiment of a process to write data to the broadcast log by a co processor. In one embodiment process is performed by a co processor to read data that is broadcasted to a plurality of hardware forwarding engines such as one of the co processors A C of above. In process begins by receiving data from a hardware forwarding engine to send to the CPU at block . In one embodiment this data is data gathered by the hardware forwarding engines to be communicated to the control plane e.g. counter data statistics regarding the network traffic processed alarms or other data gathered or generated by the hardware forwarding engine . For example and in one embodiment this data can be counters activity bits or MAC table entries. At block process writes the data to the broadcast log. In one embodiment process writes the hardware forwarding engine identifier corresponding value and co processor identifier to a broadcast log entry. The broadcast log entry can be the next available entry in the broadcast log or can be a fixed entry. At block process signals the CPU that the data is available to be read from the broadcast log. In one embodiment process sends a signal by writing an entry reference or co processor identifier to the memory of the CPU.

In one embodiment a detail to address is where does the coprocessor write the data into main memory. For example and in one embodiment having all of the coprocessors write to the broadcast log and use the next entry is possible but would require a synchronization between coprocessors to ensure that two coprocessors do not write to the same memory location. Instead the following embodiments can be used without requiring synchronization or coordination with locks or atomic writes by the coprocessors. In one embodiment the broadcast entry points to an address in main memory and the coprocessor writes to that address plus some multiple of its hardware forwarding engine identifier. In this embodiment writing to different memory locations prevents two coprocessors from trying to write to the same memory. The offset associated with the hardware forwarding identifier can be the one assigned to hardware forwarding identifier or can be implied in the request based on the number of bits set in the target hardware forwarding engine bit vector. For example and in one embodiment a compact implementation is that if a broadcast request with a target HWFE vector has three hardware forwarding engines as the targets 10 11 and 17 then each hardware forwarding engine would write to plus times where HWFE10 would use offset 0 HWFE1 would write to offset 11 and HWFE17 would write to offset 2. The base address and multiple could be implied by the type of the broadcast entry or explicitly encoded in fields of the broadcast entry.

In one embodiment the result data is inline in the log and the main CPU that is generating the broadcast request would reserve room immediately following it in the log corresponding to the size of the result times the number of targeted hardware forwarding engines. In another embodiment there is a separate region of memory per hardware forwarding engine where the associated coprocessor can place return value from broadcast log entries that indicate that something should be read from the hardware forwarding engine.

In another embodiment the CPU can read data from the broadcast log that is put there by a co processor. is a flow diagram of one embodiment of a process for a CPU to read data from a broadcast log. In one embodiment process is performed by a CPU to broadcast data to a plurality of hardware forwarding engines such as CPU of above. In process begins by receiving a signal from a co processor that a broadcast log entry is available to be read at block . In one embodiment the co processor sends the signal to process by writing the signal into the memory of the CPU. At block process finds the entry in the broadcast log corresponding the signal. In one embodiment the signal includes a reference to the entry in the broadcast log e.g. an index a pointer sequence number or some other references to a broadcast entry . In one embodiment the signal includes references to more than one entry. In another embodiment the broadcast log can be content addressable memory and upon receiving the signal process searches the content addressable memory for a broadcast log entry that includes the co processor identifier or a co processor identifier mask that encompasses the co processor identifier. In this embodiment the signal includes a co processor identifier that can be used to search the content addressable memory. Process reads the data from the one or more broadcast log entries at block . In one embodiment process reads the data which includes the hardware forwarding engine identifier and the value associated with that entry. In one embodiment the values in the broadcast log can be counter or statistics data that is being pushed over from the hardware forwarding engines to the control plane. In another embodiment the values in the broadcast log can be dynamic state data e.g. newly discovered routes or MAC addresses that is to be propagated to other hardware forwarding engines via the CPU.

As shown in the computer system which is a form of a data processing system includes a bus which is coupled to a microprocessor s and a ROM Read Only Memory and volatile RAM and a non volatile memory . The microprocessor may retrieve the instructions from the memories and execute the instructions to perform operations described above. The bus interconnects these various components together and also interconnects these components and to a display controller and display device and to peripheral devices such as input output I O devices which may be mice keyboards modems network interfaces printers and other devices which are well known in the art. In one embodiment the system includes a plurality of network interfaces of the same or different type e.g. Ethernet copper interface Ethernet fiber interfaces wireless and or other types of network interfaces . In this embodiment the system can include a forwarding engine to forward network date received on one interface out another interface.

Typically the input output devices are coupled to the system through input output controllers . The volatile RAM Random Access Memory is typically implemented as dynamic RAM DRAM which requires power continually in order to refresh or maintain the data in the memory.

The mass storage is typically a magnetic hard drive or a magnetic optical drive or an optical drive or a DVD RAM or a flash memory or other types of memory systems which maintains data e.g. large amounts of data even after power is removed from the system. Typically the mass storage will also be a random access memory although this is not required. While shows that the mass storage is a local device coupled directly to the rest of the components in the data processing system it will be appreciated that the present invention may utilize a non volatile memory which is remote from the system such as a network storage device which is coupled to the data processing system through a network interface such as a modem an Ethernet interface or a wireless network. The bus may include one or more buses connected to each other through various bridges controllers and or adapters as is well known in the art.

Portions of what was described above may be implemented with logic circuitry such as a dedicated logic circuit or with a microcontroller or other form of processing core that executes program code instructions. Thus processes taught by the discussion above may be performed with program code such as machine executable instructions that cause a machine that executes these instructions to perform certain functions. In this context a machine may be a machine that converts intermediate form or abstract instructions into processor specific instructions e.g. an abstract execution environment such as a process virtual machine e.g. a Java Virtual Machine an interpreter a Common Language Runtime a high level language virtual machine etc. and or electronic circuitry disposed on a semiconductor chip e.g. logic circuitry implemented with transistors designed to execute instructions such as a general purpose processor and or a special purpose processor. Processes taught by the discussion above may also be performed by in the alternative to a machine or in combination with a machine electronic circuitry designed to perform the processes or a portion thereof without the execution of program code.

The present invention also relates to an apparatus for performing the operations described herein. This apparatus may be specially constructed for the required purpose or it may comprise a general purpose computer selectively activated or reconfigured by a computer program stored in the computer. Such a computer program may be stored in a computer readable storage medium such as but is not limited to any type of disk including floppy disks optical disks CD ROMs and magnetic optical disks read only memories ROMs RAMs EPROMs EEPROMs magnetic or optical cards or any type of media suitable for storing electronic instructions and each coupled to a computer system bus.

A machine readable medium includes any mechanism for storing or transmitting information in a form readable by a machine e.g. a computer . For example a machine readable medium includes read only memory ROM random access memory RAM magnetic disk storage media optical storage media flash memory devices etc.

An article of manufacture may be used to store program code. An article of manufacture that stores program code may be embodied as but is not limited to one or more memories e.g. one or more flash memories random access memories static dynamic or other optical disks CD ROMs DVD ROMs EPROMs EEPROMs magnetic or optical cards or other type of machine readable media suitable for storing electronic instructions. Program code may also be downloaded from a remote computer e.g. a server to a requesting computer e.g. a client by way of data signals embodied in a propagation medium e.g. via a communication link e.g. a network connection .

The preceding detailed descriptions are presented in terms of algorithms and symbolic representations of operations on data bits within a computer memory. These algorithmic descriptions and representations are the tools used by those skilled in the data processing arts to most effectively convey the substance of their work to others skilled in the art. An algorithm is here and generally conceived to be a self consistent sequence of operations leading to a desired result. The operations are those requiring physical manipulations of physical quantities. Usually though not necessarily these quantities take the form of electrical or magnetic signals capable of being stored transferred combined compared and otherwise manipulated. It has proven convenient at times principally for reasons of common usage to refer to these signals as bits values elements symbols characters terms numbers or the like.

It should be kept in mind however that all of these and similar terms are to be associated with the appropriate physical quantities and are merely convenient labels applied to these quantities. Unless specifically stated otherwise as apparent from the above discussion it is appreciated that throughout the description discussions utilizing terms such as detecting receiving determining writing forwarding transmitting sending reading or the like refer to the action and processes of a computer system or similar electronic computing device that manipulates and transforms data represented as physical electronic quantities within the computer system s registers and memories into other data similarly represented as physical quantities within the computer system memories or registers or other such information storage transmission or display devices.

The processes and displays presented herein are not inherently related to any particular computer or other apparatus. Various general purpose systems may be used with programs in accordance with the teachings herein or it may prove convenient to construct a more specialized apparatus to perform the operations described. The required structure for a variety of these systems will be evident from the description below. In addition the present invention is not described with reference to any particular programming language. It will be appreciated that a variety of programming languages may be used to implement the teachings of the invention as described herein.

The foregoing discussion merely describes some exemplary embodiments of the present invention. One skilled in the art will readily recognize from such discussion the accompanying drawings and the claims that various modifications can be made without departing from the spirit and scope of the invention.

