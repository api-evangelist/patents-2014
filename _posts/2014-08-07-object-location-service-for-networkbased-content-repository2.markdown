---

title: Object location service for network-based content repository
abstract: A distributed object store in a network storage system uses location-independent global object identifiers (IDs) for stored data objects. The global object ID enables a data object to be seamlessly moved from one location to another without affecting clients of the storage system, i.e., “transparent migration”. The global object ID can be part of a multilevel object handle, which also can include a location ID indicating the specific location at which the data object is stored, and a policy ID identifying a set of data management policies associated with the data object. The policy ID may be associated with the data object by a client of the storage system, for example when the client creates the object, thus allowing “inline” policy management. An object location subsystem (OLS) can be used to locate an object when a client request does not contain a valid location ID for the object.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09565254&OS=09565254&RS=09565254
owner: NetApp, Inc.
number: 09565254
owner_city: Sunnyvale
owner_country: US
publication_date: 20140807
---
This is a continuation of U.S. patent application Ser. No. 12 633 745 filed on Dec. 8 2009 which is incorporated herein by reference in its entirety.

At least one embodiment of the present invention pertains to network storage systems and more particularly to an object location service of a content repository in a network storage server system.

Network based storage or simply network storage is a common approach to backing up data making large amounts of data accessible to multiple users and other purposes. In a network storage environment a storage server makes data available to client host systems by presenting or exporting to the clients one or more logical containers of data. There are various forms of network storage including network attached storage NAS and storage area network SAN . In a NAS context a storage server services file level requests from clients whereas in a SAN context a storage server services block level requests. Some storage servers are capable of servicing both file level requests and block level requests.

There are several trends that are relevant to network storage technology. The first is that the amount of data being stored within a typical enterprise is approximately doubling from year to year. Second there are now multiple classes of storage devices available on the market today each with its own performance characteristics. These two trends together have caused users to want storage systems that mix different kinds of storage in such a way that it is possible to seamlessly move data across storage tiers based on some policy or policies.

In addition users often would like to apply policies to collections of data objects. For example an online social networking site service might want to replicate all of its original size photos e.g. photos of its members users three times but not the thumbnail versions since the thumbnails can be recreated from the originals. Yet today setting policy within a storage system is a cumbersome process that has to be done out of band by a system administrator. Application writers and users cannot specify policies on groups of files objects.

A problem associated with conventional storage systems is that the use of path names such as in a traditional filesystem imposes a hierarchical organization on the data to which applications need to conform and use for different purposes such as navigation and retrieval access control and data management. However a hierarchical organization may not make sense for uses other than navigation and retrieval and as a result it can lead to inefficiencies such as duplication of content and consequent administrative overhead.

Furthermore a hierarchical organization has also proven to be ineffective for navigation and retrieval. Consider a photo that is stored under a given path name such as home eng myname office.jpeg . In a traditional storage system this name maps to a specific server controller a specific volume and a specific file location e.g. inode number within that volume. Thus path names are tied to storage location.

The techniques introduced here provide a distributed object store in a network storage server system. The distributed object store can be part of a content repository which aside from the distributed object store includes a presentation layer a metadata subsystem and a policy based management subsystem. The content repository can be implemented in a multi node storage server cluster.

The distributed object store creates and uses system generated location independent location transparent global object identifiers IDs for sub volume level data objects e.g. files managed by the storage system. A data object can be any unit of data such as a file a block of data or a logical unit LUN . A sub volume level data object is a data object that can be stored within a volume defined below . The global object ID described herein enables the corresponding data object to be seamlessly moved from one location to another e.g. from one physical or logical storage container to another without affecting clients of the storage system i.e. transparently to the clients this capability can be called transparent migration .

The global object ID can be part of a multilevel object handle which also includes in addition to the global object ID a location identifier that indicates the specific location at which the data object is stored. The multilevel object handle can also include other information such as a policy ID that identifies a set of one or more data management policies associated with the data object. The policy ID may be associated with the data object by a client of the storage system for example at the time the client creates the data object. Embedding policy information within the object handle allows policy management to be implemented efficiently within the input output I O path of the server system i.e. inline policy management. For example in response to receiving from a client a request that includes the object handle the server system uses the policy ID in the object handle to look up in a database the particular policy or policies associated with that policy ID and then applies such policy or policies to the request and or to the data object.

When a client submits a data access request that includes a valid location ID i.e. within an object handle the server system can often use that location ID to directly locate and access the target data object. However in some instances the location ID in the object handle may be invalid such as if the target data object has been moved or if the client did not provide a complete object handle. For use in such instances the server system also includes an object location subsystem OLS to locate the target data object. The OLS includes a data structure that maps global object IDs to corresponding valid up to date location IDs of data objects. The server system further maintains a namespace which is independent of the OLS mapping structure and which includes a mapping of path names to global object IDs of the data objects stored in the server system. A namespace as the term is used herein is a mechanism for allowing end users or applications to name and organize data objects which may for example provide hierarchical naming and or organization of data such as a directory file structure . The namespace together with the OLS provides a layer of indirection between i.e. provides a logical separation of path names and storage locations of the stored data objects. This separation facilitates transparent migration i.e. an object can be moved without affecting its name and moreover it enables any particular data object to be represented by multiple paths names thereby facilitating navigation. In particular this allows the implementation of a hierarchical protocol such as NFS or CIFS on top of an object store while at the same time maintaining the ability to do transparent migration.

Other aspects of the technique will be apparent from the accompanying figures and from the detailed description which follows.

References in this specification to an embodiment one embodiment or the like mean that the particular feature structure or characteristic being described is included in at least one embodiment of the present invention. Occurrences of such phrases in this specification do not necessarily all refer to the same embodiment.

The storage server or servers may be for example one of the FAS xxx family of storage server products available from NetApp Inc. The client systems . . are connected to the storage server via the computer network which can be a packet switched network for example a local area network LAN or wide area network WAN . Further the storage server is connected to the disks via a switching fabric which can be a fiber distributed data interface FDDI network for example. It is noted that within the network data storage environment any other suitable numbers of storage servers and or mass storage devices and or any other suitable network technologies may be employed.

The storage server can make some or all of the storage space on the disk s available to the client systems . . in a conventional manner. For example each of the disks can be implemented as an individual disk multiple disks e.g. a RAID group or any other suitable mass storage device s . The storage server can communicate with the client systems . . according to well known protocols such as the Network File System NFS protocol or the Common Internet File System CIFS protocol to make data stored on the disks available to users and or application programs. The storage server can present or export data stored on the disk as volumes to each of the client systems . .. A volume is an abstraction of physical storage combining one or more physical mass storage devices e.g. disks or parts thereof into a single logical storage object the volume and which is managed as a single administrative unit such as a single file system. A file system is a structured e.g. hierarchical set of stored logical containers of data e.g. volumes logical unit numbers LUNs directories files . Note that a file system does not have to include or be based on files per se as its units of data storage.

Various functions and configuration settings of the storage server and the mass storage subsystem can be controlled from a management station coupled to the network . Among many other operations a data object migration operation can be initiated from the management station .

Each of the nodes is configured to include several modules including an network module a data module and an management host each of which can be implemented by using a separate software module and an instance of a replicated database RDB . Specifically node . includes an network module . a data module . and an management host . node .N includes an network module .N a data module .N and an management host .N and so forth. The network modules . .M include functionality that enables nodes . .N respectively to connect to one or more of the client systems over the network while the data modules . .N provide access to the data stored on the disks . .N respectively. The management hosts provide management functions for the clustered storage server system . Accordingly each of the server nodes in the clustered storage server arrangement provides the functionality of a storage server.

The RDB is a database that is replicated throughout the cluster i.e. each node includes an instance of the RDB . The various instances of the RDB are updated regularly to bring them into synchronization with each other. The RDB provides cluster wide storage of various information used by all of the nodes including a volume location database VLDB not shown . The VLDB is a database that indicates the location within the cluster of each volume in the cluster i.e. the owning data module for each volume and is used by the network modules to identify the appropriate data module for any given volume to which access is requested.

The nodes are interconnected by a cluster switching fabric which can be embodied as a Gigabit Ethernet switch for example. The network modules and data modules cooperate to provide a highly scalable distributed storage system architecture of a clustered computing environment implementing exemplary embodiments of the present invention. Note that while there is shown an equal number of network modules and data modules in there may be differing numbers of network modules and or data modules in accordance with various embodiments of the technique described here. For example there need not be a one to one correspondence between the network modules and data modules. As such the description of a node comprising one network module and one data module should be understood to be illustrative only.

The storage controller can be embodied as a single or multi processor storage system executing a storage operating system that preferably implements a high level module such as a storage manager to logically organize the information as a hierarchical structure of named directories files and special types of files called virtual disks hereinafter generally blocks on the disks. Illustratively one processor can execute the functions of the network module on the node while another processor executes the functions of the data module .

The memory illustratively comprises storage locations that are addressable by the processors and adapters for storing software program code and data structures associated with the present invention. The processor and adapters may in turn comprise processing elements and or logic circuitry configured to execute the software code and manipulate the data structures. The storage operating system portions of which is typically resident in memory and executed by the processors s functionally organizes the storage controller by among other things configuring the processor s to invoke storage operations in support of the storage service provided by the node . It will be apparent to those skilled in the art that other processing and memory implementations including various computer readable storage media may be used for storing and executing program instructions pertaining to the technique introduced here.

The network adapter includes a plurality of ports to couple the storage controller to one or more clients over point to point links wide area networks virtual private networks implemented over a public network Internet or a shared local area network. The network adapter thus can include the mechanical electrical and signaling circuitry needed to connect the storage controller to the network . Illustratively the network can be embodied as an Ethernet network or a Fibre Channel FC network. Each client can communicate with the node over the network by exchanging discrete frames or packets of data according to pre defined protocols such as TCP IP.

The storage adapter cooperates with the storage operating system to access information requested by the clients . The information may be stored on any type of attached array of writable storage media such as magnetic disk or tape optical disk e.g. CD ROM or DVD flash memory solid state disk SSD electronic random access memory RAM micro electro mechanical and or any other similar media adapted to store information including data and parity information. However as illustratively described herein the information is stored on disks . The storage adapter includes a plurality of ports having input output I O interface circuitry that couples to the disks over an I O interconnect arrangement such as a conventional high performance Fibre Channel FC link topology.

Storage of information on disks can be implemented as one or more storage volumes that include a collection of physical storage disks cooperating to define an overall logical arrangement of volume block number VBN space on the volume s . The disks can be organized as a RAID group. One or more RAID groups together form an aggregate. An aggregate can contain one or more volumes file systems.

The storage operating system facilitates clients access to data stored on the disks . In certain embodiments the storage operating system implements a write anywhere file system that cooperates with one or more virtualization modules to virtualize the storage space provided by disks . In certain embodiments a storage manager logically organizes the information as a hierarchical structure of named directories and files on the disks . Each on disk file may be implemented as set of disk blocks configured to store information such as data whereas the directory may be implemented as a specially formatted file in which names and links to other files and directories are stored. The virtualization module s allow the storage manager to further logically organize information as a hierarchical structure of blocks on the disks that are exported as named logical unit numbers LUNs .

In the illustrative embodiment the storage operating system is a version of the Data ONTAP operating system available from NetApp Inc. and the storage manager implements the Write Anywhere File Layout WAFL file system. However other storage operating systems are capable of being enhanced or created for use in accordance with the principles described herein.

In addition the storage operating system includes a set of layers organized to form a backend server that provides data paths for accessing information stored on the disks of the node . The backend server in combination with underlying processing hardware also forms the data module . To that end the backend server includes a storage manager module that manages any number of volumes a RAID system module and a storage driver system module .

The storage manager primarily manages a file system or multiple file systems and serves client initiated read and write requests. The RAID system manages the storage and retrieval of information to and from the volumes disks in accordance with a RAID redundancy protocol such as RAID 4 RAID 5 or RAID DP while the disk driver system implements a disk access protocol such as SCSI protocol or FCP.

The backend server also includes a CF interface module to implement intra cluster communication with network modules and or other data modules. The CF interface modules and can cooperate to provide a single file system image across all data modules in the cluster. Thus any network port of an network module that receives a client request can access any data container within the single file system image located on any data module of the cluster.

The CF interface modules implement the CF protocol to communicate file system commands among the modules of cluster over the cluster switching fabric . Such communication can be effected by a data module exposing a CF application programming interface API to which an network module or another data module issues calls. To that end a CF interface module can be organized as a CF encoder decoder. The CF encoder of e.g. CF interface on network module can encapsulate a CF message as i a local procedure call LPC when communicating a file system command to a data module residing on the same node or ii a remote procedure call RPC when communicating the command to a data module residing on a remote node of the cluster. In either case the CF decoder of CF interface on data module de encapsulates the CF message and processes the file system command.

In operation of a node a request from a client is forwarded as a packet over the network and onto the node where it is received at the network adapter . A network driver of layer processes the packet and if appropriate passes it on to a network protocol and file access layer for additional processing prior to forwarding to the storage manager . At that point the storage manager generates operations to load retrieve the requested data from disk if it is not resident in memory . If the information is not in memory the storage manager indexes into a metadata file to access an appropriate entry and retrieve a logical VBN. The storage manager then passes a message structure including the logical VBN to the RAID system the logical VBN is mapped to a disk identifier and disk block number DBN and sent to an appropriate driver e.g. SCSI of the disk driver system . The disk driver accesses the DBN from the specified disk and loads the requested data block s in memory for processing by the node. Upon completion of the request the node and operating system returns a reply to the client over the network .

The data request response path through the storage operating system as described above can be implemented in general purpose programmable hardware executing the storage operating system as software or firmware. Alternatively it can be implemented at least partially in specially designed hardware. That is in an alternate embodiment of the invention some or all of the storage operating system is implemented as logic circuitry embodied within a field programmable gate array FPGA or an application specific integrated circuit ASIC for example.

The network module and data module each can be implemented as processing hardware configured by separately scheduled processes of storage operating system however in an alternate embodiment the modules may be implemented as processing hardware configured by code within a single operating system process. Communication between an network module and a data module is thus illustratively effected through the use of message passing between the modules although in the case of remote communication between an network module and data module of different nodes such message passing occurs over the cluster switching fabric . A known message passing mechanism provided by the storage operating system to transfer information between modules processes is the Inter Process Communication IPC mechanism. The protocol used with the IPC mechanism is illustratively a generic file and or block based agnostic CF protocol that comprises a collection of methods functions constituting a CF API.

The techniques introduced here generally relate to a content repository implemented in a network storage server system such as described above. illustrates the overall architecture of the content repository according to one embodiment. The major components of the content repository include a distributed object store and object location subsystem OLS a presentation layer a metadata subsystem MDS and a management subsystem . Normally there will be a single instance of each of these components in the overall content repository and each of these components can be implemented in any one server node or distributed across two or more server nodes . The functional elements of each of these units i.e. the OLS presentation layer MDS and management subsystem can be implemented by specially designed circuitry or by programmable circuitry programmed with software and or firmware or a combination thereof. The data storage elements of these units can be implemented using any known or convenient form or forms of data storage device.

The distributed object store provides the actual data storage for all data objects in the server system and includes multiple distinct single node object stores . A single node object store is an object store that is implemented entirely within one node. Each single node object store is a logical non physical container of data such as a volume or a logical unit LUN . Some or all of the single node object stores that make up the distributed object store can be implemented in separate server nodes . Alternatively all of the single node object stores that make up the distributed object store can be implemented in the same server node. Any given server node can access multiple single node object stores and can include multiple single node object stores .

The distributed object store provides location independent addressing of data objects i.e. data objects can be moved among single node object stores without changing the data objects addressing with the ability to span the object address space across other similar systems spread over geographic distances. Note that the distributed object store has no namespace the namespace for the server system is provided by the presentation layer .

The presentation layer provides access to the distributed object store . It is generated by at least one presentation module i.e. it may be generated collectively by multiple presentation modules one in each multiple server nodes . A presentation module can be in the form of specially designed circuitry or programmable circuitry programmed with software and or firmware or a combination thereof.

The presentation layer essentially functions as a router by receiving client requests translating them into an internal protocol and sending them to the appropriate data module . The presentation layer provides two or more independent interfaces for accessing stored data e.g. a conventional NAS interface and a Web Service interface . The NAS interface allows access to the object store via one or more conventional NAS protocols such as NFS and or CIFS. Thus the NAS interface provides a filesystem like interface to the content repository.

The Web Service interface allows access to data stored in the object store via either named object access or raw object access also called flat object access . Named object access uses a namespace e.g. a filesystem like directory tree interface for accessing data objects as does NAS access whereas raw object access uses system generated global object IDs to access data objects as described further below. The Web Service interface allows access to the object store via Web Service as defined by the W3C using for example a protocol such as Simple Object Access Protocol SOAP or a RESTful REpresentational State Transfer ful protocol over HTTP.

The presentation layer further provides at least one namespace for accessing data via the NAS interface or the Web Service interface. In one embodiment this includes a Portable Operating System Interface POSIX namespace. The NAS interface allows access to data stored in the object store via the namespace s . The Web Service interface allows access to data stored in the object store via either the namespace s by using named object access or without using the namespace s by using raw object access . Thus the Web Service interface allows either named object access or raw object access and while named object access is accomplished using a namespace raw object access is not. Access by the presentation layer to the object store is via either a fast path or a slow path as discussed further below.

The function of the OLS is to store and provide valid location IDs and other information such as policy IDs of data objects based on their global object IDs these parameters are discussed further below . This is done for example when a client requests access to a data object by using only the global object ID instead of a complete object handle including the location ID or when the location ID within an object handle is no longer valid e.g. because the target data object has been moved . Note that the system thereby provides two distinct paths for accessing stored data namely a fast path and a slow path . The fast path provides data access when a valid location ID is provided by a client e.g. within an object handle . The slow path makes use of the OLS and is used in all other instances of data access. The fast path is so named because a target data object can be located directly from its valid location ID whereas the slow path is so named because it requires a number of additional steps relative to the fast path to determine the location of the target data object.

The MDS is a subsystem for search and retrieval of stored data objects based on metadata. It is accessed by users through the presentation layer . The MDS stores data object metadata which can include metadata specified by users inferred metadata and or system defined metadata. The MDS also allows data objects to be identified and retrieved by searching on any of that metadata. The metadata may be distributed across nodes in the system. In one embodiment where this is the case the metadata for any particular data object are stored in the same node as the object itself.

As an example of user specified metadata users of the system can create and associate various types of tags e.g. key value pairs with data objects based on which such objects can be searched and located. For example a user can define a tag called location for digital photos where the value of the tag e.g. a character string indicates where the photo was taken. Or digital music files can be assigned a tag called mood the value of which indicates the mood evoked by the music. On the other hand the system can also generate or infer metadata based on the data objects themselves and or accesses to them.

There are two types of inferred metadata 1 latent and 2 system generated. Latent inferred metadata is metadata in a data object which can be extracted automatically from the object and can be tagged on the object examples include Genre Album in an MP3 object or Author DocState in a Word document . System generated inferred metadata is metadata generated by the server system and includes working set information e.g. access order information used for object prefetching and object relationship information these metadata are generated by the system to enable better searching via metadata queries e.g. the system can track how many times an object has been accessed in the last week month year and thus allow a user to run a query such as Show me all of the JPEG images I have looked at in the last month . System defined metadata includes for example typical file attributes such as size creation time last modification time last access time owner etc.

The MDS includes logic to allow users to associate a tag value pair with an object and logic that provides two data object retrieval mechanisms. The first retrieval mechanism involves querying the metadata store for objects matching a user specified search criterion or criteria and the second involves accessing the value of a tag that was earlier associated with a specific object. The first retrieval mechanism called a query can potentially return multiple object handles while the second retrieval mechanism called a lookup deals with a specific object handle of interest.

The management subsystem includes a content management component and an infrastructure management component . The infrastructure management component includes logic to allow an administrative user to manage the storage infrastructure e.g. configuration of nodes disks volumes LUNs etc. . The content management component is a policy based data management subsystem for managing the lifecycle of data objects and optionally the metadata stored in the content repository based on user specified policies or policies derived from user defined SLOs. It can execute actions to enforce defined policies in response to system defined trigger events and or user defined trigger events e.g. attempted creation deletion access or migration of an object . Trigger events do not have to be based on user actions.

The specified policies may relate to for example system performance data protection and data security. Performance related policies may relate to for example which logical container a given data object should be placed in migrated from or to when the data object should be migrated or deleted etc. Data protection policies may relate to for example data backup and or data deletion. Data security policies may relate to for example when and how data should be encrypted who has access to particular data etc. The specified policies can also include polices for power management storage efficiency data retention and deletion criteria. The policies can be specified in any known convenient or desirable format and method. A policy in this context is not necessarily an explicit specification by a user of where to store what data when to move data etc. Rather a policy can be a set of specific rules regarding where to store what when to migrate data etc. derived by the system from the end user s SLOs i.e. a more general specification of the end user s expected performance data protection security etc. For example an administrative user might simply specify a range of performance that can be tolerated with respect to a particular parameter and in response the management subsystem would identify the appropriate data objects that need to be migrated where they should get migrated to and how quickly they need to be migrated.

The content management component uses the metadata tracked by the MDS to determine which objects to act upon e.g. move delete replicate encrypt compress . Such metadata may include user specified metadata and or system generated metadata. The content management component includes logic to allow users to define policies and logic to execute apply those policies.

In one embodiment the distributed object store is implemented by providing at least one single node object store in each of at least two data modules in the system any given data module can include zero or more single node object stores . Also implemented in each of at least two data modules in the system are an OLS store that contains mapping data structures used by the OLS including valid location IDs and policy IDs a policy store e.g. a database that contains user specified policies relating to data objects note that at least some policies or policy information may also be cached in the network module to improve performance and a metadata store that contains metadata used by the MDS including user specified object tags. In practice the metadata store may be combined with or implemented as a part of the single node object store .

The presentation layer is implemented at least partially within each network module . In one embodiment the OLS is implemented partially by the network module and partially by the corresponding management host as illustrated in . More specifically in one embodiment the functions of the OLS are implemented by a special daemon in the management host and by the presentation layer in the network module .

In one embodiment the MDS and management subsystem are both implemented at least partially within each management host . Nonetheless in some embodiments any of these subsystems may also be implemented at least partially within other modules. For example at least a portion of the content management component of the management subsystem can be implemented within one or more network modules to allow for example caching of policies in such network modules and or execution application of policies by such network module s . In that case the processing logic and state information for executing applying policies may be contained in one or more network modules while processing logic and state information for managing policies is stored in one or more management hosts . As another example at least a portion of the MDS may be implemented within one or more data modules to allow it to access more efficiently system generated metadata generated within those modules.

Administrative users can specify policies for use by the management subsystem via a user interface provided by the management host to access the management subsystem . Further via a user interface provided by the management host to access the MDS end users can assign metadata tags to data objects where such tags can be in the form of key value pairs. Such tags and other metadata can then be searched by the MDS in response to user specified queries to locate or allow specified actions to be performed on data objects that meet user specified criteria. Search queries received by the MDS are applied by the MDS to the single node object store in the appropriate data module s .

As noted above the distributed object store enables both path based access to data objects as well as direct access to data objects. For purposes of direct access the distributed object store uses a multilevel object handle as illustrated in . When a client creates a data object it receives an object handle as the response to creating the object. This is similar to a file handle that is returned when a file is created in a traditional storage system. The first level of the object handle is a system generated globally unique number called a global object ID that is permanently attached to the created data object. The second level of the object handle is a hint which includes the location ID of the data object and in the illustrated embodiment the policy ID of the data object. Clients can store this object handle containing the global object ID location ID and policy ID.

When a client attempts to read or write the data object using the direct access approach the client includes the object handle of the object in its read or write request to the server system . The server system first attempts to use the location ID within the object handle which is intended to be a pointer to the exact location within a volume where the data object is stored. In the common case this operation succeeds and the object is read written. This sequence is the fast path for I O see .

If however an object is moved from one location to another for example from one volume to another the server system creates a new location ID for the object. In that case the old location ID becomes stale invalid . The client may not be notified that the object has been moved or that the location ID is stale and may not receive the new location ID for the object at least until the client subsequently attempts to access that data object e.g. by providing an object handle with an invalid location ID . Or the client may be notified but may not be able or configured to accept or understand the notification.

The current mapping from global object ID to location ID is always stored reliably in the OLS . If during fast path I O the server system discovers that the target data object no longer exists at the location pointed to by the provided location ID this means that the object must have been either deleted or moved. Therefore at that point the server system will invoke the OLS to determine the new valid location ID for the target object. The server system then uses the new location ID to read write the target object. At the same time the server system invalidates the old location ID and returns a new object handle to the client that contains the unchanged and unique global object ID as well as the new location ID. This process enables clients to transparently adapt to objects that move from one location to another for example in response to a change in policy .

An enhancement of this technique is for a client never to have to be concerned with refreshing the object handle when the location ID changes. In this case the server system is responsible for mapping the unchanging global object id to location ID. This can be done efficiently by compactly storing the mapping from global object ID to location ID in for example cache memory of one or more network modules .

Refer now to which shows an example of the overall process by which the distributed object store services a data access request from a client . Initially at the server system receives from a client a request to access the target data object e.g. a read or write request . The request at least includes a global object ID for the target data object. The server system then determines at whether the request includes a location ID as noted above in some instances a client may provide only the global object ID with the request . If the request includes a location ID then the process proceeds with the fast path I O i.e. to operation otherwise the process proceeds with the slow path to operation .

At the distributed object store gets the location ID in the provided object handle. Next at the server system attempts to access the target data object according to that location ID. Part of attempting to access the data object is determining whether the location ID provided in the object handle is valid. In one embodiment this is accomplished by examining a flag in metadata of the target object where such flag is set whenever the object is deleted or moved. For example such a flag may exist in an inode representing the target object. If the object has been deleted or moved the location ID will be invalid.

In this regard note that the location ID maps to an internal file handle which includes a number of fields. Once a data module receives a file handle it can determine by looking at these fields whether the file handle is recent. The two relevant fields in this regard are the file ID or inode number and the generation number. The file ID or inode number can be used to determine if an inode for the target data object exists and so whether the data object itself exists and the generation number can be used to determine whether the file handle refers to the correct version of the data object. The file ID or inode number maps to the data object s inode and the generation number is a counter stored within the inode. Whenever the inode is reused e.g. the previous data object is deleted and a new one is created the generation number within the inode is incremented. This allows a data module and more specifically its storage manager to detect access to a valid inode with an invalid generation number. Once this occurs the storage manager in the data module returns a Stale file handle error which triggers an Invalid Location ID error. Thus the file ID can be used to determine if an inode for the target data object exists and so whether the data object itself exists and the generation number can be used to determine whether the file handle refers to the correct version of the data object. If one of these is not valid an Invalid Location ID error is returned and can be used to trigger access the OLS to get an updated location ID.

Referring still to if the location ID in the object handle is valid then at the server system accesses the target data object according to that location ID. The server system then sends an appropriate response to the client at e.g. including the requested data in the case of a read or a confirmation in the case or write and the process then ends.

If the location ID was not valid then the process branches to the slow path proceeding to operation . At the server system gets the global object ID from the object handle provided by the client. At the server system invokes the OLS passing the global object ID to the OLS . The OLS then determines and returns the valid location ID for the target data object at in a manner which is described below. The server system then accesses the target data object at according to the valid location ID and at the server system sends an appropriate response to the client including the new valid location ID for the target object. The process then ends.

Referring again to if the request from the client did not include a location ID the system uses the slow path proceeding to as described above.

As noted above an object handle can contain a policy ID to support inline policy management i.e. policy management within the normal I O path which allows fast execution of policies. When a data object is created the create function can also specify the policy or set of policies that needs to be applied on the object. Examples of such a policy expressed here in natural language for simplicity include replicate an object twice compress the object after storing it and store the object on cheap low power disks . One or more such policies can be represented by a policy ID.

Each time during an object read write or delete the server system uses the policy ID encoded in the object handle to quickly look up in the policy store the action that needs to be taken. For example if the policy states do not delete this file until 2015 a delete operation will fail until after that year. If for some reason a policy ID cannot be specified as may be the case with certain protocols such as NFS or CIFS a default policy or a policy based on the data object s location or type can be applied.

If the policy ID is determined not to be valid at then the process branches to where the server system looks up the valid policy ID for the object handle in the OLS using the global object ID in the object handle as a lookup key. The process then continues to .

After or after the policy ID is determined to be valid at the server system looks up in the policy store the policy or policies that correspond to the valid policy ID at . At the server system then applies the identified policy or policies. A policy may apply to a specific data object e.g. encrypt file A . A policy can also or alternatively apply to a particular client or user e.g. Joe is prohibited from accessing file A or to a particular logical container e.g. volume X is read only . The server system sends an appropriate response to the client at and the process then ends.

If it is determined at that the request from the client does not specify a policy then a default policy or a policy based on the data object s location will be used accordingly the process in that case branches to where the server system creates an object handle for the object and includes the policy ID of the default or selected policy in the object handle. The server system then proceeds to and continues as described above.

The OLS is a mechanism the primary purpose of which is to allow a valid location ID of a data object to be determined from the object s global object ID. However the OLS also allows the policy ID and or any other metadata associated with a data object to be identified in essentially the same way. An example of how this can be implemented is described now with reference to .

In one embodiment each global object ID used by the server system is a multi bit entity which is logically divided into multiple segments. Each segment includes at least one bit and typically includes multiple bits. In the example of a global object ID is a nine bit value which is divided into three segments X Y and Z each of which includes three bits. A first segment X represents the three most significant bits of the global object ID a second segment Y represents the next most significant bits of the global object ID and segment Z represents the three least significant bits of the global object ID. These particular numbers of bits and segments are used here only to facilitate description for any given system the number of segments and bits in a global object ID can be chosen to accommodate the system s anticipated storage needs i.e. a greater number of segments bits allows a greater number of data objects to be represented .

The OLS includes a mapping data structure which can be stored in the OLS store in that maps global object IDs to their corresponding location IDs and policy IDs and or any other metadata that may be associated with a data object . Each predefined segment of the global object ID is used to index into a different level of the mapping data structure . In the example of each three bit segment of the global object ID can have eight possible values e.g. 0 1 2 . . . 7 and therefore can represent eight different entries within a given level of the mapping data structure . For example the value of segment X is used to select the appropriate entry in the top level of the mapping data structure the value of segment Y is used to select the appropriate entry in the next lower level of the mapping data structure and the value of segment Z is used to select the appropriate entry in the lowest level of the mapping data structure . The selected entry in the lowest level contains the current valid location ID and policy ID of the global object ID . In this way the OLS enables the current location ID policy ID and or any other metadata associated with a data object to be easily located based on the global object ID of the object.

In one embodiment each node in the structure depicted in is a directory in a file system and the traversal of the tree structure is accomplished by a conventional directory traversal operation performed by the storage manager of a data module . In another embodiment the leaf nodes can contain multiple mappings instead of just one. In that case the entries in each leaf node have the form . That is the remaining least significant bits of the object ID that were not used in the directory traversal to locate the leaf node are used as the lookup key in the directory that is the leaf node.

These nodes both the leaves and the internal nodes can reside on any storage container on any data module in the system. The use of a global namespace in the storage cluster allows the stitching of these nodes into a single tree that can be traversed using standard directory tree traversal. By spreading the tree across multiple data modules the performance of the OLS can be scaled out and we can avoid the OLS becoming a centralized bottleneck.

Note also that the OLS tree can be populated on demand as objects are created that fall into specific areas of the tree. This approach represents a trade off between space and time i.e. the space consumed for storing potentially unused sections of the tree versus the increased latency of creating objects due to having to create these OLS nodes in line during object creation.

As noted above the server system logically separates path names from object handles. In a traditional storage system a file is represented by a path such as u foo bar file.doc . In this example u is a directory under the root directory foo is a directory under u and so on. Each component in this path gets mapped to a specific handle that identifies a specific storage location on a specific storage device. Therefore the entire path name maps to a specific location making it very difficult to move files around without having to rename them.

The multi level object handle technique introduced here allows the server system to break the tight relationship between path names and location that is characteristic of conventional storage systems. In one embodiment path names in the server system are stored in a POSIX namespace which is maintained by the presentation layer and is independent of actual locations of objects. The POSIX namespace includes a data structure for mapping path names to corresponding global object IDs. By using this mapping in conjunction with the OLS i.e. by mapping path name to global object ID and then mapping global object ID to location ID the server system can mimic a traditional filesystem hierarchy. In certain embodiments the global object ID is stored within the object handle presented by the NAS protocol thus avoiding a need to lookup the mapping on every access.

The POSIX namespace together with the OLS thereby provides a layer of indirection between i.e. provides a logical separation of path names of stored data objects and the storage locations of the data objects and also provides a layer of indirection between object identifiers of the stored data objects and the storage locations of the data objects. This separation facilitates transparent migration i.e. an object can be moved without affecting its name and moreover it enables any particular data object to be represented by multiple paths names thereby facilitating navigation. In particular this allows the implementation of a hierarchical protocol such as NFS on top of an object store while at the same time maintaining the ability to do transparent migration. For example when an object is moved to a new location all that is necessary is update its OLS mapping to point to the new location. After that subsequent requests by path name are carried out by mapping the existing path name to the existing global object ID and then mapping that global object ID to the new location ID.

The techniques introduced above can be implemented by programmable circuitry programmed or configured by software and or firmware or entirely by special purpose circuitry or in a combination of such forms. Such special purpose circuitry if any can be in the form of for example one or more application specific integrated circuits ASICs programmable logic devices PLDs field programmable gate arrays FPGAs etc.

Software or firmware for implementing the techniques introduced here may be stored on a machine readable storage medium and may be executed by one or more general purpose or special purpose programmable microprocessors. A machine readable medium as the term is used herein includes any mechanism that can store information in a form accessible by a machine a machine may be for example a computer network device cellular phone personal digital assistant PDA manufacturing tool any device with one or more processors etc. . For example a machine accessible medium includes recordable non recordable media e.g. read only memory ROM random access memory RAM magnetic disk storage media optical storage media flash memory devices etc. etc.

The term logic as used herein can include for example special purpose hardwired circuitry software and or firmware in conjunction with programmable circuitry or a combination thereof.

Although the present invention has been described with reference to specific exemplary embodiments it will be recognized that the invention is not limited to the embodiments described but can be practiced with modification and alteration within the spirit and scope of the appended claims. Accordingly the specification and drawings are to be regarded in an illustrative sense rather than a restrictive sense.

