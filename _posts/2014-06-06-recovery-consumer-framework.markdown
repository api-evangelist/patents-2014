---

title: Recovery consumer framework
abstract: A recovery consumer framework provides for execution of recovery actions by one or more recovery consumers to enable efficient recovery of information (e.g., data and metadata) in a storage system after a failure event (e.g., a power failure). The recovery consumer framework permits concurrent execution of recovery actions so as to reduce recovery time (i.e., duration) for the storage system. The recovery consumer framework may coordinate (e.g., notify) the recovery consumers to serialize execution of the recovery actions by those recovery consumers having a dependency while allowing concurrent execution between recovery consumers having no dependency relationship. Each recovery consumer may register with the framework to associate a dependency on one or more of the other recovery consumers. The dependency association may be represented as a directed graph where each vertex of the graph represents a recovery consumer and each directed edge of the graph represents a dependency. The framework may traverse (i.e., walk) the framework graph and for each vertex encountered, notify the associated recovery consumer to initiate its respective recovery actions.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09372767&OS=09372767&RS=09372767
owner: NetApp, Inc.
number: 09372767
owner_city: Sunnyvale
owner_country: US
publication_date: 20140606
---
The present disclosure relates to storage systems and more specifically to efficient use of recovery information to recover a storage system.

A storage system typically includes one or more storage devices into which information may be entered and from which information may be obtained as desired. The storage system may logically organize the information stored on the devices as storage containers i.e. a logical pool of storage. Each storage container may be implemented as a set of data structures such as data blocks that store data for the storage containers and metadata blocks that describe the data of the storage containers. The storage system may be configured to log portions of the data and metadata as recovery information for the storage containers in the event of a storage system failure e.g. a power failure .

The storage system also may be configured to provide services relating to access of the data in the storage containers stored on the storage devices. The services may be configured to operate according to a client server model of information delivery to thereby allow many clients hosts to access the storage containers. In other words the storage system may service client data access requests from the storage containers.

In the event of a failure the storage system may be configured to initiate recovery actions that apply recovery information to the storage containers so as to avoid information i.e. data and metadata loss. Depending on the recovery needs for a storage container a different amount of recovery information may be applied. That is the amount of recovery information and recovery time required for a storage container after a failure event may vary from storage container to storage container. Further recovery actions are often performed serially across all storage containers regardless of the amount of recovery time actually required for each storage container. Serial recovery is typically performed because a dependency of the recovery information and recovery actions needed among the storage containers is often unknown. Generally it is desirable for the storage containers to become available to service data access requests as soon as practicable after a failure event. However when the dependency of recovery information among the storage containers is unknown recovery of the storage containers may be inefficient. As a result there may be substantial disparity among the storage containers with respect to duration of recovery e.g. some storage containers may require only minutes of recovery time while others may require hours. As such the availability of some storage containers to service data access requests after a failure event may be unnecessarily delayed.

The subject matter described herein provides a recovery consumer framework for execution of recovery actions by one or more recovery consumers to enable efficient recovery of information e.g. data and metadata in a storage system after a failure event e.g. a power failure . To that end the recovery consumer framework permits concurrent execution of recovery actions so as to reduce recovery time i.e. duration for the storage system.

Illustratively the recovery consumers may include recovery actions i.e. procedures using one or more associated recovery logs whereas the recovery actions may include various types of actions such as a replay of one or more of the recovery logs. As used herein a recovery log may include any self describing set of information e.g. a log of host data access requests which includes a set of specific properties such as whether the log is online. Furthermore the recovery log may be an intent log which includes information e.g. data and metadata recorded in the log prior to storage of that information in one or more storage containers of the storage system. When executing the recovery actions the recovery consumers may apply the recovery logs to the one or more storage containers to avoid loss of information after a failure event. The recovery consumer framework may coordinate e.g. notify the recovery consumers to serialize execution of the recovery actions by those recovery consumers having a dependency. However for those consumers having no dependency relationship the recovery consumer framework allows parallel i.e. concurrent execution of their recovery actions thereby substantially increasing efficiency i.e. reducing recovery time after the failure event.

Illustratively each recovery consumer may register with the framework to associate a dependency on one or more of the other recovery consumers. That is the recovery consumer may declare a dependency such that the respective recovery actions of the consumer are not executed until all dependent consumers have completed their respective recovery actions e.g. consumer that depends on consumer waits until consumer has completed all of its recovery actions. Accordingly the recovery consumers may be ordered such that their recovery actions are executed serially according to their dependencies. The framework may implement such dependency association as a directed graph where each vertex i.e. graph node of the graph represents a recovery consumer and each directed edge of the graph represents a dependency. Alternatively the declaration of a dependency among the recovery consumers may be determined by a dependency rule set based on registration of a type of recovery action rather than a registration of the dependency. For example consumer may include and thus register a type of action to replay logs related to redundancy information among storage devices whereas consumer may include and thus register a type of action to replay logs related to host data access requests. An exemplary rule may specify that the log having the redundancy information be replayed before the log having the data access requests. As a result a framework graph may be constructed i.e. vertex and edge added in accordance with the exemplary rule to illustrate that consumer first vertex depends on directed edge consumer second vertex and a path from the first vertex to the second vertex may denote that the recovery actions of consumer are executed after i.e. wait on the recovery actions of consumer . Notably the framework graph may be implemented such that independent paths in the graph represent those recovery consumers which may execute their recovery actions in parallel.

Illustratively the framework may traverse i.e. walk the framework graph and for each vertex encountered notify the associated recovery consumer to initiate its respective recovery actions. Specifically the framework may traverse the graph breadth first so that recovery consumers without associated dependencies may be notified in parallel and thus may execute their actions concurrently while consumers having associated dependencies may be organized for serialized execution of their recovery actions. Illustratively a framework technique may be employed whereby each recovery consumer may tag i.e. associate one or more of the recovery logs depending on the nature of the failure and the properties of the logs e.g. whether the log is online . Further the tags may direct the actions e.g. encode instructions of the recovery consumers. As each recovery consumer completes execution of its respective recovery actions the framework is notified of completion and the tag is removed from the associated log. The framework may then notify e.g. in serial fashion a next dependent i.e. waiting consumer to initiate its respective recovery actions. In this manner the consumer recovery framework ensures that all recovery consumers execute their respective recovery actions while permitting a high degree of parallelism and satisfying the consumer dependencies.

Each host may be embodied as a general purpose computer configured to interact with any node in accordance with a client server model of information delivery. That is the client host may request the storage services of the node and the node may return the results of the services requested by the host by exchanging packets over the network . The host may issue packets forming a data access request according to file based access protocols such as the Network File System NFS protocol over the Transmission Control Protocol Internet Protocol TCP IP when accessing information on the node in the form of files and directories. Alternatively the host may issue packets forming a data access request according to block based access protocols such as the Small Computer Systems Interface SCSI protocol encapsulated over TCP iSCSI and SCSI encapsulated over FC FCP when accessing information in the form of logical units LUNs . Notably any of the nodes may service the data access request for information in any form stored on the HA storage environment .

As described herein the components of each node may include hardware and software functionality that enable the node to connect to one or more hosts over computer network as well as to a storage pool of one or more storage devices over a storage interconnect to thereby render the storage services in accordance with the HA storage architecture. Each storage pool may be organized into one or more groups of storage devices arranged as Redundant Array of Independent or Inexpensive Disks RAID . Generally RAID implementations enhance the reliability i.e. integrity of data storage through a redundancy technique of writing information in stripes across a number of storage devices together with appropriate parity information with respect to each stripe. Each RAID group may be implemented according to a RAID level such as RAID 4 or another type of RAID implementation such as RAID double parity RAID DP . It should be understood that a variety of other levels and types of RAID may alternatively be implemented and may differ from RAID group to RAID group e.g. RAID group may use RAID 4 while RAID group may use RAID DP . Further one or more RAID groups may be organized into aggregates not shown that represent collections of storage within the storage pool .

To facilitate access to data stored in the HA storage environment the nodes may virtualize the storage pool. To that end the storage pool may be logically organized into one or more storage containers e.g. volumes formed from a portion e.g. a whole or part of one or more RAID groups . In other words storage of information may be implemented as one or more volumes that illustratively include a collection of storage devices e.g. RAID group cooperating to define an overall logical arrangement of the volume s . A storage container may span one or more RAID groups in whole or in part. Each storage container may be owned by a single node which is configured to perform operations e.g. service the data access requests directed to that storage container in fulfillment of the storage services.

To promote recovery in the event of failure each node may log information such as operations e.g. data access requests directed to the storage container owned by the node that have been acted upon i.e. serviced but not yet been committed i.e. persistently stored to the storage devices. The logged information may be maintained in a non volatile random access memory NVRAM of the node that owns the storage container. During normal operation information logged in the NVRAM of the node may be mirrored to the NVRAM of its HA partner node to provide redundancy. For example information logged in the NVRAM e.g. of the node may be mirrored to and maintained in the NVRAM of the HA partner node . In response to a failure of the node the HA partner node may initiate a takeover sequence to e.g. assume the identity of the failed node access the storage devices owned by the failed node and replay the mirrored information maintained in its NVRAM as well as otherwise take over for the failed node. As part of the sequence contents of the mirrored information of the NVRAM may be copied i.e. saved to the storage pool for subsequent processing such as when storage containers or the underlying aggregates are off line during recovery. An example takeover sequence is described in commonly owned U.S. Pat. No. 7 730 153 titled EFFICIENT USE OF NVRAM DURING TAKEOVER IN A NODE CLUSTER to Gole et al. issued on Jun. 1 2010.

The HA interface may include one or more ports adapted to couple the node to the HA partner node or a plurality of other nodes . Illustratively Ethernet may be used as the clustering protocol and HA interconnect although it will be apparent to those skilled in the art that other types of protocols and interconnects such as Infiniband may be utilized within the implementations described herein.

The memory may include memory locations for storing at least some of the software functionality illustratively implemented as software programs and data structures. Among the software programs may be storage operating system that functionally organizes the node by among other things invoking operations in support of the storage services provided by the node . In an implementation the storage operating system is the NetApp Data ONTAP operating system available from NetApp Inc. Sunnyvale Calif. that implements a Write Anywhere File Layout WAFL file system. Accordingly one or more storage containers e.g. volumes may be configured according to the WAFL file system. However a variety of other types of storage operating systems that implement other types of file systems may alternatively be utilized. As such where the term WAFL is employed it should be taken broadly to refer to any storage operating system that is otherwise adaptable to the teachings of the disclosure herein.

As noted the NVRAM may store record information such as operations e.g. data access requests serviced by the node that have not yet been committed i.e. persistently stored to the storage devices. Such information may be maintained in one or more recovery logs stored in a portion of the NVRAM . That is a portion of the NVRAM may be configured as one or more non volatile logs i.e. recovery logs configured to temporarily record log data access requests e.g. write requests received from the host s . In order to persistently store the logged information the NVRAM may include a back up battery or be designed to intrinsically have last state retention capability e.g. utilize non volatile semiconductor memory that allows the NVRAM to maintain information through system restarts power failures and the like. It should be noted that the recovery logs may be persistently stored in the storage pool.

The storage operation system may include software components e.g. modules such as one or more recovery consumers which may implement procedures and techniques described herein. Illustratively the recovery consumers may include recovery actions i.e. procedures to restore the storage container s using the information logged in the recovery logs of the NVRAM when a failure event e.g. a power failure a node failure occurs. To that end the storage operating system may also include a recovery framework for efficiently coordinating the recovery consumers in response to the failure event. Additionally the recovery framework may provide a recovery framework application programming interface API to support coordination among the recovery consumers.

In addition the storage operating system includes storage layers such as a RAID system layer that implements a redundancy technique e.g. RAID 4 for the RAID groups and a storage driver layer that implements a disk access protocol e.g. SCSI . Additional software layers of the storage operating system include a HA interface layer for controlling the operation of the HA interconnect between nodes a NVRAM layer for storing and retrieving information from the NVRAM and a failover monitor that monitors the health of the HA storage system. The failover monitor may include a data structure such as a failover monitor resource table configured to itemize monitored resources. Bridging the network protocol layers and the storage layers of the storage operating system is a file system layer that organizes and controls i.e. manages storage and retrieval of information on the storage devices and in particular on the storage container s of the storage pool .

It should be noted that the operating system layers described above may alternatively be implemented in hardware. That is in an alternative aspect of the disclosure one or more layers of the storage operating system may be implemented as logic circuitry embodied within a field programmable gate array FPGA or an application specific integrated circuit ASIC . This type of hardware implementation increases the performance of the storage services provided by node .

Illustratively a data access request directed to the storage container is received via the network and processed by a file system process e.g. a WAFL process of the storage operating system . The request may be recorded i.e. written as an entry in the recovery log i.e. file system recovery log . The entry may include metadata of the data access request such as a type of access request e.g. create file and write file data as well as any data included with the data access request. As such the recovery log may operate as an intent log configured to retain information e.g. data and metadata recorded prior to storage of that information in the storage container . Widely used file and block data access protocols such as NFS and iSCSI respectively specify that a servicer i.e. node of a data access request should not reply i.e. return a response to the data access request until the request is completed e.g. data of the access request is written to persistent storage . This specification may be satisfied by recording the data access request to NVRAM thereby permitting a reply to be quickly returned to the requesting host client . Accordingly data access requests which modify storage containers are recorded in the recovery log . Further the recovery log may include properties pertaining to a state or aspect of the log such as a number of entries in the log and whether the log is online. Notably the recovery log may operate as a transaction journal of storage container changes e.g. entry which may be replayed so as to restore a consistent state of the storage container in the event of a failure. It will be understood to those skilled in the art that other variations of data structures may be used to store or maintain the information in the NVRAM including logical organizations of the logs by type of entry and data structures with no logs.

A RAID process of the storage operating system may provide a RAID implementation as previously described. The RAID process may log information such as parity and metadata related to a RAID operation e.g. writing a stripe prior to committing i.e. flushing that information to the RAID group . A RAID computation may be stored as an entry in a recovery log RAID recovery log . Further the recovery log may include properties pertaining to a state or aspect of the log such as a number of entries in the log and whether the log is online. Notably the RAID recovery log may operate as a transaction journal of changes e.g. entries to the RAID group which may be replayed so as to restore a consistent state of the RAID group in the event of a failure.

The NVRAM may temporarily store entries of the recovery logs until such time as a consistency point CP is reached which may occur at a fixed time interval e.g. 10 seconds or at other triggering events e.g. NVRAM half full . At such time accumulated entries in the recovery logs may be committed i.e. flushed to the storage devices thereby completing the CP by ensuring that the storage container s and RAID groups are consistent. Illustratively the file system layer may include a consistency point CP process that implements the CP by flushing recovery log information i.e. contents of the entries in the recovery logs to the storage devices i.e. the storage container s and the RAID groups . The CP process may also track the occurrence of CPs wherein each CP may be identified by a unique CP number that is generated by a monotonically increasing CP count. For example a value of the CP count may be incremented for each CP occurrence such that the CP count value increases over time and a greater value of the CP count indicates a CP later in time. Accordingly entries in the recovery logs may be discarded when associated with an older CP i.e. lesser CP count value than the CP currently processed. Alternatively the CP count may be generated using other techniques to provide each CP with a unique identifier. The CP count may be stored in the recovery log properties . In an implementation the CP count may also be stored as part of each log entry . Further the CP count may be stored as metadata within the storage container at the completion of the CP so as to timestamp a consistent state of the storage container. Following completion of the CP the recovery logs may be discarded emptied .

However prior to completion of the CP any system interruption power loss or failure may be recoverable through replay of the recovery logs from the previous CP. Since the NVRAM is constructed to survive power failures and other mishaps the contents of the recovery logs are appropriately protected. These logs may be retrieved by the recovery consumers under coordination of the recovery framework during recovery i.e. when a failure occurs. As noted the storage container s may be formed from one or more RAID groups . Accordingly any storage or retrieval of data to from the storage container depends on the underlying RAID group s . When a failure occurs the RAID recovery log may be replayed to restore the RAID group s to a consistent state prior to replay of the file system recovery log to restore the storage container to a consistent state. Illustratively a first recovery consumer e.g. the RAID consumer having actions that process e.g. replay the RAID recovery log may execute prior to a second recovery consumer e.g. the file system consumer having actions that process e.g. replay the file system recovery log . Thus the second consumer depends on the first consumer. Further a third recovery consumer e.g. a NVSAVE consumer may copy i.e. write the mirrored contents of the failed HA partner node s NVRAM to the storage pool immediately after a takeover. The third recovery consumer actions i.e. copying NVRAM contents may be performed concurrently with the first and second recovery consumer actions respectively which process recovery logs from the NVRAM. In other words the first consumer and second consumer are independent of i.e. may execute concurrently with the third consumer. In general recovery consumers that process recovery logs which are independent of each other may be executed concurrently.

As described herein the recovery consumer framework provides for execution of recovery actions by one or more recovery consumers to enable efficient recovery of information e.g. data and metadata in a storage system after a failure event e.g. a power failure . Illustratively the recovery consumers may include recovery actions i.e. procedures using one or more associated recovery logs whereas the recovery actions may include various types of actions such as a replay of one or more of the recovery logs. As used herein a recovery log may include any self describing set of information e.g. a log of the host data access request . Each recovery consumer may process e.g. replay one or more of the recovery logs. For example the recovery consumer may process the file system recovery log and another recovery consumer may process the RAID recovery log . When executing the recovery actions the recovery consumers may apply the recovery logs to the one or more storage containers to avoid loss of information after a failure event. The recovery consumer framework may coordinate e.g. notify the recovery consumers to serialize execution of the recovery actions by those recovery consumers having a dependency. However of those recovery consumers having no dependency relationship the recovery consumer framework allows parallel i.e. concurrent execution of their recovery actions thereby substantially increasing efficiency i.e. reducing recovery time after the failure event.

To determine whether a recovery log is needed in recovery a tagging technique may be employed to determine whether a recovery log is used by a recovery consumer during recovery. Illustratively each recovery consumer may register one or more callbacks with the recovery framework in accordance with the tagging technique whereby the recovery framework may invoke i.e. call the registered tag callback for the recovery consumer to determine whether a recovery log is used by that recovery consumer during recovery. Further each recovery consumer may also register a dependency on one or more of the other recovery consumers.

Illustratively the recovery consumer tag callback i.e. tag callback may include querying of the log properties such as the number of entries in the recovery log and in response may tag the log. For example in response to a query of a log having zero entries the tag procedure may determine not to process the log and thus not tag the log. In another implementation the tag procedure may query a CP count log and in response tag the log. For instance when the CP count associated with the log is less than the CP count of the storage container to be recovered the tag procedure may determine not to process i.e. replay that log. That is in response to determining not to process the recovery log the tag procedure does not tag the log.

In addition to querying the recovery log properties to determine whether to process and thus tag a recovery log individual log entries such as a CP count associated with a log entry may be examined. When a portion e.g. at least one log entry of a recovery log is used by a recovery consumer during recovery the recovery consumer tag callback may tag that log i.e. the recovery log as a whole may be tagged by the recovery consumer tag callback. However failure to tag a recovery log may indicate that the log is unused by the recovery consumer that registered that tag callback. For example a recovery log may include one log entry with an associated CP count value that is greater i.e. newer than that of the last CP count value of the storage container in recovery. Replay of the one log entry may thus suffice to restore the storage container to a consistent state i.e. only the single log entry from the entire log is replayed.

Tagging i.e. marking of the recovery log by the tag procedure i.e. tag callback registered by the recovery consumer may be performed by a tagging function provided by the recovery framework API. Illustratively the tag callback may invoke a tagging function of the framework API in order to tag i.e. mark a recovery log when the tag callback determines that the log is to be tagged. The recovery log may be marked by the tagging function from a set of distinct tag values. In an implementation the set of distinct tag values may be formed from numeric powers of 2 e.g. 2 4 8 16 . Accordingly multiple distinct tag values tags may be collected into a distinct tag group for a recovery log by bitwise OR operation of one or more distinct tags. Thus repeated invocation of the tagging function by the tag procedure may accumulate tags e.g. bitwise OR . In addition a reserved distinct tag value e.g. zero may be used to remove i.e. clear tags.

In an aspect of the disclosure the tags may have semantic meaning e.g. instructions to one or more of the recovery consumers. For example a tag associated with the name REPLAY may instruct a recovery consumer e.g. the file system consumer to replay entries in the recovery log marked with that tag. In another example a tag associated with the name SAVE LOG may instruct a recovery consumer e.g. the NVSAVE consumer to save the recovery log contents from NVRAM to the storage pool. Accordingly the one or more distinct tags may be used to instruct i.e. direct actions of the recovery consumer during recovery. That is the distinct tags may encode instructions i.e. actions to the recovery consumers. Notably the distinct tags may be opaque to the recovery framework and among the recovery consumers. For example the REPLAY tag may be used to direct actions of the file system consumer but may have no meaning i.e. directs no action for the NVSAVE consumer.

In an implementation the framework entry may also include an is tagged callback field registered by the recovery consumer and used in accordance with the recovery framework tagging technique. The is tagged callback field may contain a callback e.g. function pointer to a procedure associated with the registering recovery consumer to determine whether a recovery log is tagged i.e. marked with any of the distinct tags . The is tagged callback may include a parameter that indicates a recovery log and that returns a Boolean value e.g. true false . It will be apparent to those skilled in the art that the is tagged callback parameter may be a pointer to a data structure representing one or more of the recovery logs .

In an aspect of the disclosure the framework entry may include a dependency data structure used by a registering recovery consumer to register a dependency association with one more or more other recovery consumers. Each recovery consumer may be identified with a distinct recovery consumer identifier recovery consumer ID stored in the dependency data structure and used to register the dependency association. Accordingly a recovery consumer may register a dependency association with another recovery consumer by providing a recovery consumer ID of the other recovery consumer. As such one or more invocations may be rendered of a registration function included in the recovery framework API. For example assume the RAID consumer is assigned a consumer ID and the file system consumer is assigned a consumer ID . The file system consumer may establish a dependency on the RAID consumer by registering a dependency association with the recovery consumer having consumer ID . Illustratively multiple dependency associations may be registered in a list of dependency data structures . It will be understood to those skilled in the art that other variations of data structures may be used to maintain the dependency data structures such as trees and arrays.

Alternatively declaration of a dependency association among the recovery consumers may be determined by a dependency rule set based on registration of a type of recovery action e.g. file system consumer and RAID consumer rather than a registration of the dependency association. For example consumer e.g. RAID consumer may include and thus register a type of action to replay logs related to redundancy information among storage devices in RAID groups whereas consumer e.g. file system consumer may include and thus register a type of action to replay logs related to host data access requests. An exemplary dependency rule may specify that the log having the redundancy information e.g. RAID log be replayed before the log having the data access requests e.g. file system log since as noted file system recovery depends on RAID recovery. Accordingly one or more dependency data structures may be generated e.g. automatically by the dependency rule set to establish the dependency i.e. create a dependency association based on the dependency rule set. Specifically the rule set may be determined from a resource table not shown maintained by the failover monitor . The resource table may organize resources according to dependency such that recovery consumers associated with the resources incur the same dependency as specified by the organization of the resources in the resource table. Accordingly a dependency rule set may be extracted from the resource table and used to automatically generate the dependency data structures of the recovery framework. In an implementation registration of a circular dependency may be avoided by construction of a framework graph described herein.

Illustratively the distinct tag groups may include the distinct tags C C C C and C each representative of a respective recovery consumer and e.g. having consumer IDs and respectively . As noted each distinct tag e.g. C and C may instruct an action and a use by the representative recovery consumer for the recovery log tagged with that distinct tag. For example C e.g. encoding instruction REPLAY may instruct the recovery consumer e.g. RAID consumer to replay the recovery log

Illustratively the tag group having tags C C and C may indicate that recovery consumers and process i.e. use recovery log according to the action s instructed by tags C C and C. Specifically recovery consumer e.g. consumer ID may tag recovery log with distinct tag C according the framework tagging technique i.e. via the tag callback and responsive invocation of the framework API tagging function described above and recovery consumer e.g. consumer ID may tag recovery log with distinct tag C and so on. The is tagged callback may be used to determine whether the recovery log has been tagged. Notably a recovery consumer e.g. consumer ID may not tag any log i.e. fails to tag any recovery log and thus may be ignored in recovery.

Illustratively the framework graph may include a root vertex and one or more directed edges to one or more vertices . The graph may be constructed i.e. vertex and edge added in accordance with the registered dependency. For example the dependency file system consumer depends on RAID consumer yields a framework graph wherein consumer ID vertex depends on directed edge consumer ID vertex and a path from the vertex to the vertex via edge denotes that the recovery actions of consumer are executed after i.e. wait on the recovery actions of consumer ID . Notably the framework graph may be implemented such that independent paths in the graph represent recovery consumers which may execute their recovery actions in parallel. In other words two or more vertices e.g. and of the framework graph having adjacent directed edges e.g. and may have no dependency and thus may be executed concurrently. For example consumer vertex with directed edge is adjacent to consumer ID e.g. vertex with directed edge and thus consumer e.g. RAID consumer may execute concurrently with consumer e.g. NVSAVE consumer .

In an implementation circular dependencies may be avoided by checking for circular paths within the graph as the graph is constructed i.e. directed edges are added to the graph . Further the framework graph may be pruned by removing vertices i.e. recovery consumers that do not tag any log e.g. consumer ID using e.g. the is tagged callback.

Illustratively the framework may traverse i.e. walk the framework graph and for each vertex encountered notify the associated recovery consumer to initiate its respective recovery actions. Specifically the framework may traverse the graph breadth first so that recovery consumers without associated dependencies may be notified in parallel and thus may execute their actions concurrently. For example the breadth first path of the framework graph may be traversed via a consumer ID sequence and . Consumers and share no dependency and thus may be notified in parallel i.e. executed concurrently . Similarly consumer IDs and as well as consumer IDs and may execute in parallel. However consumer IDs and share a dependency e.g. directed edge and accordingly consumer ID must wait for consumer consumer to complete because directed edge indicates that consumer ID depends on consumer ID .

While there have been shown and described illustrative implementations of a recovery consumer framework for execution of recovery actions by one or more recovery consumers to enable efficient recovery of information in a storage system after a failure event it is to be understood that various other adaptations and modifications may be made within the spirit and scope of the embodiments herein. For example implementations have been shown and described herein with relation to parallel or concurrent execution of recovery actions to reduce recovery time i.e. duration for the storage system. However use of the terms parallel concurrent and or parallelized in their broader sense are not limited and may in fact may allow for execution of the described procedures and techniques on i a uniprocessor that carries out simultaneous multiple threads of execution of processes ii parallel processors or combinations thereof.

The foregoing description has been directed to specific aspects of the disclosure. It will be apparent however that other variations and modifications may be made to the described implementations with the attainment of some or all of their advantages. For instance it is expressly contemplated that the components and or elements described herein can be implemented as software encoded on a tangible non transitory computer readable medium e.g. disks and or CDs having program instructions executing on a computer hardware firmware or a combination thereof. Accordingly this description is to be taken only by way of example and not to otherwise limit the scope of the aspects of the disclosure. Therefore it is the object of the appended claims to cover all such variations and modifications as come within the true spirit and scope of the subject matter.

