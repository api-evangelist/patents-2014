---

title: Scalable multi-thread video decoding
abstract: Decoding tasks are identified for decoding encoded video. Decoding tasks may include entropy decoding tasks, motion compensation tasks, inverse frequency transform tasks, inverse quantization tasks, intra decoding tasks, loop filtering tasks, or other tasks. Task dependencies are identified for the video decoding tasks. For example, one or more decoding tasks may depend on prior completion of entropy decoding tasks. The decoding tasks are prioritized based at least in part on the task dependencies. For example, a higher priority may be assigned to tasks that must be completed before other tasks that depend on them can begin. Prioritized decoding tasks are selected to be performed by hardware threads. For example, a first hardware thread may perform a first decoding task that does not depend on any uncompleted tasks while a second hardware thread performs a second decoding task that does not depend on any uncompleted tasks.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09161034&OS=09161034&RS=09161034
owner: Microsoft Technology Licensing, LLC
number: 09161034
owner_city: Redmond
owner_country: US
publication_date: 20140430
---
This application is a continuation of U.S. patent application Ser. No. 13 848 631 filed Mar. 21 2013 which is a continuation of U.S. patent application Ser. No. 11 703 613 filed Feb. 6 2007 the disclosure of which is hereby incorporated by reference.

Companies and consumers increasingly depend on computers to process distribute and play back high quality video content. Engineers use compression also called source coding or source encoding to reduce the bit rate of digital video. Compression decreases the cost of storing and transmitting video information by converting the information into a lower bit rate form. Decompression also called decoding reconstructs a version of the original information from the compressed form. A codec is an encoder decoder system.

Compression can be lossless in which the quality of the video does not suffer but decreases in bit rate are limited by the inherent amount of variability sometimes called source entropy of the input video data. Or compression can be lossy in which the quality of the video suffers and the lost quality cannot be completely recovered but achievable decreases in bit rate are more dramatic. Lossy compression is often used in conjunction with lossless compression lossy compression establishes an approximation of information and the lossless compression is applied to a representation of the approximation.

In general video compression techniques include intra picture sometimes called intra frame or simply intra compression and inter picture sometimes called inter frame or simply inter compression. Intra picture compression techniques compress a picture with reference to information within the picture and inter picture compression techniques compress a picture with reference to a preceding and or following picture or pictures often called reference or anchor pictures .

For intra picture compression for example an encoder splits a picture into 8 8 blocks of samples where a sample is a number that represents the intensity of brightness or the intensity of a color component for a small elementary region of the picture and the samples of the picture are organized as arrays or planes. The encoder applies a frequency transform to individual blocks. The frequency transform converts an 8 8 block of samples into an 8 8 block of transform coefficients. The encoder quantizes the transform coefficients which may result in lossy compression. For lossless compression the encoder entropy codes the quantized transform coefficients.

Inter picture compression techniques often use motion estimation and motion compensation to reduce bit rate by exploiting temporal redundancy in a video sequence. Motion estimation is a process for estimating motion between pictures. For example for an 8 8 block of samples or other unit of the current picture the encoder attempts to find a match of the same size in a search area in another picture the reference picture. Within the search area the encoder compares the current unit to various candidates in order to find a candidate that is a good match. When the encoder finds an exact or close enough match the encoder parameterizes the change in position between the current and candidate units as motion data such as a motion vector MV . In general motion compensation is a process of reconstructing pictures from reference picture s using motion data.

The example encoder also computes the sample by sample difference between the original current unit and its motion compensated prediction to determine a residual also called a prediction residual or error signal . The encoder then applies a frequency transform to the residual resulting in transform coefficients. The encoder quantizes the transform coefficients and entropy codes the quantized transform coefficients.

If an intra compressed picture or motion predicted picture is used as a reference picture for subsequent motion compensation the encoder reconstructs the picture. A decoder also reconstructs pictures during decoding and it uses some of the reconstructed pictures as reference pictures in motion compensation. For example for an 8 8 block of samples of an intra compressed picture an example decoder reconstructs a block of quantized transform coefficients. The example decoder and encoder perform inverse quantization and an inverse frequency transform to produce a reconstructed version of the original 8 8 block of samples.

As another example the example decoder or encoder reconstructs an 8 8 block from a prediction residual for the block. The decoder decodes entropy coded information representing the prediction residual. The decoder encoder inverse quantizes and inverse frequency transforms the data resulting in a reconstructed residual. In a separate motion compensation path the decoder encoder computes an 8 8 predicted block using motion vector information for displacement from a reference picture. The decoder encoder then combines the predicted block with the reconstructed residual to form the reconstructed 8 8 block.

Quantization and other lossy processing can result in visible lines at boundaries between blocks. This might occur for example if adjacent blocks in a smoothly changing region of a picture such as a sky area in an outdoor scene are quantized to different average levels. Blocking artifacts can be especially troublesome in reference pictures that are used for motion estimation and compensation. To reduce blocking artifacts the example encoder and decoder use deblock filtering to smooth boundary discontinuities between blocks in reference pictures. The filtering is in loop in that it occurs inside a motion compensation loop the encoder and decoder perform it on reference pictures used for subsequent encoding decoding. Deblock filtering improves the quality of motion estimation compensation resulting in better motion compensated prediction and lower bitrate for prediction residuals. In loop deblocking filtering is often referred to as loop filtering. 

In some cases the example encoder and example decoder process video frames organized as shown in A B and C. For progressive video lines of a video frame contain samples starting from one time instant and continuing through successive lines to the bottom of the frame. An interlaced video frame consists of two scans one for the even lines of the frame the top field and the other for the odd lines of the frame the bottom field .

A progressive video frame can be divided into 16 16 macroblocks such as the macroblock shown in . The macroblock includes four 8 8 blocks Y0 through Y3 of luma or brightness samples and two 8 8 blocks Cb Cr of chroma or color component samples which are co located with the four luma blocks but half resolution horizontally and vertically.

Therefore interlaced video frames can be rearranged according to a field structure with the odd lines grouped together in one field and the even lines grouped together in another field. This arrangement known as field coding is useful in high motion pictures. shows the interlaced video frame of organized for encoding decoding as fields . Each of the two fields of the interlaced video frame is partitioned into macroblocks. The top field is partitioned into macroblocks such as the macroblock and the bottom field is partitioned into macroblocks such as the macroblock . The macroblocks can use a format as shown in and the organization and placement of luma blocks and chroma blocks within the macroblocks are not shown. In the luma plane the macroblock includes 16 lines from the top field the macroblock includes 16 lines from the bottom field and each line is 16 samples long.

On the other hand in stationary regions image detail in the interlaced video frame may be more efficiently preserved without rearrangement into separate fields. Accordingly frame coding is often used in stationary or low motion interlaced video frames. shows the interlaced video frame of organized for encoding decoding as a frame . The interlaced video frame has been partitioned into macroblocks such as the macroblocks and which use a format as shown in . In the luma plane each macroblock includes 8 lines from the top field alternating with 8 lines from the bottom field for 16 lines total and each line is 16 samples long. The actual organization and placement of luma blocks and chroma blocks within the macroblocks are not shown and in fact may vary for different encoding decisions. Within a given macroblock the top field information and bottom field information may be coded jointly or separately at any of various phases the macroblock itself may be field coded or frame coded.

While some video decoding and encoding operations are relatively simple others are computationally complex. For example inverse frequency transforms fractional sample interpolation operations for motion compensation in loop deblock filtering post processing filtering color conversion and video re sizing can require extensive computation. This computational complexity can be problematic in various scenarios such as decoding of high quality high bit rate video e.g. compressed high definition video .

Some decoders use video acceleration to offload selected computationally intensive operations to a graphics processor. For example in some configurations a computer system includes a primary central processing unit CPU as well as a graphics processing unit GPU or other hardware specially adapted for graphics processing. A decoder uses the primary CPU as a host to control overall decoding and uses the GPU to perform simple operations that collectively require extensive computation accomplishing video acceleration.

The acceleration interface is exposed to the decoder as an application programming interface API . The device driver associated with the video accelerator is exposed through a device driver interface DDI . In an example interaction the decoder fills a buffer with instructions and information then calls a method of an interface to alert the device driver through the operating system. The buffered instructions and information opaque to the operating system are passed to the device driver by reference and video information is transferred to GPU memory if appropriate. While a particular implementation of the API and DDI may be tailored to a particular operating system or platform in some cases the API and or DDI can be implemented for multiple different operating systems or platforms.

In some cases the data structures and protocol used to parameterize acceleration information are conceptually separate from the mechanisms used to convey the information. In order to impose consistency in the format organization and timing of the information passed between the decoder and device driver an interface specification can define a protocol for instructions and information for decoding according to a particular video decoding standard or product. The decoder follows specified conventions when putting instructions and information in a buffer. The device driver retrieves the buffered instructions and information according to the specified conventions and performs decoding appropriate to the standard or product. An interface specification for a specific standard or product is adapted to the particular bit stream syntax and semantics of the standard product.

Although some prior designs have proposed mapping particular decoding operations to different processing units such as by mapping particular decoding operations to GPUs prior designs are limited in terms of flexibility and efficiency. For example a design that statically determines which processing units will perform particular decoding operations is susceptible to long periods of inactivity when processing units are forced to wait for their assigned operations to begin.

This Summary introduces a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used to limit the scope of the claimed subject matter.

In summary the Detailed Description is directed to various techniques and tools for multi threading for video coding and decoding.

In one aspect encoded video information is received at a video decoder on a computer system comprising multiple hardware threads. For example a computer system has multiple processing units and each of the multiple processing units has multiple hardware threads. Decoding tasks are identified for decoding the encoded video. For example decoding tasks include entropy decoding tasks motion compensation tasks inverse frequency transform tasks inverse quantization tasks intra decoding tasks loop filtering tasks and or other tasks. Task dependencies are identified for at least one of the video decoding tasks. For example one or more decoding tasks depend on prior completion of entropy decoding tasks. The decoding tasks are prioritized based at least in part on the task dependencies. For example a higher priority is assigned to tasks that must be completed before other tasks that depend on them can begin. The prioritized decoding tasks are selected to be performed by the hardware threads. For example a first hardware thread performs a first decoding task that does not depend on any uncompleted tasks while a second hardware thread performs a second decoding task that does not depend on any uncompleted tasks.

Additional features and advantages will be made apparent from the following detailed description of various embodiments that proceeds with reference to the accompanying drawings.

Various alternatives to the implementations described herein are possible. For example certain techniques described with reference to flowchart diagrams can be altered by changing the ordering of stages shown in the flowcharts by repeating or omitting certain stages etc. while achieving the same result. As another example although some implementations are described with reference to specific macroblock formats other formats also can be used. As another example described video decoding techniques can be applied to decoding of other kinds of encoded information where the decoding can be divided into interdependent decoding tasks such as audio information. Different embodiments implement one or more of the described techniques and tools. Some of the techniques and tools described herein address one or more of the problems noted in the Background. Typically a given technique tool does not solve all such problems however.

With reference to the computing environment includes at least two processing units and associated memory . The processing units may include a GPU or other co processing unit for video acceleration. In this most basic configuration is included within a dashed line. The processing unit executes computer executable instructions and may be a real or a virtual processor. In a multi processing system multiple processing units execute computer executable instructions to increase processing power. A host encoder or decoder process uses available processing units to perform decoding operations. Certain operations e.g. in loop deblock filtering may be performed by a specialized processing unit such as a GPU. The memory may be volatile memory e.g. registers cache RAM non volatile memory e.g. ROM EEPROM flash memory etc. or some combination of the two. The memory may be specific to one processor or shared by two or more processors. The memory stores software for an encoder and or decoder implementing multi threaded video decoding.

A computing environment may have additional features. For example the computing environment includes storage one or more input devices one or more output devices and one or more communication connections . An interconnection mechanism not shown such as a bus controller or network interconnects the components of the computing environment . Typically operating system software not shown provides an operating environment for other software executing in the computing environment and coordinates activities of the components of the computing environment .

The storage may be removable or non removable and includes magnetic disks magnetic tapes or cassettes CD ROMs DVDs or any other medium which can be used to store information and which can be accessed within the computing environment . The storage stores instructions for the software .

The input device s may be a touch input device such as a keyboard mouse pen touch screen or trackball a voice input device a scanning device or another device that provides input to the computing environment . For audio or video encoding the input device s may be a sound card video card TV tuner card or similar device that accepts audio or video input in analog or digital form or a DVD CD ROM or CD RW that reads audio or video samples into the computing environment . The output device s may be a display printer speaker CD or DVD writer or another device that provides output from the computing environment .

The communication connection s enable communication over a communication medium to another computing entity. The communication medium conveys information such as computer executable instructions audio or video input or output or other data in a modulated data signal. A modulated data signal is a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal. By way of example and not limitation communication media include wired or wireless techniques implemented with an electrical optical RF infrared acoustic or other carrier.

The techniques and tools can be described in the general context of computer readable media. Computer readable media are any available media that can be accessed within a computing environment. By way of example and not limitation with the computing environment computer readable media include memory storage communication media and combinations of any of the above.

The techniques and tools can be described in the general context of computer executable instructions such as those included in program modules being executed in a computing environment on a target real or virtual processor. Generally program modules include routines programs libraries objects classes components data structures etc. that perform particular tasks or implement particular abstract data types. The functionality of the program modules may be combined or split between program modules as desired in various embodiments. Computer executable instructions for program modules may be executed within a local or distributed computing environment.

For the sake of presentation the detailed description uses terms like check and select to describe computer operations in a computing environment. These terms are high level abstractions for operations performed by a computer and should not be confused with acts performed by a human being. The actual computer operations corresponding to these terms vary depending on implementation.

The relationships shown between modules within the decoder indicate general flows of information in the decoder other relationships are not shown for the sake of simplicity. In particular some operations of modules of the decoder depend on other operations being completed first. Such dependencies can vary depending on implementation and are described in more detail below.

A video accelerator can be used to perform operations such as inverse frequency transforms fractional sample interpolation motion compensation in loop deblocking filtering color conversion post processing filtering and or picture re sizing. For example the decoder passes instructions and information to the video accelerator as described in Microsoft DirectX VA Video Acceleration API DDI version 1.01. Alternatively the decoder passes instructions and information to the video accelerator using another mechanism such as one described in a later version of DXVA or another acceleration interface. In some implementations different video acceleration profiles result in different operations being performed by different hardware threads.

Returning to the decoder processes video pictures which may be video frames video fields or combinations of frames and fields. The bitstream syntax and semantics at the picture and macroblock levels may depend on whether frames or fields are used. The decoder is block based and uses a 4 2 0 macroblock format for frames. For fields the same or a different macroblock organization and format may be used. 8 8 blocks may be further sub divided at different stages. Alternatively the decoder uses a different macroblock or block format or performs operations on sets of samples of different size or configuration.

The decoder receives information for a compressed sequence of video pictures and produces output including a reconstructed picture e.g. progressive video frame interlaced video frame or field of an interlaced video frame . The decoder system decompresses predicted pictures and key pictures. For the sake of presentation shows a path for key pictures through the decoder system and a path for predicted pictures. Many of the components of the decoder system are used for decompressing both key pictures and predicted pictures. The exact operations performed by those components can vary depending on the type of information being decompressed.

A demultiplexer and buffer receives the information for the compressed video sequence and makes the received information available to the entropy decoder . The entropy decoder entropy decodes entropy coded quantized data as well as entropy coded side information typically applying the inverse of entropy encoding performed in the encoder. A motion compensator applies motion information to one or more reference pictures to form motion compensated predictions of subblocks blocks and or macroblocks of the picture being reconstructed. One or more picture stores store previously reconstructed pictures for use as reference pictures.

The decoder also reconstructs prediction residuals. An inverse quantizer inverse quantizes entropy decoded data. An inverse frequency transformer converts the quantized frequency domain data into spatial domain video information. For example the inverse frequency transformer applies an inverse block transform to subblocks and or blocks of the frequency transform coefficients producing sample data or prediction residual data for key pictures or predicted pictures respectively. The inverse frequency transformer may apply an 8 8 8 4 4 8 4 4 or other size inverse frequency transform.

For a predicted picture the decoder combines reconstructed prediction residuals with motion compensated predictions to form the reconstructed picture . A motion compensation loop in the video decoder includes an adaptive deblocking filter . The decoder applies in loop filtering to the reconstructed picture to adaptively smooth discontinuities across block subblock boundary rows and or columns in the picture. The decoder stores the reconstructed picture in a picture buffer for use as a possible reference picture. For example the decoder performs in loop deblock filtering operations.

Depending on implementation and the type of compression desired modules of the decoder can be added omitted split into multiple modules combined with other modules and or replaced with like modules. In alternative embodiments encoders or decoders with different modules and or other configurations of modules perform one or more of the described techniques. Specific embodiments of video decoders typically use a variation or supplemented version of the generalized decoder .

The number of processing cores available to computing systems grows nearly every year. For example Intel Corporation has announced plans for a 32 core processor with 128 hardware threads in the next 5 years. At the same time decoding of high quality video such as high definition HD video demands increasing amounts of processing power. To take advantage of the increasing number of available hardware threads scalable multi thread video decoding techniques and tools are described.

Various described techniques and tools facilitate multi thread decoding of video by breaking decoding into tasks that can be performed with different threads. Described techniques and tools allow complex video decoding to be performed flexibly and efficiently with the ability to scale up to multi thread processing or down to single thread processing depending on the type of hardware being used and or the type of decoding being performed. For example when decoding video that has been encoded according to a particular video codec standard described multi thread video decoding tools can be adjusted to allow more or fewer hardware threads to perform decoding tasks to meet hardware constraints usage constraints or other criteria.

A decoding task for a current picture may depend on one or more other decoding tasks for the current picture and or one or more other pictures. Preliminary analysis of task dependencies is performed and the dependencies are updated during decoding to allow accurate determination of which tasks are currently runnable. A task is considered to be runnable for example if its completion does not depend on any other uncompleted task. Different kinds of encoded video may have different combinations of task dependencies and some of the described multi thread video decoding tools can be used to identify different kinds of task dependencies and prioritize tasks accordingly. For example when decoding video that has been encoded according to a particular video codec standard some of the described multi thread video decoding tools can determine task dependencies and task priorities particular to that standard.

Described techniques and tools allow decoding tasks to be performed in an efficient way while reducing wait time for processing units. For example available threads perform tasks based on the relative priority of these tasks as reflected in data structures such as priority queues.

Available hardware threads perform runnable tasks. The available threads generally are not limited to performing particular kinds of tasks i.e. particular decoding operations or to performing tasks for particular pictures. In some cases however specialized processors such as GPUs are limited in the kinds of tasks they can perform.

Some example techniques and tools are described with reference to a currently available multi core system the Xbox 360 game console available from Microsoft Corporation which has three cores running at 3.2 GHz with two hardware threads on each core and one graphics processor running at 500 MHz. However it should be understood that the examples described herein are adaptable to other platforms scenarios with some modifications. For example described techniques and tools have achieved near linear scalability with up to 8 cores for HD video content.

In general the term thread as used herein can mean a hardware thread HT bound or otherwise associated with a particular hardware core e.g. for a specific hardware configuration such as the Xbox 360 or a generic software thread.

Alternatively decoding processes can be divided into more or fewer stages and more or fewer buffers can be used. For example a separate inverse quantization stage may be included along with the stages shown in . Or various stages may be combined into a smaller number of stages. For some picture types and or decoder configurations some of the stages shown in are not present. For example for a progressive I frame the motion compensation stage is not present.

Sometimes a set of operations on a picture or portion of a picture can be performed by a single thread as a task. A decoding process on a single picture comprises a number of tasks. Tasks are considered to be non overlapping when they belong to different stages or involve different portions of the picture.

Depending on the nature of the decoding stage and the encoding of the picture decoding stages can be serial or parallel. For pictures with slices which are considered to be independently decodable for entropy decoding entropy decoding is a parallel process because more than one instance of coefficient decoding can be performed at the same time on different individual slices of a single picture. For pictures without slices entropy decoding is essentially a serial process because it is complicated or even impossible to perform more than one instance of coefficient decoding at the same time on a single picture. For other stages however it is easier to segment the picture with or without slices such that more than one instance of the corresponding decoding operation can be performed at a time on different segments of the picture as long as the segments are non overlapping. Such stages include motion compensation and inverse frequency transform . Segmentation also can be used to split up serial processes into smaller tasks but the segments are processed from top to bottom one at a time. Intra decoding is typically a serial process whereas loop filtering could be a parallel process in some cases.

The ordering and relationships shown in can be varied depending on implementation. For example it is possible to move the addition of residual error from the inverse frequency transform IDCT stage to the motion compensation stage and perform IDCT ahead of motion compensation. In one implementation this reordering requires a bigger buffer for the IDCT results twice the size as we move from calculations involving pixelc one byte to calculations involving pixelI two bytes . However this may be useful to consider for a segment based multi threading scheme which is described in further detail below.

In some embodiments a decoding process for an individual picture can take place using a data structure called a working frame holder WFH . Generally speaking a WFH is a place in memory where picture decoding develops throughout its several decoding stages. A non working frame holder NWFH is a WFH in its unused state without a current picture buffer. Though referred to herein as frame holders it should be understood that fields of frames also can be held and or processed in frame holders.

A NWFH becomes a WFH when picture decoding begins. A current picture buffer is attached to the WFH and after the picture decoding is completed the current picture buffer is detached converting the WFH back to a NWFH again. In a single threading scenario only one picture is decoded at a time and so only one frame holder is present. However additional frame holders play an important role in multi thread design in these embodiments.

The state of the WFH is updated based on decoding results. For example before the decoding of a current picture the current state of the WFH is updated based on decoding results of previous picture s to set up reference pictures for the decoding of the current picture. In turn the current picture may be a reference picture for subsequent inter coded pictures.

The number of frame holders used can vary depending on implementation. For example in a system having plural hardware threads it is preferable to have at least one frame holder for each hardware thread to allow each thread to perform decoding operations on a different picture simultaneously. However the utility of additional frame holders is balanced with the use of memory resources by the additional frame holders.

In one implementation a WFH is an instance of a main decoder data structure consisting of I O buffers described above with respect to as well as other member fields used by the decoding process e.g. a pWMVDec structure .

The data flow shown in the suggests dependency relationships among the decoding stages in the sense that some stages cannot proceed until their respective input data are fully available. Dependency relationships are important properties of tasks.

A task that cannot start to run until another task has been completed is dependent on the other task. shows a graphic representation of a task having dependency relationships with other tasks not shown . The incoming arrows represent other tasks that depend on the task and the outgoing arrows represent other tasks that the task depends on. A task that does not depend on any other tasks is ready to run or runnable. 

With this representation of a task the decoding process diagram of can be converted into a decoding task dependency diagram shown in for single thread decoding. Task T0 entropy decoding is an independent task the decoding of the bit stream is not dependent on any other tasks. Therefore task T0 has no outgoing arrows. Task T1 motion compensation depends on the decoding of motion vectors in task T0. In reality motion compensation also depends on the availability of reference picture s . However in single thread processing the fact that the current picture is being decoded means that any reference pictures have already been decoded. Therefore in this representation which shows tasks for single thread decoding Task T1 has just one outgoing arrow to task T0.

Task T2 depends on task T1 for the part of the task that adds residual error to motion compensated predictions from task T1. Task T2 also depends on task T0 for the decoded coefficients for residuals but because task T1 already depends on task T0 only one outgoing arrow is shown for task T2.

Task T3 intra decoding depends only on task T0 for decoded intra coefficients. Task T3 includes its own inverse frequency transform IDCT calculations and so does not depend on task T2. Task T4 loop filtering depends on the completion of both inter decoding and intra decoding tasks. Therefore task T4 has two outgoing arrows to task T2 the last inter decoding task for the picture and task T3.

In this way a single thread decode process can be represented as a number of non overlapping tasks that operate using a WFH which is updated before and after decoding of the picture. Task dependencies within individual pictures and between pictures for multi threading and for additional decoding operations such as color conversion are described in detail below.

A goal of multi threaded task based decoding is for different threads to perform simultaneous decoding using WFHs for different pictures. When properly synchronized the tasks using different WFHs are executed in order and the task dependencies are resolved just as if they were part of a single thread decoding process.

In one implementation the frame holders described above are replicated for multi threaded task based decoding. Specifically a total of N frame holders are generated each of which becomes a WFH for pictures with the same picture index. For example for progressive frames the picture index frm idx can be represented as where m t is the current frame count in decoding order.

Post processing and color conversion processes generally take decoded pictures as input and output the post processed and or color converted picture to the output picture pool . In one implementation this functionality is provided by a call to the function DecodeDataMT which is described in further detail below or by an internal output picture pool and later copying to an external output buffer.

1. Inter Picture Task Dependency for Progressive Frames or Interlaced Video Frames Organized for Decoding as Frames

Each frame has an associated WFH and five tasks except for the I frame which has three tasks. The P frame tasks and B frame tasks have both intra frame dependencies e.g. for intra coded macroblocks and inter frame dependencies. The notation Ti j indicates the jth task of the ith frame. The inter frame dependencies are for motion compensation tasks Ti1 that depend on the previous frames. Although the motion compensation tasks are shown as being dependent on the completion of the loop filtering task Ti4 of the reference frames the motion compensation tasks may instead depend on completion of some other task in the reference frame e.g. if loop filtering is omitted .

Tasks for three frames organized for decoding as fields in one implementation are shown in . The frames are labeled as an I P frame P P frame and BB frame respectively and are shown in coded order. Although the P P frame precedes the B B frame in coded order and decoding order the P P frame follows the B B frame in display order. For Ti j indicates the jth task for the top field of frame I and Ti  j indicates the jth task for the bottom field in the ith frame.

In the example shown in the following rules apply for finding inter field task dependencies. In a P P frame the motion compensation task for each P field labeled as T1 for the top field and T1 for the bottom field depends on one or two previously decoded fields and the number of reference fields is indicated in the bitstream. In a B B frame each B field has four inter field dependencies. The top B field T2 refers to the first and the second fields from the previous and next anchor frames in display order . On the other hand the bottom B field T2 refers to the top B field in the same frame the second bottom field of the previous anchor frame and the first and second field of the subsequent anchor frame in display order .

In one implementation the task dependency rules for a P field or B field are independent of the type of frame they are in. For example for a P field its dependency rules are the same whether it is in a P P frame or I P frame. Other possible combinations of fields in a single frame include I I P I P P B BI BI B and BI BI where a BI field is an intra coded B field .

Alternatively the rules for interlace field task dependency can be simplified. One purpose of task dependency simplification is to reduce the total number of task dependency links in the graph. One simplification is to serialize the two fields in the same frame e.g. by making the motion compensation stage of the bottom field always depend on the loop filtering stage of the top field . In terms of task dependency by looking above the field level to the frame level the two fields are treated as a whole frame instead of two individual fields and the inter frame dependency chain becomes similar to progressive mode.

For example within the same frame for purposes of simplification the motion compensation task T1 1 for the bottom field T1 of the P P frame can depend only on the loop filtering task T14 of the top field T1 . Between frames the motion compensation task T11 for the top field T1 of the P P frame can depend only on the loop filtering task T0 4 of the bottom field T0 of its previous anchor frame for P fields . Similarly the motion compensation task T21 of the top field T2 of the B B frame can depend only on the loop filtering tasks T0 4 T1 4 of the previous and next anchor frames. This way the dependencies for the interlace field tasks will be much like the dependencies for progressive frame tasks.

As long as the frame level dependency relationship is intact there are many ways to simplify the intra frame dependency. However as dependencies between fields become simpler less task parallelism may increase thread wait times.

A task synchronization is an operation performed at completion time for a current task. The current task is examined to identify all the other tasks depending on it and the dependencies on the current task are removed for those other tasks. The task synchronization is typically followed by task scheduling for those tasks that used to depend on the current task but are no longer dependent on any tasks and can run immediately. These newly runnable tasks are put into a priority queue for future execution via a proper scheduling order as described below.

In some implementations a priority queue which can also be referred to as a ready queue an accelerator queue or for GPU runnable tasks a GPU queue is a queue or set of queues to which runnable tasks are added. Available threads check the ready queue for runnable tasks and select a task to run. shows a generalized technique for using a priority queue for runnable tasks in one implementation. In a system comprising plural hardware threads suitable for performing decoding tasks a thread checks the priority queue for a runnable task. If a runnable task is available the thread selects and runs a runnable task from the priority queue. After performing the check of the priority queue and running a runnable task if one is available if decoding is done the process ends. Otherwise the thread checks the priority queue again for runnable tasks.

More than one priority queue can be used at a time. For example a priority queue for available CPU threads may be used in combination with a GPU priority queue for GPU runnable tasks in a system comprising one or more GPUs.

The priority queue may be prioritized based only on when the task became runnable a simple first in first out queue . More preferably however tasks are prioritized by some measure of their importance to the overall decoding process as described below.

The design of a scheduling order also called priority order for runnable tasks is an important factor in the performance of the multi thread decoder. Scheduling order affects how soon a particular task can run relative to the other tasks in the priority queue. In one implementation tasks with higher priority are added closer to the front of the queue than tasks with lower priority which are added closer to the back of the queue . One example of a high priority task is an entropy decoding task because many other decoding tasks will typically depend on it. Another example of a high priority task is a top field task because bottom field tasks for the bottom field of a frame often depend on completion of top field tasks in the same frame.

Different priorities and therefore different orderings are possible based on design criteria and can result in different best average worst performance in terms of throughput and latency. A combination of theoretical analysis heuristics and empirical experimentation can be used to determine a scheduling order suitable to achieve desirable results.

In some implementations the scheduling scheme takes into account whether threads are treated as software threads or hardware threads HTs bound to a particular hardware core. Scheduling design can incorporate mechanisms such as HT binding to the same core for better instruction cache or ICache pressure or other benefits. When multiple cores and or HTs share the same instruction cache it is preferable to run the same tasks on these cores so that their code is shared on the same instruction cache lines. This is especially beneficial when the instruction cache is small. Therefore a scheme to intelligently bind those tasks could be beneficial.

One of the simpler priority ordering schemes is to set inter picture priority for tasks based on a current picture count and to set intra picture priority based on a task index. The example priority order shown in is in a raster scan pattern left to right within a row top row to bottom row for the example shown in . In the notation Ti j is used to represent the task j in the ith frame. Tasks for additional pictures not shown in could be represented in the ordering as well.

The priority ordering is straightforward for frame JO tasks T00 T03 and T04 since there are no inter frame dependencies for these tasks. However some tasks for frame P1 depend on tasks for frame I0 so priorities for frame P1 tasks are less clear. For example task T11 depends on both T10 and T04. A simple choice is to give the tasks on which T11 depends equal priority. Alternatively the average completion time for the task T10 and T04 can be taken into account giving the task with longer average completion time higher priority in order to start the task sooner and possibly reduce the overall wait time for T11.

One shortcoming of the raster scan order scheduling model described above is the possibility of priority inversion. For example if coefficient decoding takes a large amount of time in the example frames shown in the tasks Ti0 take a long time to complete while other tasks Ti j where j 0 wait on their dependency resolutions.

Suppose when decoding begins the tasks in a priority queue are T00 T10 T20 T30 and T40. Referring again to tasks like T20 and T30 have lower priority than other tasks for frames P1 and I0 but these lower priority tasks appear in the priority queue at the very beginning of decoding because other higher priority tasks e.g. T03 cannot run until tasks on which they depend e.g. T00 are completed. When T00 is completed a decoder will often be better served to immediately run T03 in favor of running task T30. But if a thread has already started running task T30 a priority inversion occurs a lower priority task here T30 occupies a thread while a higher priority task here T03 waits for execution. One way to reduce priority inversion is by segmenting tasks.

The tasks described so far have involved performing a decoding stage e.g. entropy decoding motion compensation etc. for an entire picture. However in some cases there are advantages to defining a task as being for only a portion of a picture. This idea is referred to herein as segmenting tasks or task segmentation.

For example since motion compensation and inverse frequency transforms are inherently parallel processes it is possible to divide a picture into M equal segments and have M segmented tasks e.g. T11 k where the index k where 0 k

Taking the decoding of frame P1 from as an example is a diagram showing dependencies for the segmented tasks for frame P1. As shown in each stage has been divided into 4 segmented tasks. Each segmented task is now conceptually only a quarter of a task that involved the entire frame in although in reality different segments may be for arbitrarily sized portions and take more or less than a quarter of the processing time for that stage . The segmented tasks for stages that depended on the entropy decoding stage are now dependent on the corresponding segmented entropy decoding tasks. This can reduce latency particularly when segmented tasks can be performed in parallel as in later tasks .

In some embodiments without segmentation when decoding begins the tasks in the ready queue are entropy decoding tasks which do not depend on completion of any other decoding stage. Referring again to OA tasks T00 T10 T20 T30 and T40 begin in the priority queue. Tasks T20 and T30 have lower priority than tasks such as T11 but the lower priority tasks T20 and T30 appear in the priority queue at the very beginning of decoding because the other higher priority tasks e.g. T11 cannot run until tasks on which they depend e.g. T10 are completed.

With task segmentation entropy coding tasks in the priority queue can now be assigned the example priority order shown in . In the priority order the entropy decoding tasks for frame index 0 T00 k are higher priority than entropy decoding tasks for frame index 1 T10 k the entropy decoding tasks for frame index 1 are higher priority than the entropy decoding tasks for frame index 2 T20 k and so on. The available threads now have a better chance of working on more important tasks e.g. T03 k a quicker completion time for each segmented task and a shorter wait time for other segmented tasks to become runnable. If a lower priority segmented task like T30 k is being executed by a thread its completion time is much shorter than completing entropy decoding T30 for an entire frame and the thread that completes T30 k can check for runnable higher priority tasks such as T03 or T11 more quickly reducing priority inversion effects.

Task segmentation also provides flexibility for scheduling from a hardware threading point of view. For example task segmentation provides the opportunity to keep hardware threads on high priority tasks for groups of frames comprising lots of potential anchor frames e.g. I and P frames and increases the opportunity to pair tasks to hardware threads which share resources e.g. memory or to keep tasks apart if they will cause resource contention or overload.

In some cases true parallel processing of loop filtering tasks may not be possible. For example in the advanced profile of the VC 1 video codec standard the 4 4 transform process couples rows together making loop filtering a serial process.

One option is to make the loop filtering stage a two pass process in which the first pass is a parallel process and the second pass is a serial process. is based on and shows a two pass task arrangement for loop filtering. The first pass parallel process is shown as tasks T14 k and the second pass serial process is shown as tasks T15 k. In the segments for the first pass each depend on the corresponding segmented tasks T12 k and T13 k and the segments for the second pass each depend on the corresponding task in the first loop filtering pass Ti4 k and the previous segment of the second loop filtering pass. Tasks in the first loop filtering pass Ti4 k now can run in parallel and it might reduce latency if tasks Ti2 k also run in parallel. However the tasks Ti5 k still need to be scheduled serially. Extra instruction cache pressure is another factor to consider.

Picture decoding initialization takes place between the main decoding processes for individual pictures. For example during single thread decoding where loop filtering is the last stage in the main decoding process for a picture picture decoding initialization occurs after loop filtering and before the entropy decoding stage for the next frame starts. In one implementation picture decoding initialization includes the following operations.

In one implementation the output stage which includes color conversion and other post processing stages is decoupled from the decoding loop e.g. loop filtering and preceding stages because the decoder need not wait for the output of post processing to be completed to perform decoding tasks for other pictures and because the GetOutputMT call from the application is asynchronous to the decoding process. GetOutputMT is described in further detail below.

For priority ordering post processing tasks can generally be set as lower priority than other decoding tasks. But if their priorities are too low they may get stacked up and keep resources available for the free picture pool and the output picture pool low which eventually may cause an increase in overall latency. One option is to lower their priority to the level of the next picture or some other subsequent picture.

This section describes an example multi thread decoder design and application programming interface API including example code listings for various modules. Other API implementations can vary from the specific details described in this example in accordance with the principles described herein. The example API is similar to an API used for a single threaded decoding system.

DecodeDataMT is non blocking and returns a RETRY signal when there is no free frame holder available to take a new picture. The application calling the API is on a separate thread independent from the hardware threads HT0 HT1 HT2 HT3 performing the decoding tasks. In one implementation the API is on a thread associated with core 2 in a three core Xbox 360 system. The non blocking nature of DecodeDataMT and GetOutputMT allows the application to call DecodeDataMT and GetOutputMT frequently without affecting the main work of the decoding threads.

DecodeDataMT also takes a parameter pInputBuffer which is used for the compressed picture and a parameter pOutputBuffer which is used to fill the decompressed picture associated with this compressed picture but in display order asynchronously by the decoding process. When GetOutputMT is called it returns the decompressed frames in display order and if a frame is not ready for output it will return a RETRY.

Both DecodeDataMT and GetOutputMT keep their own counts of pictures internally. The application is not required to keep and supply these counts to the decoder.

In one implementation a streaming mode is not supported. For a high bit rate application such as a DVD application the lack of a streaming mode should not adversely affect performance.

A difference from single threaded decoding is that DecodeDataMT directly takes an input compressed buffer as opposed to using a callback mechanism. One reason for this implied by the example described with reference to is that entropy decoding processes could be running for all 4 threads and a callback would need to go through the application thread on a different core e.g. core 2 in a three core Xbox 360 system making an unnecessary cross core context switch Also maintaining multiple sessions of bitstream feeds at the same time on the application side could be a burden.

Alternatively the application passes in a NULL pointer as pOutputBuffer for this decodeDataMT call. The decoder keeps an internally pre allocated output picture pool from which it can retrieve a picture in place of the external picture. Then at GetOutputMT time for this picture the application passes in a valid external buffer pointer so that the decoder can memcpy the internal picture into the provided external picture buffer. The internal output picture then gets re cycled. Note it is only in this case that GetOutputMT should pass a valid pExternalOutputBuffer. The application may want to consider using memcpy and a valid external buffer pointer when its output buffer memory is limited it still wants the decoder to minimize its decoding latency and it can take an extra memcpy. The application can consider selecting this scheme on a picture by picture basis or some other basis. Also DecodeDataMT takes pInputBuffer as the compressed buffer and a streaming mode is not supported.

Alternatively the decoder uses other code to implement the DecodeDataMT and GetOutputMT functions or different API functions.

A light weight time stamping mechanism can be used to track tasks. This not only can help in tuning the scheduling algorithm and debugging with different content scenarios it could be an integral part of the scheduling if statistical timing information is used.

Having described and illustrated the principles of our invention with reference to various embodiments it will be recognized that the various embodiments can be modified in arrangement and detail without departing from such principles. It should be understood that the programs processes or methods described herein are not related or limited to any particular type of computing environment unless indicated otherwise. Various types of general purpose or specialized computing environments may be used with or perform operations in accordance with the teachings described herein. Elements of embodiments shown in software may be implemented in hardware and vice versa.

In view of the many possible embodiments to which the principles of the disclosed invention may be applied it should be recognized that the illustrated embodiments are only preferred examples of the invention and should not be taken as limiting the scope of the invention. Rather the scope of the invention is defined by the following claims. We therefore claim as our invention all that comes within the scope and spirit of these claims.

