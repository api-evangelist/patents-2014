---

title: Fault tolerance in a distributed file system
abstract: A method for providing fault tolerance in a distributed file system of a service provider may include launching at least one data storage node on at least a first virtual machine instance (VMI) running on one or more servers of the service provider and storing file data. At least one data management node may be launched on at least a second VMI running on the one or more servers of the service provider. The at least second VMI may be associated with a dedicated IP address and the at least one data management node may store metadata information associated with the file data in a network storage attached to the at least second VMI. Upon detecting a failure of the at least second VMI, the at least one data management node may be re-launched on at least a third VMI running on the one or more servers.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09612924&OS=09612924&RS=09612924
owner: Amazon Technologies, Inc.
number: 09612924
owner_city: Reno
owner_country: US
publication_date: 20140625
---
Cloud computing is the use of computing resources hardware and software that are available in a remote location and accessible over a network such as the Internet. In a computing environment with many computing devices such as a virtual server or cloud computing environment with many server computers the use of computing resources can provide a number of advantages including cost advantages and or the ability to adapt rapidly to changing computing resource needs.

With the increased use of cloud computing resources some cloud computing environments may be inefficient in managing resource allocation. Additionally multiple application installations and reboots may cause latencies contributing to the inefficient use of the cloud computing environment resources as well as difficulties in providing fault tolerance for data processing resources.

A virtual machine image contains an operating system e.g. Linux and other data needed to launch a virtual machine in a virtual environment. The virtual machine image is similar to a physical computer s disk volume and may include a file system the operating system and other components needed to boot up as a machine. In order to launch a virtual machine hardware needs to be selected. The hardware selection may be accomplished through instance types which may allow a variety of different sizes of memory CPU capacity I O performance and so forth. The combination of the virtual machine image and the instance type can be used to create an instance or a virtual machine which may be launched on a cloud computing resource such as a host server computer in a multi tenant network environment. As used herein the terms virtual machine and virtual machine instance are interchangeable.

As used herein the term fault tolerance refers to the ability of a computing environment e.g. a data processing network environment to continue operating properly e.g. according to one or more specifications in the event of failure of one or more components e.g. failure or unavailability of one or more network nodes in the computing environment .

As used herein the term distributed file system refers to a file system designed to hold a large amount of data while providing high throughput access to the information. The files of the distributed file system are stored in a redundant fashion across multiple nodes to ensure durability and easy access to data. In some instances a distributed file system may also be known as a network file system using a network protocol to access the file data. Additionally files may be divided into parts or blocks with multiple copies of each block being stored across multiple nodes. The file blocks may be accessed using the same interfaces and semantics as when accessing local files e.g. mounting unmounting listing directories read write at byte boundaries native permission models and so forth . Examples of distributed file systems may include the Google File System GFS the Apache Software Foundation s Hadoop Distributed File System HDFS Inktank s Ceph the Moose File System MooseFS Windows Distributed File System DFS Fraunhofer Parallel File System FhGFS or Fraunhofer FS Red Hat s GlusterFS Lustre Ibrix and so forth.

As used herein the term data storage node refers to a node in a distributed file system which is used for storing file data e.g. one or more file blocks for at least one file . For example a DataNode in HDFS may be considered an example of a data storage node.

As used herein the term data management node refers to a node that stores and manages e.g. updates existing entries adds new entries deletes entries etc. all the metadata information associated with the distributed file system. Examples of such metadata information may include directory tree information file names file blocks identification file block IDs file blocks location file block mapping to IDs file blocks access permissions file checksums and so forth. For example a NameNode in HDFS may be considered an example of a data management node. In some instances a distributed file system may have multiple data storage nodes and a single data management node.

The following description is directed to techniques and solutions supporting fault tolerance in a distributed file system. A service provider may use a plurality of virtual machines to run nodes in a distributed file system configuration. For example to provide fault tolerance in instances of failure of a data management node a storage volume e.g. a network storage drive may be mounted to the virtual machine instance VMI used to run the data management node. The storage volume may be used to store the metadata information of the distributed file system. If the data management node fails e.g. the node becomes unresponsive for a certain time period a new VMI may be launched and may be used to run the data management node. The storage volume may then be simply re attached to the data management node after the new VMI is launched. In this regard the integrity and availability of the metadata information may be insured in case the data management node experiences failure thereby providing fault tolerance in the distributed file system.

Similar fault tolerance solution may be provided to avoid failure of a data storage node and the use of valuable network resources to replicate any missing file data that has been stored on the failed data storage node. More specifically a storage volume may be attached e.g. mounted to a data storage node and may be used to copy the file data e.g. file blocks stored by the file storage node. If the file storage node fails a new VMI may be launched and may be used to run a new storage node. The storage volume may be disconnected from the failed node and attached to the new storage node. The data management node may be notified e.g. via a handshake upon starting the new data storage node of the file data blocks available at the new data storage node.

The fault tolerance service may comprise suitable logic circuitry interfaces and or code and may be operable to fault tolerance related functionalities within the service provider . For example the fault tolerance service may be configured to launch or deactivate VMIs run nodes e.g. data management nodes and data storage nodes for implementing a distributed file system mount one or more storage volumes to VMIs detect failure of nodes and launch replacement nodes and so forth.

The VMIs may be used to run one or more nodes implementing a distributed file system. For example the VMIs . . . may be used to run data storage nodes DSNs . . . respectively and VMI may be used to run a data management node DMN .

In operation the fault tolerance service may mount a storage volume e.g. network storage to the VMI for use by the data management node . The DMN may use the network storage to store metadata files . is a block diagram of example metadata files which may be used for fault tolerance in accordance with an embodiment of the disclosure. Referring to the metadata files may include file system metadata FSM and a command log CL . The FSM may include for example directory tree information file names file blocks identification file block IDs file blocks location file block mapping to IDs file blocks access permissions file checksums and other metadata associated with the distributed file system that includes DSNs . . . and DMN .

The command log may include a list of commands associated with updates to the FSM . For example the data management node may receive FSM update commands which may be stored in the command log for subsequent execution e.g. during a checkpointing process as illustrated in . The commands may include FSM edit commands for existing metadata e.g. renaming a file adding new metadata e.g. adding a new file deleting existing metadata e.g. deleting a file and so forth. In an example file addition event the data management node may receive the new file along with the FSM update command e.g. add a new file divide the new file into blocks and store each block in one of the DSNs . . . . The CL can then be updated to include all instructions associated with processing the new file metadata e.g. adding identification for all new file blocks and identification of the DSNs storing them into the FSM

Since the command log includes pending unexecuted commands associated with the FSM the DMN or the fault tolerance service may periodically perform checkpointing to update the FSM with all pending commands from the CL . is a block diagram of an example checkpointing process for updating file system metadata in accordance with an embodiment of the disclosure. Referring to during a checkpointing procedure the DMN may read all unperformed commands from the CL and apply them to the FSM . In this regard a new and updated FSM may be generated. Since the pending commands from the CL have been executed during the checkpointing a new CL may be generated which will be empty. In an example embodiment only a single CL e.g. may be used which may keep a running list of all commands entered for updating the FSM . In this regard if checkpointing is performed all executed performed FSM update commands may be marked in the CL as being executed performed e.g. by activating a flag for each command entry .

The DMN may identify e.g. in a setting the network storage as the location where the metadata files are stored. In operation the fault tolerance service may detect that the DMN is unresponsive. For example the fault tolerance service may detect at that the DMN has failed e.g. DMN is unresponsive to one or more instructions related to processing of data files within the distributed file system . The failure of the DMN may be due to unresponsiveness of the DMN itself or failure unavailability of the VMI used to run the DMN . After the fault tolerance service detects that the DMN has failed a new VMI may be launched at and a replacement DMN may be run at the new VMI . In this regard the fault tolerance service or another service of the service provider may ensure that the necessary code e.g. software for running DMN or DSN is installed after a VMI is launched so that a new node can be run after the VMI is launched. In some instances the code for running DMN or DSN can be part of the hypervisor or software kernel of a host server used for launching the VMI.

After the new replacement DMN is launched on VMI the fault tolerance service may disconnect the network storage from the VMI and attach it e.g. mount it to the replacement DMN . The DMN may then access the file system metadata and the command log within the metadata files in order to retrieve the metadata associated with the files stored by the DSNs . . . . In some instances the replacement DMN may perform checkpointing to update the file system metadata with all unexecuted commands stored in the command log

Each of the DSNs . . . may store an address or a uniform resource identifier URI identifying the DMN of the distributed file system. Such URI may be stored in a configuration record and may include at least one of a protocol identifier i.e. communication protocol used by the DMN a host name e.g. IP address of the VMI running the DMN or an access port associated with the DMN. After the replacement DMN is launched on VMI the new IP address of VMI may be communicated to each of the DSNs . . . and saved in their configuration record . The fault tolerance service may then restart the DSNs . . . so that a handshake or connection to the replacement DMN may be established using the updated IP address in each configuration record of the DSNs.

In an example embodiment the DMN may be associated with a dedicated IP address e.g. an IP address assigned to a customer which may be used for addressing a VMI running the DMN of a distributed file system of the customer . More specifically each of the configuration records may store the dedicated IP address as the DMN s IP address or the IP address of the VMI running the DMN . Upon detecting the failure of DMN and re launching a new VMI the dedicated IP address may be re assigned to the new VMI . In this regard there will be no need for the fault tolerance service to update each configuration record with a new IP address of the new VMI running the replacement DMN .

In a particular implementation the distributed file system of the service provider e.g. as illustrated in may be a Hadoop Distributed File System. In this regard the data storage nodes . . . may be Hadoop DataNodes and the data management node or may be Hadoop NameNode. Additionally the file system metadata of the metadata files may include the Hadoop fsimage file and the Command Log may include the Hadoop editlog file both of which are stored in a Hadoop directory dfs.name.dir at the NameNode. The configuration record may be stored in a Hadoop directory fs.default.name at each of the DataNodes . . . .

More specifically the fault tolerance service may attach the network storage to the VMI and the DSN and the network storage may be used to store the file data segments or blocks designated for storage by the DSN . More specifically each configuration record of a DSN may store a path to the location where the DSN should store the file data segments . In this regard the configuration record for DSN may indicate that all data segments should be stored in the network storage . In some instances the configuration record may indicate that file data segments should be stored at the DSN e.g. local storage associated with the VMI as well as at the network storage .

If the fault tolerance service detects at that the DSN has failed e.g. DSN is unresponsive to one or more instructions related to processing of data files within the distributed file system such as data segment access or storage instruction . The failure of the DSN may be due to unresponsiveness of the DSN itself or failure unavailability of the VMI used to run the DSN . After the fault tolerance service detects that the DSN has failed a new VMI may be launched at and a replacement DSN may be run at the new VMI . In this regard the fault tolerance service or another service of the service provider may ensure that the necessary code e.g. software for running DMN or DSN is installed after a VMI is launched so that a new node can be run on the VMI after the VMI is launched.

After the new replacement DSN is launched on VMI the fault tolerance service may disconnect the network storage from the VMI and attach it e.g. mount it to the replacement DSN . The DMN may then access the file data segments from the network storage via the replacement DSN running on VMI .

In an example embodiment the service provider can be established for an organization by or on behalf of the organization. That is the service provider may offer a private cloud environment. In another embodiment the service provider supports a multi tenant environment wherein a plurality of customers operate independently i.e. a public cloud environment . Generally speaking the service provider can provide the following models Infrastructure as a Service IaaS Platform as a Service PaaS and or Software as a Service SaaS . Other models can be provided. For the IaaS model the service provider can offer computers as physical or virtual machines and other resources. The virtual machines can be run as guests by a hypervisor as described further below. The PaaS model delivers a computing platform that can include an operating system programming language execution environment database and web server. Application developers can develop and run their software solutions on the service provider platform without the cost of buying and managing the underlying hardware and software. The SaaS model allows installation and operation of application software in the service provider. In some embodiments end users access the service provider using networked customer devices such as desktop computers laptops tablets smartphones etc. running web browsers or other lightweight customer applications. Those skilled in the art will recognize that the service provider can be described as a cloud environment.

The particular illustrated service provider includes a plurality of server computers A D. While only four server computers are shown any number can be used and large centers can include thousands of server computers. The server computers A D can provide computing resources for executing software instances A D. In one embodiment the instances A D are virtual machines. As known in the art a virtual machine is an instance of a software implementation of a machine i.e. a computer that executes applications like a physical machine. In the example each of the server computers A D can be configured to execute a hypervisor or another type of program configured to enable the execution of multiple instances on a single server. For example each of the servers A D can be configured e.g. via the hypervisor to support one or more virtual machine partitions with each virtual machine partition capable of running a virtual machine instance e.g. server computer A could be configured to support three virtual machine partitions each running a corresponding virtual machine instance . Additionally each of the instances can be configured to execute one or more applications.

In an example embodiment each of the server computers A D may also comprise distributed file system DFS software which may be used by one or more of the instances to run nodes in connection with a distributed file system. For example the DFS software may be used to run one or more data storage nodes DSNs and or data management nodes DMNs as illustrated in .

The service provider may also comprise a fault tolerance service which may have the functionalities described herein in connection with fault tolerance service . The fault tolerance service may be implemented as a stand alone service within the provider as a dedicated server similar to the servers A D and or may be implemented as part of the server computer that performs management functions. For example the fault tolerance service may be implemented as part of the management component as seen in . Additionally the fault tolerance service may use one or more network storage volumes to implement the fault tolerance functionalities described herein.

It should be appreciated that although the embodiments disclosed herein are described primarily in the context of virtual machines other types of instances can be utilized with the concepts and technologies disclosed herein. For instance the technologies disclosed herein can be utilized with storage resources data communications resources and with other types of computing resources. The embodiments disclosed herein might also execute all or a portion of an application directly on a computer system without utilizing virtual machine instances.

One or more server computers can be reserved for executing software components for managing the operation of the server computers the instances the hypervisors and or the fault tolerance service . For example the server computer can execute a management component . A customer can access the management component to configure various aspects of the operation of the instances purchased by the customer. For example the customer can purchase rent or lease instances and make changes to the configuration of the instances. The customer can also specify settings regarding how the purchased instances are to be scaled in response to demand.

The server computer may further comprise memory which may be used as processing memory by the fault tolerance service . An auto scaling component can scale the instances based upon rules defined by the customer. In one embodiment the auto scaling component allows a customer to specify scale up rules for use in determining when new instances should be instantiated and scale down rules for use in determining when existing instances should be terminated. The auto scaling component can consist of a number of subcomponents executing on different server computers or other computing devices. The auto scaling component can monitor available computing resources over an internal management network and modify resources available based on need.

A deployment component can be used to assist customers in the deployment of new instances of computing resources. The deployment component can have access to account information associated with the instances such as who is the owner of the account credit card information country of the owner etc. The deployment component can receive a configuration from a customer that includes data describing how new instances should be configured. For example the configuration can specify one or more applications to be installed in new instances provide scripts and or other types of code to be executed for configuring new instances provide cache logic specifying how an application cache should be prepared and other types of information. The deployment component can utilize the customer provided configuration and cache logic to configure prime and launch new instances . The configuration cache logic and other information may be specified by a customer using the management component or by providing this information directly to the deployment component . The instance manager e.g. in can be considered part of the deployment component .

Customer account information can include any desired information associated with a customer of the multi tenant environment. For example the customer account information can include a unique identifier for a customer a customer address billing information licensing information customization parameters for launching instances scheduling information auto scaling parameters previous IP addresses used to access the account and so forth.

A network can be utilized to interconnect the server computers A D and the server computer . The network can be a local area network LAN and can be connected to a Wide Area Network WAN so that end users can access the service provider . It should be appreciated that the network topology illustrated in has been simplified and that many more networks and networking devices can be utilized to interconnect the various computing systems disclosed herein.

In order to access and utilize instances such as instances of a customer device can be used. The customer device can be any of a variety of computing devices mobile or otherwise including a cell phone smartphone handheld computer Personal Digital Assistant PDA desktop computer etc. The customer device can communicate with the service provider through an end point which can be a DNS address designed to receive and process application programming interface API requests. In particular the end point can be a web server configured to expose an API. Using the API requests a customer device can make requests to implement any of the functionality described herein e.g. accessing file data managed by the DMN . Other services which can be internal to the service provider can likewise make API requests to the end point . The API requests from the client can pass through the admission control and onto the fault tolerance service in order to access file data related functionalities of the distributed file system and or change one or more settings of the fault tolerance service e.g. replication parameters designating a number of copies that the DSN have to maintain for each file block and so forth .

Other general management services that may or may not be included in the service provider and or within the management component include an admission control e.g. one or more computers operating together as an admission control web service. The admission control can authenticate validate and unpack the API requests for service or storage of data within the service provider . The capacity tracker is responsible for determining how the servers need to be configured in order to meet the need for the different instance types by managing and configuring physical inventory in terms of forecasting provisioning and real time configuration and allocation of capacity. The capacity tracker maintains a pool of available inventory in a capacity pool database . The capacity tracker can also monitor capacity levels so as to know whether resources are readily available or limited.

An instance manager controls launching and termination of virtual machine instances in the network. When an instruction is received such as through an API request to launch an instance the instance manager pulls resources from the capacity pool and launches the instance on a decided upon host server computer. Similar to the instance manager are the storage manager and the network resource manager . The storage manager relates to initiation and termination of storage volumes while the network resource manager relates to initiation and termination of routers switches subnets etc. A network of partitions is described further in relation to and includes a physical layer upon which the instances are launched.

A health monitoring service can provide monitoring for resources and the applications customers run on the service provider . System administrators can use the monitoring service to collect and track metrics and gain insight to how applications are running. For example the monitoring service can allow system wide visibility into application performance and operational health. Metrics generated by the health monitoring service can be stored in the metrics database . In an example embodiment the fault tolerance service may be part of the health monitoring service .

The fault tolerance service may perform the fault tolerance and distributed file system related functionalities described herein. The fault tolerance service may communicate with the capacity tracker to receive information regarding available partitions and or host servers that can be used for launching an instance e.g. to launch a replacement DMN or DSN for fault tolerance . Additionally communications with the admission control the storage manager and the network of partitions may be used to launch or re launch an instance in accordance with a fault tolerance functionalities described herein.

The router reads address information in a received packet and determines the packet s destination. If the router decides that a different data center contains a host server computer then the packet is forwarded to that data center. If the packet is addressed to a host in the data center then it is passed to a network address translator NAT that converts the packet s public IP address to a private IP address. The NAT also translates private addresses to public addresses that are bound outside of the data center . Additional routers can be coupled to the NAT to route packets to one or more racks of host server computers. Each rack can include a switch coupled to multiple host server computers. A particular host server computer is shown in an expanded view at .

Each host has underlying hardware . Running a layer above the hardware is a hypervisor or kernel layer . The hypervisor or kernel layer can be classified as a type 1 or type 2 hypervisor. A type 1 hypervisor runs directly on the host hardware to control the hardware and to manage the guest operating systems. A type 2 hypervisor runs within a conventional operating system environment. Thus in a type 2 environment the hypervisor can be a distinct layer running above the operating system and the operating system interacts with the system hardware. Different types of hypervisors include Xen based Hyper V ESXi ESX Linux etc. but other hypervisors can also be used. In an example embodiment the hypervisor layer may include the DFS software which may be used to install DSNs or DMNs as described herein.

A management layer can be part of the hypervisor or separated therefrom and generally includes device drivers needed for accessing the hardware . The partitions are logical units of isolation by the hypervisor. Each partition can be allocated its own portion of the hardware layer s memory CPU allocation storage etc. Additionally each partition can include a virtual machine and its own guest operating system e.g. VMI may be running on partition and VMIn may be running on partition n . As such each partition is an abstract portion of capacity designed to support its own virtual machine independent of the other partitions.

Any applications executing on the instances can be monitored using the management layer which can then pass the metrics to the health monitoring service for storage in the metrics database . Additionally the management layer can pass to the monitoring service the number of instances that are running when they were launched the operating system being used the applications being run etc. All such metrics can be used for consumption by the health monitoring service and stored in database .

Additionally when the fault tolerance service or performs fault tolerance in connection with a failed DMN running on VMI the NAT may be used to assign the same dedicated public IP address to the replacement VMI . This is illustrated at the expanded view of the NAT . As seen at even though each VMI and or each server running the VMI may be associated with its own private IP address PIP and PIP after the replacement DMN is launched on VMI the dedicated IP address is assigned to the replacement VMI . In an example embodiment the dedicated IP address may be part of a virtual network interface associated with one or more of the VMIs e.g. VMI . In addition to the dedicated public IP address such virtual network interface may also provide one or more dedicated private IP addresses one or more security groups a MAC address a source destination check flag and so forth. In this regard if the private IP address PIP is part of the virtual network interface for VMI together with the dedicated public IP address the private IP address PIP may also be dedicated to VMI . Consequently after the replacement DMN is launched on VMI the dedicated IP address as well as the dedicated private IP address PIP are both assigned to the replacement VMI .

The at least one data management node DMN may be configured to store metadata information e.g. metadata files associated with the file data in a network storage e.g. attached to the at least second VMI VMI . At the fault tolerance service may detect failure of the at least second VMI VMI and may re launch the at least one data management node re launched as DMN on at least a third VMI e.g. VMI running on the one or more servers. The at least third VMI may be replacing the failed at least second VMI to provide fault tolerance in the distributed file system of the service provider e.g. the distributed file system comprising the DSNs . . . DMNs network storage and fault tolerance service .

At the fault tolerance service may attach the network storage to the at least third VMI VMI . The fault tolerance service may associate the dedicated IP address e.g. dedicated IP address with the at least third VMI VMI . At the fault tolerance service may cause the DMN to retrieve the stored metadata information . The metadata information may include file system metadata and a command log . The file system metadata may include for each file in the file data at least one of a file name of the file a file checksum of the file information identifying one or more file blocks of the file information identifying one or more file directories storing each of the file blocks or information identifying one or more of the at least one data storage nodes hosting the identified one or more file directories. The command log may include at least one instruction for updating the file system metadata

Upon re launching the at least one data management node as DMN on the at least third VMI VMI the fault tolerance service may perform checkpointing of the metadata information by updating the file system metadata based on the at least one instruction in the command log . The command log may be reset after the checkpointing by deleting the at least one instruction and creating a clean empty log .

Upon launching the at least one data management node as DMN on the at least second VMI the fault tolerance service may mount the network storage as an external volume attached to the at least second VMI . The fault tolerance service may store an address e.g. a dedicated IP address associated with the at least one data management node in a configuration record e.g. for the at least one data storage node . . . .

Detecting the failure of the at least second VMI includes detecting inactivity by the at least one data management node for a duration of time where the inactivity is in response to the at least one data management node receiving at least one instruction associated with the file data.

At the fault tolerance service may detect a failure of the at least second VMI and may re launch the at least one data storage node as DSN on at least a third VMI e.g. VMI running on the one or more servers. At the fault tolerance service may attach the network storage to the at least third VMI . At the fault tolerance service may cause the DMN to access the file data from the network storage DSN attached to the at least third VMI . The at least third VMI may be replacing the failed at least second VMI to provide fault tolerance in the distributed file system.

With reference to the computing environment includes one or more processing units and memory . In this basic configuration is included within a dashed line. The processing units execute computer executable instructions. A processing unit can be a general purpose central processing unit CPU processor in an application specific integrated circuit ASIC or any other type of processor. In a multi processing system multiple processing units execute computer executable instructions to increase processing power. For example shows a central processing unit as well as a graphics processing unit or co processing unit . The tangible memory may be volatile memory e.g. registers cache RAM non volatile memory e.g. ROM EEPROM flash memory etc. or some combination of the two accessible by the processing unit s . The memory stores software implementing one or more innovations e.g. functionalities described herein in the form of computer executable instructions suitable for execution by the processing unit s .

A computing system may have additional features. For example the computing environment includes storage one or more input devices one or more output devices and one or more communication connections . An interconnection mechanism not shown such as a bus controller or network interconnects the components of the computing environment . Typically operating system software not shown provides an operating environment for other software executing in the computing environment and coordinates activities of the components of the computing environment .

The tangible storage may be removable or non removable and includes magnetic disks magnetic tapes or cassettes CD ROMs DVDs or any other medium which can be used to store information in a non transitory way and which can be accessed within the computing environment . The storage stores instructions for the software implementing one or more innovations described herein.

The input device s may be a touch input device such as a keyboard mouse pen or trackball a voice input device a scanning device or another device that provides input to the computing environment . The output device s may be a display printer speaker CD writer or another device that provides output from the computing environment .

The communication connection s enable communication over a communication medium to another computing entity. The communication medium conveys information such as computer executable instructions audio or video input or output or other data in a modulated data signal. A modulated data signal is a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal. By way of example and not limitation communication media can use an electrical optical RF or other carrier.

Although the operations of some of the disclosed methods are described in a particular sequential order for convenient presentation it should be understood that this manner of description encompasses rearrangement unless a particular ordering is required by specific language set forth below. For example operations described sequentially may in some cases be rearranged or performed concurrently. Moreover for the sake of simplicity the attached figures may not show the various ways in which the disclosed methods can be used in conjunction with other methods.

Any of the disclosed methods can be implemented as computer executable instructions stored on one or more computer readable storage media e.g. one or more optical media discs volatile memory components such as DRAM or SRAM or non volatile memory components such as flash memory or hard drives and executed on a computer e.g. any commercially available computer including smart phones or other mobile devices that include computing hardware . The term computer readable storage media does not include communication connections such as signals and carrier waves. Any of the computer executable instructions for implementing the disclosed techniques as well as any data created and used during implementation of the disclosed embodiments can be stored on one or more computer readable storage media. The computer executable instructions can be part of for example a dedicated software application or a software application that is accessed or downloaded via a web browser or other software application such as a remote computing application . Such software can be executed for example on a single local computer e.g. any suitable commercially available computer or in a network environment e.g. via the Internet a wide area network a local area network a customer server network such as a cloud computing network or other such network using one or more network computers.

For clarity only certain selected aspects of the software based implementations are described. Other details that are well known in the art are omitted. For example it should be understood that the disclosed technology is not limited to any specific computer language or program. For instance the disclosed technology can be implemented by software written in C Java Perl JavaScript Adobe Flash or any other suitable programming language. Likewise the disclosed technology is not limited to any particular computer or type of hardware. Certain details of suitable computers and hardware are well known and need not be set forth in detail in this disclosure.

It should also be well understood that any functionality described herein can be performed at least in part by one or more hardware logic components instead of software. For example and without limitation illustrative types of hardware logic components that can be used include Field programmable Gate Arrays FPGAs Program specific Integrated Circuits ASICs Program specific Standard Products ASSPs System on a chip systems SOCs Complex Programmable Logic Devices CPLDs etc.

Furthermore any of the software based embodiments comprising for example computer executable instructions for causing a computer to perform any of the disclosed methods can be uploaded downloaded or remotely accessed through a suitable communication means. Such suitable communication means include for example the Internet the World Wide Web an intranet software applications cable including fiber optic cable magnetic communications electromagnetic communications including RF microwave and infrared communications electronic communications or other such communication means.

The disclosed methods apparatus and systems should not be construed as limiting in any way. Instead the present disclosure is directed toward all novel and nonobvious features and aspects of the various disclosed embodiments alone and in various combinations and sub combinations with one another. The disclosed methods apparatus and systems are not limited to any specific aspect or feature or combination thereof nor do the disclosed embodiments require that any one or more specific advantages be present or problems be solved.

In view of the many possible embodiments to which the principles of the disclosed invention may be applied it should be recognized that the illustrated embodiments are only preferred examples of the invention and should not be taken as limiting the scope of the invention. Rather the scope of the invention is defined by the following claims. Therefore what is claimed as the invention is all that comes within the scope of these claims.

