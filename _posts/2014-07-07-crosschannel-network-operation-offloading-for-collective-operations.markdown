---

title: Cross-channel network operation offloading for collective operations
abstract: A Network Interface (NI) includes a host interface, which is configured to receive from a host processor of a node one or more cross-channel work requests that are derived from an operation to be executed by the node. The NI includes a plurality of work queues for carrying out transport channels to one or more peer nodes over a network. The NI further includes control circuitry, which is configured to accept the cross-channel work requests via the host interface, and to execute the cross-channel work requests using the work queues by controlling an advance of at least a given work queue according to an advancing condition, which depends on a completion status of one or more other work queues, so as to carry out the operation.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09344490&OS=09344490&RS=09344490
owner: Mellanox Technologies Ltd.
number: 09344490
owner_city: Yokneam
owner_country: IL
publication_date: 20140707
---
 This application is a continuation of U.S. patent application Ser. No. 12 945 904 which claims the benefit of U.S. Provisional Patent Application 61 261 339 filed Nov. 15 2009 whose disclosure is incorporated herein by reference. 

The present invention relates generally to computer networks and particularly to application communication over computer networks.

Operation of computers in High Performance Computing HPC environment often involves fast execution of collective operations. A commonly used Application Programming Interface API for initiating collective operations in HPC environment is specified by the Message Passing Interface MPI forum in MPI A Message Passing Interface Standard version 2.2 Sep. 4 2009 which is incorporated herein by reference.

Computers and storage devices in HPC environment commonly interconnect through a switched network that is specified by the InfiniBand Trade Association in InfiniBand Architecture Specification release 1.2.1 January 2008 which is incorporated herein by reference.

An embodiment of the present invention that is described herein provides a Network Interface NI including 

a host interface which is configured to receive from a host processor of a node one or more cross channel work requests that are derived from an operation to be executed by the node 

a plurality of work queues for carrying out transport channels to one or more peer nodes over a network and

control circuitry which is configured to accept the cross channel work requests via the host interface and to execute the cross channel work requests using the work queues by controlling an advance of at least a given work queue according to an advancing condition which depends on a completion status of one or more other work queues so as to carry out the operation.

In some embodiments the operation includes a collective operation to be executed by the node together with the one or more peer nodes. In an embodiment the operation is initiated by one or more Message Passing Interface MPI commands. In a disclosed embodiment the MPI commands include non blocking commands. In another embodiment the operation is initiated by one or more Shared Memory SHMEM commands. In yet another embodiment the control circuitry is configured to control the advance of the given work queue by holding the given work queue in a wait state until verifying that the advancing condition is met.

In some embodiments the NI includes a computation unit that is configured to execute a calculation that is specified in the cross channel work requests and the control circuitry is configured to send one or more results of the calculation to at least one target. The calculation may include at least one operation selected from a group of operations including maximum minimum sum product logical AND bit wise AND logical OR bit wise OR logical exclusive OR XOR and bit wise exclusive OR XOR . Additionally or alternatively the calculation may include arguments of at least one data type selected from a group of data types including a vector that is contiguously organized in memory a vector that is non contiguously organized in memory and a multidimensional vector. In some embodiments the target includes a peer node. In alternative embodiments the target includes a host memory.

In an embodiment the control circuitry is configured to control the advance of the given work queue by enabling sending to a peer node a message that is stored at a head of the given work queue only upon fulfillment of the advancing condition. In another embodiment the control circuitry is configured to control the advance of the given work queue by enabling receiving in the given queue a message from a peer node only upon fulfillment of the advancing condition. In yet another embodiment the control circuitry is configured to estimate the completion status of the other work queues according to one or more counting objects that are indicative of the completion status.

In some embodiments the network includes an Infiniband network and the work queues include Infiniband work queues. In some embodiments the NI includes one or more completion queues and the control circuitry is configured to estimate the completion status of the other queues according to the completion status of the one or more completion queues. In an embodiment the control circuitry is configured to reuse at least one of the work queues for carrying out multiple transport channels over the at least one work queue. In another embodiment the control circuitry and the work queues are included in a chip set. In yet another embodiment the control circuitry and the work queues are included in a single chip.

There is additionally provided in accordance with an embodiment of the present invention a method including 

in a Network Interface NI that includes a plurality of work queues for carrying out transport channels to one or more peer nodes over a network receiving from a host processor of a node one or more cross channel work requests that are derived from an operation to be executed by the node and

executing the cross channel work requests using the work queues by controlling an advance of at least a given work queue according to an advancing condition which depends on a completion status of one or more other work queues so as to carry out the operation.

There is also provided in accordance with an embodiment of the present invention a computer software product including a computer readable storage medium in which program instructions are stored which instructions when read by a computer cause the computer to receive from a host processor of a node one or more cross channel work requests that are derived from an operation to be executed by the node and to execute the cross channel work requests using a plurality of work queues by controlling an advance of at least a given work queue according to an advancing condition which depends on a completion status of one or more other work queues so as to carry out the operation.

The present invention will be more fully understood from the following detailed description of the embodiments thereof taken together with the drawings in which 

Embodiments of the present invention provide improved data communication and computation methods and devices for use in parallel computing environments such as High Performance Computing HPC systems which in particular achieve fast execution of collective operations. In the described embodiments host servers issue Message Passing Interface MPI collective operation commands that perform the collective operations over Infiniband IB switched networks. In an embodiment an IB Host Channel Adaptor HCA is typically implemented in a Network Card NIC that is directly connected to a host server. The HCA communicates with other nodes such as servers and storage systems over an IB network in a HPC environment. The disclosed techniques provide fast and host CPU independent execution of the collective operations by offloading a considerable part of the associated processing burden from the host processor to the NIC. The offloaded part is executed by the NIC without the need for software intervention and therefore the effect of Operating System OS noise on the communication is mitigated. This sort of offloading is especially important for collective operations in large systems.

In an embodiment a HPC application runs on a host server and issues collective operations that necessitate fast messaging transfer and accurate synchronization in time among multiple hosts. The collective operations are converted by the host software to collective work requests shortly denoted requests and respective control commands which the host processor transfers to the HCA.

The HCA typically comprises multiple Work Queues WQs . Each WQ comprises Work Queue Entries WQEs wherein each WQE comprises an information element that is related to one or more network events such as send receive messages to from peer nodes. A typical WQE may comprise for example a received message a message to be transmitted a Direct Memory Access DMA descriptor a collective request or a portion thereof. For example a collective request may be broken down into multiple primitives by the host processor such that each WQE comprises a respective primitive.

The WQs are arranged in Queue Pairs QPs wherein each pair comprises one Receive Queue RQ and one Send Queue SQ . A RQ typically queues receive related WQEs and a SQ typically queues send related WQEs. A QP is normally associated with a corresponding QP in a peer node thus creating a point to point transport channel for message transfer. The HCA comprises Completion Queues CQ which reflect the completion status of WQEs by virtue of associating each CQ entry with a corresponding WQE. A Producer Index PI points to the last completed entry in a CQ or to the last posted WQE in a WQ that is enabled for execution by the HCA.

In some embodiments Control Circuitry CC within the HCA loads the collective work requests coming from the host to corresponding WQs and executes them. The request content and the respective control commands imply advancing conditions that condition the advance of some WQs on execution completion on other WQEs and or on reception of peer messages in specified RQs. Thus cross channel operation is achieved in the HCA which carries out the associated collective operations thus offloading this burden from the host processor.

It is noted that the disclosed techniques are not limited to collective operations and can be used for executing various other operation types. In other words any operation to be performed by a node can be converted to cross channel work requests in which the advance of a WQ depends on the completion status of one or more other WQEs. The cross channel work requests are provided to the HCA for execution thus offloading the host processor of the node from these tasks. When the operation to be performed by the node comprises a collective operation the cross channel work requests are referred to as collective work requests.

In some embodiments this offloading is achieved by having the CC execute requests that introduce cross channel dependency between the HCA WQs such as the following In a WAIT request the CC conditions the advance of a WQ on completion of some operations in another WQ. In a RECEIVE ENABLE request the CC conditions the advance of a certain RQ on reaching the RECEIVE ENABLE request. In a SEND ENABLE request the CC conditions the advance of another SQ on reaching the SEND ENABLE request. In a CALC request the CC executes a predefined calculation on reaching the CALC request. Note that CALC is not a cross channel operation by itself however its execution often depends on execution of cross channel requests . The above example requests are described and demonstrated in detail hereinbelow.

It is possible in principle to execute the above described Input Output I O operations driven by the MPI collective operation commands in the host processor. Execution in the host however would significantly degrade HPC performance for example due to Operating System OS noise since in many systems the jitter caused by OS noise accumulates and becomes a major limiting factor of system performance. Offloading this I O burden according to the disclosed techniques as explained above and further detailed below eliminates this performance degradation. Furthermore eliminating host CPU intervention in communication tasks frees CPU resources and therefore allows for better CPU performance in computational tasks. The performance improvement achieved by the disclosed techniques is also important when executing non blocking collective operations without software intervention.

A host processor runs a HPC application that uses a memory as a dynamic memory. The HPC application includes collective MPI commands that issue collective operations over network by executing code modules of a MPI library . MPI library is provided here as an example only any other suitable libraries such as Shared Memory SHMEM can be used as well in alternative embodiments.

In an embodiment processor using MPI library code modules converts the collective operation commands to collective work requests and respective control commands and transfers them to HCA through a host interface . The distinction between collective work requests and control commands is described by way of example and is not mandatory. In an example embodiment HCA is implemented in a Network Interface Card NIC . In alternative embodiments HCA is implemented in a chip set or a single chip. Control Circuitry CC within HCA receives the requests from processor parses them and manages the HCA accordingly as explained hereinafter.

Queues comprise IB queues such RQs SQs and CQs which are further detailed hereinafter. CC loads the requests coming from the host to corresponding WQs and executes them. The WQEs content and the respective control commands imply control criteria according to which the CC controls the WQs. In particular the CC conditions the advance of a given WQ on completion of WQEs execution on other WQs and or on reception of messages from peer nodes in some RQs through interface . Thus the HCA enforces cross channel operations which carry out the collective operation commands that were specified in the work requests as explained above.

In an embodiment CC further comprises a computation unit which is configured in some embodiments as an Arithmetic Logic Unit ALU for performing calculations that are specified by the requests. Computation unit can be embedded within the CC or it can be implemented separately within the HCA and controlled by the CC. A memory unit is connected to CC and serves to store code and data that the CC and the computation unit use.

CC and computation unit are realized in an embodiment in hardware which may comprise Field Programmable Gate Arrays FPGAs and or Application Specific Integrated Circuits ASICs . CC may also comprise a programmable element comprising one or more dedicated or general purpose processors which run software for carrying out the methods described herein. The software may be downloaded to the processors in electronic form over a network for example or it may alternatively or additionally be provided and or stored on non transitory tangible media such as magnetic optical or electronic memory. Queues and memory are typically implemented in a random access memory such as Static Random Access Memory SRAM or Dynamic Random Access Memory DRAM which may be embedded within the CC or assembled separately in the HCA. In some embodiments WQs are stored in the host memory and part of them is cached in to the HCA.

Interface may comprise multiple IB ports connected to multiple ports of network thus achieving multiple simultaneous paths through the network. The disclosed offloading techniques may be applicable as well to transport technologies other than IB. For example the disclosed cross channel operations may be applicable to Ethernet RDMA over Converged Ethernet RoCE and other suitable interfaces. The configuration of HCA shown in is an example configuration which is chosen purely for the sake of conceptual clarity. In alternative embodiments any other suitable HCA configuration can also be used. HCA and host elements that are not mandatory for understanding the disclosed techniques were omitted from the figure for the sake of clarity.

At a conversion step host processor converts the collective operation command to work requests using code modules of MPI library . Sample request execution is exemplified hereinafter. At a forwarding step processor forwards the requests to HCA via host interface after adapting them to the HCA format using an appropriate HCA driver code. At an application step HCA e.g. using CC applies the requests to WQs CQs counters and any other relevant logic in the HCA.

At an execution step group execution of the work requests is illustrated by example requests WAIT and CALC. Request execution is illustrated in more detail in below. At a waiting step CC exerts the WAIT request by holding a given WQ in a wait state until reception of messages from some specified peer nodes.

In one embodiment the CC identifies reception of the awaited messages by sensing a PI of a CQ wherein the CQ is associated with a predetermined group of RQs that are configured to accept the awaited messages. Alternative implementations e.g. using counters are described further below. At a calculation step which is conditioned on exiting the wait state computation unit carries out a calculation that is specified by the CALC request. At a sending step CC sends the calculation results to peer nodes through IB interface over IB network .

A loopback signifies a transition managed by CC to the next WQE in a given WQ. A loopback depicts indication to host processor that the collective operation has been terminated and a transition to the next collective operation. Loopback emphasizes the fact that the host is not involved in the execution of a collective operation once it is transferred to HCA in step . The above loopbacks illustrate request and operation sequencing. However in an embodiment CC typically manages multiple sequences of collective operations and requests concurrently. The flowchart shown in is an example flowchart which is chosen purely for the sake of conceptual clarity. In alternative embodiments any other suitable flowchart can also be used for realizing the disclosed methods.

In alternative embodiments of the WAIT request the CC may queue it in any WQ and condition it on any other WQ. In an embodiment CC polls the other WQ or any other suitable indication for verifying whether the wait condition has been met. In alternative embodiments the indication may actively notify the CC when an awaited condition is met by an interrupt doorbell or any other suitable mechanism.

Upon resuming SQ advance CC executes a CALC request denoted which follows WAIT request while the CALC uses the data received in RQs and . CALC is a calculation type request comprising a typical syntax CALC Opcode List of argument addresses Target addresses . CC executes the calculation that is specified by the request opcode by means of computation unit on arguments whose addresses in memory are specified in the request. The specified targets to which CC will send the CALC results are typically one or more peer nodes over network and or an address in host memory . At this point the CALC collective operation is completed and the CC reports it to the host processor by means of a predefined CQ which is not shown in the figure for the sake of simplicity.

Example CALC operations are maximum minimum sum product logical AND bitwise AND logical OR bitwise OR logical exclusive OR XOR bit wise exclusive OR XOR or any other suitable operation. When the CALC opcode is minimum or maximum an index can be attached to each argument and the index of the result will be that of the minimal argument for minimum operation or that of the maximal argument for maximum operation . CALC results can be posted on either Datagram or connection oriented SQ.

Arguments of CALC operation can be of various data types including vectors. A vector of N elements is denoted in this description V N . A collective operation may involve many vectors Vi N that reside in multiple nodes over network which compose a general vector R N . In an embodiment the elements of vector V N may be stored in HCA memory in either contiguous memory addresses or in non continuous addresses for example having a fixed stride in the memory between adjacent elements. In alternative embodiment V N and R N may be of any dimension. In other embodiments however CALC operations may comprise any other suitable data types.

In particular CC sets RQs and entries in a disabled status. This setting disables reception of messages from the network into RQs and until disabled WQEs are enabled by RECEIVE EN requests as explained hereinafter.

CC loads a request denoted as RECEIVE EN QP PI in WQEs and of SQ . RECEIVE EN QP PI when executed enables reception of messages into the RQ of the specified QP in the WQE that is specified by PI. Until the RECEIVE EN execution any message that arrives from a peer node and targets the specified WQE would be discarded and the peer would be notified about this discard provided that the status of that WQE was set as disabled before the RECEIVE EN execution. The above enabling is illustrated in by dashed arrows going from WQEs and to WQEs and respectively. Some embodiments also implement a RECEIVE EN QP version of the receive enabling request wherein the specified RQ is advanced by a single increment of its corresponding PI.

Following reception of both messages counter reaches state is which causes CC to advances SQ thus executing CALC request . This advancing condition is depicted by a dashed arrow from the counter state to WQE . The CALC causes CC to execute by means of computation unit a calculation that is specified by the CALC using parameters that reside in buffers and . The data in the buffers is not affected by succeeding messages that RQs and may receive during the calculation since WQEs and are still in a disabled status.

In an embodiment CC sends the CALC results to other nodes over network as depicted in by the line from SQ to host . In alternative embodiments CC may send the CALC results to host processor or to host memory . CC completes CALC operation it advances SQ thus exerting RCV EN and and sets SQ in a wait state due to WAIT request . RCV EN and enable descriptors and respectively thus allowing advance of counter to state upon receiving additional two messages from hosts and respectively. CC then advances SQ over WAIT request which is conditioned on counter state thus allowing execution of CALC which reuses Rx buffers and . shows in principle two non blocking REDUCE collective operations over a tree Messages which include data from hosts and are reduced by host to one message which includes CALC results based on that data destined to host .

Another example for the usage of the RECEIVE EN request is an implementation of a PIPELINE operation. In a PIPELINE operation a large message e.g. 1 MB is to be sent to a destination node via an intermediate node. In some embodiments the intermediate node comprises several e.g. four small buffers e.g. 8 KB each which serve as a pipeline. Before sending the large message to the intermediate node the message is broken down into multiple 8 KB messages and the 8 KB messages are sent to the intermediate node.

Once an 8 KB message arrives in the intermediate node it occupies one of the above described buffers making it non valid to receive another message . The relevant SEND WQE that points to this buffer is enabled by SEND EN WQE once sending is completed and the data in the buffer is no longer needed in the intermediate node. The buffer is enabled again by RECEIVE EN for enabling reception of a new message on this buffer.

Note that the number of WQEs to be post to the queues in the pipeline implementation is on the order of the original message size 1 MB in the example above divided by the size of the small message 8 KB in the example above . This number can be quite large. In alternative embodiments to the pipeline implementation the software does not generate this large number of WQEs. This task is offloaded to the NIC hardware. The software will use a WQE of type PIPE. For example PIPE receive qp number send qp number intermediate buffer list number of messages .

In alternative embodiments a given control counter such as counter may be incremented upon arrival of a message that indicates increment of the given counter. In further alternative embodiments the counter to be incremented upon reception of a message is not a property of the QP but is set by the send WQE. Using this technique different WQEs on the same SQ that send messages to the same RQ will cause different counters to increment based on the counter id parameter in the send WQE. In further alternative embodiments the number of QPs that are depicted in may be significantly reduced by dynamically carrying out multiple transport channels with one work queue thus providing a Dynamically Connected Transport DCT service mode. A transport service of this sort is described in U.S. patent application Ser. No. 12 621 523 which is assigned to the assignee of the present patent application and whose disclosure is incorporated herein by reference.

In the extreme case of this approach the CC dynamically allocates a single QP comprising a RQ and a SQ and a single CQ for implementing different transport channels that are involved in collective operations that HPC application initiates. The CC then applies all the cross channel operations that are illustrated in to the single QP and single CQ. The only resource that scales when HCA offloads multiple concurrent collective requests is the number of counters.

The flow diagrams that are illustrated in are example diagrams which were chosen purely for the sake of conceptual clarity. In alternative embodiments other suitable applications and cross channel requests can also be used for realizing the disclosed methods. As an example a SEND EN QP PI request may be applied for enabling a send message event wherein the message is stored at or pointed by the head WQE of a given SQ following e.g. receiving and processing messages from a predefined group of peer nodes. Some embodiments also implement a SEND EN QP version of the send enabling request wherein the specified SQ is advanced by a single increment of its corresponding PI.

Although the embodiments described herein mainly address offloading of collective operations in HPC environment the methods and systems exemplified by these embodiments can also be used in other applications that involve computer networks and clusters such as message gathering and distribution.

It will thus be appreciated that the embodiments described above are cited by way of example and that the present invention is not limited to what has been particularly shown and described hereinabove. Rather the scope of the present invention includes both combinations and sub combinations of the various features described hereinabove as well as variations and modifications thereof which would occur to persons skilled in the art upon reading the foregoing description and which are not disclosed in the prior art.

