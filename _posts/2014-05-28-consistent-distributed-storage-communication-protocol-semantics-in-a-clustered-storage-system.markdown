---

title: Consistent distributed storage communication protocol semantics in a clustered storage system
abstract: Consistent distributed storage communication protocol semantics, such as SCSI target semantics, in a clustered storage system are disclosed. The system includes a mechanism for presenting a single distributed logical unit, comprising one or more logical sub-units, as a single logical unit of storage to a host system by associating each of the logical sub-units that make up the single distributed logical unit with a single host visible identifier that corresponds to the single distributed logical unit. The system further includes a mechanism to maintain consistent context information for each of the logical sub-units such that the logical sub-units are not visible to a host system as separate entities from the single distributed logical unit.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09256377&OS=09256377&RS=09256377
owner: NETAPP, INC.
number: 09256377
owner_city: Sunnyvale
owner_country: US
publication_date: 20140528
---
This is a continuation of U.S. patent application Ser. No. 14 167 220 filed on Jan. 29 2014 which is a continuation of U.S. patent application Ser. No. 13 244 159 filed on Sep. 23 2011 both of which are incorporated herein by reference in their entireties.

At least one embodiment of the present invention pertains to network storage systems and more particularly to maintaining consistency between distributed objects in a Storage Area Network SAN attached clustered storage system.

A storage controller is a physical processing device that is used to store and retrieve data on behalf of one or more hosts. A network storage controller can be configured e.g. by hardware software firmware or any combination thereof to operate as a storage server that serves one or more clients on a network to store and manage data in a set of mass storage devices such as magnetic or optical storage based disks tapes or flash memory. Some storage servers are designed to service file level requests from hosts as is commonly the case with file servers used in a network attached storage NAS environment. Other storage servers are designed to service block level requests from hosts as with storage servers used in a storage area network SAN environment. Storage servers in a SAN environment organize the storage into one or more logical units that can be addressed by the host and be used as containers to store data. Each logical unit can be divided into a number of fixed size logical blocks and the host can store retrieve data at the granularity of a logical block. Still other storage servers are capable of servicing both file level requests and block level requests as is the case with certain storage servers made by NetApp Inc. of Sunnyvale Calif. employing the Data ONTAP storage operating system.

A network storage system can have an individual storage server that provides one or more clients with access to data stored in a mass storage subsystem. Recently however with storage capacity demands increasing rapidly in almost every business sector there has been a trend towards the use of clustered network storage systems to improve scalability. In addition as more and more business critical applications are being deployed on virtualized shared infrastructure there has been a trend towards using clustered network storage systems to improve reliability. In a clustered storage system two or more storage server nodes are connected in a distributed architecture. Each storage server node is in fact a storage server although it has a distributed architecture. Two or more such storage server nodes are typically connected to form a storage cluster where each of the nodes in the cluster can communicate with the other nodes in the cluster.

A clustered architecture allows convenient scaling through the addition of more nodes all capable of communicating with each other. Further a storage cluster may present a single system image of stored data to clients and administrators such that the actual location of data can be made transparent to clients and administrators. However as the number of nodes in a cluster increases maintaining a consistent single system image across the nodes of the cluster becomes a challenge as management and control operations are performed on the cluster resources.

References in this specification to an embodiment one embodiment or the like mean that the particular feature structure or characteristic being described is included in at least one embodiment of the present invention. Occurrences of such phrases in this specification do not necessarily all refer to the same embodiment.

As shown in each cluster node of the clustered storage system is coupled with a corresponding mass storage device . Typically each cluster node is coupled with two or more mass storage devices. However to facilitate description a single mass storage device coupled with each corresponding cluster node is depicted in . The mass storage devices can be of any one or more of various types of storage such as magnetic disks flash memory solid state drives SSDs tape storage etc. and can be implemented as a single device multiple devices e.g. a RAID group or any other configuration of devices.

The SAN attached clustered storage system can make some or all of the storage space on the mass storage devices available to the host . For example the host can access a cluster node of the SAN attached clustered storage system using well known protocols such as Internet Small Computer System Interface iSCSI Fibre Channel Protocol FCP or Fibre Channel over Ethernet FCoE . The cluster node can present or export data stored on the mass storage devices as logical units LUNs for example to the host . A cluster node in the SAN attached clustered storage system can communicate with each other cluster node over the cluster interconnect which can be implement for example as a Gigabit Ethernet switch. In one embodiment the cluster nodes are configured as high availability pairs. However it is understood that other high availability configurations are possible.

The functional components in the S module include a SCSI target instance SCSI T that includes a SCSI engine that performs the core SCSI protocol processing. The SCSI target instance also includes functionality that allows the SCSI engine to work with other subsystems and components. The SCSI target instance interacts with peer SCSI target instances on the other cluster nodes. As described in more detail below with reference to each SCSI target instance implements one or more target sub devices which collectively form a single distributed target device such that a host connected to the SAN sees a single target device. The functional components of the S module also include a SAN manager which handles management operations in the SAN. For example the SAN manager coordinates cluster wide configuration updates. Further the functional components of the S module include a cluster interface module which implements intra cluster communication with the D module and with other S modules. Finally the functional components of the S module include a transport module that manages the FCP iSCSI or FCoE ports that connect to from the host.

In addition the storage operating system includes a set of data access components organized to provide data paths for accessing information stored on the storage devices of a node these components in combination with underlying processing hardware form a D module. To that end the data access components include for example a storage manager module a RAID system module and a storage driver system module .

The storage manager primarily manages the layout of data on the mass storage devices and serves host initiated read and write requests. The RAID system manages the storage and retrieval of information to and from the storage devices in accordance with a RAID redundancy protocol such as RAID 4 RAID 5 or RAID DP while the storage driver system implements a storage access protocol such as Small Computer System Interface SCSI or FCP. The D module also includes a cluster interface module to implement an intra cluster communication link with S modules and or other D modules.

The nodes in a cluster can cooperate through their respective cluster interface modules to provide a single file system namespace across all D modules in the cluster. Thus any S module that receives a data request can access any data container within the single file system namespace located on any D module of the cluster and the location of that data container can remain transparent to the host and its user.

The cluster interface modules and implement a protocol to communicate commands and data among the modules of cluster. Such communication can be effected by a D module exposing an application programming interface API to which an S module or another D module issues calls. To that end a cluster interface module can be organized as an encoder decoder. The encoder of for example the cluster interface on an S module can encapsulate a message as i a local procedure call LPC when communicating a file system command to a D module residing on the same node or ii a remote procedure call RPC when communicating the command to a D module residing on a remote node of the cluster. In either case the decoder of the cluster interface on the D module de encapsulates the message and processes the included command.

The D module also includes a cluster transaction manager and a cluster quorum manager . The cluster quorum manager monitors the nodes that are currently members of the cluster and maintains a list of the active and available nodes in the cluster. The cluster transaction manager provides the functionality to perform distributed operations as a single transaction that will either succeed or fail across all cluster nodes affected by the transaction. The cluster transaction manager relies on the cluster quorum manager to identify nodes that are active and available in the cluster. While the cluster transaction manager and the cluster quorum manager are shown as components of the D module in this description they can be located logically at essentially any place in the operating system. For example the operating system can include a common module shared between the S module and D module in which the cluster quorum manager and cluster transaction manager can be located.

The storage operating system includes management components which provide a path for a storage administrator to request storage management operations on the SAN attached clustered storage system. These management components are not germane to this disclosure and thus are not described in detail. However the management operations requested by a storage administrator are passed from the management module to the S module and or D module where they are processed. The management components along with underlying processing hardware form the management module .

The architecture of the SCSI target in one embodiment is based on the SCSI Architecture Model defined by T10 the SCSI standard providing body. As briefly described above the SCSI targets implement one or more target sub devices and presents a single system view of the target sub devices to the host SCSI initiator . However because of the distributed cluster model each node internally implements a SCSI target instance that cooperates with each of the other SCSI target instances in the cluster to provide a consistent and scalable cluster. The distributed SCSI target instances rely on infrastructure provided by the cluster e.g. cluster transaction manager to consistently implement SCSI semantics in each cluster node .

The SCSI engine is the core functional block of a SCSI target instance and implements among other things SCSI objects such as the target ports the SCSI target sub device s and logical sub unit s . The SCSI engine performs SCSI protocol processing functions such as for example parsing validating command descriptor blocks and parameter data implementing a generic SCSI task state machine defining SCSI objects formatting response data and selecting response and error codes based on host profiles.

As described above a target device is a distributed object that includes a set of target sub devices hosted on one or more nodes in the cluster. The target device is a representation of a storage server that stores and serves data to one or more host systems. In one embodiment the target device corresponds to a virtual server where there can be multiple virtual servers that share a single set of physical resources. The target device is distributed as the set of target sub devices such that a host accessing the system on any given node sees a consistent view of the target device. The target sub devices on each node coordinate operations using the cluster transaction manager for example to maintain consistent context information. This process is described in more detail below with reference to .

Each target sub device is multi protocol capable i.e. supports FCP iSCSI or any other SCSI transport protocol . To that end each target sub device is identified to the host based on multiple protocols. For example for a host accessing the cluster based on Fibre Channel Protocol FCP the target sub device is identified by a World Wide Node Name WWNN whereas for a host accessing the cluster based on iSCSI the target sub device is identified by an iSCSI Target Node Name e.g. an iSCSI Qualified Name IQN . In one embodiment the target sub device is also identified by a protocol agnostic identifier.

Each target sub device is associated with a set of logical target ports and contains one or more logical sub units . In one embodiment similar to the SCSI target and the target sub device one or more nodes of the cluster can each host a logical sub unit where the logical sub units collectively make up a logical unit. The logical sub units share global context information e.g. state and configuration information associated with the logical unit. The logical sub units are each associated with a task sub manager that coordinates state and configuration changes by using the cluster transaction manager to distribute changes requested at one logical sub unit to the remaining logical sub units that make up the distributed logical unit. The distributed logical unit is a representation of physical storage or an abstraction of physical storage such as a volume on which data in the cluster is stored. A collection of logical sub units distributed across multiple cluster nodes can be identified to a host by the same globally unique logical unit identifier for purposes of access by the host .

SCSI initiators e.g. host access logical sub units via logical target ports . In one embodiment multiple logical target ports can reference a single physical port on the same node. Logical target ports are associated with a physical port when the transport module in response to a command from the management module associates the identifier for the logical target port i.e. transport protocol dependent and transport protocol independent identifiers with a physical port on the node. The transport module registers the logical target port information with the SCSI target which then instantiates the logical target port . The transport module can then advertise the logical port in the SAN e.g. via Fibre Channel Fabric Login or during iSCSI discovery which enables the host to discover and connect to the logical port .

The primary function of the logical target ports is to provide routing for commands and or task management functions from the host to the appropriate logical sub unit . To this end logical target ports provide a point of access for the target sub device . Each target sub device is associated with a separate set of logical target ports . Each logical target port of the set is identified by a transport protocol dependent identifier e.g. WWPN or IQN TPG Tag and a transport protocol independent relative target port identifier RTP Id . The logical target ports are used by the SCSI engine to interface with FCP and iSCSI transport modules using the transport module . In one embodiment the transport interface is implemented as an API.

Data interface is used by the SCSI engine to send read write operations to the storage manager in the D module that hosts the physical storage where the read write operation is to take place. Data interface maps the operations requested by the SCSI engine to the format used by the cluster interface and notifies the cluster interface of the operation destination i.e. the specific D module that hosts the physical storage . The data interface also receives and interprets completion error messages from the D module . The data interface can then forward the completion error messages to the SCSI engine to determine the next steps for the read write operation.

The control interface is used by the SCSI engine to synchronize execution of SCSI semantics with corresponding SCSI engines in other cluster nodes . As briefly described above each logical sub unit is associated a task sub manager to sequence and process commands and task management requests. An example of a task management request is LOGICAL UNIT RESET which resets a logical unit to its initial power on state i.e. discards all state information and disposes all queued commands without executing them . A task management request is received at one logical sub unit but may need to be processed by all logical sub units that collectively make up the single distributed logical unit. The device sub server coordinates processing of commands and task management functions the need to be processed by each of the logical sub units such that the context information remains consistent between the logical sub units.

The control interface allows the task sub manager to communicate over the cluster interface with the cluster transaction manager . Specifically the control interface maps requests for distributed operations from the SCSI engine into transactions distributed to other instances of the distributed logical unit by the cluster transaction manager . The task sub manager uses the control interface to synchronize a set of tasks in the task sub set that affect the context information maintained by the logical sub unit . This enables each task sub manager associated with a logical unit to have a representation of a single global task set. The process of maintaining consistent context information is described in more detail below with reference to .

As described above the cluster presents a single system view of a distributed logical unit to the host such that access to a particular logical sub unit of the distributed logical unit is transparent to the host. In other words the host is not aware of the existence of the logical sub units and it appears to the host that the host is accessing a singular logical unit rather than a distributed logical unit. In one embodiment at step the S module of each cluster node instantiates a logical sub unit associated with a target sub device . The logical unit includes the task sub manager the task sub set and device sub server . In one embodiment the S module on only a subset of the cluster nodes instantiates a logical sub unit such that there may be some nodes in the cluster that do not include a target sub device or a logical sub unit associated with that target sub device.

At step the SCSI target of the S module associates each logical sub unit that is part of the single distributed logical unit with a single host visible identifier. Thus each logical sub unit is identified with a single identifier such that the logical sub units are not visible to the host a separate entities from the single distributed logical unit.

At step each logical sub unit joins a group that includes logical sub units that share the same host visible identifier. As described above a cluster transaction manager coordinates communication across the cluster. The cluster transaction manager enables each logical sub unit having the same host visible identifier to join a common group and communicate with each other logical sub unit in the group by coordinating the distribution of proposals to each member of the group.

At step the S module distributes and maintains context information for each logical sub unit that is consistent with corresponding context information of each other logical sub unit such that the logical sub units collectively make up a single distributed logical unit. The process for maintaining the context information is described in more detail below with reference to . At step the S module in each node that includes a logical sub unit generates a map that associates each logical sub unit that collectively make up the distributed logical unit with a set of storage objects that are associated with the distributed logical unit. In one embodiment the map is a database that associates a LUN or some other storage object identifier with each logical sub unit.

As described above in order to maintain a consistent view of a distributed logical unit across all of the nodes in a cluster SCSI target maintains context information for each logical sub unit that is consistent with the corresponding context information for each of the other logical sub units. SCSI target can maintain the context information in a data structure in memory for example. Such context information can include for example a data access state e.g. connected not connected an administrative state e.g. enabled disabled a SCSI reservation state and or type a power condition state e.g. active idle standby stopped etc. logical unit configuration e.g. a logical unit identifier such as a LUN Serial Number block size geometry Asymmetric Logical Unit Access ALUA non ALUA personality etc. and or logical unit metadata such as mode pages that advertise a specific logical unit behavior to the host and log pages that export various statistics associated with the logical unit to the host. The context information can be modified as a result of processing commands for example SCSI commands or calls made to APIs exposed by the SCSI target . Execution of these commands can is synchronized on each node of the cluster using the cluster transaction manager as described in more detail below.

For each command in the global task set the task sub manager that proposed the transaction to place the command in the global task set can be considered the master task sub manager. The master task sub manager is responsible for requesting that the command be placed in the task set and when the command has reached the top of the queue and is processed notifying the other task sub managers that the command has been processed.

At the master transaction sub manager requests a transaction by calling a proposal API exposed by the cluster transaction manager . The cluster transaction manager forwards the request to the Group Coordinator GC which serializes transaction requests such that a consistent task sub set is maintained by each task sub manager. If there are no other transaction requests processing the GC forwards the request to each of the Group Members GMs associated with each of the task sub managers and informs the task sub managers to add the request to the corresponding task sub set . Then at step each task sub manager adds the command to the task sub set in the order in which they are received by the GC such that each instance of the global task set remains consistent.

At step when the command requested by the master task sub manager reaches the top of the queue the master task sub manager initiates processing of the command. When the command has been processed at step the master task sub manager initiates a transaction using the cluster transaction manager to update the context information maintained for each logical sub unit. In response to receiving a confirmation from each SCSI target that the context information has been updated at step the master task sub manager removes the command from the task sub set and requests a transaction to remove the processed command from each other task sub set in the cluster so that the next command can be processed.

In an illustrative embodiment the system includes a processor subsystem that includes one or more processors. The system further includes memory a network adapter and a storage adapter all interconnected by an interconnect .

The memory illustratively comprises storage locations that are addressable by the processor s and adapters and for storing software program code and data associated with the techniques introduced here. The processor and adapters and may in turn comprise processing elements and or logic circuitry configured to execute the software code and manipulate the data structures. It will be apparent to those skilled in the art that other processing and memory implementations including various computer readable storage media may be used for storing and executing program instructions pertaining to the techniques introduced here.

The network adapter includes a plurality of physical ports such as a Fibre Channel or Ethernet port to couple the system with one or more other systems over point to point links wide area networks virtual private networks implemented over a public network Internet or a shared local area network. The network adapter thus can include the mechanical components and electrical circuitry that allows the system to connect with the cluster interconnect and or host. One or more systems can communicate with other systems over the by exchanging messages for example using packets or frames of data according to pre defined protocols.

The storage adapter cooperates with the operating system to access information on attached storage devices. The information may be stored on any type of attached array of writable storage media such as magnetic disk or tape optical disk e.g. CD ROM or DVD flash memory solid state drive SSD electronic random access memory RAM micro electro mechanical and or any other similar media adapted to store information including data and parity information. The storage adapter includes a plurality of ports having input output I O interface circuitry that couples with the disks over an I O interconnect arrangement such as a conventional high performance Fibre Channel FC link topology.

The techniques introduced above can be implemented by programmable circuitry programmed or configured by software and or firmware or they can be implemented entirely by special purpose hardwired circuitry or in a combination of such forms. Such special purpose circuitry if any can be in the form of for example one or more application specific integrated circuits ASICs programmable logic devices PLDs field programmable gate arrays FPGAs etc.

Software or firmware for use in implementing the techniques introduced here may be stored on a machine readable storage medium and may be executed by one or more general purpose or special purpose programmable microprocessors. A machine readable medium as the term is used herein includes any mechanism that can store information in a form accessible by a machine a machine may be for example a computer network device cellular phone personal digital assistant PDA manufacturing tool any device with one or more processors etc. . For example a machine accessible medium includes recordable non recordable media e.g. read only memory ROM random access memory RAM magnetic disk storage media optical storage media flash memory devices etc. etc.

The term logic as used herein can include for example special purpose hardwired circuitry software and or firmware in conjunction with programmable circuitry or a combination thereof.

Although the present invention has been described with reference to specific exemplary embodiments it will be recognized that the invention is not limited to the embodiments described but can be practiced with modification and alteration within the spirit and scope of the appended claims. Accordingly the specification and drawings are to be regarded in an illustrative sense rather than a restrictive sense.

