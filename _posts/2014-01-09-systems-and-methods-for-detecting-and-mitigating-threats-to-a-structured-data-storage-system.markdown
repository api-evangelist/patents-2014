---

title: Systems and methods for detecting and mitigating threats to a structured data storage system
abstract: Systems, methods, and computer-readable media for detecting threats on a network. In an embodiment, target network traffic being transmitted between two or more hosts is captured. The target network traffic comprises a plurality of packets, which are assembled into one or more messages. The assembled message(s) may be parsed to generate a semantic model of the target network traffic. The semantic model may comprise representation(s) of operation(s) or event(s) represented by the message(s). Score(s) for the operation(s) or event(s) may be generated using a plurality of scoring algorithms, and potential threats among the operation(s) or event(s) may be identified using the score(s).
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09185125&OS=09185125&RS=09185125
owner: DB NETWORKS, INC.
number: 09185125
owner_city: Carlsbad
owner_country: US
publication_date: 20140109
---
This application claims priority to U.S. Provisional Patent App. No. 61 751 745 filed on Jan. 11 2013 and titled System and Method for Detecting and Mitigating Threats to a Structured Data Storage System the entirety of which is hereby incorporated herein by reference. This application is also related to U.S. patent application Ser. No. 13 750 579 the 579 application filed on Jan. 25 2013 and titled Systems and Methods for Extracting Structured Application Data from a Communications Link which claims priority to U.S. Provisional Patent App. No. 61 593 075 filed on Jan. 31 2012 and titled System and Method for Extracting Structured Application Data from a Communication Link the entireties of both which are hereby incorporated herein by reference.

A portion of the disclosure of this patent document contains material which is subject to copyright protection. The copyright owner has no objection to the facsimile reproduction by anyone of the patent document or the patent disclosure as it appears in the Patent and Trademark Office patent file or records but otherwise reserves all copyright rights whatsoever.

The embodiments described herein are generally directed to the field of information technology e.g. with features of network switching routing proxying and database technologies and more particularly to the detection of security threats and breaches by analyzing traffic between database servers and their clients web servers and their clients and or in technologies other than structured data storage systems such as directory protocols Hypertext Transfer Protocol HTTP email traffic etc.

Over the last few decades structured and in particular relational database technology has become a critical component in many corporate technology initiatives. With the success of the Internet the use of database technology has exploded in many consumer and business to business applications. However with the popularity of database architectures new risks and challenges have arisen such as complex and difficult to identify performance issues and subtle gaps in security that can allow confidential data to be access by unauthorized users.

A very common practice is to use a three tiered architecture to implement applications as illustrated in . While depicts only a single web server application server database server and client it should be understood that it is common to have multiple servers and directly or indirectly connected to multiple clients . A client browser on a client provides an end user an access point to the application via one or more networks and web server . Common applications include online storefronts banking access medical records and the like. In many cases an application such as a mobile application replaces the web browser but the protocols and operations of the application are very similar. Web server parses and processes requests received from client and returns results to client over network s which may include the Internet. Application server communicatively connected to web server contains the core e.g. business logic of the application. Application server uses the resources of one or more database servers to store and query persistent state needed by the application such as account information including for example account balances purchasing information payment information shipping information and or the like. Web server application server and database server are often communicatively connected via one or more internal networks but it should be understood that they could be connected in other manners e.g. via external network s direct connection etc. .

Application server may make requests to retrieve summarize or change data stored by database server using Structured Query Language SQL . SQL is a special purpose language designed for managing sets of data stored in tables that may be related to one another using relational algebra and tuple relational calculus. It is primarily a declarative language but current commercial implementations extend it with procedural scripting elements. Application server converts HTTP requests into SQL requests that retrieve or query data.

Practical applications limit the types of operations an external user may request but ultimately the applications must generate SQL that expresses some aspect of one or more application operations to database server . Applications may generate SQL using a variety of techniques. Typically there are a set of SQL templates that are specialized with the user request data and then submitted to the database server.

A very common security flaw is for application server to allow some unanticipated portion of the external HTTP request to be aggregated with the generated SQL causing the semantics of the SQL statement to no longer match the application s intent. This may allow unauthorized extraction or modification of data on database server . This is referred to as SQL injection and can be responsible for significant data loss. An unauthorized modification to the database server state is also possible allowing an attacker to not only change the data in the database but to cause execution of arbitrary program code on database server . This code may in turn open additional security vulnerabilities in the application by providing a tunnel through security screens for the attacker to gain further unauthorized access.

Denial of service attacks may also be perpetrated on the database via direct SQL injection techniques or via more subtle parametric changes. In either case such techniques or changes can be used to cause database server to use an unusually large amount of its limited resources in too short of a timespan. Furthermore database denial of service attacks can render database server useless with only a handful of packets spread out over a long period of time. Thus since this type of attack does not require a large amount of network traffic to mount it is difficult to detect using traditional methods.

Many simple toolkits that probe for SQL injection vulnerabilities are available for free or at a cost. However available toolkits do not provide the application or its operators any direct way to detect when they are being victimized by an SQL injection attack. Thus the application will not realize it has issued hostile commands to database server and database server has no way to know when the commands that it is receiving are unauthorized.

Unauthorized access may be detected based on an access not matching the usual source e.g. location Internet Protocol IP address etc. user credentials time or date of the access and the like. Specific tables or columns in the database being accessed from an unusual source or user at an unusual time or date and or in an unusual way e.g. changing an object which is normally only read constitute another form of security threat. The response database server provides to a given request varies but aspects of the response such as the number of rows returned or an error being returned may also indicate unauthorized activity.

Existing systems that attempt to perform these functions are plagued with a very high number of false positive threat indications. This makes them unusable in practice.

Accordingly systems and methods are disclosed for detecting and mitigating unauthorized access to structured data storage and processing systems e.g. which utilize SQL for operations . In an embodiment an active or passive or inline feed of network traffic may be received. For example such an embodiment may utilize the traffic feed disclosed in the 579 Application and discussed below. The systems and methods may discover one or more database servers on one or more networks based on the received network traffic learn behaviors of application s based on the discovered database server s and evaluate ongoing usage of the application s to discover breaches or attempted breaches of server security. Any detected breaches can be reported to an operator with detailed forensic data. Alternatively or additionally the threatening activities can be blocked.

In an embodiment a method for detecting threats on a network is disclosed. The method comprises capturing target network traffic being transmitted between two or more hosts wherein the target network traffic comprises a plurality of packets and using at least one hardware processor to assemble the plurality of packets into one or more messages parse the assembled one or more messages to generate a semantic model of the target network traffic wherein the semantic model comprises one or more representations of one or more operations or events represented by the one or more messages generate one or more scores for the one or more operations or events using a plurality of scoring algorithms and identify one or more potentially threatening ones of the one or more operations or events based on the one or more scores.

In another embodiment a system for detecting threats on a network is disclosed. The system comprises at least one hardware processor and one or more executable modules that when executed by the at least one hardware processor capture target network traffic being transmitted between two or more hosts wherein the target network traffic comprises a plurality of packets assemble the plurality of packets into one or more messages parse the assembled one or more messages to generate a semantic model of the target network traffic wherein the semantic model comprises one or more representations of one or more operations or events represented by the one or more messages generate one or more scores for the one or more operations or events using a plurality of scoring algorithms and identify one or more potentially threatening ones of the one or more operations or events based on the one or more scores.

In another embodiment a non transitory computer readable medium having one or more instructions stored thereon for detecting threats on a network is disclosed. The one or more instructions when executed by a processor cause the processor to capture target network traffic being transmitted between two or more hosts wherein the target network traffic comprises a plurality of packets assemble the plurality of packets into one or more messages parse the assembled one or more messages to generate a semantic model of the target network traffic wherein the semantic model comprises one or more representations of one or more operations or events represented by the one or more messages generate one or more scores for the one or more operations or events using a plurality of scoring algorithms and identify one or more potentially threatening ones of the one or more operations or events based on the one or more scores.

In a further embodiment generating the semantic model of the target network traffic comprises generating one or more language independent representations of one or more operations or events represented by the one or more messages. Additionally each of the one or more language independent representations of one or more operations or events may identify one or more of a session a user a database server a type of operation or event a lexical structure of one or more messages associated with the operation or event a parse structure of the one or more messages associated with the operation or event a semantic structure of the one or more messages associated with the operation or event and timing data related to the operation or event.

In a further embodiment parsing the one or more messages to generate a semantic model of the target network traffic comprises lexically analyzing the assembled one or more messages into a plurality of dialect independent tokens parsing one or more sequences of the plurality of tokens into one or more parse trees comprising a plurality of parse nodes and semantically analyzing the one or more parse trees to generate one or more dialect independent semantic representations of the one or more operations or events.

In a further embodiment generating one or more scores for the one or more operations or events using a plurality of scoring algorithms comprises traversing the one or more parse trees to identify one or more operations or events generating a first score for at least one of the one or more operations or events using a first one of the plurality of scoring algorithms generating a second score for the at least one operation or event using a second one of the plurality of scoring algorithms wherein the second algorithm is different than the first algorithm and computing a total score for the at least one operation or event based at least in part on the first score and the second score.

In a further embodiment one or more representations of acceptable network traffic are received and each of one or more of the plurality of scoring algorithms are trained to score target operations or events using the one or more representations of acceptable network traffic.

In a further embodiment the one or more representations of acceptable network traffic comprise a plurality of representations of acceptable operations or events and training at least one of the one or more scoring algorithms to score target operations or events using the one or more representations of acceptable network traffic comprises parsing the plurality of representations of acceptable operations or events into a plurality of parse trees and generating a pattern matching tree that is an isomorphism between two or more of the plurality of parse trees and represents a unification of the two or more parse trees.

In a further embodiment generating one or more scores for the one or more operations or events using a plurality of scoring algorithms comprises generating a score for a target operation or event using the at least one scoring algorithm by parsing a representation of the target operation or event into a target parse tree computing a tree edit distance comprising a minimum number of edits necessary to unify the target parse tree with the pattern matching tree and based on the tree edit distance generating a scalar value indicating a probability that the target operation or event represents a malicious attack or nominal application variability.

In a further embodiment training at least one of the one or more scoring algorithms to score target operations or events using the one or more representations of acceptable network traffic comprises generating one or more profiles of normal network traffic wherein the one or more profiles of normal network traffic comprise one or more of a normal number of rows returned by an operation a normal execution time of an operation one or more normal parameter values for an operation one or more normal types of content returned by an operation e.g. to identify a return content comprising Social Security numbers and or credit card numbers as a potentially threatening operation or event a normal execution time of an operation for a certain time period e.g. certain hour s of a day certain day s of a week etc. a normal frequency of an operation for a certain time period e.g. certain hour s of a day certain day s of a week etc. an identifier of an application and a model of normal execution semantics for an operation. For example a model of normal execution semantics for an operation may be built on the specific detailed execution semantics for a database server so that access to specific objects within the database in ways at times and or with frequencies that are outside the learned behavioral norm represented by the model may be scored and or identified.

In a further embodiment training the one or more scoring algorithms comprises for each of the one or more scoring algorithms generating a model for scoring operations or events using the one or more representations of acceptable network traffic.

In a further embodiment at least one of the trained one or more scoring algorithms determines whether a structural signature of a target operation within the target network traffic matches the structural signature of an acceptable operation learned during training of the at least one scoring algorithm to generate a score for the target operation. In addition the at least one trained scoring algorithm determines a minimum edit distance between a structure of the target operation and a structure of the acceptable operation and wherein the minimum edit distance represents a minimum number of insertions required to create the structure of the target operation from the structure of the acceptable operation. The target operation may comprise a structured query language SQL statement. Furthermore the at least one trained scoring algorithm may maintain a set of one or more templates of acceptable SQL statements.

In a further embodiment at least one scoring algorithm comprises a first scoring algorithm and a second one of the plurality of scoring algorithms determines a background frequency of lexical errors within one or more acceptable operations learned during training of the first scoring algorithm identifies one or more lexical errors within a target operation within the target network traffic and computes a probability that the one or more lexical errors within the target operation are in accordance with the background frequency of lexical errors within the one or more acceptable operations learned during the training of the first scoring algorithm.

In a further embodiment at least one of the plurality of scoring algorithms searches a target operation within the target network traffic for one or more segments of structured query language SQL that potentially indicate an attack. The one or more segments of SQL may represent potentially one or more SQL injections. Alternatively or additionally the one or more segments of SQL may represent potentially one or more time consuming SQL clauses. Furthermore each of the one or more segments of SQL may be associated with one or more performance parameters and the at least one scoring algorithm may calculate an estimated performance metric for the target operation based on the one or more performance parameters associated with any of the one or more segments of SQL identified within the target operation.

In a further embodiment at least one of the plurality of scoring algorithms parses a structured query language SQL statement into a plurality of segments and determines whether the plurality of segments satisfy one or more criteria.

In a further embodiment assembling the plurality of packets into one or more messages comprises synchronizing the plurality of packets sorting each of the plurality of packets into one of two host queues according to the transmission direction of the packet processing the two host queues into a single push queue by alternately processing the packets in one of the two host queues until a packet is encountered which cannot be disposed of or the host queue is empty and then processing the packets in the other one of the two host queues until a packet is encountered that cannot be disposed of or the host queue is empty if loss of a packet is detected generating a synthetic gap packet to stand in for the lost packet and bundling packets in the single push queue into the one or more messages wherein each of the one or more messages is a request message or a response message. The synthetic gap packet may comprise an indication that it is a stand in for a lost packet.

In a further embodiment one or more identified potentially threatening operations are prevented from being performed on a database that is accessible to one of the two or more hosts.

Systems and methods are disclosed for generating a detailed semantic model or description of operations between two or more network agents. In an embodiment the disclosed systems and methods are applied to network sessions comprising device interactions that are synchronous at the application layer. This includes without limitation remote procedure calls RPCs or similar request and response interactions such as those utilizing Hypertext Transfer Protocol HTTP . In these interactions a first device transmits a request to a second device through one or more networks and the second device returns a response to the first device via the one or more networks. Both the request and the response may comprise one or more packets transmitted between the devices. The packet level flow between the request and response may overlap temporally from the perspective of either device or a network mirroring device and or may be collected from multiple points within the network architecture. In an embodiment multiple network sessions between communicating network agents may generate packets that interleave arbitrarily without affecting operation of the disclosed systems and methods.

According to an embodiment the systems and methods extract a model or description of semantic operations performed between two network agents from an imperfect copy of the network packet traffic exchanges between the network agents. This model may include without limitation raw performance data on each operation descriptive metadata e.g. query string data types data sizes etc. and or actual data. When traffic is missing out of order or the exact specification of the traffic is unknown a partial model of operations may still be generated and used at an application layer level and the framework of a session may be resynchronized based on a change in direction of data flow e.g. between request and response messages .

Database queries or operations that update the data in a database may be serviced quickly or slowly by a database server depending on the complexity of the data query or update operation the instantaneous load being experienced by the database server or by other factors which may be beyond the database server itself e.g. the storage system a varying virtual central processing unit CPU allotment etc. . In an embodiment by observing the time lag between a specific request and response using the descriptive metadata e.g. Structured Query Language SQL query string and by observing the content and format of the data itself the performance of many operational aspects of the database server can be determined in real time. In addition the nature of data and actual data being updated or retrieved is latent in the network data packets flowing bi directionally between a client system and server. By observing this traffic inappropriate attempts to extract or change parts of the database may be detected. In an embodiment semantics of the operations between a client system and server are extracted and analyzed using a copy of the existing traffic. Based on this analysis traffic may be modified to accelerate or otherwise improve performance and or mitigate against various forms of attacks.

In an embodiment a capture component is placed within a network topology such that it is exposed to traffic transmitted between the plurality of network agents to be analyzed. Observed packets may be copied and transmitted to a filter component via a series of network links and or buffer stages. The filter component may then discard packets that are not related to the network agents and or applications being analyzed. The remaining packets may be passed to a reassembly component which builds a representation of the byte stream for each network session using sequence data and other descriptive data in the packets and or the time of receipt of the packets.

Once the representation of the byte stream for a session is built by the reassembly component it may be passed to an application layer analysis component. The analysis component may unpack the contents of the byte stream into the request and response data and descriptions to generate a semantic operation model of the traffic. This semantic model may be used by an application specific component which uses the semantic model to detect security and performance issues and or mitigate detected breaches of a security policy.

It should be understood that the capture component filter component reassembly component application layer analysis component application specific component and any other components or modules discussed herein may be implemented in hardware software or both hardware and software and may be separate or integrated components. For instance the filter component reassembly component application layer analysis component and application specific components may be software modules executing on hardware of a capture device or on a separate device that is communicatively coupled to the capture device.

At the outset the layers of the Open System Interconnection OSI model will be described. The OSI model defines a networking framework to implement protocols in seven layers. A layer serves the layer above it and is served by the layer below it.

In an embodiment capture and analysis device s may not be dedicated device s and may instead be cloud instances which utilize shared resources of one or more servers. It should be understood that network agents and and capture and analysis device s may comprise any type or types of computing devices capable of wired and or wireless communication including without limitation desktop computers laptop computers tablet computers smart phones or other mobile phones servers game consoles televisions set top boxes electronic kiosks Automated Teller Machines and the like. Network agent network agent and or device s may also comprise or be communicatively coupled with one or more databases such as a MySQL Oracle IBMT Microsoft SQL Sybase Access or other types of databases including cloud based database instances. In addition while only two agents and one switch and one set of capture and analysis device s are illustrated it should be understood that the network may comprise any number of agents switches and capture and analysis devices.

Memory controller provides a path for CPU to read data from and write data to main memory via cache memory . CPU may execute a program comprising software instructions stored in main memory which implement the processes described herein.

Storage controller may be connected via bus to bus controller . Storage controller may read and write data e.g. a semantic model and program instructions to a persistent storage device via link . For example storage device may comprise a commercial one terabyte Serial Advanced Technology Attachment SATA hard drive and link may comprise a SATA II link. However it should be understood that any storage device and associated interface may be used.

Network interface controller driver controls NIC and marshals packets received on network link into packet buffers in main memory . Some packets may be discarded by a packet filter engine under the direction of capture and analysis modules . For example packet filter engine may discard packets that are not related to specific protocols of interest to the model building mechanism of modules such as administrative traffic e.g. Address Resolution Protocol ARP or other broadcasts or traffic between network agents other than those of interest. Raw packet capture module may then copy the retained packets into ingress packet buffer s used by capture and analysis modules .

Capture and analysis modules perform processing as described elsewhere herein on the ingress packet traffic placed in packet buffers to generate a semantic model of the operations taking place between network agents and . This model may be incrementally placed into model log buffers and then written by file system driver e.g. in the context of a Linux operation system an Ext4 file system driver and storage controller driver to persistent storage device .

Kernel may provide timing facilities to the capture and analysis modules so that they may interpret the packet traffic in buffers during processing . Timing facilities may include a mechanism to retrieve the current time of day at high resolution e.g. microseconds or greater . Modules may compare the time retrieved from timing facilities to timestamps written by network interface controller driver into the packets as they are received. These timestamps may be used for example to determine when expected packets are to be considered lost by the reassembly and protocol analysis code.

In an embodiment packet traffic between network agents and is copied by a network mirror or Switched Port Analyzer SPAN tap mechanism. For example a network switch may be placed in the path between network agents and such that all packets transmitted by network agent to network agent and vice versa are transmitted through switch via communication links and . In an embodiment network switch may be a Layer 2 i.e. the data link layer network switch. Switch may be configured to transmit a copy of all packets received from both network agents and via network links and respectively to capture and analysis device s via communication link . Each of the network links and or may conform to the Institute of Electrical and Electronics Engineers IEEE 802.3ab 1000BASE T Ethernet standards.

In addition one or more detectors which may be local e.g. executed on the same machine or remote to capture and analysis device e.g. executed on separate machine s communicatively connected to capture and analysis device via one or more networks may be provided. Detector s may process the output of capture and analysis device . For example detector s may utilize semantic descriptions of operations between network agents and generated by capture and analysis device to create one or more higher level models including multiple layers of higher level models and different types of higher level models e.g. models specific to a security application a performance application and or for other types of applications . Modules of capture and analysis device may interface with detector s via one or more application programming interfaces APIs .

Network agent may send an acknowledgement to network agent via link . Acknowledgement is received at switch which is on the communication path between network agents and . Switch sends a copy of acknowledgement on link to network agent and also transmits a copy of acknowledgement on link to capture and analysis device s . Acknowledgement may comprise one or more packets that indicate to network agent that request was received.

Network agent may send a response to network agent via link . Response is received at switch which sends a copy of response on link to network agent . Switch also transmits a copy of response on link to capture and analysis device s . Response comprises one or more packets that form a response to request .

Network agent may send an acknowledgement to network agent via link . Acknowledgement is received at switch which is on the communication path between network agents and . Switch sends a copy of acknowledgement on link to network agent . Switch also transmits a copy of acknowledgement on link to capture and analysis device s . Reception of acknowledgement copy by network agent completes a single application level request and response cycle that began with the transmission of request by network agent .

In an embodiment buffer engine in NIC assembles the data from MAC into representations of the packets and stores the representations in packet buffer s . Controller driver which may correspond to driver in passes the received packets stored in packet buffer through a packet filter engine . Packet filter engine may comprise or utilize instructions generated by a program which compiles an optimized packet filter from a high level network description. The resulting packet filter discards packets that are not of interest to model building process . What remains are TCP IP packets that are intended for reception by the network agents of interest e.g. network agents and and or for specific TCP ports. The filter e.g. the specific agents and or TCP ports of interest may be configured by a user of the system.

In an embodiment the filter may comprise a set of one or more specifications or criteria which may be specified via a user interface and or as text lines in a configuration file. For example a specification may include without limitation one or more IP addresses e.g. defined as singletons or ranges one or more TCP port numbers e.g. defined as singletons or ranges and or one or more Virtual Local Area Network VLAN tags. In addition each of the specifications may be positive or negative. A positive specification will keep or allow packets meeting the specification whereas a negative specification will discard or deny packets meeting the specification. Implicit specifications may also exist. For instance completely empty or non TCP packets may be discarded without an explicit specification being established. For each packet the set of specifications are processed in order until one of them matches the packet in question. Once a packet is matched to one of the specifications the action specified e.g. allow or deny is enacted. Denied packets are discarded while allowed packets are passed on to the next module in the analysis chain.

An operating system capture mechanism or facility e.g. in the case of a Linux operating system AF PACKET version 2 may copy the packets remaining after the first stage filter into raw packet buffers . Raw packet buffers may be shared with or accessible by the capture and analysis address space .

Packets placed in raw buffer by operating system capture mechanism are processed or analyzed by the programs or modules residing in the capture and analysis address space . In an embodiment the result of this analysis is a semantic model of the operations between two network agents at Layer 7 i.e. the application layer . For instance this model may describe the database operations between a database client and a database server in terms of events and their surrounding contexts.

In an embodiment illustrated in packets are processed by capture and analysis modules after they are placed in raw packet buffers by operating system capture mechanism . A second stage packet filter may be applied to discard non TCP packets that were not previously discarded by in kernel first stage filter . Filter may also discard TCP control packets e.g. packets with all flags set that are not used or are harmful to the reassembly process but can not be easily removed by first stage filter . Notably in an embodiment first stage filter is intended to run with very little state or configuration information whereas second stage filter has access to broad real time state provided by higher layers.

Examples of packets that may be harmful include those that indicate unusual or unexpected conditions in TCP state. For instance a Christmas tree packet with all control bits set may cause the internal state machine of the TCP stack to misinterpret the packet and use the data in it. This data may potentially hide an attack in a properly formatted packet received around the same time. As another example harmful packets may include a packet that duplicates the TCP sequence space of a previous packet. Sending both sets of data for processing by a higher layer would cause the higher layer to see the invalid data. Other examples of harmful packets are packets with invalid checksums or length fields. These may be misinterpreted by higher layers causing them to read un initialized storage space e.g. a buffer overrun type of attack . As yet another example packets deemed by a higher layer to not be of interest may be harmful. Such packets are identified by their source destination IP port and VLAN tuple and this identification changes dynamically. It is not practical to recompile a specific filter every time a higher layer identifies a TCP connection as uninteresting so the filtering is done in a place where dynamic state is available.

In an embodiment an Ethernet header interpreter determines the end of the Ethernet header. Ethernet header interpreter may then discard packets that are not tagged as IP unicast or VLAN e.g. according to IEEE 802.1Q . For instance multicast packets may not be of interest and can drain resources needed to handle a high load situation whereas VLAN tagged packets may need to be kept so that the underlying unicast header and other headers can be extracted from them in order to decide whether or not they should be kept. A VLAN header interpreter may extract the VLAN identifier as an identifier attribute on the final model for packets with a VLAN header. The extracted VLAN header may be used to associate a packet with a TCP connection. A TCP connection in this context may be identified by a tuple of source IP destination IP source TCP port destination TCP port VLAN identifier and or physical receive port. The use of the VLAN identifier and receive port allows the system to differentiate traffic seen on different virtual or real networks that may be using cloned identical IP configurations. VLAN header interpreter may also discard any VLAN tagged packets that are not IP.

In an embodiment an IP interpreter and reassembler which may be compliant with Request for Comments RFC 791 extracts the source address and destination address from packets and reassembles sequences of fragmented IP packets into single IP packets in IP packet buffers . Fragments of IP packets may be held in reassembly buffers until either all other fragments for the IP packet are received or a timeout occurs. If a timeout occurs all fragments for the IP packet may be discarded or alternatively assembled as incomplete and optionally marked as incomplete. A short timeout on packets held for reassembly can ensure that memory usage is kept in check in a fragmented environment with high packet loss.

Completed IP packets in IP packet buffers may be processed by a TCP header interpreter and stream reassembler which may be compliant with RFC 793 . TCP header interpreter and stream reassembler may sort IP packets into streams of data per TCP connection and data direction e.g. from agent to agent or from agent to agent and store the sorted IP packets in byte stream buffers . In other words TCP header interpreter and stream reassembler may maintain a byte stream buffer for each TCP stream direction. Out of sequence data may be held in pending data buffers . As in sequence data for a given TCP stream direction is identified it may be appended to the corresponding byte stream buffer . The data in byte steam buffers hold ordered contiguous and non duplicated payload data for each specific TCP session in each specific direction. As in order TCP data is added to a connection specific byte stream buffer a bundler may be notified. Bundler is also notified if a message boundary is detected e.g. from a control packet from a change in direction of traffic or from a timeout that indicates that no additional data has been received on a stream for a predetermined period of time .

Thus pre Layer 7 processing starts with raw Ethernet packets and ends with byte stream buffers and an event stream which describes notable events in a session. For example the notable events in a TCP session may comprise an indication that in order TCP data has been added to the byte stream buffer corresponding to the TCP session an indication that no additional data has been added after a timeout period or an indication that a TCP control message has been received which closes the session. The byte and event streams may be passed to bundler which commences the Layer 7 portion of the analysis process.

A bundle is a complete request message or a complete response message at the application layer. Bundler may use several strategies to determine the boundaries of a bundle e.g. using control packets data direction or timeouts and send a bundle of data on to the protocol analysis modules. For instance boundary determination methods may comprise one or more of the following 

In an embodiment bundler provides bundles of in sequence unidirectional application traffic and associated descriptive data to an application protocol interpreter e.g. interpreter . Bundler needs no knowledge of the application protocol specification and may pass incomplete traffic i.e. bundles with one or more regions of missing in sequence data to the application protocol interpreter if segments or packets were lost.

In an embodiment the TCP reassembly phase illustrated in comprises processing by second stage packet filter Ethernet header interpreter VLAN header interpreter IP header interpreter and reassembler and TCP header interpreter and reassembler . The arrows showing request and response data provided by the TCP reassembler to bundler represent the byte stream buffers . The full request and response data resulting from bundler comprise bundle descriptors and buffers . Bundle descriptors and buffers provide the output of bundler to the first stage of Layer 7 protocol interpretation e.g. TNS protocol interpreter in an Oracle specific context .

In the message flow illustrated in the first request segment of the request transmitted from network agent and the first segment of the acknowledgement ACK transmitted from network agent are received. Reassembly renders the payload of first segment as a stream of request data to bundler . This provision of the payload of first segment may be provided before reception of ACK or may be provided after reception of ACK which indicates that first request segment was successfully received by network agent . In addition the ACK messages may be used by the reassembler to shortcut the timeout process. For instance if an ACK message is seen for a payload packet that was not witnessed it is likely that the missing packet was lost in the capture path. In either case when bundler receives first request data there is no indication yet that the message is complete. Thus bundler queues first request data .

The second and final request segment of the request from network agent and the corresponding ACK from network agent are then received by the reassembler. The reassembler appends this second request segment in sequence to the current stream of request data to bundler and provides the payload data of second request segment to bundler . Since bundler still has no indication that the message is complete bundler queues second request data . In other words bundler appends second request data to first request data .

In the illustrated example network agent formulates a three segment response to the request from network agent . The first segment of the response from network agent and the corresponding ACK from network agent are received. The reassembler provides the payload data for first response segment to bundler . Bundler detects that the direction of traffic has changed and determines that the previous message bundle it was collating is now complete. Thus bundler sends this message bundle i.e. the full request from network agent to network agent comprising request data and to a Layer 7 protocol interpreter for further analysis.

The additional two segments and of the response from network agent to network agent and the corresponding ACK messages and are received. Second response segment and third response segment are processed into data streams and respectively and provided to bundler . Bundler collates first response data and second response data i.e. appends data and to data but does not yet pass them on to the Layer 7 protocol interpreter.

Next a first segment of a second new request from network agent to network agent and the corresponding ACK are received. The reassembler sends the request data from request segment to bundler . Bundler detects that the direction of data transmission has changed and issues the complete response i.e. comprising response data and corresponding to the first request to the Layer 7 protocol interpreter.

Bundles representing requests and responses are processed by higher level protocol processing to build a semantic model of the operations taking place between the two network agents and . While this higher level protocol processing may sometimes be described herein in the context of an Oracle client server connection it should be understood that this description is merely illustrative. The systems and methods disclosed herein may be applied to or generalized for other applications and contexts as well.

In an example embodiment specific to an Oracle client server connection a Transparent Network Substrate TNS protocol interpreter may be provided which unpacks the procedure call and response payloads and asynchronous messages from TNS wrapper structures found in bundles . TNS is a multiplexing and asynchronous message wrapper protocol used by the Oracle client server protocol. It should be understood that alternative or additional interpreters may be used for other protocols. For instance Microsoft SQL Server uses Tabular Data Stream TDS and Symmetric Multiprocessing SMP wrapper protocols which may be abstracted similarly to TNS. LDAP MySQL and Postgresql each use header wrapper protocols. In addition HTTP is a header wrapper protocol for eXtensible Markup Language XML traffic or HyperText Markup Language HTML traffic. An interpreter can be constructed for any one or more of these protocols and used as an alternative or in addition to interpreter .

In addition in an embodiment a Two Task Common TTC protocol decoder or interpreter may extract remote procedure verbs parameters and result payloads from each request bundle and response bundle. The TTC protocol provides character set and data type conversion between different characters sets or formats on a client and server.

Protocol template matching by a protocol interpreter e.g. TTC protocol template matching by TTC protocol interpreter will now be described with reference to . Messages processed by the protocol interpreter are made up of a sequence of elements e.g. RPC verbs RPC parameters RPC results etc. which are decoded by the interpreter into a data form that is useful for building a model. The transformation from elements to data is controlled by a set of attributes and or which may be specific to each element. Each message may contain a variable number of elements. For example illustrates four elements and .

A library of attribute templates may be created for each new protocol session by the protocol interpreter e.g. TNS protocol interpreter and or TTC protocol interpreter . Library may be created using pre coded knowledge of the protocol in question and may be selected as a subset of a larger library of attribute templates for example for one or more protocols available for all sessions. For a newly discovered or identified session the template library may be initially filled with a relatively small set of templates that match broad groups of protocol messages and refer to groups of more specific templates. Multiple templates in the library of attribute templates may match any given message. Thus in an embodiment templates may be ordered in the library such that more exact matches are checked by the protocol interpreter before less exact ones. A more exact match will more fully describe a message than a less exact match.

In an embodiment templates provide characterizations of negotiated data types RPC options and client server architectures. These characterizations may all be used to decode the individual fields of specific RPCs. This can be especially useful when the protocol is not fully specified or secret or when the initial negotiation for a session cannot be observed. Among other things template matching can be used to determine which side of a connection e.g. TCP connection is the client and which side of the connection is the server when the start of a communication cannot be observed.

Each template in library contains a list of one or more attributes that may be applied to elements of a message e.g. an RPC request or response message . For example a template that matches example message would apply to the elements and of message . The matching template can be used to decode message into data which is usable by model generator . Each template in library may also contain one or more references to additional templates or a reference to a list of additional templates.

In an embodiment a template may comprise a set of dynamic runtime classes e.g. written in C code . The templates or marshallers are configured to pull specific patterns of data out of the stream and compose valid data. One example is a string template which is configured to recognize a string represented by a one byte length field followed by one or more data blocks in which the last data block has a zero byte length field. Such a template can be tested by attempting to de marshal a string using the template. For example if while a reading a string the interpreter ends up attempting to read past the end of the available data in the bundle the template has failed to match. However it should be understood that this is simply one illustrative example. Other templates may fail to match for simpler reasons. For example if a high bit is never expected to be set in a specific byte location in a numeric format it may be determined that a template configured to detect a number in the numeric format has failed to match if a high bit is detected in the specific byte location.

One or more observable attributes e.g. RPC field types and common markers may be determined by direct examination of the elements. Template s may be chosen by matching one or more of their attributes to observable attributes . In other words observable attributes may be compared to the attributes of one or more templates in library to identify the best matching template s from library . Once matching template s have been identified based on attributes observed from elements other attributes may be inferred using template s .

Each template comprises a set of observable attributes. Observable attributes may be those attributes which are apparent or determinable from message e.g. from elements or already known about message . As each new template is selected for consideration in step each attribute of that template may be placed in the set of attributes to be checked or observed against message . These attributes may comprise inferred attributes i.e. attributes which may not have been determinable from message or what was previously known about message without having first identified the template comprising the inferred attributes. In step it is determined whether any attributes remain to be checked. If so an unchecked attribute is selected in step .

The template indicates to which element of the message each attribute within the template applies. In step the start of the element to which the attribute selected in step applies is located in message . The start of the element may be located by using previously validated observable or inferred attributes from the chosen template. For example the size of a previous element may be an inferred or observed attribute and this size may be used to locate the next element in the message.

In step the selected attribute e.g. attribute is checked against the located element e.g. element . If this check is successful e.g. the located element satisfies or corresponds to the selected attribute the next observable attribute in the selected template is selected and checked. The process of steps and may repeat until all observable attributes have been checked.

If in step an attribute fails to check against an element of message the process may return to step . This process may repeat until all templates in the session s library have been checked and or until it is otherwise determined that no more templates must be checked. A check may be unsuccessful for instance if the element is not present e.g. due to packet loss or due to the template not being an appropriate match for message or if the element does not fit the form of the attribute e.g. a data type or value range . Furthermore if no library template is found that successfully checks against message message may be marked as completely undecodable in step . On the other hand if all observable and or inferred attributes in a template successfully check against message the template is added to a set of matched templates or the attributes of the template are added to a set of attributes in step .

If a template is chosen for the set of matched templates in step based on matched attributes it is determined in step whether the chosen template contains an inferred attribute that references an additional set of one or more templates. For example this additional set of one or more templates may comprise more specific templates. The additional set of one or more templates is added to the template library for the session in step and the processing of message is continued in step based on the supplemented template library .

Once all templates in template library including any referenced templates added in step have been considered with respect to the elements of message message is decoded in step using one or more matched templates. Message may be decoded in step into data by applying all of the attributes e.g. observable attributes and inferred attributes from the chosen template s to the elements of message e.g. elements and . In this manner the pattern of observable attributes found in message results in the identification of a set of inferred attributes by matching the observable attributes to templates in template library that comprise both observable and inferred attributes.

All of these attributes i.e. both observable attributes and inferred attributes are applied together to message in step to generate a decoded message in step . For instance the process in step for decoding element of message comprises applying the combined observable attributes e.g. attributes and and inferred attributes e.g. attributes to element to produce data . The other elements of message i.e. elements and may be decoded in a similar manner.

Each type of attribute may imply or indicate its own form of transformation. As an illustrative non limiting example in the context of Oracle TTC protocol interpretation some examples of applicable attributes include the basic type of data e.g. string numeric date interval etc. the acceptable range of values a specific value or bit pattern e.g. an operation code the dynamic range of a value e.g. how many bits are required to represent the full range of the value how many padding bits may be included in a message and their possible values and locations the encoding of a value e.g. endianness character set bit width etc. and or the internal structure of a value e.g. simple array of characters with a single length groups of characters with a length field between each one etc. .

Some elements of a message may contain bulk data that is not of interest. Thus in an embodiment the transformation from element to data e.g. from element to data in step may involve eliding or omitting some or all of the actual data leaving only a description of the data e.g. the chosen attributes for use in building a model. The bundling mechanism described in more detail elsewhere herein ensures that the high level message boundaries are discernable even if part of a message is skipped or omitted in this fashion.

In an embodiment template library which is used to decode a message persists on a per session basis. This allows earlier messages in the session to inform the decoding of later messages in a session. This feature may be particularly critical for instance in decoding messages in a session in which the initial connection setup messages are missing.

While the embodiment illustrated in uses a TNS protocol interpreter and TTC protocol interpreter it should be understood that different interpreters e.g. for protocols other than TNS and or TTC may be used in addition to or instead of the illustrated interpreters and or a different number of interpreters may be used e.g. one two three four etc. depending on the particular protocol s being interpreted.

In an embodiment the data extracted from TNS protocol interpreter and or TTC protocol interpreter or in other contexts from one or more other interpreters may be passed to an operation filter . Operation filter may use application level semantic data to filter operations that are not of interest. Operations of interest or operations not of interest may be defined or configured by a user. As an illustrative example the application level semantic data may include a service name for a database. For instance two database instances named CRMPROD and CRMDEV may be present on or otherwise available from the same server and use the same TCP port e.g. port for RPC traffic. A user may specify that only operations involving CRMPROD are of interest or that the operations involving CRMDEV are not of interest. In either case operation filter may filter out operations involving CRMDEV from consideration prior to analysis by model generator .

At any of the interpreter or filter stages leading up to model generator e.g. stages and or processing of a bundle or group s of bundles in a session may be deferred leaving the bundle s queued until a new bundle or event is received for the session. This mechanism may be used when information from subsequent bundles may be needed by any of the stages or modules to interpret earlier bundles. For instance TTC protocol interpreter may use this queuing mechanism to defer processing of undecodable messages in a session until its template library is more refined or developed. In addition model generator may use this queuing mechanism to retain bundles while attempting to determine which side of a connection is the server and which side of the connection is the client.

Referring again to model generator uses the stream of data and events generated by one or more protocol interpreters e.g. TNS protocol interpreter and TTC protocol interpreter and in an embodiment filtered by operation filter to build an abstracted semantic traffic model of the operations taking place between network agent and network agent . Model may comprise a sequence of verbs and backing data that pertains to a single session e.g. database session . Model maintains a collection of states for each session and transaction and describes the sequence of operations applied to that state.

Additional models including multiple layers of models may be built from semantic traffic model for example by detector . The details of these higher level models may be specific to the analysis engine built to use the data of model and may vary based on the goals of the application which will utilize model . In other words different users may build different higher level models depending on the task at hand. For example for a security application a higher level model may comprise structural and parametric data that describe the normal behavior of an application and expose outlying operations that may represent attacks. As another example for a performance application the higher level model may comprise data describing the timing and size of verbs and their parameters. As a further example a database firewall may build a higher level model describing SQL statements and execution semantics surrounding them. A web application firewall WAF or WAF like system may build a higher level model from model that shows Uniform Resource Identifiers URIs and POST parameters.

Model may be built in main memory and or cache memory and written by file system driver and storage controller driver e.g. via memory controller bus controller and storage controller to persistent storage device . Specifically in an embodiment the data of model e.g. events and metadata may be queued to model log buffers which may be written to persistent storage device .

The data of model queued in model log buffers may comprise a feed that is inputted into one side of an API to be used by the specific higher level application e.g. detector providing the API to for example construct higher level models. For instance for a security application RPCs being used in monitored sessions and the parameters used in the RPCs and or SQL operations being used and the rows and columns being modified by the SQL operations may be provided from model via model log buffers to the security application via an API defined by the security application. For a performance application the types of operations being used in monitored sessions may be provided from model via model log buffers to the performance application via an API defined by the performance application. Alternatively it should be understood that the capture and analysis modules may define the API and one or more applications e.g. detector which may comprise security application s performance application s and or other types of applications may access the data of model e.g. stored in model log buffers via the API defined by capture and analysis modules .

The disclosed systems and methods may be applied to any application level protocol that is session synchronous. Such protocols include without limitation database client server protocols used by Oracle Microsoft SQL Sybase IBM DB2 PostgreSQL MySQL MongoDB and other databases. Such protocols also include non database server protocols such as HTTP HTTPS Network File System NFS Apple Filing Protocol AFP Server Message Block SMB Domain Name System DNS Simple Mail Transfer Protocol SMTP Internet Message Access Protocol IMAP Post Office Protocol POP and custom or proprietary application protocols. In addition the application protocols may be carried over transport mechanisms other than TCP over IP version 4 IPv4 including without limitation User Datagram Protocol UDP over IPv4 UDP over IP version 6 IPv6 TCP over IPv6 Remote Desktop Protocol RDP over IPv4 Internetwork Packet Exchange Sequenced Packet Exchange IPX SPX Internet Control Message Protocol ICMP over IPv4 and ICMP over IPv6. The protocols may be carried in any combination over Layer 2 bridges Network Address Translation NAT devices Virtual Private Network VPN tunnels VLAN technologies and in memory inter process communication IPC arrangements on Non Uniform Memory Access NUMA and Uniform Memory Access UMA architectures.

The disclosed systems and methods may also be applied to any packet based or stream based physical layers including arbitrary combinations of such layers within the same system. These include physical transports over any supported media including without limitation Fiber Distributed Data Interface FDDI Token Ring 100 megabit Ethernet 10 megabit Ethernet over coaxial cables 10 gigabit Ethernet and Digital Signal 1 DS1 Digital Signal 3 DS3 signaling.

The disclosed systems and methods may utilize any capture mechanism that can make copies of the traffic between network agents and provide these copies to the disclosed capture and analysis device or modules . Such capture mechanisms include without limitation electrical level taps MII proxy taps a NAT device which routes traffic between network agents and transparently captures the routed traffic a virtual SPAN or mirror facility that may be part of a Virtual Machine VM manager or hypervisor a TCP or IPC proxy running on any of the involved network agents and playback of previously captured traffic e.g. log from a storage device.

The disclosed systems and methods are not limited to analyzing traffic and building models for a single pair of network agents. Rather the systems and methods are able to simultaneously monitor many sessions between many pairs of network agents. Furthermore traffic may be captured simultaneously from a plurality of capture mechanisms in real time or from a play back. The systems and methods may differentiate between network agents based on transport addresses as well as other attributes such as MAC addresses IP addresses TCP port numbers VLAN tags application layer specific identifiers e.g. service name SID for Oracle protocols etc. and or physical ingress port tags.

It should be understood that the capture and analysis device and or mirror tap may be implemented entirely in software executing in a VM environment. The components of the system including without limitation the capture devices or mechanisms may run in a distributed fashion on a plurality of virtual or physical appliances and or operating system processes or drivers. Furthermore the systems and methods may be implemented on any operating system that supports basic networking and file system capabilities. Alternatively the systems and methods may be implemented on a physical or virtual device without an operating system e.g. incorporating required hardware drivers into an application which embodies the systems and methods itself .

Different hardware architectures may act as the base for the mirror tap or the capture and analysis device . These architectures include without limitation multiple CPU core systems and any supported network or storage peripherals and controllers which support the performance requirements of the system. Any stored program or CPU architecture e.g. Harvard CPU architecture may support the disclosed systems and methods.

The reassembly and protocol decoding or interpretation systems and methods described herein may be implemented with different layering than described. For example the Ethernet VLAN IP and or TCP reassembly modules may be a single module or entity and may not support items such as IP fragmentation or VLAN header parsing. The reassembler may use control flags e.g. ACK finish FIN reset RST etc. to help determine message boundaries and other exceptional conditions.

Semantic model may be stored on persistent storage on differing storage architectures. Such storage architectures include without limitation network file systems Storage Area Network SAN storage Redundant Array of Independent Disks RAID storage and or flash memory. Alternatively model may not be stored in persistent storage at all. Rather model may be consumed by the ultimate destination application e.g. via an API and discarded.

It should be understood that the destination application of semantic model may use model of traffic to perform other tasks than just those tasks discussed elsewhere herein. Such tasks may include without limitation informing a block proxy when to hold and when to release traffic flowing through the capture and analysis device so that it may act similarly to an Intrusion Prevention System IPS and acting as an application level proxy and modifying or locally satisfying operations for performance or security purposes e.g. to implement a database accelerator .

The disclosed systems and methods may handle extreme conditions. Such conditions may include without limitation a perfect plurality of traffic copies received due to the utilized capture architecture a perfect loss of traffic in one direction between a pair of network agents and new versions of application protocols that are completely unspecified.

In an embodiment there may be channels of communication which push data notifications indications or other information backwards down the analysis chain. Such channels may include without limitation notification from the TTC layer to the TNS layer regarding message boundaries or asynchronous signal notifications and or messages from TNS protocol interpreter to bundler and or reassemblers and or to eliminate the need for a timeout to determine the end of a message e.g. a message to bundler or reassemblers or comprising an indication that the end of the message has been determined . Such channels may be implemented to allow modules e.g. interpreters filters etc. further along the analysis chain to peek at the data and assist modules earlier in the analysis chain. For example this assistance provided by later modules to earlier modules in the analysis chain may comprise the determination of message boundaries.

In an embodiment during analysis bundler and or one or both of reassemblers and may elide blocks of data that are of no use to the application layers. The elided data may be significant in some instances and may include without limitation bulk row data and bind parameters. For example all data not required for an application at hand may be elided or redacted. The data to be elided may be predetermined e.g. by user defined parameters stored in a configuration file . For instance for a database firewall that is not processing the contents of return row data the application may elide result row payloads and or all parameter data.

In an embodiment bundler and or one or both of reassemblers and may implement a streaming protocol such that data is delivered to the protocol interpreters without the need to buffer the data or completely buffer the data.

Attributes for protocol message elements such as TTC protocol message elements may be inferred directly from clues which are intrinsic to the message or from other clues. These other clues may include without limitation known architectures and or version numbers of the network agents involved in the interaction. For example these architectures and or version numbers may be known via configuration or caching of data from a previous message or session.

In embodiments the search of attribute elements such as TTC attribute elements may be elided for a subset of one or more elements. For instance in an embodiment if clues provided from an earlier part of the connection establishment protocol indicate that certain templates are not needed they may be excluded from consideration for performance reasons. As an illustrative example certain RPC structures may never be used after a given version of an Oracle client library. Thus if the connection setup determines that a newer library version is in use the interpreters can refrain from attempting to match any templates that solely support older library versions. Additionally the results of a search for attribute elements may be cached to improve performance.

Generation of the per session template library may be informed by the results of related sessions. For example if a template library is selected for a first connection from client A to server B this previously selected library may be reused as a starting point for a second and subsequent connection from client A to server B since there may be a good chance that the second connection is from the same application as the first connection. Furthermore protocol attribute templates may be excluded or included in library based on attributes outside of the immediate protocol messages such as TNS protocol headers configuration inputs e.g. manually defined by a user IP header fields rows or bind payload data TCP header fields transport layer header fields etc.

In an embodiment additional or alternative heuristic methods than those described elsewhere herein may be used to determine at least some of the attributes of the data elements for a given message and or a set of templates that are in the scope of a particular session. For example information acquired from a session setup negotiation may be used to directly determine one or more attributes. For instance a book of templates for given server version numbers or client library versions and server types may be used to provide a starting point for the template library search. The time to search all possible combinations of templates can be significant. Thus reducing the search space can be valuable for example in terms of improving performance. In addition the disclosed bundling mechanism may be generalized and used for other purposes than those described elsewhere herein. For example the bundling mechanism may be used to determine semantics of TNS marker messages determine performance related statistics in the model builder decode row data characterize row data etc.

In an embodiment systems and methods are disclosed for detecting and mitigating unauthorized access to structured data storage or processing systems using network traffic. In an embodiment the network traffic is received from or using the systems and methods disclosed in the 579 Application and discussed above.

The disclosed systems and methods are applicable to at least systems in which some form of generated language is combined with data passed in from potentially unauthorized or compromised sources. This includes without limitation SQL database servers using any dialect of SQL with or without proprietary extensions including for example embedded relational databases other database servers with a query language component and other services containing a control language component mixed with user data such as Lightweight Directory Access Protocol LDAP HTTP Hypertext Markup Language HTML Simple Mail Transfer Protocol SMTP Internet Message Access Protocol IMAP and Post Office Protocol POP .

Initially network traffic is captured and or reassembled with inline blocking by reassembly module . As mentioned above reassembly module may be implemented using the systems and methods disclosed in the 579 Application and or described herein. For example reassembly module may comprise the bundler s and or interpreter s discussed above with respect to and or . It should be understood that reassembly module may itself capture network traffic or may receive captured traffic network from an external device or agent e.g. the capture analysis device described above with respect to for reassembly.

The reassembled network traffic is received by one or more database protocol interpreters e.g. TNS protocol interpreter TTC protocol interpreter etc. . Database protocol interpreter s parse the reassembled network traffic based on one or more database protocols to identify one or more raw database events. The one or more raw database events are then provided by protocol interpreter s to semantic traffic model generator .

Semantic traffic model generator uses the raw database events produced by database protocol interpreter s to generate a semantic traffic model which represents a model of the network traffic captured by module . In an embodiment the semantic traffic model comprises a series of abstract representations of database server operations that have been applied to the database. Each representation of an operation in the semantic traffic model may identify a session user database server type of operation and or timing data related to the operation. Semantic traffic model generator can provide inputs to a tally system log system learning system and or master scorer system each of which is described in greater detail elsewhere herein.

Events represented in the semantic traffic model are passed by semantic traffic model generator through a language processing module also referred to herein as a language system that extracts lexical syntactic and semantic data from the provided database operations e.g. SQL statements using lexical analysis module syntactic analysis module and semantic analysis module respectively each of which may be integral or external to language system . The marked up traffic may then be written to a wraparound log buffer and a summary system that keeps statistics on similar events e.g. tally system and or log system .

In an embodiment a plurality of scoring algorithms or modules are used that each look at the semantic traffic from a different perspective. Once sufficient traffic has been observed and logged learning module is able to present a subset of the traffic to each of these scoring algorithms as authorized traffic. In turn each of these scoring algorithms can build a model of the traffic in a learning phase of the algorithm according to its own perspective for the subsequent grading or scoring of the traffic in a scoring phase of the algorithm for example by master scoring module . However there may also be scoring algorithms that do not require a learning phase i.e. only comprising a scoring phase and can help detect unauthorized access even during the learning period.

After the application traffic from a database has been learned e.g. during the learning phases of the scoring algorithms discussed above master scoring module also referred to herein as the master scorer can use the scoring algorithms to evaluate all or a portion of ongoing traffic. In an embodiment each of the scoring algorithms is utilized by master scoring module to generate a set of facts that it can discern about the traffic from its unique perspective and the learned application behavior. Master scoring module can judge all of these facts received from the various analytical algorithms to make a single threat determination about each operation.

In an embodiment if the threat determination for an event e.g. score calculated by the master scoring module exceeds a threshold the event is flagged. Each event may contain forensic information supplied by all or a portion of the analytical algorithms details from the language system and or the details of the semantic traffic model itself. Each event may be logged into an internal database e.g. by master scorer such as event log for later examination and or signaled to an operator by an event notification module via a visual user interface . Alternatively or additionally each event can be sent e.g. by event notification module as one or more Syslog compliant messages to an operator specified destination. Syslog is a standard for computer message logging that permits separation of the software that generates messages from the system s that store report and or analyze them.

In an embodiment visual operator interface allows an operator to examine detailed forensics for all events as well as summaries and subsets of existing events and details of the various analytical algorithms internal models. Operator interface also provides for initiating learning and scoring phases as well as the ability to direct the analytical algorithms to learn specific instances of operations e.g. SQL statements that are authorized and or unauthorized.

Embodiments of TCP reassembly which may be performed for example by reassembly module in will now be described in detail. In an embodiment the TCP reassembly mechanism of reassembly module converts collections of passively captured IP packets from a group of capture sources into pairs of ordered and synchronized byte streams. Each byte stream represents a unidirectional TCP payload from one endpoint of a connection to the other endpoint of the connection.

The capture environment described in the 579 Application and above may comprise one or more external agents that passively sniff traffic between clients and servers e.g. client and server of on one or more external networks and send a packet by packet copy of this traffic to a monitoring device such as system . A common case illustrated in according to an embodiment is a passive network tap inserted between two hosts and with a high traffic load. Each direction of traffic flow may be at the full capacity of link between them. Thus two links and may be required to send the sniffed traffic to monitoring device which may be the same as part of or provide input to system .

In an embodiment each collection of packets a TCP stream within the sniffed traffic is identified in system by a tuple such as Realm SourceIP DestinationIP SourcePort DestinationPort or Realm Destinationlp SourceIP DestinationPort SourcePort . The SourceIP and DestinationIP are the Ipv4 or IPv6 network node addresses for the source host e.g. Host A and destination host e.g. Host B respectively. The SourcePort and DestinationPort are the TCP port numbers for the source host and destination host respectively and the Realm is an identifier that is mapped to from the tuple such as receive port identifier Virtual Local Area Network VLAN tag . Reassembly module can simultaneously process as many TCP streams as are present in the capture sources.

The application layer or monitoring application in monitoring device receives the stream data for each direction of the connection alternately with the direction changeovers synchronized to match the behavior experienced by the monitored hosts. For example if Host A sent a message AAA to Host B and Host B upon receipt of the message responded with message BBB the application layer in monitoring device would see Destination B Message AAA followed by Destination A Message BBB . These synchronization semantics at the application layer will hold regardless of the order of receipt of packets at the capture layer.

Reassembly module can receive an arbitrarily ordered collection of packets from a capture device e.g. passive tap in an embodiment via monitoring device and convert it into a group of synchronized streams. The only exception is TCP keepalive packets which duplicate data in sequence space with a valid looking payload e.g. a single byte that is not the last byte transmitted. If a keepalive packet is seen before the last payload on its connection the stream data could appear corrupted. This situation does not happen in a practical capture system and may be further mitigated with a heuristic check for this specific condition.

Reassembly module is able to perform reassembly even under the following conditions that are typically visible to the monitored systems themselves packets lost in either direction duplicated or partially duplicated packets packets received out of order packets with no payload packets with a dummy payload e.g. keepalive packets and TCP attacks in which invalid data are included in packet headers e.g. per RFC793 or any of the underlying protocol layers . It should be understood that references herein to RFC refers to the Request for Comments published by the Internet Engineering Task Force IETF and the Internet Society which are the principal technical development and standards setting bodies for the Internet. 

In an embodiment TCP reassembly may operate by sorting a collection of received packets with payload for a single TCP session into two host queues according to their starting sequence number and order of reception. For example there may be one host queue for each of hosts and depicted in wherein each queue represents one direction in a TCP session. Each queue can be associated with a push sequence number and an ACK sequence number. The push sequence number determines the highest sequence number that has been delivered to the application layer in a given direction e.g. using IEN 74 sequence space math. It should be understood that references herein to IEN refer to the Internet Experiment Notes from the series of technical publications issued by the participants of the early development work groups that created the precursors of the modern Internet. The ACK sequence number is the highest sequence number that has been acknowledged by the host receiving the data.

After adding packets to the appropriate host queue an attempt can be made to make forward progress on the connection by pushing packets off of one of the queues to the monitoring application. Packets that contain sequence space between push sequence number and the ACK sequence number are candidates for such a push. However packets may be prevented from being pushed from a queue if one or more of the following conditions are met a sequence number gap i.e. missing the next in sequence data for the direction represented by the queue no receiver acknowledgement i.e. the intended receiver of data has not yet acknowledged the data or no stream synchronization the other side must receive data before the side represented by the queue . In an embodiment mechanisms are provided that can force progress even without some conditions being met in order to handle packet loss scenarios. In these embodiments synthetic gap packets can be generated to stand in for the real data and delivered to the application layer. Such mechanisms are discussed in more detail elsewhere herein.

If there is no packet traffic pending in capture sources A and B inbound queue may be processed. On the other hand if packet traffic is present in capture sources A and B it may be merged by reassembly module onto the inbound queue until a predetermined queue size threshold e.g. of 10 000 packets is reached. Once the queue size threshold is reached processing may be forced. Such queue discipline can provide low latency when traffic is light and low overhead when traffic is heavy.

In an embodiment when inbound queue is processed all packets are removed from inbound queue and can be placed in a temporary queue for filtering. Then a TCP filter can be applied to all packets in the inbound queue to identify only those packets which contain TCP payload or control data. At this phase all non TCP packets can be discarded or sent to other protocol processing or reassembly systems or modules. In addition relevant header information can be parsed out of the Media Access Control MAC IP and TCP header fields of the identified TCP packets leaving only abstract control data and payload data in a skeletal packet structure. These packets can then be placed in demultiplexing queue .

Demultiplexer module may process the packets in demultiplexing queue each time the temporary queue of TCP filter has been fully processed. Demultiplexer module maintains a mapping between connection identifier tuples discussed above and state information. In an embodiment the state information comprises a connection state structure or queue and two host state structures or queues. The connection state structure comprises one or more states related to the overall connection and each of the two host state structures comprises one or more states related to traffic received by the associated one of the host endpoints of the connection e.g. host and respectively .

In an embodiment packets are queued to a connection state structure based on the mapping determined by demultiplexer module . Demultiplexer module also arranges for regular timing packets to be queued to each state structure as well as control packets indicating a system shutdown or flush of queues if required. After all packets from demultiplexing queue have been demultiplexed into connection queues by demultiplexer module demultiplexer module can initiate processing of all connection queues with packets in them. It should be understood that there may be a plurality of connection queues each associated with different connections between the same e.g. Host A and Host B or different host endpoints e.g. Host A and Host C not shown Host D not shown and Host E not shown etc. .

Each time connection module is activated it processes all pending packets on inbound connection queue s . Tick and control packets are handled as described below. Captured packets can be provided to both host modules and associated with the connection as either a received or a sent packet depending on the side of the connection that the particular host module is tracking. For example if a packet is sent from host to host that packet can be provided to host module as a sent packet and provided to host module as a received packet.

In an embodiment each connection module has two host modules and one to track the state of each side i.e. direction of the TCP connection. A host module e.g. either host module or provided with a sent packet can use the ACK in a packet to inform the host state of the most recent in time received data. This can be used to delay delivery until the receiver acknowledges the data. It can be considered a best effort affair. When packet data is not sorted well this aspect of the algorithm may do no good but also does not harm. A host module provided with a received packet that represents any sequence space payload SYN or FIN sorts the packets onto the host module s queue by its starting sequence number and reception order. For example host module will sort packets into host queue and host module will sort packets into host queue . Keepalives will appear as duplicate data to be discarded.

In an embodiment after each packet is presented to host modules and each host module and is then requested or otherwise caused to push its queue and respectively to push queue . For each side of the connection the host queue is processed i.e. packets are either discarded or delivered to application until a packet is encountered which cannot be disposed of. Then the other host queue is processed in a similar manner until a packet is encountered which cannot be disposed of at which point the previous host queue is processed again and so forth. For example host queue is processed until a packet is encountered which cannot be disposed of at which point host queue is processed until a packet is encountered which cannot be disposed of at which point host queue is processed again and so forth. The host queues and are processed in this manner until neither queue has made forward progress by discarding packets or delivering packets to application or until both queues are empty.

In an embodiment as the packets in host queues and are processed a series of one or more tests are applied to each packet. The test s determine whether the data should be pushed to application i.e. to the application layer discarded and or deferred. An example series of tests or rules may comprise one or more of the following 

Packets representing sequence space that passes the above tests can be removed from the host queue and put in push queue . Once all push attempts have been completed and no further progress has been made application module is allowed to process the data in push queue .

During each attempt to push packets a check can be performed to determine if conditions exist to indicate that a connection is stuck and will not make further progress by deferring action. In an embodiment such conditions may comprise one or more of the following 

In an embodiment when any one or more of these conditions are detected each host queue and is processed and if blocking conditions are met e.g. missing packet no ACK a synthetic gap packet is injected. The injected gap packet simulates the missing traffic and allows forward progress via the normal push mechanism. The gap packet can be marked so that monitoring application knows that the packet contains no valid data and represents missing sequence space.

If the sequence space on the host queues and is significantly different than the current push sequence numbers the connection has experienced a large packet loss perhaps wrapping the sequence space and the push sequence numbers on both side of the connection are jumped to just before the valid traffic. Thus application may see a gap indicating missing data. In this case the size of that gap is a small arbitrary number since the actual gap size is unknown. In an embodiment packet arrival timestamps can be used to estimate the size of the gap.

In an embodiment a count of packets received for a connection is maintained. A tick packet is received approximately once per packet capture minute which may be much faster in real time if the capture source is a stored file of packets. As each tick packet is received the packet count at the last tick is compared with the current packet count.

If there is no traffic the time span represented by a tick is considered idle. In an embodiment if five capture time minutes or other predetermined time worth of idle ticks occur the connection may be flushed via the forced progress mechanism. This acts as a failsafe for packet loss near the end of a burst of activity on the connection.

In an additional embodiment if thirty six hours or other predetermined time worth of idle ticks occur the connection is assumed to be abandoned and is flushed. Then application is notified that the connection is closed. This acts as a resource preservation mechanism to prevent memory from filling with connection state after large packet loss scenarios.

In an embodiment if a flush control packet is received by a connection a forced progress procedure is executed. This causes any pending data in host queues and to be flushed immediately into push queue with appropriate gaps as needed since it is known a priori that no more traffic will be coming down the pipeline to fill in any missing packets for which host queues or may be waiting.

Monitoring application which may comprise for example the Bundler in the 579 Application and described above may receive several notifications during the lifetime of a connection. These notifications may include for example that a new TCP connection has been identified that in sequence payload or gap traffic has been added to push queue and or that when a TCP connection will no longer receive traffic notifications a TCP connection has been closed e.g. due to control packet activity or being idle .

In an embodiment the mechanisms described above utilize a series of queues between modules that allow multiple processor cores to simultaneously handle the chain of captured traffic.

Embodiments of a semantic traffic model which may be generated for example by semantic traffic model generation module in and or model generator in will now be described in detail. It should be understood that each of the actions described in this section may be performed by semantic traffic model generation module or model generator which may be one in the same . Semantic traffic model generation module may also be referred to herein as the feed system or simply the feed. This feed system receives as input raw data and events from capture and reassembly system described above. In an embodiment feed system receives these events from the underlying capture protocol systems e.g. as described in the 579 Application and above and represented by reassembly module as direct calls with parameter data. Examples of received events may include without limitation 

In an embodiment semantic traffic model generation module uses a language system an internal database and several levels of caching to scan the input events and convert them into an abstract model of the traffic and its parameters. The output products may be cached at two major levels 

In an embodiment semantic traffic model generation module collapses redundant data into identifiers in an internal database and shared objects in runtime memory. The output of semantic traffic model generation module may be a set of in core state structures that represent the environment of the request and a set of event notifications to the modules being fed e.g. statistics or tally module logging module learning module and or scoring module .

Embodiments of language parsing and templates which may be performed and utilized for example by language and semantics module and or analysis modules and in will now be described in detail. The language processing system which may encompass modules and may be referred to herein simply as language system . It should be understood that each of the actions described in this section may be carried out by language system . In an embodiment language system analyzes statements e.g. SQL text in operations for a request and generates feed byproducts that can be used for example by tally module logging module learning module algorithm learning subsystems algorithm scoring subsystems and or scoring module . In an embodiment these byproducts may comprise 

In an embodiment language system analyzes the text of structured data access commands from lexical and syntactic points of view to produce a sequence of discrete tokens and a parse tree respectively. The semantics of parse trees may be further analyzed in multiple domain specific ways with a shared semantic analyzer which computes higher level semantic properties by analyzing parse trees. Language system may include multiple instances suited for analyzing a plurality of structured data access languages and their variants each producing tokens and a parse tree from a shared set of tokens and parse tree nodes. A specific instance of language system can be invoked by feed system with a particular input e.g. text such as SQL text .

In an embodiment language system provides a common framework for lexically and syntactically analyzing multiple structural data access languages with regular expression based lexical grammars and Look Ahead LR 1 parse grammars shared between non trivially varying dialects via term rewrite expansion. Both lexical tokens and parser productions for a set of dialects may be represented via completely shared data type definitions. This vastly simplifies clients by abstracting dialect variation. Semantic analysis may be entirely within the shared domain of parse tree nodes and vastly simplified by a top down pre order analysis over homomorphic data types representing parse nodes and parent context.

In an embodiment lexical analyzer maps input text from multiple structured data access languages to sequences of tokens from a fixed shared set of contructs. This frees abstraction clients from the lexical details of language variants.

The concrete tokens used by instances of lexical analyzer may be generated by language system from a shared set of token definitions with rewrites executed for example by the m4 macro processor an open source tool which may form a part of language system . For each of the recognized tokens e.g. of which in a current implementation there are four hundred eighty three the shared definition may comprise 

The shared token definitions may be rescanned in domain specific ways by the lexical and syntactic grammars and supporting SML components described below.

In an embodiment language system generates concrete lexical analyzers also referred to herein as lexers for language variants based on a single shared specification. This shared specification may be pre processed by the m4 macro preprocessor to produce concrete lexical analyzer specifications corresponding to the grammar required by for example the ml ulex lexical analyzer generator an open source tool . The macro definitions within the shared specification expand to handle the lexical analysis of the supported language variants while maximizing specification sharing between variants.

In an embodiment all clients of the token sequence abstraction above the generated concrete parsers of lexical analyzer utilize a single shared token representation generated for example by m4 macro expansion of a single file to produce an SML structure SQLLex . Aspects of the specification for the shared token representation may comprise one or more of the following 

In an embodiment concrete instances of lexical analyzer are generated by language system from the common specifications e.g. via m4 pre processing for a plurality of language variants or dialects e.g. postgreSQL Oracle and Microsoft SQL Server language variants . The open source ml ulex tool may be used to generate near optimal Deterministic Finite Automatons DFAs which implement the lexical analyzers for each supported structured data access language variant.

In an embodiment syntactic analyzer maps token sequences to valid concrete parse trees e.g. represented as parse nodes or a syntactic error indication based on detailed syntax rules of each supported structured data access language variant.

In an embodiment parser instances may be generated by language system from context free Look Ahead LR 1 grammars. However not all supported structured data access language variants may be capable of being fully represented this way e.g. Microsoft SQL Server . A lexical gateway may be used to augment the token sequence emitted by lexical analyzer e.g. with a single token of look ahead with a semantically equivalent stream modified as follows to allow for strict LALR 1 parsing 

In an embodiment language system implements distinct grammars for the mlyacc open source tool by expanding m4 macros from a single shared grammar file which maximizes shared definitions across all supported concrete structured data access language variants. The specification for the distinct grammars may comprise the following major sections 

In an embodiment all of the productions from the generated grammars produce concrete parse tree nodes generally fully information preserving nodes that are common to each of the structured data access language variants. Generally the common parse tree representation and other functionality free the parser clients from having to know the details of each of the language variants. Key aspects of this SML specification may comprise for example 

In an embodiment concrete instances of the parser are generated from the above m4 expanded specifications by the open source mlyacc tool for each language variant. The resulting parsers will vary by language variant but the number of generated table entries in a current implementation is in the range of twenty five thousand.

In an embodiment all functional SML level clients of the lexer and parser utilize an interface comprising one or more of the following 

In an embodiment the parse tree represented by the shared parse nodes discussed above is completely polymorphic with essentially no commonality. Clients of the parse tree can perform semantic analysis which can be accomplished via a context accumulating traversal through the nodes of the parse tree.

Much of the work associated with various domain specific semantic analyzers may be accomplished in a common node representation traversal framework which essentially transforms concrete parse tree nodes into a depth first pre order traversal through an Abstract Syntax Tree AST that is isomorphic to the parse tree generated above from structured data access language variant specific strings. In an embodiment such a traverser may utilize the following elements 

In an embodiment a plurality of domain specific semantic analyzers may be implemented. Each of the domain specific semantic analyzers may carry out one or more specific functions. Furthermore each semantic analyzer may be built using the traversal framework described above and may comprise or utilize one or more of the following functions 

In an embodiment the bulk of language system is implemented in SML and compiled to executables or dynamic libraries with the MLton open source tool. SML clients of this functionality may simply call it directly. However many clients within the disclosed database firewall may be written in the C C or other imperative programming languages. Additionally the semantics of the code generated by MLton are single threaded while the database firewall may be heavily multi threaded. Accordingly in an embodiment a generic multi threaded SML entry point framework may be provided to resolve these issues. This framework may be provided with an imperative interface to language system .

In an embodiment the code generator is a build time tool which maps general SML level interfaces to corresponding entry points e.g. C entry points . This provides the necessary utility code to ease integration e.g. with non SML based systems .

The entry point code generator may be driven by a declarative specification. This specification may comprise the following Extended Backus Naur Form EBNF grammar which specifies a mapping between SML functionality and as an example C entry points 

In an embodiment the entry point code generator processes input files satisfying the above grammar and produces one or more instances of a language specific library e.g. C library that implements the interface. This makes it very simple for programs e.g. C programs to utilize e.g. call the SML functionality.

While the code generated by the MLton open source tool is single threaded with respect to kernel threads multiple instances of SML functionality and the run time code generated above can be linked into a multi threaded program e.g. C program . This allows parallel SML execution. Accordingly in an embodiment run time support and library functionality e.g. C library is provided to implicitly or explicitly select a specific instance of the generated code and related SML dynamic library for safe parallel use from another language e.g. C thereby allowing multi instance run time. In an embodiment this interface e.g. C interface may comprise one or more of the following basic elements 

In an embodiment an interface to the SML based language system described above is provided. This interface may correspond to the following code generator specification 

Embodiments of tally system and log system illustrated in will now be described in detail. Tally system and or log system may record events that are output from feed for subsequent use by learning module and master scoring module .

In an embodiment log system maintains a large wraparound buffer mapped into memory from a file on permanent media e.g. solid state drive or other hard drive . The operating system may manage mapping the file which may be large e.g. 200 Gigabytes into a number of smaller sized in core pages using standard memory mapping facilities. Log system can use the mapping facilities to map a much smaller section of the overall log into memory. Several such sections can be mapped into memory. As each section fills up with log data the next section may be used a new section may be mapped and an old section may be released. The result in effect is a 200 Gigabyte circular buffer of structured messages. The structure of each message may include a length field which determines the start of the next record. In addition the log buffer may also contain a header region that holds an offset to the next available space e.g. the oldest record in the system or a blank space .

In an embodiment many central processing unit CPU cores may simultaneously use the log buffer. Thus entries are not maintained in strict timestamp order. A separate index may be kept in a persistent database which maps from a timestamp range to a range of offsets in the log buffer. This can be used by clients of the log system i.e. other modules to find traffic in which they are interested given a time range with a resolution of for example five minutes. It should be understood that the resolution of the time range may be any number of minutes or seconds depending on the particular design goals. To prevent too many slow accesses to the underlying database a write back cache e.g. of sixty four five minute time spans may be maintained for index entries.

The messages in the log buffer may refer to a previous message where needed to avoid duplicating data. A session creation message can contain the log offset of the connection creation message with which it is associated. A task execute message can contain the offset of the session creation message for the session with which it is associated. When a series of execution requests are chained together each one can contain a pointer to the very first execution of that request.

The pointers used by the index and the self references can contain a generation number and a byte offset in the log buffer. The generation number is used to determine if the log has wrapped around and thus that the data is no longer available for a given pointer value.

In an embodiment log system receives all feed events extracts all the unique information from them and then writes them to the next available location in the log buffer. The amount of traffic determines how much time the log buffer can represent. For example at very energetic continuous traffic rates near the top of the system s capacity a 200 Gigabyte log buffer can hold approximately seven days worth of traffic. A high level of compression can be achieved due to frequently used data e.g. SQL commands client and server IP specifications etc. being written to a separate database once e.g. the database of feed system and assigned an identifier e.g. an identifier from feed system which is only written in each individual log record. An additional measure of compression can be achieved due to later records back referencing prior records in a chain of operations rather than duplicating all the required data with every operation.

Using log system the output of feed can be reconstructed between any two logged points in time. Thus these feed outputs can drive learning module and or scoring module as if these modules were learning or scoring directly from the feed data in a live capture.

In an embodiment tally system keeps summary data for all traffic aligned for example on five minute boundaries or some other boundary duration . This summary data can be used to create summaries of traffic for learning system operating interfaces and the like. Operation of the tally system will now be described.

In an embodiment operations and events can be grouped together in tally groups based on one or more of the following 

In an embodiment learning system is responsible for coordinating learning e.g. model building for one or more algorithms e.g. analytical modules used by master scorer . In an embodiment each of the actions described in this section may be carried out by learning system . There are two primary forms of learning 1 time based and 2 event based. All learning may be specified via operator interface module . For example an operator may enter parameters and or commit events to a learning specification through one or more user interfaces provided by operator interface module . These operator specified parameters and committed events to the specification may be written to an internal database.

In time based learning an operator identifies a database and a time period or periods that are representative of normal traffic for an application. Using the operator interface provided by operation interface module a message or signal can be sent to learning system to initiate a learning cycle. Learning system can create a database transaction context which each of a plurality of scoring algorithms may use. If any algorithm s learning fails all learning can be rolled back via this transaction context. Each algorithm is then given the learning specification and calculates learned data based on the data from tally module and log module that has been stored for the relevant time period. Some algorithms may depend on calculations performed by other algorithms. Thus in an embodiment learning system executes the scoring algorithms in a specific order. For example with respect to the exemplary algorithms described in greater detail below the algorithms may be executed in the following order DS1 DS2 DS3 DS4 DS6 DS9 DS10 and DP14.

The scoring algorithms may comprise DS1 DS2 DS3 DS4 DS6 DS9 DS10 and DP14 which are described in greater detail elsewhere herein. One or more of the algorithms may be executed and generate outputs in the learning phase. For example in the illustrated embodiment DS1 is executed and produces an output ds1.statements comprising all statements seen by the database in all time intervals specified by learning specification for learned profile . In addition DS2 is executed to produce relevant bit patterns and DS3 is executed to produce an identification of a set of rules e.g. to be disabled . Notably DS4 and DS6 do not produce outputs in the illustrated learning manager run since these algorithms are only relevant to the broader context of scoring e.g. performed by master scorer . Furthermore DS9 DS10 and DP14 do not learn from the time regions or time intervals of learning specification .

In event based learning an operator may mark certain events which were previously judged as attacks as non attacks and commit these changes e.g. using one or more inputs of one or more user interfaces provided by operator interfaces module . Learning system then includes these marked and committed events in the algorithms models of acceptable traffic. Specifically each algorithm that has a learning change is notified that changes have been made to the event based learning specification s by the operator. Accordingly each algorithm may subsequently recalculate its learned state.

Embodiments and operations of master scorer module illustrated in will now be described in detail. Master scorer module coordinates operation of all the algorithms e.g. algorithms to evaluate feed events against the learned models of each algorithm.

In the illustrated embodiment master scorer module signals each scoring algorithm in turn that there is an event to be evaluated. Each algorithm then generates a group of concept scores that are dependent on the operation e.g. database operation to be evaluated described in greater detail elsewhere herein . It should be understood that the scoring algorithms may be run serially or in parallel and that some of the algorithms may be run in parallel while others are run serially. In an embodiment each of one or more of the algorithms may have access to scores from earlier runs of the same algorithm and or other algorithms such that the scoring generated by the algorithm depends and is affected by the earlier scores. For instance the illustrated algorithms may be notified by master scorer module in the following order DS1 DS2 DS3 DP14 DS4 DS6 and DS10 . In an embodiment the latter four algorithms i.e. DP14 DS4 DS6 and DS10 are only invoked for events that carry an SQL payload rather than a simple Remote Procedure Call RPC message.

An example embodiment of a scoring method performed by master scorer module will now be described. In the illustrated scoring method each algorithm provides a narrow view on the threat level of a given operation. Thus in isolation none of the algorithms may provide practical performance due to a high rate of false positives. However in the described combination the false positive rate is significantly reduced to a practically useful level without compromising detection sensitivity e.g. by combining scores output by two or more of the plurality of scoring algorithms. In the illustrated embodiment the concepts intermediate scores and final score are floating point values in the range 0 to 1 and the following operations which are binary unless otherwise noted are defined 

The illustrated scoring method follows the following logic in which a named concept from a given scorer e.g. one of algorithms is annotated as scorername.conceptname e.g. DS1.novelty and in which fuzzy logic operations are used as defined above 

At a high level the example code above calculates using an infinite valued logic system fuzzy logic the final score that the scorer will assign to events being scored. In this embodiment the final output is a threat value between zero and one where a threat value greater than 0.3 is considered an attack.

In the example above the logic blends the concept outputs from the algorithms. Each algorithm produces one or more fuzzy logic score concepts that make an assertion about some aspect of its model s analysis. For example ds1.novelty is a concept that asserts that given the specified learning and user profiles the scored statement was or was not experienced during learning. In an embodiment this ds1.novelty concept is binary e.g. its value will be either 0.0 or 1.0 with 1.0 representing that the scored statement was not learned. Other concepts such as the DS4 algorithm s concept of application variation may take on values between zero and one depending on the strength of the assertion e.g. from certain to uncertain .

In an embodiment the name of a concept or assetion implies the direction of certainty. For example a value of 1.0 for the concept isInsertion would mean that an algorithm is certain that the scored statement represents an insertion whereas a value of 0.0 would mean that the algorithm is certain that the scored statement does not represent an insertion. All of the concepts represented by the algorithms may work in this manner. For example the concept notAppVariation may assert more or less strongly that a scored event represents something that an application would not do. Given this framework the fuzzy logic and in the above code can be described as follows 

In addition to the scoring method master scorer module and or learning system may issue advisories based on scored concepts. For example concepts from DS2 DS3 and DS8 with scores greater than 0.5 or some other predetermined threshold may cause an advisory to be issued. These advisories may be delivered as events e.g. via event notification module but marked with an advisory indication. This mechanism can be used to alert an operator of potentially unsafe operations e.g. via operator interfaces module .

Embodiments and operations of an event system e.g. comprising event log module and or event notification module illustrated in will now be described in detail. In an embodiment the event system receives threat notifications from master scorer module . When a threat is signaled the event system may gather detailed forensic evidence from each of algorithms or a different set of algorithms scoring activities including the concepts generated and algorithm specific data described in greater detail elsewhere herein . The gathered forensic evidence and data surrounding the feed event itself may be logged into a database e.g. via event log module . Operator interface module may also be notified of a new event by event notification module . Thus operator interface module can display a summary message in one or more user interfaces indicating the severity of the event and or a control e.g. input frame or other display that allows an operator to inspect the forensic data. Event notification module may also send an alert notification via a SYSLOG facility to an operator defined external network entity.

Embodiments of the scoring algorithms mentioned above will now be described in detail. While one or more of these algorithms may be described as developing models of acceptable traffic in the learning phase and scoring based on whether captured traffic matches these models of acceptable traffic in the scoring phase it should be understood that in other embodiments these algorithms could alternatively develop models of suspicious traffic in the learning phase and score based on whether captured traffic matches these models of suspicious traffic in the scoring phase and vice versa.

In an embodiment the purpose of DS1 algorithm is to classify SQL statement templates as having been learned or not. An SQL statement template is identified by its structural signature. A structural signature of a statement will generally differ if the statement changes in a manner other than a change in the values of its literals.

DS1 algorithm may maintain a set of all unique SQL templates seen during a learning e.g. an execution by learning manager . For time based learning tally system may be queried for every unique statement within one or more specified time ranges for a database being learned. For event based learning SQL templates associated with operator marked events may be generated and or added to the learned set for a given database.

In step the tally is read. In step if the tally matches a database specification e.g. satisfies one or more criteria and a time range specification e.g. is within a specified time range the process proceeds to step otherwise the process returns to step . This specification check in step filters out tallies that are for databases other than the one for which learning is being performed or that are outside of the specified time range. In step an SQL identifier for an SQL template that matches the tally is added to a learned set of SQL templates.

In step it is determined whether any event specifications remain. If so the process proceeds to step otherwise the process proceeds to state in which learning is complete. In step an event specification is read and compared to a database of event specifications. In an embodiment each tally has a list of event s that it represents e.g. prepare execute fetch and or combinations thereof . Thus in step events can be filtered so that for example only events that contain an execute are scored.

In step an SQL identifier for an SQL template that matches the event i.e. an SQL identifier on which the tally is keyed is added to the learned set of SQL templates. Accordingly the output of the process in step is a learned set of SQL templates.

In an embodiment in the scoring phase DS1 algorithm marks up events with the concept DS1.novelty . This concept is 0.0 if for the event being evaluated an SQL template was found in the set of learned templates e.g. thereby indicating that the event is acceptable . On the other hand the concept is set to 1.0 if for the event being evaluated an SQL template was not found in the set of learned templates e.g. thereby indicating that the event may represent an attack .

In an embodiment DS2 algorithm and DS3 algorithm are used to detect possible attacks by looking for elements and fragments of a language e.g. SQL that are known to be used by attackers or other hackers.

For example DS2 algorithm may search incoming SQL for artifacts of an SQL injection. By way of illustration such artifacts may include SQL inside of comments multiple inline comments equality expressions e.g. 1 1 etc. The rules or criteria applied by DS2 algorithm are not attack specific and thus are much harder to fool than a typical black list expression matcher.

DS3 algorithm may search incoming SQL for segments that satisfy a set of one or more configurable and upgradable rules. In an embodiment the rules language understands the various SQL syntaxes of SQL variants. Thus the rules can be expressed in high level expressions. Each rule may also comprise or be associated with descriptions to help operators determine why the rule is important and or why a matching SQL segment is potentially dangerous. DS3 algorithm is essentially an extension and improvement of a black list expression matcher.

In an embodiment DS2 algorithm and DS3 algorithm share a number of common attributes. For instance both may operate on SQL. While neither is often definitive of an attack both provide evidence of a potential attack and clues about attack techniques. Furthermore they both can be used during learning to inform e.g. other scoring algorithms of possible attacks during scoring to influence the score and during analysis to explain what an attack might be trying to do. Both of the algorithms are able to operate regardless of errors in the incoming SQL. In addition in an embodiment for both algorithms rules that are matched during the learning phase e.g. representing normal SQL operations are automatically ignored during the scoring phase. In the analysis phase incoming SQL is rechecked by both algorithms against all of the rules for both algorithms even if the rule was disabled in the learning phase. Also an administrator can use both algorithms to improve the quality of an application s code.

However DS2 algorithm and DS3 algorithm may also differ in some respects. For example DS2 algorithm may comprise rules to identify artifacts of insertion techniques and not specific black list patterns or expressions. These rules may include a rule to identify mismatched parentheses and or quotes a rule to identify common insertion techniques such as valid SQL in comments and or a rule to identify common equality techniques such as 1 1 which can be used to invalidate comparisons.

DS3 algorithm on the other hand may comprise rules that are scripted according to what to look for and or where to look for it. Furthermore each rule may comprise or be associated with a name and or description e.g. containing the attack technique s which the rule is designed to help detect . The rules for DS3 algorithm may comprise rules to identify the use of dangerous functions often used by attackers rule s to identify erroneous operators e.g. Oracle specific operators used for Microsoft SQL Server rule s to identify privileged operations not normally allowed by applications and or rule s to identify statistical or structural information about a database that can be used by attackers. In an embodiment each rule for the DS3 algorithm comprises one or more of the following elements or attributes 

In an embodiment of the learning phase for DS2 algorithm an array of flags may be produced using the following steps 

In an embodiment scoring in DS2 algorithm is optimized to be done quickly according to the following steps 

These steps may be performed for each of a plurality of monitored databases e.g. separate database servers or separate databases on the same server and a learned set of rules can be separately stored for each of the monitored databases.

The scoring phase for DS3 algorithm may be similar to the learning phase except that only unlearned rules are examined. In other words if a match is identified for a particular rule during the learning phase DS3 algorithm will not attempt to match that rule during the scoring phase. Thus while a behavior represented by the rule may perhaps be considered suspicious in the abstract if that behavior is found to be utilized in the actual application the corresponding rule will be disregarded in the context of that application.

In an embodiment DS4 algorithm lexically examines new statements and creates artifacts representing individual concepts. In the scoring phase these concepts may be combined with concepts from other algorithms to determine whether a newly arrived statement e.g. SQL statement is likely an attack. In particular DS4 algorithm may be aimed at discovering attacks that are based on injecting further SQL into existing statements.

In an embodiment the concepts represented by the artifacts created by DS4 algorithm based on an examined new statement comprise one or more of the following 

In an embodiment DS4 algorithm operates by taking statements e.g. SQL statements and breaking them down into a series of lexemes. These lexemes are used to represent the natural grouping of characters as syntactic elements in a language such as SQL. DS4 algorithm can create either normal lexemes or raw lexemes the difference being how the literals are represented.

In an embodiment if normal lexemes are being created literals are folded into one of two reserved lexemes represented by lexeme identifiers for string and numeric literals. A first lexeme identifier would be for string literals and a second lexeme identifier would be for numeric literals. The literal lexemes are the only lexemes with a predefined value. All other lexeme values are assigned at the time that the string to be turned into lexeme s is encountered. The following table illustrates how lexeme identifiers may be assigned in one scenario 

In an embodiment if raw lexemes are being created as the raw lexemes are gathered the character s that define the start or end of a literal string are treated simply as word break characters. For example in the same scenario illustrated above identifiers for raw lexemes may be assigned as follows 

All other groups of characters may create unique lexemes. For example the statement select 1.2 foo hello from bar where foo 1.3 would turn into the following lexemes 

In an embodiment DS4 algorithm groups statements into trees by associating one statement with another statement that it may have been constructed from by only adding lexemes. For instance a first statement 3 4 5 7 8 would not be in the same group as a second statement 3 4 5 8 7 since there is no way to make one from the other simply by adding lexemes. However a third statement 3 8 4 5 7 8 would be a member of the group headed by the first statement and a fourth statement 3 8 4 6 5 7 8 would be a member of the group headed by the third statement. When grouping statements together DS4 algorithm may use folded literal lexemes.

In an embodiment of DS4 algorithm to determine how one statement is related to another statement a modified version of Myers Diff algorithm is applied to the two sets of lexemes representing the statements. The Myers Diff algorithm is described by Eugene W. Myers in his paper An O ND Difference Algorithm and Its Variations published in Algorithmica November 1986 which is hereby incorporated herein by reference. The algorithm can be modified to require that the larger statement be only insertions into the shorter statement. Use of this modified algorithm provides a means to compute the minimal number of edits and therefore to determine both the most likely statement and the most likely injection point of an attack on an existing statement.

In an embodiment of DS4 algorithm an edit distance is calculated with the primary key being the number of insertions required and the secondary key being the number of lexemes that are inserted. For instance if statement A can be created from statement B by a single insertion of ten lexemes into statement B and statement A can be created from statement C by two insertions of a single lexeme into statement C statement B is considered to have the minimal edit distance from statement A. Minimum edit distance is used to select what statement is the base or head of the group to which another statement belongs. If there is no way to create statement A from statement B by just adding lexemes then the edit distance is null.

In an embodiment DS4 algorithm begins by learning statements which may later be subject to injection attack. The learning activity can be limited to grouping statements into groups. These groups can be used to answer the group is small question. They can also be used to limit the number of statements which must be examined when looking for the statement that a new statement might be attacking.

In an embodiment of DS4 algorithm scoring initially comprises finding a victim statement. The victim statement is the learned statement with the closest edit distance to the statement being scored. If there is no victim statement a forms new group artifact is true and no other resulting artifacts have meaning. Given a statement with a closest edit distance DS4 algorithm examines the details of the diff e.g. calculated by the modified Myers Diff algorithm to create one or both of the following artifacts a fits well artifact which is generated based on the number of inserts required and an all adjacent artifact which is generated by checking to see if the lexemes which were inserted were against a literal lexeme. In addition DS4 algorithm may set a small group artifact based on the number of statements determined to be in the group headed by the victim statement.

Consider the first statement Select id from users where name joe and password xx . An example attack may involve inserting joe as the name creating the second statement Select id from users where name joe and password foo . The na ve thing would be to treat and password as a literal and remove those lexemes. This would make it such that the second statement appears to not be an insertion against the first statement.

To work around this when looking to see if one statement is an insertion on another insertion the following steps may be taken 

By doing the diff in this manner it is clear that at the spot where literals were expected what was found was joe . In an embodiment DS4 algorithm accomplishes the above by using normal lexemes with the literal lexemes elided for the statement which is being considered as a possible victim of an attack and using raw lexemes for the statement which is being scored as a possible attacker. Once the insertions have been found for the statement being scored a new set of lexemes are generated. These are generated by going through the diff output. Any runs of lexemes that were only present in the suspect statement can be turned back into character strings. These character strings can then be turned into lexemes using the normal method. This results in returning literal lexemes for SQL that has not been attacked. Any valid SQL that got subsumed by an attack will not be in the runs of lexemes that are only present in the statement being scored. A diff can then be done with the cleaned up set of lexemes and the victim statement to determine the artifacts to be returned.

When a large number of statements need to be considered either when forming groups in the learning phase or when looking at a statement during the scoring phase the time required to perform the operation can become an issue. In an embodiment one or more of the following techniques are employed to limit the number of statements which must be examined in detail 

In an embodiment the purpose of DS6 algorithm is to detect structural attacks. Specifically DS6 algorithm may identify structural SQL injection attacks that result in new structural data access statements that are distinct from those new statements commonly emitted by application code generation.

As previously described structural injection attacks occur when application level user input often but not necessarily via the web interface of an application is merged into structural data access language statements dynamically generated by an application in such a way as to form statements lexically syntactically and semantically distinct from what was intended by the application.

From the syntactic point of view the production of a distinct new structural data access statement i.e. differing in structure ignoring literal values from those previously seen in the normal operation of an application is a necessary condition for SQL injection attack. Unfortunately since application tiers can be expected to generate distinct new statements indefinitely from a wide variety of template driven code generation techniques this is not a sufficient condition. It is typical for application tiers to dynamically generate structural data access statements from complex templates instantiated from user input dynamically. Therefore the full set of syntactically distinct statements generated by the application may emerge slowly over an unknown time scale which may not even be finite.

The basic theory of operation behind DS6 algorithm is that within some syntactic contexts the evolving set of statements generated by application template expansion can be transformed into trees that are isomorphic to their parse trees and that these trees can be unified with a tree of pattern matching nodes. The evolving pattern matching tree represents the variability of the code generation template s of the application and will admit new previously unseen statements because they reflect this pattern of application variability. Conversely attacking statements arising not from application code generation but from user input e.g. SQL injection will be unlikely to unify with the tree pattern and thus will be detected.

In an embodiment the polymorphic parse tree representation produced by language system is transformed into a monomorphic representation that is isomorphic with the parse tree but with certain details flattened to ease pattern recognition. The nodes of this tree may be initially grounded to specific features of the input parse tree but can be augmented by patterns that represent variation both intrinsic to individual nodes and extrinsic to the tree structure. For any input tree a grouping can be stateles sly computed which is matched to exactly one evolving pattern. A fast ordered top down tree difference algorithm to find the minimum top down edit distance between an input ground state tree and its pattern can be employed.

This edit distance serves two functions. First the input can be unified with the pattern by interpreting the edit distance as a minimal set of pattern augmentations thereby incrementally expanding the patterns to reflect nominal application code generation. This is an example of machine learning. Second the edit distance can be transformed into an estimate of the likelihood that the input represents structural injection in the domain of the data access language versus expected application code generation variation thereby detecting injections and discriminating potential false positives.

In an embodiment the SML signature LABEL defines the intrinsic potentially comparable state of the nodes of a tree in terms of abstract comparability and equality functions. This allows for the representation of mutable but comparable label changes in contrast to non comparable label type differences.

In an embodiment the FACET signature expresses the ability to represent the detailed comparison of comparable labels in terms of abstract properties called facets. This signature supports both detailed facet wise comparison of labels and the generation of facet patterns which unify multiple comparable but non equal labels.

In an embodiment the TREE signature defines trees of labeled facetted nodes extending these base formalisms to represent top down tree to tree comparisons in terms of the insertion deletion and replacement of facetted nodes. The TREE signature defines a dynamic programming based difference computation which finds the minimum edit distance between two trees in terms of these concepts.

LABEL is an abstract SML signature for the intrinsic features of a tree node specifying the following components 

FACET is an SML signature for a specific pattern unifying feature of comparable labels. Components may include 

The TREE abstraction specifies pattern matching trees with functionality including without limitation minimal edit distance computation extrinsic inter node pattern matching and or unification in terms of the LABEL and FACET functionality discussed above. All of this functionality can be realized concretely without direct reference to LABEL or FACET functionality by the TreeFun SML functor discussed elsewhere herein. In an embodiment TREE may specify the following eliding trivial elements 

The TREE signature is implemented concretely by the TreeFun functor which takes a FACET structure which itself defines a LABEL structure as an argument i.e. completely independent of these details. A fast top down tree edit distance computation may be implemented which produces a mapping which can be efficiently unified with the source tree pattern. This process can be carried out repeatedly to implement a type of machine learning to recognize structural patterns latent in the input trees. This functionality is the core of the DS6 algorithm s syntactic pattern recognition detector. Formal SML implementations of each of the key features described in the signature plus expository text amplifying the most important features of this implementation will now be provided.

A number of simple data structures can be used to give context to functional descriptions later in this section 

The node and context types can provide fast interning of nodes and labels as integers in an atom table for portable trivial identity comparisons. In an embodiment the atom table is implemented as a hash which maps the strict equivalence features of labels and nodes to distinct integers allowing inexpensive identity comparison. The nodeAux record separates the mutable intrinsic and extrinsic pattern matching aspects of nodes from the nodes themselves. All of the mutable state may ultimately be stored off of the context object.

The following SML definitions illustrate how the above data structures can be initialized and used to implement the basic tree functionality of the signature according to an embodiment 

Nodes are initialized with ground dummy auxiliary state i.e. no pattern matching and a number of atom tables are maintained for later efficient comparison particularly in the tree difference implementation described below .

With the above definitions the features of a fast top down tree difference algorithm will now be described. An embodiment may be implemented in SML as follows in which the comments in the form of Note correspond to the subsequent exposition 

In an embodiment the computation is essentially a kind of dynamic programming that considers the replacement of a source tree node with a target tree node recursively so that only mutations at the same level from the root of the comparison down are considered. The general solution to the ordered tree difference problem is known to be at least order n given a tree of size n. The unordered tree difference problem is known to be NP complete so this simplification may be essential to making the overall approach practical.

The flow of the algorithm can be described as follows with reference to the notes embedded in the SML implementation above 

Note 1 The algorithm starts with a single call to the replacePair utility function which will determine the cost i.e. minimum edit distance and necessary augmentations to replace the source node src with the target tgt . If the associated labels are incomparable then the resulting Replace variant edit object is immediately returned as LMIncomparable with infinite cost. At the top level this indicates that the nodes are not unifiable. When called recursively below the tdd auxiliary function will optimize away from such replacements.

Note 2 In computing the labelMach if the source and target nodes are identical AND if the target is not faceted then the match is identical.

Note 3 Otherwise if the source facets are not yet cached and since identity checks failed this implies a non match then 

Note 5 Otherwise the source facets are not yet concrete and a mismatch is implied the augmentation will be computed later .

Note 6 If the source facets are already cached then a utility function matchNodeFacets is executed. Function matchNodeFacets may be defined with the following SML 

The matchNodeFacets function computes a list of mismatched facets and returns a list of non matching facets which if nil indicates a match LMMatch and if not nil indicates a mismatch LMMismatch 

Note 8 A Replace edit variant with the match and widen fields is computed. Then the DP algorithm described below recursively produces edits associated with unifying the children of the target and source nodes and assigns the produced edits as the edits field of Replace .

Note 9 The cost of the Replace edit is computed with the user defined cost function and the edit cost tuple is returned.

Note 10 The tdd function recursively analyzes the children of source and target nodes considering the various possibilities for unification. In an embodiment the implementation is broadly based on the principles of dynamic programming but has been non trivially extended 

Note 11 Like the DP algorithm the approach is to break the overall problem into a number of sub problems and to memoize the result of each sub problem to minimize time complexity. Memoization is an optimization technique that avoids repeating the calculation of results for previously processed inputs. The two dimensional array m performs this memoization. As per standard DP the dimension of the array is source size plus one by target size plus one in order to represent all left comparisons and the position to the left of the left most nodes. The type of the memoization array is tuples of cost edit list pairs.

Note 12 The add utility function takes two arguments a tuple of cost edit list pairs and a new edit which it accumulates into a new cost edit list pair.

Note 14 The non trivial utility function is a predicate that returns true if a cost edit list pair cannot be ignored either due to non zero cost or costless edits which are possible if the unification is being run simply to identify zero cost augmentations.

Note 15 The first phase of the algorithm considers the possibility that a source node might be deleted to the left of all target nodes. Since Option and Kleene source nodes admit such deletions they are considered costless. The cost edit list tuples in the zero position of the target are updated representing deletion to the left of the left most target node.

Note 16 The second phase is analogous to the first but considers target node insertions to the left of the left most source node updating the source zero memoization positions. Note that there is no source pattern to consider and thus no analogous role for source nodeType .

Note 17 The third phase is the most complex because it considers non degenerate source target pairs determining the minimum cost between replacement deletion and insertion.

Note 18 The cost of a potential replacement is just a recursive application of the replacePair function. However it may or may not be needed so it is computed lazily as a result of the local tdd function. If needed replacePair is called and if mapping is forced or replacement is non trivial returns a cost edit list pair with a single member. Otherwise the returned tuple is 0.0 nil representing no cost or edits.

Note 19 The ss value represents the potential cost of replacement but only computes replacement if the base cost previous sub problem is finite and within a user defined budget. If not then the cost is infinite so that this path will be abandoned by the DB algorithm to avoid incurring additional analytic cost .

Note 20 The is value represents the potential cost of insertion to the right of the source position. However if the source is a Plus or Kleene node there is the possibility of replacement of the source node i.e. unification because this node represents potentially more than one match. Therefore this cost is compared with the insertion cost and if lower this cost edit list pair is returned instead.

Note 21 The ds value represents the potential cost of deletion of a source node to the right of the target position. If the srcType is an Option or Kleene then this incremental cost is zero. Otherwise it is added to the base.

Note 23 Finally the source length target length member of the memoization array will contain the minimum cost edit list pairs and if non trivial an optional editSeq is returned the result of the auxiliary function tdd .

Note 24 As mentioned above the topDownDiff function involves a single call to replacePair . If the result is a Replace variant and if mapping is forced or the cost is non trivial the optional editMap is returned. Otherwise unification is trivial and NONE is returned. This is the overall result of the computation.

In an embodiment one key function of the TREE signature is given in a utility function augmentAux which takes an editMap and produces a new root node which unifies the target node or pattern into the source pattern. This guarantees that the new construct and ones like it will be recognized in the future. An implementation in SML is illustrated as follows 

Note 1 The utility function augmentAux unifies the intrinsic patterns of the source tree as required given as a replacement . This is all represented as mutable state within the context. Thus this function returns true if the extrinsic state must be modified resulting in a new tree and returns false if only intrinsic changes were made to mutable state so that the original tree can be returned.

Note 2 If the match is a mismatch then if the mismatch list is null the topDownDiff algorithm has indicated that the facet context for the source node has not yet been initialized. Thus the fact context is initialized here and then this context and the mismatching unifications are considered here. Otherwise the cached context and previously computed mismatch list are considered here.

Note 3 For each mismatch the function augmentFacets is applied. This function updates the basis with the new label and then calls the client defined augment method with the same label a concrete implementation is discussed below 

Note 5 If there are no edits then false is returned. No new tree is required since all unifications were intrinsic.

Note 6 If edits are given then the auxiliary function foldEdits is applied. This function traverses the children of the given node applying the first user defined accumulator function until the first edit is applicable. Then it traverses the edits applying the second user defined accumulator to the edit. It repeats this pattern until children or edits are exhausted and returns the final value of the accumulator. It may have a SML signature of 

Note 7 Compute a new specification. The new specification is like the original but with the map override set to false this will be in scope for the augmentTree function .

The generic LABEL signature described above which operates in terms of trees of any facetted labeled nodes per the above definitions which is isomorphic to the language system parse trees may be implemented using the elements described in the following subsections excluding trivial elements .

Data type label is a single SML data type representing all possible parse nodes. It is informally but directly presented below as SML the SQLAux components are the polymorphic parse tree nodes produced by language system 

Function mkSqlStmt generates a Tree.node for an input Tree.context and top level parse tree statement SQLAux.statement . In other words it maps concrete parse trees to isomorphic nodes which can be manipulated by the DP machinery discussed above unified in the pattern matching sense and is crucial to the system s operation as an effective injection detector. The isomorphism implemented by this function is trivial i.e. parse tree nodes map one for one to Tree.nodes except for a few crucial cases found by a combination of design and empirical evidence. This procedure may be described informally with SML source code which recursively generates a Tree.node root for a top level parse tree statement as follows 

Note 1 In most cases optional components are mapped to explicit option nodes to simplify extrinsic pattern matching in their parents i.e. a single node with or without children is always present .

Note 3 Left associative AExpr nodes are destructured as lists of expressions to give the extrinsic pattern matcher something more bushy to work on. This approach which has non trivial benefits in the overall number of false positive rejections was discovered empirically by direct experimentation with sampled applications.

Note 5 The top level result of this function is always SqlStatement with the children resulting from mkNode above.

Function hash maps each of the label variants to ordinal integers. The hash may comprise a bitwise XOR of these integers. The state associated with the variants is recursively analyzed.

Function equals provides the system equals operator since data types are eqtypes defined recursively.

Function comparable returns false if the labels are different variants. For AExpr the function also requires that the associated expression kinds match. This ensures that intrinsic matching is not attempted between dissimilar expressions. For ExecModule the function requires that module names are equal. This ensures that intrinsic matching is not attempted for argument spectra from different stored procedures. This approach was discovered empirically with actual application data.

FACET may be implemented concretely for the TREE structure. A complete implementation may be given in SML as follows 

In an embodiment the DS6 sytnactic pattern based injection detector has the following interface to the database firewall described herein and may have the following implementation that utilizes the above functionality 

function score computes a score given an integral database identifier and sqlIdent object referring to an SQL statement . The form of the sqlIdent may be represented using the following SML which reflects the fact that some statements have children e.g. a stored procedure call with an SQL text argument which are also statements and therefore must also be analyzed recursively 

The score represents the level of confidence that DS6 algorithm has in the proposition that a statement is an attack and not likely arising from application variation. In an embodiment DS6 algorithm may be implemented as follows according to an embodiment 

function createEvent given an event identifier and an sqlIdent object stores in DS6 s persistent database sufficient information to describe DS6 s attack evidence for subsequent reporting and archiving regardless of how learning sets and patterns may be modified in the future . This function may be implemented as follows 

In an embodiment a DS8 algorithm not shown is provided to test for lexical errors e.g. in SQL based operations. The DS8 algorithm may analyze the learned set from DS1 algorithm for lexical errors. The templates thus extracted can be stored in a learned set of the DS8 algorithm.

In an embodiment if the learning phase of the DS8 algorithm is initiated e.g. by an operator or automatically the DS8 algorithm iterates through each template in the learned set of templates generated by DS1 algorithm . For each template in this learned set of templates from DS1 algorithm the DS8 algorithm determines whether the template has lexical error s . If the DS8 algorithm determines that the template has lexical error s the template is stored in a learned error set by the DS8 algorithm.

In an embodiment in its scoring phase the DS8 algorithm uses the language system analysis of an SQL based event to be scored to determine if the target statement has lexical error s . The DS8 algorithm can mark up events with the concept DS8.lexicalError . If there was no lexical error detected the DS8.lexicalError concept is set to 0.0 to reflect the fact that the target statement is not likely an attack from this point of view. On the other hand if one or more lexical errors are detected the DS8 algorithm computes this quantity based on the background frequency of lexical errors within the learned set. This computation computes the conditional probability that the lexical error s observed by type might have arisen from the background error frequencies within the learned set. This probability is then returned as the value of the DS8.lexicalError concept.

In an embodiment the goal of DS9 algorithm and DS10 algorithm is to match a suspected SQL injection string with a component of an HTTP web request. If there is a positive match then the algorithms determine that the HTTP request is being used to deliver an SQL injection. The information gained from such a match can be tremendously useful.

Both algorithms may extract the same HTTP data field s but may use different sources for HTTP data. Furthermore in embodiments only DS10 connected devices e.g. web agents can perform real time response and session blocking.

In an embodiment the goal of DS9 algorithm and DS10 algorithm is to match an SQL injection string with data from an attack point. Both algorithms may accomplish this using the following steps 

In an embodiment DS10 algorithm performs a release hold calculation. Specifically DS10 algorithm may periodically receive a done event with a timestamp. DS10 algorithm can use this timestamp to calculate the point in time at which it is known that no attacks occurred and release all responses being held which occurred before that point in time.

In an embodiment DP14 algorithm detects search patterns in SQL operations that may cause a denial of service. There is no learning phase in DP14 algorithm . Rather patterns are analyzed only during the scoring phase in a stateless manner. Specifically DP14 algorithm may use the syntax analysis of language system to examine the pattern parameters to any LIKE clauses or other potentially time consuming clauses in SQL inputs. If the clause e.g. LIKE clause is detected with a leading wildcard pattern either as part of the static SQL or as part of a passed in parameter or a parameter to a function that creates a search filter e.g. to the LIKE operation a concept DP14.dosPatterns may be set 1.0. If no such patterns are detected DP14.dosPatterns is instead set to 0.0.

In an embodiment a DP15 algorithm not shown is provided that looks for denial of service and performance issues on the database by using a learned set that predicts the runtime of SQL statements.

In a learning phase the DP15 algorithm may extract performance parameters for each learned SQL operation e.g. each SQL template in the learned set of DS1 algorithm . For example these parameters may include without limitation time from execute dispatch to first response from database server e.g. in nSec number of rows returned amount of data returned in bytes and or time from execute dispatch to final result on request. For each SQL template a minimum maximum and standard deviation may be calculated for each of these parameters and this data may be stored on a per template per database basis. The average frequency of each operation may also be calculated.

In its scoring phase the performance metrics for an operation being scored may be calculated and compared by the DS15 algorithm to the stored learned per template per database performance parameters. The DS15 algorithm may then set a DP15.exceedsMargins concept based on the performance metrics. For example the DP15.exceedsMargins concept may be set to 1.0 if any of the performance metrics fall outside one standard deviation of their learned values. Otherwise the DP15.exceedsMargins concept may be set to 0.0.

In an embodiment the DS15 algorithm may use a comprehensive statistical kernel function that predicts performance metrics based on time of day location in business cycle performance of groups of operations etc. to identify distressed database situations that are not related to a specific statement.

In an embodiment a Database Firewall DBFW Web Agent is provided. The DBFW Web Agent may communicate with and receive directed actions from one or more components of system e.g. mitigation module and or master scorer . The DBFW Web Agent is a TCP proxy based application that both provides the database firewall with key information and provides the database firewall with the ability to effect the outcome of an SQL injection attack. It is considered a slave device to the database firewall which may support many such agents simultaneously. DBFW Web Agent is utilized as one of the protective components of a server based system.

The DBFW Web Agent can perform any or all of the following operations gather statistics on web traffic block or redirect individual responses to suspected attacks and or block or redirect all requests by a designated session. A DBFW Web Agent can be a stand alone device or embedded in any or all of the following system components network firewall application firewall load balancer and or application front end web server application. The web agent may monitor HTTP traffic for new servers and notify the database firewall of their existence so that monitoring blocking can be easily configured for them.

In an embodiment the DBFW Web Agent may be implemented in the form of a set of Apache2 based modules. illustrates an arrangement of components according to an embodiment. These components include a browser or other client application e.g. providing the user interface a database firewall e.g. system an Apache server a web general socket module ModWebSocket and implementation specific modules mod dbfw agt and mod websocket dbfw . Modules and represent the components of an implementation of a DBFW Web Agent module. As illustrated web traffic flows through mod dbfw aft and control traffic flows through mod websocket dbfw .

In an embodiment mod dbfw agt logically sits between browser and the application and performs the actual real time traffic monitoring. For each HTTP request received mod dbfw agt may perform the following logic 

An HTTP request can comprise various attack points which can be identified by mod dbfw agt e.g. in Step 2 above including without limitation the URL query header field and POST or PUT data e.g. via form data or in the hidden data of an HTML page . Applications perform their functions in various ways. Some applications keep application state in the query or hidden data. This state may be used in queries to the database in order to find the next step of an operation. For example some applications may record some of the optional tag information in a header field of an HTTP request into the database. Thus this tag information if unchecked represents an opening for attack. Accordingly in an embodiment mod dbfw agt identifies the tag information as a point of attack and passes this tag information to database firewall to be checked for variant information.

In an embodiment in addition to the request processing described above mod dbfw agt also performs response processing. This response processing may comprise extracting data for database firewall and or a holding operation. Specifically according to an embodiment the response processing of mod dbfw agt may comprise the following steps for each response received e.g. from an application 

From the perspective of browser or other client its request may result in any one of a plurality of configured outcomes depending on the configuration of database firewall and or the correlation determined by DS10 algorithm . An administrator can configure the appropriate directed action based on a security policy. For instance the potential directed actions may comprise one or more of the following 

In an embodiment mod websocket dbfw performs all of the communications with database firewall . For instance mod websocket dbfw may use a custom inter process communication IPC mechanism to communicate with each instance of mod dbfw agt . This IPC mechanism is illustrated in according to an embodiment. Specifically shows a multiplexing scheme where the thread running the communications to the engine of database firewall e.g. via websocket mod websocket dbfw manages and communicates with one or more DBFW agents e.g. mod dbfw agt which handle the actual interception of traffic and pending of requests. In addition the illustrated queues Q0 QN queue requests and responses regarding single requests to be scored and or analyzed. The IPC mechanism may be specific to the illustrated Apache based DBFW Web Agent with other DBFW Web Agents having other processing models with different IPC requirements.

In an embodiment to gain the full benefit from SQL injection protection an interface is provided between database firewall and a web tier monitoring point e.g. DBFW Web Agent . This web tier interface provides a means to gather web access information e.g. URL Uniform Resource Identifier URI POST parameters cookie values etc. along with optional context information e.g. sessions authenticated users etc. . This information may be used to further discriminate the activities between the application server and the database server to improve the sensitivity and reduce the false positive frequency of SQL injection detection.

In addition if the web tier device e.g. DBFW Web Agent is capable of blocking web user activity the web tier interface can be used to identify user sessions that are potentially creating an SQL injection threat.

In an embodiment the web tier interface comprises a number of RPC entry points supported by web tier agents and called dynamically by database firewall . The wire protocol for these interfaces can be encapsulated via HTTPS and comply with the JSON RPC specification. This allows easy implementation for example via one or more available libraries supplied by DB Networks of San Diego Calif. It should be understood that JSON RPC is a well known remote procedure call protocol encoded in JavaScript Object Notation.

In an embodiment in addition to the discrete synchronous RPC entry points the init call of the web tier interface enables an asynchronous stream over which high rate per request data flows to minimize overhead. Although the web tier interface is asynchronous it maintains numerous kinds of flow.

The following flow illustrates mutual authentication and that when new services are discovered their information is sent to database firewall for addition and or removal by an administrator. In the illustrated flows DBFW refers to database firewall and WTA refers to a web tier agent e.g. the DBFW Web Agent described above 

Depending on the capabilities of the web tier agent and administrator preferences for database firewall a few different traffic flow patterns can be seen. By way of illustration only three flows are illustrated that may be seen following mutual authentication simple monitoring response blocking and session blocking.

Simple Monitoring. Even if a web tier agent has no active traffic capabilities it can still feed database firewall valuable information to help in determining the existence of an attack and identifying the attacker 

Response Blocking. If the web tier agent is capable of holding messages it can be set to hold the responses for particular kinds of requests until directed to release them. This can be used to block and discard the response to an attack request. This operation may be performed even if no session is detected. For instance this may be performed in an inline proxy device e.g. proxy load balancer or firewall . When the web tier agent detects traffic it may initiate the following message flow 

Response holding i.e. holding a response until released or discarded can be used alone or in cooperation with session blocking described below.

Session Blocking. After the initial response blocking described above additional requests to a session can also be blocked before they affect the database. When the database firewall detects an evil session i.e. a session comprising a malicious request response it can block future requests on just that session using the following message flow 

Embodiments and operations of mitigation module illustrated in will now be described in detail. In an embodiment each of the actions described in this section are carried out by the mitigation module . In an embodiment three forms of attack mitigation are provided web blocking database session killing and or database inline blocking. These three forms of mitigation may be configurable by an operator on a per database basis.

In web blocking mode responses to web requests are held until they are known to not create an attack on the database. When an attack is detected data is blocked at the web tier. Furthermore future requests made by the same client session or login may be rejected. This is described above in more detail with respect to the web tier interface.

The operator may configure administrator level credentials for a database into the system. In database session killing mode when an attack is detected e.g. attacking SQL an administrative command is issued to the database server to kill the session executing the attack.

In database inline blocking mode a Layer 4 Transport Layer proxy system is used between the database server and its clients. The TCP stack in a Linux kernel may be modified to appear transparent at Layer 2 and above i.e. at the Data Link Layer and above to the database server and its clients. To the system it appears that the clients are creating a TCP connection to the proxy system and the proxy system in turn establishes a TCP connection to the database server.

In normal operation data is proxied byte for byte between the database server and the client. The capture system e.g. capture analysis device described above is configured to monitor the traffic between the client and the proxy system. When a turnaround is detected by the bundler e.g. bundler the proxy system blocks further traffic on the given TCP session and polls master scorer for adjudication of the request that was just processed. If the request is not determined to be an attack the proxy is released and the request or response flows to the server or client. On the other hand if the request is determined to be an attack the operator may configure the system such that the connection is broken or a synthetic response is returned e.g. indicating an error .

The proxy system may also be used to rewrite the SQL or parameters of the request to remove an attack but otherwise allow the request to complete. Additionally or alternatively the proxy system may be used to rewrite or limit the result rows from the database server.

An operator can physically connect the database server and its clients usually through a Layer 2 switch to ports on the proxy system. In this manner the proxy system acts as a Layer 2 proxy for any non TCP traffic and acts as a TCP back to back proxy for all TCP connections established from the client side to the server side. A Linux kernel stack may be modified such that the proxy system appears to take on the identity of the database server to the clients and take on the identity of the clients to the server at Layer 2 e.g. the MAC sublayer of the Data Link Layer and Layer 3 IP or Network Layer . This allows a simple relay or optical bypass based system at level one to provide fail safe operation. If power is lost the database server and its clients continue to operate without any reconfiguration.

Another method that may be used is to configure two VLANs and use a single port. In this configuration the same mechanism as described above applies but with no physical bypass safety. In this mode a stand alone process may monitor the health of the proxy aspect of the system and convert the kernel into a cut through Layer 2 bridge if the proxy system fails to respond to health probes.

A further method that may be used is simple IP reassignment. In this mode the proxy system is configured to look like the database server from the perspective of the clients. However from the perspective of the database server all connections appear to come from the proxy system s assigned IP address or addresses.

The system preferably includes one or more processors such as processor . Additional processors may be provided such as an auxiliary processor to manage input output an auxiliary processor to perform floating point mathematical operations a special purpose microprocessor having an architecture suitable for fast execution of signal processing algorithms e.g. digital signal processor a slave processor subordinate to the main processing system e.g. back end processor an additional microprocessor or controller for dual or multiple processor systems or a coprocessor. Such auxiliary processors may be discrete processors or may be integrated with the processor . Examples of processors which may be used with system include without limitation the Pentium processor Core i7 processor and Xeon processor all of which are available from Intel Corporation of Santa Clara Calif.

The processor is preferably connected to a communication bus . The communication bus may include a data channel for facilitating information transfer between storage and other peripheral components of the system . The communication bus further may provide a set of signals used for communication with the processor including a data bus address bus and control bus not shown . The communication bus may comprise any standard or non standard bus architecture such as for example bus architectures compliant with industry standard architecture ISA extended industry standard architecture EISA Micro Channel Architecture MCA peripheral component interconnect PCI local bus or standards promulgated by the Institute of Electrical and Electronics Engineers IEEE including IEEE 488 general purpose interface bus GPIB IEEE 696 S 100 and the like.

System preferably includes a main memory and may also include a secondary memory . The main memory provides storage of instructions and data for programs executing on the processor such as one or more of the functions and or modules discussed above. It should be understood that programs stored in the memory and executed by processor may be written and or compiled according to any suitable language including without limitation SML C C Java JavaScript Perl Visual Basic .NET and the like. The main memory is typically semiconductor based memory such as dynamic random access memory DRAM and or static random access memory SRAM . Other semiconductor based memory types include for example synchronous dynamic random access memory SDRAM Rambus dynamic random access memory RDRAM ferroelectric random access memory FRAM and the like including read only memory ROM .

The secondary memory may optionally include an internal memory and or a removable medium for example a floppy disk drive a magnetic tape drive a compact disc CD drive a digital versatile disc DVD drive other optical drive a flash memory drive etc. The removable medium is read from and or written to in a well known manner. Removable storage medium may be for example a floppy disk magnetic tape CD DVD SD card etc.

The removable storage medium is a non transitory computer readable medium having stored thereon computer executable code i.e. software and or data. The computer software or data stored on the removable storage medium is read into the system for execution by the processor .

In alternative embodiments secondary memory may include other similar means for allowing computer programs or other data or instructions to be loaded into the system . Such means may include for example an external storage medium and an interface . Examples of external storage medium may include an external hard disk drive or an external optical drive or and external magneto optical drive.

Other examples of secondary memory may include semiconductor based memory such as programmable read only memory PROM erasable programmable read only memory EPROM electrically erasable read only memory EEPROM or flash memory block oriented memory similar to EEPROM . Also included are any other removable storage media and communication interface which allow software and data to be transferred from an external medium to the system .

System may include a communication interface . The communication interface allows software and data to be transferred between system and external devices e.g. printers networks or information sources. For example computer software or executable code may be transferred to system from a network server via communication interface . Examples of communication interface include a built in network adapter network interface card NIC Personal Computer Memory Card International Association PCMCIA network card card bus network adapter wireless network adapter Universal Serial Bus USB network adapter modem a network interface card NIC a wireless data card a communications port an infrared interface an IEEE 1394 fire wire or any other device capable of interfacing system with a network or another computing device.

Communication interface preferably implements industry promulgated protocol standards such as Ethernet IEEE 802 standards Fiber Channel digital subscriber line DSL asynchronous digital subscriber line ADSL frame relay asynchronous transfer mode ATM integrated digital services network ISDN personal communications services PCS transmission control protocol Internet protocol TCP IP serial line Internet protocol point to point protocol SLIP PPP and so on but may also implement customized or non standard interface protocols as well.

Software and data transferred via communication interface are generally in the form of electrical communication signals . These signals are preferably provided to communication interface via a communication channel . In one embodiment the communication channel may be a wired or wireless network or any variety of other communication links. Communication channel carries signals and can be implemented using a variety of wired or wireless communication means including wire or cable fiber optics conventional phone line cellular phone link wireless data communication link radio frequency RF link or infrared link just to name a few.

Computer executable code i.e. computer programs or software is stored in the main memory and or the secondary memory . Computer programs can also be received via communication interface and stored in the main memory and or the secondary memory . Such computer programs when executed enable the system to perform the various functions of the present invention as previously described.

In this description the term computer readable medium is used to refer to any non transitory computer readable storage media used to provide computer executable code e.g. software and computer programs to the system . Examples of these media include main memory secondary memory including internal memory removable medium and external storage medium and any peripheral device communicatively coupled with communication interface including a network information server or other network device . These non transitory computer readable mediums are means for providing executable code programming instructions and software to the system .

In an embodiment that is implemented using software the software may be stored on a computer readable medium and loaded into the system by way of removable medium I O interface or communication interface . In such an embodiment the software is loaded into the system in the form of electrical communication signals . The software when executed by the processor preferably causes the processor to perform the inventive features and functions previously described herein.

In an embodiment I O interface provides an interface between one or more components of system and one or more input and or output devices. Example input devices include without limitation keyboards touch screens or other touch sensitive devices biometric sensing devices computer mice trackballs pen based pointing devices and the like. Examples of output devices include without limitation cathode ray tubes CRTs plasma displays light emitting diode LED displays liquid crystal displays LCDs printers vacuum florescent displays VFDs surface conduction electron emitter displays SEDs field emission displays FEDs and the like.

The system also includes optional wireless communication components that facilitate wireless communication over a voice and over a data network. The wireless communication components comprise an antenna system a radio system and a baseband system . In the system radio frequency RF signals are transmitted and received over the air by the antenna system under the management of the radio system .

In one embodiment the antenna system may comprise one or more antennae and one or more multiplexors not shown that perform a switching function to provide the antenna system with transmit and receive signal paths. In the receive path received RF signals can be coupled from a multiplexor to a low noise amplifier not shown that amplifies the received RF signal and sends the amplified signal to the radio system .

In alternative embodiments the radio system may comprise one or more radios that are configured to communicate over various frequencies. In one embodiment the radio system may combine a demodulator not shown and modulator not shown in one integrated circuit IC . The demodulator and modulator can also be separate components. In the incoming path the demodulator strips away the RF carrier signal leaving a baseband receive audio signal which is sent from the radio system to the baseband system .

If the received signal contains audio information then baseband system decodes the signal and converts it to an analog signal. Then the signal is amplified and sent to a speaker. The baseband system also receives analog audio signals from a microphone. These analog audio signals are converted to digital signals and encoded by the baseband system . The baseband system also codes the digital signals for transmission and generates a baseband transmit audio signal that is routed to the modulator portion of the radio system . The modulator mixes the baseband transmit audio signal with an RF carrier signal generating an RF transmit signal that is routed to the antenna system and may pass through a power amplifier not shown . The power amplifier amplifies the RF transmit signal and routes it to the antenna system where the signal is switched to the antenna port for transmission.

The baseband system is also communicatively coupled with the processor . The central processing unit has access to data storage areas and . The central processing unit is preferably configured to execute instructions i.e. computer programs or software that can be stored in the memory or the secondary memory . Computer programs can also be received from the baseband processor and stored in the data storage area or in secondary memory or executed upon receipt. Such computer programs when executed enable the system to perform the various functions of the present invention as previously described. For example data storage areas may include various software modules not shown .

Various embodiments may also be implemented primarily in hardware using for example components such as application specific integrated circuits ASICs or field programmable gate arrays FPGAs . Implementation of a hardware state machine capable of performing the functions described herein will also be apparent to those skilled in the relevant art. Various embodiments may also be implemented using a combination of both hardware and software.

It should be understood that system may represent the hardware components of one or more of client web server application server database server system hosts and tap and monitoring device . For example each of the modules in system may reside in one or more of a main memory and a secondary medium and be executed by one or more processors . These modules of system may all reside on one system or be distributed across a plurality of systems such that system may comprise one system or a plurality of systems . In addition modules may communicate with each other and with other modules e.g. web agent via communication interface and or antenna using standard communication protocols. It should also be understood that modules which provide a user interface e.g. operator interfaces may utilize I O interface for example to provide a display and receive input operations and or communication interface for example to serve user interfaces e.g. via web server to the browser of a user s device.

Furthermore those of skill in the art will appreciate that the various illustrative logical blocks modules circuits and method steps described in connection with the above described figures and the embodiments disclosed herein can often be implemented as electronic hardware computer software or combinations of both. To clearly illustrate this interchangeability of hardware and software various illustrative components blocks modules circuits and steps have been described above generally in terms of their functionality. Whether such functionality is implemented as hardware or software depends upon the particular application and design constraints imposed on the overall system. Skilled persons can implement the described functionality in varying ways for each particular application but such implementation decisions should not be interpreted as causing a departure from the scope of the invention. In addition the grouping of functions within a module block circuit or step is for ease of description. Specific functions or steps can be moved from one module block or circuit to another without departing from the invention.

Moreover the various illustrative logical blocks modules functions and methods described in connection with the embodiments disclosed herein can be implemented or performed with a general purpose processor a digital signal processor DSP an ASIC FPGA or other programmable logic device discrete gate or transistor logic discrete hardware components or any combination thereof designed to perform the functions described herein. A general purpose processor can be a microprocessor but in the alternative the processor can be any processor controller microcontroller or state machine. A processor can also be implemented as a combination of computing devices for example a combination of a DSP and a microprocessor a plurality of microprocessors one or more microprocessors in conjunction with a DSP core or any other such configuration.

Additionally the steps of a method or algorithm described in connection with the embodiments disclosed herein can be embodied directly in hardware in a software module executed by a processor or in a combination of the two. A software module can reside in RAM memory flash memory ROM memory EPROM memory EEPROM memory registers hard disk a removable disk a CD ROM or any other form of storage medium including a network storage medium. An exemplary storage medium can be coupled to the processor such that the processor can read information from and write information to the storage medium. In the alternative the storage medium can be integral to the processor. The processor and the storage medium can also reside in an ASIC.

Any of the software components described herein may take a variety of forms. For example a component may be a stand alone software package or it may be a software package incorporated as a tool in a larger software product. It may be downloadable from a network for example a website as a stand alone product or as an add in package for installation in an existing software application. It may also be available as a client server software application as a web enabled software application and or as a mobile application.

The above description of the disclosed embodiments is provided to enable any person skilled in the art to make or use the invention. Various modifications to these embodiments will be readily apparent to those skilled in the art and the general principles described herein can be applied to other embodiments without departing from the spirit or scope of the invention. Thus it is to be understood that the description and drawings presented herein represent a presently preferred embodiment of the invention and are therefore representative of the subject matter which is broadly contemplated by the present invention. It is further understood that the scope of the present invention fully encompasses other embodiments that may become obvious to those skilled in the art and that the scope of the present invention is accordingly not limited.

