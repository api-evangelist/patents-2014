---

title: Prioritization for cache systems
abstract: In a data storage environment, a caching system includes a synchronous I/O module to stage cache promotions, and an asynchronous I/O module to de-stage cache promotions. To reduce the likelihood that high priority cache promotions are not skipped, the caching system associations a configurable priority level with each logical storage device, and prioritizes cache promotions accordingly. Related methods are also described.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09535844&OS=09535844&RS=09535844
owner: EMC IP HOLDING COMPANY LLC
number: 09535844
owner_city: Hopkinton
owner_country: US
publication_date: 20140630
---
As is known in the art computer systems which process and store large amounts of data typically include one or more applications e.g. server applications in communication with a shared data storage system in which the data is stored. The data storage system or storage array may include one or more storage devices usually of a fairly robust nature and useful for storage spanning various temporal requirements e.g. hard disk drives HDDs . The applications perform their respective operations using the storage system. Storage systems typically include an array storage array of a plurality of storage devices with on board intelligent and communications electronics and software. Within a storage system storage devices or partitions therein are commonly identified using logical unit numbers LUNs .

To improve data read performance it is known to use a caching system or caching tier positioned between the applications and the storage array. The caching system may include large capacity Flash media devices. Applications issue read and write i.e. I O operations to the caching tier which in turn issues forwarded read and write operations to the storage array based upon the cache state and cache strategy. According to one cache strategy data is promoted from a storage device in the storage array referred to herein as a source LUN to the cache in the case of a read miss. According to another cache strategy writes issued from the application result in data being written both to the source LUN and to the cache. To improve performance cache promotions may be performed asynchronously.

It is appreciated that the I O throughput capacity commonly expressed in terms of input output operations per second iops of the storage array may exceed that of the cache e.g. Flash media . In other words source LUNs can complete read and write operations at a rate that is greater than the caching system s ability to write to flash and update cache state tables. This can result in the caching system s primary memory being exhausted. To avoid this condition backlogged cache promotions may be skipped resulting in indeterminate cache behavior.

In to one aspect a method comprises receiving a first request to read first data determining the first data is not available in a cache device e.g. a PCIe Flash memory requesting the first data from a first storage device receiving a second request to read second data determining the second data is not available in the cache device requesting the second data from a second storage device receiving the first data from the first storage device receiving the second data from the second storage device comparing a first priority level associated with the first storage device to a second priority level associated with the second storage device and after receiving the first data and the second data storing either the first data or the second data to the cache device based upon the comparing.

According to another aspect a method comprises receiving a first request to write first data writing the first data to a first storage device e.g. a PCIe Flash memory receiving a second request to write second data writing the second data to a second storage device comparing a first priority level associated with the first storage device to a second priority level associated with the second storage device and storing either the first data or the second data to the cache device based upon the comparing.

In embodiments one or more of the methods further comprise in response to receiving the first data from the first storage device allocating a first buffer in dynamic random access memory DRAM and copying the first data to the first buffer and in response to receiving the second data from the second storage device allocating a second buffer in DRAM and copying the second data to the second buffer. Copying the first and second data may be performed in a first computer processing thread and storing either the first data or the second data may be performed in a second computer processing thread. In some embodiments one or more of the methods further comprises adding a first entry to a data structure the first entry comprising a first storage locator to locate the first data in a storage array and a first buffer locator to locate the first buffer in DRAM adding a second entry to the data structure the second entry comprising a second storage locator to locate the second data in the storage array and a second buffer locator to locate the second buffer in DRAM and removing either the first entry or the second entry from the data structure based upon the first priority level and the second priority level. The storage locators may comprise logical unit numbers LUNs .

According to another aspect a system comprises a plurality of logical storage devices identified by corresponding logical unit numbers LUNs each LUN having an associated priority level a cache device and a cache driver operatively coupled to intercept requests to read data requests from one or more applications to forward the read data requests to ones of the plurality of logical storage devices to determine which read data to promote to the caching device and to prioritize the determined cache promotions based the LUN priority levels.

In some embodiments the cache driver comprises a synchronous I O module to synchronously add cache promotion information to a data structure and an asynchronous I O module to asynchronously promote data to the cache device using the data structure. The cache driver may comprise a configuration module to assign the LUN priority levels. In embodiments the cache driver is coupled to the plurality of storage devices via a Fibre Channel FC .

The drawings are not necessarily to scale or inclusive of all elements of a system emphasis instead generally being placed upon illustrating the systems and methods sought to be protected herein.

Before describing embodiments of the systems and methods sought to be protected herein some terms are explained. As used herein the term cache page refers to the smallest unit of allocation inside a cache typically a few kilobytes KB in size e.g. 8 KB . The term cache promotion is used herein to refer to the process of promoting data e.g. cache pages into a cache after they have been referenced or a change in the application access profile that begins to reference an earlier new set of data. As used herein the term working set is used to describe a collection of information that is accessed frequently by an application over a period of time.

Before describing embodiments of the systems and methods sought to be protected herein some terms are explained. The phrases computer computing system computing environment processing platform data memory and storage system and data memory and storage system environment as used herein with respect to various embodiments are intended to be broadly construed so as to encompass for example private or public cloud computing or storage systems or parts thereof as well as other types of systems comprising distributed virtual infrastructure and those not comprising virtual infrastructure.

The terms application program application program and computer application program herein refer to any type of software application including desktop applications server applications database applications and mobile applications. The terms application process and process refer to an instance of an application that is being executed within a computing environment. As used herein the terms processing thread and thread refer to a sequence of computer instructions which can execute concurrently or in parallel with one or more other such sequences.

The term memory herein refers to any type of computer memory accessed by an application using memory access programming semantics including by way of example dynamic random access memory DRAM and memory mapped files. Typically reads or writes to underlying devices is done by an operating system OS not the application. As used herein the term storage refers to any resource that is accessed by the application via input output I O device semantics such as read and write systems calls. In certain instances the same physical hardware device could be accessed by the application as either memory or as storage.

The servers are hosts configured to execute applications such as database applications and may comprise off the shelf server hardware and or software e.g. a Windows server a Sun Solaris server an HP server a Linux server etc. A storage array which may be a storage area network SAN array comprises one or more physical and or logical storage devices and may utilize storage products such as by way of example VNX and Symmetrix VMAX both commercially available from EMC Corporation of Hopkinton Mass. A variety of other storage products may be utilized to implement at least a portion of a storage array. In embodiments the storage arrays utilize fully automated storage tiering FAST technology from EMC Corporation. As is known FAST technology helps place the application data in the right storage tier based on the frequency with which data is being accessed.

In general operation a server executes an application which issues data read and write requests commands to a storage array . The storage array is configured with storage resources e.g. disk drives used to store backend data files. The storage array processes read and write commands received from the application host and in the case of a read request sends data stored thereon back to the requesting server .

The configuration module manages the caching system s configuration which may be stored within volatile memory e.g. DRAM and or non volatile memory e.g. Flash memory . In embodiments the caching system comprises a non volatile memory having a first partition to store cached data i.e. one or more of the partitions corresponds to the cache device and a second partition to store configuration information and or temporary data. The configuration module enables administrators to configure or optimize the data storage system to improve performance. For example a user may assign a cache priority level per backend storage device and or per application. Such configuration may be based on previously application usage. For example if an application is known to access a particular data set i.e. its working set is known an administrator can assign a high priority level to the source storage devices for that data set. In embodiments the caching system includes an application programming interface API to allow programmatic cache configuration.

The I O modules manage the contents of the cache device according to a defined caching policy. The synchronous I O module receives or intercepts data read and write requests from an application and processes those requests in a synchronous manner e.g. the application may block on such processing . To reduce the delay in handling application I O requests processing which can be delayed e.g. de staging writes to the cache device is performed asynchronously by the asynchronous I O module . Specific examples of synchronous and asynchronous cache processing are described below in connection with . In some embodiments the synchronous and asynchronous I O modules can execute in parallel using multithreading multiple OS level processes with inter process communication IPC or any other suitable parallel processing techniques.

The cache tables comprise one or more data structures to represent the state of the cache system. The cache tables may include for example a current state table that includes information about the current state of the cache and a staging table that includes pending cache promotion information. It will be understood that the term table is not meant to be limiting and that any suitable data structures can be used to represent the cache state including hash tables linked lists queues priority queues etc. The cache tables may be stored in volatile memory e.g. DRAM and or non volatile memory e.g. Flash . The I O modules access and modify to the cache tables in the course of the processing described below. In embodiments the synchronous I O module and the asynchronous module execute on separate processing threads and access the cache tables using a thread safe mechanism such as a mutual exclusion mechanism i.e. a mutex .

In some embodiments the application is located on separate physical and or logical hosts from the caching system e.g. caching may occur within the storage array . However it is appreciated that collocating the application and caching system may be advantageous because such a setup avoids latency associated with data traversing the network to access the data.

The cache driver comprises hardware and or software components configured to perform processing described herein below see for example . In embodiments the cache driver is provided as one or more libraries. The libraries may be grouped into one or more modules such as the modules shown in . In some embodiments the cache driver comprises a library that includes I O functions e.g. read and write . In some embodiments the cache driver transparently intercepts I O function calls issued by the application such that the application is generally unaware of cache driver s intervention. Any techniques known to those skilled in the art may be used to intercept the application I O function calls. For example many operating systems OS allow function interposition using shared libraries whereby the shared library is preloaded with an application and function implementations therein are given priority over system function implementations. Thus in some embodiments the cache driver comprises I O function names and or signatures which match those used by the standard I O system libraries e.g. the Standard C Library .

Alternatively the cache driver may comprise a library that is dynamically loadable into the OS s runtime to intercept I O function calls at the kernel level. For example the cache driver may include a Linux kernel loadable. In embodiments the cache driver includes instructions configured to be executed within kernel space and instructions configured to be executed within user space i.e. outside the kernel . In some embodiments the synchronous I O module executes in kernel space whereas other modules e.g. the asynchronous I O module execute in user space. The kernel space modules can communicate with the user space modules using any suitable inter process communication IPC techniques including shared memory pipes files sockets etc.

An illustrative storage array which may be the same or similar to one of the storage arrays shown in includes a plurality of logical storage devices . The storage array further includes one or more physical storage devices not shown and a given logical storage device corresponds to an identified portion of the physical storage devices i.e. a logical device is mapped to physical devices . The physical storage devices may include RAID disks. In embodiments the storage array comprises or is part of a SAN array operatively coupled to the host via a Fibre Channel FC network using the iSCSI protocol. The physical storage devices may correspond to physical SCSI devices or targets and the logical storage devices may be individually addressable using logical unit numbers LUNs . Thus a LUN corresponds to some portion of one or more physical disks exposed to the host as a block level data storage device. In embodiments the mapping between LUNs and disks is dynamically managed by the storage array e.g. the storage array may be capable of relocating balancing data to improve performance improve reliability reduce power consumption etc. . The host or an OS thereon maintains one or more networked filesystems corresponding to one or more LUNs and thus the applications can read and write data to the storage array using standard filesystem operations.

The host maintains a priority level for one or more of the logical storage devices . A logical storage device may have an explicit priority level e.g. a value configured by a user or an application or a default value. In embodiments the host maintains a table mapping LUNs to priority levels as shown in and described below in connection therewith. The mapping table may be maintained by the configuration module of and stored volatile memory e.g. DRAM and or within non volatile memory e.g. within the device .

Turning to an illustrative priority level table associates priority levels to logical storage devices. The table comprises a LUN column a priority level column and rows . A row indicates the priority level for a particular logical storage device identified by a LUN. In some embodiments the table is maintained and used by the caching system of in particular the table may be updated by the configuration module and accessed by the asynchronous I O module to prioritize pending cache promotions. It is understood that the data structure is merely illustrative and that any suitable data structure may be used to maintain priority levels within caching systems described herein.

Turning to an illustrative staging table which may be the same as or similar to the staging table of represents pending cache promotions within a caching system. The staging table comprises a timestamp column a LUN column a storage location column a buffer location column and rows 

A row represents a pending cache promotion and includes one or more values described herein with reference to corresponding table columns. A timestamp indicates the time the cache promotion was staged and may be used for example to identify more recently staged cache promotions. A LUN identifies the logical storage device wherein the to be promoted data is stored i.e. the source storage device . A storage location represents the location of the data within the logical storage device. In some embodiments the logical storage device is a block device e.g. a SCSI device and the storage location comprises a logical block address LBA and a transfer length as shown. In embodiments the storage location also includes the LUN and therefore the LUN column may be omitted. The LUN and the LBA may be used to uniquely identify the data within the cache e.g. it may be used to generate a cache index . In another embodiment the staging table includes a cache key column and the storage location is omitted. A buffer location represents to the location of the buffer allocated to temporarily store the data to be promoted to cache. In embodiments the buffer location comprises a virtual physical memory address and corresponding length as shown. In some embodiments the buffer location comprises a count of fixed size memory pages.

In some embodiments the synchronous I O module adds rows to the table and the asynchronous I O module accesses updates and or removes rows from the table . In some embodiments the staging table is protected by a thread safe mechanism such that the multiple threads can read write to the table in parallel. It is understood that the table structure is merely illustrative and that any suitable data structure may be used to track pending cache writes.

Referring again to in general operation the application generates data read requests and data write requests which are intercepted by the cache driver . In embodiments the application indirectly generates read and write requests by interfacing with a filesystem upon the host e.g. using file operations provided by the host s OS . A read request includes the location of the requested data within the storage array i.e. a storage location and a write request comprises a storage location along with the data to be written. The cache driver processes the request locally and or forwards the request to the storage array . The application may block on a response to the read write request and thus the any delays in processing the read write request can adversely affect the application s performance.

In response to receiving a read write request the caching driver may make a determination whether to promote the read written data to cache. Promoting data to cache involves writing to the cache device which may result in a considerable delay. To reduce the time the application may block on the read write request the caching driver synchronously stages the promotion and asynchronously de stages i.e. performs the promotion. Illustrative methods for processing a read request processing a write request intercepting a write request and de staging promotions are show in respectively and described below in connection therewith.

Alternatively the processing and decision blocks represent steps performed by functionally equivalent circuits such as a digital signal processor circuit or an application specific integrated circuit ASIC . The flow diagrams do not depict the syntax of any particular programming language but rather illustrate the functional information one of ordinary skill in the art requires to fabricate circuits or to generate computer software to perform the processing required of the particular apparatus. It should be noted that many routine program elements such as initialization of loops and variables and the use of temporary variables are not shown. It will be appreciated by those of ordinary skill in the art that unless otherwise indicated herein the particular sequence of blocks described is illustrative only and can be varied without departing from the spirit of the systems and methods sought to be protected herein. Thus unless otherwise stated the blocks described below are unordered meaning that when possible the functions represented by the blocks can be performed in any convenient or desirable order.

Referring to an illustrative method may correspond to processing performed within the caching driver when intercepting a data read request. In embodiments the synchronous I O module is configured to perform the method . At block a request to read data is received from an application and at block a determination is made whether the requested data is available within the cache i.e. within the cache device . In this context available means that the requested data exists in the cache and that the cached data is consistent with the source storage device. In embodiments this determination is based on the current state table . In some embodiments cache driver derives a cache key e.g. from a LUN and a LBA specified in the read request used to index into the current state table

If the requested data is available in the cache i.e. a read hit processing continues to block where the data is read from the cache device and at block the cached data is returned to the application . Thus in this case the read request is processed entirely within the host and the cost e.g. latency of reading data from the storage array is not incurred.

If the data is not available in the cache i.e. a read miss processing continues to block where the read request is forwarded to a storage array i.e. a logical storage device identified by a LUN . At block after the data is read from the storage array a determination is made whether to promote the requested data to cache i.e. to add the read data to cache . This determination may be based on the cache strategy. In embodiments the cache driver uses a FAST algorithm to intelligently determine which data is the hottest and to cache an application s working set within the cache ideally entirely within the cache . This determination could also be based upon the LUN priority level.

To reduce the time the application may block on the read request cache promotions may be performed asynchronous to the read request meaning that the application does not block on writes to the cache device. In general I O data is synchronously staged by copying it to an allocated DRAM buffer and asynchronously de staged from DRAM to the cache device. Because DRAM is a limited resource and the cache driver may support many applications it is generally not possible to stage all I O data. Thus at block a determination is made whether sufficient resources are available to stage the cache promotion. For example the cache driver may allocate a certain number of DRAM buffers to reserve and save I O data and at block the cache driver determines if any such buffers are available. If sufficient resources are not available the cache driver may at block free one or more reserved memory buffers i.e. skip one or more pending promotions . Here freeing memory may involve de allocating reserved memory zeroing clearing out such memory and or marking such memory as available. In embodiments the cache driver clears buffers corresponding to LUNs having a lower priority that the current request LUN. Alternatively if sufficiently resources are not available the cache driver does not promote the request data. In some embodiments block is omitted and as described below in connection with the asynchronous I O module is responsible for freeing reserved memory buffers.

If the data is to be promoted and sufficient resources are available to do so processing continues to block where the cache promotion is staged. Staging generally includes 1 allocating an application specific DRAM buffer 2 copying the read data thereto and 3 adding the promotion to a list of pending promotions. In some embodiments the list of pending promotions comprises a table such as the illustrative staging table however any suitable data structure may be used including a queue a priority queue a linked list etc. The staged cache promotion is associated with a source LUN which in turn is associated with a priority level.

After the cache promotion is staged it can be processed i.e. de staged asynchronous as shown in and described below. However it should be understood that some or all of the processing shown in may be performed synchronous the application read request.

Referring to an illustrative method may correspond to processing performed within the caching driver when intercepting a data write request. In embodiments the synchronous I O module is configured to perform the method . At block a data write request is received from the application and at block the request is forwarded to the storage array i.e. a logical storage device identified by a LUN . Thus it is understood that cache driver uses a write through process ensuring that newly written data persists to the backend storage array.

At block a determination is made whether to promote the write data to cache in this context the term promote means to write the data to cache regardless of whether an earlier version of the data currently exists in cache i.e. a write hit or not i.e. a write miss . This determination may be based on the cache strategy. For example in embodiments the cache driver may be configured to promote data to the cache in the case of a write hit but not in the case of a write miss. Such cache strategy may be configured by a user and or an application on a per request basis and or as default behavior. This determination could also be based upon a FAST algorithm and or the LUN priority level.

If it is determined to promote update the data at block another determination is made whether sufficient resources are available to stage the cache promotion. If sufficient resources are not available processing continues to block or alternatively to block . If resources are available processing continues to block . The processing of blocks and may be the same as or similar to that of blocks and of respectively. In embodiments processing block is omitted.

After the cache promotion update is staged it can be processed i.e. de staged asynchronous as shown in and described below. However it should be understood that some or all of the processing shown in may be performed synchronous the application read request.

At block regardless of whether the write data is promoted to cache cache state tables are updated if necessary. For example if an older version of the write data is present in the cache it is necessary to synchronously update the cache table to mark such data as unavailable invalid i.e. dirty . At block the synchronous request processing is complete and an acknowledgement is returned to the application.

Referring to an illustrative method may correspond to processing performed within the caching driver when de staging cache promotions. In embodiments the asynchronous I O module is configured to perform the method thus it is understood that within a caching driver the method may be performed in parallel with method or and or method of .

It is understood that the aggregate I O performance of the storage array may exceed that of the cache device and therefore the rate at which promotions are staged can exceed the rate at which they are de staged making it necessary to skip promotions. Essentially outstanding cache promotions data in the allocated DRAM buffers can only be de staged to the cache device at a rate limited by the capability of the cache device. Because DRAM an exhaustible resource the caching system is required to limit the size of DRAM it can reserve. If the inflow of pending promotions exceeds the outflow of completed promotions the caching system must necessarily skip pending promotions and free up DRAM either due to system application requirements or to accommodate promotions. In general it cannot be determined which cache promotions corresponding to which LUNs will be skipped.

Accordingly at block a determination is made whether there are sufficient resources to promote data to cache. This determination may be based upon available system resources e.g. memory .

If sufficient resources are available at block one or more pending cache promotions are selected. In embodiments this selection is based upon the LUN priorities associated with the pending cache promotions and or based upon the timestamps of the pending promotions e.g. a higher priority newer promotion is selected over a lower priority older promotion . It is appreciated that this approach may reduce the likelihood that promotions corresponding to a high priority LUN are skipped thus providing more predictable application performance. In embodiments the processing of block includes joining table and table .

At blocks the selected cache promotion or promotions is de staged as follows. At block the I O data is copied from the corresponding DRAM buffer to the cache device. At block and the corresponding staging table entry is removed or otherwise marked as completed . At block the cache state tables are updated to reflect the new data in cache. At block the reserved memory e.g. DRAM buffer associated with the selected promotions is freed i.e. de allocated or cleared .

If at block it is determined that resources are not available processing proceeds to block where the LUN priority level of the pending promotions is determined. At block one or more pending cache promotions associated with low priority LUNs are selected to be skipped. At block reserved memory buffers associated with the selected cache promotions are freed e.g. de allocated or cleared the staging table may also be updated. It is understood that by skipping cache promotions associated with lower priority LUNs the more predictable application performance can be achieved.

The processing of blocks may be repeated indefinitely or until at block a determination is made to halt processing.

If sufficient resources are not available to promote data to cache at block one or more reserved memory buffers are freed. In embodiments the cache driver clears buffers corresponding to LUNs having a lower priority that the current request LUN. Thus it is more likely that cache promotions associated with higher priority LUNs will be completed compared to cache promotions associated with lower priority LUNs.

It should be appreciated that the caching system improves existing asynchronous cache update functionality using additional information to prioritize the de staging of data from host memory. By associating a priority level with source LUNs being cached it is possible to control which cache promotions are more or less likely to be skipped in the event of host memory exhausting.

Processing may be implemented in hardware software or a combination of the two. Processing may be implemented in computer programs executed on programmable computers machines that each includes a processor a storage medium or other article of manufacture that is readable by the processor including volatile and non volatile memory and or storage elements at least one input device and one or more output devices. Program code may be applied to data entered using an input device to perform processing and to generate output information.

The system can perform processing at least in part via a computer program product e.g. in a machine readable storage device for execution by or to control the operation of data processing apparatus e.g. a programmable processor a computer or multiple computers . Each such program may be implemented in a high level procedural or object oriented programming language to communicate with a computer system. However the programs may be implemented in assembly or machine language. The language may be a compiled or an interpreted language and it may be deployed in any form including as a stand alone program or as a module component subroutine or other unit suitable for use in a computing environment. A computer program may be deployed to be executed on one computer or on multiple computers at one site or distributed across multiple sites and interconnected by a communication network. A computer program may be stored on a storage medium or device e.g. CD ROM hard disk or magnetic diskette that is readable by a general or special purpose programmable computer for configuring and operating the computer when the storage medium or device is read by the computer. Processing may also be implemented as a machine readable storage medium configured with a computer program where upon execution instructions in the computer program cause the computer to operate. Processing may be performed by one or more programmable processors executing one or more computer programs to perform the functions of the system. All or part of the system may be implemented as special purpose logic circuitry e.g. an FPGA field programmable gate array and or an ASIC application specific integrated circuit .

A non transitory machine readable medium may include but is not limited to a hard drive compact disc flash memory non volatile memory volatile memory magnetic diskette and so forth but does not include a transitory signal per se.

Having described certain embodiments which serve to illustrate various systems and methods sought to be protected herein it will now become apparent to those of ordinary skill in the art that other embodiments incorporating these concepts structures and techniques may be used. Accordingly it is submitted that that scope of the patent should not be limited to the described embodiments but rather should be limited only by the spirit and scope of the following claims.

