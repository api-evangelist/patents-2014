---

title: Using active/active asynchronous replicated storage for live migration
abstract: Examples perform live migration of VMs from a source host to a destination host. The disclosure changes the storage environment, directly or through a vendor provider, to active/active synchronous and, during migration, migrates only data which is not already stored at the destination host. The source and destination VMs have concurrent access to storage disks during migration. After migration, the destination VM executes, with exclusive access to the storage disks, and the system is returned to the previous storage environment (e.g., active/active asynchronous).
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09552217&OS=09552217&RS=09552217
owner: VMware, Inc.
number: 09552217
owner_city: Palo Alto
owner_country: US
publication_date: 20141231
---
This application claims the benefit of U.S. Provisional Patent Application Ser. No. 62 018 580 filed Jun. 28 2014 entitled Using Active Active Asynchronous Replicated Storage for Live Migration U.S. Provisional Patent Application Ser. No. 62 018 582 filed Jun. 28 2014 entitled Live Migration with Pre Opened Shared Disks and U.S. Provisional Patent Application Ser. No. 62 041 047 filed Aug. 23 2014 entitled Live Migration of Virtual Machines with Memory State Sharing all of which are incorporated by reference herein in their entireties.

This application is related to U.S. Non Provisional patent applications entitled Live Migration with Pre Opened Shared Disks and Live Migration of Virtual Machines with Memory State Sharing filed concurrently herewith both of which are incorporated by reference herein in their entireties.

Examples of the present disclosure detect cases in which the disk content of a source VM has been replicated partially or fully at a destination. The present disclosure leverages the existing content at the remote site during migration. In some cases this state serves to seed the migration to reduce the amount of disk copy operations. In other cases replicated data permits applications to skip all disk copy operations when migrating the source VM to the remote datacenter.

This summary introduces a selection of concepts that are described in more detail below. This summary is not intended to identify essential features nor to limit in any way the scope of the claimed subject matter.

For some virtual machines VMs when migrating a VM between customer datacenters there is no knowledge of the contents on the destination storage disk of the customer. As a result many processes copy the entire disk content of the source VM to the storage disk of the destination VM unaware that a replication solution may have already copied some or all of the disk content of the source VM to the destination storage disk. Copying the disk content of a source VM can be a time consuming process potentially requiring hours or days and gigabytes or terabytes of customer bandwidth. These copying efforts are redundant if an existing copy of some or all of the disk content of the source VM is already present at the remote site at the time of the replication.

Offline VM migration with existing storage is a well known technology. Some solutions for example conduct site failovers moving VMs to remote sites by leveraging replicated disk content. However online hot or live VM migration is fundamentally different and more challenging.

Aspects of the disclosure provide a live migration process that detects the presence at a destination host of at least a partial copy of the disk content of a VM to be migrated from a source host to the destination host. The detected presence of the disk content already stored at the destination host is leveraged to reduce the amount of time bandwidth and processing required to perform the live migration. In some examples knowledge of the already replicated disk content seeds the live migration thereby jumpstarting the live migration process through at least a portion of the disk copy. In other examples the presence of the replicated data at the destination host allows the live migration process to entirely skip the disk copy operations when migrating the VM from the source host to the destination host. Aspects of the disclosure accommodate cross VM data consistency and the capabilities of different replication solutions. In these examples the VM does not depend on both the source and destination to run but exists entirely on either the source or the destination.

Replication copies the data associated with a VM from one location to another e.g. from one host to another host for backup disaster recovery and or other purposes. Replication occurs every hour nightly continuously etc. Replication may be described in some examples at the VM level e.g. replication of VMs or a subset of the disks of the VMs such as in Host Based Replication HBR and or vSphere Replication from VMware Inc. Alternatively or in addition replication may be described at a deeper level with reference to logical unit numbers LUNs a group of LUNs in a consistency group and or the like. In general aspects of the disclosure are operable with replication in which at least one host writes to a LUN which backs one or more of the disks of a VM on one site with another host at another site leveraging the replicated LUN content.

There are several types of replication. In active active replication both hosts have access to their respective copies of the VM. That is the active active nature of replication ensures that both sides concurrently write to the replicated storage without issue.

Further replication may be synchronous or asynchronous. Synchronous replication requires round trips on the write path whereas asynchronous replication does not. Each party in some examples may freely write to disk. Aspects of the disclosure are operable with any mechanism e.g. locking generation number tracking etc. to ensure that one may in a distributed manner determine where the latest version of any given item of data is stored.

In active active synchronous replication one host notifies the other host of a planned data write and provides the data to be written and both hosts perform the write at approximately the same time. There may be significant latency involved to coordinate the write operations in this way especially over long distances.

In systems in which active active asynchronous replication is configured between a source host and a destination host the live migration process for a VM from the source host to the destination host is modified. In particular the modified live migration process acknowledges that the source VM may have dirty blocks that were not replicated to the destination VM before the source VM failed or before receipt of the live migration request .

In some examples of active active asynchronous replication one host notifies the other host of a planned data write and asks whether that host has a dirty copy of the data block to be written e.g. an updated version of the data block . If the other host has a dirty copy of that data block the data block is synchronized and then the data write is performed by both hosts. In this manner both hosts coordinate their writes to ensure that they do not write to the same data block without first checking to make sure they have the most updated version of the data block. This example may require several instances of communication as the query response updates and finally the writes are communicated. Those skilled in the art will note that other approaches are contemplated.

In active passive replication only one side is allowed to initiate writes to their copy of the VM. In this manner one host is considered active and the other host is considered passive. The active host is able to write to its copy of the VM whereas the passive host is not able to initiate writes to its copy of the VM. Instead in this example the passive host merely maintains a copy of the VM. In the event of failure of the active host the passive host becomes the active host and resumes execution of the VM. In some examples of active passive migration the replication of disk contents occurs periodically. For example during migration the writing of new I O is delayed while migration is complete. In this example metadata operations such as disk size query are not delayed.

Active active replication and active passive replication are operable synchronously or asynchronously. Synchronous replication implies more expensive writes e.g. round trip to write to both sides whereas asynchronous replication implies the possibility of data loss but faster input output e.g. the passive side can fall behind by some recovery point objective .

Some existing systems migrate VMs from a source host computing device to a destination host computing device while both devices are operating. For example the vMotion process from VMware Inc. moves live hot running or otherwise executing VMs from one host to another without any perceptible service interruption.

As an example a source VM hosted on a source server is migrated to a destination VM on a destination server without first powering down the source VM. After optional pre copying of the memory of the source VM to the destination VM the source VM is suspended and its non memory state is transferred to the destination VM the destination VM is then resumed from the transferred state. The source VM memory is either paged in to the destination VM on demand or is transferred asynchronously by pre copying and write protecting the source VM memory and then later transferring only the modified pages after the destination VM is resumed. In some examples the source and destination servers share common storage in which the virtual disk of the source VM is stored. This avoids the need to transfer the virtual disk contents. In other examples there is no shared storage. The lack of shared storage implies the need to copy or otherwise make disk content available at the destination host. Also some live migration schemes guarantee that page in completes prior to the VM resuming execution at the destination host.

With the advent of virtual volumes e.g. Vvols and virtual storage array networks vSANs object backed disks are now supported for live migration. In some examples disks are file extents on a VM file system VMFS or network file system NFS with disk open commands requiring little more than simply opening the flat files and obtaining locks. With virtual volumes and vSANs however opening a disk is far more complex. For example the host must call out to an external entity e.g. a vendor provider to request that the particular object be bound to the host. A number of other calls flow back and forth between the host and VP to prepare and complete the binding process. Only after that communication finishes may the lock be acquired on the disk. The disk open is then declared to have completed successfully.

In systems in which active active synchronous replication is configured between a source host and a destination host the live migration process for a VM from the source host to the destination host is modified to omit the disk copy phase of the live migration as both the source and destination hosts both have access to up to date versions of the disk content of the VM as described herein. As such no disk or configuration content copying is performed. Instead a handoff of ownership of the VM is performed from the source host to the destination host.

For replication volumes may be placed in consistency groups CGs to ensure that writes to those volumes are kept write order consistent. This ensures that the entire CG is replicated consistently to a remote site. For example if the replication link goes down the entire write replication stream halts ensuring that the CG at the remote site is still self consistent. Such consistency is important when the data files of a VM are on different volumes from its log files which is a typical scenario for performance reasons. Many commercial databases use the write ahead logging WAL protocol. With WAL database crash recovery is always possible since all updates are first durably written to the log before they are written to the data file. Utilizing CGs ensures that write order consistency is preserved. Without maintaining write order consistency it may be possible that data corruption could occur resulting in an unrecoverable database which may lead to a catastrophic loss of data.

In some examples cross VM or cross volume consistency is desired to be maintained. For instance if a user is operating multiple VMs that are writing to the same disk volumes or if multiple VMs are interacting all write order consistency requirements are met to avoid the possibility of data corruption.

In active passive storage environments the source and destination cannot concurrently write to the storage disks because one site has access only to the read only or passive replica as guaranteed by the replication solution e.g. only one site or the other will ever attempt to write to the disk content of a VM . In other examples different arrays may support different techniques. However depending on whether a single VM is moved or multiple VMs there may be problems with cross VM write order consistency. For example data may be replicated from the source VM to the destination VM but the replicated data may depend on other unreplicated data. In this example write order consistency is not maintained.

Some aspects of the disclosure contemplate switching from asynchronous replication to synchronous replication or near synchronous replication Near Sync when performing or preparing to perform a live migration. As described further herein in examples in which active passive are switched to synchronous replication or Near Sync the live migration process for a VM from the source host to the destination host is further modified. In Near Sync the storage is changed to active passive synchronous or near synchronous. Switchover time is bound to approximately one second permitting final data transmission from source VM to destination VM to occur. After migration the original replication mode is restored with the destination VM acting as the read write replication source and the original source VM acting as the replication target. This reversal of roles is called Reverse Replication. In some examples the original source VM is ultimately terminated.

Some aspects of the disclosure contemplate delta query replication when performing or preparing to perform a live migration. As described further herein in examples which cannot support switching to synchronous or near synchronous mode the active passive query delta approach may be used. A snapshot of the replication stream is taken and a bitmap is created of blocks that have not yet been replicated or alternatively are dirty. The missing delta the missing or dirty blocks identified in the bitmap is copied instead of copying all of the blocks in the source VM to the destination VM.

Some aspects of the disclosure switch an underlying replication solution from active active asynchronous to an active active synchronous replication mode. As described further herein switching to this mode includes performing various operations such as draining in flight or queued replication input output I O . These operations provide correctness and consistency thus guaranteeing that the state of the VM exists entirely on either the source host or the destination host but not depending on both sides.

These examples of live migration improve the functionality of VMs. For example the methods provide continuity of service as a VM is migrated from one host to another. Aspects of the disclosure decrease the VM downtime as live migration occurs. In some examples there is no noticeable delay for any user during the live migration disclosed herein.

Host computing device may include a user interface device for receiving data from a user and or for presenting data to user . User may interact indirectly with host computing device via another computing device such as VMware s vCenter Server or other management device. User interface device may include for example a keyboard a pointing device a mouse a stylus a touch sensitive panel e.g. a touch pad or a touch screen a gyroscope an accelerometer a position detector and or an audio input device. In some examples user interface device operates to receive data from user while another device e.g. a presentation device operates to present data to user . In other examples user interface device has a single component such as a touch screen that functions to both output data to user and receive data from user . In such examples user interface device operates as a presentation device for presenting information to user . In such examples user interface device represents any component capable of conveying information to user . For example user interface device may include without limitation a display device e.g. a liquid crystal display LCD organic light emitting diode OLED display or electronic ink display and or an audio output device e.g. a speaker or headphones . In some examples user interface device includes an output adapter such as a video adapter and or an audio adapter. An output adapter is operatively coupled to processor and configured to be operatively coupled to an output device such as a display device or an audio output device.

Host computing device also includes a network communication interface which enables host computing device to communicate with a remote device e.g. another computing device via a communication medium such as a wired or wireless packet network. For example host computing device may transmit and or receive data via network communication interface . User interface device and or network communication interface may be referred to collectively as an input interface and may be configured to receive information from user .

Host computing device further includes a storage interface that enables host computing device to communicate with one or more datastores which store virtual disk images software applications and or any other data suitable for use with the methods described herein. In some examples storage interface couples host computing device to a storage area network SAN e.g. a Fibre Channel network and or to a network attached storage NAS system e.g. via a packet network . The storage interface may be integrated with network communication interface .

The virtualization software layer supports a virtual machine execution space within which multiple virtual machines VMs may be concurrently instantiated and executed. Hypervisor includes a device driver layer and maps physical resources of hardware platform e.g. processor memory network communication interface and or user interface device to virtual resources of each of VMs such that each of VMs has its own virtual hardware platform e.g. a corresponding one of virtual hardware platforms N each virtual hardware platform having its own emulated hardware such as a processor a memory a network communication interface a user interface device and other emulated I O devices in VM . Hypervisor may manage e.g. monitor initiate and or terminate execution of VMs according to policies associated with hypervisor such as a policy specifying that VMs are to be automatically restarted upon unexpected termination and or upon initialization of hypervisor . In addition or alternatively hypervisor may manage execution VMs based on requests received from a device other than host computing device . For example hypervisor may receive an execution instruction specifying the initiation of execution of first VM from a management device via network communication interface and execute the execution instruction to initiate execution of first VM .

In some examples memory in first virtual hardware platform includes a virtual disk that is associated with or mapped to one or more virtual disk images stored on a disk e.g. a hard disk or solid state disk of host computing device . The virtual disk image represents a file system e.g. a hierarchy of directories and files used by first VM in a single file or in a plurality of files each of which includes a portion of the file system. In addition or alternatively virtual disk images may be stored on one or more remote computing devices such as in a storage area network SAN configuration. In such examples any quantity of virtual disk images may be stored by the remote computing devices.

Device driver layer includes for example a communication interface driver that interacts with network communication interface to receive and transmit data from for example a local area network LAN connected to host computing device . Communication interface driver also includes a virtual bridge that simulates the broadcasting of data packets in a physical network received from one communication interface e.g. network communication interface to other communication interfaces e.g. the virtual communication interfaces of VMs . Each virtual communication interface for each VM such as network communication interface for first VM may be assigned a unique virtual Media Access Control MAC address that enables virtual bridge to simulate the forwarding of incoming data packets from network communication interface . In an example network communication interface is an Ethernet adapter that is configured in promiscuous mode such that all Ethernet packets that it receives rather than just Ethernet packets addressed to its own physical MAC address are passed to virtual bridge which in turn is able to further forward the Ethernet packets to VMs . This configuration enables an Ethernet packet that has a virtual MAC address as its destination address to properly reach the VM in host computing device with a virtual communication interface that corresponds to such virtual MAC address.

Virtual hardware platform may function as an equivalent of a standard x86 hardware architecture such that any x86 compatible desktop operating system e.g. Microsoft WINDOWS brand operating system LINUX brand operating system SOLARIS brand operating system NETWARE or FREEBSD may be installed as guest operating system OS in order to execute applications for an instantiated VM such as first VM . Aspects of the disclosure are operable with any computer architecture including non x86 compatible processor structures such as those from Acorn RISC reduced instruction set computing Machines ARM and operating systems other than those identified herein as examples.

Virtual hardware platforms N may be considered to be part of virtual machine monitors VMM N that implement virtual system support to coordinate operations between hypervisor and corresponding VMs . Those with ordinary skill in the art will recognize that the various terms layers and categorizations used to describe the virtualization components in may be referred to differently without departing from their functionality or the spirit or scope of the disclosure. For example virtual hardware platforms N may also be considered to be separate from VMMs N and VMMs N may be considered to be separate from hypervisor . One example of hypervisor that may be used in an example of the disclosure is included as a component in VMware s ESX brand software which is commercially available from VMware Inc.

The host computing device may include any computing device or processing unit. For example the computing device may represent a group of processing units or other computing devices such as in a cloud computing configuration. The computing device has at least one processor and a memory area. The processor includes any quantity of processing units and is programmed to execute computer executable instructions for implementing aspects of the disclosure. The instructions may be performed by the processor or by multiple processors executing within the computing device or performed by a processor external to computing device. In some examples the processor is programmed to execute instructions such as those illustrated in the figures.

The memory area includes any quantity of computer readable media associated with or accessible by the computing device. The memory area or portions thereof may be internal to the computing device external to computing device or both.

After the source VM is stunned at the virtual device state of the source VM on the source host is serialized and its storage disks are closed e.g. VM file systems logical unit numbers etc. and its exclusive disk locks are released at . These operations are often collectively referred to as a checkpoint transfer . The virtual device state includes for example memory queued input output the state of all virtual devices of the VM and any other virtual device side memory. More generally operation may be described as preparing for disk close.

At this point in the timeline the destination VM prepares disks for access. For example the destination VM executes a checkpoint restore at . The checkpoint restore includes opening the storage disks and acquiring exclusive disk locks. Restoring the virtual device state includes applying checkpoints e.g. state to the destination VM to make the destination VM look like the source VM . Once the checkpoint restore is complete the destination VM informs the source VM that the destination VM is ready to execute at . Some examples contemplate a one way message sent from the destination VM to the source VM informing the source VM that the destination VM is ready to execute. This one way message is sometimes referred to as a Resume Handshake. The execution of the VM may then resume on the destination VM at .

With virtual volumes on the source host the disks are changed to multi writer access then pre opened also in multi writer mode on the destination host. The checkpoint state is then transferred and restored without closing the disks and opening them on the other side then the VM is resumed on the destination side the disks are closed on the source side and access is reverted to exclusive read write mode on the destination side. In this manner the disk open close time is removed from between the checkpoint transfer and restore thus shortening the combined time of those two operations and reducing the amount of time the VM is suspended e.g. not running on either host .

Collectively a virtualization platform the source VM and destination VM and the source host and destination host may be referred to as a virtualization environment . The APIs represent the interface between the virtualization environment and storage hardware . The storage hardware includes the VP and the storage disks of the source VM and the destination VM .

In the example of the source VM is located on the source host and the destination VM is located on the destination host . The source host and destination host communicate directly in some examples. In other examples the source host and destination host communicate indirectly through the virtualization platform . Storage disks in the illustrated example are managed by VPs or other array providers that allow shared access to the storage disks e.g. virtual volumes such as virtual volume . The storage disks illustrated in are maintained by one of the VPs . In this example the source host and destination host communicate with the storage disks through a network not illustrated .

The operations of the exemplary method of are carried out by a processor associated with the source VM . The hypervisor coordinates operations carried out by the processors associated with the source host and destination host and their associated VMs. described below illustrates the sequence of the following events.

At a request is received to perform live migration between the source host and the destination host . The request may initiate from the hypervisor from user or may be triggered by an event occurring at the source VM . For example the triggering event may be a request by user for live migration from the source host to the destination host . In other examples the triggering event is the source VM or source host reaching some operational threshold e.g. the source VM begins to exceed the resources of the source host and is to be migrated to the destination host with higher performance capabilities . As further examples the source VM is live migrated for backup purposes in order to make it more accessible to a different user . Requests for live migration are in some examples periodic or otherwise occurring at regular intervals. In other examples requests for live migration are made during system downtime when I O commands fall below a threshold amount established for instance by users . In other examples requests for live migration are in response to system conditions such as anticipated hardware upgrades downtimes or other known or predicted hardware or software events.

At a virtualization software implementing a virtualization platform or environment such as VMware Inc. s VirtualCenter invokes an API such as part of API e.g. PrepareForBindingChange to notify the storage VP to set up the replication environment before the live migration. In response the VP switches the replication mode from active active asynchronous to active active synchronous or near synchronous or approximately asynchronous in some examples . In some examples the replication mode may already be active active asynchronous when the VP issues the request. In some examples the VP also drains queued replication data I O as necessary. This call blocks further I O commands for as long as needed to switch the replication state to be synchronous. The PrepareForBindingChange API function call or other function call is issued against the shared storage disk of the VM . Switching from asynchronous replication to synchronous replication during the live migration ensures that any writes to the source VM that occur during the live migration are duplicated by the destination VM . Aspects of the disclosure ensure that the underlying replication solution flushes whatever writes are occurring synchronously to the replica LUN disk storage e.g. storage disk . The destination VM in some examples does not actually issue duplicate I O commands.

With the workload of the source VM still running the source VM downgrades its disk locks from exclusive locks to multiwriter e.g. shared disk locks at . In another example the disk locks could be downgraded to an authorized user status. The authorized users may be established as the source VM and the destination VM . This operation is omitted in the event that there are no locks on the disks . This may occur any time prior to stunning the source VM . In some examples the source VM sends a message to the destination VM that multiwriter mode is available for the disks to be migrated. In some examples the destination VM is instructed not to write to the disks .

At an instance of the source VM is created or registered at the destination host . In order to register the source VM the source VM shares its configuration including information regarding its disks . For example the new instance of the source VM registered at the destination host points to the replicated read only disk content on the disk of the source VM .

After the source VM is registered at the destination host at the newly created destination VM binds and opens all disks in non exclusive mode e.g. multiwriter lock mode at . At the memory of the source VM is pre copied from the source host to the destination host . For example ESXi servers using the vMotion network pre copy the memory state of the source VM . This may take anywhere from seconds to hours. Pre copying is complete when the memory at the destination VM is approximately the same as the memory at the source VM . Any form of memory copy is contemplated. The disclosure is not limited to pre copy. Further the memory copy may be performed at any time even post switchover e.g. after the destination VM is executing and the source VM has terminated . Only memory which is not already present at the destination host is copied.

The source VM is stunned frozen or otherwise suspended at . Stunning freezes or otherwise suspends execution of the source VM but does not quiesce the source VM in some examples. For example no cleanup or shutdown operations normally associated with quiescing are performed. The duration of the suspended execution in some examples is about one second. Several operations may be performed during this duration or interval 

A. Any remaining dirty memory state is transferred from the source VM to the destination VM . This may be performed as part of a checkpoint transfer at .

API in some examples is used to reverse the direction of replication. The source and destination VM reverse roles with the source VM becoming the replication target while the destination VM is now the read write replication source. VM downtime or switchover time refers to the time a VM is not executing guest instructions during the live migration e.g. between stunning the source VM and resuming beginning execution of the destination VM .

Once stunned at the virtual device state of the source VM is serialized for transmission to the destination VM . Serializing the virtual device state of the source VM on the source host in some examples includes closing disks e.g. VM file systems logical unit numbers etc. and releasing exclusive disk locks. These operations are often collectively referred to as checkpoint transfer. The virtual device state includes for example memory queued input output the state of all virtual devices of the source VM and any other virtual device side memory. There is no need to close any disks here.

Upon receipt of the information in the checkpoint transfer the destination VM engages in a checkpoint restore at . For example the destination VM restores the virtual device state of the source VM at the destination VM once the VP indicates that the disks have been opened successfully in multiwriter mode for the destination VM . However there is no need to open the disks at this point because that occurred earlier at .

In some examples the destination VM then transmits an explicit message to the source VM that the destination VM is ready to start executing at . The source VM in this example replies with a Resume Handshake. In other examples the source VM sends a message to the destination VM confirming receipt of the message from the destination VM . In another example the processor queries and updates both the source and the destination VMs for status reports regarding the checkpoint transmission and restoration.

After receiving that acknowledgement from the source VM the destination VM begins executing at . In some examples after the start of execution the destination VM sends a confirmation to the source VM that execution has begun successfully at . In response to receiving confirmation that the destination VM has begun execution the source VM closes e.g. terminates at which includes releasing its multiwriter disk locks. The destination VM with the workload already running and issuing disk input output I O transparently upgrades its locks from multiwriter to exclusive ownership.

At the process of cleanup occurs. This includes VirtualCenter invoking another of APIs e.g. CompleteBindingChange that allows the storage vendor to reverse the environment changes made at above. The VP reverts replication to operating asynchronously e.g. active active asynchronous and may change replication direction or bias and restore an original recovery point objective RPO .

The hypervisor directs the source VM and destination VM to change their respective replication modes. In response to the command to change their replication modes the source VM instructs the VP to downgrade its disk locks from exclusive locks to multiwriter disk locks or other shared disk locks. In another example the disk locks are downgraded to an authorized user status. The authorized users are established as the source VM and the destination VM . This step is omitted in the event that there are no locks on the disks . The destination VM in response to the direction to change its replication mode binds and opens all VM disks in multiwriter lock mode.

The source VM then registers an instance of itself at the destination. Subsequently the disk of the source VM is copied to the destination VM excluding any of the disk content of the source VM which already exists at the destination VM .

After the source VM has been precopied the source VM is stunned and a checkpoint transfer occurs between the source VM and the destination VM . The VMs then engage in a handshake after which the destination VM is executed. The destination VM confirms its execution to the source VM . After successful execution of the destination VM the source VM is free to terminate. Following termination the hypervisor receives the command to restore the original replication environment and the hypervisor directs the source VM and destination VM to return to active active asynchronous replication mode. In response to the change in replication mode the source VM releases its disk lock and the destination VM upgrades the disk locks to exclusive mode.

Aspects of the disclosure present a live migration scheme that accommodates VMs having numerous disks and accounts for longer switchover time for opening closing those disks . In some examples disk operations are performed while the source VM is still running which keeps the switchover time to a minimum. For example rather than sequentially providing access to disks involved in a live migration aspects of the disclosure overlap shared access to the disks e.g. by the source VM and the destination VM to move expensive disk operations outside the downtime window. Even though both the source VM and the destination VM share a writeable state to the disks at least the destination VM is prevented from writing to these disks while sharing access. In some examples the source VM is also prevented from writing to these disks at this time. This prevents corruption of the disks and prevents the introduction of inconsistencies in the disks .

Shared access to the disks may be implemented by shared disk locks and or multiwriter locking. For instance locking is fundamentally different in virtual volumes versus NFSs or VMFSs . In NFS VMFS a systemdisk.vmdk contains the name of the system and it points to a flat file . The locks are placed on the flat file itself e.g. the extent .

For virtual volumes a systemdisk.vmdk contains a VVOL ID which points to the virtual volume backend and to a VVOL UUID.lck file e.g. the lock file . UUID refers to universal unique identifier. For virtual volumes the lock is not on the backend data itself which has no lock primitives but instead on a proxy file e.g. the VVOL UUID.lck file .

As described herein the destination VM opens disks prior to the source VM being stunned e.g. the destination VM pre opens the disks with the destination VM taking exclusive ownership of the disks by the completion of the migration. However it is also possible that the disks associated with the system are not locked. While some examples are described herein with reference to shared disk locks .lck files and the like the disclosure contemplates any form of shared disks with or without locks. Some examples do not take any locks against the disks e.g. virtual volume .lck files and or do not create new .lck files for the destination VM . In these examples the disclosure is operable with shared disks but unshared disk locks e.g. there are no disk locks . Aspects of the disclosure are operable with any mechanism for taking exclusive ownership of the disk and or any mechanism allowing the destination VM to open the disks .

Each virtual volume is provisioned from a block based storage system. In an example a NAS based storage system implements a file system on top of data storage units DSUs and each virtual volume is exposed to computer systems as a file object within this file system.

In general virtual volumes may have a fixed physical size or may be thinly provisioned and each virtual volume has a VVOL ID identifier which is a universally unique identifier that is given to the virtual volume when the virtual volume is created. For each virtual volume a virtual volume database stores for each virtual volume the VVOL ID the container ID of the storage container in which the virtual volume is created and an ordered list of values within that storage container that comprise the address space of the virtual volume . The virtual volume database is managed and updated by a volume manager which in one example is a component of a distributed storage system manager. In one example the virtual volume database also stores a small amount of metadata about the virtual volume . This metadata is stored in the virtual volume database as a set of key value pairs and may be updated and queried by computer systems via an out of band path at any time during existence of the virtual volume . Stored key value pairs fall into three categories in some examples. One category includes well known keys in which the definition of certain keys and hence the interpretation of their values are publicly available. One example is a key that corresponds to the virtual volume type e.g. in virtual machine examples whether the virtual volume contains the metadata or data of a VM . Another example is the App ID which is the ID of the application that stored data in the virtual volume .

Another category includes computer system specific keys in which the computer system or its management module stores certain keys and values as the metadata of the virtual volume. The third category includes storage system vendor specific keys. These allow the storage system vendor to store certain keys associated with the metadata of the virtual volume. One reason for a storage system vendor to use this key value store for its metadata is that all of these keys are readily available to storage system vendor plug ins and other extensions via the out of band channel for virtual volumes . The store operations for key value pairs are part of virtual volume creation and other processes and thus the store operation are reasonably fast. Storage systems are also configured to enable searches of virtual volumes based on exact matches to values provided on specific keys.

In some replication environments the source and destination are in an active passive asynchronous replication relationship but may be changed to synchronous or near synchronous near sync . A system is considered in near sync if its switchover time can be bound to approximately 1 second in some examples. With the switchover time so limited the process may be blocked for approximately a second to allow final data transmission to occur before migration. The following is an example of the operations involved in active passive synchronous or near synchronous approach 

Virtualization software implementing a virtualization platform or environment such as VirtualCenter from VMware Inc. invokes at least one of API e.g. PrepareForBindingChange to notify the storage vendor e.g. VP to configure the replication environment before the live migration. In response the VP switches the replication mode from active passive asynchronous to active passive synchronous or near synchronous in some examples . In response the VP drains queued replication I O data as necessary. This call may block for as long as needed to switch the replication state to be synchronous or near synchronous. The PrepareForBindingChange API function call is issued against the VM s shared disk and configuration virtual volumes . Switching from asynchronous replication to synchronous replication during the live migration ensures that any writes to the source VM that occur during the live migration are duplicated by the destination VM. That is aspects of the disclosure ensure that the underlying replication solution flushes whatever writes are occurring synchronously to the replica LUN disk storage. The destination VM in some examples does not actually issue duplication I O.

An instance of the source VM is registered at the destination VM sharing the existing configuration swap and disk data virtual volumes that point to the replicated read only data virtual volumes for disk content.

The destination VM opens its disks in non exclusive mode e.g. multiwriter . However the destination VM will not write to the virtual volumes until the destination VM obtains exclusive access later in the process.

The memory state of the VM is pre copied from the source host to the destination host. For example ESXi servers using the vMotion network pre copy the memory state of the VM. Any form of memory copy is contemplated. The disclosure is not limited to pre copy. Further the memory copy may be performed at any time even post switchover e.g. after the destination VM is executing and the source VM has terminated .

The source VM is stunned frozen or otherwise suspended. Stunning freezes or otherwise suspends execution of the source VM but does not quiesce the source VM. For example no cleanup or shutdown operations normally associated with quiescing are performed. The duration of the suspended execution in some examples is about one second. Several operations may be performed during this duration or interval 

A. Any remaining dirty memory state is transferred from the source VM to the destination VM. This may be performed as part of a checkpoint transfer.

B. VirtualCenter e.g. or other virtualization software implementing a virtualization platform or environment invokes at least one of API e.g. CompleteBindingChange to notify the VP to commit the migration. The API performs the following operations in some examples 

In some examples the destination VM then transmits an explicit message to the source VM that the destination VM is ready to start executing. The source VM in this example replies with a Resume Handshake

The destination VM resumes or begins execution. In some examples after the start of execution the destination VM sends a confirmation to the source VM that execution has begun successfully.

In response to receiving confirmation that the destination VM has begun execution the source VM closes the disks e.g. and terminates which includes releasing its multiwriter disk locks.

The destination VM with the workload already running and issuing disk input output I O transparently upgrades its locks from multiwriter to exclusive ownership.

Not all environments support switching to synchronous or near synchronous replication. An alternative example of live migration for those systems is the query delta approach in accordance with aspects of the disclosure. Under query delta a snapshot is taken of the replication stream which is the CG at site replicating to CG at site . Subsequently an application uses at least one of APIs such as QueryReplicationDelta to request a bitmap of blocks that have not yet been replicated from CG to CG . In this manner the application compares the existing consistency group sets to find the delta or difference between them. The delta is used to seed the destination disk as only the absent or different blocks are copied from the source disk. In some examples after completion of the query delta from the source to the destination replication is reversed changing the source to the replication target and the destination is now the read write replication source. The following is an exemplary implementation of query delta 

An instance of the source VM is created or registered at the destination host. In order to register the source VM the source VM shares its configuration including information regarding its disks. For example the new instance of the source VM registered at the destination host points to the replicated read only disk content on the disk of the source VM.

In some examples the source VM requests that replication is flushed using an API ensuring that the replication occurs within RPO and any outstanding delta between the CG of the source and the CG of the destination is minimal.

The source VMX installs mirroring software such has svmmirror from VMware Inc. to monitor write I O commands to the disks of the source VM. This creates a dirty bitmap which may be used to determine what content remains to be copied between the source and destination hosts.

The source VMX uses an application such as QueryReplicationDelta or a function call to determine the bitmap of blocks to be copied from CG to CG thus creating a replication bitmap. QueryReplicationDelta or a similar function call or application looks for differences or delta between the two CGs and instructs only those blocks with differences to be copied from the source to the destination.

The source VMX requests that the VP present a writable snapshot S of CG at site using at least one of APIs .

The source VM uses the dirty bitmap ORed with the replication bitmap to drive XvMotion using copy and mirroring techniques.

Virtualization servers such as ESXi servers using the vMotion network from VMware Inc. precopy the memory state of the VM.

Once the memory has been precopied from the source VM the source VM is stunned Stunning freezes or otherwise suspends execution of the source VM but does not quiesce the source VM. For example no cleanup or shutdown operations normally associated with quiescing are performed.

In some examples the destination VM then transmits an explicit message to the source VM that the destination VM is ready to start executing. The source VM in this example replies with a Resume Handshake.

After receiving that acknowledgement from the source VM the destination VM begins executing. The source VM closes e.g. and terminates .

Cleanup then occurs. Cleanup includes for example changing the replication direction or bias and restoring an original RPO.

Approaches discussed herein such as Switch to Sync and Query Delta may be used as building blocks to form end to end migration workflows that handle multiple VMs multiVMs and multiple CG multiCG consistency requirements. In examples of groups composed of more than one VM or where there are inter VM CGs VMs still running at the source datacenter continue executing and writing to the source side of the CG during migration. Therefore more complicated migration techniques are contemplated which build upon these approaches. Example implementations of the APIs described herein may be included as part of the vSphere brand API for Storage Awareness VASA 2.0 by VMware Inc. However aspects of the disclosure are operable with any API performing the functionality described herein.

Some examples are too complicated to utilize the Switch to Sync or Query Delta approaches alone. In those examples different migration environments may be modified and combined into an end to end workflow that migrates multiple VMs concurrently handling different CG scenarios. In one example of a complicated case requiring such an end to end workflow there is a set of VMs with disks in a CG or set of CGs replicating to a CG which may also be a set of CGs at the remote host site. In that example the disclosure guarantees data consistency in the event of failure during migration. In other examples utilizing end to end workflow the system is moving at least one VM but not all VMs in a single CG. In other examples where end to end workflows are utilized the system is moving at least two VMs that have a required consistency to a common CG. In either case ownership of the CG cannot immediately be handed off with VM switchover otherwise one of the basic workflows described herein would have been sufficient.

To ensure that write order consistency is maintained in some examples a snapshot of the CG called Recovery CG or RCG is taken. If there are any errors in migration the system is restored or failed back in this example to RCG to regain consistency. Restoring to RCG may result in the total migration time of all VMs exceeding the configured RPO on the CG. This method of maintaining write order consistency is referred to as the Violate RPO VRPO method. If migration succeeds the result is a consistent CG even though RPO may have been violated for a while during migration if the migration time of all the VMs exceeds the RPO.

In other examples as individual VMs are migrated to the destination site write order consistency is maintained by creating subsets of required consistency. In these examples called Reverse Replication RR the source VM exists at the source host or site but it is running on the CG of the destination host. As execution is switched to the destination site the I O is synchronously replicated back to the source site thus the source site is replicating the destination site. In this example for the duration of migration the CG of site is kept up to date and the VM is able to fail back to site if there is an error during the migration. After all VMs have migrated to site the reverse replication is halted as the fail safe is no longer necessary.

RR may be implemented in multiple ways. For example in array offload an array solution supports redirecting replication I O back to other replication streams. In that example the array configures replication from a temporary snapshot back to the CG of site . Alternatively in svmmirror the system may implement RR over the network with an agent synchronously writing the I O back to the CG of the source.

Some examples contemplate the source host and or the destination host being associated with a hybrid cloud service e.g. a public private cloud . A hybrid cloud service such as vCloud Air by VMware Inc. is a public cloud platform allowing seamless transition between a private cloud and a public cloud.

The operations described herein may be performed by a computer or computing device. The computing devices communicate with each other through an exchange of messages and or stored data. Communication may occur using any protocol or mechanism over any wired or wireless connection. A computing device may transmit a message as a broadcast message e.g. to an entire network and or data bus a multicast message e.g. addressed to a plurality of other computing devices and or as a plurality of unicast messages each of which is addressed to an individual computing device. Further in some examples messages are transmitted using a network protocol that does not guarantee delivery such as User Datagram Protocol UDP . Accordingly when transmitting a message a computing device may transmit multiple copies of the message enabling the computing device to reduce the risk of non delivery.

By way of example and not limitation computer readable media comprise computer storage media and communication media. Computer storage media include volatile and nonvolatile removable and non removable media implemented in any method or technology for storage of information such as computer readable instructions data structures program modules or other data. Computer storage media are tangible non transitory and are mutually exclusive to communication media. In some examples computer storage media are implemented in hardware. Exemplary computer storage media include hard disks flash memory drives digital versatile discs DVDs compact discs CDs floppy disks tape cassettes and other solid state memory. In contrast communication media typically embody computer readable instructions data structures program modules or other data in a modulated data signal such as a carrier wave or other transport mechanism and include any information delivery media.

Although described in connection with an exemplary computing system environment examples of the disclosure are operative with numerous other general purpose or special purpose computing system environments or configurations. Examples of well known computing systems environments and or configurations that may be suitable for use with aspects of the disclosure include but are not limited to mobile computing devices personal computers server computers hand held or laptop devices multiprocessor systems gaming consoles microprocessor based systems set top boxes programmable consumer electronics mobile telephones network PCs minicomputers mainframe computers distributed computing environments that include any of the above systems or devices and the like.

Examples of the disclosure may be described in the general context of computer executable instructions such as program modules executed by one or more computers or other devices. The computer executable instructions may be organized into one or more computer executable components or modules. Generally program modules include but are not limited to routines programs objects components and data structures that perform particular tasks or implement particular abstract data types. Aspects of the disclosure may be implemented with any number and organization of such components or modules. For example aspects of the disclosure are not limited to the specific computer executable instructions or the specific components or modules illustrated in the figures and described herein. Other examples of the disclosure may include different computer executable instructions or components having more or less functionality than illustrated and described herein.

Aspects of the disclosure transform a general purpose computer into a special purpose computing device when programmed to execute the instructions described herein.

The examples illustrated and described herein as well as examples not specifically described herein but within the scope of aspects of the disclosure constitute exemplary means for performing live migration leveraging replication. For example the elements illustrated in the figures such as when encoded to perform the operations illustrated in the figures constitute exemplary means for changing a replication mode from active active asynchronous to active active synchronous in response to receiving a request to perform a live migration of a source VM on a source host to a destination VM on a destination host exemplary means for performing the live migration of the source VM to the destination VM and exemplary means for changing the replication mode from active active synchronous to active active asynchronous after completion of the live migration.

At least a portion of the functionality of the various elements illustrated in the figures may be performed by other elements in the figures or an entity e.g. processor web service server application program computing device etc. not shown in the figures. For example some examples are described herein with reference to virtual volumes such as virtual volumes . According to some examples a storage system cluster creates and exposes virtual volumes to connected computer systems. Applications e.g. VMs accessing their virtual disks etc. running in computer systems access the virtual volumes on demand using standard protocols such as SCSI small computer simple interface and NFS network file system through logical endpoints for the SCSI or NFS protocol traffic known as protocol endpoints PEs that are configured in storage systems.

While some of the examples are described with reference to virtual volumes offered by VMware Inc. aspects of the disclosure are operable with any form type origin or provider of virtual volumes.

In some examples the operations illustrated in the figures may be implemented as software instructions encoded on a computer readable medium in hardware programmed or designed to perform the operations or both. For example aspects of the disclosure may be implemented as a system on a chip or other circuitry including a plurality of interconnected electrically conductive elements.

The order of execution or performance of the operations in examples of the disclosure illustrated and described herein is not essential unless otherwise specified. That is the operations may be performed in any order unless otherwise specified and examples of the disclosure may include additional or fewer operations than those disclosed herein. For example it is contemplated that executing or performing a particular operation before contemporaneously with or after another operation is within the scope of aspects of the disclosure.

When introducing elements of aspects of the disclosure or the examples thereof the articles a an the and said are intended to mean that there are one or more of the elements. The terms comprising including and having are intended to be inclusive and mean that there may be additional elements other than the listed elements. The term exemplary is intended to mean an example of. 

Having described aspects of the disclosure in detail it will be apparent that modifications and variations are possible without departing from the scope of aspects of the disclosure as defined in the appended claims. As various changes could be made in the above constructions products and methods without departing from the scope of aspects of the disclosure it is intended that all matter contained in the above description and shown in the accompanying drawings shall be interpreted as illustrative and not in a limiting sense.

