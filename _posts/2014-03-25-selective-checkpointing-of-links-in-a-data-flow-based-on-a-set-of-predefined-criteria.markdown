---

title: Selective checkpointing of links in a data flow based on a set of predefined criteria
abstract: Techniques are disclosed for qualified checkpointing of a data flow model having data flow operators and links connecting the data flow operators. A link of the data flow model is selected based on a set of checkpoint criteria. A checkpoint is generated for the selected link. The checkpoint is selected from different checkpoint types. The generated checkpoint is assigned to the selected link. The data flow model, having at least one link with no assigned checkpoint, is executed.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09262205&OS=09262205&RS=09262205
owner: International Business Machines Corporation
number: 09262205
owner_city: Armonk
owner_country: US
publication_date: 20140325
---
This application is a continuation of co pending U.S. patent application Ser. No. 13 843 425 filed Mar. 15 2013. The aforementioned related patent application is herein incorporated by reference in its entirety.

Embodiments disclosed herein relate to data integration. More specifically embodiments disclosed herein relate to data integration on retargetable engines in a networked environment.

Cloud computing environments often include a master computing device and multiple worker computing devices. Work is distributed from the master computing device into the cloud and to the worker computing devices within the cloud. The worker computing devices perform the work and return the results to the master computing device. The master computing device then assembles the results received from the worker computing devices.

Embodiments presented in this disclosure provide a computer implemented method for qualified checkpointing of a data flow model having data flow operators and links connecting the data flow operators. The method includes selecting a link of the data flow model based on a set of checkpoint criteria. The method also includes generating a checkpoint for the selected link where the checkpoint is selected from multiple different checkpoint types and where the generated checkpoint is assigned to the selected link. The method also includes executing the data flow model where at least one link of the data flow model has no assigned checkpoint.

Other embodiments presented in this disclosure provide a computer program product for qualified checkpointing of a data flow model having data flow operators and links connecting the data flow operators. The computer program product includes a computer readable storage medium having program code embodied therewith. The program code is executable by one or more computer processors to select a link of the data flow model based on a set of checkpoint criteria. The program code is also executable to generate a checkpoint for the selected link where the checkpoint is selected from multiple different checkpoint types and where the generated checkpoint is assigned to the selected link. The program code is also executable in order to execute the data flow model where at least one link of the data flow model has no assigned checkpoint.

Still other embodiments presented in this disclosure provide a system for qualified checkpointing of a data flow model having data flow operators and links connecting the data flow operators. The system includes one or more computer processors and a memory containing a program which when executed by the one or more computer processors is configured to perform an operation that includes selecting a link of the data flow model based on a set of checkpoint criteria. The operation also includes generating a checkpoint for the selected link where the checkpoint is selected from multiple different checkpoint types and where the generated checkpoint is assigned to the selected link. The operation also includes executing the data flow model where at least one link of the data flow model has no assigned checkpoint.

Efforts of organizations in meeting business requirements while lowering cost may often be aided by cloud computing which provides infrastructure platform software and process as services and which may often simplify adoption by the organizations. A cloud computing environment also referred to herein as a cloud environment may include different types of applications of which one example is data integration applications. Data integration applications may often be designed to run in networked environments such as symmetric multiprocessing SMP massively parallel processing MPP cluster or grid environments that include only a single data processing engine also referred to herein as a processing engine. On the other hand in a cloud computing environment multiple data processing engines may coexist and share common resources. A client may submit a job request to the cloud computing environment without needing to know or specify any particular data processing engine for running the desired job. An example of a job request is a request to execute a desired data flow model also referred to herein as a data flow. Programmatically choosing an appropriate data processing engine to run the desired job may be one of the challenges posed in migrating data integration to the cloud computing environment.

In some embodiments and as discussed in further detail herein one approach to resolve this challenge involves a perspective of retargetable engines. In one embodiment an application is provided that is configured to select a data processing engine based on job characteristics and execution requirements. The data processing engine may be selected from a set of data processing engines of different types. Examples of different types of data processing engines include parallel processing engines and distributed computing engines. For example a parallel processing engine may be a scalable data integration processing engine while an example of a distributed computing engine is a scalable distributed computing engine for cloud computing environments such as Hadoop available from Apache Software Foundation. The distributed computing engine may also include a MapReduce model which provides an interface to facilitate meeting various data integration needs.

At least in some embodiments the respective paradigms of parallel processing engines and MapReduce each involves parallelizing programs by partitioning data across multiple processes in both a pipeline and partitioned manner. However the two execution mechanisms may differ dramatically in terms latency because parallel processing engines may use inter process communication IPC mechanisms for data transfer whereas MapReduce applications may use file system mechanisms for data transfer. As a result jobs that do not cause pipeline breaks or stalls may often run faster even an order of magnitude faster in some cases on parallel processing engines than on MapReduce applications.

On the other hand the MapReduce paradigm may provide a higher level of fault tolerance than parallel processing engines at least in some embodiments because all state information is stored on the file system. Doing so may at least in some cases avoid job failures caused by failure of a single mapper process. The higher level of fault tolerance may be especially important for any single significantly large scale parallel jobs where the likelihood of a particular process failing may be relatively higher.

In some embodiments data processing engines of multiple paradigms may be available to run a desired job. Accordingly at least some embodiments disclosed herein provide empirical models for programmatically selecting an appropriate data processing engine for running the desired job and without requiring any user input specifying any desired data processing engine. The data processing engine may be selected based on predefined criteria. Examples of the predefined criteria include input data job design complexity application logic performance requirement latency requirement fault tolerance checkpoint restart parallelism resource utilization parallel execution setup cost etc. Further due to the dynamic nature of the predefined criteria the appropriate data processing engine may vary from execution to execution even for the same desired job. By providing retargetable engines the needs of desired jobs may be more readily met even as the needs change over time such as in terms of data volume required execution time number of partitions etc.

One embodiment provides a decision making job execution system for efficiently running a data flow using a selected data processing engine in a networked environment such as a cloud environment. The data processing engine may be selected upon determining that the data processing engine best satisfies predefined criteria such as one or more of speed efficiency resource consumption job execution success rate user specified execution time constraints etc. In some embodiments the application may also include multiple application components also referred to herein as components.

In one embodiment the request handler manages integration of and interaction among all the components of the application . The request handler may be responsible for invoking a series of operations needed for processing the job run request . The engine manager may be responsible for running the desired job on the appropriate processing engine. To this end the engine manager may invoke the engine selector to select the appropriate processing engine based on predefined criteria. The engine manager may also invoke the score composer to generate an execution plan for the selected processing engine. The engine manager may also invoke the execution manager to implement the generated execution plan on the selected processing engine.

In one embodiment the engine selector may be responsible for selecting an appropriate processing engine for running a desired job. At least in some embodiments the processing engine may be selected based on a predefined model that has predefined criteria as inputs to the model. Examples of the predefined criteria include the job toplogy associated with the desired job the parallelization capacity for each operator in the topology the unit cost of reading and writing a fixed block of data to the backing store the expected mean time between failures MTBF for the desired job etc. The parallelization capacity also referred to herein as parallelization potential represents a measure of scalability of an individual component in a data flow.

Accordingly the engine selector may automatically select an appropriate processing engine on which to run a desired job based on a size of the input data associated with the desired job and based further on the computing resources available in the cloud environment. Further the same job may be submitted with different execution requirements which may in turn result in selection of a different processing engine for running the job.

In one embodiment the score composer is responsible for generating the execution plan for the given job on the selected processing engine. As stated above the execution manager may be configured to implement the execution plan of the given job on the selected processing engine. Accordingly the techniques disclosed herein may be adopted to facilitate migrating data integration processes to desired network environments such as cloud computing environments. At least in some embodiments the data integration processes may be migrated to a unified networked environment that supports multiple data processing engines such as parallel processing engine and distributed computing engine. Further the target or selected processing engine may vary from execution to execution of the same data flow thereby providing the retargetable property of each processing engine in the networked environment. The desired job is executed on a processing engine that is deemed by the application as being most suitable for executing the desired job based on predefined criteria such as job characteristics and execution requirements. To this end a job execution plan for the desired job and that is specific to the selected processing engine is generated and execution of the desired job commences on the selected processing engine based on the job execution plan. Consequently desired jobs may be executed more efficiently in the networked environment at least in some cases.

In some embodiments in order to evaluate the predefined criteria to select a processing engine the application generates a decision matrix that may include multiple elements. In a particular embodiment the decision matrix includes a job topology complexity index a job application logic complexity index a job performance index and a job high availability index. Depending on the embodiment each index may be represented as a numerical or string value. Based on the decision matrix a job success rate may also be determined. Depending on the embodiment the job success rate may be considered part of or external to the decision matrix. Further each of the elements other than the job success rate may be individually examined to determine its impact on the job success rate. An appropriate processing engine may then be selected based on the decision matrix. The job success rate may also be referred to as a job success likelihood or a job success probability.

In one embodiment the application uses empirical modeling to determine the job success rate and to select the appropriate processing engine. For example in a particular empirical model a data flow includes one or more branches and each branch is either independent which is characterized by no sharing of operators or correlated with another branch via one or more operators. A branch contains a chain of operators arranged in terms of producing and consuming sequences. The failure rate of an operator is equal to the estimated execution time divided by the MTBF whereas the success rate of an operator is equal to one minus its failure rate. The success rate of a branch on one partition is equal to the multiplication of the success rate of each operator on that partition while the success rate of a branch on all partitions is equal to the average success rate of all partitions. The success rate of a branch counting startup is equal to the average success rate of all partitions multiplied by the success rate of branch startup. The success rate of a data flow is equal to the minimum success rate among all branches. On the other hand in an alternative empirical model the success rate is determined by the success rate of all physical nodes and an extra cost needed for maintaining a desired level of high availability.

In one embodiment the engine selector performs one or more of the following steps to generate the decision matrix. The engine selector analyzes the data flow topology of the desired job step and application logic of each stage in the data flow step . The engine selector then estimates resource utilization associated with the desired job step . The engine selector then examines job execution requirements step and considers fault tolerance checkpoints and restart requirements if any step . In performing these steps the engine selector may determine one or more of a job topology complexity index a job application logic complexity index a job performance index and a job high availability index . Depending on the embodiment a single index of each type may be generated for a given job or multiple indices of the same type may be generated for the available processing environment each index specific to a different processing environment. Based on the generated indices the engine selector may generate one or more of a startup cost associated with the job topology complexity index a runtime cost associated with the job application logic complexity index a runtime cost associated with the job performance index and a startup and runtime cost associated with the job high availability index . The engine selector may then determine a job success rate for each available processing environment based on the decision matrix step . After the step the flowchart ends.

In one embodiment the job topology complexity index is determined as a predefined function of one or more of number of branches in the data flow of the desired job the number of operators in each branch the number of data sources the number of data sinks and the number of operators requiring intermediate storage. The job application logic complexity index is determined as a predefined function of one or more of the number of operators that require data be sorted and the number of operators which processing logic includes mapping merging aggregation transformation passthrough etc. The job performance index is determined as a measure of estimated resource consumption and as a function of job statistics obtained from previous executions of the desired job. The job high availability index is determined based on whether a given processing engine supports fault tolerance and job restart. The elements of the decision matrix and the predefined functions may be tailored to suit the needs of a particular case.

In one embodiment the success rate for each branch in the parallel engine data flow may be determined based on a predefined algorithm. One such algorithm is illustrated in Table II.

Consequently in one embodiment distributed computing jobs such as Hadoop jobs may have a higher job success rate because job failure often only occurs when all nodes fail as compared to parallel processing jobs where job failure occurs if any of the nodes fails. In one embodiment the criteria used in selecting the appropriate processing engine further takes into consideration other factors such as a service level agreement associated with a requesting client a hardware investment factor a system response time factor a job complexity factor etc.

In one embodiment for any other type of data flow the application may take into account resource utilization estimates e.g. CPU utilization total execution time for individual operators I O activity estimates etc. Using the resource utilization estimates as input into the job success rate model for the parallel processing engine in order to determine a job success rate. To this end the application may generate a decision matrix for the parallel processing engine step and then computes a job success rate for the parallel processing engine using the empirical model described above step . The application may then generate a decision matrix for the distributed computing engine step and then computes a job success rate for the distributed computing engine and by using the empirical model described above step . The application then determines which job success rate is higher step . If the job success rate for the parallel processing engine is higher then the application runs the desired job on the parallel processing engine step . On the other hand if the job success rate of the distributed computing engine is higher then the application runs the desired job on the distributed computing engine step . After the step or the method terminates.

Although some embodiments are herein described in conjunction with parallel processing engines and distributed computing engines other engine types are broadly contemplated. In this regard the number and specific types of engines may also be tailored to suit the needs of a particular case. In cases where there are more than two available types of engines available the application selects the engine having a highest job success rate as determined via the steps described above or via similar embodiments.

Accordingly at least some embodiments disclosed herein provide techniques for data integration on retargetable engines in desired network environments such as cloud computing environments. By selecting data processing engines based on the predefined criteria and according to the empirical models disclosed herein data flows may be executed more efficiently at least in some cases at least relative to alternative approaches that rely on other bases for selecting data processing engines. Examples of alternative approaches include approaches that do not consider the predefined criteria disclosed herein and approaches that rely on user input specifying a desired data processing engine to use.

In some embodiments checkpointing techniques may be applied in order to resume processing at one or more defined checkpoints upon failure of a job rather than restarting the job entirely. Parallel processing engines may scale linearly with the size of the input data modulo Amdahl s law which is used to find the maximum expected improvement to an overall system when only part of the system is improved. Checkpointing for such parallel processing engines however may often entail a high overhead. To overcome a need for checkpointing one approach involves assuming availability of resources increasing a level of parallelism to shrink the time window of processing to within acceptable bounds and then restarting any failed job without checkpointing. However with the increasing complexity of processing environments this approach may not necessarily be desirable in some cases. Software or hardware failures may occur with a higher frequency in such processing environments which may often execute jobs on a cluster of hundreds of nodes each with hundreds of cores.

Further checkpointing techniques may be used by applications for purposes such as recovery and restart. Some checkpointing techniques involve suspending each process in order to save its state. The suspension time of a data flow program as a whole may be determined by the longest suspension time among all of the processes associated with the data flow program. Accordingly such checkpointing techniques may adversely impact performance of the data flow program especially if a particular process has a lengthy suspension time. Other approaches provide simultaneous independent checkpointing in which each process may resume to a processing state once checkpointing of the respective process is complete. However performing checkpointing independently on each process may be resource intensive especially for data flow jobs that process a large volume of data. Still other approaches adopt a hammer technique in which the data flow job is checkpointed by writing data to storage on each link in the data flow also referred to herein as full checkpointing which may also be resource intensive at least in some cases. One example of such an approach is the MapReduce paradigm.

In some embodiments rather than placing checkpoints at each link in a data flow checkpoints are placed only for a subset of links in the data flow also referred to herein as qualified checkpointing. The subset of links may include each link satisfying predefined checkpoint criteria such as checkpointing overhead of the respective link likelihood of failure at the respective link etc. The checkpoint criteria may also include a predefined function of one or more criteria. Accordingly checkpointing may occur only at those links having a low checkpointing overhead and or that are more likely to exhibit failure.

At least in some embodiments in addition to or as an alternative to using checkpointing to provide fault tolerance and recovery checkpointing may also be used to optimize or at least improve data flow job performance. For example in one embodiment checkpointing may facilitate avoidance of performance degradation resulting from one or more bottlenecks in a data flow. When a bottleneck occurs at one or more links in the data flow upstream data processing is pushed back possibly all the back to input data sources even. In such a scenario pipeline and data partitioned parallelism no longer operates as desired. At least in some embodiments this scenario may be prevented altogether or made less likely to occur by placing checkpoints at the links associated with the bottlenecks.

As another example in one embodiment checkpointing may also facilitate performing a test run on a partial data flow also referred to herein as a subflow. At design time a user may desire to group a specified part of a data flow into a subflow run the subflow with specified data in order to test performance of the subflow and modify the data flow accordingly based on results of the test run. In some embodiments checkpoints may be used to connect a subflow to other subflows in the data flow. As a further example still in one embodiment checkpointing may facilitate running a data flow across different engines for specific processing needs. A data flow may include different types of processing logic such as stream processing logic and data processing logic. Stream processing subflows may run more efficiently on a distributed processing engine while data processing subflows may run more efficiently on a parallel processing engine. In some embodiments using checkpoints to connect different subflows of a data flow allows the data flow to run across different processing engine types improving the performance of the data flow at least in some cases.

Accordingly at least some embodiments disclosed herein provide checkpoint based data flow modeling techniques that can be used for one or more of improving performance of data flow jobs and recovering from data flow job failures. Rather than performing full checkpointing which involves placing a checkpoint on each link of a data flow and which may increase resource consumption and reduce data flow performance at least some techniques disclosed herein involve qualified checkpointing.

At least in some embodiments qualified checkpointing may include placing checkpoints of one or more of the checkpoint types shown in Table III.

As stated above in qualified checkpointing a subset of links may be selected based on predefined checkpoint criteria. In one embodiment each checkpoint criteria may be evaluated at one or more levels of granularity including the data flow level and the operator level. In some embodiments the predefined checkpoint criteria include one or more of the criteria shown in Table IV.

In some embodiments the techniques disclosed herein may be used to identify subflows that run in parallel either on a single processing engine or on multiple retargetable engines with one or more of increased pipeline parallelism and reduced bottleneck probability. Additionally or alternatively the operational metadata may be used to calculate a probability of bottleneck or failure within a data flow. Further one or more of data flow analysis operational metadata and cost of disk activity may be used to determine appropriate checkpoint placement. Further still programmatically placing checkpoints according to the techniques disclosed herein and without requiring any user input may reduce resource consumption reduce performance overhead and or improve efficiency of the data processing system in some cases at least relative to alternative approaches such as those in which full checkpointing is used.

As discussed above in some embodiments the application is configured to consider an entire job as a unit and decide whether to run the job on a distributed computing engine such as Hadoop or on a parallel processing engine. In some other embodiments the job is programmatically split into a set of smaller jobs of which performance centric jobs are executed on the distributed computed engine and I O centric jobs are executed on the parallel processing engine.

At step the score composer determines whether the target engine of the desired job is the parallel processing engine. If so the score composer generates a parallel execution plan for the desired job step . On the other hand if the score composer determines that the target engine is the distributed processing engine rather than the parallel processing engine step then the score composer generates a distributed execution plan for the desired job step . The score composer then serializes and stores the generated parallel or distributed execution plan to storage step .

Additionally or alternatively after the step the score composer may generate a checkpointing plan for the desired job or for one or more partial jobs created by the qualified checkpoints where each partial job corresponds to a distinct subflow of the data flow step . The score composer may generate a parallel execution plan for each partial job satisfying predefined parallel subflow criteria such as the partial job being I O centric to a predefined threshold step . The score composer may generate a distributed execution plan for each partial job satisfying predefined distributed subflow criteria such as the partial job being performance centric to a predefined threshold step . The score composer may serialize the parallel or distributed execution plan for each partial job to storage step . At least in some embodiments the checkpointing plan may be incorporated into the execution plan for each type of processing engine. After the step or if no processing engine is selected the flowchart ends.

At least in some embodiments the score composer determines checkpoint placement at multiple levels of granularity including a data flow level and an operator level. At the data flow level of granularity the score composer analyzes the desired job to determine an appropriate division of the data flow into a set of subflows based on predefined segmentation criteria. At the operator level of granularity the score composer retrieves operational metadata of previous executions of the desired job in order to determine failure rates of individual operators in the data flow. The failure rates may then be used to identify appropriate locations for qualified checkpoints in order to provide the ability to recover from potential failures in executing the desired job.

In one embodiment based on the step the score composer may identify subflows appropriate for partial runs step and generate one or more connection checkpoints based on the identified subflows step . Additionally or alternatively based on the step the score composer may identify subflows appropriate for specific processing engines step and generate one or more retargetable checkpoints based on the identified subflows step . Similarly based on the step the score composer may identify subflows appropriate for increased pipeline parallelism in processing the data flow step and generate one or more parallel checkpoints based on the identified subflows step .

Further in one embodiment the score composer may identify subflows appropriate for reducing bottlenecks in processing the data flow step and generate one or more bottleneck checkpoints based on the identified subflows step . Further still the score composer may identify subflows appropriate for failure recovery when processing the data flow step and generate one or more recovery checkpoints based on the identified subflows step . At least in some embodiments each set of identified subflows may be determined based on respective criteria specific to the checkpoint type associated with the respective set of identified subflows. At step the score composer may then generate a checkpointing plan for the data flow based on results from the above steps of the flowchart . After the step the flowchart ends.

At least in some embodiments one or more of the parallel processing engine and the distributed computing engine may also collect operational metrics when executing a desired job. Examples of operational metrics are shown in Table V.

As stated above in one embodiment the score composer may generate different types of checkpoints such as retargetable checkpoints connection checkpoints parallel checkpoints bottleneck checkpoints and recovery checkpoints at a data flow level of granularity. In some alternative embodiments the recovery checkpoints are not determined at the data flow level of granularity but only at an operator level of granularity.

As stated above in one embodiment the score composer may also generate different types of checkpoints such as retargetable checkpoints connection checkpoints parallel checkpoints bottleneck checkpoints and recovery checkpoints at an operator level of granularity. Further as stated above in some alternative embodiments the recovery checkpoints are only determined at the operator level of granularity and not at the data flow level of granularity.

In one embodiment in some distributed computing engines such as Hadoop jobs are modeled as a simple flow of a mapper process to a reducer process. The output of the mapper process may be serialized to HDFS before invoking the reducer process. In doing so the reducer process may be re executed in event of failure. Further if a node fails a new reducer process may be launched on a different node to process the same set of data.

In one embodiment the following formula may be used to estimate usage of temporary storage space by a data flow also referred to herein as scratch disk space scratch disk space Equation 8 In Equation 18 N represents the number of links in the data flow R represents the replication factor of the processing engine and S represents the size of the initial input data to the data flow. Based on the above equation the intermediate checkpoint data sets would total an estimated twelve hundred gigabytes in scratch disk space. Thus it may be costly to execute the data flow especially in scenarios where the size of the initial input data is even greater. In some embodiments persisting such large data sets on disk not only significantly increases storage costs but also adds significant processing overhead which may severely impact application performance at least in some cases. The utility of the cluster system may also be undermined as a result of significant processing time devoted to writing checkpointing information to facilitate executing the job as opposed to devoting processing time solely to executing the job and producing an output.

Accordingly selectively placing qualified checkpoints in the data flow may reduce the overhead of checkpointing lower the storage costs and improve application performance at least in some cases all while maintaining the same or a similar level of service that is provided by other approaches such as full checkpointing. In some embodiments the qualified checkpoints may be placed at an operator level of granularity based on predefined operator checkpoint criteria. In one embodiment the operator checkpointing criteria may include one or more of whether a given operator is stateful or stateless an individual failure rate of the given operator and a buffer usage pattern of the given operator. In some embodiments the operator checkpointing criteria may include a predefined function of one or more of these criteria. The predefined function may assign a distinct weight to each criterion.

At least in some embodiments one or more types of runtime operational aspects of job run metadata may be collected. Examples of runtime operational aspects are shown in Table VI.

As stated above in one embodiment the operator checkpointing criteria may include a buffer usage pattern of the given operator. At least in some embodiments any operator configured to perform an amount of buffering exceeding a predetermined threshold may be considered a candidate for a qualified checkpoint. For instance in some embodiments a sort operator may only sort data after having received a complete input data set. If any error occurs while awaiting the input data set the entire job may need to be restarted. On the other hand if the input data set is checkpointed then upon restart the sort operation may begin processing using the data set in storage. As another example a de duplication operator may also require buffering of all input data before sorting and removing duplicate records based on a specified key. Accordingly the de duplication operator may also be a candidate for a qualified checkpoint based on buffer usage pattern. At least in some embodiments the buffer usage pattern of a given link is a criterion taken into account by the score composer in conjunction with other operator checkpoint criteria in determining whether to checkpoint the given link.

Accordingly at least some embodiments disclosed herein provide techniques for qualified checkpointing for data flows processed in network environments such as cloud computing environments. At least in some cases selectively placing checkpoints based on predefined checkpointing criteria and as disclosed herein may reduce the overhead of checkpointing lower the storage costs and improve application performance while maintaining the same or a similar level of service that is provided by alternative approaches such as full checkpointing.

The computer generally includes a processor connected via a bus to a memory a network interface device a storage an input device and an output device . The computer is generally under the control of an operating system. Examples of operating systems include IBM z OS UNIX versions of the Microsoft Windows operating system and distributions of the Linux operating system. More generally any operating system supporting the functions disclosed herein may be used. The processor is included to be representative of a single CPU multiple CPUs a single CPU having multiple processing cores and the like. Similarly the memory may be a random access memory. While the memory is shown as a single identity it should be understood that the memory may comprise a plurality of modules and that the memory may exist at multiple levels from high speed registers and caches to lower speed but larger DRAM chips. The network interface device may be any type of network communications device allowing the computer to communicate with other computers via the network .

The storage may be a persistent storage device. Although the storage is shown as a single unit the storage may be a combination of fixed and or removable storage devices such as fixed disc drives solid state drives floppy disc drives tape drives removable memory cards or optical storage. The memory and the storage may be part of one virtual address space spanning multiple primary and secondary storage devices.

The input device may be any device for providing input to the computer . For example a keyboard a mouse a touchpad voice commands or any combination thereof may be used. The output device may be any device for providing output to a user of the computer . For example the output device may be any display screen or set of speakers. Although shown separately from the input device the output device and input device may be combined. For example a display screen with an integrated touch screen may be used.

As shown the memory of the computer includes the application of . Depending on the embodiment one or more of the components of the application as depicted in may execute on the computer or on one or more other computers operatively connected to the computer via the network . Further the client and the execution environment of may each also execute on one or more other computers operatively connected to the computer via the network .

In the preceding reference is made to embodiments presented in this disclosure. However the scope of the present disclosure is not limited to specific described embodiments. Instead any combination of the following features and elements whether related to different embodiments or not is contemplated to implement and practice contemplated embodiments. Furthermore although embodiments disclosed herein may achieve advantages over other possible solutions or over the prior art whether or not a particular advantage is achieved by a given embodiment is not limiting of the scope of the present disclosure. Thus the preceding aspects features embodiments and advantages are merely illustrative and are not considered elements or limitations of the appended claims except where explicitly recited in a claim s . Likewise reference to the invention shall not be construed as a generalization of any inventive subject matter disclosed herein and shall not be considered to be an element or limitation of the appended claims except where explicitly recited in a claim s .

Aspects presented in this disclosure may be embodied as a system method or computer program product. Accordingly aspects disclosed herein may take the form of an entirely hardware embodiment an entirely software embodiment including firmware resident software micro code etc. or an embodiment combining software and hardware aspects that may all generally be referred to herein as a circuit module or system. Furthermore aspects disclosed herein may take the form of a computer program product embodied in one or more computer readable medium s having computer readable program code embodied thereon.

Any combination of one or more computer readable medium s may be utilized. The computer readable medium may be a computer readable signal medium or a computer readable storage medium. A computer readable storage medium may be for example but not limited to an electronic magnetic optical electromagnetic infrared or semiconductor system apparatus or device or any suitable combination of the foregoing. More specific examples a non exhaustive list of the computer readable storage medium would include the following an electrical connection having one or more wires a portable computer diskette a hard disk a random access memory RAM a read only memory ROM an erasable programmable read only memory EPROM or Flash memory an optical fiber a portable compact disc read only memory CD ROM an optical storage device a magnetic storage device or any suitable combination of the foregoing. In the context of this disclosure a computer readable storage medium may be any tangible medium that can contain or store a program for use by or in connection with an instruction execution system apparatus or device.

A computer readable signal medium may include a propagated data signal with computer readable program code embodied therein for example in baseband or as part of a carrier wave. Such a propagated signal may take any of a variety of forms including but not limited to electro magnetic optical or any suitable combination thereof. A computer readable signal medium may be any computer readable medium that is not a computer readable storage medium and that can communicate propagate or transport a program for use by or in connection with an instruction execution system apparatus or device.

Program code embodied on a computer readable medium may be transmitted using any appropriate medium including but not limited to wireless wireline optical fiber cable RF etc. or any suitable combination of the foregoing.

Computer program code for carrying out operations for aspects disclosed herein may be written in any combination of one or more programming languages including an object oriented programming language such as Java Smalltalk C or the like and conventional procedural programming languages such as the C programming language or similar programming languages. The program code may execute entirely on the computer of a user partly on the computer of the user as a stand alone software package partly on the computer of the user and partly on a remote computer or entirely on the remote computer or server. In the latter scenario the remote computer may be connected to the computer of the user via any type of network including a local area network LAN or a wide area network WAN or the connection may be made to an external computer for example through the Internet using an Internet Service Provider .

Aspects presented in this disclosure are described above with reference to flowchart illustrations or block diagrams of methods apparatus systems and computer program products according to embodiments disclosed herein. It will be understood that each block of the flowchart illustrations or block diagrams and combinations of blocks in the flowchart illustrations or block diagrams can be implemented by computer program instructions. These computer program instructions may be provided to a processor of a general purpose computer special purpose computer or other programmable data processing apparatus to produce a machine such that the instructions which execute via the processor of the computer or other programmable data processing apparatus create means for implementing the functions acts specified in the flowchart or block diagram block or blocks.

These computer program instructions may also be stored in a computer readable medium that can direct a computer other programmable data processing apparatus or other devices to function in a particular manner such that the instructions stored in the computer readable medium produce an article of manufacture including instructions which implement the function act specified in the flowchart or block diagram block or blocks.

The computer program instructions may also be loaded onto a computer other programmable data processing apparatus or other devices to cause a series of operational steps to be performed on the computer other programmable apparatus or other devices to produce a computer implemented process such that the instructions which execute on the computer or other programmable apparatus provide processes for implementing the functions acts specified in the flowchart or block diagram block or blocks.

It is understood in advance that although this disclosure includes a detailed description on cloud computing implementation of the teachings recited herein are not limited to a cloud computing environment. Rather embodiments of the present invention are capable of being implemented in conjunction with any other type of computing environment now known or later developed.

For convenience the Detailed Description includes the following definitions which have been derived from the Draft NIST Working Definition of Cloud Computing by Peter Mell and Tim Grance dated Oct. 7 2009.

Cloud computing is a model of service delivery for enabling convenient on demand network access to a shared pool of configurable computing resources e.g. networks network bandwidth servers processing memory storage applications virtual machines and services that can be rapidly provisioned and released with minimal management effort or interaction with a provider of the service. This cloud model may include at least five characteristics at least three service models and at least four deployment models.

On demand self service a cloud consumer can unilaterally provision computing capabilities such as server time and network storage as needed automatically without requiring human interaction with the service s provider.

Broad network access capabilities are available over a network and accessed through standard mechanisms that promote use by heterogeneous thin or thick client platforms e.g. mobile phones laptops and PDAs .

Resource pooling the provider s computing resources are pooled to serve multiple consumers using a multi tenant model with different physical and virtual resources dynamically assigned and reassigned according to demand. There is a sense of location independence in that the consumer generally has no control or knowledge over the exact location of the provided resources but may be able to specify location at a higher level of abstraction e.g. country state or datacenter .

Rapid elasticity capabilities can be rapidly and elastically provisioned in some cases automatically to quickly scale out and rapidly released to quickly scale in. To the consumer the capabilities available for provisioning often appear to be unlimited and can be purchased in any quantity at any time.

Measured service cloud systems automatically control and optimize resource use by leveraging a metering capability at some level of abstraction appropriate to the type of service e.g. storage processing bandwidth and active user accounts . Resource usage can be monitored controlled and reported providing transparency for both the provider and consumer of the utilized service.

Software as a Service SaaS the capability provided to the consumer is to use the provider s applications running on a cloud infrastructure. The applications are accessible from various client devices through a thin client interface such as a web browser e.g. web based e mail . The consumer does not manage or control the underlying cloud infrastructure including network servers operating systems storage or even individual application capabilities with the possible exception of limited user specific application configuration settings.

Platform as a Service PaaS the capability provided to the consumer is to deploy onto the cloud infrastructure consumer created or acquired applications created using programming languages and tools supported by the provider. The consumer does not manage or control the underlying cloud infrastructure including networks servers operating systems or storage but has control over the deployed applications and possibly application hosting environment configurations.

Infrastructure as a Service IaaS the capability provided to the consumer is to provision processing storage networks and other fundamental computing resources where the consumer is able to deploy and run arbitrary software which can include operating systems and applications. The consumer does not manage or control the underlying cloud infrastructure but has control over operating systems storage deployed applications and possibly limited control of select networking components e.g. host firewalls .

Private cloud the cloud infrastructure is operated solely for an organization. It may be managed by the organization or a third party and may exist on premises or off premises.

Community cloud the cloud infrastructure is shared by several organizations and supports a specific community that has shared concerns e.g. mission security requirements policy and compliance considerations . It may be managed by the organizations or a third party and may exist on premises or off premises.

Public cloud the cloud infrastructure is made available to the general public or a large industry group and is owned by an organization selling cloud services.

Hybrid cloud the cloud infrastructure is a composition of two or more clouds private community or public that remain unique entities but are bound together by standardized or proprietary technology that enables data and application portability e.g. cloud bursting for load balancing between clouds .

A cloud computing environment is service oriented with a focus on statelessness low coupling modularity and semantic interoperability. At the heart of cloud computing is an infrastructure comprising a network of interconnected nodes.

Referring now to a schematic of an example of a cloud computing node for data integration on retargetable engines is shown. Cloud computing node is only one example of a suitable cloud computing node and is not intended to suggest any limitation as to the scope of use or functionality of embodiments of the invention described herein. Regardless cloud computing node is capable of being implemented and or performing any of the functionality set forth hereinabove.

In cloud computing node there is a computer system server which is operational with numerous other general purpose or special purpose computing system environments or configurations. Examples of well known computing systems environments and or configurations that may be suitable for use with computer system server include but are not limited to personal computer systems server computer systems thin clients thick clients hand held or laptop devices multiprocessor systems microprocessor based systems set top boxes programmable consumer electronics network PCs minicomputer systems mainframe computer systems and distributed cloud computing environments that include any of the above systems or devices and the like.

Computer system server may be described in the general context of computer system executable instructions such as program modules being executed by a computer system. Generally program modules may include routines programs objects components logic data structures and so on that perform particular tasks or implement particular abstract data types. Computer system server may be practiced in distributed cloud computing environments where tasks are performed by remote processing devices that are linked through a communications network. In a distributed cloud computing environment program modules may be located in both local and remote computer system storage media including memory storage devices.

As shown in computer system server in cloud computing node is shown in the form of a general purpose computing device. The components of computer system server may include but are not limited to one or more processors or processing units a system memory and a bus that couples various system components including system memory to processor .

Bus represents one or more of any of several types of bus structures including a memory bus or memory controller a peripheral bus an accelerated graphics port and a processor or local bus using any of a variety of bus architectures. By way of example and not limitation such architectures include Industry Standard Architecture ISA bus Micro Channel Architecture MCA bus Enhanced ISA EISA bus Video Electronics Standards Association VESA local bus and Peripheral Component Interconnects PCI bus. Computer system server typically includes a variety of computer system readable media. Such media may be any available media that is accessible by computer system server and it includes both volatile and non volatile media removable and non removable media.

System memory can include computer system readable media in the form of volatile memory such as random access memory RAM and or cache memory . Computer system server may further include other removable non removable volatile non volatile computer system storage media. By way of example only storage system can be provided for reading from and writing to a non removable non volatile magnetic media not shown and typically called a hard drive . Although not shown a magnetic disk drive for reading from and writing to a removable non volatile magnetic disk e.g. a floppy disk and an optical disk drive for reading from or writing to a removable non volatile optical disk such as a CD ROM DVD ROM or other optical media can be provided. In such instances each can be connected to bus by one or more data media interfaces. As will be further depicted and described below memory may include at least one program product having a set e.g. at least one of program modules that are configured to carry out the functions of embodiments of the invention.

Program utility having a set at least one of program modules may be stored in memory by way of example and not limitation as well as an operating system one or more application programs other program modules and program data. Each of the operating system one or more application programs other program modules and program data or some combination thereof may include an implementation of a networking environment. Program modules generally carry out the functions and or methodologies of embodiments of the invention as described herein.

Computer system server may also communicate with one or more external devices such as a keyboard a pointing device a display etc. one or more devices that enable a user to interact with computer system server and or any devices e.g. network card modem etc. that enable computer system server to communicate with one or more other computing devices. Such communication can occur via I O interfaces . Still yet computer system server can communicate with one or more networks such as a local area network LAN a general wide area network WAN and or a public network e.g. the Internet via network adapter . As depicted network adapter communicates with the other components of computer system server via bus . It should be understood that although not shown other hardware and or software components could be used in conjunction with computer system server . Examples include but are not limited to microcode device drivers redundant processing units external disk drive arrays RAID systems tape drives and data archival storage systems etc.

Referring now to illustrative cloud computing environment for data integration on retargetable engines is depicted. As shown cloud computing environment comprises one or more cloud computing nodes with which local computing devices used by cloud consumers such as for example personal digital assistant PDA or cellular telephone A desktop computer B laptop computer C and or automobile computer system N may communicate. Nodes may communicate with one another. They may be grouped not shown physically or virtually in one or more networks such as Private Community Public or Hybrid clouds as described hereinabove or a combination thereof. This allows cloud computing environment to offer infrastructure platforms and or software as services for which a cloud consumer does not need to maintain resources on a local computing device. It is understood that the types of computing devices A N shown in are intended to be illustrative only and that computing nodes and cloud computing environment can communicate with any type of computerized device over any type of network and or network addressable connection e.g. using a web browser .

Referring now to a set of functional abstraction layers provided by cloud computing environment for data integration on retargetable engines is shown. It should be understood in advance that the components layers and functions shown in are intended to be illustrative only and embodiments of the invention are not limited thereto. As depicted the following layers and corresponding functions are provided 

Hardware and software layer includes hardware and software components. Examples of hardware components include mainframes in one example IBM zSeries systems RISC Reduced Instruction Set Computer architecture based servers in one example IBM pSeries systems IBM xSeries systems IBM BladeCenter systems storage devices networks and networking components. Examples of software components include network application server software in one example IBM WebSphere application server software and database software in one example IBM DB2C database software. IBM zSeries pSeries xSeries BladeCenter WebSphere and DB2 are trademarks of International Business Machines Corporation registered in many jurisdictions worldwide 

Virtualization layer provides an abstraction layer from which the following examples of virtual entities may be provided virtual servers virtual storage virtual networks including virtual private networks virtual applications and operating systems and virtual clients.

In one example management layer may provide the functions described below. Resource provisioning provides dynamic procurement of computing resources and other resources that are utilized to perform tasks within the cloud computing environment. Metering and Pricing provide cost tracking as resources are utilized within the cloud computing environment and billing or invoicing for consumption of these resources. In one example these resources may comprise application software licenses. Security provides identity verification for cloud consumers and tasks as well as protection for data and other resources. User portal provides access to the cloud computing environment for consumers and system administrators. Service level management provides cloud computing resource allocation and management such that required service levels are met. Service Level Agreement SLA planning and fulfillment provide pre arrangement for and procurement of cloud computing resources for which a future requirement is anticipated in accordance with an SLA. The SLA generally specifies the services priorities responsibilities guarantees and or warranties that exist between a service provider and a customer.

Workloads layer provides examples of functionality for which the cloud computing environment may be utilized. Examples of workloads and functions which may be provided from this layer include mapping and navigation software development and lifecycle management virtual classroom education delivery data analytics processing transaction processing and mobile desktop.

Embodiments disclosed herein may be provided to end users through a cloud computing infrastructure. Cloud computing generally refers to the provision of scalable computing resources as a service over a network. More formally cloud computing may be defined as a computing capability that provides an abstraction between the computing resource and its underlying technical architecture e.g. servers storage networks enabling convenient on demand network access to a shared pool of configurable computing resources that can be rapidly provisioned and released with minimal management effort or service provider interaction. Thus cloud computing allows a user to access virtual computing resources e.g. storage data applications and even complete virtualized computing systems in the cloud without regard for the underlying physical systems or locations of those systems used to provide the computing resources.

Typically cloud computing resources are provided to a user on a pay per use basis where users are charged only for the computing resources actually used e.g. an amount of storage space consumed by a user or a number of virtualized systems instantiated by the user . A user can access any of the resources that reside in the cloud at any time and from anywhere across the Internet. In context of the embodiments presented herein a user may request a data flow to be executed on an appropriate data processing engine available in the cloud. The application may determine the appropriate data processing engine based on predefined criteria and according to an empirical model disclosed herein. Additionally or alternatively the application may generate one or more qualified checkpoints for a desired data flow based on the techniques disclosed herein. Thus the user may request execution of data flows and access results thereof from any computing system attached to a network connected to the cloud e.g. the Internet and be charged based on the processing environment s used.

The flowchart and block diagrams in the Figures illustrate the architecture functionality and operation of possible implementations of systems methods and computer program products according to various embodiments disclosed herein. In this regard each block in the flowchart or block diagrams may represent a module segment or portion of code which comprises one or more executable instructions for implementing the specified logical function s . In some alternative implementations the functions noted in the block may occur out of the order noted in the figures. For example two blocks shown in succession may in fact be executed substantially concurrently or the blocks may sometimes be executed in the reverse order depending upon the functionality involved. Each block of the block diagrams or flowchart illustration and combinations of blocks in the block diagrams or flowchart illustration can be implemented by special purpose hardware based systems that perform the specified functions or acts or combinations of special purpose hardware and computer instructions.

While the foregoing is directed to embodiments presented in this disclosure other and further embodiments may be devised without departing from the basic scope of contemplated embodiments and the scope thereof is determined by the claims that follow.

