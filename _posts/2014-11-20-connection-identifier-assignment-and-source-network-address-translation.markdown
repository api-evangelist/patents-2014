---

title: Connection identifier assignment and source network address translation
abstract: A controller of a network control system for configuring several middlebox instances is described. The middlebox instances implement a middlebox in a distributed manner in several hosts. The controller assigns a first set of identifiers to a first middlebox instance that associates an identifier in the first set with a first packet. The controller assigns a second set of identifiers to a second middlebox instance that associates an identifier in the second set with a second packet.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09306909&OS=09306909&RS=09306909
owner: NICIRA, INC.
number: 09306909
owner_city: Palo Alto
owner_country: US
publication_date: 20141120
---
This application is a divisional application of U.S. patent application Ser. No. 13 678 518 filed Nov. 15 2012 now issued as U.S. Pat. No. 8 913 611. U.S. patent application Ser. No. 13 678 518 claims the benefit of U.S. Provisional Application 61 560 279 entitled Virtual Middlebox Services filed Nov. 15 2011. U.S. patent application Ser. No. 13 678 518 now issued as U.S. Pat. No. 8 913 611 and U.S. Provisional Application 61 560 279 are incorporated herein by reference.

Many enterprises have large and sophisticated networks comprising switches hubs routers middleboxes servers workstations and other networked devices which support a variety of connections applications and systems. The increased sophistication of computer networking including virtual machine migration dynamic workloads multi tenancy and customer specific quality of service and security configurations require a better paradigm for network control. Networks have traditionally been managed through low level configuration of individual network components. Network configurations often depend on the underlying network for example blocking a user s access with an access control list ACL entry requires knowing the user s current IP address. More complicated tasks require more extensive network knowledge for example forcing guest users port traffic to traverse an HTTP proxy requires knowing the current network topology and the location of each guest. This process is of increased difficulty where the network switching elements are shared across multiple users.

In response there is a growing movement towards a new network control paradigm called Software Defined Networking SDN . In the SDN paradigm a network controller running on one or more servers in a network controls maintains and implements control logic that governs the forwarding behavior of shared network switching elements on a per user basis. Making network management decisions often requires knowledge of the network state. To facilitate management decision making the network controller creates and maintains a view of the network state and provides an application programming interface upon which management applications may access a view of the network state.

Some of the primary goals of maintaining large networks including both datacenters and enterprise networks are scalability mobility and multi tenancy. Many approaches taken to address one of these goals results in hampering at least one of the others. For instance one can easily provide network mobility for virtual machines within an L domain but L domains cannot scale to large sizes. Furthermore retaining user isolation greatly complicates mobility. As such improved solutions that can satisfy the scalability mobility and multi tenancy goals are needed.

Some embodiments of the invention provide a network control system that allows a user to specify a logical network that includes one or more logical forwarding elements e.g. logical switches logical routers etc. and one or more middleboxes e.g. firewalls load balancers network address translators intrusion detection systems IDS wide area network WAN optimizers etc. . The system implements the user specified logical forwarding elements across numerous managed switching elements on numerous physical machines that also host virtual machines of the logical network. The system implements the user specified middleboxes across the numerous physical machines. Typically the system of some embodiments configures in one physical machine a managed switching element that implements at least part of the logical switching elements and a distributed middlebox instance that provides a middlebox service to the packets forwarded by the managed switching element.

In some embodiments a managed switching element that receives a packet from a VM that is hosted in the same physical machine performs all or most of the logical forwarding processing of the logical forwarding elements on the received packet. Because the managed switching element receives the packet from the VM and performs forwarding processing on the packet the managed switching element is the first hop managed switching element with respect to the packet. While the first hop managed switching element is performing the logical forwarding of the packet the first hop managed switching element has the distributed middlebox instance that is running in the same host to process the packet according to the middlebox service that the distributed middlebox instance provides.

Since the distributed middlebox instances provide middlebox services to the packets forwarded by the managed switching elements that are running in the same hosts in which the distributed middlebox instances runs possibly using the same algorithm or mechanism packets processed by these distributed middlebox instances that are heading to the same destination may look identical from the viewpoint of the destination. For instance packets sent out by virtual machines in different physical machines to establish connections with other virtual machines may be processed by the distributed middlebox instances hosted in the different physical machines. The distributed middlebox instances provide a source network address translation SNAT service to the packets e.g. by translating the source network addresses of the packets into different network addresses to hide the real source network addresses . These packets then may have the same network address as the source network addresses of the packets. When these packets are heading to the same destination these packets may be identical in terms of the five tuples that the packets have e.g. source and destination network addresses source and destination port numbers transport protocol type even though these packets originate from different virtual machines. Consequently the packets may appear to be packets of the same connection even though the packets should each be packets of their own connections.

The network control system of some embodiments configures the distributed middlebox instances in such a way that the distributed middlebox instances assign identifiers to the packets having the same five tuple so that the connections established by the packets are distinguishable. Different embodiments assign the connection identifiers differently. For instance in some embodiments the system assigns a non overlapping range of connection identifiers to each of the distributed middlebox instances that implement a middlebox. The distributed middlebox instances use identifiers within the range and the packets processed by these distributed middlebox instance can be uniquely identified by the identifier that is not used for other live connections. Alternatively or conjunctively the network control system of some embodiments provides a set of application programming protocols APIs that each distributed middlebox instance can use to obtain and release a range of connection identifiers on demand. In these embodiments the network control system maintains the available i.e. not being used and unavailable i.e. being used ranges of connection identifiers.

In some embodiments the network control system lets each distributed middlebox maintain the entire available range of connection identifiers and assign connection identifiers to the packets forwarded by the managed switching element that are last hop managed switching elements with respect to the packets. A managed switching element is a last hop managed switching element with respect to a packet when the managed switching element forwards the packet to a destination virtual machine that runs in the same host in which the managed switching element runs.

The network control system of some embodiments implements a middlebox that provides a SNAT service in a distributed manner. The network control system receives from a user configuration data for configuring the middlebox including SNAT rules to use to translate source addresses of incoming packets. The network control system configures the distributed middlebox instances that implement the middlebox to provide SNAT service in a similar way to how the network control system configures the managed switching elements to perform logical forwarding processing of the logical switching elements of the user.

In some embodiments the network control system has several controllers including logical controllers and physical controllers. A logical controller is a master of logical switching elements of a user. A logical controller of some embodiments receives a specification of the logical switching elements from the user in the form of logical control plane LCP data. A logical controller translates the LCP data into logical forwarding plane LFP data which define control plane and forwarding plane of the logical switching elements. A logical controller then translates the LFP data to the universal physical control plane data. A logical controller then identifies a set of physical controllers each of which is responsible for managing a managed switching element. A logical controller sends the universal control plane data only to the identified set of physical controllers that manages managed switching elements each of which at least partially implements the logical switching elements of the user.

A physical controller translates the universal physical control plane data into customized physical control plane data which is control plane data for the managed switching elements that implement the logical switching elements. The physical controller sends the customized physical control plane data to the managed switching element. The managed switching element then translates the customized control plane to perform the logical forwarding processing of the logical switching elements specified by the user.

Similarly a logical controller receives configuration data for configuring the middlebox. The logical controller identifies the same set of physical controllers which are masters of the managed switching elements that implement at least partially the logical switching elements specified by the user. The logical controller sends the middlebox configuration data to the identified set of physical controllers. The physical controller of some embodiments then sends the middlebox configuration data to the managed switching elements so that the managed switching elements can send the middlebox configuration data to the distributed middlebox instances that run in the same host in which the managed switching elements run. Alternatively the physical controller sends the middlebox configuration data directly to the distributed middlebox instance which runs in the same host with the managed switching elements of which the physical controller is the master.

The preceding Summary is intended to serve as a brief introduction to some embodiments of the invention. It is not meant to be an introduction or overview of all inventive subject matter disclosed in this document. The Detailed Description that follows and the Drawings that are referred to in the Detailed Description will further describe the embodiments described in the Summary as well as other embodiments. Accordingly to understand all the embodiments described by this document a full review of the Summary Detailed Description and the Drawings is needed. Moreover the claimed subject matters are not to be limited by the illustrative details in the Summary Detailed Description and the Drawing but rather are to be defined by the appended claims because the claimed subject matters can be embodied in other specific forms without departing from the spirit of the subject matters.

In the following detailed description of the invention numerous details examples and embodiments of the invention are set forth and described. However it will be clear and apparent to one skilled in the art that the invention is not limited to the embodiments set forth and that the invention may be practiced without some of the specific details and examples discussed.

Some embodiments of the invention provide a network control system that allows the logical datapath sets of different users to be implemented by switching elements of a physical network. These switching elements are referred to below as managed switching elements MSEs or managed forwarding elements as they are managed by the network control system in order to implement the logical datapath sets. Examples of such switching elements include virtual or physical network switches software switches e.g. Open vSwitch routers etc. In some embodiments the logical datapath sets are implemented in the managed switching element in a manner that prevents the different users from viewing or controlling each other s logical datapath sets i.e. each other s switching logic while sharing the same switching elements.

To implement logical datapath sets the network control system of some embodiments generates physical control plane data from logical datapath set data specified by the users. The physical control plane data is then downloaded to the MSEs. The MSEs convert the physical control plane data into physical forwarding plane data that allows the MSEs to perform forwarding of the packets that these MSEs receive. Based on the physical forwarding data the MSEs can process data packets in accordance with the logical processing rules specified within the physical control plane data.

In some embodiments each of the logical datapath sets defines a logical network that includes one or more logical switching elements. A logical switching element can process incoming packets in layer L or layer L . That is a logical switching element can function as a logical switch for switching packets at L and or as a logical router for routing packets at L. The network control system implements the logical switching elements of different users across the MSEs.

In addition to the logical switching elements the network control system of some embodiments allows the users to specify middleboxes. As known in the art middleboxes perform data processing other than forwarding the data e.g. network address translation load balance firewall intrusion detection and prevention wide area network optimization etc. . The middleboxes provide these middlebox services to the users respective logical switching elements. The network control system implements the specified middleboxes in the physical infrastructure of the physical network including the hosts in which the MSEs operate.

Several examples of such systems are described below in Section I. Section II then describes distributed middlebox instances that provide SNAT service. Section III describes an electronic system that implements some embodiments of the invention.

As shown in the top half of the figure the logical network includes two logical switches and a logical router and a middlebox . The logical switch is connected to virtual machines VMs and and the logical router . There may be many other VMs connected to the logical switch but they are not depicted in this figure for the simplicity of illustration and description. The logical switch forwards data between VMs connected to the logical switch at L e.g. by using MAC addresses and between the VMs and the logical router when the data needs routing at L e.g. by using IP addresses . Like the logical switch the logical switch forwards data between the logical router and the VMs connected to the logical switch .

The logical router routes data at L among the logical switches connected to the logical router and the middlebox . When the data needs middlebox service e.g. source network address translation the logical router sends the data to the middlebox to process and in some cases receives the processed data back from the middlebox to route the data to the data s destination. The logical router also routes data to and from the external network which includes network elements that do not belong to the logical network .

As shown in the bottom half of the physical network includes hosts . A host is a machine that is managed by an operating system e.g. Linux Windows etc. that is capable of running software applications and virtual machines. Each of the hosts has several network elements running in the host including several MSEs several distributed middlebox instances and or several VMs. Not all of these network elements are depicted in each host in this figure for the simplicity of illustration and description. In some embodiments a MSE is a software switching element that has components running in the user space and or the kernel of the host on which the software is running Also a distributed middlebox instance in some embodiments is a software application that has components running in the user space and or the kernel. In some embodiments a distributed middlebox instance is provisioned in a VM running in the host in which the MSE is running

As shown the host includes MSE a distributed middlebox instance and VM . The host includes MSE a distributed middlebox instance and VM . The host includes MSE a distributed middlebox instance and VM . The host includes MSE and a distributed middlebox instance .

The MSEs implement the logical switches and and the logical router in a distributed manner. That is the MSEs of some embodiments collectively perform the data forwarding operations of the logical switches and and the logical router . Specifically the ports not shown of the logical switches are mapped to physical ports e.g. virtual interfaces VIFs not shown of the MSEs . The VMs that send and receive data to and from the logical switches through the ports of the logical switches actually send and receive the data to and from the MSEs through the physical ports of the MSEs to which the ports of the logical switches are mapped. The MSEs have forwarding tables not shown that include the physical forwarding plane data in the form of flow entries. In some embodiments a flow entry includes a qualifier and an action. The qualifier specifies a condition which when it is met directs the MSE to perform the action. The MSEs perform the data forwarding operations of the logical switching elements logical switches and logical routers according to the actions specified in the flow entries. Forwarding tables and flow entries will be described further below by reference to .

The MSE that receives data from a VM is referred to as a first hop MSE with respect to that data. In some embodiments the first hop MSEs performs all or most of the logical processing that are to be performed on the received data in order for the data to reach the data s destination. For instance when the logical switch receives a data packet from VM that is addressed to VM the logical switch forwards the packet to the logical router . The logical router then routes the packet to the logical switch which will forward the packet to VM . In the physical network the MSE is the first hop MSE with respect to this packet and performs logical processing to send the packet to VM which is connected to the MSE . That is the MSE performs the forwarding operations of the logical switch the logical router and the logical switch to send the packet from VM to the VM . Likewise for packets from VM to VM or VM the MSE as the first hop MSE for these packets performs the forwarding operations of the logical switch the logical router and the logical switch . The MSE will also perform the forwarding operations of the logical switch the logical router and the logical switch to send data packets from VM to VM or VM .

The MSEs exchange data amongst themselves via tunnels established between them. These tunnels allow the data to be exchanged among the MSEs over the other network elements not shown of the physical network . In some embodiments the network control system does not manage these other network elements of the physical network . These other network elements thus serve as switching fabric for the MSEs to use to exchange data. As shown each of the MSEs establishes a tunnel to each of the other MSEs.

Different types of tunneling protocols are supported in different embodiments. Examples of tunneling protocols include control and provisioning of wireless access points CAPWAP generic route encapsulation GRE GRE Internet Protocol Security IPsec among others.

In some embodiments the MSEs are edge switching elements because these MSEs are considered to be at the edge of the physical network . Being at the edge of the network means either 1 the MSEs directly interface with virtual machines to send and receive data to and from the virtual machines or 2 the MSEs connect the physical network to another physical network which may or may not be managed by the network control system. As shown the MSEs directly interface with VMs respectively. The MSE interfaces the external network and functions as an integration element to facilitate data exchange between the network elements of the physical network and the external network. The non edge MSEs not shown may facilitate data exchange between the MSEs and or other unmanaged switching elements not shown of the physical network .

The middlebox in the logical network is implemented in the physical network in a distributed manner too. In some embodiments a distributed middlebox instance is running in the same host in which a MSE is running in order to provide the middlebox service to the packets forwarded by the MSE. For instance the distributed middlebox instance running in the host provides the middlebox service to the packets forwarded by the MSE . That is the distributed middlebox instance receives data packets from the MSE and performs middlebox operations e.g. source NAT to the packets. The distributed middlebox instance then returns the packets back to the MSE so that the packets are forwarded to the destinations of the packets. Likewise the distributed middlebox instances and running in the hosts and respectively next to the MSEs and respectively provide the middlebox service to the packets coming to and from VMs and respectively. The distributed middlebox instance running in the host next to the MSE provides the middlebox service for the packets forwarded by the MSE .

An example operation of the physical network that implements the logical network is now described by reference to . Specifically illustrates a processing pipeline that is performed by the MSEs and and the distributed middlebox instance in order to send a data packet from VM to VM via the distributed middlebox instance . shows only VM and VM the logical switching elements and hosts that are connected to or include VM and VM to illustrate data being sent from VM to VM . The middlebox service that the middlebox provides is SNAT in this example.

When VM that is coupled to the logical switch sends a packet not shown addressed to VM that is coupled to the logical switch the packet is first sent to the MSE . The MSE then performs processing . The L processing is a set of operations that define the logical switch s forwarding processing on the packet. By performing the L processing the MSE forwards the packet from VM to the logical router . The packet is forwarded to the logical router because VM is not coupled to the logical switch and thus has to be routed by the logical router to the logical switch to which VM is coupled.

The MSE then performs the L processing . The L processing is a set of operations that define the logical router s routing of the packet. The logical router routes the packet to the middlebox to have the middlebox change the packet source address e.g. source IP address to another address. By performing the L processing the MSE sends the packet to the distributed middlebox instance .

The distributed middlebox instance which implements the middlebox then performs SNAT processing on the packet. In some embodiments the distributed middlebox instance changes the received packet s source IP address i.e. VM s IP address to a different address. In other embodiments the distributed middlebox instance creates flow entries and installs in the forwarding table not shown of the MSE so that when the distributed middlebox instance sends a packet back to the MSE this packet s source IP address is changed by the MSE based on those flow entries installed by the distributed middlebox instance . Creating and installing flow entries will be described further below by reference to .

The MSE then receives the packet sent from the distributed middlebox instance and performs L processing and L processing on this packet. This packet has the source IP address that is assigned by the distributed middlebox instance . The L processing is a set of operations that define the logical router s routing of the packet. By performing the L processing the MSE routes the packet from the middlebox to the logical switch .

The MSE then performs L processing . The L processing is a set of operations that define the logical switch l s forwarding processing on the packet. By performing the L processing the MSE forwards the packet from logical router to VM . However because VM is not physically coupled to the MSE the MSE has to identify a MSE to which VM is coupled. The MSE identifies the MSE e.g. through address learning process and sends the packet to the MSE over the tunnel established between the MSEs and .

In some embodiments the MSE performs L processing which defines a portion of the set of operations that define the logical switch s forwarding processing on the packet. For instance the MSE performs an egress access control list ACL processing on the packet before forwarding the packet to VM . In other embodiments the MSE does not perform the L processing nor the L processing . That is the MSE will perform all L processing for the logical switch . When VM sends a packet to VM in response to receiving a packet from VM the

MSE the distributed middlebox instance and the MSE perform the processing pipeline in the reverse order. Because most or all of the logical processing was performed by the MSE for the packet that went to VM from VM most or all of logical processing for the response packet from VM to VM is also performed in the MSE . By having the MSE perform most or all of logical processing on the packets going both ways between VM and VM some embodiments avoid sharing state information e.g. original and translated source IP addresses mapping between the MSEs and . More detailed example operations of the MSEs and will be described further below by reference to .

As described above the MSEs of some embodiments implement logical switches and logical routers based on flow entries supplied to the MSEs by the network control system. The network control system of some embodiments is a distributed control system that includes several controller instances that allow the system to accept logical datapath sets from users and to configure the MSEs to implement these logical datapath sets i.e. datapath sets defining the logical switching elements of the users . The distributed control system also receives middlebox configuration data from the users and configures the distributed middlebox instances by sending the configuration data to the distributed middlebox instances. These controller instances of the distributed control system form a cluster and thus the network control system is referred to as a controller cluster.

In some embodiments the logical controller is a device e.g. a general purpose computer that executes one or more modules that transform the user input from a LCP to a LFP and then transform the LFP data to universal physical control plane data. These modules in some embodiments include a control module and a virtualization module not shown . A control module allows a user to specify and populate a logical datapath set while a virtualization module implements the specified logical datapath set by mapping the logical datapath set onto the physical switching infrastructure.

As shown on the left side of the logical controller the logical controller of some embodiments receives logical datapath set data from a user in a form of application protocol interface API calls that are supported by the logical controller . The API not shown of the logical controller translates the logical datapath set data for configuring logical switches and logical routers into LCP data. The LCP data is the control plane data for the logical switching elements e.g. logical switches and logical routers that the user is managing through the controller cluster. The logical controller generates LFP data from the LCP data. The LFP data is the forwarding plane data for the logical switching elements of the user. In some embodiments the logical controller has a set of modules not shown including a translation engine that translates the LCP data into the LFP data. In some such embodiments the translation performed by the translation engine involves database table mapping.

From the LFP data for a particular logical datapath set of the user the virtualization module of the logical controller of some embodiments generates universal physical control plane UPCP data that is the control plane data for any MSE that implements the logical datapath set. The UPCP data does not include specifics of the MSEs e.g. information that is local to the MSE such as a port number etc. . In some embodiments the translation engine translates the LFP data into UPCP data.

The set of modules of the logical controller also includes a module that identifies a set of physical controllers that is responsible for controlling a set of MSEs that implement the logical datapath set i.e. that implement the logical switching elements of the user . The logical controller sends the UPCP data only to the identified set of physical controllers in some embodiments. The logical controller of different embodiments communicates with the physical controllers differently. For instance in some embodiments the logical controller establishes a communication channel e.g. a remote procedure call RPC channel with each of the physical controllers in the identified set. Alternatively or conjunctively the logical controller and the physical controller use a storage as a medium of communication by placing and pulling UPCP data in the storage.

The physical controller is one of the physical controllers of the controller cluster . The physical controller is responsible for managing the MSE . The physical controller receives the UPCP data from the logical controller and converts the UPCP data into customized physical control plane CPCP data for the MSE . In contrast to the UPCP data the CPCP data for a MSE includes the specifics of the MSE. The CPCP data is the control plane data for the MSE. In some embodiments the physical controller has a set of modules not shown including a translation engine that translates the UPCP data into the CPCP data. In some such embodiment the translation performed by the translation engine involves database table mapping.

The CPCP data includes the attachment data which defines the coupling of the managed switching element and the distributed middlebox instance that implement the logical switching elements the logical switches and the logical routers of the user. For instance the attachment data specifies the port number of a port of the MSE through which the MSE and the distributed middlebox instance exchange packets.

The physical controller also sends slicing data to the MSE. Slicing data in some embodiments includes identifiers for identifying different slices of a distributed middlebox instance. In some embodiments a distributed middlebox instance may provide a middlebox service to several different VMs that belong to several different users i.e. several different logical domains . The distributed middlebox may be sliced so that each slice of the distributed middlebox instance provides the middlebox service one of these different VMs. When the managed switching element that forwards packets for the VMs sends packets to the distributed middlebox instance the MSE uses the slice identifiers to indicate to which particular user or logical domain that a packet belongs so that the slice for the particular user processes the packet.

In some embodiments the slicing data includes a binding between a long form slice identifier and a short form slice identifier. The long form slice identifier is relatively long e.g. 128 bit and the short form slice identifier is relatively short e.g. 16 bit . In some embodiments the long term slice identifier is used to make an identity of a user unique across the numerous MSEs that might be implementing numerous users logical domains. The short form slice identifier is used for packet exchange between a MSE and a distributed middlebox instance running in a host.

The user also configures the middlebox service for the user s logical switching elements. As shown on the right side of the controller cluster the logical controller of some embodiments includes a middlebox API for taking API calls specifying the configuration of the middlebox service e.g. SNAT rules from the user. The middlebox API of the logical controller extracts the configuration data from the middlebox API calls received from the user and sends the configuration data to the same set of physical controllers to which the logical controller sends the UPCP data.

The physical controller of some embodiments receives the configuration data from the logical controller and then relays the configuration data to all MSEs which the physical controller manages that implement at least part of the user s logical switching elements including the MSE . The MSE then sends this configuration data to the distributed middlebox instance . Alternatively or conjunctively the physical controller directly sends the middlebox configuration data to the distributed middlebox instance .

In some embodiments the physical controller also sends the slicing data and the attachment data to the distributed middlebox instances that the physical controller manages. The distributed middlebox instance performs translation of the configuration data using the slicing and attachment data to complete the configuration of the distributed middlebox instance as specified by the user. The distributed middlebox instance also creates a binding of slicing data. Specifically the distributed middlebox instance of some embodiments creates a binding between short form slice identifiers and internal slice identifiers to use only within the distributed middlebox instance . An example usage of the internal slice identifiers may be for populating a data structure that allows only certain lengths for the slice identifiers to have.

Each of the controllers illustrated in is shown as a single controller. However each of these controllers may actually be a controller cluster that operates in a distributed fashion to perform the processing of a logical controller or physical controller.

In some embodiments the input tables include tables with different types of data depending on the role of the controller in the network control system. For instance when the controller functions as a logical controller for a user s logical forwarding elements the input tables include LCP data and LFP data for the logical forwarding elements. When the controller functions as a physical controller the input tables include LFP data. The input tables also include middlebox configuration data received from the user or another controller. The middlebox configuration data is associated with a logical datapath set parameter that identifies the logical switching elements to which the middlebox to be is integrated.

In addition to the input tables the control application includes other miscellaneous tables not shown that the rules engine uses to gather inputs for its table mapping operations. These miscellaneous tables tables include constant tables that store defined values for constants that the rules engine needs to perform its table mapping operations e.g. the value a dispatch port number for resubmits etc. . The miscellaneous tables further include function tables that store functions that the rules engine uses to calculate values to populate the output tables .

The rules engine performs table mapping operations that specifies one manner for converting input data to output data. Whenever one of the input tables is modified referred to as an input table event the rules engine performs a set of table mapping operations that may result in the modification of one or more data tuples in one or more output tables.

In some embodiments the rules engine includes an event processor not shown several query plans not shown and a table processor not shown . Each query plan is a set of rules that specifies a set of join operations that are to be performed upon the occurrence of an input table event. The event processor of the rules engine detects the occurrence of each such event. In some embodiments the event processor registers for callbacks with the input tables for notification of changes to the records in the input tables and detects an input table event by receiving a notification from an input table when one of its records has changed.

In response to a detected input table event the event processor 1 selects an appropriate query plan for the detected table event and 2 directs the table processor to execute the query plan. To execute the query plan the table processor in some embodiments performs the join operations specified by the query plan to produce one or more records that represent one or more sets of data values from one or more input and miscellaneous tables. The table processor of some embodiments then 1 performs a select operation to select a subset of the data values from the record s produced by the join operations and 2 writes the selected subset of data values in one or more output tables .

Some embodiments use a variation of the datalog database language to allow application developers to create the rules engine for the controller and thereby to specify the manner by which the controller maps logical datapath sets to the controlled physical switching infrastructure. This variation of the datalog database language is referred to herein as nLog. Like datalog nLog provides a few declaratory rules and operators that allow a developer to specify different operations that are to be performed upon the occurrence of different events. In some embodiments nLog provides a limited subset of the operators that are provided by datalog in order to increase the operational speed of nLog. For instance in some embodiments nLog only allows the AND operator to be used in any of the declaratory rules.

The declaratory rules and operations that are specified through nLog are then compiled into a much larger set of rules by an nLog compiler. In some embodiments this compiler translates each rule that is meant to address an event into several sets of database join operations. Collectively the larger set of rules forms the table mapping rules engine that is referred to as the nLog engine.

Some embodiments designate the first join operation that is performed by the rules engine for an input event to be based on the logical datapath set parameter. This designation ensures that the rules engine s join operations fail and terminate immediately when the rules engine has started a set of join operations that relate to a logical datapath set i.e. to a logical network that is not managed by the controller.

Like the input tables the output tables include tables with different types of data depending on the role of the controller . When the controller functions as a logical controller the output tables include LFP data and UPCP data for the logical switching elements. When the controller functions as a physical controller the output tables include CPCP data. Like the input tables the output tables may also include the middlebox configuration data. Furthermore the output tables may include a slice identifier when the controller functions as a physical controller.

In some embodiments the output tables can be grouped into several different categories. For instance in some embodiments the output tables can be rules engine RE input tables and or RE output tables. An output table is a RE input table when a change in the output table causes the rules engine to detect an input event that requires the execution of a query plan. An output table can also be an RE input table that generates an event that causes the rules engine to perform another query plan. An output table is a RE output table when a change in the output table causes the exporter to export the change to another controller or a MSE. An output table can be an RE input table a RE output table or both an RE input table and a RE output table.

The exporter detects changes to the RE output tables of the output tables . In some embodiments the exporter registers for callbacks with the RE output tables for notification of changes to the records of the RE output tables. In such embodiments the exporter detects an output table event when it receives notification from a RE output table that one of its records has changed.

In response to a detected output table event the exporter takes each modified data tuple in the modified RE output tables and propagates this modified data tuple to one or more other controllers or to one or more MSEs. When sending the output table records to another controller the exporter in some embodiments uses a single channel of communication e.g. a RPC channel to send the data contained in the records. When sending the RE output table records to MSEs the exporter in some embodiments uses two channels. One channel is established using a switch control protocol e.g. OpenFlow for writing flow entries in the control plane of the MSE. The other channel is established using a database communication protocol e.g. JSON to send configuration data e.g. port configuration tunnel information .

In some embodiments the controller does not keep in the output tables the data for logical datapath sets that the controller is not responsible for managing i.e. for logical networks managed by other logical controllers . However such data is translated by the translator into a format that can be stored in the PTD and is then stored in the PTD. The PTD propagates this data to PTDs of one or more other controllers so that those other controllers that are responsible for managing the logical datapath sets can process the data.

In some embodiments the controller also brings the data stored in the output tables to the PTD for resiliency of the data. Therefore in these embodiments a PTD of a controller has all the configuration data for all logical datapath sets managed by the network control system. That is each PTD contains the global view of the configuration of the logical networks of all users.

The importer interfaces with a number of different sources of input data and uses the input data to modify or create the input tables . The importer of some embodiments receives the input data from another controller. The importer also interfaces with the PTD so that data received through the PTD from other controller instances can be translated and used as input data to modify or create the input tables . Moreover the importer also detects changes with the RE input tables in the output tables .

As described above by reference to the first hop MSEs performs all or most of the logical processing that is to be performed on a data packet in order for the data packet to reach the data packet s destination. The packets from different VMs may be sent to the same VM. These packets are processed by logical switching elements and middleboxes implemented in multiple first hop MSEs and distributed middlebox instances. The multiple first hop MSEs and distributed middlebox instances may apply the same processing to these packets heading to the same destination. Thus from the viewpoint of the destination MSE the packets may not be distinguishable from one another.

For instance a packet sent from VM to VM of has a source IP address of VM and the destination IP address of VM . When the distributed middlebox instance applies a middlebox processing e.g. SNAT on this packet the packet will have a source IP address assigned by the distributed middlebox instance and the destination IP address of VM . Likewise a packet sent from VM to VM of initially has a source IP address of VM and the destination IP address of VM . When the distributed middlebox instance applies the same middlebox processing on this packet this packet will have source IP address assigned by the distributed middlebox instance and the destination IP address of VM . However these two packets may have the same source IP address after being processed by the respective distributed middlebox instance because the middlebox processing performed on these two packets by the distributed middlebox instances are the same. Hence from the viewpoint of the destination MSE attached to the destination of the packets i.e. the MSE for VM of these two packets from two different VMs have same field values. For instance these two packets may have the same five tuple e.g. source IP address source port number destination IP address destination port number and protocol type .

In order for the destination MSE to forward response packets from the destination to the appropriate origins of the packets with the identical five tuples the destination MSE needs additional information to distinguish between those packets. In some embodiments the MSEs assign and use connection identifiers to distinguish between those packets with the identical five tuples from multiple different first hop MSEs.

The first approach shows slicing the identifier space in advance. That is the controller cluster assigns a non overlapping range of connection identifiers to each distributed middlebox instance as the controller cluster configures the distributed middlebox instance. Each middlebox instance will have a pre assigned range of identifiers and will assign an identifier from the range to a connection for which the corresponding MSE is a first hop MSE. This approach can be taken when the number of connection identifiers is sufficient for the number of the distributed middlebox instances for which the identifiers should be sliced.

The second approach shows slicing the identifier space on demand. In this approach a distributed middlebox instance asks for a range of connection identifiers from the controller cluster whenever the distributed middlebox instance needs more connection identifiers. The distributed middlebox instance can release a range of connection identifiers when the distributed middlebox instance does not need the range of connection identifiers. The controller cluster maintains the identifier space to keep track of the ranges of identifiers that are being used by the distributed middlebox instances and the identifiers that are available to be assigned to the distributed middlebox instances. Specifically the controller cluster of some embodiments taking this approach supports a connection identifier assignment API that enables the distributed middlebox instances to obtain and release a range of connection identifiers on demand. An example API call for obtaining a range of connection identifiers is 

The key specifies the distributed middlebox instance that is asking for a range of identifiers. The number of identifiers is the number of identifiers that the distributed middlebox instance asking for. Purpose indicates whether this range of identifiers is going to be used for sanitizing the packets. Sanitizing packets will be described further below by reference to . The controller cluster returns 1 a range of connection identifiers which includes the requested number of connection identifiers and 2 a range identifier for identifying the range.

The range id is the range identifier for the range of connection identifiers to release. In response to receiving this API call the controller cluster makes this range of connection identifiers available for assigning to the distributed middlebox instances.

The third approach shows assigning the entire range of connection identifiers to each of the distributed middlebox instances. This approach can be taken when the identifier assignment to a connection happens at the destination MSE for the connection rather than at the first hop MSE for the connection. Because the identifier assignment to the connection happens at the destination MSE the identifier assignment is used only by the destination MSE and the corresponding middlebox instance. Therefore there is no need to uniquely identify a connection across different MSEs.

As mentioned above one of the middlebox services that a middlebox can provide is a SNAT service. When a middlebox is providing the SNAT service the middlebox replaces the source network address e.g. the source IP address with a different source network address in order to hide the real source network address from the recipient of the packet. illustrate example operations of the MSEs and the corresponding distributed middlebox instances . The distributed middlebox instances provides SNAT service unless otherwise specified below.

As shown the logical switch has three ports ports . Port is associated with VM s L address e.g. a MAC address . Port is associated with VM s L address. Port is associated with the MAC address of port X of the logical router . The logical switch has two ports ports . Port is associated with the MAC address of port Y of the logical router . In this example the MAC address of port X is 01 01 01 01 01 01 and the MAC address of port Y is 01 01 01 01 01 02.

The logical router has ports X Y and N. Port X is coupled to port of the logical switch . In this example the logical switch forwards packets between VMs that have IP addresses that belong to a subnet IP address of 10.0.1.0 24. Port X is therefore associated with a subnet IP address of 10.0.1.0 24. Port Y is coupled to port of the logical switch . In this example the logical switch forwards packets between VMs that have IP addresses that belong to a subnet IP address of 10.0.2.0 24. Port Y is therefore associated with a subnet IP address of 10.0.2.0 24. Port N is for sending packets to the middlebox and is not associated with any IP subnet in this example. In some embodiments a port of the MSE that the MSE uses to communicate with the distributed middlebox instance e.g. port N is a port that does not have a physical port e.g. VIF to which the port is mapped. Also VM has an IP address of 10.0.1.1 and VM has an IP address of 10.0.1.2. VM has an IP address of 10.0.2.1 in this example. The middlebox in this example has a set of IP addresses 11.0.1.1 11.0.1.100 to use to translate source IP addresses of packets that originate from the logical switch e.g. packets having the source IP addresses that belong to the subnet IP address of 10.0.1.0 24 .

Shown in the bottom half of are hosts on which the MSEs and the distributed middlebox instances respectively run. The MSE has ports A C. The MSE has ports G I. The MSE has ports D F. In this example the tunnel that is established between the MSEs and terminates at ports B and G. The tunnel that is established between the MSEs and terminates at ports A and D. The tunnel that is established between the MSEs and terminates at ports H and E. Port C of the MSE is mapped to port of the logical switch and therefore port C is associated with the MAC address of VM . Port of the MSE is mapped to port of the logical switch and therefore port is associated with the MAC address of VM . Port F of the MSE is mapped to port of the logical switch and therefore port F is associated with the MAC address of VM .

The process begins by receiving at a packet and several flow templates from a MSE that is a first hop MSE with respect to this packet. That is the MSE sending the packet has received the packet from a source VM with which the MSE directly interfaces. This packet s destination IP address is the IP address of a destination VM which is not coupled to the logical switch to which the source VM is coupled. The packet has the IP address of the source VM as the source IP address.

Next the process identifies at the source IP address of the received packet so that the process can translate this address into another IP address. The process then determines at whether there is an available IP address to which to translate the source IP address. In some embodiments the process maintains a set of IP addresses. When all IP addresses in the maintained set are used the process determines that no address is available. When there is an IP address in the maintained set of addresses that the process can use the process determines that an address to which to translate the source IP address of the received packet is available.

When the process determines at that there is no available address to which to translate the source IP address of the packet the process creates at and installs a failure flow entry. In some embodiments the process creates the failure flow entry by filling in a received at flow template with an instruction to drop the packet. The MSE will drop the packet according to the failure flow entry. The process then proceeds to which will be described further below.

When the process determines at that there is an available address to which to translate the source IP address of the packet the process maps at the source IP address of the packet to the address to which to translate the source IP address and stores the mapping.

Next at the process creates and installs forward and reverse flow entries. A forward flow entry is a flow entry that directs the first hop MSE to modify the packet by replacing the source IP address with the IP address to which the source IP address is mapped at . In some embodiments the process creates the forward flow entry by filling in a received at flow template with the address to which the source IP address is mapped at . A reverse flow entry is a flow entry that directs the first hop MSE to modify a response packet that is sent from the destination of the initial packet i.e. the packet that is sent to the destination in response to receiving the initial packet. The response packet will have a destination IP address which is the IP address to which the source IP address of the initial packet is translated. The first hop MSE translates the destination IP address of the response packet so that the response packet can reach the source VM of the initial packet.

Next the process then sends at the packet back to the first hop MSE. The process then ends. The first hop MSE will process the packet based on the flow entries which will include the forward and reverse flow entries and or the failure flow entry.

The bottom half of the figure illustrates the MSEs and and VM . As shown the MSE includes a table for storing flow entries for the logical switch not shown a table for storing flow entries for the logical router and a table for storing flow entries for the logical switch . Although these tables are depicted as separate tables the tables do not necessarily have to be separate tables. That is a single table may include all the flow entries for the MSE to use to perform the logical processing of the logical router and the logical switches and .

When VM that is coupled to the logical switch sends packet to VM that is coupled to the logical switch the packet is first sent to the MSE through port of the MSE . The MSE performs an L processing on packet based on the forwarding tables of the MSE . In this example packet has a destination IP address of 10.0.2.1 which is the IP address of VM as described above by reference to . Packet s source IP address is 10.0.1.1. Packet also has VM s MAC address as a source MAC address and the MAC address of port X 01 01 01 01 01 01 of the logical router as a destination MAC address.

The MSE identifies a flow entry indicated by an encircled referred to as record in the forwarding table that implements the context mapping of the stage . The record identifies packet s logical context based on the ingress port which is port through which packet is received from VM . In addition the record specifies that the MSE stores the logical context of packet in a set of fields e.g. a VLAN id field of packet s header. In some embodiments a logical context represents the state of the packet with respect to the logical switching element. For example some embodiments of the logical context may specify the logical switching element to which the packet belongs the port of the logical switching element through which the packet was received the port of the logical switching element through which the packet is to be transmitted the stage of the LFP of the logical switching element the packet is at etc.

The record also specifies packet is to be further processed by the forwarding tables e.g. by sending packet to a dispatch port . A dispatch port not shown is a port of a MSE to send the processed packet back to the ingress port of the MSE so that the MSE can further process the packet.

Based on the logical context and or other fields stored in packet s header the MSE identifies a flow entry indicated by an encircled referred to as record in the forwarding tables that implements the ingress ACL of the stage . In this example the record allows packet to be further processed and thus specifies packet is to be further processed by the MSE . In addition the record specifies that the MSE stores the logical context i.e. packet has been processed by the stage of packet in the set of fields of packet l s header.

Next the MSE identifies based on the logical context and or other fields stored in packet s header a flow entry indicated by an encircled referred to as record in the forwarding tables that implements the logical L forwarding of the stage . The record specifies that a packet with the MAC address of port X of the logical router as a destination MAC address is to be sent to port of the logical switch .

The record also specifies that packet is to be further processed by the MSE . Also the record specifies that the MSE stores the logical context i.e. packet has been processed by the third stage in the set of fields of packet l s header.

Next the MSE identifies based on the logical context and or other fields stored in packet s header a flow entry indicated by an encircled referred to as record in the forwarding tables that implements the egress ACL of the stage . In this example the record allows packet to be further processed e.g. packet can get out of the logical switch through port of the logical switch and thus specifies packet is to be further processed by the MSE e.g. by sending packet to the dispatch port . In addition the record specifies that the MSE stores the logical context i.e. packet has been processed by the stage of the processing pipeline of packet in the set of fields of packet s header. It is to be noted that all records specify that a MSE updates the logical context store in the set of fields whenever the MSE performs some portion of logical processing based on a record. 

The MSE continues processing packet based on the flow entries. The MSE identifies based on the logical context and or other fields stored in packet s header a flow entry indicated by an encircled referred to as record in the L entries that implements L ingress ACL of the logical router by specifying that the MSE should accept the packet through port X of the logical router based on the information in the header of packet . The record also specifies that packet is to be further processed by the MSE e.g. by sending packet to a dispatch port . In addition the record specifies that the MSE stores the logical context i.e. packet has been processed by the stage of the processing pipeline of packet in the set of fields of packet l s header.

The MSE then identifies a flow entry indicated by an encircled referred to as record in the L entries table implements the L routing by specifying that a packet received through port X of the logical router is to be sent to the middlebox through port N. That is the record specifies that a packet having a source IP address that belongs to the subnet IP address of 10.0.1.0 24 is to be sent to the middlebox . Because packet has the source IP address 10.0.1.1 that belongs to the subnet IP address of 10.0.1.0 24 the MSE will send the packet to the distributed middlebox instance .

The MSE then identifies a flow entry indicated by an encircled referred to as record in the L entries that implements L egress ACL by specifying that the MSE allows the packet to exit out through port N of the logical router based on the information e.g. source IP address in the header of packet . In addition the record specifies that the MSE removes the logical context of packet from the set of fields of packet s header. The MSE sends packet to the distributed middlebox instance which implements the middlebox . The record also specifies that several flow templates are to be sent to the middlebox along with packet . The managed switching element of some embodiments also sends a slice identifier to the distributed middlebox instance so that the slice of the distributed middlebox instance for the user of the logical switching elements in the logical network processes packet .

Upon receiving packet the distributed middlebox instance identifies an IP address to which to translate the source IP address 10.0.1.1 of packet . In this example the distributed middlebox instance selects 11.0.1.1 from the range of IP addresses 11.0.1.1 11.0.1.100 described above by reference to . The distributed middlebox instance also creates a forward flow entry that specifies that the MSE modifies a packet that has a source IP address of 10.0.1.1 by replacing the source IP address 10.0.1.1 with the selected IP address 11.0.1.1 . The distributed middlebox instance also creates a reverse flow entry that specifies that the MSE modifies a packet with a destination IP address of 11.0.1.1 by replacing the destination IP address of this packet with an IP address of the VM . The reverse flow entry ensures that a response packet from VM reaches the correct destination VM . The distributed middlebox instance installs the created flow entries and sends packet back to the MSE . In some embodiments the MSE treats the packet returned from the distributed middlebox instance as a new packet to route. Thus this new packet is referred to as packet in this example. As shown the forward and reverse flow entries are installed e.g. placed in the table indicated by encircled F and R respectively.

Upon receiving packet the MSE performs the L processing on packet based on the table . In this example because packet is still same as packet packet has a destination IP address of 10.0.2.1 which is the IP address of VM . Packet s source IP address is still 10.0.1.1. The MSE identifies a flow entry indicated by an encircled referred to as record in the forwarding table that implements the context mapping of the stage . The record identifies packet s logical context based on the ingress port which is port N through which packet is received from the middlebox . In addition the record specifies that the MSE stores the logical context of packet in a set of fields e.g. a VLAN id field of packet s header. The record also specifies packet is to be further processed by the MSE e.g. by sending packet to a dispatch port .

The MSE continues processing packet based on the flow entries. The MSE identifies based on the logical context and or other fields stored in packet s header a flow entry indicated by an encircled referred to as record in the L entries that implements L ingress ACL of the logical router by specifying that the MSE should accept the packet through port N of the logical router based on the information in the header of packet . The record also specifies that packet is to be further processed by the MSE . In addition the record specifies that the MSE stores the logical context i.e. packet has been processed by the stage of the processing pipeline of packet in a set of fields of packet s header.

The MSE then identifies a flow entry indicated by an encircled referred to as record in the L entries that implements L routing by specifying that packet with its destination IP address 10.0.2.1 should exit out of port Y of the logical router . The record also specifies that packet is to be further processed by the MSE . In addition the record specifies that the MSE stores the logical context i.e. packet has been processed by the stage of the processing pipeline of packet in a set of fields of packet s header.

In some embodiments the flow entries have associated priority levels. The priority levels are used to select one of several flow entries when a packet satisfies the conditions specified by the qualifiers of the several flow entries. The MSE identifies a flow entry indicated by an encircled F referred to as record F in the L entries table . The record F is the forward flow entry that the distributed middlebox instance has created and installed in the table . Packet meets the condition specified in the record F as well as the condition specified in the record because packet s source IP address is 10.0.1.1 that is specified as a condition in the record F and packet s source IP address belongs to the subnet IP address of 10.0.1.0 24 specified as a condition in the record . In some embodiments the record F that is created by the distributed middlebox instance has a priority level that is higher than that of the record which directs the MSE to send the packet to the distributed middlebox instance . In addition the record F specifies that the MSE stores the logical context i.e. packet has been processed by the stage of the processing pipeline of packet in the set of fields of packet s header. It is to be noted that the record F may be identified ahead of the record so that the MSE replaces the source IP address of the packet before routing the packet according to the record .

The MSE then identifies a flow entry indicated by an encircled referred to as record in the L entries that implements L egress ACL by specifying that the MSE allows the packet to exit out through port Y of the logical router based on the information e.g. source IP address in the header of packet . Also the record or another record in the routing table not shown indicates that the source MAC address for packet is to be rewritten to the MAC address of port Y of the logical router 01 01 01 01 01 02 . Record may also specify that the MSE resolves the destination IP address of packet in order to obtain the MAC address of VM . In some embodiments the MSE uses address resolution protocol ARP to resolve the destination IP address into the MAC address of the destination. Record or another record may specify that the MSE replaces the destination MAC address of the packet currently the MAC address of port of the MSE with the MAC address of VM to which the destination IP address has been resolved. In addition the record specifies that the MSE stores the logical context i.e. packet has been processed by the stage of the processing pipeline of packet in the set of fields of packet s header.

Packet has exited the logical router through port Y and has entered the logical switch through port of the logical switch . The MSE then performs L processing . Based on the logical context and or other fields stored in packet s header the MSE identifies a flow entry indicated by an encircled referred to as record in the L entries that implements the ingress ACL of the stage . In this example the record specifies that packet is to be further processed by the MSE . In addition the record specifies that the MSE stores the logical context i.e. packet has been processed by the stage of the processing pipeline of packet in the set of fields of packet s header.

Next the MSE identifies based on the logical context and or other fields stored in packet s header a flow entry indicated by an encircled referred to as record in the L entries that implements the logical L forwarding of the stage . The record specifies that a packet with the MAC address of VM as the destination MAC address should be forwarded through port of the logical switch that is connected to VM . The record also specifies that packet is to be further processed by the MSE . Also the record specifies that the MSE stores the logical context i.e. packet has been processed by the stage of the processing pipeline in the set of fields of packet s header.

Based on the logical context and or other fields stored in packet s header the MSE identifies a flow entry indicated by an encircled referred to as record in the L entries that implements the context mapping of the stage . In this example the record identifies the MSE as the MSE to which the packet exiting port of the logical switch should be sent. The record additionally specifies that packet be further processed by the MSE . In addition the record specifies that the MSE stores the logical context i.e. packet has been processed by the stage of the processing pipeline of packet in the set of fields of packet s header.

Based on the logical context and or other fields stored in packet s header the MSE then identifies a flow entry indicated by an encircled referred to as record in the L entries that implements the physical mapping of the stage . The record specifies port A of the MSE as a port through which packet is to be sent in order for packet to reach the MSE . In this case the MSE is to send packet out of port A of MSE that is coupled to the MSE through a tunnel. In addition the record specifies that the MSE stores the logical context i.e. packet has been processed by the stage of the processing pipeline of packet in the set of fields of packet s header.

As shown in the difference between the processing of the very first packet packet and the processing of a subsequent packet packet by the MSE is that the MSE does not send the subsequent packet to the distributed middlebox instance . This is because after the stage is performed according to the record the MSE goes with the record F rather than the record which would have directed the MSE to send the subsequent packet to the distributed middlebox instance. As described above by reference to the record F i.e. the forward flow entry created and installed by the distributed middlebox instance has a higher priority level than the record has. This shows that only the first packet for establishing a connection between the source and the destination needs to be sent to the distributed middlebox instance and thus makes it faster to process the subsequent packets being sent from the source to the destination.

As mentioned above in some embodiments a particular MSE that is a first hop with respect to a particular packet performs all or most of the logical processing that is to be performed on the particular packet in order for the particular packet to reach the packet s destination. In some such embodiments the particular MSE also performs all or most of the logical processing that is to be performed on a response packet that is sent from the destination of the particular packet in response to receiving the particular packet. By having the particular MSE perform the logical processing on the response packet some embodiments avoid having to share state information e.g. mapping of the source IP address of the particular packet and the translated source IP address between MSEs. That is had the first hop MSE to the response packet performed the logical operation on the response packet that MSE would need the state information in order to restore the original source IP address and send the response packet back to the origin of the particular packet.

The top half of illustrates a processing pipeline that is performed by the MSE . The processing pipeline includes L processing for the logical switch L processing for the logical router and L processing for the logical switch which have stages stages and stages respectively. The bottom half of the figure illustrates the MSEs and and VM . As shown the MSE includes the tables and for storing flow entries for the logical switch the logical router and the logical switch respectively.

When the MSE receives from the MSE packet that is originated from VM through port A of the MSE the MSE performs the L processing to forward packet from VM to the logical router . The MSE performs the L processing based on the flow entries indicated by encircled . Packet has VM s IP address 10.0.2.1 as the source IP address and has the destination IP address of 11.0.1.1 because packet is a response packet to a packet that has the source IP address of 11.0.1.1.

The MSE then performs the L processing to route the packet out of the logical router through port X of the logical router . Specifically based on the logical context and or other fields stored in packet s header the MSE identifies a flow entry indicated by an encircled referred to as record in the forwarding tables that implements the L ingress ACL of the stage . In this example the record allows packet to be further processed and thus specifies that packet is to be further processed by the MSE . In addition the record specifies that the MSE stores the logical context i.e. packet has been processed by the stage of packet in the set of fields of packet s header.

The MSE then identifies the reverse flow entry indicated by encircled R referred to as record R . As mentioned above the reverse flow entry specifies that the MSE modifies a packet with a destination IP address of 11.0.1.1 by replacing the destination IP address of this packet with the IP address of the VM 10.0.1.1 .

The MSE then identifies a flow entry indicated by an encircled referred to as record in the L entries that implements L routing by specifying that packet with its destination IP address 10.0.1.1 should exit out of port X of the logical router . Also the record or another record in the routing table not shown indicates that the source MAC address for packet is to be rewritten to the MAC address of port X of the logical router 01 01 01 01 01 01 .

The MSE then identifies a flow entry indicated by an encircled referred to as record in the L entries that implements L egress ACL by specifying that the MSE allows the packet to exit out through port X of the logical router based on the information e.g. source IP address in the header of packet . In addition the record specifies that the MSE stores the logical context i.e. packet has been processed by the stage of the processing pipeline of packet in the set of fields of packet s header.

The MSE then performs the L processing for the logical switch according to the flow entries indicated by encircled . The MSE will send packet out of the logical switch through port of the logical switch. Because port C of the MSE is mapped to port of the logical switch the MSE will physical send out packet to VM through port C of the MSE .

In some embodiments sanitizing packets is done at the last hop MSE when the first hop MSEs sending the packets to the last hop MSEs do not assign unique identifiers to the packets. When the packets from different first hop MSEs come into the same last hop MSE without having been assigned unique identifiers the last hop MSE in some cases would not be able to send response packets to the right first hop MSE because the incoming packets may have the same five tuple e.g. source IP address destination IP address transport protocol type source port number destination port number . Sanitizing packets includes adding a unique identifier to the packets in addition to the 5 tuples or modifying the 5 tuples of the packets e.g. changing the source port number to make the 5 tuples distinguishable.

In some embodiments the process is performed by a distributed middlebox instance that runs in the same host in which a MSE runs. The MSE is the last hop MSE with respect to the packets that the MSE sends to the distributed middlebox instance. The distributed middlebox instance of some embodiments also receives flow templates along with the packets. In these embodiments the distributed middlebox provides the middlebox service by creating flow entries by filling in the flow templates with actual values and installing the created flow entries in the flow tables of the last hop MSE. The distributed middlebox also sends the packets back to the last hop MSE so that the packets are processed by the MSE based on the flow entries installed by the distributed middlebox instance.

The process begins by receiving at a packet and several flow templates from a MSE that is a last hop MSE with respect to this packet. That is the MSE has received the packet from another MSE and not from a VM with which the receiving MSE directly interfaces. The packet has a five tuple in the header of this packet. The process also receives the identification of the other MSE from which the receiving MSE received the packet.

Next the process determines at whether the process has previously received a packet that has the same five tuple from a different MSE. The process in some embodiments maintains a look up table of five tuples and the identifications of the MSE that has sent the packets with the five tuples to the last hop MSE. The process looks up this table to determine whether a packet with the same five tuple as the received packet has been received from a MSE that is different from the MSE that has sent the currently received packet to the last hop MSE.

When the process determines at that the process has not seen a packet with the same five tuple as that of the received packet from a different MSE the process proceeds to to add the five tuple and the MSE identification of the received packet in the look up table. The process then proceeds to which will be described further below.

When the process determines at that the process has seen a packet with the same five tuple as that of the received packet from a different MSE the process proceeds to to create and install a forward sanitization flow entry and a reverse sanitization flow entry in the flow tables of the last hop MSE. A forward sanitization flow entry is a flow entry that directs the last hop MSE to modify the received packet s five tuple to make the packet s five tuple unique e.g. by replacing the source port number with a new source port number . A reverse sanitization flow entry is a flow entry that directs the last hop MSE to modify response packets that are sent from the received packet s destination to the source of the received packet. According to the reverse sanitization flow entry the MSE un does the sanitization performed based on the forward sanitization flow entry. That is for instance the last hop MSE replaces the destination port number i.e. the new source port number of the received packet of the response packets with the original source port number of the received packet. The process records the new source port number so that the process does not reuse the same new source port number to sanitize other packets.

Next the process then sends at the received packet back to the last hop MSE. The process then ends. The last hop MSE will process the packet based on the flow entries which will include the forward and reverse sanitization flow entries.

The top side of the figure shows two processing pipelines and that are performed by the MSE . The processing pipeline includes stages and . The processing pipeline includes stages and . The bottom side of the figure shows the MSEs and and VM . As shown the MSE includes the table for storing flow entries for the logical switch .

When the MSE receives from the MSE packet that is originated from VM through port D of the MSE the MSE performs the processing pipeline to forward packet to the distributed middlebox instance . The MSE performs the processing pipeline based on the flow entries indicated by encircled and . As described above by reference to packet has the source IP address of 11.0.1.1 which was translated from the IP address of VM 10.0.1.1 and has the IP address of VM 10.0.2.1 as the destination IP address. The packet also has the MAC address of port Y of the logical router 01 01 01 01 01 02 as a source MAC address and has the MAC address of VM as the destination MAC address.

Based on the logical context stored in packet header the MSE then identifies a flow entry indicated by an encircled referred to as record in the L entries table that implements the context mapping of the stage . The record identifies the packet s logical context based on the logical context that is stored in packet s header. The logical context specifies that packet has been processed by the stage of the processing pipeline which was performed by the MSE . As such the record specifies that packet is to be further processed by the MSE e.g. by sending the packet to a dispatch port of the MSE .

The MSE then identifies a flow entry indicated by encircled referred to as record in the table that implements the stage . The record specifies that packet is to be sent to the distributed middlebox instance . The record also specifies that several flow templates for generating forward and reverse sanitization flow entries are to be sent to the distributed middlebox instance. The record also specifies that the MSE is to send an identification of the MSE to indicate that packet came from the MSE . The managed switching element of some embodiments also sends a slice identifier to the distributed middlebox instance so that the slice of the distributed middlebox instance for the user of the logical switches and and the logical router processes packet .

Upon receiving packet and the identification of the MSE from the MSE the distributed middlebox instance identifies the five tuple of packet and determines whether the distributed middlebox instance has received a packet that has the same five tuple from another MSE. In this example the MSE had sent a packet from VM to VM before. This packet had the same five tuple as packet s because the distributed middlebox instance running in host for the MSE and the distributed middlebox instance running in host for the MSE are configured to implement the middlebox and thus the distributed middlebox instance translated the source IP address of the packet from VM from the source IP address of VM to 11.0.1.1.

In some embodiments the distributed middlebox instance maintains a look up table of five tuples and the identifications of the MSE that has sent the packets with the five tuples to the last hop MSE. In this example the look up table of the distributed middlebox instance has an entry for the packet from VM and VM . The distributed middlebox instance thus creates a forward sanitization flow entry that specifies that the MSE modifies a packet that has the five tuple of packet e.g. source IP address 11.0.1.1 destination IP address 10.0.2.1 source port number 1234 destination port number transport protocol TCP and the identification of the MSE by replacing the source port number with a new source port number e.g. 12340 . The new source port number serves as a connection identifier because the new source port number makes the connection over which the packet is being sent unique.

The distributed middlebox instance also creates a reverse sanitization flow entry that specifies that the MSE modifies a packet not shown which is sent from VM to VM in response to receiving packet by replacing the destination port number to the port number of VM from which packet came. This reverse sanitization is to restore the correct port number so that the response packet from VM to VM reaches the correct port of VM . In this example the flow entry indicated by encircled RS the record RS specifies that the MSE modifies a packet which has a five tuple of a packet from VM to VM in response to packet e.g. source IP address 10.0.2.1 destination IP address 11.0.1.1 source port number 80 destination port number 12340 transport protocol TCP by replacing the destination port number with the source port number e.g. 1234 of packet before being sanitized.

The distributed middlebox instance installs the created flow entries and sends packet back to the MSE . In some embodiments the MSE treats the packet returned from the distributed middlebox instance as a new packet to route. Thus this new packet is referred to as packet in this example. As shown the forward and reverse sanitization flow entries are installed e.g. placed in the table indicated by encircled FS and RS respectively.

In some embodiments the distributed middlebox instance may keep separate slices for generating forward and reverse flow entries and generating sanitization flow entries. That is the distributed middlebox instance has one slice for the packets for which the distributed middlebox instance provides the SNAT service and has another slice for the packets for which the distributed middlebox instance provides sanitization even though all these packets belong to the same logical domain of a single user.

Upon receiving packet the MSE performs the processing pipeline on packet based on the table . In this example because packet is still the same as packet packet has the same five tuple with the source port number . The MSE identifies a flow entry indicated by an encircled referred to as record in the forwarding table that implements the context mapping of the stage which is described above. The record also specifies packet is to be further processed by the MSE e.g. by sending packet to a dispatch port .

The MSE identifies a flow entry indicated by an encircled FS referred to as record FS in the table . The record FS is the forward sanitization flow entry that the distributed middlebox instance has created and installed in the table . Packet meets the condition specified in the record FS as well as the condition specified in the record because packet s five tuple is specified as a condition in the record FS and in the record . In some embodiments the record FS that is created by the distributed middlebox instance has a higher priority level than that of the record which directs the MSE to send the packet to the distributed middlebox instance . In addition the record FS specifies that the MSE stores the logical context i.e. packet has been processed by the stage of the processing pipeline of packet in the set of fields of packet s header.

The MSE then identifies a flow entry indicated by an encircled referred to as record in the L entries that implements generating a reverse hint flow entry of the stage . In some embodiments the last hop MSE creates and installs a reverse hint. A reverse hint in some embodiments is a flow entry that directs the MSE which is the last hop MSE with respect to a particular packet to send a response packet to the origin of the particular packet without performing logical processing on the response packet. A reverse hint is set up in order to allow the first hop MSE with respect to the particular packet to process all or most of the logical processing in some embodiments. As shown the MSE has installed a reverse hint flow entry indicated by encircled RH referred to as record RH . In this example the record RH specifies that the MSE sends a packet which has a five tuple of a packet from VM to VM in response to receiving the sanitized packet e.g. source IP address 10.0.2.1 destination IP address 11.0.1.1 source port number destination port number transport protocol TCP to the MSE .

Next the MSE identifies based on the logical context and or other fields stored in packet s header a flow entry indicated by an encircled referred to as record in the forwarding tables that implements the egress ACL of the stage . In this example the record allows packet to be further processed e.g. packet can get out of the logical switch through port of the logical switch and thus specifies packet is to be further processed by the MSE . In addition the record specifies that the MSE stores the logical context i.e. packet has been processed by the stage of the processing pipeline of packet in the set of fields of packet s header.

Based on the logical context and or other fields stored in packet s header the MSE then identifies a flow entry indicated by an encircled referred to as record in the table that implements the physical mapping of the stage . The record specifies port F of the MSE as a port through which packet is to be sent in order for packet to reach VM . In addition the record specifies that the MSE removes the logical context of packet from the set of fields of packet s header. The MSE sends packet to VM .

The top half of the figure shows a processing pipeline that the MSE performs on packet . The processing pipeline includes the stages and which are described above. As shown in the difference between the processing of the very first packet packet and the processing a subsequent packet packet by the MSE i.e. the last hop MSE with respect to packets and is that the MSE does not send the subsequent packet to the distributed middlebox instance . This is because after the stage is performed according to the record the MSE goes with the record FS rather than the record which would have directed the MSE to send the subsequent packet to the distributed middlebox instance . As described above the record FS i.e. the forward sanitization flow entry created and installed by the distributed middlebox instance has a higher priority level than the record s priority level. This shows that only the first packet for establishing a connection between the source and the destination needs to be sent to the distributed middlebox instance and thus makes it faster to process the subsequent packets being sent from the source to the destination. Also the MSE regenerates or refreshes the record RH by performing the stage for packet .

The top half of illustrates a processing pipeline that is performed by the MSE . The processing pipeline includes stages . The bottom half of the figure illustrates the MSEs and and VM . As shown the MSE includes the table for storing flow entries.

When the MSE receives packet from VM through port F of the MSE the MSE performs the L processing to forward packet from VM to the logical router . The MSE performs the processing pipeline based on the flow entries indicated by encircled RH RS and . Packet has a destination port number that is the destination port number of the sanitized packet e.g. 12340 .

The MSE identifies a flow entry indicated by an encircled referred to as record in the forwarding table that implements the context mapping of the stage . The record identifies packet s logical context based on the ingress port which is port F of the MSE through which packet is received from VM . In addition the record specifies that the MSE stores the logical context of packet in a set of fields of packet s header. The record also specifies packet is to be further processed by the forwarding tables

The MSE then identifies the reverse hint flow entry the record RH. As mentioned above the record RH specifies that the MSE sends a packet which has a five tuple of a packet from VM to VM in response to receiving the sanitized packet e.g. source IP address 10.0.2.1 destination IP address 11.0.1.1 source port number destination port number 1234 transport protocol TCP to the MSE .

The MSE then identifies the reverse sanitization flow entry the record RS which is created and installed by the distributed middlebox instance . As mentioned above the record RS specifies that the MSE modifies a packet which has a five tuple of a packet from VM to VM in response to packet e.g. source IP address 10.0.2.1 destination IP address 11.0.1.1 source port number 80 destination port number 12340 transport protocol TCP by replacing the destination port number with the source port number e.g. 1234 of packet before being sanitized. The MSE modifies packet accordingly.

Based on the logical context and or other fields stored in packet s header the MSE then identifies a flow entry indicated by an encircled referred to as record in the table that implements the physical mapping of the stage . The record specifies port D of the MSE as a port through which packet is to be sent in order for packet to reach VM . The MSE sends packet to the MSE through port D accordingly.

Many of the above described features and applications are implemented as software processes that are specified as a set of instructions recorded on a computer readable storage medium also referred to as computer readable medium . When these instructions are executed by one or more processing unit s e.g. one or more processors cores of processors or other processing units they cause the processing unit s to perform the actions indicated in the instructions. Examples of computer readable media include but are not limited to CD ROMs flash drives RAM chips hard drives EPROMs etc. The computer readable media does not include carrier waves and electronic signals passing wirelessly or over wired connections.

In this specification the term software is meant to include firmware residing in read only memory or applications stored in magnetic storage which can be read into memory for processing by a processor. Also in some embodiments multiple software inventions can be implemented as sub parts of a larger program while remaining distinct software inventions. In some embodiments multiple software inventions can also be implemented as separate programs. Finally any combination of separate programs that together implement a software invention described here is within the scope of the invention. In some embodiments the software programs when installed to operate on one or more electronic systems define one or more specific machine implementations that execute and perform the operations of the software programs.

The bus collectively represents all system peripheral and chipset buses that communicatively connect the numerous internal devices of the electronic system . For instance the bus communicatively connects the processing unit s with the read only memory the system memory and the permanent storage device .

From these various memory units the processing unit s retrieve instructions to execute and data to process in order to execute the processes of the invention. The processing unit s may be a single processor or a multi core processor in different embodiments.

The read only memory ROM stores static data and instructions that are needed by the processing unit s and other modules of the electronic system. The permanent storage device on the other hand is a read and write memory device. This device is a non volatile memory unit that stores instructions and data even when the electronic system is off. Some embodiments of the invention use a mass storage device such as a magnetic or optical disk and its corresponding disk drive as the permanent storage device .

Other embodiments use a removable storage device such as a floppy disk flash drive or ZIP disk and its corresponding disk drive as the permanent storage device. Like the permanent storage device the system memory is a read and write memory device. However unlike storage device the system memory is a volatile read and write memory such a random access memory. The system memory stores some of the instructions and data that the processor needs at runtime. In some embodiments the invention s processes are stored in the system memory the permanent storage device and or the read only memory . From these various memory units the processing unit s retrieve instructions to execute and data to process in order to execute the processes of some embodiments.

The bus also connects to the input and output devices and . The input devices enable the user to communicate information and select commands to the electronic system. The input devices include alphanumeric keyboards and pointing devices also called cursor control devices . The output devices display images generated by the electronic system. The output devices include printers and display devices such as cathode ray tubes CRT or liquid crystal displays LCD . Some embodiments include devices such as a touchscreen that function as both input and output devices.

Finally as shown in bus also couples electronic system to a network through a network adapter not shown . In this manner the computer can be a part of a network of computers such as a local area network LAN a wide area network WAN or an Intranet or a network of networks such as the Internet. Any or all components of electronic system may be used in conjunction with the invention.

Some embodiments include electronic components such as microprocessors storage and memory that store computer program instructions in a machine readable or computer readable medium alternatively referred to as computer readable storage media machine readable media or machine readable storage media . Some examples of such computer readable media include RAM ROM read only compact discs CD ROM recordable compact discs CD R rewritable compact discs CD RW read only digital versatile discs e.g. DVD ROM dual layer DVD ROM a variety of recordable rewritable DVDs e.g. DVD RAM DVD RW DVD RW etc. flash memory e.g. SD cards mini SD cards micro SD cards etc. magnetic and or solid state hard drives read only and recordable Blu Ray discs ultra density optical discs any other optical or magnetic media and floppy disks. The computer readable media may store a computer program that is executable by at least one processing unit and includes sets of instructions for performing various operations. Examples of computer programs or computer code include machine code such as is produced by a compiler and files including higher level code that are executed by a computer an electronic component or a microprocessor using an interpreter.

While the above discussion primarily refers to microprocessor or multi core processors that execute software some embodiments are performed by one or more integrated circuits such as application specific integrated circuits ASICs or field programmable gate arrays FPGAs . In some embodiments such integrated circuits execute instructions that are stored on the circuit itself.

As used in this specification and any claims of this application the terms computer server processor and memory all refer to electronic or other technological devices. These terms exclude people or groups of people. For the purposes of the specification the terms display or displaying means displaying on an electronic device. As used in this specification and any claims of this application the terms computer readable medium and computer readable media are entirely restricted to tangible physical objects that store information in a form that is readable by a computer. These terms exclude any wireless signals wired download signals and any other ephemeral signals.

While the invention has been described with reference to numerous specific details one of ordinary skill in the art will recognize that the invention can be embodied in other specific forms without departing from the spirit of the invention. In addition a number of the figures including conceptually illustrate processes. The specific operations of these processes may not be performed in the exact order shown and described. The specific operations may not be performed in one continuous series of operations and different specific operations may be performed in different embodiments. Furthermore the process could be implemented using several sub processes or as part of a larger macro process. Thus one of ordinary skill in the art would understand that the invention is not to be limited by the foregoing illustrative details but rather is to be defined by the appended claims.

