---

title: Compute instance migrations across availability zones of a provider network
abstract: A provider network may implement compute instance migrations across availability zones. Compute instances may be located in a particular availability zone of provider network that is implemented across multiple availability zones. A request may be received, from a client of the provider network or other component of the provider network, to migrate a compute instance that is currently operating for a client and located in one availability zone to another availability zone. A destination compute instance may be provisioned in the other availability zone based on a configuration of the currently operating compute instance. In some embodiments, other computing resources utilized by the currently operating compute instance, such as data storage resources, may be moved to the other availability zone. Migration may be completed such that the destination compute instance is currently operating for the client and the compute instance is not.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09350682&OS=09350682&RS=09350682
owner: Amazon Technologies, Inc.
number: 09350682
owner_city: Reno
owner_country: US
publication_date: 20140730
---
The advent of virtualization technologies for commodity hardware has provided benefits with respect to managing large scale computing resources for many customers with diverse needs allowing various computing resources to be efficiently and securely shared by multiple customers. For example virtualization technologies may allow a single physical computing machine to be shared among multiple users by providing each user with one or more virtual machines hosted by the single physical computing machine with each such virtual machine being a software simulation acting as a distinct logical computing system that provides users with the illusion that they are the sole operators and administrators of a given hardware computing resource while also providing application isolation and security among the various virtual machines. As another example virtualization technologies may allow data storage hardware to be shared among multiple users by providing each user with a virtualized data store which may be distributed across multiple data storage devices with each such virtualized data store acting as a distinct logical data store that provides users with the illusion that they are the sole operators and administrators of the data storage resource.

Virtual machines may be provided as part of a provider network that offers use of virtual machines to multiple different clients. The virtual machines may be hosted in different locations of which each location may continue operating independent of faults or failures that may occur in other locations. Clients may wish to locate virtual machines in diverse locations to provide greater fault tolerance for applications implemented by the virtual machines.

While embodiments are described herein by way of example for several embodiments and illustrative drawings those skilled in the art will recognize that the embodiments are not limited to the embodiments or drawings described. It should be understood that the drawings and detailed description thereto are not intended to limit embodiments to the particular form disclosed but on the contrary the intention is to cover all modifications equivalents and alternatives falling within the spirit and scope as defined by the appended claims. The headings used herein are for organizational purposes only and are not meant to be used to limit the scope of the description or the claims. As used throughout this application the word may is used in a permissive sense i.e. meaning having the potential to rather than the mandatory sense i.e. meaning must . Similarly the words include including and includes mean including but not limited to.

The systems and methods described herein may perform compute instance migrations across availability zones for provider networks according to some embodiments. A provider network may supply clients operators or other customers with access to and or control of one or more computing resources. These resources may include various types of computing systems or devices configured for communication over a network. For example in some embodiments a provider network may provide virtual computing resources to clients users or other type of customers in the form of compute instances e.g. a virtual machine acting as a distinct logical computing system that provides users with the illusion that they are the sole operators and administrators of a given hardware computing resource . Clients of the provider network may reserve i.e. purchase or buy one or more compute resources such as compute instances to perform various functions services techniques and or applications.

For durability and availability provider networks may distribute computing systems nodes servers or other devices across different availability zones. An availability zone may be a fault tolerant zone that may operate independently of a failure of another availability zone in the provider network. Availability zones may be geographically diverse in some embodiments. For instance one or more data centers that may implement a particular availability zone may be located in a different geographic location e.g. different city county state country or continent . Independent power sources networking infrastructure or various other configurations of resources in an availability zone may allow for availability zones to be independently accessible to client or management e.g. control plane requests. In at least some embodiments network infrastructure allowing communication between availability zones e.g. limited bandwidth .

For various reasons it may be desirable to locate compute instances and other computing resources for clients in a particular availability zones. If for instance a client wishes to provide greater durability for a service application or other functionality provided by compute instances the compute instances may be distributed across different availability zones insulating the individual instances from failures in the other availability zones. In another example clients may also receive purchase or utilization incentives to operate compute instances and resources in particular availability zones. In some scenarios a client can be notified of or determine that a particular availability zone may be failing encountering networking problems e.g. network partition or experiencing high utilization which may be alleviated if client instances located in the availability zone were migrated to another availability zone. Consequently for compute instances that are already operating in one availability zone it may be advantageous to migrate the operating compute instance to another availability zone.

As illustrated in scene availability zone has a host with an available instance slot . Therefore in scene a new destination compute instance may be provisioned and or configured for migration . The configuration of compute instance may be based on the configuration of instance in order to provide the appearance of moving the instance e.g. software hardware stored data to the new availability zone . In some embodiments other computing resources such as a data storage volume located on storage nodes in availability zone and utilized by instance may be moved to newly provisioned storage nodes in availability zone as well.

As illustrated in scene the instance may then be currently operating for the client while the original instance at host in availability zone may be stopped and or released for hosting another compute instance completing the migration of instance to availability zone . Various different techniques may be employed to complete the migration such as reboot migration described below with regard to and live migration described below with regard to . A network address for instance may also be moved such that requests directed toward instance are now directed toward instance in some embodiments.

Please note that previous descriptions are not intended to be limiting but are merely provided as an example of provider networks availability zones and compute instances. Various other components may interact with or assist in performing cross availability zone migrations for compute instances.

The specification next includes a general description of a provider network including multiple availability zones which may perform cross availability zone migrations. Then various examples of a provider network are discussed including different components modules or arrangements of components module that may be employed as part of implementing a provider. A number of different methods and techniques to implement cross availability zone migrations for compute instances of a provider network are then discussed some of which are illustrated in accompanying flowcharts. Finally a description of an example computing system upon which the various components modules systems devices and or nodes may be implemented is provided. Various examples are provided throughout the specification.

A virtual compute instance may for example comprise one or more servers with a specified computational capacity which may be specified by indicating the type and number of CPUs the main memory size and so on and a specified software stack e.g. a particular version of an operating system which may in turn run on top of a hypervisor . A number of different types of computing devices may be used singly or in combination to implement the compute instances of provider network in different embodiments including general purpose or special purpose computer servers storage devices network devices and the like.

Compute instances may operate or implement a variety of different platforms such as application server instances Java virtual machines JVMs general purpose or special purpose operating systems platforms that support various interpreted or compiled programming languages such as Ruby Perl Python C C and the like or high performance computing platforms suitable for performing client applications without for example requiring the client to access an instance . In some embodiments compute instances have different types or configurations based on expected uptime ratios. The uptime ratio of a particular compute instance may be defined as the ratio of the amount of time the instance is activated to the total amount of time for which the instance is reserved. Uptime ratios may also be referred to as utilizations in some implementations. If a client expects to use a compute instance for a relatively small fraction of the time for which the instance is reserved e.g. 30 35 of a year long reservation the client may decide to reserve the instance as a Low Uptime Ratio instance and pay a discounted hourly usage fee in accordance with the associated pricing policy. If the client expects to have a steady state workload that requires an instance to be up most of the time the client may reserve a High Uptime Ratio instance and potentially pay an even lower hourly usage fee although in some embodiments the hourly fee may be charged for the entire duration of the reservation regardless of the actual number of hours of use in accordance with pricing policy. An option for Medium Uptime Ratio instances with a corresponding pricing policy may be supported in some embodiments as well where the upfront costs and the per hour costs fall between the corresponding High Uptime Ratio and Low Uptime Ratio costs.

Compute instance configurations may also include compute instances with a general or specific purpose such as computational workloads for compute intensive applications e.g. high traffic web applications ad serving batch processing video encoding distributed analytics high energy physics genome analysis and computational fluid dynamics graphics intensive workloads e.g. game streaming 3D application streaming server side graphics workloads rendering financial modeling and engineering design memory intensive workloads e.g. high performance databases distributed memory caches in memory analytics genome assembly and analysis and storage optimized workloads e.g. data warehousing and cluster file systems . Size of compute instances such as a particular number of virtual CPU cores memory cache storage as well as any other performance characteristic. Configurations of compute instances may also include their location in a particular data center availability zone geographic location etc. . . . and in the case of reserved compute instances reservation term length.

In various embodiments compute instances may be associated with one or more different security groups. Security groups may enforce one or more network traffic policies for network traffic at members of the security group. Membership in a security group may not be related to physical location or implementation of a compute instance. A security group may enforce respective network traffic policies for their member instances. The previous descriptions are not intended to be limiting but merely illustrative of the many different configurations possible for a compute instances provided by provider network .

Different availability zones may be configured to host different virtual compute instances . For example availability zone hosts instances availability zone hosts instances and availability zone hosts instances . Multiple virtualization hosts e.g. compute nodes systems or servers such as computing system described below with regard to may be respectively located in individual availability zones may implement and or manage multiple compute instances in some embodiments. A virtualization host may include a virtualization management module capable of instantiating and managing a number of different client accessible virtual machines or compute instances . The virtualization management module may include for example a hypervisor and an administrative instance of an operating system which may be termed a domain zero or dom0 operating system in some implementations. The dom0 operating system may not be accessible by clients on whose behalf the compute instances run but may instead be responsible for various administrative or control plane operations of the network provider including responding to requests to provision configure start and or stop compute instances .

In various embodiments other computing services may provide resources in similar ways. For instance resources may be located in availability zone resource s may be located in availability zone and resources may be located in availability zone . Various compute nodes servers systems or devices e.g. computing system in may be located in respective availability zones and may implement or host the resources . One or more storage nodes for example may store a data volume resource that acts as a virtual block storage device for a particular instance

In various embodiments provider network may also implement a control plane . Control plane may provide and or perform various administrative functions and tasks for virtual computing service and other computing service s . For example control plane may implement various client management features. For example control plane may coordinate the metering and accounting of client usage of network based services including computing resources such as by tracking the identities of requesting clients the number and or frequency of client requests the types and times instances are utilized size of data stored or retrieved on behalf of clients overall storage bandwidth used by clients or any other measurable client usage parameter. Control plane may also implement financial accounting and billing systems or may maintain a database of usage data that may be queried and processed by external systems for reporting and billing of client usage activity. In certain embodiments control plane may be configured to collect monitor and or aggregate a variety of provider network operational metrics such as metrics reflecting the rates and types of requests received from clients bandwidth utilized by such requests system processing latency for such requests system component utilization e.g. network bandwidth and or utilization within the provider network and within specific availability zones rates and types of errors or any other suitable metrics. In some embodiments such metrics may be used by system administrators to tune and maintain system components while in other embodiments such metrics or relevant portions of such metrics may be exposed to clients to enable such clients to monitor their usage of virtual computing service and or other computing service s or the underlying systems that implement those services .

In some embodiments control plane may also implement user authentication and access control procedures. For example for a given network based services request to access a particular instance control plane may be configured to ascertain whether the client associated with the request is authorized to access the particular instance. Control plane may determine such authorization by for example evaluating an identity password or other credential against credentials associated with the particular database or evaluating the requested access to the particular instance against an access control list for the particular instance.

In at least some embodiments control plane may implement migration manager . Migration manager may be configured manage direct and otherwise facilitate cross availability zone migrations for compute instances and resources in provider network . Facilitating cross availability zone migrations may account for the various communication constraints between availability zones which may be implemented as part of providing independently accessible fault tolerant zones. describe various different techniques messages and other capabilities of migration manager .

In some embodiments provider network may include the hardware e.g. modems routers switches load balancers proxy servers etc. and software e.g. protocol stacks accounting software firewall security software etc. necessary to establish a networking links between different components of provider network such as virtualization hosts control plane components as well as external networks e.g. the Internet . In some embodiments provider network may employ an Internet Protocol IP tunneling technology to provide an overlay network via which encapsulated packets may be passed through the internal network using tunnels. The IP tunneling technology may provide a mapping and encapsulating system for creating an overlay network and may provide a separate namespace for the overlay layer and the internal network layer. Packets in the overlay layer may be checked against a mapping directory e.g. provided by mapping service in control plane to determine what their tunnel target should be. The IP tunneling technology provides a virtual network topology the interfaces that are presented to clients may be attached to the overlay network so that when a client provides an IP address that they want to send packets to the IP address is run in virtual space by communicating with a mapping service that knows where the IP overlay addresses are. Implementing the overlay network may allow network addresses to moved or attached to different resources such as compute instances when performing cross availability zone migrations for compute instances.

Clients may encompass any type of client configurable to submit requests to network provider . For example a given client may include a suitable version of a web browser or may include a plug in module or other type of code module configured to execute as an extension to or within an execution environment provided by a web browser. Alternatively a client may encompass an application such as a computing resource tool or application or user interface thereof a media application an office application or any other application that may make use of compute instances or resources to perform various operations. In some embodiments such an application may include sufficient protocol support e.g. for a suitable version of Hypertext Transfer Protocol HTTP for generating and processing network based services requests without necessarily implementing full browser support for all types of network based data. In some embodiments clients may be configured to generate network based services requests according to a Representational State Transfer REST style network based services architecture a document or message based network based services architecture or another suitable network based services architecture. In some embodiments a client e.g. a computational client may be configured to provide access to a compute instance in a manner that is transparent to applications implement on the client utilizing computational resources provided by the compute instance .

Clients may convey network based services requests to provider network via network . In various embodiments network may encompass any suitable combination of networking hardware and protocols necessary to establish network based communications between clients and provider network . For example a network may generally encompass the various telecommunications networks and service providers that collectively implement the Internet. A network may also include private networks such as local area networks LANs or wide area networks WANs as well as public or private wireless networks. For example both a given client and provider network may be respectively provisioned within enterprises having their own internal networks. In such an embodiment a network may include the hardware e.g. modems routers switches load balancers proxy servers etc. and software e.g. protocol stacks accounting software firewall security software etc. necessary to establish a networking link between given client and the Internet as well as between the Internet and provider network . It is noted that in some embodiments clients may communicate with provider network using a private network rather than the public Internet.

Once the availability zone is identified migration manager may then provision configure compute instance to serve as a destination compute instance for compute instance . This provision configuration may configure compute instance based at least in part on the current configuration of compute instance which may include moving any data stored locally at compute instance to compute instance . describes in further detail the various ways in which compute instance may be provisioned and configured in some embodiments. If as illustrated in zonal resources e.g. resources provided by other computing service s in above are being utilized by compute instance the migration manager may provision configure new zonal resources to be utilized by compute instance in some embodiments. In some embodiments zonal resource s may be resources that contain data or information that may be transferred as part of moving zonal resources to zonal resources . Thus migration manager may be configured to direct and or facilitate the transfer of resources . As direct communications between availability zones may be constrained e.g. limited bandwidth or unavailable e.g. due to network partition in some embodiments migration manager may be configured to provide alternative network transfer paths. For instance in some embodiments migration manager may be configured to compress and or encrypt resources such as data and utilize public networks e.g. the Internet to transfer resources to the new availability zone

Disable operation may stop operation of compute instance and zonal resources along network address . The virtualization hosts and other system components may then be free to host other virtual compute instances or resources. However in some embodiments some resources such as networking resources may remain operational to redirect network traffic to instance .

In some embodiments network address may be implemented for compute instance in order to handle requests for compute instance . Once the provisioning and configuring of new instance and new resources is complete and instance is disabled migration manager may move 322 network address to network address . For example mapping information or other network controls may be modified to direct traffic for compute instance to network address . In some embodiments the same network address may be assigned to as was previously used for requests to compute instance . discussed below provide many different examples of techniques that migration manager may perform to provide cross availability zone migrations in some embodiments.

In various embodiments control plane or migration manager may implement a network based interface for via which compute instance migration requests may be received and responded. is a block diagram illustrating an interface for a provider network via which clients of a provider network may submit migration requests for compute instances according to some embodiments. Interface may be a network based interface that may provide a programmatic interface and or a graphical interface for interacting with migration manager . For example in some embodiments interface may be implemented as an application programming interface API according to which the requests messages and or responses illustrated in as well as others not illustrated may be formatted and processed. In another example interface may be implemented as a graphical user interface GUI which may be implemented in various ways such as a website or other client accessible tool that provides an interface via which clients can interact with .

In some embodiments migration manager may provide availability zone recommendations to client . Availability zone recommendations may be provided in response to a request for recommendations not illustrated a request to migrate not illustrated or may be pushed out to clients at various times. Availability zone recommendations may be determined in various ways such as based on the size type and or configuration of the currently operating virtual compute instance which may then be compared with the capacity or capabilities of different availability zones in order to identify availability zones that are able to host a destination virtual compute instance

Client may submit a migration request via interface to migration manager . The migration request as discussed below with regard to may specify various information about the migration operation such as the type of migration to be performed e.g. reboot or live migration the timing of the migration and or the specific availability zone to host the destination virtual compute instance in some embodiments. In some embodiments separate migration scheduling instructions may be provided via interface to migration manager . For example an availability zone may be specified in a request to migrate and in response recommended timeslots may be provided to client in order to make a choice for migration.

An indication or notification may be provided to client via interface that migration is complete. In some embodiments client may implement a long poll or other completion discovery technique requesting status of the migration from migration manager . The migration complete indication may also provide information about the configuration access and or networking of the new destination virtual compute instance.

The examples of performing compute instance migrations across availability zones of a provider network discussed above with regard to have been given in regard to virtual computing resources offered by a provider network. Various other types or configurations of a provider network may implement these techniques. Compute instance migrations may in some embodiments be performed as part of a larger migration of many different computing resources of a provider network including multiple compute instances. is high level flowchart illustrating various methods and techniques for performing compute instance migrations across availability zones in a provider network according to some embodiments. These techniques as well as the techniques of described below may be implemented using various components of a provider network as described above with regard to or other types or systems implementing a provider network.

A virtual compute instance may be currently operating for a client in a particular availability zone of a provider network performing various operations functions services or tasks. As indicated at a request to migrate the virtual compute instance located in the particular availability zone to another availability zone in the provider network may be received in some embodiments. Availability zones as noted earlier may be geographically diverse as well as fault tolerant zones. Virtual computing resources may be independently hosted such that failures of an availability zone may be isolated to a particular availability zone. Inter availability zone communication or network may be unavailable limited or otherwise constrained. Thus a client may be unable to perform a cross availability zone migration without assistance from a control plane migration manager or other component of a provider network that may be configured to perform operations across availability zones that operate independently.

The request may specify a type of migration operation for the virtual compute instance which may also be referred to herein as the original virtual compute instance . For example in some embodiments a reboot operation may be specified to perform migration. The currently operating virtual compute instance may be rebooted and from the perspective of the client appear to reboot in the other availability zone. discusses performing cross availability zone migration using a reboot operation in further detail. In another example the request may specify that live migration be performed to migrate the currently operating virtual compute instance to the other availability zone. discusses performing cross availability zone migration using live migration.

In some embodiments the request may specify the availability zone to transfer the virtual compute instance to. For example the request may include an identifier or other indicator of the specified availability zone. In some embodiments the request may include scheduling and or performance instructions for performing the migration. A particular timeslot of a set of available timeslots or time periods may be indicated in some embodiments. The request may provide other types of scheduling instructions such as a request that the migration be performed as soon as possible or be completed by a particular time date. For migrations of multiple virtual compute instances as part of batch or group migration the request may include a scheduling instruction indicating an order for performing the migration for the virtual compute instance with respect to other virtual compute instances also being migrated. In some embodiments the request may be received from a client control plane for the provider network or other component or device with authorization to migrate the virtual compute instance.

In some embodiments the request may not specify or indicate the availability zone to migrate the virtual compute instance to. Instead the availability zone may be determined after the request is received. For example different availability zones may be evaluated to determine whether the capacity is available to host the destination virtual compute instance. The size type and or configuration of the currently operating virtual compute instance may be compared with the capacity or capabilities of different availability zones to identify availability zones that are able to host the destination virtual compute instance. From amongst those availability zones that are identified as able to host the destination virtual compute instance an availability zone may be selected. For instance an availability zone may be selected if it already hosts another virtual compute instance for the client or alternatively if the availability zone does not host another virtual compute instance for the client . Similarly the request may include criteria or conditions for selecting a new availability zone though not necessarily specifying a specific availability zone which may be used to determine the new availability zone.

In some embodiments the request to migrate the virtual compute instance may be denied. For example the migration of the virtual compute instance may be too costly e.g. in terms of resources to perform the migration . If for instance an estimated time to perform the migration of the virtual compute instance exceeds some migration time limit then the request to perform the migration may be denied. Alternatively if a new availability zone to host the destination virtual compute instance cannot be determined or identified e.g. lacking the capacity or resources to provide the virtual compute instance then the request to perform the migration may be denied in some embodiments. Similarly as discussed below with regard to migration may not complete e.g. due to control plane or client abort . A restored version of the virtual compute instance may be provided to the client in the availability zone even if it is not implemented on the same underlying host .

The request to migrate the virtual compute instance may be received acknowledged or denied via a network based interface in some embodiments. The network based interface as discussed above with regard to may allow a client or other system or device to programmatically e.g. via an API or interactively e.g. via a graphical user interface submit the migration request. Information pertaining to the migration request may also be sent and received via the network based interface such as scheduling instructions for performing the migration.

However the other availability zone is identified a new virtual compute instance may be provisioned in the other availability zone as a destination virtual compute instance for the migration. For instance a server compute node or other system hosting virtual compute instances for the provider network may be directed to start or enable the destination virtual compute instance at the server compute node or other system. As indicated at the destination virtual compute instance located in the other availability zone that is provisioned for the migration of the virtual compute instance may be configured based at least in part on a configuration of the virtual compute instance in various embodiments. As discussed above with regard to virtual compute instances may be implemented in different types sizes or configurations may utilize different computing resources or more generally may operate or perform in different ways. For example the virtual compute instance may be a type of compute instance with certain specified performance characteristics e.g. processing time network capabilities graphics processing etc. . These performance characteristics may be provided by underlying hardware at the virtualization host that would be implementing the destination virtual compute instance. In this scenario the destination virtual compute instance may be configured in such a way as to utilize the underlying hardware in order to provide the specified performance characteristics of the original virtual compute instance. In some embodiments configuring the destination virtual compute instance may include launching the destination virtual compute instance with a particular machine image software package or any other software configuration that may be implemented running at the original virtual compute node. A machine image may for instance be purchased for and running on the original virtual compute instance. Configuring the destination virtual compute instance may effectively transfer the license use or purchase of the machine image to the destination virtual compute instance allowing the machine image to run on the destination virtual compute instance. In some embodiments software hardware interfaces or controls may be updated when configuring the destination virtual compute instance in such a way so that the destination virtual compute instance may appear to be the same virtual compute instance even if improvements upgrades and or patches may be implemented at the destination virtual compute instance that were not implemented for the original virtual compute instance.

In some embodiments the original virtual compute instance may utilize other computing resources in the provider network. These other computing resources may be offered as part of a same network based service or different network based services. Other computing resources may include storage resources e.g. storage volumes database systems or backup systems networking resources e.g. load balancing resources traffic control resources or private network configurations or any other type of computing resource that may be located in the same availability zone as the original virtual compute instance. As indicated at one or more resources utilized by the virtual compute instance and located in the same availability zone may be moved to the other availability zone in various embodiments. For example storage resources such as data volume database or other set of data may be transferred to equivalent compatible resources located in the same availability zone as the destination virtual compute instance. As with the destination virtual compute instance computing resources may be provisioned and configured to serve as destination resources for the moved resources. If for example a data volume is to be transferred for use by the destination virtual compute instance one or more storage nodes that provide storage services may be provisioned and configured to store the data volume in a same format access scheme or configuration as the data volume was stored in the previous availability zone. Similarly other computing resources may be provisioned and configured to operate in a same manner as in the previous availability zone.

As indicated at the migration of the virtual compute instance may then be completed such that the destination virtual compute instance is currently operating for the client of the provider network instead of the virtual compute instance in various embodiments. For example operation at the original virtual compute instance may be stopped and the destination virtual compute instance may be made available to the client in a coordinated fashion so that it appears to the client that the virtual compute instance moved from the first availability zone to the new availability zone. In at least some embodiments the network address e.g. IP address for the original virtual compute instance may be moved to the destination virtual compute instance. For instance network traffic controllers routers mapping services directories or other network controls may be configured to redirect network traffic received for the original virtual compute instance to the destination virtual compute instance. discussed below provider further examples of different migration techniques such as reboot migration and live migration.

As indicated at a notification may be sent to the client indicating that the migration of the virtual compute instance is complete. The notification may include information for accessing the destination virtual compute instance e.g. new network address identification of the new availability zone and or identification of differences between the original virtual compute instance and the destination virtual compute instance . In some embodiments the notification may be provided in response to a request from a client for a status of the migration e.g. by a client polling to determine when the migration is complete .

In at least some embodiments the techniques described above with regard to may be performed for multiple virtual compute instances. For example a client or control plane component or service of the provider network may schedule a batch or group of virtual compute instances for migration. The batch compute instance migration may include a schedule or ordering for performing the migrations though some migrations may be performed in parallel in some embodiments.

As discussed above in some embodiments a reboot operation may be performed to implement the migration of a virtual compute instance from one availability zone to another. Performing a reboot operation may allow a migration to appear from the perspective of a client as merely a reboot of the virtual compute instance powering down in one availability zone and starting up in another. is high level flowchart illustrating various methods and techniques for providing reboot compute instance migration to another availability zone according to some embodiments. As indicated at a request may be received to reboot a virtual compute instance for which migration to another availability zone has been initiated in some embodiments. The request may be received from a client for example allowing the client to initiate completion of the migration. In some embodiments a notification or other indication may be provided to a client indicating that the reboot operation may be performed to complete the migration. Reboot requests may be received from a control plane or other system or device that is not the client of the provider network but has authorization to complete the migration of the virtual compute instance. For example the migration manager discussed above in may send a request to reboot the virtual compute instance.

As indicated at in response to receiving the reboot request operation at the virtual compute instance in the availability zone may be stopped in some embodiments. For example the virtualization host that implements the virtual compute instance may be instructed to shutdown and or release the virtual hardware resources and software resources of the virtual compute instance effectively making the virtualization host able to host another virtual compute instance using the released resources. Thus although the request may be to reboot the virtual compute instance the operation performed may terminate or shutdown the virtual compute instance. As indicated at operation of a destination virtual compute instance in the other availability zone may be started in some embodiments. An instruction or request for instance may be provided to the virtualization host that implements the destination virtual compute instance which may already be provisioned and configured for operation to begin actively performing work providing a service other otherwise operating as the original virtual compute instance. Network traffic sent to the currently operating destination virtual compute instance may then be directed to the destination virtual compute instance as indicated at . For example a network endpoint or other network traffic component may be modified or programmed to now direct traffic for the virtual compute instance to the destination virtual compute instance.

As discussed above in some embodiments a live migration operation may be performed to implement the migration of a virtual compute instance from one availability zone to another. Performing a live migration operation may allow the transition from an original compute instance to a destination compute instance in another availability zone to be performed live without experiencing any downtime for the virtual compute instance. is a high level flowchart illustrating various methods and techniques for providing live compute instance migration to another availability zone according to some embodiments. As indicated at a live migration request to migrate from a currently operating virtual compute instance to a destination virtual compute instance located in another availability zone may be received in various embodiments. As discussed above with regard to element in a destination virtual compute resource may be provisioned and configured based on the currently operating virtual compute instance.

As indicated at operation at the destination virtual compute instance may be started to initiate the migration in some embodiments. For instance the destination virtual compute instance may begin operating as configured. The operation of the destination virtual compute instance may be synchronized with the currently operating virtual compute instance as indicated at in some embodiments. For example tasks operations or other functions performed at the original virtual compute instance may be replicated at the destination virtual compute instance. A stream of messages or indications of these tasks may be sent from the original virtual compute instance to the destination virtual compute instance so that they may be replicated for instance. Access to other computing resources e.g. a data volume located in the availability zone with the original virtual compute instance and utilized by the original virtual compute instance may be provided to the destination virtual compute instance in order to replicate or be aware of the current state of operations at the original virtual compute instance in some embodiments.

Once synchronized in some embodiments network traffic for the currently operating virtual compute instance may be directed to the destination virtual compute instance as indicated at . For example a network endpoint or other network traffic component may be modified or programmed to now direct traffic for the virtual compute instance to the destination virtual compute instance. Operation of the original virtual compute instance that is currently operating may be stopped as indicated at . For example the virtualization host that implements the virtual compute instance may be instructed to shutdown and or release the virtual hardware resources and software resources of the virtual compute instance effectively making the virtualization host able to host another virtual compute instance using the released resources.

As noted above in some embodiments the migration request may include various information specifying certain aspects of the migration such as a specific availability zone in which the destination virtual compute instance is located and or scheduling instructions for determining when the virtual compute instance may be migrated. is a high level flowchart illustrating various methods and techniques for handling migration requests to move compute instances to specific availability zones according to some embodiments. As indicated at a request may be received to migrate a currently operating virtual compute instance located in an availability zone to a different specified availability zone during a specified timeslot. As indicated at it may be determined whether capacity for the virtual compute instance in the specified availability zone exists to host a destination virtual compute instance. For example if the currently operating compute instance is a particular type of compute instance with a certain set of performance characteristics e.g. certain amount speed of system memory processing capabilities and networking capabilities then the specified availability zone may be evaluated to determine whether a virtualization host located in the availability zone implementing hardware sufficient to provide the performance characteristics exists and is available to host a virtual compute instance.

If the specified availability zone does not have the capacity to host a destination virtual compute instance for the migration then as indicated by the negative exit from alternative availability zones may be recommended to the client as indicated at in some embodiments. Techniques to identify availability zones for recommendation may be similar to the techniques discussed above for identifying an availability zone to migrate an instance to. The size type and or configuration of the currently operating virtual compute instance may be compared with the capacity or capabilities of different availability zones in order to identify availability zones that are able to host the destination virtual compute instance. From amongst those availability zones that are identified as able to host the destination virtual compute instance recommended availability zone s may be selected. In some embodiments further constraints preferences or other criteria may be used to narrow down the selection of availability zones for recommendation e.g. client may indicate preferences for an availability zone that increases diversity of availability zones for resources used by the client providing greater durability availability . Please note that in some embodiments recommendations for availability zones may be provided to a client upon request or prior to receiving a request for migration in some embodiments as illustrated above in .

If however the specified availability zone is available as indicated by the positive exit from then it may be determined whether the specified timeslot in the request is available for performing the migration as indicated at . For example in some embodiments migration requests may be managed and limited to a specific number of migrations into or out of an availability zone at a time which may reduce congestion on inter availability zone communications that may have limited bandwidth . Limitations on migrations may be enforced by providing a certain number of slots and different times for performing cross availability zone migrations. These limitations may be client specific so as not to provide too great a performance impact on a client if multiple virtual compute instance migrations are requested for a client. Alternatively limitations on migrations may be implemented to provide a level of service performance at the provider network. Thus if no more timeslots are available at the specified timeslot then as indicated by the negative exit from alternative timeslot recommendations may be provided to the client which are available. As with the availability zone recommendations above timeslot recommendations or other scheduling instructions recommendations be provided to a client upon request or prior to receiving a request for migration in some embodiments. If the specified timeslot is available then migration of the currently operating virtual compute instance to the specified availability zone and specified timeslot may be initiated as indicated according to the various techniques discussed above.

As indicated at migrations for some virtual compute instances may be unable to complete. For example the size configuration of the virtual compute instance may be difficult to transfer a destination availability zone may no longer be available or attached or utilized zonal resources may be unavailable for transfer. In some embodiments a client or control plane may request the migration be aborted. If the migration is unable to be completed as indicated by the negative exit from then a restored version of the original virtual compute instance may be provided in the original availability zone. Note that in some embodiments the restored version of the availability zone may not be implemented on the same virtualization host as prior to initiating the migration. However the restored version of the original virtual compute instance may be configured similar to the configuration of a destination compute instance described above with regard to .

Please note that the above description and illustration in is not intended to be limiting as to other ways in which specified availability zones timeslot requests and or migration abortions may be handled. In some embodiments a request may specify either an availability zone or a timeslot and thus only some of the different elements may be performed possibly along with additional elements .

The methods described herein may in various embodiments be implemented by any combination of hardware and software. For example in one embodiment the methods may be implemented by a computer system e.g. a computer system as in that includes one or more processors executing program instructions stored on a computer readable storage medium coupled to the processors. The program instructions may be configured to implement the functionality described herein e.g. the functionality of various servers and other components that implement a provider network described herein . The various methods as illustrated in the figures and described herein represent example embodiments of methods. The order of any method may be changed and various elements may be added reordered combined omitted modified etc.

Performing compute instance migrations across availability zones of a provider network as described herein may be executed on one or more computer systems which may interact with various other devices. is a block diagram illustrating an example computer system according to various embodiments. For example computer system may be configured to implement nodes of a compute cluster a distributed key value data store and or a client in different embodiments. Computer system may be any of various types of devices including but not limited to a personal computer system desktop computer laptop or notebook computer mainframe computer system handheld computer workstation network computer a consumer device application server storage device telephone mobile telephone or in general any type of computing device.

Computer system includes one or more processors any of which may include multiple cores which may be single or multi threaded coupled to a system memory via an input output I O interface . Computer system further includes a network interface coupled to I O interface . In various embodiments computer system may be a uniprocessor system including one processor or a multiprocessor system including several processors e.g. two four eight or another suitable number . Processors may be any suitable processors capable of executing instructions. For example in various embodiments processors may be general purpose or embedded processors implementing any of a variety of instruction set architectures ISAs such as the x86 PowerPC SPARC or MIPS ISAs or any other suitable ISA. In multiprocessor systems each of processors may commonly but not necessarily implement the same ISA. The computer system also includes one or more network communication devices e.g. network interface for communicating with other systems and or components over a communications network e.g. Internet LAN etc. . For example a client application executing on system may use network interface to communicate with a server application executing on a single server or on a cluster of servers that implement one or more of the components of the provider network described herein. In another example an instance of a server application executing on computer system may use network interface to communicate with other instances of the server application or another server application that may be implemented on other computer systems e.g. computer systems .

In the illustrated embodiment computer system also includes one or more persistent storage devices and or one or more I O devices . In various embodiments persistent storage devices may correspond to disk drives tape drives solid state memory other mass storage devices or any other persistent storage device. Computer system or a distributed application or operating system operating thereon may store instructions and or data in persistent storage devices as desired and may retrieve the stored instruction and or data as needed. For example in some embodiments computer system may host a storage system server node and persistent storage may include the SSDs attached to that server node.

Computer system includes one or more system memories that are configured to store instructions and data accessible by processor s . In various embodiments system memories may be implemented using any suitable memory technology e.g. one or more of cache static random access memory SRAM DRAM RDRAM EDO RAM DDR 10 RAM synchronous dynamic RAM SDRAM Rambus RAM EEPROM non volatile Flash type memory or any other type of memory . System memory may contain program instructions that are executable by processor s to implement the methods and techniques described herein. In various embodiments program instructions may be encoded in platform native binary any interpreted language such as Java byte code or in any other language such as C C Java etc. or in any combination thereof. For example in the illustrated embodiment program instructions include program instructions executable to implement the functionality of a provider network in different embodiments. In some embodiments program instructions may implement multiple separate clients server nodes and or other components.

In some embodiments program instructions may include instructions executable to implement an operating system not shown which may be any of various operating systems such as UNIX LINUX Solaris MacOS Windows etc. Any or all of program instructions may be provided as a computer program product or software that may include a non transitory computer readable storage medium having stored thereon instructions which may be used to program a computer system or other electronic devices to perform a process according to various embodiments. A non transitory computer readable storage medium may include any mechanism for storing information in a form e.g. software processing application readable by a machine e.g. a computer . Generally speaking a non transitory computer accessible medium may include computer readable storage media or memory media such as magnetic or optical media e.g. disk or DVD CD ROM coupled to computer system via I O interface . A non transitory computer readable storage medium may also include any volatile or non volatile media such as RAM e.g. SDRAM DDR SDRAM RDRAM SRAM etc. ROM etc. that may be included in some embodiments of computer system as system memory or another type of memory. In other embodiments program instructions may be communicated using optical acoustical or other form of propagated signal e.g. carrier waves infrared signals digital signals etc. conveyed via a communication medium such as a network and or a wireless link such as may be implemented via network interface .

In some embodiments system memory may include data store which may be configured as described herein. In general system memory e.g. data store within system memory persistent storage and or remote storage may store data blocks replicas of data blocks metadata associated with data blocks and or their state configuration information and or any other information usable in implementing the methods and techniques described herein.

In one embodiment I O interface may be configured to coordinate I O traffic between processor system memory and any peripheral devices in the system including through network interface or other peripheral interfaces. In some embodiments I O interface may perform any necessary protocol timing or other data transformations to convert data signals from one component e.g. system memory into a format suitable for use by another component e.g. processor . In some embodiments I O interface may include support for devices attached through various types of peripheral buses such as a variant of the Peripheral Component Interconnect PCI bus standard or the Universal Serial Bus USB standard for example. In some embodiments the function of I O interface may be split into two or more separate components such as a north bridge and a south bridge for example. Also in some embodiments some or all of the functionality of I O interface such as an interface to system memory may be incorporated directly into processor .

Network interface may be configured to allow data to be exchanged between computer system and other devices attached to a network such as other computer systems which may implement one or more storage system server nodes database engine head nodes and or clients of the database systems described herein for example. In addition network interface may be configured to allow communication between computer system and various I O devices and or remote storage . Input output devices may in some embodiments include one or more display terminals keyboards keypads touchpads scanning devices voice or optical recognition devices or any other devices suitable for entering or retrieving data by one or more computer systems . Multiple input output devices may be present in computer system or may be distributed on various nodes of a distributed system that includes computer system . In some embodiments similar input output devices may be separate from computer system and may interact with one or more nodes of a distributed system that includes computer system through a wired or wireless connection such as over network interface . Network interface may commonly support one or more wireless networking protocols e.g. Wi Fi IEEE 802.11 or another wireless networking standard . However in various embodiments network interface may support communication via any suitable wired or wireless general data networks such as other types of Ethernet networks for example. Additionally network interface may support communication via telecommunications telephony networks such as analog voice networks or digital fiber communications networks via storage area networks such as Fibre Channel SANs or via any other suitable type of network and or protocol. In various embodiments computer system may include more fewer or different components than those illustrated in e.g. displays video cards audio cards peripheral devices other network interfaces such as an ATM interface an Ethernet interface a Frame Relay interface etc. 

It is noted that any of the distributed system embodiments described herein or any of their components may be implemented as one or more network based services. For example a compute cluster within a computing service may present computing services and or other types of services that employ the distributed computing systems described herein to clients as network based services. In some embodiments a network based service may be implemented by a software and or hardware system designed to support interoperable machine to machine interaction over a network. A network based service may have an interface described in a machine processable format such as the Web Services Description Language WSDL . Other systems may interact with the network based service in a manner prescribed by the description of the network based service s interface. For example the network based service may define various operations that other systems may invoke and may define a particular application programming interface API to which other systems may be expected to conform when requesting the various operations. though

In various embodiments a network based service may be requested or invoked through the use of a message that includes parameters and or data associated with the network based services request. Such a message may be formatted according to a particular markup language such as Extensible Markup Language XML and or may be encapsulated using a protocol such as Simple Object Access Protocol SOAP . To perform a network based services request a network based services client may assemble a message including the request and convey the message to an addressable endpoint e.g. a Uniform Resource Locator URL corresponding to the network based service using an Internet based application layer transfer protocol such as Hypertext Transfer Protocol HTTP .

In some embodiments network based services may be implemented using Representational State Transfer RESTful techniques rather than message based techniques. For example a network based service implemented according to a RESTful technique may be invoked through parameters included within an HTTP method such as PUT GET or DELETE rather than encapsulated within a SOAP message.

Although the embodiments above have been described in considerable detail numerous variations and modifications may be made as would become apparent to those skilled in the art once the above disclosure is fully appreciated. It is intended that the following claims be interpreted to embrace all such modifications and changes and accordingly the above description to be regarded in an illustrative rather than a restrictive sense.

