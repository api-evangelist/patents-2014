---

title: Video stabilization
abstract: Method, device and computer program product for transmitting a video signal from a user device includes capturing a plurality of frames of the video signal using a camera at the user device, determining a functional state of the device and selectively stabilizing the video signal prior to transmission based on the functional state.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09635256&OS=09635256&RS=09635256
owner: Skype
number: 09635256
owner_city: Dublin
owner_country: IE
publication_date: 20140331
---
This application claims priority under 35 U.S.C. 119 or 365 to Great Britain Application No. GB 1116566.9 entitled Video Stabilization filed Sep. 26 2011 and U.S. application Ser. No. 13 307 800 entitled Video Stabilization filed Nov. 30 2011. The entire teachings of the above applications are incorporated herein by reference.

Cameras can be used to capture a sequence of images to be used as frames of a video signal. Cameras may be fixed to stable objects for example a camera may be mounted on a stand such as a tripod to thereby keep the camera still while the video frames are captured. However often cameras may be implemented in mobile devices and are not necessarily mounted to fixed objects for example a camera may be held or may be on a moving object such as a vehicle. Movement of the camera while the camera is capturing frames of a video signal may result in unwanted movement in the video signal itself.

Image stabilization is a method that can be used to compensate for the unwanted movement in a video signal. Some systems perform motion estimation in order generate motion vectors for use by an image stabilization process. One such system is described in Online Video Stabilization Based on Particle Filters by Junlan Yang et. al. Image stabilization algorithms may consist of three main parts motion estimation motion smoothing and motion compensation. A motion estimation block may estimate local motion vectors within the video signal and on the basis of these local estimates calculate a global motion vector. A motion smoothing block may then deal with filtering of the estimated global motion vector in order to smooth the calculated value and prevent large and undesirable differences between motion vectors calculated previously. A motion compensation block may then shift an image in the opposite direction to the filtered global motion vector to thereby stabilize the video signal. The motion compensation block may take into account sophisticated transformations like rotation warping or zooming.

It can require large amounts of processing resources to perform image stabilization based on motion vectors as described above. This can be a problem when the video signal is to be stabilized in real time i.e. when a stabilized version of the video signal is to be used e.g. transmitted in a video call or output from a device at the same time as it is being captured by the camera. This can also be a problem when the device which is performing the image stabilization is a small mobile device such as a mobile telephone in which the processing resources are limited.

In recent years motion sensors have become simpler and cheaper to manufacture and the size of motion sensors has reduced significantly. It is now feasible to implement motion sensors in mobile devices. Motion sensors generate samples representing the motion of the sensor. Two prior art documents Using Sensors for Efficient Video Coding in Hand held devices by Andy L. Lin and Accelerometer Based Digital Video Stabilization for General Security Surveillance Systems by Martin Drahansky et. al mention the possibility of using data from motion sensors for stabilization of a video signal.

A known processing utility VirtualDub provides offline stabilization i.e. not in real time that requires cropping of the video signal.

One mechanism for online i.e. real time send side digital stabilization has been proposed in GB Application No. 1109071.9. This mechanism is effective in many situations but requires cropping of the transmitted video. This can affect the quality of the transmitted video.

According to one aspect there is provided a method of transmitting a video signal from a user device the method comprising capturing a plurality of frames of the video signal using a camera at the user device determining a functional state of the device and selectively stabilizing the video signal prior to transmission based on the functional state.

The functional state may be a degree of motion of the camera the method comprising monitoring the motion of the camera and comparing it with a threshold.

The monitoring may comprise using a motion sensor associated with the camera to generate a plurality of samples representing motion of the camera using the samples to determine a displacement of the camera between successive frames captured by the camera and determining a pixel displacement representing motion in the video signal between the successive frames caused by the determined displacement of the camera the method further comprising and the method may further comprise comparing the pixel displacement to the threshold stabilizing the video signal prior to transmission if the pixel displacement exceeds said threshold and transmitting the video signal without stabilization otherwise.

The motion of the camera may be rotational motion the motion sensor is a rotational motion sensor and the displacement of the camera is an angular displacement of the camera.

The user device comprises a front facing camera and a rear facing camera and when the functional state is a selection of the front facing camera the video signal may be transmitted without stabilization. In some embodiments the video signal may be stabilized when the rear facing camera of the device is selected.

In some embodiments the sample rate of the samples generated using the motion sensor is higher than the frame rate of the video signal.

The step of stabilizing the video signal may comprise filtering the pixel displacement and shifting the image of at least one of the first and second frames in accordance with the filtered pixel displacement.

The step of filtering the pixel displacement may comprise determining an accumulated pixel displacement based on said determined pixel displacement for the second frame and determining a filtered accumulated pixel displacement for the second frame based on a weighted sum of the determined accumulated pixel displacement for the second frame and a filtered accumulated pixel displacement for the first frame.

In some embodiments a time offset is added to at least one of i the captured plurality of frames and ii the generated plurality of samples such that the timing of the captured plurality of frames matches the timing of the generated plurality of samples.

If the pixel displacement exceeds the threshold a timer may be used to determine whether a predetermined period of time has elapsed and the video signal is only stabilized when the time period has elapsed. The timer may be reset if the pixel displacement does not exceed said threshold 

According to a second aspect there is provided a device for stabilizing a video signal the device comprising a camera configured to capture a plurality of frames of the video signal means for determining a functional state of the device and means for selectively stabilizing the video signal prior to transmission based on the functional state.

In one or more embodiments the functional state is a degree of motion of the camera the device comprising means for monitoring the motion of the camera and comparing it with a threshold.

The device may further comprise a motion sensor associated with the camera configured to generate a plurality of samples representing motion of the camera a pixel displacement determining block configured to use the samples to determine a pixel displacement of the camera between successive frames captured by the camera said pixel displacement representing motion in the video signal between the successive frames caused by motion of the camera a comparison block configured to compare the pixel displacement to a predetermined threshold and a motion compensation block configured to stabilize the video signal prior to transmission if the pixel displacement exceeds said threshold otherwise no stabilization is implemented prior to transmission.

In one or more embodiments the motion of the camera is rotational motion the motion sensor is a rotational motion sensor and the displacement of the camera is an angular displacement of the camera.

According to a third aspect there is provided a computer program product for stabilizing a video signal the computer program product being embodied on a computer readable medium and configured so as when executed on a processor of a device to perform the operations described herein.

The inventors have realized that in order to maximize output video resolution it is desirable to only implement video stabilization when required to avoid unnecessary cropping of the transmitted video signal.

The teachings of all patents published applications and references cited herein are incorporated by reference in their entirety.

A stabilizer is described herein which can be in three states. The stabilizer may be implemented as code executed on the CPU .

The state of the stabilizer is selected based on the functional state of the mobile device. The first state is where the stabilizer is off and the process of stabilizing a video signal described below is not implemented. When the front facing camera captures a video signal the stabilizer is in the first state. Stabilization is disabled on front facing camera because cropping narrows the viewing angle and stabilizer stabilizes the background potentially adding more motion to the video signal.

The second state of the stabilizer is where high frequency motion shakiness of the device is monitored however no cropping of the video signal is implemented. When the rear facing camera captures a video signal the stabilizer is in the second state.

The third state of the stabilizer is where stabilization is applied whereby a video signal captured by the rear facing camera is cropped.

In step S the process determines whether the video signal is captured by the front facing camera . That is a determination is made as to whether the stabilizer is in the first state. If the video signal is captured by the front facing camera then the process does not proceed. If the video signal is not captured by the front facing camera i.e. the video signal is captured the rear facing camera then the process proceeds to step S.

The monitoring step S is shown in more detail in . A step S the rear facing camera captures images to be used as frames of a video signal. For example the camera may have an array of light sensors which record the level of light that is incident on the sensors during the time allocated to a frame of the video signal. A shutter of the camera is used to separate the frames in time such that during each frame the shutter is open for a period of time and closed for another period of time. The captured frames of video signal are provided to a pre processor e.g. implemented in a processing block by the CPU . The pre processor operates to stabilize the images in the frames of the video signal before the frames are encoded using a video encoding technique as is known in the art.

In step S while the camera is capturing frames of the video signal the motion sensor generates samples representing the motion of the device . For example the motion sensor may be a rotational motion sensor such as a gyroscope. The gyroscope measures angular velocity of the device and outputs samples representing the angular velocity at particular intervals. The intervals may or may not be regular intervals. In some cases on average the sample rate of the samples output from the gyroscope is higher than the frame rate of the video signal. For example the sample rate output from the gyroscope may be 60 samples per second which reflects the maximum usual shaking frequency of the device and is currently independent of frame rate. The samples generated by the gyroscope are provided to the pre processor.

In step S the angular displacement of the camera between two frames frame 1 and frame 2 of the video signal is determined. This determination may be performed by a processing block of the CPU . The inventors have identified that in order to effectively determine the angular displacement between the two frames using data from the gyroscope it is useful to integrate the angular velocity over the time interval between the midpoints of the exposure times of the frames captured by the camera . The inventors have also determined that this can be particularly problematic as it may not be possible to synchronies the sampling rate of the gyroscope with the frame rate of the camera particularly when 

the camera is arranged to adjust the exposure times in dependence on the available light which many cameras are 

the time stamps for the frames of the video signal provided by the camera relate to the times at which the shutter closes i.e. the end times of the frames as opposed to the midpoints of the exposure times of the frames and

As described above the pre processor receives video frames from the camera and also receives the samples from the gyroscope . The samples from the gyroscope are provided to the pre processor e.g. at regular intervals at a rate at least equivalent to the frame rate of the video signal captured by the camera . Using a higher sampling rate in the gyroscope gives more accurate angle estimates but can be more costly in terms of CPU usage.

A time stamp t provided by the camera to a first frame frame 1 of the video signal indicates the end time of the frame i.e. the time at which the shutter of the camera is closed to end frame 1. Similarly a time stamp t provided by the camera to a second frame frame 2 of the video signal indicates the end time of the frame i.e. the time at which the shutter of the camera is closed to end frame 2. In order to determine the angular displacement of the device between the first frame and the second frame rather than using the time stamps of the frames to denote the times of the frames it is more accurate to use the midpoints of the exposure time of frame 1 and frame 2. The exposure times of the first and second frames are denoted by eand e. The angular displacement is determined by integrating the angular velocity represented by the samples output from the gyroscope of the device between a time t 0.5 eand a time t 0.5 e. Therefore the angular displacement between frame 1 and frame 2 is given by 

Since the samples of the gyroscope are not synchronized with the timings of the frames of the video signal captured by the camera it might be the case that the gyroscope does not generate samples at the midpoints of the frames frame 1 and frame 2 . In which case the angular velocity of the device at the midpoints of the frames can be determined by interpolating the angular velocity represented by the samples generated by the gyroscope . The angular velocity is evaluated by interpolation at any time instant and the midpoints of the exposure times of the frames define the integral interval used when calculating the angular displacement according to the equation above.

There may arise a situation in which a frame to be stabilized is received at the pre processor after the latest sample from the gyroscope . For example when the frame 2 is captured at the camera the frame 2 may be received at the pre processor before any samples from the gyroscope have been generated subsequent to the midpoint of the exposure time of the frame 2 t 0.5 e . For example frame 2 may be received at the pre processor before the sample shown in . In this situation delay may be introduced into the video stream in order for the sample to be received at the pre processor before the frame 2 is processed thereby allowing the angular velocity at time t 0.5 e to be determined before the frame 2 is processed by the pre processor. Alternatively the angular velocity may be extrapolated from the previously received samples from the gyroscope in order to determine the angular velocity of the device at the time t 0.5 e .

In the case of no motion of the camera e.g. for fixed placement of the device the gyroscope may be disabled in order to save battery life. The state of no motion can be determined by feedback from a video encoder which encodes the video signal subsequent to the image stabilization method described herein and implemented by the pre processor. The video encoder may perform motion estimation as part of the encoding process and as such can determine whether the camera is moving. A state of motion can also be determined and used to enable the gyroscope when the camera is moved. When the device operates in the state of no motion the motion sensor may be polled at a slow interval to determine whether the device has started moving again. There may be computationally cheaper ways to determine when the device starts moving depending on hardware and Application Programming Interfaces APIs implemented in the operating system of the device .

The timings of the operation of the hardware used for the camera and for the gyroscope might not match. This may be because the camera and the gyroscope are implemented in independent hardware chips. Therefore it may be beneficial to add an offset to the time stamps of either or both the samples generated by the gyroscope and the frames of the video signal. In this way the timing of the samples from the gyroscope can be matched with the timing of the frames of the video signal correctly. The offset may be constant for a particular combination of hardware chips. Therefore a delay may be computed offline and used at the device without incurring a processing penalty for the method described herein.

Referring back to in step S a pixel displacement representing the motion of the camera is determined. In general a rotation of the camera results in an approximately constant pixel displacement across the image of a frame of the video signal independent of distances to objects in the image. This is in contrast to linear camera motion for which pixel displacement is a function of the distance to the object. A function or algorithm mapping the rotation of the device to a pixel displacement depends on parameters of the camera e.g. focal length and width of lens of the camera and the resolution of the images captured by the camera .

Encoder feedback can be useful to determine the accuracy of the samples generated by the gyroscope and to adapt the mapping algorithm. There are also some cases of motion and object placement where the stabilization model described herein based on the samples from the gyroscope is not accurate e.g. for rotation of the camera around a user s face the user s face may be stable in the middle of the frame but the gyroscope detects rotation and therefore the stabilization process will attempt to stabilize the background which may be detected by the encoder and fed back to the stabilization algorithm. In this way the stabilization algorithm can be adapted.

The pixel displacement determined in step S represents the magnitude of the motion in the images of the frames of the video signal resulting from the motion of the camera as opposed to motion in the subject matter of the images . In this way the pixel displacement determined in step S represents unwanted high frequency motion shaking in the images of the frames of the video signal.

Referring back to following the monitoring step S the magnitude of the motion in the images of the frames of the video signal resulting from the motion of the camera is compared to a predetermined threshold at step S to determine if the device is shaking.

If it is determined at step S that the device is not shaking the process proceeds back to the monitoring step S whereby no cropping of the video signal is implemented such that the video signal maintains maximum output video resolution. A timer implemented at the device not shown in is reset upon this determination.

If it is determined at step S that the device is shaking the process proceeds to step S. In step S a determination is made as to whether a period of time has elapsed since the timer was last reset.

If the period of time has not elapsed then the process proceeds back to monitoring step S. If the period of time has elapsed the process proceeds to a stabilization step S.

In step S the pixel displacement determined in step S is filtered. This is done in order to smooth the changes that are applied to the video signal in the image stabilization process over time to thereby provide a smoother stabilized video signal. The filter used to filter the pixel displacement can be designed in different ways depending on for example the resolution of the images captured by the camera the acceptable delay which may be applied to the video signal and the allowed amount of cropping which can be applied to the images of the original video signal received at the pre processor from the camera . For example higher resolution video frames may benefit from a larger filter attenuation of high frequency changes to the pixel displacement applied in the image stabilization process. On the other hand the amount of cropping sets a hard limit to the maximum filter attenuation.

An exponential filter may be used which filters the pixel displacements according to the equation filt 1  filt 1 

where n represents the frame number of the video signal x represents the accumulated displacement or position according to the pixel displacement determined in step S and x filt represents the filtered accumulated displacement which is subsequently used to determine how to align the input image in order to stabilize it as described in more detail below. In this way the filter acts as an exponential filter. When motion stops x filt x will converge to zero which implies no shifting of the image. The filter smoothes out changes to the determined pixel displacement over time by basing the filtered pixel displacements on the corresponding filtered pixel displacement of the previous frame as well as on the pixel displacement determined for the current frame in step S. The weighting applied to the filtered pixel displacement of the previous frame is 1 w whereas the weighting applied to the pixel displacement determined for the current frame is w. Therefore adjusting the weighting parameter w will adjust how responsive the filter is to changes in the pixel displacement x . A recursive Infinite Impulse Response IIR filter is more suited than a Finite Impulse Response FIR filter when the output x filt is clipped to be in the range x crop x crop as the clipped value is fed back to the filter loop and makes subsequent output of x filt less prone to clipping.

The weighting parameter w is adapted to the resolution and instant frame rate of the video signal to obtain a constant physical cut off frequency which is measured in Hertz. If the filter were an ideal filter then the physical cut off frequency would define the highest frequency component of changes to x which will be incorporated into x filt. Changes to x which have higher frequency than the cut off frequency will be attenuated by an ideal filter and will not be present in x filt. However the filter is not an ideal filter and as such the cut off frequency defines the highest frequency for which the attenuation applied by the filter is below 3 dB. So for non ideal filters there will be some attenuation below the cut off frequency and there will not be perfect attenuation above the cut off frequency. The filter output is clipped so that that the difference between x filt and x is not larger than the frame cropping size. w is adapted so that the physical cut off frequency is constant e.g. 0.5 Hz. From the filter transfer function a function w fc fs can be derived that maps a physical cut off frequency fc tow. When the sampling frequency frame rate fs changes w also changes even though fc is constant. The filter according to the filter equation above is well suited for instant changing of the cut off frequency changing w compared to other filters.

In step S the image of the second frame frame 2 is shifted using the filtered pixel displacement from step S. In this way the motion in the image of the second frame relative to the first frame due to the motion of the camera is attenuated. In other words the filtered pixel displacement is used to compensate for the motion in the video signal between the first and second frames caused by the motion of the camera to thereby stabilize the video signal.

The filtered pixel displacements are rounded to full pixel displacements i.e. integer pixel displacements . This allows a simple method to be employed to shift the image of the second frame. The image is represented using a stride value indicating memory space of the image a plurality of pixel values a pointer indicating the position of a first pixel of the image and a width value indicating the width of the image. The shifting of the image comprises adjusting the pointer and the width value without adjusting the stride value. It can be seen that the width value is independent of the stride value which allows the width of the image to be changed without affecting the stride of the image. Therefore the memory space of the image e.g. in the memory does not need to be changed when the image is shifted and or resized . This means that no copying of data in the memory is necessary with this approach. This is in contrast to a conventional method of cropping an image in which the crop area of the image is copied into a new memory area. Copying the crop area may be computationally complex which may be detrimental particularly when the method is to be implemented on a mobile device in which the processing resources available to the CPU may be limited. With the method described herein since the width value is independent of the stride value the new shifted image can be created by changing the pointer and the width while the stride is kept intact.

The image may be represented by multiple image planes for example a luma plane Y and two chroma planes U and V . The image planes of the input image may be shifted and resized by simply changing the pointers to the luma and chroma planes thereby modifying the width of the image planes whilst keeping the stride intact. The image planes are shifted by the same amount to ensure that the shifted image planes can be used together to represent the shifted image.

In order for this image shifting process to be implemented the image planes require respective pointers i.e. they cannot all be represented by the same single pointer. Furthermore as described above it is necessary that the image has independent width and stride values.

In summary of the method described above the following stages are implemented in the pre processor to stabilize the images of the frames of the video signal before the video signal is encoded with a video encoder 

In step S consideration is given to the next frame captured at step S to determine if the device is still shaking. That is the angular displacement of the camera between two frames frame 2 and 3 is determined and mapped to a pixel displacement representing the motion in the images of the frames of the video signal resulting from the motion of the camera . This has been described in detail hereinabove with reference to steps S and S.

At step S the filtered magnitude of the motion in the images of the frames of the video signal resulting from the motion of the camera is compared to a predetermined threshold to determine if the device is still shaking.

If it is determined at step S that the device is still shaking the process proceeds to back to step S. In step S unintended motion in the image of frame 3 is removed by applying a filter to the sequence of pixel displacements or to the accumulated pixel displacements as described above and a stabilized image for frame 3 is created by shifting the image to the position calculated by the filter this has been described in detail hereinabove. A timer implemented at the device not shown in is reset upon this operation.

If it is determined at step S that the device is not shaking the process proceeds to step S where a determination is made as to whether a period of time has elapsed since the timer was last reset.

If it is determined at step S that the period of time has not elapsed then the process proceeds back to stabilization step S where a stabilized image for frame 3 is created by shifting the image to the position calculated by the filter.

If it is determined at step S that the period of time has elapsed the process proceeds back to monitoring step S whereby no cropping of frame 3 is implemented.

It will be appreciated that using a timeout value defining the period of time referred to above at steps S and S improves the continuity of the transmitted video signal.

The predetermined threshold used at step S for turning on stabilization is calculated by comparing the change in acceleration to a threshold after filtering . The predetermined threshold used at step S for turning off stabilization is calculated by comparing the pixel motion vectors to a threshold of one pixel in each direction.

It will be appreciated that the thresholds and timeouts described hereinabove can be implementation specific depending on sensor data accuracy and the complexity of required resolution changes.

It will be appreciated that the process described hereinabove will continually loop to consider whether each of the frames captured by the camera require stabilization. Using the process described hereinabove with reference to ensures stabilization is enabled only when required i.e. cropping the video signal only when necessary thus maintaining maximum resolution of the video signal.

In the embodiments described above the motion sensor is a gyroscope which generates samples representing the rotational motion of the device . In other embodiments the motion sensor may sense other types of motion such as translational motion and generate samples representing the translational motion of the device . These samples can be used in the same way as described above in relation to the rotational motion to stabilize the video signal. However as described above with translational motion the pixel displacement will depend on the distance to the object in the image and so this must be taken into account when determining the pixel displacements. For example multiple accelerometers may be able to estimate rotational motion and in this case accelerometers can be used without further modification. For more general translational stabilization it may become more difficult to implement the method described herein since different areas in the image move by different amounts of pixels. However if the distance to the object is constant and known it may be simple to implement the method with translation motion. Even where the distance to the objects is not constant but is still known it would be possible to implement the method with translation motion but extra complication is added in determining the pixel displacements caused by the translation motion of the camera .

After stabilizing the video signal the video signal is encoded using a video encoding process. The encoded video signal may be transmitted e.g. as part of a video call to another user or as a broadcast signal. Therefore it is important for the video signal to be able to be stabilized and encoded in real time i.e. with very little delay for use in events such as video calls or other communication events where users are perceptually very aware of delay in the signals. Alternatively the encoded video signal could be stored at the device e.g. in the memory .

The method steps S S could be implemented at the device in software or in hardware. For example the CPU may execute processing blocks to implement the steps S S. For example a computer program product for stabilizing a video signal may be provided which can be stored in the memory and executed by the CPU . The computer program product may be configured so as when executed on the CPU to perform the method steps S S. Alternatively hardware blocks may be implemented in the device to implement the steps S S.

It should be understood that the block flow and network diagrams may include more or fewer elements be arranged differently or be represented differently. It should be understood that implementation may dictate the block flow and network diagrams and the number of block flow and network diagrams illustrating the execution of embodiments of the invention.

It should be understood that elements of the block flow and network diagrams described above may be implemented in software hardware or firmware. In addition the elements of the block flow and network diagrams described above may be combined or divided in any manner in software hardware or firmware. If implemented in software the software may be written in any language that can support the embodiments disclosed herein. The software may be stored on any form of computer readable medium such as random access memory RAM read only memory ROM compact disk read only memory CD ROM flash memory hard drive and so forth. In operation a general purpose or application specific processor loads and executes the software in a manner well understood in the art.

Furthermore while this invention has been particularly shown and described with reference to various embodiments it will be understood to those skilled in the art that various changes in form and detail may be made without departing from the scope of the invention as defined by the appended claims.

