---

title: Systems, methods, and interfaces for adaptive persistence
abstract: A storage module may be configured to service I/O requests according to different persistence levels. The persistence level of an I/O request may relate to the storage resource(s) used to service the I/O request, the configuration of the storage resource(s), the storage mode of the resources, and so on. In some embodiments, a persistence level may relate to a cache mode of an I/O request. I/O requests pertaining to temporary or disposable data may be serviced using an ephemeral cache mode. An ephemeral cache mode may comprise storing I/O request data in cache storage without writing the data through (or back) to primary storage. Ephemeral cache data may be transferred between hosts in response to virtual machine migration.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09058123&OS=09058123&RS=09058123
owner: Intelligent Intellectual Property Holdings 2 LLC
number: 09058123
owner_city: Wilmington
owner_country: US
publication_date: 20140425
---
This application is a continuation of U.S. patent application Ser. No. 13 829 835 entitled Systems Methods and Interfaces for Adaptive Persistence filed Mar. 14 2013 for Vikram Joshi et al. and which claims priority to U.S. Provisional Patent Application Ser. No. 61 696 126 entitled Systems Methods and Interfaces for Adaptive Persistence filed Aug. 31 2012 for Vikram Joshi et al. each of which is hereby incorporated by reference.

The disclosure relates to systems methods and interfaces for adaptive persistence and in several embodiments to systems methods and interfaces for adaptive persistence in virtual computing environments.

Input output I O requests may have different data persistence needs. For example some write requests may pertain to data that needs to be secured against data loss or corruption. Other write requests may pertain to data that does not need to be preserved for long periods of time and or is expected to be lost on restart and or reboot. A storage system may treat all requests alike which may reduce I O performance.

Disclosed herein are embodiments of a method for adaptive persistence. The disclosed methods may comprise one or more machine executable operations and or steps. The disclosed operations and or steps may be embodied as program code stored on a computer readable storage medium. Accordingly embodiments of the methods disclosed herein may be embodied as a computer program product comprising a computer readable storage medium storing computer usable program code executable to perform one or more method operations and or steps.

In some embodiments the disclosed method comprises identifying a plurality of I O requests selecting respective levels of persistence for the I O requests based on one or more properties of the I O requests and servicing the I O requests using one or more storage resources in accordance with the level of persistence selected for the I O requests. Identifying the I O request may comprise monitoring I O requests in a storage stack.

The level of persistence may correspond to one or more storage resources for use in servicing I O requests having the respective level of persistence a configuration of one or more storage resources used to service I O requests or the like. One or more of the levels of persistence may be configured to specify a storage mode. A persistence level may specify redundant storage on two or more storage resources and the method may further include servicing I O requests having the one persistence level using the two or more storage resources.

In some embodiments the method comprises selecting a first persistence level for a first I O request based on one or more of a file identifier associated with the first I O request and an application associated with the first I O request. One of the persistence levels may correspond to an ephemeral caching mode and the method may further comprise storing data of I O requests having the one persistence level in a cache without storing the data on a primary storage resource.

Disclosed herein are embodiments of a method comprising receiving an input output I O request originating from a storage client determining one of a plurality of persistence levels for the I O request based on one or more characteristics of the I O request wherein each of the persistence levels specifies one or more of a storage resource to use to service the I O request a configuration of the storage resource and a storage mode and or servicing the I O request according to the selected persistence level. A first one of the persistence levels may specify an ephemeral caching mode and a second one of the persistence levels specifies a write through caching mode. Two or more of the persistence levels differ with respect to storage resources used to service I O request storage configuration and or caching mode.

A first one of the persistence levels may specify an ephemeral cache mode and the method may further comprise acknowledging completion of write I O requests of the first persistence level in response to writing data of the write I O requests to a cache resource and without writing data of the write I O requests to primary storage. In some embodiments a first one of the persistence levels specifies a first error correction code encoding and a second one of the persistence levels specifies a second different error correction code encoding.

The method may further include determining a persistence level for an I O request based on one or more of a source identifier of the I O request a file identifier of the I O request an application associated with the I O request and a parameter of the I O request.

Disclosed herein are embodiments of an apparatus comprising a persistence level module configured to chose one of a plurality of adaptive persistence levels for each of a plurality of storage requests based on characteristics of the storage requests wherein the adaptive persistence level chosen for a storage request determines one or more of a storage resource a configuration of the storage resource and a storage mode for servicing the storage request an interface module configured to interface with one or more storage resources and a storage request execution module configured to service the storage requests according to the adaptive persistence levels assigned to the storage requests by use of the interface module. The persistence level module may be configured to choose an adaptive persistence level for a storage request based on one or more of a file identifier of the storage request an application associated with the storage request a parameter of the storage request and an input output IO control parameter. The storage request execution module may be configured to cache data of a storage request having an ephemeral adaptive persistence level in a write never cache mode. A first one of the adaptive persistence levels may comprise storing data in a first RAID configuration and a second one of the adaptive persistence levels comprises storing data in a second different RAID configuration.

The embodiments described herein relate to the management of data input output I O requests and operations in various computing environments including but not limited to virtualized computing environments bare metal computing environments and the like. Therefore although particular examples and or embodiments disclosed herein relate to virtualized computing environments the disclosure is not limited in this regard.

A storage module may be configured to manage I O operations for one or more storage clients which may include but are not limited to operating systems virtual operating systems hypervisors file systems volume managers database applications user applications or the like. The storage module may be configured to service I O requests for the storage clients. The storage module may be configured to service I O requests according to a selected level of persistence. As used herein a level of persistence or persistence level refers to one or more characteristics and or properties of an I O request. The level of persistence of an operation may pertain to the storage device s and or storage media to use to service the I O request e.g. volatile memory or non volatile storage media the configuration of the selected storage device s and or media e.g. redundant array of inexpensive disks RAID level just a bunch of disks JBOD configuration mirroring or the like the storage mode and or format for the I O request e.g. write through cache mode ephemeral cache mode ECC encoding or the like and so on. Different levels of persistence may therefore comprise storing data on different types of storage device s and or storage media such as volatile memory non volatile storage media e.g. magnetic optical tape solid state or the like or the like storing data in different storage configurations such as different RAID levels mirroring configurations parity configurations and so on and or storing data in one or more different modes such as different ECC encodings encryption levels caching modes atomicity verification and so on.

The storage module may be configured to select a level of persistence for I O requests according to properties of the I O requests which may include but are not limited to an explicit request for a particular level of persistence included and or associated with the I O request a request for particular storage characteristics characteristics and or properties of the I O request policy profiling testing and experience and the like. For example in response to an I O request pertaining to data that needs to be available for a long period of time the storage module may select a level of persistence that comprises storing the data on a non volatile storage medium in a RAID and or mirrored configuration. The selected level of persistence may further comprise caching data of the I O request in a write through cache configuration such that updates to the data are written through to the primary storage. In another example in response to an I O request pertaining to data that only needs to be retained for a limited time the storage module may select a level of persistence that comprises caching data of the I O request in an ephemeral or write never cache mode. As used herein an ephemeral cache mode refers to a cache configuration in which cached data is not written through and or written back to a primary storage ephemeral data may be stored only in the cache. As such caching data in an ephemeral cache configuration comprises storing data in the cache without accessing primary storage and or without writing and or copying the data to the primary store . Data cached in an ephemeral cache configuration may be lost if or when the data is evicted from the cache e.g. the cache client power cycles reboots or the like unless the data is transitioned to another level of persistence which may comprise caching the data in a different cache mode such as a write through and or write back cache mode.

The computing environment may comprise processing resources volatile memory resources persistent storage resources and or a communication interface . The processing resources may comprise one or more general and or special purpose processing elements and or cores. The processing resources may be configured to execute instructions loaded from the persistent storage resources . Portions of the modules and or methods disclosed herein may be embodied as machine readable instructions stored on the persistent storage resources . The resources and or may comprise physical computing components and or virtualized computing resources provided by and or through a virtualization environment e.g. a hypervisor .

The storage module may include an I O request receiver module persistence level module and I O request execution module . Portions of the storage module may operate on or in conjunction with the computing environment . Alternatively or in addition portions of the storage module may be implemented separately from the computing environment for example portions of the storage module may be connected using a system bus such as a peripheral component interconnect express PCI e bus a Serial Advanced Technology Attachment serial ATA bus universal serial bus USB connection an Institute of Electrical and Electronics Engineers IEEE 1394 bus FireWire an external PCI bus Infiniband communication network or the like.

The storage module may be communicatively coupled to one or more storage resources A N. The storage resources A N may comprise local storage resources accessible via respective interface mechanisms which may include but are not limited to bus interfaces and or protocols e.g. a system bus a local bus an I O bus or the like . The storage resources A N may further comprise one or more remote network accessible storage resources C which may include but are not limited to a Storage Area Network SAN resource network attached storage NAS or the like which may be accessible via a remote I O interface and or protocol .

The storage resources A N may include different types of storage device s and or storage media in various configurations. The storage resources A N may include but are not limited to volatile storage resources such as volatile memory DRAM processor cache and the like non volatile storage resources such as magnetic hard drives optical storage media solid state storage media and the like cache resources and so on. Accordingly the storage resources A N may include but are not limited to flash memory nano random access memory nano RAM or NRAM nanocrystal wire based memory silicon oxide based sub 10 nanometer process memory graphene memory Silicon Oxide Nitride Oxide Silicon SONOS Resistive Random Access Memory RRAM Programmable Metallization Cell PMC Conductive Bridging RAM CBRAM Magneto Resistive RAM MRAM Dynamic RAM DRAM Phase change RAM PRAM magnetic media e.g. one or more hard disks optical media or the like.

The storage resources A N may be configured in various storage configurations and or modes such as one or more different RAID levels mirroring configurations caching configurations and or modes ECC encodings parity configurations and the like. For example one or more of the storage resources A N may comprise a set of non volatile storage elements e.g. hard disks in a RAID and or mirroring configuration whereas other storage resources A N may be non redundant. The storage resources A N may be configured to store data according to different retention policies e.g. caching modes . For example the storage resource A may be configured to cache data for another primary storage resource B. The storage module may configure the cache resources to operate in particular caching modes and or configurations. For example the storage module may cache data of a first I O request in a write through configuration e.g. write the data to the cache storage resource A and the primary storage resource B and may cache data of a second I O request in an ephemeral cache mode in which the data is stored only in the cache storage resource A and is lost on eviction e.g. reboot .

The storage module may be configured to service I O requests from one or more storage clients. The I O request s may be received at the I O request receiver module via one or more interface s of the storage module which may include but are not limited to one or more drivers libraries modules block device interfaces interface extensions e.g. input output control IOCTL interfaces Application Programming Interfaces API application binary interfaces ABI object classes remote interfaces e.g. Remote Procedure Call Simple Object Access Protocol or the like and the like.

The storage module may further comprise a persistence level module configured to select a persistence level for the I O request . As disclosed above determining the persistence level may include but is not limited to selecting one or more storage resource s A N to service the I O request selecting a storage configuration of the one or more selected storage resource s A N e.g. RAID level mirroring or the like and or selecting a storage mode for the I O request e.g. caching mode ECC encoding or the like .

The persistence level module may determine the persistence level for I O requests based on a persistence level policy . The persistence level policy may comprise persistence level criteria configured to assign persistence levels to respective I O requests based on characteristics and or properties of the I O requests that may include but are not limited to file properties and or characteristics e.g. file level knowledge such as the file name path volume and or other file identifier corresponding to the I O request properties and or characteristics of the application and or storage client of the I O request e.g. application level knowledge inferences drawn from the I O requests one or more parameters of the I O request parameters associated with the I O request e.g. IOCTL information profiling metadata pertaining to the I O requests preferences and or configuration testing and experience and so on. For example in some embodiments one or more of the interfaces A and or B may comprise mechanisms for specifying persistence level characteristics for I O requests . For example the I O request may request storage in a redundant storage configuration e.g. a particular RAID level and or mirroring configuration . The I O request may further specify a caching mode for the I O request e.g. specify a write through cache mode . I O requests pertaining to temporary data may specify a different persistence level or different persistence level characteristics . For instance an I O request pertaining to temporary data may indicate that redundancy and or mirroring is not required and may allow caching in an ephemeral cache configuration disclosed in further detail herein . Alternatively or in addition the persistence level module may determine a persistence level based on persistence level criteria pertaining to other non explicit information pertaining to the I O request . For example persistence level criteria may identify temporary files that do not need to be retained between reboots e.g. by use of file selection criteria as described herein . The persistence level module assigns I O requests pertaining to such temporary files to an appropriate persistence level e.g. caching data of the I O requests in a an ephemeral cache .

The storage module may further comprise an I O request execution module configured to service I O requests . The I O request execution module may be configured to store data of I O requests on one or more storage resources A N in one or more storage configurations and or modes in accordance with the respective persistence levels of the I O requests as determined by the persistence level module . Accordingly the I O request execution module may be configured to store data on one or more different storage resources A N according to one or more different storage configurations and or in one or more different storage modes e.g. write through cache write back cache ephemeral cache or the like .

In some embodiments one or more of the storage clients A N are configured to issue I O requests directly to the storage module via one or more interfaces A. Alternatively or in addition the storage module may be configured to receive and or monitor I O requests within an I O stack of the computing environment . The I O stack may comprise a storage stack or other I O subsystem of an operating system A or virtual operating system A . Accordingly in some embodiments the I O request receiver module may comprise one or more agent s configured to monitor I O requests in the I O stack . The agent s include but are not limited to I O drivers I O filter drivers file filter drivers volume filter drivers disk filter drivers SCSI drivers and or filters virtual logical number VLUN drivers or the like.

The storage module may further comprise one or more I O interfaces A N configured to interface with one or more respective storage resources A N and or perform storage operations thereon. Each of the I O interfaces A N may be configured to interface with one or more storage resources A N by use of a particular interface mechanism A N which may include but is not limited to storage resource interfaces e.g. block device interfaces storage layers APIs protocols or the like bus protocols communication interface protocols network protocols and or interfaces virtualization protocols and or interfaces or the like. For example an I O interface B may be configured to interface with a virtualized storage resource B via a virtual I O interface B and an I O interface C may be configured to access a remote storage resource C via a network and or communication interface .

One or more of the storage resources A N may comprise a non volatile storage medium such as a solid state storage media. For example the I O interface A may be communicatively coupled to a solid state storage resource A. Therefore in some embodiments the I O interface A may comprise and or be communicatively coupled to one or more solid state storage controllers as described in U.S. patent application Ser. No. 11 952 091 filed Dec. 6 2007 entitled Apparatus System and Method for Managing Data Using a Data Pipeline and published as United States Patent Application Publication No. 2008 0141043 on Jun. 12 2008 which is hereby incorporated by reference.

The storage module may comprise a translation module configured to maintain mappings and or associations between logical identifiers and storage resources. As used herein a logical identifier refers to any identifier for referencing an I O resource e.g. data stored on the non volatile storage resources including but not limited to a logical block address LBA a cylinder head sector CHS address a file name an object identifier an inode a Universally Unique Identifier UUID a Globally Unique Identifier GUID a hash code a signature an index entry a range an extent or the like. The mappings may be any to any such that any logical identifier can be associated with any physical storage location and vice versa .

The translation module may be configured to maintain I O metadata pertaining to data of the I O requests and or the storage resources A N. The I O metadata may include but is not limited to a forward index e.g. an index of mappings and or associations between logical identifiers and storage resources A N cache tags validity metadata atomicity and or transactional metadata persistence level metadata and so on. For example persistence level metadata may indicate the persistence level of a particular logical identifier which may be used to service I O requests pertaining to the logical identifier. The persistence level metadata of an I O request may identify the storage resource s A N that comprise data pertaining to the I O request the storage configuration of the storage resources A N information pertaining to the storage mode of the data and so on. The I O metadata may comprise one or more data structures which may include but are not limited to a tree a B tree a range encoded B tree a radix tree a map a list a content addressable map CAM a table a hash table a memory layout e.g. contiguous memory layout or other suitable layout a combination of data structures or the like. The I O metadata may be maintained within the volatile memory resource . Portions of the I O metadata may be persisted to one or more non volatile and or non transitory storage resources such as the persistent storage resource and or one or more of the storage resources A N.

In some embodiments the storage module may leverage the arbitrary any to any mappings of the translation module to store data in a log format such that data is updated and or modified out of place on one or more of the storage resources A N. As used herein writing data out of place refers to modifying and or overwriting data to different media storage location s rather than overwriting the data in place e.g. overwriting the original physical location of the data . Storing data in a log format may result in obsolete and or invalid data remaining on the non volatile storage resources . For example overwriting data of logical identifier A out of place may result in writing data to new physical storage location s and updating the I O metadata to associate A with the new physical storage locations s e.g. in a forward index . The original physical storage location s associated with A are not overwritten and comprise invalid out of date data. Similarly when data of a logical identifier X is deleted or trimmed the physical storage locations s assigned to X may not be immediately erased but may remain on the non volatile storage resources as invalid data.

The storage module may further comprise a log storage module configured to store data on one or more of the storage resources A N in a log format e.g. an event log . As used herein a log format refers to a data storage format that defines and or preserves an ordered sequence of storage operations performed on the storage resources A N. Accordingly a log format may define an event log of storage operations performed on the storage resources A N. In some embodiments the log storage module is configured to store data sequentially from an append point. The log storage module may be further configured to associate data and or physical storage locations on the non volatile storage resources with respective sequence indicators. The sequence indicators may be applied to individual data segments packets and or physical storage locations on the storage resources A N and or may be applied to groups of data and or physical storage locations e.g. erase blocks . In some embodiments sequence indicators may be applied to physical storage locations when the storage locations are reclaimed e.g. erased in a grooming operation and or when the storage locations are first used to store data.

In some embodiments the log storage module may be configured to store data according to an append only paradigm. The storage module may maintain a current append point within a physical address space of one or more of the storage resources A N. As used herein an append point refers to a pointer or reference to a particular physical storage location e.g. sector page storage division offset or the like . The log storage module may be configured to append data sequentially from the append point. As data is stored at the append point the append point moves to a next available physical storage location of the storage resource A N. The log order of data stored on the storage resource A N may therefore be determined based upon the sequence indicator associated with the data and or the sequential order of the data on the storage resource A N. The log storage module may be configured to identify the next available storage location by traversing the physical address space of the storage resource A N e.g. in a reverse index as described below to identify a next available physical storage location.

The storage module may comprise a groomer configured to groom a non volatile storage resource non volatile storage media which may comprise reclaiming physical storage location s comprising invalid obsolete or trimmed data as described above. As used herein grooming a non volatile storage resource e.g. a solid state storage medium refers to operations that may include but are not limited to wear leveling removing invalid and or obsolete data removing deleted e.g. trimmed data refreshing and or relocating valid data reclaiming physical storage resources e.g. erase blocks identifying physical storage resources for reclamation and so on. The groomer may operate autonomously and in the background from servicing other I O requests . Accordingly grooming operations may be deferred while other I O requests are processed. Alternatively grooming may operate in the foreground with other I O requests . Reclaiming a physical storage location may comprise erasing invalid data from the physical storage location so that the physical storage location can be reused to store valid data. For example reclaiming a storage division e.g. an erase block or logical erase block may comprise relocating valid data from the storage division erasing the storage division and initializing the storage division for storage operations e.g. marking the storage division with a sequence indicator . The groomer may wear level the non volatile storage medium such that data is systematically spread throughout different physical storage locations which may improve performance and data reliability and avoid overuse and or underuse of particular physical storage locations. Embodiments of systems and methods for grooming non volatile storage media are disclosed in U.S. Pat. No. 8 074 011 issued Dec. 6 2011 and entitled Apparatus System and Method for Storage Space Recovery After Reaching a Read Count Limit which is hereby incorporated by reference.

In some embodiments the storage module may be configured to manage asymmetric write once non volatile storage resources such as solid state storage media. As used herein write once refers to a storage medium that is reinitialized e.g. erased each time new data is written or programmed thereon. As used herein asymmetric refers to storage media having different latencies and or execution times for different types of storage operations. For example read operations on asymmetric solid state non volatile storage resources may be much faster than write program operations and write program operations may be much faster than erase operations. The solid state non volatile storage resources may be partitioned into storage divisions that can be erased as a group e.g. erase blocks in order to inter alia account for these asymmetric properties. As such modifying a single data segment in place may require erasing an entire erase block and rewriting the modified data on the erase block along with the original unchanged data if any . This may result in inefficient write amplification which may cause excessive wear. Writing data out of place as described above may avoid these issues since the storage module can defer erasure of the obsolete data e.g. the physical storage location s comprising the obsolete data may be reclaimed in background grooming operations .

Further embodiments of systems methods and interfaces for managing I O metadata including mappings and or associations between logical identifiers and storage resources and or log storage are disclosed in U.S. patent application Ser. No. 12 986 117 filed on Jan. 6 2011 entitled Apparatus System and Method for a Virtual Storage Layer and published as United States Patent Application Publication No. 20120011340 on Jan. 12 2012 and U.S. patent application Ser. No. 13 424 333 filed on Mar. 19 2012 and entitled Logical Interface for Contextual Storage each of which is hereby incorporated by reference.

Step may comprise identifying and or accessing an I O request . Step may comprise receiving the I O request from a storage client A N e.g. via an interface A monitoring and or intercepting an I O request e.g. within an I O stack and or the like.

Step may comprise determining a persistence level for the I O request . Step may comprise the persistence level module determining a persistence level for the I O request based on a persistence level policy which may include but is not limited to persistence level criteria properties of the I O request inferences drawn from the I O request profiling metadata and so on.

Step may comprise servicing the I O request according to the persistence level selected and or assigned to the I O request at step . Servicing the I O request may comprise an I O request execution module performing one or more storage operations on the storage resources A N by use of the I O interface s A N . The persistence level of the I O request may determine the storage resource s A N that are to be used to service the I O request the configuration of the one or more storage resources A N e.g. RAID level mirroring and so on and or the mode of the storage operation s e.g. write through cache ephemeral cache ECC encoding and so on .

The storage module disclosed herein may be leveraged and or adapted to provide caching services. Accordingly in some embodiments the storage module may comprise a cache management system CMS . The CMS may comprise one or more of the modules of the storage module disclosed herein. For clarity however these modules may be referenced using cache specific terms when described in conjunction with the CMS.

The CMS may service I O requests of one or more storage clients . The storage clients may be local to the virtual machine A may be hosted within other virtual machines B N deployed on the host and or may operate on other computing devices e.g. on other hosts and or remote computing environments such as the remote storage client E of . The CMS may comprise an I O request receiver module configured to receive monitor and or intercept I O requests . The CMS may be configured to service I O requests according to a particular level of persistence. Accordingly the CMS may comprise a persistence level module configured to determine a persistence level for the I O requests as described herein which may comprise selecting a caching mode for the I O requests e.g. write through ephemeral or other caching configuration or mode .

The I O interface s may be configured to access one or more virtualized storage resources which may or may not be shared between other virtual machines A N on the host . As used herein a virtualized storage resource refers to a storage resource that is accessible through a virtualization kernel such as a hypervisor storage layer virtualization layer or the like. Virtualized storage resources may include but are not limited to VLUN storage resources virtual disks e.g. virtual machine disk format VMDK disks storage module s virtualized cache resources and the like.

The CMS may be configured to service I O requests by use of one or more virtualized storage resources A N including a virtual machine cache . The virtual machine cache may comprise a cache provisioner module and cache storage cache . The cache may include but is not limited to one or more memory devices such as non volatile storage devices and or media solid state storage Random Access Memory RAM or the like. As used herein a solid state memory device refers to a non volatile persistent memory that can be repeatedly erased and reprogrammed. Accordingly a solid state memory device may comprise a solid state storage device and or solid state storage drive SSD e.g. a Flash storage device . The cache provisioner module may be configured to provision resources of the cache to the CMS of the virtual machines A N which may comprise dynamically provisioning and or sharing cache storage and or I O operations IOPS . The cache provisioner module may be further configured to protect and or secure data stored within the cache to prevent more than one virtual machine A N from accessing the same cache data e.g. prevent read before write hazards . For example in some embodiments the cache provisioner module is configured to associate cached data with a virtual machine identifier which may be used to control access to data in the cache.

The cache may be shared between a plurality of virtual machines on a host. A cache chunk may be assigned or allocated to a particular one of the virtual machines based upon inter alia the cache needs of the virtual machine and or the cache needs of other virtual machines. The number of chunks assigned to a particular virtual machine can change over time as the cache needs of the virtual machine change. The number of chunks assigned to a specific virtual machine may determine the cache capacity of that virtual machine. For example if two 256 MB chunks are assigned to a specific virtual machine that virtual machine s cache capacity is 512 MB. The assignment of chunks to particular virtual machines is handled by the cache provisioner such as the cache provisioner module described above.

The CMS may comprise one or more cache tags to map and or associate identifiers I O addresses of a virtual machine A N to resources in the cache e.g. particular cache pages . The cache tags may therefore be used to perform translations between identifiers in the cache tags e.g. address of blocks on a primary storage resource B and a cache address. In some embodiments cache tags may be organized linearly in RAM or other memory. This allows the address of the cache tag to be used to locate a physical cache page because of the algorithmic assumption that each cache tag has a linear 1 1 correspondence with a physical cache page . Alternatively or in addition cache tags may be organized into another data structure such as a hash table tree or the like.

Referring back to cache tags associated with a particular virtual machine A N may be stored within that virtual machine A N. The cache tags contain metadata that associates storage I O addresses to specific cache pages in the cache. In a particular embodiment each cache tag is associated with a particular page in the cache. The virtual machine cache may cache data for one or more storage resources B N such as the primary storage resource B and or other storage resources C N . Accordingly one or more of the storage resources B N may be the primary storage and or backing store of data cached in the virtual machine cache . In some embodiments the I O address of the cache tags may be storage addresses and or references to one or more of the storage resources B N.

The cache tag data structure may further comprise a valid unit map field which is a dynamic field that identifies which units in a page are cached. An example of a unit within a cache page is a sector. For example a particular cache page may have one or more sectors that are missing or no longer valid. The valid unit map identifies the status of all units associated with a particular cache page to prevent accessing data in units that is not valid.

The cache tag data structure may further comprise a persistence metadata field. The persistence metadata field may comprise metadata pertaining to the persistence level of the cache tag which may include but is not limited to the cache mode for the cache tag e.g. write through write back ephemeral or other cache configuration or mode the primary storage resource s B N associated with the cache tag and so on. The persistence metadata may be determined by the persistence level module by use of the persistence level policy at the time the cache tag is admitted into the cache. The CMS and or persistence level module may modify the persistence metadata in response to changes to the persistence level of the cache tag . For example in some embodiments a cache tag associated with ephemeral cache data may be flushed to a primary storage resource B N which may comprise modifying a cache mode of the cache tag from ephemeral to another cache mode. The cache tag may revert to an ephemeral cache mode with a corresponding update to the persistence metadata . Further embodiments of cache tag data structures clock hands metadata and or cache tag state transitions are disclosed in U.S. patent application Ser. No. 13 028 149 entitled Systems and Methods for Managing I O Operations filed Feb. 15 2011 and published as United States Patent Application Publication No. 2012 0210043 on Aug. 16 2012 which is hereby incorporated by reference.

As described above the CMS may be configured to receive I O requests from one or more storage clients which may comprise receiving the I O requests via one or more interfaces A monitoring and or intercepting I O requests within a storage stack e.g. by use of an agent such as an I O filter a driver or the like . The I O requests may be routed to the CMS which may service the I O requests using the virtual machine cache .

In response to a request to read data e.g. a read I O request the CMS may determine whether data pertaining to the I O request is available in the virtual machine cache which may comprise determining whether the CMS comprises a cache tag that corresponds to the I O request e.g. whether the CMS comprises a cache tag having an identifier corresponding to an identifier of the read request . If a cache tag for the I O request is found the CMS may request the data from the virtual machine cache using the cache address associated with the cache tag . If a cache tag is not found the CMS may determine whether to admit the data into the virtual machine cache based on various cache admission policies and or other factors such as the availability of cache tags . The CMS may admit the data into the virtual machine cache by allocating a cache tag corresponding to the request accessing the data in the primary storage resource B and instructing the virtual machine cache to store the data in the cache . Admitting the data may further comprise determining a persistence level of the I O request using the persistence level module and or persistence level policy . The persistence level may specify a cache mode for data of the I O request . For example the persistence level of the I O request may specify that the data is to be stored in an ephemeral cache mode such that the data is written to the virtual machine cache and is not written through to the primary storage resource B.

In response to a request to write data e.g. a write I O request the CMS may determine whether data pertaining to the request has been admitted into the virtual machine cache as described above e.g. by determining whether there is a cache tag corresponding to the I O request . If no cache tag exists for the I O request the CMS may determine whether to admit the data into the virtual machine cache determine a persistence level of the I O request and so on as described above. If a cache tag exists the CMS may be configured to service the I O request according to a particular level of persistence as indicated by the persistence level module which may inter alia determine a cache mode for the data e.g. write through caching ephemeral caching or the like . Servicing a write I O request according to an ephemeral persistence level may comprise storing data of the I O request in the virtual machine cache without storing the data in a primary storage resource B. Servicing a write I O request according to a write through persistence level may comprise storing data of the I O request in both the virtual machine cache and one or more primary storage resources B. The I O request may not complete until data of the request is stored in the one or more primary storage resources B.

In some embodiments the virtual machines A N may be configured to be transferred and or relocated from the host to other host computing devices. The virtualization kernel or other virtualization layer may be configured to prevent virtual machines that reference local resources of the host such as local disk storage or the like from being transferred. Accordingly virtual machines A N may be configured to access the virtual machine cache using an access mechanism that does not prevent virtual machine migration. In some embodiments the CMS is configured to access the virtual machine cache through emulated shared storage and or a virtual disk or VLUN which the virtualization kernel treats as a shared device and or a device that does not prevent virtual machine migration . The virtual disk may be provided as a VMDK supported by the host and or virtualization kernel . In some embodiments the I O interface may comprise an I O filter that is configured to monitor I O operations of the virtual machines A N intercept I O requests and or operations directed to the virtual disk and forward the I O requests and other related data to the virtual machine cache via an interface . The I O filter may operate above an SCSI and or vSCSI of the virtual machine A N I O stack . The I O filter may provide for passing I O requests and responses between the CMS of the virtual machines A N and the virtual machine cache . The I O filter may further provide for communicating other data such as configuration command and or control data. The virtual disk used to communicate between the CMS and virtual machine cache may be very small e.g. a few megabytes since the virtual disk is not used for actual storage but as a communication interface between the CMS and virtual machine cache .

The virtual machines A N may be configured to emulate shared storage in other ways. For example in some embodiments the virtual machines A N may be configured to replicate one or more shared VLUN disks across a plurality of hosts such that to the hosts the VLUN disks appear to be shared devices. For instance the VLUN disks may share the same serial number or other identifier. The host and or the virtualization kernel may therefore treat the VLUN disks as shared devices and allow virtual machines A N to be transferred to and from the host . The VDMK approach described above may provide advantages over this approach however since a smaller number of shared disks need to be created which may prevent exhaustion of limited storage references e.g. a virtual machine may be limited to referencing storage devices .

The cache provisioner module may be configured to provision cache resources between the virtual machines A N. The allocation information associated with a particular virtual machine e.g. virtual machine A may be communicated to the corresponding CMS which may maintain cache tag metadata in accordance with the cache resources that are allocated to the CMS . The CMS may be configured to request cache resources from the cache provisioner module . The request may be transmitted in response to a CMS initialization operation e.g. cache warm up . As used herein cache initialization refers to one or more operations to prepare the CMS for operation. The CMS may be configured to perform one or more initialization operations in response to the virtual machine A N being powered on restarted transferred to a new host e.g. in a VMotion operation or the like.

The CMS may be configured to maintain cache metadata including cache tags in accordance with the cache storage that has been allocated to the virtual machine A N by the cache provisioner module . As used herein a cache tag refers to an association between an identifier and a cache resource e.g. a page or other cache storage location in the cache . Accordingly the cache tags may represent cache resources that have been allocated to a particular virtual machine A N by the cache provisioner module . As used herein an identifier of a cache tag refers to an identifier used by the virtual machine A N to reference data that has been or will be stored in the cache . A cache tag identifier may include but is not limited to a logical identifier an address e.g. a memory address physical storage address or logical block address such as an address on the primary storage system a name e.g. file name directory name volume name or the like a reference or the like.

The cache tags may be stored within the respective virtual machines A N e.g. in volatile memory allocated to the virtual machine A N by the host . In some embodiments the cache tags may represent a working set of cache data of the virtual machine A N. As used herein a working set of cache tags refers to a set of cache tags that has been admitted and or retained in the cache by the CMS through inter alia the application of one or more cache policies such as cache admission policies cache retention and or eviction policies e.g. cache aging metadata cache steal metadata least recently used LRU hotness and or coldness and so on cache profiling information file and or application level knowledge and the like. Accordingly the working set of cache tags may represent the set of cache data that provides optimal I O performance for the virtual machine A N under a particular set of operating conditions.

In some embodiments the CMS may be configured to preserve and or maintain the cache tags which may comprise persisting the cache tags in a non volatile storage medium such as the primary storage system persistent cache storage device e.g. cache or the like. As used herein a snapshot refers to the working set of the cache at a particular time. A snapshot may comprise all or a subset of the cache tags and or related cache metadata . In some embodiments a snapshot may further comprise pinning data in the cache device which may cause data referenced by the one or more cache tags to be retained in the cache . Alternatively the snapshot may reference only the data identifiers and may allow the underlying data to be removed e.g. evicted from the cache . The CMS may be configured to load a snapshot from persistent storage and to use the snapshot to populate the cache tags . A snapshot may be loaded as part of an initialization operation e.g. cache warmup and or in response to configuration and or user preference. For example the CMS may be configured to load different snapshots that are optimized for particular application s and or service s . Loading a snapshot may further comprise requesting cache storage from the cache provisioner module as described above. In some embodiments the CMS may load a subset of a snapshot if the virtual machine A N cannot allocate sufficient cache space for the full snapshot.

In some embodiments the cache provisioner module is configured to maintain mappings between the virtual machines A N and respective cache storage locations allocated to virtual machines A N e.g. as depicted in . The mappings may be used to secure cache data of the virtual machines A N e.g. by limiting access to the virtual machine A N mapped to the cached data and or to provide for retaining and or transferring cache data of one or more virtual machines A N transferred from the host to other remote hosts as described herein.

In some embodiments the virtual machine cache is configured to implement a thin provisioning approach to cache resource allocation. Each virtual machine A N may be allocated a particular number of chunks of the cache . However the entire cache capacity of the cache may be published to each of the virtual machines A N through a virtual disk such as a VLUN or the like . For example if the total cache size is 1 TB each virtual machine A N may report that it has access to the entire 1 TB of cache capacity. However the actual allocation of cache chunks may be considerably smaller e.g. 256 MB or 512 MB based on the current needs of the virtual machine A N. The allocated cache chunks represent a specific range of cache addresses available within the cache . The cache provisioner module dynamically changes these cache chunk allocations as the working set requirements of the virtual machines A N change and or virtual machines A N are transferred to from the host . Regardless of the number of cache chunks actually allocated to a particular virtual machine A N that virtual machine A N reports that it has access to the entire 1 TB cache. Accordingly the guest operating system of the virtual machine A N may operate with a virtual disk of size 1 TB. Accordingly the actual storage space allocated to the virtual machine A N can be changed dynamically without the guest operating system indicating an error condition. Therefore the cache page referenced by a cache tag may correspond to an indirect address within the cache .

The map module may be configured to map virtual cache storage allocations e.g. indirect cache addressees of the cache tags within the virtual machine A to physical cache resources e.g. cache chunks and or cache pages . In some embodiments the mapping may comprise an any to any index of associations between indirect cache addresses of the virtual machines A N and the physical address space of the cache .

In some embodiments the virtual machine cache may leverage the map module to secure data stored in the cache . For example the mappings of the map module may be used as a form of access control wherein access to physical cache chunks is restricted to the virtual machine to which the physical cache chunk is mapped and or allocated. For example the cache chunk labeled VM 1may only be accessible to the virtual machine to which the chunk is mapped e.g. virtual machine A . Moreover by virtue of the indirect accessing of the mapping layer the virtual machines A N may be incapable of directly referencing and or addressing physical cache chunks of other virtual machines A N.

The map module may be configured to map virtual cache storage using the VMID of the corresponding virtual machine. Accordingly when a virtual machine is transferred and or migrated between hosts mappings between the VMID and cache data of the virtual machine may remain valid e.g. given the VMID the retained cache data of the corresponding virtual machine A N may be identified and accessed . Further embodiments of systems and methods for dynamically allocating cache storage in a virtualized environment are disclosed in U.S. patent application Ser. No. 13 192 365 entitled Managing Data Input Output Operations filed on Jul. 27 2011 which is hereby incorporated by reference.

Referring to the cache provisioner module may be configured to dynamically allocate cache resources to the virtual machines A N. Once provisioned each virtual machine A N may have access to a predetermined and or contiguous range of cache storage resources e.g. cache chunks and or cache pages . The cache provisioner module may be configured to divide the cache into cache chunks that can be dynamically provisioned to respective virtual machines A N.

In some embodiments the virtual machines A N may be configured for use with disks having a fixed size and may operate improperly if there is a sudden atypical change to the size of a disk e.g. virtual disk . Accordingly the cache provisioner module may be configured to expose cache storage resources that appear to have a fixed size while providing for dynamic reallocation of the underlying cache resources. According to some embodiments and as depicted in the cache provisioner module may emulate fixed sized cache resources using a virtual disk driver which may be configured to expose respective virtual disks VLUN disks within the virtual machines A N. The cache tags may therefore reference cache resources in the virtual disk which may indirectly reference physical cache resources within the cache e.g. the cache tags may comprise indirect references to cache resources per a virtual disk mapping of the map module as described herein . The virtual disks may appear to have a fixed size comprising the full capacity of the cache e.g. 2 TB whereas only a portion of the cache is actually allocated to the virtual machine A N e.g. 4 GB per the cache provisioner module . Accordingly the cache provisioner module may be configured to dynamically allocate cache storage to virtual machines A N without adversely affecting the operation of the virtual machines A N. The virtual disk driver and or the map module may manage the mappings between indirect references of the virtual machines A N and physical cache resources.

The cache provisioner module may be configured to dynamically allocate different amounts of cache storage to the virtual machines A N in accordance with different cache requirements of the virtual machines A N. The cache provisioner module may comprise a map module configured to map virtual storage resources exposed to the virtual machines A N via a virtual disk to physical addresses in the cache .

As described above the virtual disk driver may be configured to present fixed sized contiguous cache storage allocations to the virtual machines A N through respective virtual disks . The map module may be configured to map references to the virtual disk to physical cache addresses e.g. cache chunks and or cache pages . For example the cache storage provisioned to the virtual machine A is illustrated diagrammatically as space in the cache . The cache space may comprise 4 GB. However the virtual disk driver may represent this limited cache capacity as a fixed TB virtual disk . Moreover the cache capacity cache chunks allocated to the virtual machines A N may be disbursed within the physical address space of the cache in an arbitrary manner the chunks may be discontiguous whereas the cache capacity represented through the virtual disk may be contiguous. The cache provisioner module may be configured to dynamically shift cache storage allocations between the virtual machines A N in response to changing cache requirements and or as virtual machines A N are transferred to and from the host .

The CMS may be configured to interface with the virtual machine cache through one or more I O interfaces . The CMS may comprise a SCSI filter of I O interface which may be configured to communicate data and or control information between the virtual machine A and the CMS operating therein and the virtual machine cache via the virtual disk . In some embodiments the SCSI filter may be incorporated into an I O stack or other I O infrastructure and or messaging system of the virtual machine A. The SCSI filter may be configured to identify the virtual disk respond to allocation changes within the virtual disk e.g. dynamic allocation by the cache provisioner module and so on. As described above the virtual disk may be configured to report a larger fixed storage capacity than the actual physical cache capacity allocated to the virtual machine A such that the cache provisioner module can dynamically provision cache storage without adversely affecting the virtual machine A. In some embodiments the SCSI filter may be configured to manage the actual physical capacity of the virtual disk which may be hidden from other applications and or operating systems of the virtual machine host . For example the VLUN disk may be presented as a read only storage device which may prevent other applications within the virtual machine A and or host from writing data to the virtual disk .

The cache provisioner module may report the actual physical cache storage allocated to the virtual machine A by way of a communication link comprising the SCSI filter . The communication link may operate separately from I O traffic between the virtual disk driver and the SCSI filter . Thus asynchronous out of band messages may be sent between the virtual disk driver and the SCSI filter . The SCSI filter may report allocation information and other command and or control information to the CMS which may use the allocation information to determine the number of cache tags available to the virtual machine A. Accordingly the cache may be thinly provisioned with respect to the virtual machines A N and cache allocations may vary dynamically in accordance with the cache requirements of the virtual machines A N.

The cache policy module may be configured to select data for admission into the cache based on various cache admission criteria and or policy. The cache policy module may be further configured to determine a persistence level for data admitted into the cache. The persistence level may inter alia determine a caching mode and or configuration for the data such as write through write back ephemeral or the like. In some embodiments the cache policy module comprises a persistence level module and persistence level policy for assigning persistence levels to cache data. Alternatively or in addition the persistence level of data to be admitted into the cache may be determined by another entity or process such as a storage module as described herein. The CMS may further comprise a cache execution module which may be configured to cache data according to a selected persistence level. The cache execution module may therefore be configured to cache data in one or more different cache modes and or configurations. For instance the cache execution module may be configured to cache data in a write through cache mode which may comprise servicing write I O requests by writing data to one or more primary storage resources B N and to the virtual machine cache . The CMS may not acknowledge completion of the write I O request until the data is written to the one or more primary storage resources B N. The cache execution module may be configured to cache data in an ephemeral cache mode which may comprise servicing write I O requests by writing data to the virtual machine cache without storing the data in a primary storage resource B N or other backing store. Accordingly the CMS may acknowledge completion of a write I O request assigned an ephemeral cache mode in response to writing the data to the virtual machine cache .

The translation module may be configured to correlate logical identifiers e.g. addresses in a primary storage system of the cache tags with cache storage locations e.g. cache addresses cache chunks cache pages or the like within the virtual machine cache . As described above the cache tags may correspond to indirect and or virtual cache storage resources within a virtual disk which may be mapped to physical cache storage resources e.g. cache chunks by the cache provisioner module and or map module .

The cache tag manager may be configured to manage the cache tags allocated to the CMS as described herein which may comprise maintaining associations between virtual machine identifiers e.g. logical identifiers addresses primary storage addresses and data in the cache and maintaining cache metadata such as access characteristics persistence level cache mode and the like.

The clock sweep module may be configured to determine and or maintain cache aging metadata using inter alia one or more clock hand sweep timers. The steal candidate module may be configured to identify cache data and or cache tags that are candidates for eviction based upon inter alia clock sweep metadata or other cache policy. In some embodiments the steal candidate module may be configured to predicate cache tag eviction reference on persistence level metadata of the cache tags . For example the steal candidate module may not select cache tags that are cached in an ephemeral cache mode for eviction. Alternatively the steal candidate module may first flush ephemeral cache tags to a primary storage resource B N before eviction.

The cache page management module may be configured to manage cache resources e.g. cache page data and related operations. The valid unit map module may be configured to identify valid data stored in the virtual machine cache and or a primary storage resource B. The page size management module may be configured to perform various page size analysis and adjustment operations to enhance cache performance as described herein. The interface module may be configured to provide one or more interfaces to allow other components devices and or systems to interact with the CMS .

The cache tag retention module may be configured to retain cache tags in response to transferring the CMS to a different host. As described above the cache tags may represent a working set of the cache which may be developed through the use of one or more cache admission and or eviction policies e.g. the clock sweep module and or steal candidate module and in response to the I O characteristics of the virtual machine and or the applications running on a virtual machine A N . The cache tag retention module may be configured to retain the cache tags after the virtual machine is transferred to a new host e.g. transferred from host A to host B in inter alia a VMotion operation despite the fact that the underlying cache data to which the cache tags refer may not be available on the cache storage device of the new host. The virtual machine cache described herein however may be configured to populate the cache at the new host such that the CMS can continue to use the working set of cache tags .

As described above data of the retained cache tags may be transferred to the new host from the previous host and or from primary storage resources B N or other source . The cache data may be transferred via a demand paging model which may comprise populating the cache on demand as the cache data of various retained cache tags is requested by the virtual machine . Alternatively or in addition cache data may be prefetched and or transferred in a bulk transfer operation which may comprise transferring cache data independent of requests for the cache tag data. In some embodiments data may be selectively prefetched based upon a cache transfer policy which may be based at least in part on the cache aging metadata of the clock sweep module and or steal candidate module and or other cache policy metadata e.g. hotness coldness least recently used or the like .

The cache tag snapshot module may be configured to maintain one or more snapshots of the working set of the cache e.g. the cache tags . As described above a snapshot refers to a set of cache tags at a particular time. The snapshot module may be configured to store a snapshot of the cache tags on a persistent storage medium and or load a stored snapshot as described above.

As described above the CMS may be configured to cache data according to one or more different levels of persistence which may correspond to caching data in one or more different cache modes and or on one or more different primary storage device s B. The CMS may utilize adaptive persistence levels to improve I O performance and or increase the consolidation ratio of the host . As used herein a consolidation ratio of a host refers to the number of virtual machines A N that can operate on the host . The number of virtual machines A N that can operate on the host may be limited by the computing resources of the host and or the I O overhead of the hosts .

In highly consolidated VDI environments the virtual machines A N may share common I O characteristics which may strain the I O infrastructure of the host . The virtual machines A N may share similar data sets e.g. operating systems applications user profile information or the like and the virtual machines A N may be configured to access this common data at similar times e.g. during boot up power on at log in time or the like . The resulting boot storms may overwhelm the primary storage resources B N and or I O infrastructure of the host which may significantly degrade overall system performance. Similar file access storms may occur in response to the virtual machines A N loading particular applications accessing shared data accessing user profile information executing a login process and so on. Moreover each of the virtual machines A N may comprise a respective CMS each of which may admit similar data sets into the shared virtual machine cache which may fill the virtual machine cache with duplicative data. Embodiments of systems and methods for improving performance of read centric I O operations are disclosed in U.S. patent application Ser. No. 13 750 904 entitled Systems and Methods for a De Duplication Cache filed Jan. 25 2013 and which is hereby incorporated by reference.

The performance of write centric I O operations may be improved by use of the adaptive persistence levels disclosed herein. For example a virtual machine A N may issue a large number of I O requests to write data that does not need to be preserved for a long period of time e.g. does not need to be retained between reboot cycles . Such data may be considered to be ephemeral e.g. temporary disposable and or dispensable data . As used herein a reboot cycle or restart operation comprises an intentional or unintentional restart and or reboot of the computing environment and or storage module which may be caused by inter alia a loss of power a hardware fault a software fault an intentional shutdown or restart or the like. Accordingly a restart operation may comprise a system reboot reset or shutdown event a power fault power loss or power failure event or another interruption of power. Ephemeral data may be of critical importance while the computing device e.g. virtual machine A N is operating but may not be needed after a reboot cycle and or restart event. Examples of such data include but are not limited to swap files such as virtual memory files e.g. pagefile.sys or the like temporary files such as the contents temporary directories e.g. tmp or the like temporary application files e.g. local cache of Microsoft Word or the like and so on. By contrast permanent data refers to data that should be retained between boot cycles and or restart events.

In some embodiments the CMS is configured to identify I O requests pertaining to dispensable data and may cache data of the I O requests in an ephemeral cache configuration. As disclosed above an ephemeral cache configuration refers to a cache mode in which data is written to and or read from cache storage but is not written through written back and or copied to a primary storage resource B N. Accordingly data stored in an ephemeral cache may be lost when the data is evicted from the cache and or if the cache is lost in a reboot crash or the like.

Storing data in an ephemeral cache mode and or configuration may provide significant performance benefits I O requests pertaining to ephemeral cache data do not require accesses to primary storage resources B N and the latency associated with primary storage resources B N may be removed from the critical path for servicing I O requests . As used herein the critical path refers to the timing and or latency path of an I O operation. As described above in write through cache configurations write I O requests may not complete e.g. return control to the storage client A N until data is written through to primary storage resources B N. Therefore the critical path of the I O operation comprises one or more high latency primary storage accesses. By contrast cache operations pertaining to ephemeral data may be completed exclusively within the cache without accessing primary storage resources B N. Accordingly the critical path of an ephemeral I O operation does not include high latency accesses to primary storage resources B N. Therefore in addition to reducing the latency for storage clients A N ephemeral caching may provide the additional benefit of reducing the I O bandwidth and or load on the primary storage resources B N.

In some embodiments the CMS may identify data for ephemeral caching based upon a persistence level of the I O request as determined by a cache policy module and or persistence level module which may determine a persistence level for an I O request according to a persistence level policy e.g. persistence level criteria which may be based on one or more of properties of the I O requests inferences drawn from the I O requests profiling metadata pertaining to the I O requests file level knowledge application level knowledge preferences and or configuration testing and experience and so on.

The I O request receiver module of the CMS may comprise an agent that is configured to monitor I O requests in an I O stack of the operating environment . The I O stack may comprise a plurality of layers A N including a file system layer A. The agent may be configured to monitor I O requests within any of the layers A N. The I O requests may relate to file operations such as file open close read write modify and the like. The agent may monitor other types of I O requests such as volume mount and or unmount disk mount and or unmount paging I O and so on. The agent may monitor I O requests using an interface provided by the computing environment the I O stack and or the like. Accordingly the agent may comprise one or more I O monitors file filter drivers volume filter drivers SCSI filter drivers and or other suitable monitoring and or filtering modules.

The CMS may comprise a cache policy module which may inter alia define cache admission criteria cache eviction criteria and so on. Accordingly the cache policy module may comprise a clock sweep module steal candidate module cache tag retention module and or a cache tag snapshot module as described herein. The CMS may use the cache policy module to make cache admission and or eviction decisions based upon cache access metadata admission criteria e.g. file selection criteria and so on.

In some embodiments the cache policy module may be further configured to determine and or assign a persistence level to I O requests . The persistence level of an I O request may determine inter alia a cache mode and or configuration for data admitted into the storage resource A. Accordingly the cache policy module may comprise a persistence level module configured to determine a persistence level of I O request data as described herein. In some embodiments determining and or assigning a persistence level to an I O request may comprise applying persistence level criteria to one or more characteristics of the I O request which may include file level criteria such as file name path directory volume or the like. depicts one embodiment of a persistence level policy . As depicted in the persistence level criteria pertain to file related characteristics of I O requests . The persistence level policy may map particular file names e.g. pagefile.sys file paths and or volumes to respective persistence levels . Files pertaining to temporary disposable data such as the pagefile.sys and or local cache data may be assigned a persistence level that specifies an ephemeral write never cache mode. Other potential characteristics of the persistence level are omitted to avoid obscuring the details of these embodiments.

The persistence level policy may specify other types of persistence levels. For example data of important files may be stored in a persistence level indicating a write through caching mode. The persistence level may further indicate that the data is to be stored in one or more redundant storage resources B N e.g. in a particular RAID level mirroring configuration or the like . In response the CMS may be configured to write through data of I O requests pertaining to files that satisfy the Application critical data file selection criteria to a redundant primary storage resource A N and or in a corresponding storage mode. Other storage services such as the storage module not shown in may leverage the persistence level policy to store data according to the persistence level s designated therein regardless of whether the particular I O request is serviced by the CMS .

The persistence level policy may incorporate other I O request characteristics in addition to and or in place of file related characteristics. For example the persistence level policy may identify and or designate applications known to make use of temporary dispensable write operations such that I O requests of such applications are cached according to a persistence level comprising an ephemeral cache configuration. For instance the persistence level criteria may indicate that I O requests originating from the Word application and directed to files in a local cache should be serviced in an ephemeral persistence level . The persistence level policy may further comprise default and or wildcard criteria such as a default write through cache mode persistence level .

The CMS may be configured to transparently extend an existing cache manager of the operating environment . Accordingly the CMS and or agent may be configured to distinguish different I O request types including but not limited to non paging I O requests paging I O requests direct I O requests and the like. The CMS and or agent may be configured to ignore certain types of I O requests such as direct I O requests that are expected to access primary storage resources B N directly and or non paging I O requests pertaining to the existing cache manager .

As disclosed herein the persistence level module assigns a persistence level to an I O request based at least in part on a target and or source of the I O request e.g. file name . In some embodiments the CMS may maintain I O request metadata which may include but is not limited to the source identifier of I O requests e.g. file name path or the like the persistence level of the I O requests and the like. The agent may generate context metadata in response to detecting an initial I O request pertaining to a file operation such as a file open read copy delete create truncate modify write or the like. The agent may use the I O request metadata e.g. context to associate the I O request and subsequent related I O requests with a corresponding source identifier such as file identifier file name volume identifier disk identifier or the like. In some embodiments the agent may include an indication of whether a particular file and or corresponding I O request is cacheable pertains to data that should be admitted into the cache and or the persistence level of the I O request . The CMS may access the I O request metadata to determine whether subsequent I O requests should be cached and or determine the persistence level of the I O requests without re applying cache admission criteria and or the persistence level policy . The I O request metadata may comprise context data of an I O request and or may be maintained in a separate datastructure. The I O request metadata may be stored in any suitable datastructure e.g. table hash table map or tree and may be indexed by source identifier file name or the like.

In one example a storage client A N may open a plurality of files resulting in corresponding file open I O requests . The agent may associate the I O requests with corresponding I O request metadata which may include a source identifier of the file an indication of whether the file should be admitted into the cache a persistence level of the I O requests and so on. The storage client A N may perform I O operations on the files which may be serviced using the existing cache manager and which may be ignored by the agent . In accordance with cache policy of the computing environment the existing cache manager may be flushed resulting in one or more paging I O requests which may be detected by the agent . In response the agent may access the I O request metadata e.g. context of the paging I O requests to determine the source identifier s of the requests determine whether the I O requests pertain to data to be admitted into the cache determine a persistence level of the I O requests and so on. I O requests that pertain to data that is to be admitted into the cache may be serviced using the CMS in accordance with an assigned persistence level as described herein.

As disclosed above the persistence level of the I O requests may be predicated on persistence level policy such as persistence level criteria . The persistence level policy may be configurable by a user or other entity such as a profiling process or the like . The CMS may comprise a configuration interface through which persistence level policy and or persistence level criteria may be specified.

In some embodiments the CMS may be configured to modify a persistence level of a cache tag during operation. For example the CMS may be configured to flush data stored in an ephemeral cache configuration to primary storage resource B N. As used herein flushing ephemeral cache data comprises storing the ephemeral cache data in a primary storage resource B N and or evicting the data from the cache. Ephemeral cache data may be evicted in response to an I O request pertaining to certain types of I O requests e.g. unhandled I O requests such as truncate transactional or the like . In response to such an I O request the CMS may be configured to flush the ephemeral cache data to primary storage resource s B N allow the operating environment to service the I O request and re admit the data into the cache in ephemeral cache mode and or configuration .

The CMS may be further configured to manage I O conflicts pertaining to ephemeral cache data. In some embodiments the CMS handles I O conflicts e.g. concurrent requests for access to a cache tag by invalidating the cache tag which may comprise evicting the data from the cache. The CMS may handle ephemeral cache data differently. Since ephemeral cache data is only stored within the cache and flushing the ephemeral cache data requires a high latency access to primary storage resources B N the CMS may be configured to schedule and or defer conflicting requests rather than invaliding the ephemeral cache tag .

Further embodiments of systems and methods for caching data according to I O requests in an I O stack are disclosed in U.S. patent application Ser. No. 13 287 998 entitled Systems and Methods for a File Level Cache filed Aug. 25 2012 and U.S. patent application Ser. No. 13 288 005 entitled Systems and Methods for a Multi Level Cache filed Aug. 25 2012 both of which are hereby incorporated by reference.

Data stored in a non ephemeral cache mode e.g. write through and or write back cache mode as determined by the cache policy module and or persistence level module may involve storing the data in both the cache storage resource A and or one or more primary storage resources B N. In response to a non ephemeral cache mode I O request the cache execution module may be configured to fork the request into a fast path operation to store the data in the cache storage resource A and a slow path operation to store the data in one or more primary storage resources B N. As used herein a fork refers to invoking two or more separate processing operations e.g. two or more processing threads . The CMS may not acknowledge completion of the I O request until the slow path operation is complete. Accordingly the slow path may determine the critical path for servicing non ephemeral I O requests . As used herein a critical path refers to a processing path that determines the timing and or latency for processing operations e.g. the processing steps and or path. The critical path for servicing non ephemeral I O requests may comprise the slow path since completion of such I O requests are not acknowledged until the corresponding data is stored on the one or more primary storage resources B N.

By contrast the cache execution module may be configured to service write I O requests pertaining to ephemeral data as determined by the cache policy module and or persistence level module exclusively in a fast path without involving slow path operations within the I O stack and or primary storage resources B N. Moreover servicing an I O request to write ephemeral cache data may not require the fork since no separate slow path processing is required. The CMS may acknowledge completion of ephemeral I O requests in response to completion of the fast path operation and without waiting for slow path operations to complete. Therefore ephemeral I O requests may reduce the critical path for servicing I O requests and reduce the load on the I O stack and or primary storage resources B N.

Step may comprise determining whether to admit data of the I O request into a cache such as the virtual machine cache . However the disclosure is not limited to virtual computing environments and could be applied to bare metal environments and or non virtualized cache resources. Step may comprise determining whether data of the I O request has already been admitted if so the I O request may be serviced according to persistence metadata associated with the I O request e.g. previously determined persistence level and or cache mode of the I O request as indicated in a cache tag of the I O request . If data of the I O request is to be admitted per cache policy and or availability the flow continues to step .

Step may comprise assigning an adaptive persistence level to the I O request which may comprise selecting a cache mode for the I O request e.g. ephemeral write through write back or the like . Assigning the persistence level may comprise determining whether the I O request pertains to temporary and or dispensable data as described above. Accordingly step may comprise comparing file level knowledge pertaining to the I O request to a persistence level criteria. For example the I O request may be assigned an ephemeral cache mode if the I O request pertains to a swap file temporary file local cache file or the like. Alternatively the I O request may be assigned a write through cache mode or other cache mode such as write back if the I O request pertains to permanent data. Step may further comprise updating a cache tag associated with the I O request to indicate the persistence level and or cache mode determined at step e.g. updating persistence metadata of the cache tag . If the persistence level determined at step indicates an ephemeral cache mode the flow continues to step otherwise the flow continues to step .

Step may comprise admitting data of the I O request into the cache in an ephemeral cache mode. Step may comprise storing data of the I O request in a cache e.g. the virtual machine cache but not storing the data in a primary storage resource B N. Accordingly step may comprise storing the data exclusively in the cache . In virtualized environments step may further comprise providing an indication to the virtual machine cache that the data is being cached in an ephemeral cache mode. As described below the virtual machine cache may use cache mode metadata to determine whether to retain the cache data after the virtual machine A N transfers to another host and or how long to retain the cache data . Since the virtual machine cache may be the only source for ephemeral cache data the virtual machine cache may be configured to retain the ephemeral cache data until the data is transferred to the new host of the virtual machine A N.

Step may comprise admitting data of the I O request according to the persistence level determined at step which may comprise storing the data in a shared virtual machine cache or other cache writing the data through and or back to one or more primary storage resources B N and so on.

Step may comprise determining a persistence level associated with the I O request which as described herein may determine the cache mode of the I O request . If the cache mode comprises an ephemeral cache mode the flow continues to step otherwise the flow continues to step .

Step may comprise servicing the I O request in an ephemeral cache mode. Accordingly step may comprise servicing the I O request in one or more fast path operations that do not involve access to primary storage resources B N and or other layers of the I O stack . Step may comprise acknowledging completion of the I O request in response to completion of the one or more fast path operation s . Accordingly the critical path for servicing the I O request only includes fast path operation s and excludes a fork for slow path operation s .

Step may comprise servicing the I O request according to another non ephemeral cache mode such as a write through cache mode. Step may comprise servicing the I O request in one or more slow path operations and one or more fast path operations e.g. forking the I O operations . The one or more slow path operations may comprise operations on one or more primary storage resources B N and or may involve one or more layers of the I O stack . Step may comprise acknowledging completion of the I O request in response to completion of the one or more slow path operations .

As disclosed above the CMS may be configured to maintain cache tags which may represent an optimal working set of the cache per the application of one or more cache policies such as cache admission policies cache retention and or eviction policies e.g. cache aging metadata cache steal metadata LRU hotness and or coldness and so on cache profiling information file and or application level knowledge and the like. Accordingly the working set may represent the set of cache data that provides optimal I O performance for the virtual machine A N under certain operating conditions. The working set may take considerable time to develop and or refine.

The CMS may be configured to retain the cache tags in response to relocating transferring and or migrating the virtual machine A N to another host e.g. in a VMotion operation . Retaining the cache tags may comprise maintaining the cache tags in the memory of the virtual machine A N and or not invalidating the cache tags during reallocation. Retaining the cache tags may further comprise requesting cache storage from the cache provisioner module of the new host and or selectively adding and or removing cache tags in response to being allocated a different amount of cache storage on the new host . In some embodiments the CMS may retain the cache tags despite the fact that the cache data referenced by the cache tags does not exist in the cache of the new host . As described below the virtual machine cache may be configured to populate the cache with cache data from a previous host of the virtual machine A N e.g. via a network transfer and or from primary storage. Certain cache data such as ephemeral cache data may only be available from the previous host .

Migrating a virtual machine A N may comprise reallocating cache resources on the new host re populating the cache and so on. The cache data may be transferred from the previous host of the virtual machine A N and or primary storage. However certain types of cache data such as ephemeral cache data may only be available on the virtual machine cache of the previous host A since the cache data is not written through and or written back to primary storage the data only exists in the cache . As described herein the virtual machine cache may be configured to transfer cache data between hosts such that the ephemeral cache data is not lost. Alternatively or in addition the CMS of a virtual machine A N may be configured to flush ephemeral cache data before being transferred and or migrated to a new host which may avoid data loss in the event the cache data is removed from the previous host and or the previous host experiences a failure condition .

The virtualized environment may comprise a primary storage system which may be shared among the hosts A N and or the virtual machines . The primary storage system may comprise any suitable persistent storage device and or storage system including but not limited to one or more magnetic disks e.g. hard drives a redundant array of inexpensive disks RAID a storage area network SAN or the like. The hosts A N may be configured to access the primary storage system via the network .

In some embodiments each virtual machine may be assigned a respective VMID. The VMID may be assigned when the virtual machine is instantiated e.g. loaded on a host A N e.g. during a handshake protocol described above . The VMID may comprise a process identifier thread identifier or any other suitable identifier. In some embodiments the VMID may uniquely identify the virtual machine on a particular host A N and or within a group of hosts A N. For example the hosts A N may operate within the same namespace such as a cluster and the VMID of each virtual machine may be unique within the namespace of the cluster unique across the virtual machines A N deployed on hosts A N in the cluster . In some embodiments the VMID may comprise a host identifier such as a Media Access Control MAC address network address distinguished name or the like. Accordingly in some embodiments a VMID may uniquely identify a virtual machine in a particular namespace and may identify the host A N upon which the virtual machine is current deployed or was previously deployed . Alternatively or in addition each virtual machine may be configured to maintain a current host identifier and a previous host identifier.

In some embodiments one or more of the virtual machines A N may be capable of being relocated and or transferred between the hosts A N. For example a virtual machine X may be migrated from the host A to the host B e.g. in a VMotion or similar operation . In some embodiments the CMS of the virtual machine X may be configured to detect a transfer and or migration operation and in response may attempt to flush ephemeral cache data to the primary storage system . As described above flushing ephemeral cache data may prevent data loss in the event the ephemeral cache data is unavailable from the previous host A. Alternatively the CMS may be configured to maintain the persistence level of ephemeral cache data to avoid migration storms e.g. avoid overloading the primary storage system and or I O infrastructure of the network .

The virtual machine cache B may be configured to identify the transfer in response to receiving a request from the CMS of the transferred virtual machine X. The request may comprise the VMID of the transferred virtual machine X from which the virtual machine cache B may determine that the virtual machine X is new to the host B e.g. requests comprising the VMID have not been received before . In response the virtual machine cache B may initiate a handshake protocol with the virtual machine X. The virtual machine cache B may determine that the virtual machine X was transferred to the host B based at least in part on a host identifier of the VMID and or host identifier s maintained by the virtual machine X. The host identifier of the virtual machine X may reference the host A whereas the host identifier of a newly powered on virtual machine may reference the host B or may be blank . Alternatively or in addition the virtual machine X may comprise a separate host identifier which may reference host A and may be accessed in the handshake protocol with the virtual machine cache B.

The cache provisioner module may be configured to allocate storage for the virtual machine X in the cache B. The cache provisioner module may be configured to determine how much cache storage to provision based at least in part upon the size of the cache storage allocated to the virtual machine X on the previous host host A . As disclosed above the CMS of the virtual machine X may be configured to retain the working set of the cache e.g. retain the cache tags after the transfer to host B. The cache provisioner module B may attempt to allocate sufficient cache storage in the cache B to support the retained cache tags . If sufficient cache storage cannot be allocated the CMS may be configured to selectively remove the retained cache tags in accordance with the new cache storage allocation on host B. Alternatively if excess cache storage is available the CMS may be configured to add new tags to the retained cache tags . The allocation may be reflected through a virtual disk as described herein.

The virtual machine cache A may comprise a retention module A which may be configured to retain cache data of the virtual machine X after the virtual machine X is transferred from the host A. The cache data may be retained for a retention period and or until the virtual machine cache A determines that the retained cache data is no longer needed. The retention module A may determine whether to retain the cache data and or determine the cache data retention period based upon various retention policy considerations including but not limited to availability of cache A availability of cache B relative importance of the retained cache data as compared to cache requirements of other virtual machines whether the cache data is backed up in the primary storage system a cache mode and or persistence level of the cache data and so on. For example cache data stored in an ephemeral cache mode may only be available on the original virtual machine cache A. Therefore the cache retention module A may be configured to prioritize retention of ephemeral cache data until the ephemeral cache data is transferred to the new host B. By contrast cache data stored in different cache modes e.g. write through and or write back cache mode may have a lower retention priority since this data will be available from the primary storage system .

The CMS of the virtual machine X may be configured to retain the working state of the cache the cache tags despite the fact that the cache B does not comprise the cache data to which the cache tags refer. As disclosed below the virtual machine cache B may be configured to populate the cache B with cache data transferred from the cache A of host A and or the primary storage system to reconstruct the working set of the transferred virtual machine X.

The virtual machine cache B may comprise a cache transfer module B which may be configured to access cache data of the virtual machine X stored at the previous host A. The cache transfer module B may be configured to identify the previous host A by use of the VMID and or by interrogating the virtual machine X e.g. accessing a previous host identifier maintained by the virtual machine X . The cache transfer module B may use the host identifier to issue one or more requests for the cache data to the virtual machine cache of the host A via the network . In some embodiments the cache transfer module B is configured to determine and or derive a network address or network identifier of the host A from the host identifier.

The virtual machine cache A may comprise a cache transfer module A that is configured to selectively provide access to retained cache data of the transferred virtual machine X. In some embodiments the cache transfer module A is configured to secure the retained cache data. For example the cache transfer module A may be configured to verify that the requesting entity e.g. the virtual machine cache B is authorized to access the cache data of the transferred virtual machine X which may comprise verifying that the virtual machine X is deployed on the host B. For example the cache transfer module A may request a credential associated with the transferred virtual machine X such as the VMID or the like. Alternatively or in addition the cache transfer module A may implement a cryptographic verification which may comprise verifying a signature generated by the transferred virtual machine X or the like.

The cache transfer module B may be configured to transfer the cache data by one or more demand paging transfers prefetch transfers and or bulk transfers. A demand paging transfer may comprise transferring cache data in response to I O requests for the cache data from the virtual machine X e.g. on demand . The transferred data may be used to service the I O requests . In addition the transferred data may be admitted into the cache B of the new host B. Alternatively the transferred data may be admitted at a later time not not at all in accordance with cache policy.

A prefetch transfer may comprise transferring data according to a prefetch cache policy e.g. by proximity or the like and or persistence level of the cache data. The amount and or extent of cache data to prefetch may be determined by inter alia cache metadata of the CMS e.g. cache aging metadata hotness and so on . Accordingly in some embodiments the cache transfer module B may be configured to query the CMS to identify the cache data to prefetch if any .

A bulk transfer may comprise transferring cache data in bulk independent of storage requests from the virtual machine X. A bulk transfer may comprise transferring populating the entire cache storage allocated to the virtual machine X. Alternatively a bulk transfer may comprise populating a subset of the cache which as disclosed above may be selected based upon cache metadata of the virtual machine CMS .

The cache transfer module B may be further configured to prioritize cache transfers e.g. prefetch and or bulk transfers in accordance with the persistence level of the cache data. For example data that is cached in an ephemeral cache mode may only be available from the previous host A and as such may be prioritized over other cache data that may be available from alternative sources e.g. primary storage system . Therefore the cache transfer module B may be configured to prefetch and or bulk transfer ephemeral cache data rather than waiting for on demand paging and or transferring other data.

The cache storage module B may be configured to store cache data transferred from the cache A of the host A or acquired from other sources such as the primary storage system in the cache B. The cache storage module B may be configured to store the cache data at cache storage locations that have been allocated to the transferred virtual machine X by the cache provisioner module B. The cache data may be stored at the same cache storage location e.g. same offset with the cache storage as in the original cache A such that the references in the retained cache tags remain valid per the mappings implemented by the map module of the cache provisioner module .

In response to requests for cache data the cache transfer module A may be configured to identify the requested cache data using inter alia the VMID of the transferred virtual machine X by use of the map module . The cache transfer module A may then transfer the requested cache data if available to the cache transfer module B via the network .

The cache transfer module B may be configured to access cache data from the previous host A. Data cached in an ephemeral cache mode may only be accessible from the previous host A. Data cached in other cache modes may be available from other sources such as the primary storage system and or other sources of the data e.g. other persistent storage systems hosts N or the like . The cache transfer module B may select the source of the cache data based upon various policy considerations e.g. a cache transfer policy which may include a network policy bandwidth policy host resource policy primary storage resource policy and the like. For example in response to determining that the network is highly congested the cache transfer module B may be configured to reduce the amount of data to transfer defer a bulk transfer and or transfer the cache data from another source that is independent of the network . Similarly the cache transfer module B may direct requests to the host as opposed to the primary storage system in response to determining that the primary storage system is heavily loaded and or has limited available bandwidth.

The cache data retained on the host A may represent cache resources that cannot be used by the other virtual machines A N operating on the host A. As such the cache retention module A may be configured to selectively remove the retained cache data when the data is no longer needed and or according to a retention policy. The retention policy may be determined based upon the retention policy factors described above. In some embodiments the cache transfer module B is configured to inform the previous host A of cache data that has been transferred to the host B from other sources so that the cache retention module A can remove the corresponding data from the cache A. The cache transfer module B may be further configured to inform the host A of other conditions in which the cache data no longer needs to be retained such as when the cache data is overwritten deleted e.g. trimmed evicted from the cache B or the like. As disclosed above the cache retention module A may be configured to prioritize cache retention based on the persistence level of the cache data such that the retention of ephemeral cache data is prioritized over cache data available from other sources.

In some embodiments the cache transfer module A may be configured to push cache data of the virtual machine X to the new host B. Pushing cache data may comprise transferring retained cache data of the virtual machine X to the cache transfer module B without receiving a request for the cache data independent of requests for the cache data . The cache transfer module A may determine the host identifier of the new host B through user configuration the verification process described above active polling by the cache transfer module A a call back implemented by the transferred virtual machine X or the like. In some embodiments the virtual machine cache of the new host B may identify that the virtual machine X was transferred from the host A in response to receiving cache data pushed from the host A as described above. The cache transfer module A may be configured to selectively push high priority cache data such as ephemeral cache data to prevent data loss.

Step may comprise a cache provisioner module B allocating cache storage to a virtual machine X on a host B in response to the virtual machine X migrating to the new host B from a previous host A. The new host B and the previous host A may be communicatively coupled e.g. via a network . As described above the cache provisioner module B may identify the transferred virtual machine X in response to receiving an I O request from the CMS of the virtual machine X or other query receiving cache data of the virtual machine X from the cache transfer module A of the previous host A or the like. Step may further comprise distinguishing the transferred virtual machine X from an initial power on and or restart condition e.g. based on the VMID of the virtual machine X a host identifier or the like . The cache may be dynamically allocated to the virtual machine X in a virtual disk via a virtual disk driver as described above.

Step may comprise the cache transfer module B of the new host B determining the previous host A of the transferred virtual machine X. Step may comprise accessing a host identifier in the VMID of the virtual machine X querying the transferred virtual machine X receiving pushed cache data from the previous host B or the like as described above.

Step may comprise populating at least a portion of the cache storage allocated to the virtual machine X with cache data retained at the remote host A as described above. The cache data may correspond to cache data stored in an ephemeral cache configuration that is only available at the previous host . Step may comprise requesting the cache data verifying that the new host B is authorized to access the cache data receiving the cache data in a push operation or the like. Cache data may be requested transferred and or pushed according to a cache transfer policy of the cache transfer modules A and or B as described above. In some embodiments step further comprises populating the allocated cache storage with data accessed from primary storage or another source . Step may further comprise informing the remote host A that the retained cache data of the virtual machine X no longer needs to be retained in response to populating the cache on the new host B as described above.

Step may comprise retaining cache data of a virtual machine X in a cache cache storage device in response to transferring and or migrating the virtual machine X off of the host A. The cache data may be retained by a cache retention module A in accordance with a retention policy as described above. In some embodiments step may comprise prioritizing retention of ephemeral cache data which may not be available on primary storage system e.g. may only be available within the virtual machine cache A of the previous host A .

Step may comprise determining a cache address of the retained cache data in response to a request for the cache data. The cache address may be based at least in part on a VMID of the transferred virtual machine. The cache address of the data may be determined by a map module configured to associate cache resources e.g. cache chunks with the virtual machines A N to which the resources are allocated.

Step may comprise providing retained cache data as described above. Step may comprise responding to requests for the cache data from a cache transfer module B of the new host B of the virtual machine X pushing the cache data to the cache transfer module B or the like.

Step may comprise filtering I O operations and directing selected I O requests to a CMS in accordance with the retained cache tags . Step may comprise requesting data of one or more cache tags that have not yet been transferred to the new host B of the virtual machine X and or have not been allocated to the virtual machine X. As described above in response to such a request the virtual machine cache B of the new host B may identify the virtual machine X as a transferred virtual machine e.g. as opposed to an initial boot up or power on allocate cache storage for the virtual machine X determine the previous host A of the virtual machine X and or transfer cache data from the previous host A via the cache transfer module B .

Step may comprise requesting data of a retained cache tag as described above. Requesting access may comprise performing an on demand transfer of cache data from the previous host A to the virtual machine cache B of the new host B.

Reference throughout this specification to features advantages or similar language does not imply that all of the features and advantages that may be realized are included in any single embodiment. Rather language referring to the features and advantages is understood to mean that a specific feature advantage or characteristic described in connection with an embodiment is included in at least one embodiment. Thus discussion of the features and advantages and similar language throughout this specification may but do not necessarily refer to the same embodiment.

Furthermore the features advantages and characteristics described herein may be combined in any suitable manner in one or more embodiments. One skilled in the relevant art will recognize that the disclosed embodiments may be practiced without one or more of the specific features or advantages of a particular embodiment. In other instances additional features and advantages may be recognized in certain embodiments that may not be present in all embodiments. These features and advantages of the disclosed embodiments will become more fully apparent from the following description and appended claims or may be learned by the practice of the embodiments as set forth hereinafter.

Many of the functional units described in this specification have been labeled as modules in order to more particularly emphasize their implementation independence. For example a module may be implemented as a hardware circuit comprising custom VLSI circuits or gate arrays off the shelf semiconductors such as logic chips transistors or other discrete components. A module may also be implemented in programmable hardware devices such as field programmable gate arrays programmable array logic devices programmable logic devices or the like.

Modules may also be implemented in software for execution by various types of processors. An identified module of executable code may for instance comprise one or more physical or logical blocks of computer instructions that may for instance be organized as an object procedure or function. Nevertheless the executables of an identified module need not be physically located together but may comprise disparate instructions stored in different locations that when joined logically together comprise the module and achieve the stated purpose for the module.

Indeed a module of executable code may be a single instruction or many instructions and may even be distributed over several different code segments among different programs and across several memory devices. Similarly operational data may be identified and illustrated herein within modules and may be embodied in any suitable form and organized within any suitable type of data structure. The operational data may be collected as a single data set or may be distributed over different locations including over different storage devices and may exist at least partially merely as electronic signals on a system or network. Where a module or portions of a module are implemented in software the software portions are stored on one or more computer readable media.

Reference throughout this specification to one embodiment an embodiment or similar language means that a particular feature structure or characteristic described in connection with the embodiment is included in at least one embodiment. Thus appearances of the phrases in one embodiment in an embodiment and similar language throughout this specification may but do not necessarily all refer to the same embodiment.

Reference to a computer readable medium may take any form capable of storing machine readable instructions on a digital processing apparatus. A computer readable medium may be embodied by a compact disk digital video disk a magnetic tape a Bernoulli drive a magnetic disk a punch card flash memory integrated circuits or other digital processing apparatus memory device.

Furthermore the features structures or characteristics disclosed herein may be combined in any suitable manner in one or more embodiments. In the following description numerous specific details are provided such as examples of programming software modules user selections network transactions database queries database structures hardware modules hardware circuits and hardware chips to provide a thorough understanding of the disclosed embodiments. One skilled in the relevant art will recognize however that the teachings of the disclosure may be practiced without one or more of the specific details or with other methods components materials and so forth. In other instances well known structures materials or operations are not shown or described in detail to avoid obscuring aspects of the disclosed embodiments.

The schematic flow chart diagrams included herein are generally set forth as logical flow chart diagrams. As such the depicted order and labeled steps are indicative of one embodiment of the presented method. Other steps and methods may be conceived that are equivalent in function logic or effect to one or more steps or portions thereof of the illustrated method. Additionally the format and symbols employed are provided to explain the logical steps of the method and are understood not to limit the scope of the method. Although various arrow types and line types may be employed in the flow chart diagrams they are understood not to limit the scope of the corresponding method. Indeed some arrows or other connectors may be used to indicate only the logical flow of the method. For instance an arrow may indicate a waiting or monitoring period of unspecified duration between enumerated steps of the depicted method. Additionally the order in which a particular method occurs may or may not strictly adhere to the order of the corresponding steps shown.

