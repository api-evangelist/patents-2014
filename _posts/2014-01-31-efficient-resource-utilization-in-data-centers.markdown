---

title: Efficient resource utilization in data centers
abstract: A method includes identifying high-availability jobs and low-availability jobs that demand usage of resources of a distributed system. The method includes determining a first quota of the resources available to low-availability jobs as a quantity of the resources available during normal operations, and determining a second quota of the resources available to high-availability jobs as a quantity of the resources available during normal operations minus a quantity of the resources lost due to a tolerated event. The method includes executing the jobs on the distributed system and constraining a total usage of the resources by both the high-availability jobs and the low-availability jobs to the quantity of the resources available during normal operations.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09213576&OS=09213576&RS=09213576
owner: Google Inc.
number: 09213576
owner_city: Mountain View
owner_country: US
publication_date: 20140131
---
A distributed system generally includes many loosely coupled computers each of which typically includes a computing resource e.g. one or more processors and or storage resources e.g. memory flash memory and or disks . A distributed storage system overlays a storage abstraction e.g. key value store or file system on the storage resources of a distributed system. In the distributed storage system a server process running on one computer can export that computer s resources to client processes running on other computers. Remote procedure calls RPC may transfer data from server processes to client processes. Alternatively Remote Direct Memory Access RDMA primitives may be used to transfer data from server hardware to client processes.

One aspect of the disclosure provides a method for efficiently using resources e.g. processors and or memory devices in data centers. The method includes identifying high availability jobs and low availability jobs that demand usage of resources of a distributed system and determining a first quota of the resources available to low availability jobs as a quantity of the resources available during normal operations. The method also includes determining a second quota of the resources available to high availability jobs as a quantity of the resources available during normal operations minus a quantity of the resources lost due to a tolerated event. The method includes executing the jobs on the resources of the distributed system and constraining a total usage of the resources by both the high availability jobs and the low availability jobs to the quantity of the resources available during normal operations.

Implementations of the disclosure may include one or more of the following features. In some implementations the resources include data processing devices networking systems power systems or cooling systems. For these types of resources the method may include migrating or re executing jobs assigned to resources lost due to the tolerated event to remaining resources.

In some implementations the resources include non transitory memory devices also referred to as storage resources . For this type of resource the method may include leaving jobs assigned to the lost resources without reassigning the jobs to remaining resources and reconstructing any unavailable data associated with the lost resources. Storage resources have two types of usage a byte usage storage capacity and an access bandwidth such as the number of input output operations per second allowed to hard disk storage or the amount of spindle utilization access allowed for such storage . The allocation of jobs to storage resources may depend on its type of usage. In some examples up to 100 of the normal byte usage storage capacity may be assigned to high availability jobs whereas a fraction of the normal bandwidth spindle usage may be assigned to high availability jobs.

The method may include determining the second quota of the resource available to high availability jobs as the quantity of the resources available during normal operations minus the quantity of the resources lost due to a tolerated event minus an increased quantity of the remaining resources needed due to the tolerated event. Additionally or alternatively the method may include limiting a sum of the first quota and the second quota to a maximum quota.

In some implementations the method includes monitoring a usage of the resources by the high availability jobs. When a high availability job exceeds a threshold usage the method includes downgrading the high availability job to a low availability job. The method may include lowering the first quota of the resources available to low availability jobs for a period of time before the tolerated event and increasing the second quota of the resources available to high availability jobs for the period of time before the tolerated event. Additionally or alternatively the method may include suspending or ending at least some of the low availability jobs for the period of time before the tolerated event.

In some examples the method includes determining the quantity of the resources lost due to a tolerated event based on an assignment of the jobs to particular resources and a system hierarchy of the distributed system. The system hierarchy includes system domains. Each system domain has an active state or an inactive state. The system hierarchy may include system levels such as first second third and fourth system levels. The first system level corresponds to host machines of data processing devices non transitory memory devices or network interface controllers. Each host machine has a system domain. The second system level corresponds to power deliverers communication deliverers or cooling deliverers of racks housing the host machines. Each power deliverer communication deliverer or cooling deliverer of the rack has a system domain. The third system level corresponds to power deliverers communication deliverers or cooling deliverers of cells having a system domain. Each power deliverer communication deliverer or cooling deliverer of the cell has a system domain. The fourth system level corresponds to a distribution center module of the cells each distribution center module having a system domain.

Another aspect of the disclosure provides a system for efficiently utilizing resources of a distributed system. The system includes resources of a distributed system and a computer processor in communication with the resources. The computer processor identifies high availability jobs and low availability jobs that demand usage of the resources and determines a first quota of the resources available to low availability jobs as a quantity of the resources available during normal operations. In addition the computer processor determines a second quota of the resources available to high availability jobs as a quantity of the resources available during normal operations minus a quantity of the resources lost due to a tolerated event. The processor executes the jobs on the distributed system and constrains a total usage of the resources by both the high availability jobs and the low availability jobs to the quantity of the resources available during normal operations.

In some implementations the system resources include data processing devices networking systems power systems or cooling systems. For these types of resources the computer processor may migrate or re execute jobs assigned to resources lost due to the tolerated event to remaining resources.

In some implementations the system resources include non transitory memory devices. For this type of resource the computer processor may leave jobs assigned to the lost resources without reassigning the jobs to remaining resources and reconstructs any unavailable data associated with the lost resources.

The computer processor may determine the second quota of the resources available to high availability jobs as the quantity of the resources available during normal operations minus the quantity of the resources lost due to a tolerated event minus an increased quantity of the remaining resources needed due to the tolerated event. Additionally or alternatively the computer processor may limit a sum of the first quota and the second quota to a maximum quota.

In some implementations the computer processor monitors a usage of the resources by the high availability jobs. When a high availability job exceeds a threshold usage the computer processor downgrades the high availability job to a low availability job. The computer processor may further lower the first quota of the resources available to low availability jobs for a period of time before the tolerated event and increase the second quota of the resources available to high availability jobs for the period of time before the tolerated event. Additionally or alternatively the computer processor may suspend or end at least some of the low availability jobs for the period of time before the tolerated event.

In some examples the computer processor determines the quantity of the resources lost due to a tolerated event based on an assignment of the jobs to particular resources and a system hierarchy of the distributed system. The system hierarchy includes system domains. Each system domain has an active state or an inactive state. The system hierarchy may include system levels such as first second third and fourth system levels. The first system level corresponds to host machines of data processing devices non transitory memory devices or network interface controllers. Each host machine has a system domain. The second system level corresponds to power deliverers communication deliverers or cooling deliverers of racks housing the host machines. Each power deliverer communication deliverer or cooling deliverer of the rack has a system domain. The third system level corresponds to power deliverers communication deliverers or cooling deliverers of cells having associated racks. Each power deliverer communication deliverer or cooling deliverer of the cell has a system domain. The fourth system level corresponds to a distribution center module of the cells each distribution center module having a system domain.

Another aspect of the disclosure provides a method that includes identifying high availability jobs and low availability jobs that demand usage of resources of a distributed system determining a first quota of the resources available to low availability jobs as a quantity of the resources available during normal operations and determining a second quota of the resources available to high availability jobs based on a resource type. For storage capacity resources the second quota of the resources available to high availability jobs is the quantity of the resources available during normal operations. For storage bandwidth resources the second quota of the resources available to high availability jobs is the quantity of the resources available during normal operations minus a quantity of the resources lost due to a tolerated event and minus an increased quantity of the remaining resources needed due to the tolerated event. For other i.e. non storage resources the second quota of the resources available to high availability jobs is the quantity of the resources available during normal operations minus the quantity of the resources lost due to a tolerated event. The method further includes executing the jobs on the resources of the distributed system and constraining a total usage of the resources by both the high availability jobs and the low availability jobs to the quantity of the resources available during normal operations.

The details of one or more implementations of the disclosure are set forth in the accompanying drawings and the description below. Other aspects features and advantages will be apparent from the description and drawings and from the claims.

Referring to in some implementations a distributed system includes loosely coupled resource hosts e.g. computers or servers each having a computing resource e.g. one or more processors or central processing units CPUs in communication with storage resources e.g. memory flash memory dynamic random access memory DRAM phase change memory PCM and or disks that may be used for caching data. A storage abstraction e.g. key value store or file system overlain on the storage resources allows scalable use of the storage resources by one or more clients . The clients may communicate with the resource hosts through a network e.g. via RPC .

The distributed system may include multiple layers of redundancy where data is replicated and stored in multiple data centers. Data centers not shown house computer systems and their associated components such as telecommunications and distributed systems . Data centers usually include backup power supplies redundant communications connections environmental controls to maintain a constant temperature and security devices. Data centers can be large industrial scale operations that use a great amount of electricity e.g. as much as a small town . Data may be located in different geographical locations e.g. different cities different countries and different continents . In some examples the data centers or a portion thereof requires maintenance e.g. due to a power outage or disconnecting a portion of the system for replacing parts or a system failure or a combination thereof . The data stored in these data centers and in particular the distributed system may be unavailable to users clients during the maintenance period resulting in the impairment or halt of a user s operations. Therefore it is desirable to provide a distributed system capable of efficiently using the resource hosts the processors storage resources and the network resources of the system during maintenance and or failure of the system or portions thereof.

In some implementations the distributed system is single sided eliminating the need for any server jobs for responding to remote procedure calls RPC from clients to store or retrieve data on their corresponding resource hosts and may rely on specialized hardware to process remote requests instead. Single sided refers to the method by which most of the request processing on the resource hosts may be done in hardware rather than by software executed on CPUs of the resource hosts . Rather than having a processor of a resource host e.g. a server execute a server process that exports access of the corresponding storage resource e.g. non transitory memory to client processes executing on the clients the clients may directly access the storage resource through a network interface controller NIC of the resource host . In other words a client process executing on a client may directly interface with one or more storage resources without requiring execution of a routine of any server processes executing on the computing resources . This single sided distributed architecture offers relatively high throughput and low latency since clients can access the storage resources without interfacing with the computing resources of the resource hosts . This has the effect of decoupling the requirements for storage and CPU cycles that typical two sided distributed systems carry. The single sided distributed system can utilize remote storage resources regardless of whether there are spare CPU cycles on that resource host furthermore since single sided operations do not contend for server CPU resources a single sided system can serve cache requests with very predictable low latency even when resource hosts are running at high CPU utilization. Thus the single sided distributed system allows higher utilization of both cluster storage and CPU resources than traditional two sided systems while delivering predictable low latency.

In some implementations the distributed system includes a storage logic portion a data control portion and a data storage portion . The storage logic portion may include a transaction application programming interface API e.g. a single sided transactional system client library that is responsible for accessing the underlying data for example via RPC or single sided operations. The data control portion may manage allocation and access to storage resources with tasks such as allocating storage resources registering storage resources with the corresponding network interface controller setting up connections between the client s and the resource hosts handling errors in case of machine failures etc. The data storage portion may include the loosely coupled resource hosts 

The distributed system may store data in dynamic random access memory DRAM and serve the data from the remote hosts via remote direct memory access RDMA capable network interface controllers . A network interface controller also known as a network interface card network adapter or LAN adapter may be a computer hardware component that connects a computing resource to the network . Both the resource hosts and the client may each have a network interface controller for network communications. A host process executing on the computing processor of the resource host registers a set of remote direct memory accessible regions of the memory with the network interface controller . The host process may register the remote direct memory accessible regions of the memory with a permission of read only or read write. The network interface controller of the resource host creates a client key for each registered memory region 

The single sided operations performed by the network interface controllers may be limited to simple reads writes and compare and swap operations none of which may be sophisticated enough to act as a drop in replacement for the software logic implemented by a traditional cache server job to carry out cache requests and manage cache policies. The transaction API translates commands such as look up or insert data commands into sequences of primitive network interface controller operations. The transaction API interfaces with the data control and data storage portions of the distributed system .

The distributed system may include a co located software process to register memory for remote access with the network interface controllers and set up connections with client processes . Once the connections are set up client processes can access the registered memory via engines in the hardware of the network interface controllers without any involvement from software on the local CPUs of the corresponding resource hosts .

Referring to in some implementations the distributed system includes multiple cells each cell including resource hosts a curator in communication with the resource hosts and a job management system in communication with the resource hosts . The curator e.g. process may execute on a computing processor e.g. server having a non transitory memory connected to the network and manage the data storage e.g. manage a file system stored on the resource hosts control data placements and or initiate data recovery. Moreover the curator may track an existence and storage location of data on the resource hosts . Redundant curators are possible. In some implementations the curator s track the striping of data across multiple resource hosts and the existence and or location of multiple copies of a given stripe for redundancy and or performance. In computer data storage data striping is the technique of segmenting logically sequential data such as a file in a way that accesses of sequential segments are made to different physical storage devices e.g. cells and or resource hosts . Striping is useful when a processing device requests access to data more quickly than a storage device can provide access. By performing segment accesses on multiple devices multiple segments can be accessed concurrently. This provides more data access throughput which avoids causing the processor to idly wait for data accesses. The job management system schedules jobs e.g. processing jobs or memory jobs across the resource hosts .

In some implementations the transaction API interfaces between a client e.g. with the client process and the curator . In some examples the client communicates with the curator through one or more remote procedure calls RPC . In response to a client request the transaction API may find the storage location of certain data on resource host s and obtain a key that allows access to the data . The transaction API communicates directly with the appropriate resource hosts via the network interface controllers to read or write the data e.g. using remote direct memory access . In the case that a resource host is non operational or the data was moved to a different resource host the client request fails prompting the client to re query the curator .

Referring to in some implementations the curator stores and manages file system metadata . The metadata may include a file map that maps files to file descriptors . The curator may examine and modify the representation of its persistent metadata . The curator may use three different access patterns for the metadata read only file transactions and stripe transactions.

Referring to data may be one or more files where each file has a specified replication level and or error correcting code . The curator may divide each file into a collection of stripes with each stripe being replicated or encoded independently from the remaining stripes . For a replicated file each stripe is a single logical chunk that the curator replicates as stripe replicas and stores on multiple storage resources . In that scenario a stripe replica is also referred to as a chunk . For an encoded file each stripe consists of multiple data chunks Dand non data chunks C e.g. code chunks that the curator places on multiple storage resources where the collection of data chunks D and non data chunks C forms a single code word. In general the curator may place each stripe on storage resources independently of how the other stripes in the file are placed on the storage resources . The error correcting code adds redundant data or parity data to a file so that the file can later be recovered by a receiver even when a number of errors up to the capability of the code being used were introduced. The error correcting code is used to maintain data integrity in storage devices to reconstruct data for performance latency or to more quickly drain machines.

Referring back to in some implementations file descriptors . stored by the curator contain metadata such as the file map which maps the stripes to stripe replicas or to data chunks D and code chunks C as appropriate stored on the resource hosts . To open a file a client sends a request to the curator which returns a file descriptor . The client uses the file descriptor to translate file chunk offsets to remote memory locations . The file descriptor may include a client key e.g. a 32 bit key that is unique to a chunk on a resource host and is used to RDMA read that chunk . After the client loads the file descriptor the client may access the data of a file via RDMA or another data retrieval method.

The curator may maintain status information for all resource hosts that are part of the cell . The status information may include capacity free space load on the resource host latency of the resource host from a client s point of view and a current state. The curator may obtain this information by querying the resource hosts in the cell directly and or by querying a client to gather latency statistics from a client s point of view. In some examples the curator uses the resource host status information to make rebalancing draining recovery decisions and allocation decisions.

The curator s may allocate chunks in order to handle client requests for more storage space in a file and for rebalancing and recovery. In some examples the processor replicates chunks among the storage devices differently than distributing the data chunks D and the code chunks C among the storage devices . The curator may maintain a load map of resource host load and liveliness. In some implementations the curator allocates a chunk by generating a list of candidate resource hosts and sends an allocate chunk request to each of the candidate resource hosts . If the resource host is overloaded or has no available space the resource host can deny the request. In this case the curator selects a different resource host . Each curator may continuously scan its designated portion of the file namespace examining all the metadata every minute or so. The curator may use the file scan to check the integrity of the metadata determine work that needs to be performed and or to generate statistics. The file scan may operate concurrently with other operations of the curator . The scan itself may not modify the metadata but schedules work to be done by other components of the system and computes statistics.

Referring to the job management system may determine or receives a system hierarchy of the distributed system to identify the levels e.g. levels at which maintenance or failure may occur without affecting a user s access to stored data and or the processors allowing access to the stored data . Maintenance or failures strict hierarchy non strict hierarchy may include power maintenance failure cooling system maintenance failure networking maintenance failure updating or replacing parts or other maintenance or power outage affecting the distributed system . Maintenance may be scheduled and in some examples an unscheduled system failure may occur.

The system hierarchy includes system levels e.g. levels with maintenance units system domains spanning one or more system levels . Each system domain has an active state or an inactive state. A distribution center module includes one or more cells and each cell includes one or more racks of resource hosts . Each cell also includes cell cooling cell power e.g. bus ducts and cell level networking e.g. network switch es . Similarly each rack includes rack cooling rack power e.g. bus ducts and rack level networking e.g. network switch es .

The system levels may include first second third and fourth system levels . The first system level corresponds to resource hosts or host machines of data processing devices non transitory memory devices or network devices e.g. NICs . Each host machine resource host has a system domain . The second system level corresponds to racks and cooling deliverers power deliverers e.g. bus ducts or communication deliverers e.g. network switches and cables of the host machines at the rack level. Each rack or rack level cooling deliverer power deliverer or communication deliverer has a system domain . The third system level corresponds to any cells of the distribution center module and the cell cooling cell power or cell level networking supplied to the associated racks . Each cell or cell cooling cell power or cell level networking has a system domain . The fourth system level corresponds to the distribution center module . Each distribution center module has a system domain .

The job management system determines based on the mappings of the hierarchy components which resource hosts are inactive when a hierarchy component is undergoing maintenance. Once the job management system maps the system domains to the resource hosts and therefore to their corresponding processor resources and storage resources the job management system determines a highest level e.g. levels at which maintenance can be performed while maintaining processor or data availability.

A system domain includes a hierarchy component undergoing maintenance and any hierarchy components depending therefrom. Therefore when one hierarchy component undergoes maintenance that hierarchy component is inactive and any other hierarchy components in the system domain of the hierarchy component are also inactive. For example when a resource host is undergoes maintenance a level system domain which includes the storage device the data processor and the NIC is in the inactive state. When a rack undergoes maintenance a level system domain which includes the rack and any resource hosts depending from the rack is in the inactive state. When a cell component for example to any one of the cell cooling component the bust duct and or the network switch of the cell component undergoes maintenance a level system domain which includes the cell and any hierarchy components in levels and that depend from the cell component is in the inactive state. Finally when a distribution center module undergoes maintenance a level system domain which includes the distribution center module and any hierarchy components in levels to depending from the distribution center module is in the inactive state.

In some examples as shown in a non strict hierarchy component may have dual feeds i.e. the hierarchy component depends on two or more other hierarchy components . For example a cell component may have a feed from two distribution center modules and or a rack may have a dual feed from two cell components . As shown a level system domain may include two racks where the second rack includes two feeds from two cell components . Therefore the second rack is part of two system domains and . Therefore the lower levels of the system hierarchy are maintained without causing the loss of the higher levels of the system hierarchy . This causes a redundancy in the distributed system which allows the for data accessibility. In particular the distribution center module may be maintained without losing any of the cell components depending from it. In some examples the racks include a dual powered rack that allows the maintenance of the bus duct without losing power to the dual powered racks depending from it. In some examples system domains that may be maintained without causing outages are ignored when distributing chunks to allow for maintenance however the ignored system domains may be included when distributing the chunks since an unplanned outage may still cause the loss of chunks

In some examples a cooling device such as the cell cooling and the rack cooling are used to cool the cell components and the racks respectively. The cell cooling component may cool one or multiple cell components . Similarly a rack cooling component may cool one or more racks . The curator stores the association of the resource hosts with the cooling devices i.e. the cell cooling and the rack cooling . In some implementations the job management system considers all possible combinations of maintenance that might occur within the system to determine a system hierarchy or a combination of maintenance hierarchies before storing the association of the resource hosts with the cooling devices . For example a system hierarchy where one or more cooling devices fail or a system hierarchy where the network devices fail or a system hierarchy where the power distribution fails.

Therefore when a hierarchy component in the system undergoes maintenance or fails that hierarchy component and any hierarchy components that are mapped to or depending from that hierarchy component are in an inactive state. A hierarchy component in an inactive state is inaccessible by a user while a hierarchy component in an active state is accessible by a user allowing the user to process access data stored supported maintained by that hierarchy component . As previously mentioned during the inactive state a user is incapable of accessing the resource host associated with the system domains undergoing maintenance and therefore the user is incapable of accessing the files i.e. chunks which include stripe replicas data chunks D and non data chunks C .

In some implementations the curator restricts a number of chunks distributed to storage devices of any one system domain e.g. based on the mapping of the hierarchy components . Therefore if a level system domain is inactive the curator maintains accessibility to the file or stripe although some chunks may be inaccessible. In some examples for each file or stripe the curator determines a maximum number of chunks that may be placed within any storage device within a single system domain so that if a system domain associated with the storage device storing chunks for a file is undergoing maintenance the curator may still retrieve the file . The maximum number of chunks ensures that the curator is capable of reconstructing the file although some chunks may be unavailable. In some examples the maximum number of chunks is set to a lower threshold to accommodate for any system failures while still being capable of reconstructing the file from the chunks . When the curator places chunks on the storage devices the curator ensures that within a stripe no more than the maximum number of chunks are inactive when a single system domain undergoes maintenance. Moreover the curator may also restrict the number of processing jobs on a data processor of a resource host within a system domain e.g. based on the mapping of the hierarchy components . Therefore if a level system domain is inactive the curator maintains accessibility to the jobs although some of the processors of the resource hosts are inactive.

Referring to in some implementations the system follows the operations shown for efficiently using storage resources and data processors when a maintenance event occurs. The system identifies at step a resource type 1 non storage resources e.g. computing usage of a data processor networking power delivery cooling etc. 2 storage capacity resources e.g. a byte usage storage capacity of a storage device or 3 storage bandwidth resources e.g. a number of input output operations per second allowed to hard disk storage or the amount of spindle utilization access allowed for such storage .

The system also identifies two classes of jobs requests . A first class of jobs includes high availability jobs and a second class includes standard or low availability jobs . The system executes processing jobs on the processor of the resource host and storage jobs for accessing storing data on the storage devices of the resource hosts . The high availability jobs are jobs that have a higher priority than the low availability jobs when both types of jobs are within a quota discussed below .

In some implementations when the system designates which types of resource losses may be tolerated the system determines a strategy at step for tolerating that loss based on whether the loss includes a loss of data processors or memory devices or both. For non storage resource usage the job management system uses a first strategy of migrating and or restarting jobs from failed non storage resources e.g. data processor to other available non storage resources. For storage resource usage the job management system uses a second strategy of leaving jobs accessing failed storage resources in place at least for a certain period of time and or may use data reconstruction to retrieve unavailable data . In some examples the first strategy handles resources relating to computing resources e.g. computation or networking and the second strategy handles storage resources e.g. storage devices such as hard disks and flash memory . When a failure loss occurs the job management system may determine whether the loss is a loss of data processors or storage device . If the loss is a loss of storage devices the job management system employs strategy at step otherwise the job management system employs strategy at step

When the job management system employs the first strategy at step that relates to data processors i.e. computation and networking the job management system migrates and or re executes high availability computing jobs assigned to data processors lost due to tolerated events to the remaining available data processors . For example if a certain power or network maintenance event renders 10 of the data processors unavailable the job management system moves the jobs to run on the remaining 90 of the data processors .

When the job management system employs the second strategy at step used for storage resources the job management system leaves the data stored on the storage devices that are in the inactive state during the maintenance or failure events and allows the system to use the replication and or coding discussed with respect to to reconstruct the unavailable data . In some examples the system employs the first and second strategies simultaneously.

Once the system determines which strategy to use the system calculates the quantity i.e. a quota Q of each resource i.e. the storage resource and the data processor resource that is available to the jobs during normal use i.e. when the system is not undergoing maintenance at step .

The job management system determines a first quota Qand a second quota Qof available resources . The first quota Qof available resources includes resources available to low availability jobs and is a quantity of the resources available during normal operations i.e. when the system is not undergoing maintenance or a failure . The second quota Qof available resources includes resources available to high availability jobs and is a quantity of the resources available during normal operations. When infrequent maintenance or a failure event occurs the low availability jobs encounter insufficient resources which results in degraded or nonexistent performance. The total available quota or job capacity Q may be calculated using the following equation 1 where Q is the total available quota of resources demanded by the jobs high availability and low availability Qis the first quota and Qis the second quota. Since the total available demanded quota capacity Q may exceed the quantity of resources available during normal operations R the job management system constrains a total usage of the resources by both the high availability jobs and the low availability jobs to the quantity of the resources that are available during normal operations R i.e. when the system is not undergoing maintenance or a failure to ensure that the number of jobs allocated to the resources does not exceed an actual available capacity of the resources e.g. R . The system may use equation 1 to determine the available quota Q for storage resources and processing resources . The available quota Q may be the capacity a capacity percentage a bandwidth or a size measurement of the resources.

In some examples the system lowers the first quota Qof the resources available to low availability jobs for a period of time before the tolerated event i.e. maintenance event or system failure and increases the second quota Qof the available resources to high availability jobs for the period of time before the tolerated event. Additionally or alternatively the system may suspend or end at least some of the low availability jobs for the period of time before the tolerated event. This allows the system to move the high availability jobs to the available resources .

In some implementations the system may determine the quantity of the resources lost due to a tolerated event based on an assignment of the jobs to particular resources associated with the system hierarchy of the distributed storage system . In some implementations the job management system designates certain types of host resource losses as tolerated when a planned maintenance or failure occurs. The quota Q available to these jobs is reduced by the maximum amount of loss that is tolerated while maintaining data accessibility and data processing accessibility. The system hierarchy the distribution center module the cell component the rack or the resource host may be designated as being tolerated.

In some examples during normal operations of the system 100 of the resources are available for storing data on the storage devices or for processing the data on the data processors of the resource hosts . A largest tolerated event i.e. maintenance event or system failure may result in a loss of 20 of the available resources based on the system hierarchy and the assignment of the resources to system domains . This leaves 80 of the resources always available despite a maximum loss of 20 of the resources . Therefore job management system assigns a maximum of 80 of the resources to high availability jobs allowing the system to have enough resources when a scheduled maintenance or system failure occurs. If the job management system re assigns jobs related to the data processor to 80 of available processors then the system will have enough processors to execute the high availability jobs . In addition if the system has to reconstruct data unavailable due to the inactive state of the system domain that includes the storage device storing the data the system has enough storage devices that have enough chunks e.g. data chunks D and non data chunks C and replicas to reconstruct the lost data .

When determining at step the second quota Qof resources available to high availability jobs for non storage resource usage i.e. under strategy the second quota Qof available resources is a quantity of the resources available during normal operations minus a quantity of the resources lost due to a tolerated event. Therefore the second quota Qmay be calculated based on the following equation when the resource lost is data processors 2 where Ris the quantity of the resources available during normal operations and Ris the maximum quantity of the resources lost due to a tolerated event.

In some implementations the system determines the second quota Qwhen the resource lost is a storage device . When determining at step the second quota Qof resources available to high availability jobs for storage capacity usage i.e. under strategy the second quota Qof the storage devices available to high availability jobs may equal the full amount of the resource that is normally available although the usage of that resource could require redundancy of replicated chunks or coded chunks C D for example as calculated using equation 3. 3 This may apply for byte usage capacity of the storage device . For example up to 100 of the byte usage capacity of the storage device can be assigned to high availability jobs

When determining at step the second quota Qof resources available to high availability jobs for storage bandwidth usage i.e. under strategy the second quota Q i.e. bandwidth or usage of that storage resource such as the number of input output operations per second allowed to hard disk storage or the amount of spindle utilization access allowed for such storage may be calculated as the amount of resource normally available Rminus the largest loss Rof that resource due to a tolerated event minus the increased quantity Rof that resource used due to the largest loss due to a tolerated event as shown in the following equation 4 

In some examples the system may determine the second quota Qusing the following equation when calculating the resources lost are storage devices such as hard disks 

For example if up to 10 of the hard disk spindles can be lost in a tolerated event and if each access to the data that is lost requires on average 4 times as much spindle usage due to the need to reconstruct the data using a code then using equation 5A 

Therefore 69 of the spindle resource can be made available to high availability jobs this calculation is based on the fact that only 90 of the spindles are available and support 130 of the normal load from high availability jobs .

Once the system calculates the second quota Qof the resource the system allows both low availability jobs and high availability jobs to run in the data center subject to two constraints at step . The first constraint is that the total amount of each resource used by the high availability and the low availability jobs Q Q cannot exceed the quantity of the resource that is normally available R as shown in the below equation 6 

The second constraint is that the amount of each resource used Qby the high availability jobs cannot exceed the second quota Q as shown in the following equation 7 In some implementations the job management system monitors a usage of the resources by the high availability jobs to ensure that they do not consume too many resources . When a high availability job exceeds a threshold usage the job management system downgrades the high availability job to a low availability job . When a high availability job is downgraded the job management system may terminate the downgraded job which increases the available capacity second quota available for the high availability jobs

After the job management system determines the quotas Qthat may be used the job management system implements the processes for accommodating the maintenance and failure events. When a maintenance or failure event decreases the amount of a resource that is available the job management system may suspend kill evict or otherwise prevent a sufficient number of low availability jobs are from using the resource so that the resource is available for the high availability jobs . In addition if these jobs are serving live traffic the amount of traffic sent to them may be reduced or diverted to other data centers.

Furthermore in the case of planned maintenance the job management system may suspend kill evict or otherwise prevent some or all low availability jobs from using certain resources for some amount of time prior to the maintenance in order to allow the high availability jobs to move to use those resources .

While the above description focused on resources in a data center the same principles can be applied at the level of a resource host machine a rack a cell a network cluster building site region or entire global collection of data centers.

In some implementations the resources include data processing devices networking systems power systems or cooling systems . For these types of resources the method may include migrating or re executing jobs assigned to resources lost due to the tolerated event to remaining resources to maintain accessibility of the data or accessibility of the data processors .

In some implementations the resources include non transitory memory devices . For this type of resources the method may include leaving jobs assigned to the lost resources without reassigning the jobs to remaining resources and reconstructing any unavailable data associated with the lost resources .

The method may include determining the second quota Qof the resource available to high availability jobs as the quantity of the resources available during normal operations Rminus the quantity of the resources lost due to a tolerated event Rminus an increased quantity of the remaining resources needed due to the tolerated event R. Additionally or alternatively the method may include limiting a sum of the first quota Q and the second quota Qto a maximum quota.

In some implementations the method includes monitoring a usage of the resources by the high availability jobs . When a high availability job exceeds a threshold usage the method includes downgrading the high availability job to a low availability job

The method may further include lowering the first quota Qof the resources available to low availability jobs for a period of time before the tolerated event and increasing the second quota Qof the resources available to high availability jobs for the period of time before the tolerated event. Additionally or alternatively the method may include suspending or ending at least some of the low availability jobs for the period of time before the tolerated event.

In some examples the method includes determining the quantity of the resources lost due to a tolerated event Rbased on an assignment of the jobs to particular resources and a system hierarchy of the distributed system . The system hierarchy includes system domains or units . Each system domain has an active state or an inactive state. The system hierarchy may include system levels such as levels . The first system level e.g. level corresponds to resource hosts having data processing devices non transitory memory devices or network interface controllers . Each data processing device or memory device has one or more system domains . The second system level e.g. level corresponds to host machines e.g. racks of the memory devices or the data processing devices each host machine having one or more system domain . A third system level e.g. level corresponding to power deliverers e.g. bus ducts communication deliverers or cooling deliverers for the host machines . Each power deliverer communication deliverer or cooling deliverer has a system domain . The fourth system level e.g. level corresponds to a distribution center module of the power deliverer communication deliverer or cooling deliverer . Each distribution center module has a system domain .

Various implementations of the systems and techniques described here can be realized in digital electronic circuitry integrated circuitry specially designed ASICs application specific integrated circuits computer hardware firmware software and or combinations thereof. These various implementations can include implementation in one or more computer programs that are executable and or interpretable on a programmable system including at least one programmable processor which may be special or general purpose coupled to receive data and instructions from and to transmit data and instructions to a storage system at least one input device and at least one output device.

These computer programs also known as programs software software applications or code include machine instructions for a programmable processor and can be implemented in a high level procedural and or object oriented programming language and or in assembly machine language. As used herein the terms machine readable medium and computer readable medium refer to any computer program product apparatus and or device e.g. magnetic discs optical disks memory Programmable Logic Devices PLDs used to provide machine instructions and or data to a programmable processor including a machine readable medium that receives machine instructions as a machine readable signal. The term machine readable signal refers to any signal used to provide machine instructions and or data to a programmable processor.

Implementations of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry or in computer software firmware or hardware including the structures disclosed in this specification and their structural equivalents or in combinations of one or more of them. Moreover. subject matter described in this specification can be implemented as one or more computer program products i.e. one or more modules of computer program instructions encoded on a computer readable medium for execution by or to control the operation of data processing apparatus. The computer readable medium can be a machine readable storage device a machine readable storage substrate a memory device a composition of matter affecting a machine readable propagated signal or a combination of one or more of them. The terms data processing apparatus computing device and computing processor encompass all apparatus devices and machines for processing data including by way of example a programmable processor a computer or multiple processors or computers. The apparatus can include in addition to hardware code that creates an execution environment for the computer program in question e.g. code that constitutes processor firmware a protocol stack a database management system an operating system or a combination of one or more of them. A propagated signal is an artificially generated signal e.g. a machine generated electrical optical or electromagnetic signal that is generated to encode information for transmission to suitable receiver apparatus.

A computer program also known as an application program software software application script or code can be written in any form of programming language including compiled or interpreted languages and it can be deployed in any form including as a stand alone program or as a module component subroutine or other unit suitable for use in a computing environment. A computer program does not necessarily correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data e.g. one or more scripts stored in a markup language document in a single file dedicated to the program in question or in multiple coordinated files e.g. files that store one or more modules sub programs or portions of code . A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.

The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by and apparatus can also be implemented as special purpose logic circuitry e.g. an FPGA field programmable gate array or an ASIC application specific integrated circuit .

Processors suitable for the execution of a computer program include by way of example both general and special purpose microprocessors and any one or more processors of any kind of digital computer. Generally a processor will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data. Generally a computer will also include or be operatively coupled to receive data from or transfer data to or both one or more mass storage devices for storing data e.g. magnetic magneto optical disks or optical disks. However a computer need not have such devices. Moreover a computer can be embedded in another device e.g. a mobile telephone a personal digital assistant PDA a mobile audio player a Global Positioning System GPS receiver to name just a few. Computer readable media suitable for storing computer program instructions and data include all forms of non volatile memory media and memory devices including by way of example semiconductor memory devices e.g. EPROM EEPROM and flash memory devices magnetic disks e.g. internal hard disks or removable disks magneto optical disks and CD ROM and DVD ROM disks. The processor and the memory can be supplemented by or incorporated in special purpose logic circuitry.

To provide for interaction with a user one or more aspects of the disclosure can be implemented on a computer having a display device e.g. a CRT cathode ray tube LCD liquid crystal display monitor or touch screen for displaying information to the user and optionally a keyboard and a pointing device e.g. a mouse or a trackball by which the user can provide input to the computer. Other kinds of devices can be used to provide interaction with a user as well for example feedback provided to the user can be any form of sensory feedback e.g. visual feedback auditory feedback or tactile feedback and input from the user can be received in any form including acoustic speech or tactile input. In addition a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user for example by sending web pages to a web browser on a user s client device in response to requests received from the web browser.

One or more aspects of the disclosure can be implemented in a computing system that includes a backend component e.g. as a data server or that includes a middleware component e.g. an application server or that includes a frontend component e.g. a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification or any combination of one or more such backend middleware or frontend components. The components of the system can be interconnected by any form or medium of digital data communication e.g. a communication network. Examples of communication networks include a local area network LAN and a wide area network WAN an inter network e.g. the Internet and peer to peer networks e.g. ad hoc peer to peer networks .

The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client server relationship to each other. In some implementations a server transmits data e.g. an HTML page to a client device e.g. for purposes of displaying data to and receiving user input from a user interacting with the client device . Data generated at the client device e.g. a result of the user interaction can be received from the client device at the server.

While this specification contains many specifics these should not be construed as limitations on the scope of the disclosure or of what may be claimed but rather as descriptions of features specific to particular implementations of the disclosure. Certain features that are described in this specification in the context of separate implementations can also be implemented in combination in a single implementation. Conversely various features that are described in the context of a single implementation can also be implemented in multiple implementations separately or in any suitable sub combination. Moreover although features may be described above as acting in certain combinations and even initially claimed as such one or more features from a claimed combination can in some cases be excised from the combination and the claimed combination may be directed to a sub combination or variation of a sub combination.

Similarly while operations are depicted in the drawings in a particular order this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order or that all illustrated operations be performed to achieve desirable results. In certain circumstances multi tasking and parallel processing may be advantageous. Moreover the separation of various system components in the embodiments described above should not be understood as requiring such separation in all embodiments and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.

A number of implementations have been described. Nevertheless it will be understood that various modifications may be made without departing from the spirit and scope of the disclosure. Accordingly other implementations are within the scope of the following claims. For example the actions recited in the claims can be performed in a different order and still achieve desirable results.

