---

title: System, method, and computer program product for graphics processing unit (GPU) demand paging
abstract: A system, method, and computer program product are provided for GPU demand paging. In operation, input data is addressed in terms of a virtual address space. Additionally, the input data is organized into one or more pages of data. Further, the input data organized as the one or more pages of data is at least temporarily stored in a physical cache. In addition, access to the input data in the physical cache is facilitated.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09235512&OS=09235512&RS=09235512
owner: NVIDIA Corporation
number: 09235512
owner_city: Santa Clara
owner_country: US
publication_date: 20140109
---
This application claims the benefit of U.S. Provisional Patent Application No. 61 754 500 filed Jan. 18 2013 the entire contents of which are incorporated herein by reference.

The present invention relates to graphics processing and more particularly to storing and accessing data in the context of graphics processing.

Some graphics processing unit GPU based applications require that input data is processed when the overall size of the input data is too large to fit completely into GPU device memory. One solution for this problem is to split computation into multiple kernel runs and process one chunk of data at a time. This approach works well with algorithms that process data sequentially but can become problematic in cases where random read write access to data is required e.g. recursive ray tracing etc. . Thus there is a need for addressing this issue and or other issues associated with the prior art.

A system method and computer program product are provided for GPU demand paging. In operation input data is addressed in terms of a virtual address space. Additionally the input data is organized into one or more pages of data. Further the input data organized as the one or more pages of data is at least temporarily stored in a physical cache. In addition access to the input data in the physical cache is facilitated.

Additionally in one embodiment a size of the virtual address space may be determined dynamically. For example rather than using a fixed size virtual address space the size of the virtual address space may be determined dynamically before initially beginning computation. In one embodiment the virtual address space may be determined based on the size of the input data.

As shown in operation the input data is organized into one or more pages of data. Further as shown in operation the input data organized as the one or more pages of data is at least temporarily stored in a physical cache e.g. associated with a GPU CPU etc. . Moreover in operation access to the input data in the physical cache is facilitated.

In one embodiment each page of data may be assigned a time stamp. Further in one embodiment each time stamp may be updated each time the page of data is accessed e.g. during a read operation etc. . In one embodiment the method may further include identifying a number of least recently used pages of data by sorting the time stamps.

In this case in one embodiment the time stamps may be sorted using a sorting kernel that is separate and run before a page processing kernel. In one embodiment the sorting kernel may generate a list that the page processing kernel is capable of utilizing to update at least one of a page table or the physical cache.

Still yet in one embodiment facilitating access to the input data in the physical cache may include receiving a GPU thread request to access a particular page of data. As an option the method may further include performing a page table lookup to determine the availability of the particular page of data in the physical cache.

In this case if the particular page of data is available in the physical cache a page table entry may provide a current physical address of the requested particular page of data. In this way a thread may be capable of using the physical address of the requested particular page of data to access the particular page of data. On the other hand if the particular page of data is not available in the physical cache the particular page of data may be requested for upload and a thread may interrupt a current computation e.g. and may yield to a host processor etc. .

For example in one embodiment threads may receive and or fetch work items from queues. When a thread is blocked on a page fault the thread may store the live state to GPU memory. The thread may then fetch another work item. Once there are no more work items or no more space to store the state of blocked work items the kernel terminates. At this point control may return to the host for processing page requests.

In one embodiment if there are no more threads able to continue working without requested data being uploaded the kernel may terminate. Further as an option the page table and the physical cache may be updated and modified pages may be downloaded to the host. In one embodiment old pages of data may be evicted from the physical cache.

In various embodiments the page table may be a variety of sizes and or structures. For example in one embodiment the page table may include a non hierarchical page table with one 64 bit page descriptor per page of data. Of course in various embodiments the page descriptor may be various sizes.

More illustrative information will now be set forth regarding various optional architectures and features with which the foregoing framework may or may not be implemented per the desires of the user. It should be strongly noted that the following information is set forth for illustrative purposes and should not be construed as limiting in any manner. Any of the following features may be optionally incorporated with or without the exclusion of other features described.

In one embodiment the PPU includes an input output I O unit configured to transmit and receive communications i.e. commands data etc. from a central processing unit CPU not shown over the system bus . The I O unit may implement a Peripheral Component Interconnect Express PCIe interface for communications over a PCIe bus. In alternative embodiments the I O unit may implement other types of well known bus interfaces.

The PPU also includes a host interface unit that decodes the commands and transmits the commands to the task management unit or other units of the PPU e.g. memory interface as the commands may specify. In one embodiment the PPU comprises U memory interfaces U where each memory interface U is connected to a corresponding memory device U . The host interface unit is configured to route communications between and among the various logical units of the PPU .

In one embodiment a program encoded as a command stream is written to a buffer by the CPU. The buffer is a region in memory e.g. memory or system memory that is accessible i.e. read write by both the CPU and the PPU . The CPU writes the command stream to the buffer and then transmits a pointer to the start of the command stream to the PPU . The host interface unit provides the task management unit TMU with pointers to one or more streams.

The TMU selects one or more streams and is configured to organize the selected streams as a pool of pending grids. In one embodiment a thread block comprises 32 related threads and a grid is an array of one or more thread blocks that execute the same stream and the different thread blocks may exchange data through global memory. The pool of pending grids may include new grids that have not yet been selected for execution and grids that have been partially executed and have been suspended.

A work distribution unit that is coupled between the TMU and the SMs manages a pool of active grids selecting and dispatching active grids for execution by the SMs . Pending grids are transferred to the active grid pool by the TMU when a pending grid is eligible to execute i.e. has no unresolved data dependencies.

An active grid is transferred to the pending pool when execution of the active grid is blocked by a dependency. When execution of a grid is completed the grid is removed from the active grid pool by the work distribution unit . In addition to receiving grids from the host interface unit and the work distribution unit the TMU also receives grids that are dynamically generated by the SMs during execution of a grid. These dynamically generated grids join the other pending grids in the pending grid pool.

In one embodiment the CPU executes a driver kernel that implements an application programming interface API that enables one or more applications executing on the CPU to schedule operations for execution on the PPU . An application may include instructions i.e. API calls that cause the driver kernel to generate one or more grids for execution. In one embodiment the PPU implements a SIMT Single Instruction Multiple Thread architecture where each thread block i.e. warp in a grid is concurrently executed on a different data set by different threads in the thread block. The driver kernel defines thread blocks that are comprised of k related threads such that threads in the same thread block may exchange data through shared memory.

In one embodiment the PPU may include 15 distinct SMs . Each SM is multi threaded and configured to execute a plurality of threads e.g. 32 threads from a particular thread block concurrently. Each of the SMs is connected to a level two L2 cache via a crossbar or other type of interconnect network . The L2 cache is connected to one or more memory interfaces . Memory interfaces implement 16 32 64 128 bit data buses or the like for high speed data transfer. In one embodiment the PPU may be connected to up to 6 memory devices such as graphics double data rate version 5 synchronous dynamic random access memory GDDR5 SDRAM .

In one embodiment the PPU implements a multi level memory hierarchy. The memory is located off chip in SDRAM coupled to the PPU . Data from the memory may be fetched and stored in the L2 cache which is located on chip and is shared between the various SMs . In one embodiment each of the SMs also implements an L1 cache. The L1 cache is private memory that is dedicated to a particular SM . Each of the L1 caches is coupled to the shared L2 cache . Data from the L2 cache may be fetched and stored in each of the L1 caches for processing in the functional units of the SMs .

In one embodiment the PPU comprises a graphics processing unit GPU . The PPU is configured to receive commands that specify shader programs for processing graphics data. Graphics data may be defined as a set of primitives such as points lines triangles quads triangle strips and the like. Typically a primitive includes data that specifies a number of vertices for the primitive e.g. in a model space coordinate system as well as attributes associated with each vertex of the primitive. The PPU can be configured to process the graphics primitives to generate a frame buffer i.e. pixel data for each of the pixels of the display . The driver kernel implements a graphics processing pipeline such as the graphics processing pipeline defined by the OpenGL API.

An application writes model data for a scene i.e. a collection of vertices and attributes to memory. The model data defines each of the objects that may be visible on a display. The application then makes an API call to the driver kernel that requests the model data to be rendered and displayed. The driver kernel reads the model data and writes commands to the buffer to perform one or more operations to process the model data.

The commands may encode different shader programs including one or more of a vertex shader hull shader geometry shader pixel shader etc. For example the TMU may configure one or more SMs to execute a vertex shader program that processes a number of vertices defined by the model data. In one embodiment the TMU may configure different SMs to execute different shader programs concurrently. For example a first subset of SMs may be configured to execute a vertex shader program while a second subset of SMs may be configured to execute a pixel shader program.

The first subset of SMs processes vertex data to produce processed vertex data and writes the processed vertex data to the L2 cache and or the memory . After the processed vertex data is rasterized i.e. transformed from three dimensional data into two dimensional data in screen space to produce fragment data the second subset of SMs executes a pixel shader to produce processed fragment data which is then blended with other processed fragment data and written to the frame buffer in memory . The vertex shader program and pixel shader program may execute concurrently processing different data from the same scene in a pipelined fashion until all of the model data for the scene has been rendered to the frame buffer. Then the contents of the frame buffer are transmitted to a display controller for display on a display device.

The PPU may be included in a desktop computer a laptop computer a tablet computer a smart phone e.g. a wireless hand held device personal digital assistant PDA a digital camera a hand held electronic device and the like. In one embodiment the PPU is embodied on a single semiconductor substrate. In another embodiment the PPU is included in a system on a chip SoC along with one or more other logic units such as a reduced instruction set computer RISC CPU a memory management unit MMU a digital to analog converter DAC and the like.

In one embodiment the PPU may be included on a graphics card that includes one or more memory devices such as GDDR5 SDRAM. The graphics card may be configured to interface with a PCIe slot on a motherboard of a desktop computer that includes e.g. a northbridge chipset and a southbridge chipset. In yet another embodiment the PPU may be an integrated graphics processing unit iGPU included in the chipset i.e. Northbridge of the motherboard.

As described above the work distribution unit dispatches active grids for execution on one or more SMs of the PPU . The scheduler unit receives the grids from the work distribution unit and manages instruction scheduling for one or more thread blocks of each active grid. The scheduler unit schedules threads for execution in groups of parallel threads where each group is called a warp. In one embodiment each warp includes 32 threads. The scheduler unit may manage a plurality of different thread blocks allocating the thread blocks to warps for execution and then scheduling instructions from the plurality of different warps on the various functional units i.e. cores DPUs SFUs and LSUs during each clock cycle.

In one embodiment each scheduler unit includes one or more instruction dispatch units . Each dispatch unit is configured to transmit instructions to one or more of the functional units. In the embodiment shown in the scheduler unit includes two dispatch units that enable two different instructions from the same warp to be dispatched during each clock cycle. In alternative embodiments each scheduler unit may include a single dispatch unit or additional dispatch units .

Each SM includes a register file that provides a set of registers for the functional units of the SM . In one embodiment the register file is divided between each of the functional units such that each functional unit is allocated a dedicated portion of the register file . In another embodiment the register file is divided between the different warps being executed by the SM . The register file provides temporary storage for operands connected to the data paths of the functional units.

Each SM comprises L processing cores . In one embodiment the SM includes a large number e.g. 192 etc. of distinct processing cores . Each core is a fully pipelined single precision processing unit that includes a floating point arithmetic logic unit and an integer arithmetic logic unit. In one embodiment the floating point arithmetic logic units implement the IEEE 754 2008 standard for floating point arithmetic. Each SM also comprises M DPUs that implement double precision floating point arithmetic N SFUs that perform special functions e.g. copy rectangle pixel blending operations and the like and P LSUs that implement load and store operations between the shared memory and the register file via the J texture unit L1 caches and the interconnect network . The J texture unit L1 caches are coupled between the interconnect network and the shared memory and are also coupled to the crossbar . In one embodiment the SM includes 64 DPUs 32 SFUs and 32 LSUs . In another embodiment the L1 cache is not included within the texture unit and is instead included with the shared memory with a separate direct connection to the crossbar .

Each SM includes an interconnect network that connects each of the functional units to the register file and to the shared memory through the interconnect network . In one embodiment the interconnect network is a crossbar that can be configured to connect any of the functional units to any of the registers in the register file to any of the J texture unit L1 caches or the memory locations in shared memory .

In one embodiment the SM is implemented within a GPU. In such an embodiment the SM comprises J texture unit L1 caches . The texture unit L1 caches are configured to access texture maps i.e. a 2D array of texels from the memory and sample the texture maps to produce sampled texture values for use in shader programs. The texture unit L1 caches implement texture operations such as anti aliasing operations using mip maps i.e. texture maps of varying levels of detail . In one embodiment the SM includes 16 texture unit L1 caches . In one embodiment the texture unit L1 caches may be configured to receive load and store requests from the LSUs and to coalesce the texture accesses and the load and store requests to generate coalesced memory operations that are output to a memory system that includes the shared memory . The memory system may also include the L2 cache memory and a system memory not shown .

The PPU described above may be configured to perform highly parallel computations much faster than conventional CPUs. Parallel computing has advantages in graphics processing data compression biometrics stream processing algorithms and the like.

In one embodiment the systems described herein may function to implement a software demand paging approach. For example in one embodiment an input data set may be addressed in terms of a virtual address space organized in pages. In this case in one embodiment actual data may reside temporarily in a device side cache e.g. holding a fixed maximum number of pages at a time etc. . Further in one embodiment this technique may be integrated into the NVIDIA OptiX JIT Just In Time compiler which creates a GPU kernel that performs these actions.

As shown in it is determined whether a GPU thread access request to a virtual page is received. See decision . Once a GPU thread requests access to a virtual page a page table lookup is performed to determine the availability of the page in physical cache memory. See operation .

Further it is determined whether the page is available in the physical cache. See decision . If the page is available in the physical cache a page table entry provides the current physical address of the requested page. See operation . Additionally the thread uses this address to access the page data. See operation .

If the page is not available in the cache the page data is requested for upload and the thread interrupts its current computation and yields to the host processor. See operation . Further it is determined whether any threads have more work to perform. See decision .

Once there are no more threads that can continue working without requested data being uploaded the kernel terminates. See operation . Furthermore the page table and physical page cache are updated. See operation .

Modified pages are then downloaded to the host device and newly requested pages are uploaded to the GPU device. See operation . During this process old pages are evicted from the cache as needed. Subsequently the kernel is restarted and computation is resumed at the point after where threads requested data. See operation . Moreover the update cycle repeats with page table lookups to determine the availability of the page in physical cache until the kernel has finished its algorithm and all required data has been processed. See decision .

In this way threads may receive and or fetch work items from queues. If a thread is blocked on a page fault the thread may store a live state to memory e.g. GPU memory etc. . The thread may then fetch another work item. Once there are no more work items or no more space to store the state of blocked work items the kernel may terminate. Upon termination control may return to the host for processing page requests. In one embodiment a ray tracing compiler may insert store instructions to save all live states to global memory. Further when the kernel is restarted after page requests are satisfied prior work items from the queue are resumed by loading their live state from global memory.

In one embodiment a GPU may be utilized to implement the method . In various embodiments this GPU based approach may differ in certain areas from typical CPU hardware software based demand paging methods. For example the GPU based implementation may be configured to implement a dynamic virtual address space. In this case rather than using a fixed size virtual address space e.g. 64 bit etc. the size of the virtual address space may be determined dynamically before initially starting computation on the GPU. In one embodiment before starting computation the size of the virtual address space may be defined as the size of the input source data. In one embodiment in case the input data changes during the life of a host application the virtual address space may grow or shrink as needed.

Additionally in one embodiment the GPU based implementation may utilize a flat page table. For example due to dynamically adapting the size of the virtual address space it is possible to employ a small flat non hierarchical page table e.g. with one 64 bit page descriptor per virtual page etc. . In contrast CPU page tables typically need to be organized as a tree structure in order to manage a sparsely occupied address space.

Further in one embodiment the GPU may implement GPU based page request processing. In this case page requests may be processed once the main kernel cannot proceed with computation due to page misses and the main kernel has terminated. In one embodiment to identify requested pages a bit field may be used containing as many bits as there are virtual pages. In this case relevant request bits may be set when pages are requested. In one embodiment processing of these requests may be implemented in a separate GPU helper kernel that scans the request bits to find requested pages. In one embodiment the GPU helper kernel may also determines suitable physical pages to be evicted from the cache update page table descriptors and generate a list of pages that are requested to be uploaded from the host.

In addition in one embodiment the GPU approach may implement asynchronous data upload. For example after the page processing kernel is launched a second helper kernel may be launched which is responsible for data upload. In one embodiment this loader kernel may copy requested pages from host memory into the page cache with the help of the GPU s zero copy functionality. This enables the copy process to be interleaved with the host CPU which can asynchronously fetch e.g. over a network etc. and stage further pages for upload.

Still yet in one embodiment the GPU approach may function to implement a least recently used LRU page eviction policy. For example in order to determine suitable pages in the cache that can be replaced by newly requested pages a least recently used page replacement policy may be implemented. This technique may be implemented to evict pages first that have not been accessed for the longest time in the past. The LRU algorithm represents the theoretically optimal evictions strategy. This method is typically only approximated in CPU paging systems due to its high costs.

More information associated with the LRU page eviction policy may be found in Virtual Memory Denning P. J. 1970 2 3 153 189 which is hereby incorporated by reference in its entirety.

As shown in operation each page may be assigned a time stamp which is updated each time the page gets accessed. In one embodiment multiple memory accesses to a single page may be combined such that they do not each require an update of the time stamp.

In order to find a number of least recently used pages the time stamps may be sorted by a separate sorting kernel that is run before the page processing kernel as shown in operation . The sort kernel may then generate a list which the processing kernel may use to update the page table and the cache as shown in operation . In this way a GPU may implement GPU software based demand paging by implementing a method to support random access of arbitrarily sized data independent of available GPU device memory. For example in one embodiment GPU software based demand paging may be implemented in the context of ray tracing.

Ray tracing on a GPU differs from rasterization in that for rasterization when a geometric object is drawn the textures and shader programs for that geometry need to be resident and bound and that geometry needs to reside or be sent to the GPU. However no other textures shaders or geometry need to be resident. With ray tracing on the other hand rays can hit any geometry in the scene especially reflected rays. This means that in a straightforward implementation all geometry textures and shaders need to be resident and bound the entire time that any rays are being traced. Thus a ray traceable scene is limited to the size of the GPU memory. In OptiX all shaders are made to be bound and resident by linking them into a single CUDA megakernel that is used for all ray tracing. In one embodiment this may be implemented using the OptiX compiler.

As shown in operation a set of data buffers e.g. mostly textures acceleration data structures and geometry etc. may be analyzed. Based on the analysis it may be determined which buffers are to reside in GPU memory and which buffers are to reside in host memory and be demand paged into GPU memory as shown in operation .

When compiling the megakernel all load instructions that read from buffers marked for paging are analyzed as shown in operation . Further each load is rewritten to a paged buffer as a virtual memory load as shown in operation .

In one embodiment a rewritten load instruction may include a variety of steps. For example one of the steps may include a page translation. In this case the virtual address may be split into the page number and the offset. Furthermore a look up of the page number in the page table may be performed. In one embodiment the page table may include a bit for indicating whether the page is resident and a physical address e.g. an address in CUDA memory space etc. of the page if the page is resident. If the page is resident the offset to the page s physical address may be added and the load may be performed. Additionally the page may be marked as in use e.g. requested etc. so that it does not get evicted.

In addition one of the steps may include a page fault. For example if the page is not resident no progress on a corresponding ray may be made until the load is satisfied. In this case the page may be marked as requested by setting a bit in the page table entry PTE or adding the page number to a request queue. Furthermore all live registers that pertain to this ray may be stored into a GPU global memory buffer. Additionally the program counter may be stored to return to once the page request is fulfilled. The process may the return to the main computation loop such that more work is requested or the loop is exited. In one embodiment the compiler may automatically write the code that loads a current state and jumps to a faulted load instruction.

As shown in operation for page fulfillment once all pages have been requested by all threads that hit a page fault the kernel terminates and control returns to the host. In one embodiment the host may fill the page requests by making a list of all requested pages and choosing locations in the GPU s page cache to store the requested pages. In one embodiment this may be implemented using a series of CUDA kernels that implement an LRU page replacement policy. Doing this on the GPU means the computation can be implemented efficiently without copying the page table back to the CPU. Further any pages that were modified on the GPU may be downloaded to host memory and all of the requested pages may be uploaded to GPU memory.

Once all page requests have been satisfied the megakernel is relaunched as shown in operation . On the GPU this may include each thread requesting work and any rays that were suspended due to page faults may be resumed. Additionally a state that was live at the time of the page fault may be reloaded from GPU memory. The thread may then jump to the continuation of the code that was being executed. Further the page translation may be performed again and the load instruction may be executed.

When a kernel launch completes with no pages requested the ray tracing frame is complete. See decision . As an option the method described in the context of may be implemented utilizing a ray tracing application compiler e.g. an OptiX compiler etc. . Further in one embodiment cluster buffers may be utilized to minimize paged loads.

The system also includes input devices a graphics processor and a display i.e. a conventional CRT cathode ray tube LCD liquid crystal display LED light emitting diode plasma display or the like. User input may be received from the input devices e.g. keyboard mouse touchpad microphone and the like. In one embodiment the graphics processor may include a plurality of shader modules a rasterization module etc. Each of the foregoing modules may even be situated on a single semiconductor platform to form a graphics processing unit GPU .

In the present description a single semiconductor platform may refer to a sole unitary semiconductor based integrated circuit or chip. It should be noted that the term single semiconductor platform may also refer to multi chip modules with increased connectivity which simulate on chip operation and make substantial improvements over utilizing a conventional central processing unit CPU and bus implementation. Of course the various modules may also be situated separately or in various combinations of semiconductor platforms per the desires of the user.

The system may also include a secondary storage . The secondary storage includes for example a hard disk drive and or a removable storage drive representing a floppy disk drive a magnetic tape drive a compact disk drive digital versatile disk DVD drive recording device universal serial bus USB flash memory. The removable storage drive reads from and or writes to a removable storage unit in a well known manner.

Computer programs or computer control logic algorithms may be stored in the main memory and or the secondary storage . Such computer programs when executed enable the system to perform various functions. For example a compiler program that is configured to examiner a shader program and enable or disable attribute buffer combining may be stored in the main memory . The compiler program may be executed by the central processor or the graphics processor . The main memory the storage and or any other storage are possible examples of computer readable media.

In one embodiment the architecture and or functionality of the various previous figures may be implemented in the context of the central processor the graphics processor an integrated circuit not shown that is capable of at least a portion of the capabilities of both the central processor and the graphics processor a chipset i.e. a group of integrated circuits designed to work and sold as a unit for performing related functions etc. and or any other integrated circuit for that matter.

Still yet the architecture and or functionality of the various previous figures may be implemented in the context of a general computer system a circuit board system a game console system dedicated for entertainment purposes an application specific system and or any other desired system. For example the system may take the form of a desktop computer laptop computer server workstation game consoles embedded system and or any other type of logic. Still yet the system may take the form of various other devices including but not limited to a personal digital assistant PDA device a mobile phone device a television etc.

Further while not shown the system may be coupled to a network e.g. a telecommunications network local area network LAN wireless network wide area network WAN such as the Internet peer to peer network cable network or the like for communication purposes.

While various embodiments have been described above it should be understood that they have been presented by way of example only and not limitation. Thus the breadth and scope of a preferred embodiment should not be limited by any of the above described exemplary embodiments but should be defined only in accordance with the following claims and their equivalents.

