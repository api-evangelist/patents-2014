---

title: System and method for parallelized performance data collection in a computing system
abstract: A system and method for of prioritizing accumulation of time-dependent data is disclosed. In an embodiment, a plurality of data elements are identified to be retrieved. The data elements include a high-priority data element and a low-priority data element. A first data retrieval operation is performed to retrieve the high-priority data element, to store a copy of the high-priority data element in a memory structure, and to reserve a memory space in the memory structure for the low-priority data element based on the low-priority data element corresponding to the high-priority data element. In parallel with the first data retrieval operation, a second data retrieval operation is performed to analyze the memory structure to detect the reserved memory space, upon detecting the reserved memory space, to retrieve the low-priority data element, and to store a copy of the low-priority data element in the reserved memory space.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09304702&OS=09304702&RS=09304702
owner: NETAPP, INC.
number: 09304702
owner_city: Sunnnyvale
owner_country: US
publication_date: 20140529
---
The present disclosure relates generally to computing system data management and more particularly to a technique for accumulating and merging data originating from different processes with reduced overhead.

Networks and distributed storage allow data to be shared between devices located anywhere a connection is available. Improvements in capacity and network speeds have enabled a move away from locally attached storage devices and towards centralized storage repositories such as cloud based data storage. These storage systems may be scalable and may range from a single shared folder to a cluster of file servers attached to and controlling racks of disk arrays. These centralized offerings are delivering the promised advantages of security worldwide accessibility and data redundancy. To provide these services storage systems may incorporate Network Attached Storage NAS devices Storage Area Network SAN devices and other configurations of storage elements and controllers in order to provide data and manage its flow. Improvements in distributed storage have given rise to a cycle where applications demand increasing amounts of data delivered with reduced latency greater reliability and greater throughput. Building out a storage architecture to meet these expectations enables the next generation of applications which is expected to bring even greater demand. NetApp storage systems offer NAS and SAN capabilities and support a wide range of standards.

To detect bottlenecks and overcapacity conventional storage systems monitor and report performance data obtained from their various components. This performance data may be used for performance monitoring optimization planning and troubleshooting. In one example system that includes a logical storage volume that stores data to a multitude of underlying physical storage devices the performance of each storage device affects the overall performance of the logical storage volume. Accordingly a storage controller coupled to the storage devices has a performance reporting function that samples performance data of the logical volume and archives saves the performance data and or transmits it to one or more analytical programs. Archiving the data allows a posteriori diagnostic of customer performance issues by support teams without having to artificially reproduce the event. When archiving performance data the storage controller samples the performance data of the logical volume including among other things performance data of the underlying physical storage drives at preconfigured intervals triggered by a system clock e.g. from once every second to once a week in the current embodiment.

The processing resources used to obtain and report the sampled performance data are considered overhead because they are temporarily unavailable to perform the primary functions of serving storage operations. As the number of tracked objects in a storage system increases the amount of data sampled and transferred also increases thereby increasing the amount of overhead. This means that as a storage system grows and more objects including both tracked hardware components and tracked software components are included in the system the overhead of performance data collection grows which may adversely impact the actual performance of the system. In an exemplary embodiment a storage system tracks over 300 objects each of which may in turn track up to hundreds of thousand instances. Longer term object growth is expected to continue making the current approach untenable.

Improvements that reduce system overhead including the overhead of performance monitoring free up resources to handle storage operations thereby allowing the storage system to wring more performance from the same hardware. For these reasons and others improved systems and techniques for performance monitoring are important to the next generation of information storage system.

In the following description specific details are set forth describing some embodiments consistent with the present disclosure. It will be apparent however to one skilled in the art that some embodiments may be practiced without some or all of these specific details. The specific embodiments disclosed herein are meant to be illustrative but not limiting. One skilled in the art may realize other elements that although not specifically described here are within the scope and the spirit of this disclosure. In addition to avoid unnecessary repetition one or more features shown and described in association with one embodiment may be incorporated into other embodiments unless specifically described otherwise or if the one or more features would make an embodiment non functional.

Various embodiments of the present disclosure provide a technique for accumulating data for archiving that prioritizes the collection of some data values to ensure prompt collection while deprioritizing others to avoid delaying other tasks. The data being merged may include any suitable type and amount of data and in an exemplary embodiment the data includes performance metrics of a storage system that are recorded at specified intervals to track performance over time and information on the origin of the performance data. The data to be accumulated is divided by priority determined in part by the data s time dependency the computational burden of collecting the data the hierarchical level by which it is stored and or other suitable criteria. Higher priority data is retrieved first and placed into a memory structure such as a buffer by a first set of process threads. In an exemplary embodiment each thread in the first set has a corresponding buffer. A secondary set of parallel threads running concurrently but scheduled less frequently monitors the memory structure retrieves corresponding lower priority data and inserts the data in the memory structure. The data recorded in the memory structure is then written to a storage device. By combining the high priority data and low priority data a full picture of the system can be stored and correlated including human readable information of all the resources tracked. By reviewing changes in the data over time an administrator can gauge how the system performs under load and where capacity should be added.

In an exemplary embodiment the archival process includes a set of kernel threads. Within the kernel are a set of tracking counters that contain performance values number of instructions performed queue depth throughput latencies resource usage counts events counts and or other performance metrics . The counter values are designated high priority because they are numerous and frequently changing and their quick retrieval may be valuable for other reasons as well. One or more worker threads retrieve the high priority data elements and place them into a memory structure e.g. a buffer . Associated low priority data that is less frequently changed and slower to retrieve is collected later. In the embodiment the low priority data includes data that puts the high priority data in context such as storage device identifiers metric identifiers system identifiers and other suitable data. One or more cleaner threads which run concurrently with the worker threads monitor the memory structures for an indicator requesting for a low priority data element. In one example the cleaner thread detects that a collected counter has a missing value not provided by a worker threads. The cleaner thread determines a method for how the missing value is determined i.e. a translation function for the respective input counter . Based on the translation function the cleaner thread retrieves the lower priority data for the missing value and stores it in the memory structure. The cleaner thread may request this data in the background without interrupting the collection of the higher priority data or other tasks by providing the translation function to a user space daemon which runs the translation function to return the missing lower priority data to the kernel cleaner thread. To improve performance such requests may be grouped to offset the latency of individual requests. In an exemplary storage system the cleaner threads are not in the critical path of the data collection and can cope with the latency of the operation.

Various embodiments provide one or more advantages over conventional systems. For instance in some embodiments this technique allows highly time varying values to be retrieved more promptly by deprioritizing the collection of less variable values. In some embodiments where processing resources are shared between data accumulation tasks and other tasks deprioritizing part of the data accumulation task may reduce the delays experienced by the other tasks and or increase the quantity of performance data the system can support.

Distinguishing high priority and low priority data also allows the underlying data to be stored in different virtual address spaces having different access latency in some embodiments. For example in some embodiments high priority data is stored within the kernel level virtual address space in order to reduce latency while low priority data is stored at a higher level address space having greater access latency. Locating low priority data away from the kernel may avoid data duplication or increasing data management. However low priority data is generally not available in the kernel address space because it is not used internally. Nevertheless the low priority data may be important to the archival process because it allows users to interpret the performance data. Because collection of high priority data and low priority data may proceed in parallel and asynchronously the low priority data may be removed from the critical path. In addition slower retrieval tasks can be batched which may reduce total latency and reduce the burden on interface elements. It is understood that these advantages are merely exemplary and no particular advantage or benefit is necessary or required for any particular embodiment.

Referring first to illustrated is a schematic diagram of a network storage environment according to some embodiments of the present disclosure. The network storage environment includes a storage system connected to one or more host systems sometimes referred to as clients via a network . The storage system receives data transactions from the host systems and performs the data transactions on their behalf. The storage system is merely one example of a computing system that performs a data archival process the collection and storage of data that is ancillary to the primary function of the system. It is understood that the principles and techniques described herein for collecting distributed data while well suited to aggregating performance data apply equally to any other data accumulation tasks and in that regard may be applied to computing systems generally.

While the storage system is referred to as a singular entity it may include any number of computing devices and may range from a single computing system to a system cluster of any size. Accordingly in the illustrated embodiment the storage system includes at least one computing system which in turn includes one or more processors such as a microcontroller or a central processing unit CPU operable to perform various computing instructions. The computing system may also include any combination of memory devices such as random access memory RAM static RAM SRAM or Flash memory a non transitory computer readable storage medium such as a magnetic hard disk drive HDD a solid state drive SSD or an optical memory e.g. CD ROM DVD BD a video controller such as a graphics processing unit GPU a communication interface such as an Ethernet interface a Wi Fi IEEE 802.11 or other suitable standard interface or any other suitable wired or wireless communication interface and or a user I O interface coupled to one or more user I O devices such as a keyboard mouse pointing device or touchscreen.

The storage system includes one or more storage controllers communicatively coupled to one or more storage devices . In various examples the storage devices include hard disk drives HDDs solid state drives SSDs optical drives and or any other suitable volatile or non volatile data storage medium. It is understood that for clarity and ease of explanation only a limited number of storage controllers and storage devices are illustrated although the storage system may include any number of storage devices coupled to any number of storage controllers .

The storage controllers exercise low level control over the storage devices in order to execute perform data transactions on behalf of the storage system and may also perform data transactions on behalf of other computing systems such as network attached host systems . The storage controllers provide an interface for communicating with the storage devices and in that regard may conform to any suitable hardware and or software protocol. In various embodiments the storage controllers include Serial Attached SCSI SAS SATA iSCSI InfiniBand Fibre Channel and or Fibre Channel over Ethernet FCoE controllers. Other suitable protocols include eSATA PATA USB FireWire etc. The physical connection between the storage controllers and the connected storage devices may depend in part on the communication protocol and may take the form of a direct connection e.g. a single wire or other point to point connection a networked connection or any combination thereof. Thus in some embodiments a storage controller is communicatively coupled to a storage device over a network which may include any number of wired and or wireless networks such as a Local Area Network LAN an Ethernet subnet a PCI or PCIe subnet a switched PCIe subnet a Wide Area Network WAN a Metropolitan Area Network MAN the Internet or the like. For redundancy a single storage device may be coupled to more than one storage controller .

As discussed above the storage system may be connected to the host systems over a network such as a local area network LAN an Ethernet subnet a PCI or PCIe subnet a switched PCIe subnet a wide area network WAN a metropolitan area network MAN the Internet or the like. The storage system receives various data transactions e.g. data read commands data write commands etc. from the host systems via the network .

Host system represents any connected computing device capable of issuing data transactions to the storage system . In an example a host system runs one or more applications e.g. word processing virtual machine or database programs that utilize the storage provided by the storage system . In response to a request by a running application the host system formats a data transaction and dispatches it to the storage system to be executed.

In even a modest network storage environment with a limited number of host systems the ability of the storage system to quickly execute data transactions has a measurable impact on the performance of the applications running on the host systems . Accordingly in an embodiment the storage system includes one or more a performance monitors that collect and analyze performance data from storage devices storage controllers and or other elements of the storage system and or of the host systems . In various exemplary embodiments the performance data includes total completed transactions throughput information queue wait time execution time number of concurrent data transactions other performance metrics configuration data and or other data used to interpret the performance data. In some such embodiments the performance monitor periodically collects performance data from counters associated with the storage devices storage controllers and or other elements of the storage system or host systems and stores the data for future reference. By tracking the changes in performance data over time system administrators are able to assess demand identify potential bottlenecks determine when and what to upgrade and recognize components likely to fail. By storing historical data system administrator can also troubleshoot past performance problems.

To obtain accurate measurements it is desirable to collect the performance data as quickly as possible because many of the values are highly time dependent i.e. the values change frequently . Each data transaction may update multiple performance counters and may update them multiple times. Accordingly performance data may change hundreds or even thousands of times per second. However data collection may delay other operations such as serving data to host systems . This is because while the performance monitor may include discrete processing hardware it may also be implemented as software executed by the main processor of the storage system . This brings the performance monitor in contention with the other software of the storage system . In some embodiments the performance monitor may temporarily redirect computing resources away from data serving tasks. In these embodiments especially the efficiency of the performance monitor may have a measurable effect on overall storage system performance.

The operation of the performance monitor of the storage system will now be described from a software perspective. is a schematic diagram of a software environment according to some embodiments of the present disclosure. The software environment represents the programming instructions running on the processing resources of the storage system including the instructions of the performance monitor used to collect and store performance data. The software environment is described in the context of a storage system but is equally characteristic of any computing system.

The software environment includes one or more hierarchical levels of abstraction. Program elements can be understood as running at one or more of the hierarchical levels. To communicate between levels program elements may communicate with each other according to standard protocols often referred to as application programming interfaces APIs . So long as the program elements comply with the API they may remain agnostic to the inner workings of the other program elements and other hierarchical levels.

The kernel runs at the lowest hierarchical level often referred to as the kernel level . The kernel initializes and runs the other program elements of the software environment . Additionally the kernel handles interactions with hardware including the processors memory and or peripheral devices . For example the kernel may handle scheduling the allocation of processing resources among program elements. The kernel may also provide a virtual memory environment for itself and for the other program elements of the software environment by receiving data access requests directed to a virtual address and redirecting the request to the appropriate memory device. Virtual memory address spaces are represented in by clouded boxes and may be associated with a respective hierarchical level e.g. kernel level address space daemon level address space user level address space etc. corresponding to the level of the program that requested the address space to be assigned. Although virtual memory address spaces are drawn separated from the memory it is understood that data read from or written to a virtual memory space is redirected by the kernel to a memory device of the memory . Suitable memory devices include RAM the aforementioned storage devices and or memory mapped peripheral devices. Accordingly in an embodiment the kernel communicates with the storage devices of the storage system via the storage controllers .

In some embodiments the majority of the performance monitor tasks such as updating performance data and storing the data for further reference are performed by the kernel . In such embodiments a portion of the performance monitor is integrated into the kernel . This does not preclude other functions of the performance monitor from being performed by other program elements running at other levels of the hierarchy. Accordingly in some embodiments performance data obtained by the kernel is analyzed to detect long term trends by a program element running at another level of the hierarchy or outside the storage system .

The kernel also oversees the execution of these and other program elements running at the various level of hierarchy. For example the kernel may run one or more daemons at the daemon level . In some embodiments the daemon level is considered part of the kernel level although in other embodiments a discrete daemon level exists for running these program elements. A daemon is a type of program element that runs in the background meaning it typically does not request or receive user input. The kernel may run a daemon to handle various tasks such as data logging tracking configuration data or handling communications to a user console or APIs for other systems to interface to the storage system . A particular type of daemon gateway daemons e.g. user level gateway daemon handle data transactions between hierarchical levels. Accordingly in the illustrated embodiment user level gateway daemon exchanges data between the kernel and the user level program elements running at the user level .

In contrast to daemons user level program elements commonly respond to user input and are considered to run in the foreground. These program elements are often directly visible to the users. Applications e.g. word processing applications database applications programming applications etc. are groups of one or more programming elements the bulk of which are run at the user level . The user level is often a multi tiered construct that includes further hierarchal levels each with its own associated access permissions. As will be described in more detail below by distributing data appropriately throughout the hierarchy of the software environment the collection of the most critical data may be expedited while less time sensitive may be pushed up to higher levels of the hierarchy. be retrieved first 

A technique for data accumulation across the levels the software environment of will now be described with reference to . When used to accumulate performance data the technique may include storing accumulated performance metrics for a particular interval and comparing the metrics over a set of intervals to detect trends in system performance. is a flow diagram of a method of data accumulation according to some embodiments of the present disclosure. It is understood that additional steps can be provided before during and after the steps of method and that some of the steps described can be replaced or eliminated for other embodiments of the method . are diagrams of address spaces within a software environment of a computing system performing data accumulation according to some embodiments of the present disclosure. As described above the address spaces map to physical addresses within a volatile e.g. RAM or non volatile e.g. storage device storage element of the computing system.

The method for data collection and accumulation may be performed in response to any suitable trigger. For example in some embodiments the method is performed at a set time or at a set interval. In some embodiments the method is performed based on a user request. In some embodiments the method is performed based on data exceeding a predefined threshold. For example the method may collect data from a buffer when the buffer indicates that it is full. In some embodiments the method is performed based on combinations of these triggers and others.

Referring to block of and to upon detecting a triggering event the computing system identifies data to be collected. In the illustrated embodiment the data to be collected includes performance metrics representing the performance of a storage system and the performance monitor determines a subset of the performance metrics to be collected for a particular interval. For example the performance metrics may include values tracked by work based counters. In some exemplary embodiments the storage system includes at least one work based counter for each performance aspect e.g. completed data transactions throughput information queue wait time execution time number of concurrent data transactions etc. of each respective storage entity e.g. storage volume storage device storage drive accumulates node etc. . The counters are incremented as work is performed by their corresponding storage entities. It is understood that this is merely one example of a possible type of data. In further embodiments the data to be collected includes any and all other suitable types of data.

Referring to elements of the performance data are stored in one or more different address spaces within the software environment of the computing system. The method is operable to accumulate data across these disparate address spaces. In the illustrated embodiment a first portion of the data is stored within the kernel level address space a second portion of the data is stored within the daemon level address space and a third portion of the data is stored within the user level address space . Data of any type may be stored within any address space and the data stored by one address space may be related to data stored in another.

In an exemplary embodiment the first portion of the data includes performance values such as counter values. These counter values may be stored in any suitable format including a linked list a tree a table such as a hash table an associative array a state table a flat file a relational database and or other memory structure. In the illustrated embodiment the data elements of the first portion each include a counter ID paired with a counter value . There are advantages to storing at least the counter values within the kernel level address space . For example kernel level counter values may be updated quicker because inter level communications are avoided and kernel level counter values may allow performance monitoring tasks to be run in the kernel to take advantage of the kernel s higher execution priority.

Continuing with the illustrated embodiment the second portion and or third portion includes data related to that of the first portion . For example data within the second portion and or third portion may identify which storage element a counter is monitoring what system or subsystem contains the storage element what aspect of performance a counter measures what host or host groups a counter is monitoring and or other suitable aspects of performance. In contrast to the counter values and other data within the first portion data elements within the second portion and or third portion may be updated less frequently e.g. once a second or less .

Data within the second portion and or third portion may be stored in any suitable format including a linked list a tree a table such as a hash table an associative array a state table a flat file a relational database and or other memory structure. In the illustrated embodiment some data elements of the second portion and third portion include a counter ID paired with a corresponding storage element ID and some elements include a counter ID paired with a corresponding performance metric . It is understood that the illustrated data types are merely exemplary.

Referring still to block of the computing system may identify high priority data elements and or low priority data elements within the set of data to be collected. In many embodiments an administrator and or system designer determines the appropriate category for a data element based on its behavior. However in some embodiments the computing system determines which data elements are high priority and low priority based on a set of heuristics. In various examples data elements are labeled high priority based on their time dependent nature based on the computational burden of collecting the data based on their respective level within the hierarchy and or other suitable criteria. For example counter values are often updated several orders of magnitude more often than other data elements and therefore may be designated high priority. In some embodiments because data elements stored within the kernel level address space can be retrieved fastest they are designated high priority whereas data elements stored within other address spaces are designated low priority because of the latency involved in retrieving the data. Information regarding the high priority and low priority designations may be stored in a preset file.

Referring to block of and to the kernel initializes a memory structure within the kernel level address space to hold the collected data. The memory structure may take any suitable format including a linked list a tree a table such as a hash table an associative array a state table a flat file a relational database and or other memory structure. In an embodiment structure is a buffer that stores counter values at known offsets based on the ordering of the counters and their sizes.

Referring to block of the kernel initializes a first accumulation thread sometimes referred to as a worker thread. A thread in this example includes a sequence of logical instructions. Referring to block the first accumulation thread retrieves the data elements designated as high priority and creates a copy of the data in the memory structure . In the illustrated embodiment the counter values within the first portion are designated high priority and are retrieved by the first accumulation thread.

In embodiments in which some data is stored in address spaces other than the kernel level address space the kernel may request the data from the respective owner such as daemons running at the daemon level and or program elements running at the user level . For example while the kernel may store data addressed to the user level address space on behalf of the user level program elements the kernel may not understand the significance of the data being stored. Accordingly to retrieve a particular data element the kernel may request the element from the respective user level program element rather than retrieving it from memory directly.

The request for data from other virtual address spaces may be made directly or through an intermediary. In one such embodiment a user level gateway daemon handles interactions between the first accumulation thread running in the kernel and the program elements at the user level . The gateway daemon receives requests for data from the kernel and generates a corresponding request in a protocol understood by the respective user level program element . For example in some embodiments a Remote Procedure Call RPC protocol typically used to request data between user level program elements running on different systems is repurposed by kernel and the gateway daemon to exchange data. RPC protocols are common and well documented and the user level program elements may already support RPC commands without further modification.

Referring to block of and to once retrieved the first accumulation thread stores a copy of the high priority data element within the memory structure . The first accumulation thread may also store an indicator of any associated low priority data element within the memory structure. In an example the first accumulation thread creates a reserved area in memory within the memory structure for the corresponding low priority data element. The first accumulation thread repeats the process of blocks until all the high priority data elements have been retrieved and stored. In on such embodiment the memory structure includes a buffer used to collect the data. Within the buffer space is reserved to hold the data. The high priority data counters are collected first leaving room in the buffer for the low priority data counters which may be interspersed between the high priority counters. Before the buffer is stored to disk the missing low priority data retrieved and filled in as described in more detail below. In brief a low priority data thread scans the counter configuration for this buffer entry and detects the address of the low priority data. It then requests the low priority data from a daemon via an RPC.

Referring to block the kernel initializes a second accumulation thread sometimes referred to as a cleaner thread. The second accumulation thread may be run in parallel to and concurrently with the first accumulation thread such that during some period of time both threads are running. The kernel may assign the second accumulation thread a lower execution priority than the first accumulation thread. Referring to block the second accumulation thread analyzes the memory structure for indicators signifying low priority data to be retrieved. For example the second accumulation thread may detect reserved memory areas set aside by the first accumulation thread. In many embodiments the second accumulation thread selectively retrieves only the low priority data that corresponds to the high priority data being collected e.g. retrieving only those storage entity identifiers that correspond to the counter values currently being collected . This may reduce the number of requests for irrelevant data or unused data. Additionally or in the alternative the second accumulation thread may identify the low priority data to retrieve based on the preset file of block .

Referring to block the second accumulation thread retrieves the low priority data from a respective address space. In embodiments in which low priority data is stored within the kernel level address space the kernel may retrieve it directly. In embodiments in which low priority data is stored in other address spaces the kernel requests the data from the respective owner such as daemons running at the daemon level and or program elements running at the user level . In one such embodiment the kernel requests the data from a daemon by providing the daemon an RPC request for the data. Similar to the process describe above the request may be made directly or through an intermediary. In one such embodiment a user level gateway daemon handles interactions between the first accumulation thread running in the kernel and the program elements at the user level . The gateway daemon receives requests from data from the kernel and generates a corresponding request in a protocol understood by the respective user level program element . For example the user level gateway daemon may provide RPC requests for the data to the user level program elements on behalf of the kernel . To reduce the number of inter level requests data requests may be grouped into a batch a single request for multiple data elements by the kernel and or the gateway daemon .

Referring to block of and to the second accumulation thread stores a copy of the retrieved low priority data element in the memory structure . As can be seen the first accumulation thread has continued to run and in the illustrated embodiment has written subsequent high priority data elements to the memory structure in the interim. The second accumulation thread repeats the process of blocks until all the low priority data elements have been retrieved and stored.

Referring to block of and to once both threads have update the memory structure the kernel stores the memory structure including the high priority data elements and the low priority data elements in a non volatile storage medium such as one or more of the aforementioned storage devices . In the example where the memory structure includes performance data accumulated over an interval of time the stored memory data is compared with other performance data accumulated over other intervals. By tracking the changes in performance data over time system administrators are able to assess demand identify potential bottlenecks and performance limiting components determine when and what to upgrade recognize components likely to fail and or make other assessments of the computing system as shown in block . For example if a buffer utilization is unacceptably high but it is only an isolated occurrence it may indicate a one time spike in demand. In contrast if the buffer utilization remains consistently high it may indicate that the storage system should be upgraded to better handle the load. In another example if storage device utilization remains below a threshold for a certain number of intervals it may indicate that the respective data volume could be moved to a slower storage device. In an embodiment the high priority data is collected into a buffer. The buffer is then checked by the low priority data thread for low priority counters to update and if there are some gets the counters and stores them into the buffer. Once this is done the buffer is complete and the low priority thread proceeds to commit the data to a file on disk.

There are a number of advantages to dividing the data elements into high priority and low priority categories. For example as discussed above data elements such as performance metrics may be time varying. In the time it takes to retrieve data from a given counter the next subsequent counter may have updated. By identifying and separating time critical data these high priority data elements may be retrieved more rapidly. In addition separating high priority and low priority data allows high priority data to be stored at a level of hierarchy closer to the kernel . This may improve the retrieval time of the high priority data. Likewise shifting low priority data to higher levels of the hierarchy may free up kernel resources for other time dependent tasks.

In an embodiment the low priority data includes configuration information such as labels describing different parts of the system such as names of storage volumes or location IDs of hardware. In the embodiment the data is stored at the daemon level and may be requested outside of the archival process. For example when a user requests information about a part of the system a numerical ID is passed to the kernel when the data is in the kernel and on retrieval the ID is matched to a textual description. Similarly when archiving kernel counters the names of the parts of the system are considered to be low priority data available outside of the kernel and requested via RPC to the daemon holding them. The request may be made using the numerical ID of the part of the system to identify. This solution allows the kernel to archive information on the performance of the system that is fully self contained.

Furthermore this division allows the first accumulation thread the more time critical of the two to be run more frequently or run at a higher priority level than the second accumulation thread. It may also allow the first accumulation thread to collect more data in an allotted run time. Performance data accumulation is often ancillary to the functioning of the computing system. For example in a storage system performance monitoring is ancillary to servicing data transactions and may even be an impediment. By assigning the first accumulation thread a priority that is greater than for example servicing data transactions and assigning the second accumulation thread a priority that is less than servicing data transactions the data transactions are only pre empted by the most critical portion of method .

In some embodiments requesting and exchanging data between hierarchical levels is slower than intra level exchanges by an order of magnitude or more. By designating data within user level address spaces as low priority these exchanges can be performed in parallel with the retrieving of the high priority data. This takes the inter level exchanges out of the critical path. In addition these inter level exchanges can be batched to increase bandwidth which may reduce total latency cost per counter and reduce the burden on interface elements such as the gateway daemon . It is understood that these advantages are merely exemplary and no particular advantage or benefit is necessary or required for any particular embodiment.

Embodiments of the present disclosure can take the form of a computer program product accessible from a tangible computer usable or computer readable medium providing program code for use by or in connection with a computer or any instruction execution system. For the purposes of this description a tangible computer usable or computer readable medium can be any apparatus that can store the program for use by or in connection with the instruction execution system apparatus or device. The medium can be an electronic magnetic optical electromagnetic infrared or a semiconductor system or apparatus or device . In some embodiments one or more processors of the storage system execute code to implement the actions described above.

Accordingly a system and method for collecting and aggregating data is provided. In some exemplary embodiments the method of prioritizing accumulation of time dependent data includes identifying a plurality of data elements to be retrieved that includes a high priority data element and a low priority data element. A first data retrieval operation is performed that includes retrieving the high priority data element storing a copy of the high priority data element in a memory structure and reserving a memory space in the memory structure for the low priority data element based on the low priority data element corresponding to the high priority data element. In parallel with the first data retrieval operation a second data retrieval operation is performed that includes analyzing the memory structure to detect the reserved memory space upon detecting the reserved memory space retrieving the low priority data element and storing a copy of the low priority data element in the reserved memory space. In one such embodiment the low priority data element is retrieved from a user level virtual address space which may include providing a Remote Procedure Call RPC request for the low priority data element to a user level program element.

In further exemplary embodiments the data management method includes performing a first logical sequence that in turn includes retrieving a plurality of data elements and storing the plurality of data elements in a memory structure. The data management method further includes performing a second logical sequence concurrent with the first where the second logical sequence includes monitoring the memory structure to identify a related data element corresponding to at least one element of the plurality of data elements retrieving the related data element and storing the related data element in the memory structure. In one such embodiment the plurality of data elements include a performance metric associated with a storage element and the related data element includes at least one of an identifier of the storage element and an identifier of the performance metric.

In yet further exemplary embodiments the apparatus comprises a non transitory tangible computer readable storage medium storing a computer program. The computer program has instructions that when executed by a computer processor carry out identifying a first data element based on at least one of time dependency an access time and a corresponding hierarchical level performing a first sequence including retrieving the first data element storing a copy of the first data element in a memory structure and storing an indicator in the memory structure wherein the indicator indicates a second data element associated with the first data element performing a second sequence in parallel with the first sequence wherein the second sequence includes analyzing the memory structure to detect the indicator of the second data element retrieving the second data element and storing a copy of the second data element in the memory structure.

Although illustrative embodiments have been shown and described a wide range of modification change and substitution is contemplated in the foregoing disclosure and in some instances some features of the embodiments may be employed without a corresponding use of other features. One of ordinary skill in the art would recognize many variations alternatives and modifications. Thus the scope of the invention should be limited only by the following claims and it is appropriate that the claims be construed broadly and in a manner consistent with the scope of the embodiments disclosed herein.

