---

title: Correlation and associated display of virtual machine data and storage performance data
abstract: Virtual machine data records are obtained from a virtual system manager that manages one or more virtual machines. Storage data records are obtained from a storage controller. The virtual machine data records include one or more particular virtual machine data records relating to a particular virtual machine and identify a particular volume that is configured for use by the particular virtual machine. The storage data records including one or more particular storage data records that specify performance information associated with the particular volume. Based on information in the particular virtual machine data records and information in the particular storage data records, it is determined that the particular storage data records are related to the particular volume used by the particular virtual machine. Graphical user interface(s) displaying virtual machine information relating to the particular virtual machine in association with volume performance information relating to the particular volume are displayed.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09052938&OS=09052938&RS=09052938
owner: Splunk Inc.
number: 09052938
owner_city: San Francisco
owner_country: US
publication_date: 20140415
---
The disclosure generally relates to determining and presenting information relating to virtual machines and the performance of volumes used by the virtual machines for storage.

In a virtualized environment a large number of virtual machines can operate on a single physical host. Many customers elect to use virtual machines for their data computing needs due to the various advantages that a virtualized environment can offer over a non virtualized environment such as greater availability lower costs and simpler upgrades. When a virtual machine is created the physical host allocates resources such as central processing units CPUs and memory to the virtual machine. For disk space the virtual machine may use the storage resources of a storage provider that is different from the physical host that provides CPU and memory resources to the virtual machine. For example the data generated and used by the virtual machine may be stored in volumes managed by a storage controller such as a filer that operates on a separate machine than the physical host of the virtual machine and uses software provided by a different vendor than the vendor of the virtualization software.

A performance issue originating in the storage environment such as a problem affecting a particular volume or a storage controller that manages the particular volume can affect the performance of the virtual machine that utilizes the particular volume. However in other cases the poor performance of a virtual machine can be attributed to a different source for example the problem may be specific to the virtual machine itself to the communication network used by the virtual machine the underlying physical host or some other entity. Many times it is difficult for customers to pinpoint the source of the problem. Better approaches for presenting information to assist a customer in diagnosing the source of a problem that affects a virtual machine are needed.

The approaches described in this section are approaches that could be pursued but not necessarily approaches that have been previously conceived or pursued. Therefore unless otherwise indicated it should not be assumed that any of the approaches described in this section qualify as prior art merely by virtue of their inclusion in this section.

Example embodiments which relate to correlation and associated display of virtual machine data and storage performance data are described herein. In the following description for the purposes of explanation numerous specific details are set forth in order to provide a thorough understanding of the present invention. It will be apparent however that the present invention may be practiced without these specific details. In other instances well known structures and devices are shown in block diagram form in order to avoid unnecessarily obscuring the present invention.

This overview presents a basic description of some aspects of embodiment s of the present invention. It should be noted that this overview is not an extensive or exhaustive summary of aspects of the embodiment. Moreover it should be noted that this overview is not intended to be understood as identifying any particularly significant aspects or elements of the embodiment s nor as delineating any scope of the embodiment s in particular nor the invention in general. This overview merely presents some concepts that relate to example embodiments in a condensed and simplified format and should be understood as merely a conceptual prelude to a more detailed description of example embodiments that follows below.

In an embodiment a data management system obtains data from both a virtual system manager that manages the virtualized environment and a storage controller that manages the storage of information in one or more disks. The virtual system manager may provide a variety of information relating to the virtual machines managed by the virtual machine manager or the physical hosts of the virtual machines including but not limited to names of the entities in the virtualized environment changes in the configuration of various virtual machines information relating to migrations of the virtual machines amount of memory or CPU resources assigned to the virtual machines performance of the virtual machines an identification of volumes that the virtual machines are configured to use and the amount of storage space utilized in the volumes by various virtual machines.

Similarly the storage controller may provide a variety of information relating to the entities managed by the storage controller or the storage controller itself such as the virtual machines managed by the storage controller the names of and hierarchy between different entities of the storage environment and performance of the volumes or disks managed by the storage controller or the storage controller itself.

Records obtained from the virtual system manager and records obtained from the storage controller may be correlated. For example based on a determination that one or more particular storage data records relate to a particular volume used by a particular virtual machine to which one or more particular virtual machine data records relate the data management system may correlate the one or more particular storage data records with the one or more particular virtual machine data records.

Based on the correlated data records the data management system may display virtual machine data in association with storage data. In one embodiment performance information for a particular virtual machine is displayed on a first screen and in response to a user selection performance information specifically for the volume that the particular virtual machine uses is displayed on the next screen within the same application.

In another embodiment the data management system displays an interface that identifies the capacity of a volume the path that a virtual machine uses to access the volume and performance information for the particular volume. The capacity of the volume the path that a virtual machine uses to access the volume and the performance of the virtual machine may be determined based on records obtained from the virtual system manager and the volume performance information may be identified based on records obtained from the storage controller.

The associated display of virtual machine data and the storage data may allow a user to more easily diagnose the root cause of a performance issue affecting a virtual machine.

Various modifications to the preferred embodiments and the generic principles and features described herein will be readily apparent to those skilled in the art. Thus the disclosure is not intended to be limited to the embodiments shown but is to be accorded the widest scope consistent with the principles and features described herein.

Other embodiments include without limitation a non transitory computer readable medium that includes processor executable instructions that enable a processing unit to implement one or more aspects of the disclosed methods as well as a system configured to implement one or more aspects of the disclosed methods.

There is tremendous growth in the amount of data generated in the world. With decreasing storage costs and seemingly infinite capacity due to cloud services there are fewer reasons to discard old data and many reasons to keep it. As a result challenges have shifted towards extracting useful information from massive quantities of data.

Mining a massive dataset is non trivial but a more challenging task is to cross correlate and mine multiple datasets from various sources. For example a datacenter monitors data from thousands of components the log format and collection granularities vary by component type and generation. The only underlying assumption that can be made is that each component has a notion of time either via timestamps or event sequences that is captured in the logs. As the quantity and diversity of data grow there is an increasing need for performing full text searches to mine the data.

Another challenge is that a large fraction of the world s data is unstructured making it difficult to index and query using traditional databases. Even if a dataset is structured the specifics of the structure may evolve with time for example as a consequence of system upgrades or more less restrictive data collection retention policies.

SPLUNK ENTERPRISE is software produced and sold for on premise and cloud use by Splunk Inc. of San Francisco Calif. SPLUNK ENTERPRISE is a comprehensive system that generates stores retrieves and searches event data. SPLUNK ENTERPRISE has gained particular appeal in the market for deriving events from unstructured data and machine data. It is the leading software for providing real time operational intelligence enabling organizations to collect index and harness machine generated big data coming from the websites applications servers networks mobile devices etc. that power their businesses.

At a high level SPLUNK ENTERPRISE can take raw data unstructured data or machine data such as data in Web logs syslogs sensor readings etc. divide the data up into portions and optionally transform at least part of the data in these portions to produce time stamped events. The software derives the time stamp for each event by extracting it from the event data itself or by interpolating an event s time stamp relative to other events for which the software can derive a time stamp. SPLUNK ENTERPRISE then stores the events in a time series data store against which it can run queries to retrieve events that meet specified criteria such as having certain keywords and or having certain value s for certain defined field s .

SPLUNK ENTERPRISE is particularly noteworthy for employing a so called late binding schema. As noted an event in SPLUNK ENTERPRISE typically contains a portion of raw data or a transformed version of such . To run queries against events other than those involving keyword searches a schema can be developed. Such a schema can include extraction rules for one or more fields. Each field can be defined for a subset of the events in the data store and an extraction rule can specify how to extract a value from each of the subset of events for which the field has been defined. The extraction rule for a field is often defined using a regular expression regex rule and it associates event data with a logical type of information that is contained within an event for which it is defined. The term late binding schema refers to a system such as in SPLUNK ENTERPRISE which does not define the schema at index time as with database technology rather in a system involving late binding schema the schema can be developed on an ongoing basis up until the time it needs to be applied which is query time as a query often specifies the criteria for events of interest in terms of events having specified value s for specified field s . As a data analyst learns more about the data in stored events using a late binding schema he can continue to develop the schema up until the next time it is needed for a query.

Because SPLUNK ENTERPRISE maintains the underlying searchable raw data and enables application of a late binding schema it has great power to enable dynamic investigation of issues that arise as a data analyst learns more about the data stored in the system s events.

As discussed herein time series data and time series machine data may include among other things a series or sequence of data points generated by one or more data sources computing devices or sensors. Each data point may be a value a small segment of data or a large segment of data and each data point may be associated with a timestamp or be associated with a particular point in time that provides the basis for a timestamp for the data point. The series of data points or values statistics derived from the data points may be plotted over a time range or time axis representing at least a portion of the time range. The data can be structured unstructured or semi structured and can come from files directories network packets network events and or sensors. Unstructured data may refer for example to data whose structure is not fully understood or appreciated at the time the data is obtained by a data storage system or it may refer to data that was generated without a particular schema in mind to facilitate the extraction of values for fields in the data during a search on the data. Machine data generated by for example data sources within an enterprise network environment is generally considered to be unstructured data. The visualization of such time series data may be used to display statistical trends over time. The time series machine data collected from a data source may be segmented or otherwise transformed into discrete events where each event can be associated with a timestamp.

An event may include a single record of activity from a particular data source associated with a single timestamp. Such an event may correspond to for example one or more lines in a log file or other data input. Further events may be derived from processing or indexing machine data as described herein or may include other kinds of events or notable events described herein. Events can also correspond to any time series data such as performance measurements of an IT component e.g. a computer cluster node host virtual machine etc. a sensor measurement etc.

In an example a field extractor within an enterprise network environment may be configured to automatically identify e.g. using regular expression based rules delimiter based rules etc. certain fields in the events while the events are being created indexed and or stored. Alternatively one or more fields can be identified within the events and added to the field extraction rules used by the field extractor to identify fields within the events by a user using a variety of techniques. Additionally fields that correspond to metadata about the events such as a timestamp host source and source type for an event may also be created such fields may in some cases be referred to as default fields if they are determined automatically for all events at the time such events are created indexed and or stored.

In some implementations a given tag or alias may be assigned to a set of two or more fields to identify multiple fields that correspond to equivalent pieces of information even though those fields may have different names or be defined for different sets of events. A set of tags or aliases used to identify equivalent fields in this way may be referred to as a common information model.

Data generated by various data sources may be collected and segmented into discrete events each event corresponding to data from a particular point in time. Examples of such data sources include but are not limited to web servers application servers databases firewalls routers operating systems software applications executable at one or more computing devices within the enterprise data system mobile devices sensors etc. The types of data generated by such data sources may be in various forms including for example and without limitation server log files activity log files configuration files messages network packet data performance measurements or metrics sensor measurements etc.

The indexer determines a time stamp for each event at block . The time stamp can be determined by extracting the time from data in the event or by interpolating the time based on time stamps from other events. In some cases a time stamp can be determined from the time the data was received or generated. The indexer associates the time stamp with each event at block . For example the time stamp may be stored as metadata for the event.

At block the data included in a given event can be transformed. Such a transformation can include such actions as removing part of an event e.g. a portion used to define event boundaries extraneous text characters etc. or removing redundant portions of an event. A user can specify a portion to remove using a regular expression or any similar method.

Optionally a key word index can be built to facilitate fast keyword searching of events. To build such an index in block the indexer identifies a set of keywords contained in the events. At block the indexer includes each identified keyword in an index which associates with each stored keyword pointers to each event containing that keyword or locations within events where that keyword is found . When an indexer receives a keyword based query the indexer can then consult this index to quickly find those events containing the keyword without having to examine again each individual event thereby greatly accelerating keyword searches.

The indexer stores events in a data store at block . The data can be stored in working short term and or long term memory in a manner retrievable by query. The time stamp can be stored along with each event to help optimize searching the events by time range.

In some instances the stored data includes a plurality of individual storage buckets each corresponding to a time range. An event can then be stored in a bucket associated with a time range inclusive of the event s time stamp. This not only optimizes time based searches but it can allow events with recent time stamps that may have a higher likelihood of being accessed to be stored at preferable memory locations that lend to quicker subsequent retrieval such as flash memory instead of hard disk media .

Data stores may be distributed across multiple indexers each responsible for storing and searching a subset or buckets of the events generated by the system. By distributing the time based buckets among the indexers the indexers can find events responsive to a query in parallel using map reduce techniques each returning their partial responses for specific buckets to the query to a search head that combines the results together to answer the query.

At block the search head is responsible for analyzing the search query to determine what part can be delegated for execution by indexers and what part needs to be executed by the search head. Streaming commands can be trivially delegated to the indexers. Conversely aggregating commands are more complex to distribute.

The search head can perform optimization steps in order to make the search more efficient. As mentioned above the indexers may create an index of keywords. In one optimization before the search starts executing the search head determines the time range required for the search and a set of common keywords that all matching events must have. The retrieval phase uses these parameters to query the indexers for a superset of the eventual results. The indexers return the superset of results that the search head can perform a filtering stage on. The filtering stage performs field extraction on the superset to arrive at a reduced set of search results.

In another optimization to achieve better computation distribution and minimize the amount of data transferred between indexers and the search head many aggregating commands implement a map operation which the search head can delegate to the indexers while executing the reduce operation locally. shows an example of a search query received from a client that the search head can split into two parts one part to be executed by indexers and one part to be executed by the search head . Here the search query makes the indexers responsible for counting the results by host and then sending their results to the search head. The search head then performs the merging . This achieves both computation distribution and minimal data transfer.

The search head distributes the indexer search query to one or more distributed indexers. The search query may contain one or more regular expressions that the indexer is to apply to any event data that is found to fall within the parameters of the regular expression. These indexers can include those with access to data stores having events responsive to the query. For example the indexers can include those with access to events with time stamps within part or all of a time period identified in the query.

At block one or more indexers to which the query was distributed searches its data store for events responsive to the query. To determine events responsive to the query a searching indexer finds events specified by the criteria in the query. This criteria can include that the events have particular keywords or contain a specified value or values for a specified field or fields because this employs a late binding schema extraction of values from events to determine those that meet the specified criteria occurs at the time this query is processed . It should be appreciated that to achieve high availability and to provide for disaster recovery events may be replicated in multiple data stores in which case indexers with access to the redundant events and not assigned as the primary indexer for the events would not respond to the query by processing the redundant events. In an example the indexer finds events that it is the primary indexer for that fall within a block of time specified by the one or more regular expressions. The indexer then processes the contents of the events using the one or more regular expressions extracting information associated with fields specified in the one or more regular expressions. The indexers can either stream the relevant events back to the search head or use the events to calculate a partial result responsive to the query and send the partial result back to the search head. At block the search head combines or reduces all of the partial results or events received from the parallel processing indexers together to determine a final result responsive to the query.

Data intake and query system and the processes described with respect to are further discussed and elaborated upon in Carasso David. Exploring Splunk Search Processing Language SPL Primer and Cookbook. New York CITO Research 2012 and in Ledion Bitincka Archana Ganapathi Stephen Sorkin and Steve Zhang. Optimizing data analysis with a semi structured time series database. In SLAML 8070. Each of these references is hereby incorporated by reference in its entirety for all purposes.

SPLUNK ENTERPRISE can accelerate some queries used to periodically generate reports that upon each subsequent execution are intended to include updated data. To accelerate such reports a summarization engine periodically generates a summary of data responsive to the query defining the report for a defined non overlapping subset of the time period covered by the report. For example where the query is meant to identify events meeting specified criteria a summary for a given time period may include only those events meeting the criteria. Likewise if the query is for a statistic calculated from events such as the number of events meeting certain criteria then a summary for a given time period may be the number of events in that period meeting the criteria.

Because the report whenever it is run includes older time periods a summary for an older time period can save the work of having to re run the query on a time period for which a summary was generated so only the newer data needs to be accounted for. Summaries of historical time periods may also be accumulated to save the work of re running the query on each historical time period whenever the report is updated.

A process for generating such a summary or report can begin by periodically repeating a query used to define a report. The repeated query performance may focus on recent events. The summarization engine determines automatically from the query whether generation of updated reports can be accelerated by creating intermediate summaries for past time periods. If it can then a summarization engine can periodically create a non overlapping intermediate summary covering new data obtained during a recent non overlapping time period and stores the summary in a summary data store.

In parallel to the creation of the summaries the query engine schedules the periodic updating of the report defined by the query. At each scheduled report update the query engine determines whether intermediate summaries have been generated covering parts of the time period covered by the current report update. If such summaries exist then the report is based on the information from the summaries optionally if additional data has been received that has not yet been summarized but that is required to generate a complete report then the query is run on this data and together with the data from the intermediate summaries the updated current report is generated. This process repeats each time an updated report is scheduled for creation.

Search and report acceleration methods are described in U.S. Pat. No. 8 589 403 issued on Nov. 19 2013 and U.S. Pat. No. 8 412 696 issued on Apr. 2 2011 both of which are hereby incorporated by reference in their entirety for all purposes.

The data processing techniques described herein are suitable for use by systems deployed in a variety of operating environments. illustrates an example network based system of computing devices in which the described techniques may be practiced according to an embodiment.

Data management system represents one or more computing devices that may collect index and correlate data from both virtual system manager and storage controller .

VM data collection node obtains data relating to virtual environment from virtual system manager . In an embodiment VM data collection node collects the data by making calls to an Application Program Interface API made available by virtual system manager .

Virtual environment comprises virtual machine manager which manages virtual machines and and virtual machine manager which manages virtual machines and . Virtual machine managers and may be hypervisors that provide services such as allocation and partitioning which allow the respective virtual machines that they manage to share the same physical host. Virtual system manager manages the virtual machine managers and virtual machines in virtual environment by providing services such as configuration of virtual machine managers and virtual machines performance monitoring and optimization of resource usage. Virtual system manager may operate on a virtual machine within virtual environment or on a physical or virtual machine outside virtual environment . VM data collection node may be configured to re structure or otherwise modify the data obtained from virtual system manager to conform to a particular format before forwarding the data to VM data manager app at data management system .

VM data manager app stores the data received from virtual system manager in one or more virtual machine data indexes in VM data repository which is communicatively coupled to data management system . VM data manager app comprises instructions for the display of graphical interfaces that may be presented to customers for the monitoring of events occurring in virtual environment or for troubleshooting any problems affecting virtual environment . VM data manager app may cause the graphical interfaces to display at a customer device such as client device . Client device may be any computing device including but not limited to a personal computer laptop mobile phone mobile device tablet computer or a wearable computing device.

Storage data collection node collects data relating to storage environment from storage controller . In on embodiment storage data collection node collects the data by making calls to an API made available by storage controller . VM data collection node and storage data collection node may be forwarders such as forwarders in .

Storage controller manages the storage of data across various storage units such as storage units and . Storage units and may each be separate physical disks managed by storage controller . Storage controller performs a variety of storage management tasks such as selecting the layout of data across different storage units and monitoring the performance of different storage units.

Virtual machines in virtual environment may store their data in storage environment . Storage controller may present portions of different storage units as a single contiguous volume to a virtual machine. For example virtual machine may send a request to storage controller to store or retrieve data from a particular volume and storage controller may determine the locations at which to store the data or from which to retrieve the data in response to the request. The determined locations may span multiple storage units.

Storage data collection node may be configured to re structure or otherwise modify the data obtained from storage controller to conform to a particular format before forwarding the data to storage data manage app at data management system . VM data manager app and storage data manager app may both modify the data they respectively obtain to conform to the same format for easier retrieval of both types of data.

Storage data manager app stores the data received from storage controller in one or more storage data indexes in storage data repository which is communicatively coupled to data management system . Storage data manager app also comprises instructions for the display of graphical interfaces that may be presented to customers for monitoring events occurring in storage environment or for troubleshooting any problems affecting storage environment . Storage data manager app may cause the graphical interfaces to display at a customer device such as client device .

Search unit in VM data manager app may search for data in both VM data repository and storage data repository and may perform a correlation of the retrieved data. Thus VM data manager app may have access to both virtual machine data records stored in VM data repository and storage data records stored in storage data repository . In other embodiments storage data manager app may perform the searching and correlation and in such an embodiment search unit may be a component of storage data manager app .

In an embodiment software used to implement virtual environment including software which when executed performs the functionality of virtual system manager and virtual machine managers and is provided by vendor of virtualization software that is different than the storage vendor that provides software used to implement storage environment . The storage vendor may provide software that performs the functions of storage controller . For example a virtualization company such as VMware Inc. may provide software that performs the functions of virtual system manager and virtual machine managers and and a storage company such as NetApp Inc. or EMC Corp. may provide software that performs the functions of storage controller .

At block VM data manager app obtains from virtual system manager which manages one or more virtual machines virtual machine data records including one or more particular virtual machine data records relating to a particular virtual machine and identifying a particular volume that is configured for use by the particular virtual machine. The received virtual machine data records may relate to a plurality of virtual machines including the particular virtual machine to which the particular virtual machine data records relate.

The virtual machine data records may indicate a variety of information including but not limited to the performance of virtual machines and e.g. CPU usage and or memory usage by each of the virtual machines etc. the performance of virtual machine managers and which host virtual machines and the names of virtual machines and and virtual machine managers and the topology of virtual environment e.g. for each virtual machine an identification of the physical machine that hosts the virtual machine and for each virtual machine the virtual system manager that manages the virtual machine etc. and tasks and events that occurred within virtual environment e.g. a log of configuration changes to virtual machines and etc. . The virtual machine data records may identify a volume that a virtual machine uses by specifying the name of the volume and identifying the storage controller that manages the volume such as by specifying the storage controller s Internet Protocol IP address. In an embodiment the virtual machines in virtual environment use volumes in storage environment for their data storage needs and the volumes in storage environment are managed by a separate device and or application than virtual system manager . Thus virtual system manager may not provide performance information for any of the volumes used by the virtual machines in virtual environment .

Data management system may receive virtual machine data records from virtual system manager via data collector node . In an embodiment data collector node obtains the data from virtual system manager by periodically issuing requests for particular information to virtual system manager . Data collector node may modify data obtained from virtual manager such as reformatting the data to conform to a particular format before forwarding the data to data management system .

At block VM data manager app stores the virtual machine data records. The virtual machine data records may be stored within indexes at VM data repository . In some embodiments data management system may modify the virtual machine data records to conform to a particular format in addition to or instead of any modifications that data collector node performs to the virtual machine data records.

At block storage data manager app obtains from a storage controller storage data records including one or more particular storage data records that specify performance information associated with the particular volume. The received storage data records may relate to a plurality of volumes including the particular volume to which the particular storage data records relate.

Storage data records may indicate a variety of information including but not limited to the performance of various storage entities in storage environment such as storage controller storage units volume etc. Performance of the various storage entities may be indicated using any of a variety of metrics including an average or median amount of latency for requests sent to the storage entity or as an amount of input output operations performed per second by the storage entity. Storage data records may specify the names and capacity of various storage entities in storage environment e.g. storage units and volume the Internet Protocol IP addresses of storage controller and the topology of the storage environment e.g. an identification of which storage units and which volumes are managed by which storage controllers etc. . The storage data records may identify a volume by specifying a volume name and identifying the storage controller that manages the volume.

In an embodiment storage controller is a separate device and or application than virtual system manager and storage controller only provides information about the storage environment and does not provide any general information about the performance of virtual machines that utilize storage entities in storage environment . For example although storage data records received from storage controller may indicate an amount of disk space utilized by a virtual machine the storage data records may not specify how the virtual machine is performing.

Storage data manager app may receive storage data records from storage controller via storage data collection node . In an embodiment storage data collection node obtains the data from storage controller by periodically issuing requests for particular information to storage controller . Storage data collection node may modify data obtained from storage controller such as by reformatting the data to conform to a particular data format before forwarding the data to data management system .

At block storage data manager app stores the storage data records. The storage data records may be stored within indexes at storage data repository . In some embodiments data management system may modify the storage data records to conform to a particular format in addition to or instead of any modifications that data collector node performs to the storage data records.

Although the storage data records and the virtual machine data records may be obtained by different data collection nodes and or stored by different applications VM data manager app and data manager app may modify the virtual machine data records and storage data records respectively to conform to the same data format. Such an approach may allow for easier correlation of virtual machine data records and storage data records.

At block VM data manager app determines based on information in the particular virtual machine data records and information in the particular storage data records that the particular storage data records relate to the particular volume used by the particular virtual machine.

At block VM data manager app causes in response to the determination display of one or more graphical user interfaces displaying virtual machine information relating to the particular virtual machine in association with volume performance information relating to the particular volume where the virtual machine information is determined based on the particular virtual machine data records and the volume performance information is determined based on the particular storage data records.

Example processes for the indexing searching and display of virtual machine information is described in U.S. patent application Ser. No. 14 167 316 titled Correlation For User Selected Time Ranges Of Values For Performance Metrics Of Components In An Information Technology Environment With Log Data From That Information Technology Environment filed Jan. 29 2014 the entire contents of which are hereby incorporated by reference for all purposes as if set forth herein.

At block VM data manager app causes display of a first graphical interface displaying information about a virtual machine including performance information relating to the virtual machine and an identification of a volume associated with the virtual machine. The first graphical interface may be displayed at display device .

The first graphical interface may indicate information determined based on information received from virtual machine managers such as virtual machine manager and not based on information received from storage controller such as storage controller .

Interface depicts an example virtual machine view which identifies different attributes of a particular virtual machine including the name of the virtual machine item the operating system item power state and the status of tools available to the virtual machine item the relationship between the amount of CPUs and cores available to the virtual machine item the amount of memory available to the virtual machine item the cluster to which the virtual machine belongs where the cluster is a grouping of physical hosts item and the physical host of the virtual machine item .

Interface also identifies the name of the volume configured for use with the particular virtual machine item the amount of disk space committed for use by the particular virtual machine item the amount of disk uncommitted for use by the particular virtual machine item the amount of unshared disk space item whether the volume is accessible to the particular virtual machine item the path that the particular virtual machine uses to connect to the volume item and the uniform resource locator URL of the volume item .

Region describes properties of recent changes to the configuration of the particular virtual machine including the time at which the configuration change was performed item the description of the configuration change item the state of the configuration change item the type of task that caused the configuration change item whether the task was scheduled item whether the configuration change was cancelled item and the system hosting the particular virtual machine at the time of the configuration change item .

Region identifies information relating to any migrations that the particular virtual machine may have experienced from one physical host to another. For example region may identify for each recent migration the physical host of the particular virtual machine before the migration the physical host of the particular virtual machine after migration and the time at which the migration occurred.

Region identifies performance information for the particular virtual machine. Graph indicates the average CPU latency for all the CPUs used by the particular virtual machine at different times. The average CPU latency is represented as a percentage of time the particular virtual machine is in the ready state awaiting CPU resources from its physical host.

Graph indicates average CPU latencies for the past four hours. According to various embodiments a variety of performance statistics may be presented. For example in response to a user selecting the resource of memory for which to view performance information using menu graph may update to indicate performance related to memory resources.

Using menu a user may specify whether performance statistics should be aggregated or be specific to a particular resource i.e. a particular CPU . Using menu a user may select to view a different performance metric such as the amount of memory pages that were used by the particular virtual machine at various times. Using menu the user may specify which types of metric values to view e.g. average maximum or minimum values .

Referring to at block data management system receives a selection to view volume information. A user may indicate a selection to view volume information by selecting item which specifies the name of the volume configured for use with the particular virtual machine.

At block in response to receiving the selection data management system identifies storage data records associated with the selected volume. For example VM data manager app may determine the volume identifier of the selected volume by searching virtual machine data records in VM data repository . In particular search unit may locate the virtual machine data records associated with the particular virtual machine for which interface displays information and search for a volume identifier in the located virtual machine data records. In response to determining the volume identifier of the selected volume search unit of VM data manager app may search storage data repository for storage data records relating to the selected volume by searching for storage data records that include the determined volume identifier.

In one embodiment a volume may be identified in virtual machine data records and storage data records by a volume name and an IP address of the controller that manages the volume. In response to determining the volume name and IP address of the selected volume based on the virtual machine data records VM data manager app may cause storage data records containing the same determined volume name and IP address to be retrieved from storage data repository . Performance information for the selected volume may be determined for display based on the retrieved storage data records. For example performance information from different performance information records may be aggregated and displayed in graph format.

Data management system may comprise indexer s and search head s such as indexers and search head for the indexing and searching of virtual machine data records and storage data records. In an embodiment in response to receiving the selection a search heads formulates a schema for retrieving storage data records associated with the selected volume and distributes the schema to one or more indexers. The one or more indexers may apply the late binding schema to events stored in one or more data repositories and return the retrieved storage data records to the search head. The search head may determine the performance information for the selected volume and other information for display based on the retrieved storage data records.

At block VM data manager app causes display of a second graphical interface displaying information about the volume including performance information relating to the volume. VM data manager app may determine and send the instructions for display of the second graphical interface to display device . VM data manager app may be located on the search head that receives the retrieved data from different indexers.

The second graphical interface may include information determined based on both virtual machine data records and storage data records. The virtual machine data records and storage data records may be stored according to the same format. For example any identifications of a volume name in both the virtual machine data records and storage data records may be tagged with the same field name. As a result VM data manager app may determine that the value for a particular field in certain virtual machine data records and retrieve the records from storage data repository that contain the same value for the particular field.

In region interface identifies the amount of space available in the particular volume item the total space in the particular volume item the amount of space provisioned in the particular volume for virtual machines item and the percentage of the volume that is overprovisioned item .

Region identifies the path of the particular volume item the URL of the particular volume item and the number of virtual machines that utilize the particular volume item .

Region identifies for each virtual machine using the particular volume the name of the physical host of the virtual machine item the name of the virtual machine item the amount of space committed to the virtual machine item the amount of uncommitted space in the particular volume for the virtual machine item and the amount of space provisioned for the virtual machine item .

Interface displays various graphs indicating the performance of the particular volume and the storage controller that manages the particular volume. For example graph indicates the latency rate for the storage controller that manages the particular volume over the past four hours in milliseconds. Item identifies the name of the storage controller that manages the particular volume. Line indicates the average latency for write operations line indicates the average latency for read operations and line indicates the average latency for other operations.

In graph indicates the average input output operations performed per second IOPS by the storage controller that manage the particular volume over the past four hours. Line indicates the average IOPS for write operations line indicates the average IOPS for read operations and line indicates the average IOPS for all operations.

Graph indicates the latency rate for the particular volume over the past four hours in milliseconds. Line indicates the average latency for write operations line indicates the average latency for read operations and line indicates the average latency for all operations.

Graph indicates the average input output operations performed per second IOPS by the storage controller that manage the particular volume over the past four hours. Line indicates the average IOPS for write operations line indicates the average IOPS for read operations line indicates the average IOPS for other operations and line indicates the average IOPS for all operations.

In some embodiments the information displayed in region is determined based on virtual machine data records obtained from virtual machine manager and the information displayed in regions and and graphs and is determined based on storage data records received from storage controller .

Although the process of is described as having been performed at VM data manager app in other embodiments the process of may be performed at storage data manager app or some other application. For example storage data manager app may cause display of graphical interfaces depicting both virtual machine information and storage information determined based on virtual machine data records and storage data records.

In response to a user selection to view further information about storage resources further information about storage controller may be displayed in a third graphical interface. The user may select item in interface which identifies the name of a storage controller . The third graphical interface a detailed storage view may be displayed in and by a separate application such as storage data manager application . The third graphical interface may indicate performance metrics such as how the CPU of storage controller is performing and how many input output operations storage controller is handling per second IOPS .

The second and or third graphical interface may also identify the physical disks that a particular volume spans. VM data manager app or storage data manager app app may determine which disks a volume spans based on storage data records in storage data repository .

Using the approaches described herein a user may troubleshoot performance issues in a virtual machine more efficiently and easily than before. As one example after noticing that a virtual machine is under performing a user may navigate to a virtual machine view e.g. interface to determine the status of the virtual machine. Based on the information displayed in the virtual machine view the user may determine whether the performance issue is being caused by a resource of the physical host. For example if none of the information in interface indicates an existence of an issue with the resources of the physical host the user may choose to view volume information by selecting the volume that contains the virtual machine s data e.g. item in interface . If the information in the volume view e.g. interface indicates poor performance or a sharp change in the performance of a volume or a storage controller the user may determine that the virtual machine performance is indeed being affected by a problem in the storage environment. If so a user may view further information about the storage environment for example by selecting item which identifies the name of the storage controller that manages the volume. Selecting item may result in the display of a third interface a detailed storage view that provides details such as which disks a volume spans and what the performance statistics are for the storage controller that manages a particular volume e.g. CPU utilization metrics . Based on the performance metrics displayed in the third interface the user may determine which physical components may be causing the issue. For example based on the second graphical interface the user may determine whether it is a problem in the storage environment or elsewhere and if it is a problem in the physical environment based on the third graphical interface the user may determine whether the problem is being caused by a particular disk a particular storage controller or some other storage entity.

According to various embodiments one or more of the steps of the processes illustrated in may be removed or the ordering of the steps may be changed. Additionally although separate embodiments are discussed herein any combination of embodiments and or partial embodiments discussed herein may be combined to form further embodiments.

In an embodiment system may be a distributed system where VM data collection node data management system storage data collection node VM data repository and storage data repository each represent multiple entities in a distributed system. For example VM data collection node and storage data collection node may each represent multiple data collection nodes that forward information from different virtual machine managers and different storage controllers respectively to different computers within data management system . Virtual machine data repository and storage data repository may each represent multiple different repositories within which virtual machine data and storage data is stored. VM data manager app and storage data manager app may each execute on multiple machines and different instances of the apps may store information in different repositories.

In response to a request for information such as a request to view information for a volume records of all the different repositories that collectively represent virtual machine data repository and storage data repository may be searched. In some cases they repositories may be searched by different search units on different machines. Records from the different repositories that collectively represent virtual machine data repository and storage data repository may be used to determine performance information for display in a single graphical interface.

Additionally in other embodiments other performance metrics of virtual environment and storage environment may be identified in the graphical interface s or used to determine performance information that is identified in the graphical interface s .

Metrics relating to virtual environment may describe properties of the virtual environment a particular virtual machine a particular physical host a particular virtual machine manage and or a particular virtual system manager. Performance metrics may include a CPU performance metric a memory performance metric a summary performance metric a performance metric based on a max CPU usage a performance metric based on a max memory usage a performance metric based on a ballooned memory a performance metric based on a swapped memory a performance metric based on an average memory usage percentage a performance metric based on the total amount of memory that is reclaimed from all of the VMs on a host a performance metric based on the total amount of memory that is being swapped from all of the VMs on a host a performance metric that changes state based on the remaining disk space on a data store a performance metric that changes state based on how much space is over provisioned i.e. negative numbers are a representation of an under provisioned data store a performance metric based on a VM s average CPU usage in percent a performance metric based on a VM s average memory usage in percent a performance metric based on a VM s state waiting for CPU time a performance metric based on a VM s memory that is actively in use a performance metric based on a VM s memory saved by memory sharing a performance metric based on a VM s memory used to power the VM a performance metric based on physical memory that is mapped to a VM i.e. memory not including overhead memory a performance metric based on an amount of physical memory that is being reclaimed by a host through a ballooning driver a performance metric based on memory that is being read by a VM from a host s swap file a performance metric based on an amount of memory a VM has had to write to a swap file a performance metric based on an amount of memory from a VM that has been swapped by a host. Other example metrics may include task assignment count task assignment types task completion counts and or may describe migrations to from a virtual machine or to from a host.

Included below is a non exhaustive list of known virtual machine performance metrics relating to virtual environment that may be identified in graphical interface s displayed by data management or used to determine performance information that is identified in the graphical interface s .

PercentHighCPUVm PercentHighMemVm PercentHighSumRdyVm VMInvCpuMaxUsg VMInvMemMaxUsg PercentHighBalloonHosts PercentHighSwapHosts PercentHighCPUHosts BalloonedMemory MB swappedMemory MB RemainingCapacity GB Overprovisioned GB p average cpu usage percent p average mem usage percent p summation cpu ready millisecond p average mem active kiloBytes p average mem consumed kiloBytes p average mem overhead kiloBytes p average mem granted kiloBytes p average mem vmmemctl kiloBytes 20 p average mem swapin kiloBytes p average mem swapout kiloBytes p average mem swapped kiloBytes p average disk read kiloBytesPerSecond p average disk write kiloBytesPerSecond p average disk usage kiloBytesPerSecond p summation disk numberWrite number p summation disk numberRead number p latest disk maxTotalLatency millisecond p summation disk commandsAborted number p summation disk busResets number p average net received kiloBytesPerSecond p average net transmitted kiloBytesPerSecond p average net usage kiloBytesPerSecond p average cpu usage percent p summation cpu ready millisecond p average mem usage percent p average mem active kiloBytes p average mem consumed kiloBytes p average mem overhead kiloBytes p average mem granted kiloBytes p average mem vmmemctl kiloBytes p average mem swapin kiloBytes p average mem swapout kiloBytes p average mem11SwapUsed kiloBytes p average disk numberReadAveraged number p average disk numberWriteAveraged number p average disk usage kiloBytesPerSecond p summation disk numberWrite number p summation disk numberRead number p latest disk maxTotalLatency millisecond p average disk queueLatency millisecond p summation disk commandsAborted number p summation d 5 isk busResets number p average net received kiloBytesPerSecond p average net transmitted kiloBytesPerSecond p average net usage kiloBytesPerSecond p average cpu demand megaHertz p average cpu demand megaHertz p average cpu usagemhz megaHertz p average cpu usagemhz megaHertz and or AvgUsg pctPercentHighCPUVm PercentHighMemVm PercentHighSumRdyVm VMInvCpuMaxUsg VMInvMemMaxUsg PercentHighBalloonHosts PercentHighSwapHosts PercentHighCPUHosts BalloonedMemory MB swappedMemory MB RemainingCapacity GB Overprovisioned GB p average cpu usage percent p average mem usage percent p summation cpu ready millisecond p average mem active kiloBytes p average mem consumed kiloBytes p average mem overhead kiloBytes p average mem granted kiloBytes p average mem vmmemctl kiloBytes p average mem swapin kiloBytes p average mem swapout kiloBytes p average mem swapped kiloBytes p average disk read kiloBytesPerSecond p average disk write kiloBytesPerSecond p average disk usage kiloBytesPerSecond p summation disk numberWrite number p summation disk numberRead number p latest disk maxTotalLatency millisecond p summation disk commandsAborted number p summation disk busResets number p average net received kiloBytesPerSecond p average net transmitted kiloBytesPerSecond p average net usage kiloBytesPerSecond p average cpu usage percent p summation cpu ready millisecond p average mem usage percent p average mem active kiloBytes p average mem consumed kiloBytes p average mem overhead kiloBytes p average mem granted kiloBytes p average mem vmmemctl kiloBytes p average mem swapin kiloBytes p average mem swapout kiloBytes p average mem11SwapUsed kiloBytes p average disk numberReadAveraged number p average disk numberWriteAveraged number p average disk usage kiloBytesPerSecond p summation disk numberWrite number p summation disk numberRead number p latest disk maxTotalLatency millisecond p average disk queueLatency millisecond p summation disk commandsAborted number p summation disk busResets number p average net received kiloBytesPerSecond p average net transmitted kiloBytesPerSecond p average net usage kiloBytesPerSecond p average cpu demand megaHertz p average cpu demand5 megaHertz p average cpu usagemhz megaHertz p average cpu usagemhz megaHertz and or AvgUsg pct.

Of course any of the above or below listed performance metrics could also or alternatively be monitored and reported in any of bytes MegaBytes GigaBytes and or any other byte or memory amount.

Any performance metrics described herein could also or alternatively be monitored and reported in any of hertz MegaHertz GigaHertz and or any hertz amount. Moreover any of the performance metrics disclosed herein may be monitored and reported in any of percentage relative and or absolute values.

Other performance metrics that may be collected or displayed may include any type of cluster performance metrics such as latest clusterServices cpufairness number average clusterServices effectivecpu megaHertz average clusterServices effectivemem megaBytes latest clusterServices failover number and or latest clusterServices memfairness number.

CPU performance metrics that may be collected or displayed may include any of average cpu capacity.contention percent average cpu capacity.demand megaHertz average cpu capacity.entitlement megaHertz average cpu capacity.provisioned megaHertz average cpu capacity.usage megaHertz none cpu coreUtilization percent average cpu coreUtilization percent maximum cpu coreUtilization percent minimum cpu coreUtilization percent average cpu corecount.contention percent average cpu corecount.provisioned number average cpu corecount.usage number summation cpu costop millisecond latest cpu cpuentitlement megaHertz average cpu demand megaHertz latest cpu entitlement megaHertz summation cpu idle millisecond average cpu latency percent summation cpu maxlimited millisecond summation cpu overlap millisecond summation cpu ready millisecond average cpu reservedCapacity megaHertz summation cpu run millisecond summation cpu swapwait millisecond summation cpu system millisecond average cpu totalCapacity megaHertz average cpu totalmhz megaHertz none cpu us 5 age percent average cpu usage percent minimum cpu usage percent maximum cpu usage percent none cpu usagemhz megaHertz average cpu usagemhz megaHertz minimum cpu usagemhz megaHertz maximum cpu usagemhz megaHertz summation cpu used millisecond none cpu utilization percent average cpu utilization percent maximum cpu utilization percent minimum cpu utilization percent and or summation cpu wait millisecond.

Host based replication hbr performance metrics that may be collected or displayed may include any of average hbr hbrNetRx kiloBytesPerSecond average hbr hbrNetTx kiloBytesPerSecond and or average hbr hbrNumVms number.

Management Agent performance metrics that may be collected or displayed may include any of average managementAgent5 cpuUsage megaHertz average managementAgent memUsed kiloBytes average managementAgent swapIn kiloBytesPerSecond average managementAgent swapOut kiloBytesPerSecond and or average managementAgent swapUsed kiloBytes.

none mem active kiloBytes average mem active kiloBytes minimum mem active kiloBytes maximum mem active kiloBytes average mem activewrite kiloBytes average mem capacity.contention percent average mem capacity.entitlement kiloBytes average mem capacity.provisioned kiloBytes average mem capacity.usable kiloBytes average mem capacity.usage kiloBytes average mem capacity.usage.userworld kiloBytes average mem capacity.usage.vm kiloBytes average mem capacity.usage.vmOvrhd kiloBytes average mem capacity.usage.vmkOvrhd kiloBytes average mem compressed kiloBytes average mem compressionRate kiloBytesPerSecond none mem consumed kiloBytes average mem consumed kiloBytes minimum mem consumed kiloBytes maximum mem consumed kiloBytes average mem consumed.userworlds kiloBytes average mem consumed. vms kiloBytes average mem decompressionRate kiloBytesPerSecond average mem entitlement kiloBytes none mem granted kiloBytes average mem granted kiloBytes minimum mem granted kiloBytes maximum mem granted kiloBytes none mem heap kiloBytes average mem heap kiloBytes minimum mem heap kiloBytes maximum mem heap kiloBytes none mem heapfree kiloBytes average mem heapfree kiloBytes minimum mem heapfree kiloBytes maximum mem heapfree kiloBytes average mem latency percent none mem11SwapIn kiloBytes average mem11SwapIn kiloBytes maximum mem11SwapIn kiloBytes minimum mem11SwapIn kiloBytes average mem11SwapInRate kiloBytesPerSecond none mem11SwapOut kiloBytes average mem11SwapOut kiloBytes maximum mem11SwapOut kiloBytes minimum mem11SwapOut kiloBytes average mem11SwapOutRate kiloBytesPerSecond none mem11SwapUsed kiloBytes average mem11SwapUsed kiloBytes maximum mem11SwapUsed kiloBytes minimum mem11SwapUsed kiloBytes average mem lowfreethreshold kiloBytes latest mem mementitlement megaBytes none mem overhead kiloBytes average mem overhead kiloBytes minimum mem overhead kiloBytes maximum mem overhead kiloBytes average mem overheadMax kiloBytes average mem overheadTouched kiloBytes average mem reservedCapacity megaBytes average mem reservedCapacity.userworld kiloBytes average mem reservedCapacity.vm kiloBytes average mem reservedCapacity.vmOvhd kiloBytes average mem reservedCapacity.vmkOvrhd kiloBytes average mem reservedCapacityPct percent none mem shared kiloBytes average mem shared kiloBytes minimum mem shared kiloBytes maximum mem shared kiloBytes none mem sharedcommon kiloBytes average mem sharedcommon kiloBytes minimum mem sharedcommon kiloBytes maximum mem sharedcommon kiloBytes latest mem state number none mem swapIn kiloBytes average mem swapIn kiloBytes minimum mem swapIn kiloBytes maximum mem swapIn kiloBytes none mem swapOut kiloBytes average mem swapOut kiloBytes minimum mem swapOut kiloBytes maximum mem swapOut kiloBytes none mem swapin kiloBytes average mem swapin kiloBytes maximum mem swapin kiloBytes minimum mem swapin kiloBytes average mem swapinRate kiloBytesPerSecond none mem swapout kiloBytes average mem swapout kiloBytes maximum mem swapout kiloBytes minimum mem swapout kiloBytes average mem swapoutRate kiloBytesPerSecond none mem swapped kiloBytes average mem swapped kiloBytes minimum mem swapped kiloBytes maximum mem swapped kiloBytes none mem swaptarget kiloBytes average mem swaptarget kiloBytes minimum mem swaptarget kiloBytes maximum mem swaptarget kiloBytes none mem swapunreserved kiloBytes average mem swapunreserved kiloBytes minimum mem swapunreserved kiloBytes maximum mem swapunreserved5 kiloBytes none mem swapused kiloBytes average mem swapused kiloBytes minimum mem swapused kiloBytes maximum mem swapused kiloBytes none mem sysUsage kiloBytes average mem sysUsage kiloBytes maximum mem sysUsage kiloBytes minimum mem sysUsage kiloBytes average mem totalCapacity megaBytes average mem totalmb megaBytes none mem unreserved kiloBytes average mem unreserved kiloBytes minimum mem unreserved kiloBytes maximum mem unreserved kiloBytes none mem usage percent average mem usage percent minimum mem usage percent maximum mem usage percent none mem vmmemctl kiloBytes average mem vmmemctl kiloBytes minimum mem vmmemctl kiloBytes maximum mem vmmemctl kiloBytes none mem vmmemctltarget kiloBytes average mem vmmemctltarget kiloBytes minimum mem vmmemctltarget kiloBytes maximum mem vmmemctltarget kiloBytes none mem zero kiloBytes average mem zero kiloBytes minimum mem zero kiloBytes maximum mem zero kiloBytes latest mem zipSaved kiloBytes and or latest mem zipped kiloBytes.

Network performance metrics that may be collected or displayed may include any of summation net broadcastRx number summation net broadcastTx number average net bytesRx kiloBytesPerSecond average net bytesTx kiloBytesPerSecond summation net droppedRx number summation net droppedTx number summation net errorsRx number summation net errorsTx number summation net multicastRx number summation net multicastTx number summation net packetsRx number summation net packetsTx number average net received kiloBytesPerSecond summation net throughput.contention number average net throughput.packetsPerSec number average net throughput.provisioned kiloBytesPerSecond average net throughput.usable kiloBytesPerSecond average net throughput.usage kiloBytesPerSecond average net throughput.usage.ft kiloBytesPerSecond average net throughput.usage hbr kiloBytesPerSecond average net throughputusage.iscsi kiloBytesPerSecond average net throughput.usage.nfs kiloBytesPerSecond average net throughput.usage.vm kiloBytesPerSecond average net throughput.usage.vmotion kiloBytesPerSecond average net transmitted kiloBytesPerSecond summation net unknownProtos number none net usage kiloBytesPerSecond average net usage kiloBytesPerSecond minimum net usage kiloBytesPerSecond and or maximum net usage kiloBytesPerSecond.

Power performance metrics that may be collected or displayed may include any of average power capacity.usable watt average power capacity.usage watt average power capacity.usagePct percent summation power energy joule average power power watt and or average power powerCap watt.

Rescpu performance metrics that may be collected or displayed may include any of latest rescpu actav1 percent latest rescpu actav15 percent latest rescpu actav5 percent latest rescpu actpk1 percent latest rescpu actpk15 percent latest rescpu actpk5 percent latest rescpu maxLimited1 percent latest rescpu maxLimited15 percent latest rescpu maxLimited5 percent latest rescpu runav1 percent latest rescpu runav15 percent latest rescpu runav5 percent latest rescpu runpk1 percent 25 latest rescpu runpk15 percent latest rescpu runpk5 percent latest rescpu sample count number and or latest rescpu samplePeriod millisecond.

System performance metrics that may be collected or displayed may include any of latest sys diskUsage percent summation sys heartbeat number latest sys osUptime second latest sys resourceCpuAct1 percent latest sys resourceCpuAct5 percent latest sys resourceCpuAllocMax megaHertz latest sys resourceCpuAllocMin megaHertz latest sys resourceCpuAllocShares number latest sys resourceCpuMaxLimited1 percent latest sys resourceCpuMaxLimited5 percent latest sys resourceCpuRun1 percent latest sys resourceCpuRun5 percent none sys resourceCpuUsage megaHertz average sys resourceCpuUsage megaHertz maximum5 sys resourceCpuUsage megaHertz minimum sys resourceCpuUsage megaHertz latest sys resourceMemAllocMax kiloBytes latest sys resourceMemAllocMin kiloBytes latest sys resourceMemAllocShares number latest sys resourceMemConsumed kiloBytes latest sys resourceMemCow kiloBytes latest sys resourceMemMapped kiloBytes latest sys resourceMemOverhead kiloBytes latest sys resourceMemShared kiloBytes latest sys resourceMemSwapped kiloBytes latest sys resourceMemTouched kiloBytes latest sys resourceMemZero kiloBytes and or latest sys uptime second.

Debug performance metrics that may be collected or displayed may include any of maximum vcDebugInfo activationlatencystats millisecond minimum vcDebugInfo activationlatencystats millisecond summation vcDebugInfo activationlatencystats millisecond maximum vcDebugInfo activationstats number minimum vcDebugInfo activationstats number summation vcDebugInfo activationstats number maximum vcDebugInfo hostsynclatencystats millisecond minimum vcDebugInfo hostsynclatencystats millisecond summation vcDebugInfo hostsynclatencystats millisecond maximum vcDebugInfo hostsyncstats number minimum vcDebugInfo hostsyncstats number summation vcDebugInfo hostsyncstats number maximum vcDebugInfo inventorystats number minimum vcDebugInfo inventorystats number summation vcDebugInfo inventorystats number maximum vcDebugInfo lockstats number minimum vcDebugInfo lockstats number summation vcDebugInfo lockstats number maximum vcDebugInfo lrostats number minimum vcDebugInfo lrostats number summation vcDebugInfo lrostats number maximum vcDebugInfo miscstats number minimum vcDebugInfo miscstats number summation vcDebugInfo miscstats number maximum vcDebugInfo morefregstats number minimum vcDebugInfo morefregstats number summation vcDebugInfo morefregstats number maximum vcDebugInfo scoreboard number minimum vcDebugInfo scoreboard number summation vcDebugInfo5 scoreboard number maximum vcDebugInfo sessionstats number minimum vcDebugInfo sessionstats number summation vcDebugInfo sessionstats number maximum vcDebugInfo systemstats number minimum vcDebugInfo systemstats number summation vcDebugInfo systemstats number maximum vcDebugInfo vcservicestats number minimum vcDebugInfo vcservicestats number and or summation vcDebugInfo vcservicestats number.

Resource performance metrics that may be collected or displayed may include any of average vcResources cpuqueuelength number average vcResources ctxswitchesrate number average vcResources diskqueuelength number average vcResources diskreadbytesrate number average vcResources diskreadsrate number average vcResources diskwritebytesrate number average vcResources diskwritesrate number average vcResources netqueuelength number average vcResources packetrate number average vcResources packetrecvrate number average vcResources packetsentrate number average vcResources pagefaultrate number average vcResources physicalmemusage kiloBytes average vcResources poolnonpagedbytes kiloBytes average vcResources poolpagedbytes kiloBytes average vcResources priviledgedcpuusage percent average vcResources processcpuusage percent average vcResources processhandles number average vcResources processthreads number average vcResources syscallsrate number average vcResources systemcpuusage percent average vcResources systemnetusage percent average vcResources systemthreads number average vcResources usercpuusage percent and or average vcResources virtualmemusage kiloBytes.

VM operation performance metrics that may be collected or displayed may include any of latest vmop numChangeDS number latest vmop numChangeHost number latest vmop numChangeHostDS number latest vmop numClone number latest vmop numCreate number latest vmop numDeploy number latest vmop numDestroy number latest vmop numPoweroff number latest vmop numPoweron number latest vmop numRebootGuest number latest vmop numReconfigure number latest vmop numRegister number latest vmop numReset number latest vmop numSVMotion number latest vmop numShutdownGuest number latest vmop numStandbyGuest number latest vmop numSuspend number latest vmop numUnregister number and or latest vmop numVMotion number.

Included below is a non exhaustive list of known performance metrics relating to storage environment that may be identified in the graphical interface s displayed by data management system or used to determine performance information that is identified in the graphical interface s .

VOLUME PERF METRICS other ops rate other latency average avg latency average write latency average read ops rate write ops rate read latency average total ops rate cifs write ops wvblk past eof cifs read latency cifs read ops clone blks copied nfs write ops rate process name repl metafile logical xfer dw indirect blks cnt clone read redirected clone num share stopped wvblk rsrv parent overwrite always nfs protocol write latency labels delete log labels nfs protocol read latency delta san other ops bad zombie ind blk read err not propagated write data rate wvsblk vvrd spcflags iscsi read ops rate fcp protocol read latency labels df worker rate wv blk snap reserve bad container user blk read error propagated san read latency average clone afs sub file msgs allowed in nvfailed state wv fsinfo blks vbn zero in plane0 iscsi protocol read latency wv playlist no raidbufs iscsi protocol write latency labels nfs read latency average iscsi write data rate clone snap full file other latency cifs other latency average fcp protocol other latency wvbd whole frees o flexcache send data wv playlist entries clone inline split source destination dirty write data wvblk reclaim time done nfs write data wv fsinfo blks used wvblk saved fsinfo private inos total total protocol other latency delta wv fsinfo fs version sub clone latencies hist nfs read latency asynchronous frees iscsi read latency clone split ra repl metafile logical xfer buffer blks cnt clone inline split beyond eof wvsblk lev0 over nominal wv playlist not present wvbd active frees wv fsinfo blks reserve cifs protocol other latency labels cad iron fixed bad fixable blk read error not propagated wv playlist apfi collision accesses fcp write ops bad container fixable snap blk read error propagated iscsi protocol read latency delta wv vol type clone sizes hist labels wvzmb num zmsgs inuse wvblk rsrv holes cifs64 total protocol write latency sub clone latencies hist labels flexcache receive data rate nfs other latency cifs read data rate nfs protocol other latency labels wv playlist prefetch end time nfs read ops rate total protocol write latency labels wvblk rsrv parent holes cifs read ops rate wv playlist prefetch not started wv fsinfo blks used by plane0 internal msgs rate wv playlist load end time read ops wv fsinfo blkr cp wv fsinfo blks64 blks rsrv holes cifs clone inline split source spec vbn wvblk rsrv overwrite wv playlist misses bad container fixable snap blk read error not propagated nfs protocol read latency labels clone lookups node name total protocol read latency labels wv fsinfo blks blks rsrv overwrite wv playlist cont indirects wvi2p wip wi size wvdf enabled iscsi other latency bad fixable metafile blk read error not propagated wvblk reclaim time reset san write data cifs write latency clone prune tmpclonedir err delete log wvsblk vvrd spc clone inherited nfs protocol other latency delta write ops wvblk saved fsinfo public inos reserve wv fsinfo blks vvol dbys df cnt write blocks rate wv fsinfo blks total wvbd owner changed y cifs protocol read latency flexcache other ops rate fcp other ops fcp protocol other latency delta wvip vvol container wi blk cnt wv playlist apfi collision inserts wv playlist prefetch start time iscsi read data extent size instance name iscsi write latency average wv fsinfo containment version vmalign bad container user blk read error not propagated iscsi write data nfs read ops parent aggr san read ops cad clone create inserts cifs protocol write latency delta wvblk rsrv parent overwrite iscsi read ops wv fsinfo public inos total iscsi write ops rate iscsi read data rate bad container non fixable blk read error propagated wvblk speres in parent wvblk rsrv holes64 nfs write latency average wvsblk vvrd last fbn wvblk saved fsinfo private inos used fcp read data rate nfs read data rate cifs protocol other latency delta clone split ra lag stream sizes hist labels synchronous frees rate bad container non fixable blk read error not propagated wv fsinfo blks blks rsrv holes cad cli deletes clone eio blks fcp write data fcp protocol write latency flexcache send data rate flexcache read data rate nfs protocol write latency delta wvblk zombie blks asynchronous frees rate wvblk rsrv holes cifs wvblk saved fsinfo public inos total wv fsinfo blks64 blks rsrv holes cifs other ops rate cifs protocol read latency labels bad container fixable afs blk read error not propagated flexcache read ops rate clone max streams san write latency san write ops wv fsinfo blks res state wv fsinfo containment version highest compression nfs write data rate other ops cifs write data rate wvdf last fbn iscsi other latency average fcp read latency fcp write latency san read latency bad non fixable blk read error not propagated wvblk saved fsinfo private inos reserve wv fsinfo containment version spare1 wv fsinfo containment version spare2 full clone latencies hist wvdf max frees san read data nfs protocol read latency wv playlist vvbn holes clone sizes hist san write ops rate nfs other ops rate wvsblk vvrd vol size wvblk saved fsinfo public inos used iscsi protocol other latency delta wvblk rsrv holes cifs protocol write latency labels iscsi other ops wvol number suspended rate clone inline split range size limitation wvsnap incore count cifs other ops clone inline split enospc fcp write ops rate clone snap sub file clone num entries total protocol other latency labels wv fsinfo blks rsrv absents san write latency average iscsi protocol write latency delta synchronous frees wvblk reclaim time start total protocol read latency delta cifs protocol write latency wvblk delalloc clone afs full file clone inodes wv fsinfo containment version highest slc fcp protocol other latency labels wv fsid wv fsinfo containment version highest sle wv fsinfo public inos used vserver name nfs write latency san read data rate full clone latencies hist labels fcp other latency cad cli updates clone max entries san read ops rate wvip public inofile wi blk cnt wv fsinfo private inos reserve fcp write latency average wvip vvol container wi size wv fsinfo blks rsrv parent san other latency average wvdf inconsistent scores bad user blk read error propagated cad cli inserts flexcache receive data clone storage blocks wvbd active frees y cifs read data cifs write ops rate wvblk rsrv absents wvip vvol container indirects total protocol write latency delta wv playlist hits wvip private inofile wi blk cnt wvblk past eof64 fcp protocol write latency labels flexcache write ops rate iscsi protocol other latency bad fixable metafile blk read error propagated nfs read data bad user blk read error not propagated iscsi other ops rate fcp protocol read latency san other latency read data rate total protocol read latency total protocol other latency instance uuid fcp other ops rate cifs write data cifs protocol read latency delta internal msgs node uuid flexcache write data read blocks wv fsinfo containment version compression nfs other ops fcp read latency average nfs protocol write latency flexcache read data clone streams efbig iscsi write ops clone lookup hits nonzero dbys cnt bad fixable blk read error propagated write blocks fcp read data iscsi write latency bad zombie ind blk read err propagated write latency wv volinfo fs options read blocks rate df worker wv fsinfo containment version highest spare2 wv fsinfo containment version highest spare1 fcp read ops rate fcp other latency average wv playlist reqs wv fsinfo containment version highest vmalign nfs protocol other latency wv fsinfo blks overwrite slider pct cifs protocol other latency wv fsinfo blks snap reserve pct iscsi read latency average iscsi protocol read latency labels wv fsinfo containment version slc wv fsinfo containment version sle wv fsinfo private inos used nfs write ops iscsi protocol write latency wvsnap ondisk count nfs other latency average repl metafile logical xfer checker blks cnt clone inline split edquot wv playlist getbuf failures wvsblk space tax read latency wv fsinfo private inos total san write data rate wvbd whole frees stream sizes hist cifs read latency average flexcache other ops flexcache write ops wvdf total score iscsi protocol other latency labels wv volinfo fs flags cifs other latency msgs rejected in nvfailed state fcp protocol read latency delta wv fsinfo blks blks rsrv holes cifs wv playlist apfi used slots cad iron removed read data wvol number suspended fcp write data rate wvdf watermark cifs write latency average clone inline split kireeti in progress clone max hierarchy wv fsinfo public inos reserve cad crtime updates clone inline splits vserver uuid wvblk reclaim time abort fcp read ops wvsnap loaded total flexcache read ops flexcache write data rate wvblk parent to be reclaimed total ops avg latency bad non fixable blk read error propagated clone unsplit snap entries wvsblk vvrd flags repl metafile logical xfer rebuild buffer blks cnt fcp protocol write latency delta bad container fixable afs blk read error propagated san other ops rate.

SYSTEM PERFORMANCE METRICS write ops rate total ops rate read ops rate sys latency hist delta node name fcp data sent avg processor busy percent system id iscsi ops disk data read cpu busy sys latency hist labels hdd data read rate num processors http ops rate sys read latency hist labels sys read latency average sys avg latency hdd data written nfs ops sys write latency hist labels wafliron delta net data sent rate sys avg latency average hdd data written rate instance uuid nfs ops rate fcp data recv total processor busy disk data written rate ssd data read net data sent fcp data recv rate cifs ops rate ssd data written total ops sys latency hist fcp data sent rate hdd data read disk data read rate sys read latency hist wafliron http ops system model sys write latency average net data recv rate sys read latency total processor busy percent wafliron labels node uuid serial no sys write latency hostname iscsi ops rate cifs ops net data recv instance name sys write latency hist fcp ops disk data written cpu elapsed time process name fcp ops rate ssd data read rate avg processor busy sys write latency hist delta sys read latency hist delta ontap version cpu elapsed time1 write ops cpu elapsed time2 read ops cpu busy percent ssd data written rate uptime.

VFILER PERF METRICS vfiler read ops node name vfiler write ops vfiler net data sent vfiler misc ops vfiler read bytes rate vfiler net data sent rate vfiler net data recv rate vfiler misc ops rate vfiler cpu busy percent vfiler cpu busy base instance uuid vfiler cpu busy vfiler net data recv vfiler read bytes vfiler write ops rate node uuid vfiler write bytes instance name vfiler write bytes rate process name vfiler read ops rate.

QTREE PERF METRICS parent vol internal ops rate nfs ops rate cifs ops rate internal ops nfs ops cifs ops objname.

QUOTA PERF METRICS node name quota lookups labels quota name db blocks quota lookups quota db blocks quota bplus tree blocks instance name quota disk records labels instance uuid quota types labels quota records node uuid quota disk records quota records labels quota usermap lookups labels process name quota fsi state quota types quota usermap lookups quota state.

AGGREGATE PERF METRICS blkr async no msg delta cp reads hdd rate wvblk saved private fsinfo inos total wvblk rsrv overwrite wv fsinfo containment version compression blkr blocks redirected reread delta blkr wa used csc aa delta blkr redirect blocks ok delta blkr wa used csc aa blkr free blocks scanned delta blkr blocks redirected noio delta blkr segments scanned delta wv fsinfo containment version spare1 wv fsinfo containment version spare2 user write blocks hdd rate wvdf max frees cp reads rate blkr redirect demand rereq blkr redirect indirects inspected labels blkr blocks redirected noll delta wvblk child delalloc total transfers rate blkr full segments scanned delta wvblk child pledge percent blkr rejected segments in current aa delta wvblk saved public fsinfo inos used total transfers wvblk rsrv child holes blkr non csc used empty delta total transfers hdd blkr csc empty aa wv volinfo fs options blkr aggrsnap blocks scanned wvblk rsrv holes blkr blocks redirected delta wvblk past eof blkr rejected segments scanned user reads hdd rate user read blocks rate wv fsinfo ssdblks used blkr blocks dummy read delta delete log blkr policy1 reject reasons labels blkr rejected segments before scan delta user reads blkr redirect blocks invalid delta blkr redirect ra11 delta blkr csc full aa delta cp reads blkr empty segments scanned wvblk lev0 over nominal process name blkr rejected blocks scanned ext cache ilog full wv fsinfo blks blks rsrv holes blkr redirect blocks updated wv fsinfo containment version highest spare2 wv fsinfo containment version highest spare1 blkr blocks redirected maybe delta blkr redirect indirects updated labels blkr blocks redirected cp read blocks rate delete log labels user writes hdd user write blocks ssd blkr async offline delta blkr blocks read delta blkr csc aa requested blkr async no mem blkr blocks scanned wv fsinfo blks overwrite slider pct user reads rate blkr aggrsnap blocks scanned delta blkr redirect ra11 blkr redirect ra10 wvblk snap reserve blkr redirect blocks ok blkr redirect indirects inspected wv fsinfo blks snap reserve pct user write blocks rate wv fsinfo blks rsrv absents wv fsinfo containment version slc blkr blocks redirected noio wv fsinfo containment version highest compression wv fsinfo blks total wvbd owner changed y wv fsinfo containment version sle wvblk saved private fsinfo inos reserve user write blocks ssd rate wv fsinfo private inos used blkr csc total aa cleaned cp reads ssd blkr blocks redirected noverify blkr redirect demand req blkr blocks redirected noread delta blkr csc buf suspended delta user read blocks ssd rate blkr rejected blocks scanned delta cp reads ssd rate wvblk delalloc blkr async launched blkr csc empty aa delta blkr non csc used empty blkr segments scanned blkr blocks dummy read wv fsinfo blks blks rsrv overwrite instance name wvbd whole frees o user reads ssd rate cp reads hdd wv fsinfo containment version highest slc wv fsid blkr wa used non csc aa wv fsinfo containment version highest sle blkr super blocks scanned wv fsinfo public inos used blkr async completed delta blkr reads launched delta blkr blocks reallocated delta blkr csc msg failed delta user reads ssd blkr rejected segments before scan blkr csc full aa blkr redirect indirects ok delta blkr policy1 reject reasons wvbd whole frees blkr redirect kireetis scanned delta user read blocks hdd rate cp read blocks ssd blkr policy1 reject reasons delta wvblk child indirect blk cnt blkr redirect indirects ok labels node name blkr csc total aa cleaned delta blkr non csc used full delta wv fsinfo blks used wv fsinfo ssdblks total wv fsinfo fs version disk type blkr redirect demand rereq delta wv fsinfo public inos total user write blocks hdd cp read blocks blkr rejected segments scanned delta blkr redirect ra10 delta cp read blocks ssd rate wv volinfo fs flags blkr blocks redirected noverify delta blkr blocks redirected maybe parent host wvblk rsrv holes64 wvblk rsrv child overwrite blkr csc aa requested delta wvip public inofile wi blk cnt blkr blocks postfiltered delta blkr blocks postfiltered labels wv fsinfo blks blks rsrv holes cifs wvblk space tax blkr async launched delta blkr csc aa inventory blkr wa used non csc aa delta wvblk saved private fsinfo inos used blkr async completed wv fsinfo private inos reserve blkr blocks reallocated blkr async no msg blkr blocks redirected nomem delta wvbd active frees blkr blocks redirected nomem wv fsinfo blks reserve user reads hdd wv fsinfo ssdblks used to write cache blkr free blocks scanned blkr reads launched blkr redirect indirects inspected delta blkr blocks postfiltered blkr csc buf suspended blkr aa blocks scanned delta blkr blocks overwritten blkr csc msg completed total transfers hdd rate blkr async offline blkr super blocks scanned delta wvbd active frees y blkr redirect demand req delta blkr csc msg completed delta blkr csc msg failed user writes ssd rate blkr rejected ssd rgs blkr redirect kireetis scanned blkr redirect indirects ok wv fsinfo public inos reserve blkr blocks redirected noll wvblk zombie blks blkr blocks redirected reread user read blocks hdd blkr redirect ra map delta wvip private inofile wi blk cnt wvblk past eof64 blkr blocks read blkr redirect susps wvblk rsrv holes cifs blkr redirect indirects updated delta wv vol type wv fsinfo blks64 blks rsrv holes wvzmb num zmsgs inuse wvblk saved public fsinfo inos reserve cp read blocks hdd rate blkr async no mem delta blkr blocks overwritten delta wvblk rsrv holes cifs64 blkr rejected ssd rgs delta total transfers ssd rate wv fsinfo ssdblks used by plane0 instance uuid blkr redirect indirects updated user read blocks ssd user write blocks blkr full segments scanned wvblk rsrv child overwrite always blkr blocks scanned delta blkr non csc used full user writes blkr blocks redirected noread user writes ssd blkr redirect demand drop delta node uuid wvblk saved public fsinfo inos total wv fsinfo blks res state wv fsinfo private inos total blkr redirect demand drop blkr rejected segments in current aa wvblk child to be reclaimed user writes rate blkr redirect ra map wv fsinfo blks used by plane0 cp read blocks hdd blkr redirect blocks invalid blkr redirect blocks updated delta user writes hdd rate blkr redirect susps delta blkr empty segments scanned delta blkr aa blocks scanned total transfers ssd wv fsinfo blkr cp user read blocks wv fsinfo blks64 blks rsrv holes cifs.

DISK PERF METRICS disk busy percent node name cp reads rate user write chain user read latency average user reads rate total transfers rate user read chain average guarenteed write chain user read blocks rate raid group skip blocks rate cp read chain average guarenteed read blocks cp read blocks guarenteed write latency average disk io latency histogram guarenteed read blocks rate raid type io pending guarenteed write latency guarenteed write blocks user writes in skip mask rate dlsched distant cp read latency total transfers io queued average user write latency average disk capacity user read chain instance uuid raid group id user read latency user write blocks dlsched max distant dlsched immediate disk busy user skip write ios rate dlsched count rate guarenteed read chain user writes in skip mask user writes display name guarenteed read chain average io pending average user write latency guarenteed reads rate node uuid guarenteed write chain average dlsched max background dlsched count cp read chain guarenteed write blocks rate user reads guarenteed reads skip blocks instance name disk io latency histogram labels cp reads user writes rate process name guarenteed writes dlsched wait average user write chain average raid name base for disk busy guarenteed writes rate user write blocks rate dlsched wait disk io latency histogram delta cp read latency average cp read blocks rate user skip write ios disk speed guarenteed read latency dlsched io time io queued user read blocks guarenteed read latency average objtype.

LUN PERF METRICS read data rate read align histo write ops write data scsi partner data rate avg other latency write partial blocks queue full display name avg other latency average total ops rate read data read ops rate scsi partner ops write align histo avg write latency average scsi partner ops rate read partial blocks percent read ops1 write align histo labels read align histo labels read ops avg write latency avg read latency average total ops queue full rate read align histo percent write align histo percent read partial blocks write ops1 queue depth lun other ops write partial blocks percent avg latency write data rate write ops rate avg read latency avg latency average other ops rate scsi partner data.

In an embodiment an apparatus comprises a processor and is configured to perform any of the foregoing methods.

In an embodiment a non transitory computer readable storage medium storing software instructions which when executed by one or more processors cause performance of any of the foregoing methods.

According to one embodiment the techniques described herein are implemented by one or more special purpose computing devices. The special purpose computing devices may be hard wired to perform the techniques or may include digital electronic devices such as one or more application specific integrated circuits ASICs or field programmable gate arrays FPGAs that are persistently programmed to perform the techniques or may include one or more general purpose hardware processors programmed to perform the techniques pursuant to program instructions in firmware memory other storage or a combination. Such special purpose computing devices may also combine customizable hard wired logic ASICs or FPGAs with customizable programming to accomplish the techniques. The special purpose computing devices may be desktop computer systems portable computer systems handheld devices televisions wearable computing devices networking devices or any other device that incorporates hard wired and or program logic to implement the techniques.

For example is a block diagram that illustrates a computer system upon which an embodiment of the invention may be implemented. Computer system includes a bus or other communication mechanism for communicating information and a hardware processor coupled with bus for processing information. Hardware processor may be for example a general purpose microprocessor.

Computer system also includes a main memory such as a random access memory RAM or other dynamic storage device coupled to bus for storing information and instructions to be executed by processor . Main memory also may be used for storing temporary variables or other intermediate information during execution of instructions to be executed by processor . Such instructions when stored in non transitory storage media accessible to processor render computer system into a special purpose machine that is customized to perform the operations specified in the instructions.

Computer system further includes a read only memory ROM or other static storage device coupled to bus for storing static information and instructions for processor . A storage device such as a magnetic disk optical disk or solid state drive is provided and coupled to bus for storing information and instructions.

Computer system may be coupled via bus to a display such as a cathode ray tube CRT for displaying information to a computer user. An input device including alphanumeric and other keys is coupled to bus for communicating information and command selections to processor . Another type of user input device is cursor control such as a mouse a trackball or cursor direction keys for communicating direction information and command selections to processor and for controlling cursor movement on display . This input device typically has two degrees of freedom in two axes a first axis e.g. x and a second axis e.g. y that allows the device to specify positions in a plane.

In some embodiments a customer interacts with computer system via touch for example by tapping or gesturing over certain locations. A display screen of display may also be capable of detecting touch.

Computer system may implement the techniques described herein using customized hard wired logic one or more ASICs or FPGAs firmware and or program logic which in combination with the computer system causes or programs computer system to be a special purpose machine. According to one embodiment the techniques herein are performed by computer system in response to processor executing one or more sequences of one or more instructions contained in main memory . Such instructions may be read into main memory from another storage medium such as storage device . Execution of the sequences of instructions contained in main memory causes processor to perform the process steps described herein. In alternative embodiments hard wired circuitry may be used in place of or in combination with software instructions.

The term storage media as used herein refers to any non transitory media that store data and or instructions that cause a machine to operate in a specific fashion. Such storage media may comprise non volatile media and or volatile media. Non volatile media includes for example optical disks magnetic disks or solid state drives such as storage device . Volatile media includes dynamic memory such as main memory . Common forms of storage media include for example a floppy disk a flexible disk hard disk solid state drive magnetic tape or any other magnetic data storage medium a CD ROM any other optical data storage medium any physical medium with patterns of holes a RAM a PROM and EPROM a FLASH EPROM NVRAM any other memory chip or cartridge.

Storage media is distinct from but may be used in conjunction with transmission media. Transmission media participates in transferring information between storage media. For example transmission media includes coaxial cables copper wire and fiber optics including the wires that comprise bus . Transmission media can also take the form of acoustic or light waves such as those generated during radio wave and infra red data communications.

Various forms of media may be involved in carrying one or more sequences of one or more instructions to processor for execution. For example the instructions may initially be carried on a magnetic disk or solid state drive of a remote computer. The remote computer can load the instructions into its dynamic memory and send the instructions over a telephone line using a modem. A modem local to computer system can receive the data on the telephone line and use an infra red transmitter to convert the data to an infra red signal. An infra red detector can receive the data carried in the infra red signal and appropriate circuitry can place the data on bus . Bus carries the data to main memory from which processor retrieves and executes the instructions. The instructions received by main memory may optionally be stored on storage device either before or after execution by processor .

Computer system also includes a communication interface coupled to bus . Communication interface provides a two way data communication coupling to a network link that is connected to a local network . For example communication interface may be an integrated services digital network ISDN card cable modem satellite modem or a modem to provide a data communication connection to a corresponding type of telephone line. As another example communication interface may be a local area network LAN card to provide a data communication connection to a compatible LAN. Wireless links may also be implemented. In any such implementation communication interface sends and receives electrical electromagnetic or optical signals that carry digital data streams representing various types of information.

Network link typically provides data communication through one or more networks to other data devices. For example network link may provide a connection through local network to a host computer or to data equipment operated by an Internet Service Provider ISP . ISP in turn provides data communication services through the world wide packet data communication network now commonly referred to as the Internet . Local network and Internet both use electrical electromagnetic or optical signals that carry digital data streams. The signals through the various networks and the signals on network link and through communication interface which carry the digital data to and from computer system are example forms of transmission media.

Computer system can send messages and receive data including program code through the network s network link and communication interface . In the Internet example a server might transmit a requested code for an application program through Internet ISP local network and communication interface .

The received code may be executed by processor as it is received and or stored in storage device or other non volatile storage for later execution.

In the foregoing specification embodiments have been described with reference to numerous specific details that may vary from implementation to implementation. The specification and drawings are accordingly to be regarded in an illustrative rather than a restrictive sense. The sole and exclusive indicator of the scope of the embodiments and what is intended by the applicants to be the scope of the embodiments is the literal and equivalent scope of the set of claims that issue from this application in the specific form in which such claims issue including any subsequent correction.

In drawings various system components are depicted as being communicatively coupled to various other components by arrows. These arrows illustrate only certain examples of information flows between the components of the depicted systems. Neither the direction of the arrows nor the lack of arrow lines between certain components should be interpreted as indicating the absence of communication between the certain components. Indeed each component of the depicted systems may feature an open port API or other suitable communication interface by which the component may become communicatively coupled to other components of the depicted systems as needed to accomplish any of the functions of the systems described herein.

