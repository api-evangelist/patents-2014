---

title: Accelerator system for remote data storage
abstract: Data processing and an accelerator system therefor are described. An embodiment relates generally to a data processing system. In such an embodiment, a bus and an accelerator are coupled to one another. The accelerator has an application function block. The application function block is to process data to provide processed data to storage. A network interface is coupled to obtain the processed data from the storage for transmission.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09645951&OS=09645951&RS=09645951
owner: DRC Computer Corporation
number: 09645951
owner_city: Santa Clara
owner_country: US
publication_date: 20140724
---
This is a divisional application of U.S. nonprovisional patent application Ser. No. 13 117 821 filed May 27 2011 which claims benefit to U.S. provisional patent application No. 61 349 560 entitled Hardware Accelerator with Networking Capability filed May 28 2010 each of which is incorporated herein by reference in its entirety for all purposes.

One or more embodiments generally relate to data processing and more particularly to an accelerator system for data processing for remote data storage or other networking application.

Standard microprocessors may not include circuitry for performing some algorithms. By using a Field Programmable Gate Array FPGA for example to provide an accelerator system an algorithm can be programmed into hardware to build a circuit for an algorithm resulting in significant acceleration in the execution of such algorithm. However even with an accelerator system data transactions associated with such algorithms are often handled by system resources such as system memory a central processing unit CPU a Southbridge or a Northbridge collectively and singly motherboard system resources .

Furthermore data may be stored remotely from such motherboard system resources using computing and storage resources that may be coupled to such motherboard system resources over a network. Such computing and storage resources may be referred to as cloud computing resources and such remote storage of data is sometimes referred to as cloud storage. However data handling via a network interface coupled to motherboard system resources may burden operation of a host system.

Accordingly it would be desirable and useful to provide an accelerator system for offloading at least some of such data transactions from such motherboard system resources for remote data storage and or networking.

One or more embodiments generally relate to data processing and more particularly to an accelerator system for data processing for remote data storage or other networking application.

An embodiment relates generally to a data processing system. In such an embodiment a bus and an accelerator are coupled to one another. The accelerator has an application function block. The application function block is to process data to provide processed data to storage. A network interface is coupled to obtain the processed data from the storage for transmission.

Another embodiment relates generally to a kernel mode driver architecture. In such an embodiment a filter driver is to receive a request where the request includes a write command and payload or a read command the filter driver generates write commands responsive to the write command and read commands responsive to the read command and the filter driver provides for separation of command information from data for taking separate processing paths. A device driver is to receive the write commands and the payload or the read commands from the filter driver to provide to a peripheral interface. A port driver is to receive the write commands or the read commands from the filter driver to provide to the peripheral interface.

Yet another embodiment relates generally to a computer system. In such an embodiment a general purpose processor is for execution of a user application in an application mode and kernel mode drivers in a kernel mode. An accelerator system is coupled to the general purpose processor via a first bus where the kernel mode drivers include a class driver a filter driver a device driver a stack driver and a port driver. The class driver is in communication with the user application to receive a request packet to provide a request block in response to the request packet. The filter driver is in communication with the class driver to receive the request block. The request block includes a command and a system payload pointer. The filter driver generates first commands and second commands responsive to the command where each of the first commands and the second commands include a same set of local payload pointers generated by the filter driver. The device driver is in communication with the filter driver to receive the system payload pointer and the first commands and the device driver is in communication with the accelerator system to provide the first commands and the payload pointer thereto.

Still yet another embodiment relates generally to a method for processing data. In such an embodiment data and a system payload pointer are provided from a host system to an accelerator system. The data is processed in the accelerator system to provided processed data. The processed data is stored in memory of the accelerator system. The system payload pointer is converted into at least one local payload pointer for the storing. The at least one local payload pointer is passed to an interface. The processed data is accessed from the memory by the interface using the at least one local payload pointer. The processed data accessed by the interface is transmitted.

A further embodiment relates generally to another method for processing data. In such an embodiment a command and a payload pointer are provided to an accelerator system. The accelerator system obtains data responsive to the payload pointer. The data is processed by the accelerator system responsive to the command to provide processed data. The processed data is stored locally in memory of the accelerator system. A memory access is initiated by a network interface of the accelerator system. The processed data is obtained from the memory responsive to the memory access and the processed data obtained is transmitted by the network interface to cloud storage.

A yet further embodiment relates generally to a method for a kernel mode driver. In such an embodiment a request is obtained by a filter driver where the request includes a write command and payload or a read command. Generated by the filter driver are write commands responsive to the write command or read commands responsive to the read command. Received by a device driver are the write commands and the payload or the read commands from the filter driver to provide to a peripheral interface. Received by a port driver are the write commands or the read commands from the filter driver to provide to the peripheral interface.

A still yet further embodiment relates generally to a non transitory machine readable medium having stored thereof information representing instructions that when executed by a processor cause the processor to perform operations. In such an embodiment a request is provided to a filter driver where the request includes a write command and payload or a read command. Generating by the filter driver are write commands responsive to the write command or read commands responsive to the read command. Received by a device driver are the write commands and the payload or the read commands from the filter driver to provide to a peripheral interface and received by a port driver are the write commands or the read commands from the filter driver to provide to the peripheral interface.

Lastly an embodiment relates generally to another non transitory machine readable medium having stored thereof information representing instructions that when executed by a processor cause the processor to perform operations. In such an embodiment data and a system payload pointer are provided from a host system to an accelerator system where the data is processed by the accelerator system to provided processed data. The processed data is stored in memory of the accelerator system. The system payload pointer is converted into at least one local payload pointer for the storing. The at least one local payload pointer is passed to an interface where the processed data is accessed from the memory by the interface using the at least one local payload pointer.

In the following description numerous specific details are set forth to provide a more thorough description of the specific embodiments of the invention. It should be apparent however to one skilled in the art that the invention may be practiced without all the specific details given below. In other instances well known features have not been described in detail so as not to obscure the invention. For ease of illustration the same number labels are used in different diagrams to refer to the same items however in alternative embodiments the items may be different. Furthermore although particular integrated circuit parts are described herein for purposes of clarity by way of example it should be understood that the scope of the description is not limited to these particular examples as other integrated circuit parts may be used.

Reference will now be made in detail to embodiments examples of which are illustrated in the accompanying drawings. In the following detailed description numerous specific details are set forth in order to provide a thorough understanding of the following described embodiments. It should be apparent however to one skilled in the art that the embodiments described below may be practiced without all the specific details given below. Moreover the embodiments are not intended to be exhaustive or to limit the invention to the precise forms disclosed and modifications and variations are possible in light of the following teachings or may be acquired from practice of the invention. The embodiments were chosen and described in order to best explain principles and practical applications of the invention to enable others skilled in the art to utilize the invention in various embodiments and with various modifications as are suited to the particular use contemplated. In other instances well known methods procedures components circuits and networks have not been described in detail so as not to unnecessarily obscure the described embodiments.

For purposes of explanation specific nomenclature is set forth to provide a thorough understanding of the various inventive concepts disclosed herein. However the terminology used herein is for the purpose of describing particular embodiments only and is not intended to be limiting of the invention. As used herein the singular forms a an and the are intended to include the plural forms as well unless the context clearly indicates otherwise. As used herein the term if may be construed to mean when or upon or in response to determining or in response to detecting depending on the context. Similarly the phrase if it is determined or if a stated condition or event is detected may be construed to mean upon determining or in response to determining or upon detecting the stated condition or event or in response to detecting the stated condition or event depending on the context. It will also be understood that the term and or as used herein refers to and encompasses any and all possible combinations of one or more of the associated listed items. It will be further understood that the terms includes and or including when used in this specification specify the presence of stated features integers steps operations elements and or components but do not preclude the presence or addition of one or more other features integers steps operations elements components and or groups thereof. It will also be understood that although the terms first second etc. may be used herein to describe various elements these elements should not be limited by these terms as these terms are only used to distinguish one element from another.

Some portions of the detailed descriptions that follow are presented in terms of algorithms and symbolic representations of operations on data bits within a computer memory. These algorithmic descriptions and representations are the means used by those skilled in the data processing arts to most effectively convey the substance of their work to others skilled in the art. An algorithm is here and generally conceived to be a self consistent sequence of steps leading to a desired result. The steps are those involving physical manipulations of physical quantities. Usually though not necessarily these quantities take the form of optical electrical or magnetic signals capable of being stored transferred combined compared and otherwise manipulated. It has proven convenient at times principally for reasons of common usage to refer to these signals as bits values elements symbols characters terms numbers or the like.

It should be borne in mind however that all of these and similar terms are to be associated with the appropriate physical quantities and are merely convenient labels applied to these quantities. Unless specifically stated otherwise as apparent from the following discussion it is appreciated that throughout the description discussions utilizing terms such as processing or computing or calculating or determining or displaying or the like refer to the action and processes of a computer system or similar electronic computing device that manipulates and transforms data represented as physical electronic quantities within the computer system s registers and memories into other data similarly represented as physical quantities within the computer system memories or registers or other such information storage transmission or display devices.

Inventive concepts described herein may be embodied as apparatus method system or computer program product. Accordingly one or more of such embodiments may take the form of an entirely hardware embodiment an entirely software embodiment including firmware resident software and micro code among others or an embodiment combining software and hardware and for clarity any and all of these embodiments may generally be referred to herein as a circuit module system or other suitable terms. Furthermore such embodiments may be of the form of a computer program product on a computer usable storage medium having computer usable program code in the medium.

Any suitable computer usable or computer readable medium may be utilized. The computer usable or computer readable medium may be for example but not limited to an electronic magnetic optical electromagnetic infrared or semiconductor system apparatus device or propagation medium. More specific examples a non exhaustive list of the computer readable medium would include the following an electrical connection having one or more wires a portable computer diskette a hard disk a random access memory RAM a read only memory ROM an erasable programmable read only memory EPROM or Flash memory an optical fiber a portable compact disc read only memory CD ROM an optical storage device a transmission media such as those supporting the Internet or an intranet or a magnetic storage device. The computer usable or computer readable medium could even be paper or another suitable medium upon which the program is printed as the program can be electronically captured via for instance optical scanning of the paper or other medium then compiled interpreted or otherwise processed in a suitable manner if necessary and then stored in a computer memory. In the context of this document a computer usable or computer readable medium may be any medium that can contain or store the program for use by or in connection with the instruction execution system apparatus or device.

Computer program code for carrying out operations in accordance with inventive concepts described herein may be written in an object oriented programming language such as Java Smalltalk C or the like. However the computer program code for carrying out such operations may be written in conventional procedural programming languages such as the C programming language or similar programming languages. The program code may execute entirely on the user s computer partly on the user s computer as a stand alone software package partly on the user s computer and partly on a remote computer or entirely on the remote computer or server. In the latter scenario the remote computer may be connected to the user s computer through a local area network LAN or a wide area network WAN or the connection may be made to an external computer for example through the Internet using an Internet Service Provider .

Systems and methods described herein may relate to an apparatus for performing the operations associated therewith. This apparatus may be specially constructed for the purposes identified or it may include a general purpose computer selectively activated or reconfigured by a computer program stored in the computer.

Notwithstanding the algorithms and displays presented herein are not inherently related to any particular computer or other apparatus. Various general purpose systems may be used with programs in accordance with the teachings herein or it may prove convenient to construct a more specialized apparatus to perform the operations. In addition even if the following description is with reference to a programming language it should be appreciated that any of a variety of programming languages may be used to implement the teachings as described herein.

The embodiments are described below with reference to flowchart illustrations and or block diagrams of methods apparatus including systems and computer program products. It will be understood that each block of the flowchart illustrations and or block diagrams and combinations of blocks in the flowchart illustrations and or block diagrams may be implemented by computer program instructions. These computer program instructions may be provided to a processor of a general purpose computer special purpose computer or other programmable data processing apparatus to produce a machine such that the instructions which execute via the processor of the computer or other programmable data processing apparatus create means for implementing the functions acts specified in the flowchart and or block diagram block or blocks. These computer program instructions may also be stored in a computer readable memory that can direct a computer or other programmable data processing apparatus to function in a particular manner such that the instructions stored in the computer readable memory produce an article of manufacture including instruction means which implement the function act specified in the flowchart and or block diagram block or blocks. The computer program instructions may also be loaded onto a computer or other programmable data processing apparatus to cause a series of operational steps to be performed on the computer or other programmable apparatus to produce a computer implemented process such that the instructions which execute on the computer or other programmable apparatus provide steps for implementing the functions acts specified in the flowchart and or block diagram block or blocks.

The flowcharts and block diagrams in the Figures illustrate the architecture functionality and operation of possible implementations of apparatuses including systems methods and computer program products according to various embodiments. In this regard each block in the flowchart or block diagrams may represent a module segment or portion of code which comprises one or more executable instructions for implementing the specified logic function s . It should also be noted that in some alternative implementations the functions noted in the block may occur out of the order noted in the figures. For example two blocks shown in succession may in fact be executed substantially concurrently or the blocks may sometimes be executed in the reverse order depending upon the functionality involved. It will also be noted that each block of the block diagrams and or flowchart illustration and combinations of blocks in the block diagrams and or flowchart illustration can be implemented by special purpose hardware based systems which perform the specified functions or acts or combinations of special purpose hardware and computer instructions.

It should be understood that although the flow charts provided herein show a specific order of operations it is understood that the order of these operations may differ from what is depicted. Also two or more operations may be performed concurrently or with partial concurrence. Such variation will depend on the software and hardware systems chosen and on designer choice. It is understood that all such variations are within the scope of the invention. Likewise software and web implementations of the present invention could be accomplished with standard programming techniques with rule based logic and other logic to accomplish the various database searching operations correlation operations comparison operations and decision operations. It should also be understood that the word component as used herein and in the claims is intended to encompass implementations using one or more lines of software code and or hardware implementations and or equipment for receiving manual inputs.

Motherboard may have a central processing unit CPU which may include one or more processor cores. Additionally it should be understood that a motherboard may have more than one CPU as well as chipset chips among other components not illustratively depicted here for purposes of clarity and not limitation. Additionally motherboard may have a socket to which a memory module may be inserted. For purposes of clarity by way of example not limitation it shall be assumed that memory module is system memory coupled to CPU by one or more buses including a system bus not illustratively shown for purposes of clarity and not limitation. In other embodiments system memory may be directly attached to motherboard .

Daughter card may include a reconfigurable processor unit RPU in order to provide acceleration for processing data in comparison to processing data in software. RPUs are described in additional detail in U.S. Pat. Nos. 7 856 545 and 7 856 546 each of which is incorporated by reference herein in its entirety.

Data may be provided as input as generally indicated by arrow from motherboard . More particularly data may be read from memory module used for system memory for input to RPU . After such data is processed by RPU such processed data may be provided as output from daughter card to system memory as generally indicated by output arrow . However this may impose a burden on resources of motherboard not imposed in a host system illustratively depicted in .

Daughter card likewise has an RPU and an interface connector . However rather than unidirectional input and output traffic as illustratively depicted in with arrows and respectively traffic to and from daughter card may be bidirectional as generally indicated by arrow as there is less traffic between motherboard and daughter card in host system than in host system . However it should be understood that either bidirectional or unidirectional traffic as described herein may be used in host system .

Additionally daughter card includes a switch a network interface and transceiver modules . Even though four transceiver modules are illustratively depicted it should be understood that fewer or more transceivers may be used. Transceiver modules are for bidirectional traffic as generally indicated with arrows . Furthermore even though transceivers are described it should be understood that separate receivers and transmitters may be used.

An accelerator system is described below in terms of daughter card for reasons that will become more apparent from the following description.

An accelerator system described below is employed for networking and or storage applications. For purposes of clarity by way of example not limitation a secure data storage application is described. However it will be appreciated that other uses of daughter card may be employed. For example daughter card may be utilized to implement a secure network functions such as data compression and or virus checking in addition to or apart from secure data storage. Other uses should be apparent from the following description.

As described below in additional detail for secure data storage amounts of data may be processed before transferring over a link whether a network link and or a storage link. Such processing may include functions such as encryption decryption authentication error checking addition of an error code data parsing and or addition of redundancy among other types of data processing. Such data processing may be computationally or otherwise motherboard resource intensive and thus offloading such processing to other hardware such as an RPU may cause a host system such as host system to operate faster. For example it should be appreciated that by not tying up a general purpose CPU by attempting to do such data processing entirely in software use of an RPU may accelerate not only such data processing but also may free up motherboard resources for other tasks.

As described below in additional detail embodiments of accelerator systems are provided in order to enhance throughput while reducing load on one or more motherboard resources. Again for purposes of clarity by way of example not limitation it shall be assumed that the data processing operations are performed for a secure data storage application. Such secure data storage application may include encrypting decrypting data parsing and data concatenating. However it should be understood that the described embodiments of accelerator systems may be used for applications other than secure data storage as well as other forms of secure data storage.

In an embodiment daughter card may be a Peripheral Component Interconnect Express PCIe card that interfaces via a PCIe bus to a CPU of a motherboard or more particularly a main CPU of a motherboard . In an embodiment a 16 lane PCIe bus is used however other sizes and types of busing may be used.

Motherboard may be a server or workstation motherboard having a Southbridge chip Southbridge interconnected to a PCIe bus. Such Southbridge may be interconnected to a Northbridge chip Northbridge where such Northbridge is interconnected to a main CPU and system memory. In other server or workstation motherboards the Northbridge may be eliminated and the Southbridge may communicate directly with the main CPU. Furthermore a motherboard may include more than one Northbridge and or Southbridge as well as more than one CPU.

It should be appreciated that there is a limited amount of bandwidth in a Northbridge and or Southbridge in any of these system architectures. Furthermore there is limited bandwidth of one or more buses including a system bus interconnecting for example a main CPU with a Northbridge and or Southbridge. Bandwidth of a PCIe bus interfacing daughter card to CPU is also a limited resource.

Use of daughter card as described below in additional detail may keep a significant amount of high bandwidth data traffic local to such card so as to reduce load for example on a main CPU a Southbridge a Northbridge and or other motherboard system resources. Additionally it should be appreciated that daughter card may use a readily available interface such as PCIe or any other readily available interface so as to be used with a readily available server or workstation motherboard.

Accelerator system may be located on daughter card as previously described. Accelerator system includes RPU switch network interface and transceiver modules . For purposes of clarity by way of example and not limitation an exemplary embodiment of each of these components of accelerator system is described below. However it will be appreciated that these and or other components may be used in accordance with the following description.

Even though a daughter card is described as having RPU switch network interface and transceiver modules it should be understood that in other embodiments a System on Chip SoC may be used as for example an FPGA may include many or all of the resources of daughter card . Thus the number of chips on daughter card may be significantly reduced. Furthermore in still other embodiments daughter card may be socketed to a CPU socket or bus socket other than a peripheral socket such as a PCIe socket or accelerator system may be directly mounted on motherboard . However for purposes of clarity and not limitation a daughter card embodiment is described below even though other embodiments may be used.

Switch may be a PCIe switch having multiple ports. These ports may be configured with any of a variety of different data widths and may be used to move data from any port to any other port without intervention by a main CPU of a motherboard . One of such ports of such PCIe switch may be interconnected to a connector such as socket which attaches to a PCIe bus on a motherboard when daughter card is plugged in. Such a port may be referred to as an upstream port in a bus hierarchy because such port is for coupling to a main CPU controlling such PCIe bus namely such port or bus associated therewith is on a host system side.

In a block labeled illustratively depicts such an upstream port. In an embodiment upstream port may be a PCIe Generation 2 Gen2 by 16 lane port. Other PCIe ports of switch may be referred to as downstream ports because such other ports interconnect to devices farther away from such main CPU in a bus hierarchy than such upstream port .

In an embodiment such downstream ports may each be PCIe Gen2 by 8 lane ports. In this exemplary embodiment four PCIe ports of switch are illustratively depicted however it should be understood that a minimum of three ports may be used in another embodiment where there is only one PCIe downstream port for communication with PLD . PLD may include hard macros or soft cores of PCIe interface ports coupled to downstream PCIe ports of switch . It should be understood that even though the terms upstream port and downstream port are used herein it should be understood that both types of such ports are bidirectional. The term hard macro generally refers to dedicated circuitry and the term soft core generally refers to a circuit instantiated in programmable logic through use of a configuration bitstream.

The main CPU such as CPU of motherboard may enumerate a PCIe bus namely determine all devices connected to such PCIe bus and some characteristics thereof. After CPU has acquired such information other information including commands and or data may be transferred to or from any of such devices connected to such PCIe bus. Additionally switch may include peer to peer routing for example routing data from one downstream device to another downstream device through switch without having to go through CPU . In an embodiment a PEX8648 switch device from PLX Technology Inc. of Sunnyvale Calif. is used as PCIe switch however it should be appreciated that this or another similar device may likewise be used.

An accelerator such as RPU may include a Programmable Logic Device such as a Field Programmable Gate Array FPGA or other integrated circuit having field programmable logic for instantiation of circuitry by programming with a configuration bitstream. Such configuration bitstream may be packet based or frame based for example. However in other embodiments an Application Specific Standard Processor ASSP an Application Specific Integrated Circuit ASIC or any other integrated circuit IC having programmable logic may be used to provide an accelerator. For purposes of clarity by way of example and not limitation it shall be assumed that programmable logic device is an FPGA however in other embodiments other integrated circuits may be used as indicated.

Use of PLD allows one or more algorithms such as for example an encryption algorithm a decryption algorithm a data parsing algorithm and or a data concatenation algorithm such as for a secure data storage application to be instantiated as hardware circuitry via field programmable logic as an application function block. The ability to have any or all of these tasks performed in hardware rather than software accelerates i.e. speeds up data processing such as for secure data storage for example. However it should be appreciated that these or other algorithms may be instantiated in whole or in part in programmable logic fabric of PLD such as an FPGA for example in other embodiments such as for other applications.

Additionally PLD may have expansion ports . In an embodiment each of expansion ports has four lanes. Expansion ports may be used to allow RPU to connect to one or more other RPUs so that they can share large processing tasks. Additionally or optionally expansion ports may be used as a way to add additional functions to RPU .

RPU may further include storage memory nonvolatile storage memory code executable memory and a controller . Controller may be a Complex Programmable Logic Device CPLD . Nonvolatile storage memory may be a form of flash memory or a form of EPROM for example. Code executable memory may be NOR flash or SRAM for example. Storage memory may be SRAM DRAM or NAND flash for example. Other details regarding RPU may be found in U.S. Pat. Nos. 7 856 545 and 7 856 546.

For purposes of clarity and not limitation it shall be assumed that storage memory is DRAM which is externally coupled to a memory interface implemented in the form of programmable logic in PLD . Use of DRAM for a secure data storage application allows any data therein to be generally erased once power is removed from such DRAM as DRAM is a volatile form of memory.

DRAM may be any of a variety of types of DRAM including without limitation DDR DDR2 or DDR3 DRAM. In an embodiment RPU has DDR3 DRAM for DRAM however other types of DDR DRAM as well as other types of DRAM may be used.

In an embodiment a Stratus IV EP4SGX230 FPGA from Altera Corporation of San Jose Calif. is used for PLD . However it should be understood that other FPGAs such as FPGAs from Xilinx Inc. of San Jose Calif. may be used. Moreover it should be understood that PCIe daughtercard includes RPU with DRAM interconnected to an FPGA via a memory controller interface memory interface of such PLD . Thus DRAM is local or subsystem memory of daughter card or PLD . The term local or subsystem memory is used to differentiate between memory on daughtercard or directly coupled to PLD in contrast to memory elsewhere in a host system including without limitation system memory .

Network interface of accelerator system is coupled to another downstream PCIe port of switch . Network interface may be a network interface chip which may be referred to as a NIC though not to be confused with a network interface card. However in other embodiments a network interface card may be used instead of a network interface chip.

Network interface may include ports . For purposes of clarity and not limitation it shall be assumed that ports are bidirectional high speed serial I O ports. Serial I O ports allow for transfer of data to or from devices or systems coupled via a network to daughtercard . Such other devices or systems may be remotely located from host system associated with daughtercard .

Network interface may include one or more physical devices. In particular a Media Access Control MAC and Physical Layer PHY functions of network interface may reside in separate physical devices. Optionally network interface may be implemented using programmable logic of PLD . Such a programmable logic implementation of network interface however uses a substantial portion of the programmable resources of PLD .

Network interface may be used to offload processing associated with network protocols such as Transmission Control Protocol Internet Protocol TCP IP Internet Small Computer System Interface iSCSI or Fibre Channel over Ethernet FCoE among others from a main CPU of a host system. In an embodiment a Terminator ASIC from Chelsio of Sunnyvale Calif. is used for a network interface chip. However in other embodiments other similar network interface chips may likewise be used. For example other network interface chips may be obtained from Broadcom Corporation.

Coupled to serial I O ports of network interface are transceiver modules . In this exemplary embodiment there are four transceiver modules however fewer or more than four transceiver modules may be used in other embodiments. In other embodiments transceiver modules may be omitted with respect to communication with one or more proximal devices as network interface may communicate directly with one or more proximal devices coupled via a network particularly if such one or more proximal devices coupled via a network are relatively close to daughter card . In this embodiment enhanced Small Form factor Pluggable SFP transceivers are used. SFP transceivers are available for many different speeds protocols and types of physical connections. In this embodiment ports of a transceiver modules are 10 Gb s ports which may be used for 10 Gigabit Ethernet or 8 Gb s Fibre Channel connectivity however other types of transceivers with other bandwidths may be used in other embodiments. Transceiver modules and network interface may support metal wire or optical cabling for interconnectivity via high speed serial ports . Numerous other components of daughtercard such as power supplies connectors capacitors and resistors among others are not described herein for purposes of clarity.

Motherboard may include system memory a main CPU and a Southbridge SB such as of a CPU or motherboard chipset. PCIe bus interconnects switch to Southbridge . PCIe buses interconnect switch to PLD . PCIe bus interconnects switch to network interface . Thus PLD and network interface as well as switch are discoverable by CPU .

Switch PLD and network interface appear as three separate PCIe devices to CPU . More particularly responsive to CPU enumerating PCIe buses through CPU discovers PCIe switch and what appears as three downstream devices. Two of these three downstream devices are associated with two PCIe ports in PLD and the other of these three downstream devices is associated with a PCIe port of network interface .

By discovering such downstream devices CPU may initiate data transfers to or from PLD and or network interface . More particularly by discovering PCIe ports of switch PLD and network interface CPU may configure such devices and allocate address spaces such as physical address spaces for example respectively to each of such devices. Allocation of such address spaces allows CPU to communicate with switch PLD and network interface and additionally may allow switch PLD and network interface to communicate with each other without intervention from CPU or other motherboard system resources.

Such data block may be processed by a compute function of PLD . In this exemplary embodiment for secure data storage a secure parser may be used as such compute function. More particularly such secure parser may include a parse block and a restore block . Parse block may encrypt parse and or split data for example to provide outbound traffic. Restore block may restore inbound traffic such as restoring data using restore functions of secure parser for example to provide data in its original form.

Secure parser may be instantiated in whole or in part using field programmable logic of PLD . Algorithmic operations performed by secure parser may include one or more arithmetic operations or other data processing operations. Thus for example such data unit or other information may be cryptographically split into any size units of data. Such cryptographically split units of data for example may then be stored in DRAM or other subsystem or local memory coupled to PLD as generally indicated by arrow .

It should be understood that PLD may have a memory interface whether a hard macro or a soft core for writing data to or reading data from DRAM where such memory interface is accessible by secure parser . PLD may have internal memory which may be used instead of DRAM provided however the amount of such internal memory is sufficient for an application such as secure data storage for example.

For network interface to transmit encrypted data units stored in DRAM a Direct Memory Access DMA operation may be initiated by network interface using a DMA controller thereof. In other words DMA controller of network interface may provide one or more pointers or addresses to read out encrypted data units from DRAM as described below in additional detail. It should be understood that DMA controller is effectively coupled to DRAM via a memory interface of PLD through PCIe bussing and peer to peer routing of switch .

In order to obtain access to DRAM via a memory interface of PLD such DMA access may use addresses allocated by CPU for example as previously described to provide a read request that passes through switch to PLD using PCIe bussing and and peer to peer routing of PCIe switch . Such read request is processed by PLD including a memory interface thereof to read encrypted data units out of DRAM . Such read encrypted data units are passed back to network interface using the reverse of the above described path as generally indicated by arrow . Such read data units may then be transmitted via one or more of transceiver modules .

Accordingly it should be appreciated that once an initial data unit is passed from motherboard to daughtercard processed data from such data unit need not be routed back over a host system bus such as PCIe bus . Thus such processed data does not have to encumber CPU or other motherboard system resources. In other words data processing of such data unit is offloaded from CPU and subsequent movement of such processed data units does not have to pass over a system bus or otherwise encumber performance of other operations on motherboard . In particular this avoids burdening a system PCIe bus Southbridge a Northbridge and or a main CPU .

In an embodiment RPU may add redundancy as part of a parse function namely parse block . In such an embodiment an amount of data passing between RPU and network interface may be substantially greater due to addition of redundant data to an amount of data originally passed from system memory to RPU for such processing by parse block . It should be appreciated that in such an embodiment motherboard resources are not burdened with having to handle such added redundant data as well as any information associated therewith for such redundancy.

Secure parser is the same unit in . However in secure parser may be thought of as a secure restorer when in a restore mode. Restoration may vary from application to application. Accordingly for the above mentioned secure data storage restoration may generally be thought of as providing a data unit or units representing an original data unit or units respectively.

Responsive to a DMA initiated write by DMA controller of network interface such data blocks may be written to DRAM . Such a DMA initiated write command as well as received data blocks follow a direction as generally indicated by arrow . For example data blocks may go from network interface to switch via PCIe bus and from switch such data blocks may be routed to PLD for DRAM via a PCIe bus . Again addressing and peer to peer routing as previously described though in a reverse data flow direction may be used. Such data blocks may be written to DRAM and from DRAM such data blocks may be read out to a restore function block such as restore block as generally indicated by arrow .

Restore block may be instantiated in whole or in part in field programmable logic of PLD . In an embodiment assuming data blocks obtained by network interface are encrypted data read from memory into restore block may be decrypted by restore block as described elsewhere herein. For example two or more parsed and split portions of original data may be read from DRAM into restore block such as restored according to any of the techniques described in U.S. Patent Publication US20100299313A1 for example.

The resulting data unit or units may be provided to system memory in a data flow direction as generally indicated by arrow . More particularly such data unit or units may be provided from PLD to switch via a PCIe bus and then from switch to Southbridge via PCIe bus . Such data unit or units may be provided from Southbridge to system memory . It should be understood that such a data unit or units transferred via PCIe bus may already be completely processed with respect to a secure data storage application. Accordingly such PCIe bus as well as CPU among other resources of motherboard is not burdened with the processing of such data unit or units received by network interface . Furthermore it should be appreciated that each such data unit may be an exact copy of the data unit originally sent from system memory as previously described with reference to .

I O request packets IRPs are obtained by one or more upper filter drivers . Such IRPs may be provided from a user application or another driver higher in a storage driver stack. Thus user applications or higher level drivers may provide IRPs to one or more upper filter drivers . Such IRPs may be modified by one or more upper filter drivers before being passed to a next lower level driver as IRP . Such next lower level driver may be another storage filter driver or may be a storage class driver such as storage class driver . It should be understood that filter drivers may monitor performance of an underlying device.

Storage class driver may be configured to build one or more SCSI Request Blocks SRBs responsive to such one or more IRPs . Storage class driver may provide such one or more SRBs to one or more lower filter drivers . Such one or more lower filter drivers may modify SRBs to provide SRBs to storage port driver . Storage port driver may provide bus specific commands responsive to such one or more SRBs or may further modify SRBs to provide one or more other SRBs. Thus storage port driver may output bus specific commands or SRBs .

It should be understood that such one or more upper filter drivers unlike lower filter drivers can intercept IRPs sent to a class driver such as storage class driver and can alter such IRPs before forwarding them to a next lower level device object. So an upper filter driver can intercept read or write IRPs and transform data of such read or write IRPs as well as define additional I O control codes IOCTLs for example to cause a user application to supply passwords or other related information.

Disk partition device objects PDOs respectively at through may be generated as respective partitions namely partition 1 partition 2 and partition 3. Such disk PDOs may be generated by a disk class driver. Such disk class driver may generate a functional DO FDO for partition 0 at . In other words a disk class driver creates an FDO for a disk as a whole and PDOs for each partition on such disk.

At a disk PDO is generated by SCSI port miniport driver and at a SCSI adapter FDO is generated by such SCSI port mini port driver. Examples of other DOs that may be generated include those at through . More particularly at a CD ROM FDO may be generated by a CD ROM driver at a CD audio filter DO may be generated by a CD audio filter driver and at a CD ROM PDO may be generated by such SCSI port miniport driver that generated DOs at and . At a SCSI adapter PDO may be generated by a PCI bus driver. Optionally at a DO for an IEEE 1394 controller may be generated by an IEEE1394 controller driver. At a 1394 adapter PDO may be generated by a PCI bus driver employed at and such PCI bus driver may generate a PCI bus FDO at .

It should be appreciated that provide a general context for the description of . Additional general context for the description of some of the figures of may be obtained with reference to .

More particularly is a block diagram depicting a conventional Hyper V architecture and is a block diagram depicting a conventional Hyper V architecture for a storage model.

With simultaneous reference to in Microsoft s Hyper V hypervisor based virtualization architectures and a hypervisor or virtual machine monitor VMM is generally a hardware virtualization that allows multiple operating systems or virtual machines to run concurrently on a host computer. Such hardware virtualization is used to support isolation in terms of a parent partition and a child partition . It should be understood that a physical device may be controlled by an existing device driver without having to create a new device driver by using such a hypervisor.

A virtualization stack generally runs in a parent partition and has direct access to hardware devices. Such parent partition creates one or more child partitions which may host one or more guest operating systems. Child partitions do not have direct access to hardware resources such as disk storage for example but do have a virtual view of such resources in terms of virtual devices. Requests to virtual devices may be redirected via a virtual machine bus VMBus . Parent partitions execute a Virtualization Service Provider VSP which connects to a VMBus and handles device access requests from one or more child partitions . Generally a VSP runs within a parent partition or other partition that owns a hardware device such as disk storage . A VSP may communicate with a device driver and act as a multiplexer for offering hardware services. Child partition virtual devices execute a Virtualization Service Client VSC which redirects requests to one or more VSPs in a parent partition via a VMBus . Generally a VSC consumes a service.

There may be a VSP VSC pair per device type. A device protocol may be specific to a device type but generally operating system agnostic. Microsoft provided VSP VSC pairs include pairs for storage network video input and Universal Serial Bus USB uses.

As described below in additional detain VSP VSC pairs for storage and networking are used. As such Hyper V architectures of and VSP VSC pairs are well known they are not described in unnecessary detail herein for purposes of clarity.

Generally a VMware Server is a layer that exists between an operating system OS and virtual machines . An OS such as Windows or Linux runs on a hardware platform such as a server motherboard. Thus a VMware Server installs and runs as an application on top of a host Windows or Linux operating system.

A thin virtualization layer partitions a physical server to allow multiple virtual machines to be run simultaneously on such a single physical server. Computing resources of such a physical server may be treated as a uniform pool of resources that may be allocated to such virtual machines in a controlled manner. A VMware Server isolates each virtual machine from its host and other virtual machines which leaves each operating virtual machine unaffected if another virtual machine in the group were to crash or experience a cyber attack.

Moreover data does not leak across virtual machines and applications of such virtual machines may communicate over configured network connections. A VMware Server encapsulates a virtual machine environment as a set of files which may be backed up moved and or copied.

Having this context borne in mind the following descriptions of embodiments of a kernel mode a driver stack and a software flow among others should be more clearly understood.

An IRP is received by class driver . A general purpose processor such as CPU as previously described with reference to for example may execute a user application in an application mode causing such user application to provide one or more IRPs such as IRP to a class driver in a kernel mode.

In kernel mode flow in addition to class driver there is a filter driver a network software stack a network miniport driver and a device driver . Device driver may follow a framework for device drivers introduced by Microsoft known as a Windows Driver Model WDM . Within such WDM framework there are device function drivers including class drivers and miniport drivers. Further within such WDM framework there are bus drivers and optional filter drivers. An upper level filter driver is located above a primary driver for a device such as a class driver while a lower level filter driver is located below such class driver and above a bus driver. Thus filter driver is a lower level filter driver.

It should be understood that filter driver and device driver are not provided by Microsoft however filter driver and device driver are written to work within Microsoft s WDM framework. Filter driver and device driver are written to support accelerator system .

In contrast class driver and network software stack are provided by Microsoft. Furthermore network miniport driver may be provided by an independent hardware vendor IHV of network interface . Accordingly for purposes of clarity and not limitation generally only inter workings of filter driver and device driver are described below in additional detail.

Even though the following description is in terms of a WDM framework for purposes of clarity and not limitation it should be understood that other driver models may be used for operating with operating systems other than a Windows based operating system. Along those lines it should be understood that an operating system such as Linux may have similar software components to those of a WDM framework as described herein. Thus filter driver and device driver are applicable to operating systems other than Windows. Moreover drivers and may be implemented as virtual drivers such as in a virtual driver model and thus are applicable to virtual operating systems.

Again it should be understood that a secure data storage application is described for purposes of clarity and not limitation as other applications involving accelerated data processing may be used. So even though a network software stack and a network miniport driver are described it should be understood that another type of stack driver and or another type of miniport driver may be used in other applications. For example if storage devices were locally coupled namely not coupled through network interface then network software stack would be a storage software stack and network miniport driver would be a storage miniport driver . However for it shall be assumed that a network interface is used for communicating with multiple storage devices such as in cloud storage for example for purposes of clarity and not limitation.

For this secure data storage application data is encrypted and stored redundantly in multiple locations so that it may only be recovered by an authorized user yet such data may still be recovered if one or more of the storage devices is or becomes inoperable. Other details regarding such secure data storage application may be found in U.S. Patent Publication US20100299313A1 and in the above referenced provisional patent application.

For this secure data storage application when a user application issues a write or read such as to write or read a file of information it issues such command as if such data file was stored locally on a storage device such as a hard disk drive for example of a host system hosting such user application. Thus IRP from outward appearances may be a write or read for a data file stored locally on a hard disk drive for example. However such file data is encrypted parsed split stored within and or recombined from multiple storage devices such as multiple hard disk drives and such multiple storage devices may be at locations remote with respect to a computer system executing such user application. Even though the example of a hard disk drive is used it should be understood that any of a variety of storage devices many of which are listed elsewhere herein may be used.

For a write command of a data file IRP may include payload data . Class driver passes an SRB responsive to IRP to filter driver . Such SRB may include a command and a payload pointer for such write command. Filter driver provides a command responsive to IRP or more particularly SRB to device driver . Command which may be an Application Program Interface API command may include a system payload pointer pointing to payload data such as payload data in system memory for example. Such system payload pointer indicates an address where a host system believes such data file namely payload data is located. Filter driver may pass such API command to device driver where such API command includes a system payload pointer pointing to payload data . Device driver in communication with PLD invokes an API responsive to such API command to obtain and processes payload data responsive to command . Such payload data is obtained by PLD using such system payload pointer as generally indicated by dashed lines and .

Such payload data may be parsed split and or separated into two or more parts or portions by PLD and such parts or portions may be encrypted by PLD for storing in local DRAM as parsed payload data . Once parsed payload data is written into local DRAM PLD provides a notice of completion signal to device driver and device driver provides such complete signal to filter driver .

To recapitulate IRP may represent a single read or write command. Class driver may pass IRP to filter driver as an SRB . Alternatively IRP may be intercepted by filter driver . Such SRB includes such single read or write command and such single read or write command includes a system payload pointer. Such system payload pointer points to or indicates where a host system believes such payload is locally stored.

Continuing the example of IRP representing a single write command filter driver generates multiple write commands with payload pointers namely commands through N for N a positive integer greater than one collectively and singly commands . Generally such multiple commands are passed from filter driver to network software stack and network software stack passes such commands to network miniport driver . Network miniport driver provides such commands to network interface .

It should be understood that filter driver in generating payload pointers associated with commands effectively replaces a system payload pointer with local payload pointers for pointing to local DRAM as generally indicated by dashed line . Such local payload pointers are in read commands for reading local DRAM .

In this example application network interface uses such local payload pointers to read out parsed payload data namely to read out encrypted data blocks. It should be understood that for this secure data storage application redundancy information may be appended to payload data and thus parsed payload data may be significantly larger than payload data . Such redundancy information may be appended to the payload data to allow for restoration of such payload data using fewer than all of the portions of such payload data and such redundancy data may be stored in different remotely located storage devices. Furthermore as described above such payload data as well as such redundancy data thereof may be parsed split and or separated into smaller parts or portions. Filter driver when generating local payload pointers for commands accounts for payload size information in each command as such pointers have to account for payload size after processing by PLD .

It should further be understood that filter driver in generating commands accounts for storing parsed payload data in multiple storage devices one or more of which may be for redundancy using address information provided by a user application. More particularly with reference to such user application in an embodiment is an RPU administrative configuration application and such user application provides addressing information for both reads and writes. Such addresses or pointers may be in one or more generated SRBs as described below in additional detail.

Network interface may be coupled to a network as generally indicated for communication with such multiple storage devices. Network interface may be a host bus adapter communications HBA COM chip. As network interface receives each storage command associated with commands having traveled down a software stack into a miniport driver network interface performs a DMA operation to read parsed payload data using local payload pointers in commands . Such retrieved parsed payload data may be combined with command information in such storage commands to provide packets such as SRBs mentioned above and described below and such assembled packets may be transferred over a network to multiple storage devices.

If IRP were for a read operation namely a read command then such IRP would not include payload data. A user application may issue such a read command namely a single read command as if the data to be read such as a data file were located on a local storage device such as a local disk drive.

IRP is provided to class driver and class driver passes IRP to filter driver as an SRB . Alternatively IRP may be intercepted by filter driver as generally indicated by dashed line .

Filter driver generates multiple read commands responsive to IRP or SRB . Such read commands include address information for retrieval of data stored on multiple storage devices in a network cloud. Such commands are passed down through network software stack to network miniport driver . From such multiple storage devices network interface obtains data blocks and network interface asserts a DMA command for passing such data blocks to local DRAM for writing thereto as parsed payload data .

After parsed payload data is written back into local DRAM via network interface PLD provides a notice of completion signal to device driver and such notice of completion signal is provided to filter driver . Filter driver provides a read command to device driver in response to IRP or SRB . Device driver provides read command to PLD .

In response to read command PLD reverse processes parsed payload data such as for example decrypts data and then restores the data using the restore functions of secure parser to provide payload data as a single data file or single data block such as originally received for example. As described herein data may be restored according to any of the techniques described in U.S. Patent Publication US20100299313A1 as previously described.

PLD transfers such single data block as payload data in response to such IRP from a user application. In an embodiment PLD uses a DMA transfer into system memory to write payload data therein. PLD asserts a notice of completion signal to device driver for filter driver to indicate such writing of payload data to system memory . In response to notice of completion signal filter driver indicates to a user application that such read request has been completed.

Accordingly it should be understood that such secure data storage application as described may operate transparently with respect to a user application. In other words a user application may issue read and write requests as though requesting operations to be performed on a local storage device without knowledge that such above described operations are performed for providing parsed payload data for example. It should further be appreciated that because of parsing and or redundancy parsed payload data may be significantly larger than payload data and thus data transferred over network interface may be significantly more voluminous than payload data namely data seen by a user application.

Furthermore locally temporarily stored or maintained data may be processed in an accelerated manner by PLD by instantiating one or more data processing algorithms in programmable logic where such algorithms are effectively replicated in circuitry. Along those lines only original payload data for a write operation or process data to restore such original payload data for a read operation is transferred over system PCIe bus such as for going from or to system memory . Thus the data handling and or data processing burden on one or more motherboard system resources as previously described herein is significantly reduced. Such burden reduction may enhance overall operational efficiency of a host system.

Application is in communication with class driver and class driver is in communication with filter driver . Again for purposes of clarity and not limitation the example of a secure data storage application is used and accordingly filter driver is parenthetically indicated as a secure parser. Filter driver is in communication with device driver and port driver . Port driver is in communication with miniport driver . Port driver and miniport driver respectively correspond to software stack and miniport driver . Miniport driver is in communication with network interface and device driver is in communication with RPU .

Application which is a user application communicates with class driver . Class driver communicates with filter driver . Class driver may pass what may be termed plaintext to filter driver . Filter driver separates a control path from a data path as described below in additional detail.

PCIe bus is the relative location at which software components transition to hardware blocks. Accelerator system of is generally represented by network interface coupled to switch and switch is coupled to RPU . Accordingly RPU includes DRAM . Switch may be thought of as a point to point bus P2P bus . Communication between network interface and RPU through switch may be generally thought of as a data only path .

Filter driver is in communication with device driver via a command and data path . Device driver is in communication with RPU via command and data path . Command and data paths and may be referred to as cleartext paths. In contrast data only path is an encrypted only data path namely a ciphertext path. RPU is further in communication with device drivers via command only path . Device driver is further in communication with filter driver via command only path . In other words only commands are passed via paths and .

Command only paths and are cleartext paths. Moreover commands provided via command only paths and are parsed out commands from a single command as previously described with reference to . In other words commands provided via command only paths and may be thought of as N shares corresponding to N parts or portions of data stored in DRAM . Thus filter driver may provide N shares of commands via command only path for device driver and device driver may pass such N shares of commands to RPU via command only path . N shares of commands may be passed from filter driver to port driver as previously described with reference to .

In kernel mode class driver is broken out into four parts provided by Microsoft namely a transport driver interface winsock kernel TDI WSK module and I O manager forwards requests to file system module a file system driver processes and forwards modified request module and an I O manager . Generally commands and data to be transferred over network go through module and commands and data going to or from storage media go through modules and . Commands to configure and initialize an iSCSI initiator go through I O manager . Other known details regarding class driver are not provided for purposes of clarity and not limitation.

Commands and data from class driver are provided as cleartext to one or more filter drivers . Commands to set up and initialize filter driver and device driver are respectively provided via paths and . Commands to set up and initialize RPU are provided via path to device driver for RPU via PCIe bus using command and data path .

One or more filter drivers are used to separate command information from data so such separate types of information may take separate paths through software and hardware as previously described. One or more filter drivers are in communication with port driver via command only path .

Port driver may generally be separated out into two software stacks of Microsoft software components namely one for network commands and another one for storage device commands. The stack for network commands follows a TCP IP protocol and the stack for storage device commands follows a SCSI protocol. Port driver for network commands includes a TCP IP module a TCP offload engine bus and a network driver interface specification NDIS module . Port driver for storage commands includes volume manager partition manager and disk manager . Other known details regarding port driver are not provided for purposes of clarity and not limitation.

Miniport driver which may be supplied by a vendor of a communication device or storage device depending on whether such miniport driver is for a network interface or a storage device interface likewise may be separated out as was port driver . A software stack for network commands of port driver is in communication with an NDIS miniport driver of miniport driver . More particularly NDIS miniport driver is in communication with NDIS module . NDIS miniport driver is used to manage a network interface such as a NIC including sending and receiving data through such a NIC.

A software stack for storage device commands of port driver is in communication with a SCSI miniport driver of miniport driver . SCSI miniport driver or HBA driver manages an HBA for SCSI commands data and processing. SCSI miniport driver is in communication with disk manager and I O manager .

Both an NDIS miniport driver and a SCSI miniport driver may be used as supplied by an IHV of a network interface such as a NIC. It should be understood that miniport drivers and both communicate with a hardware network interface device. Other known details regarding miniport driver are not provided for purposes of clarity and not limitation.

In such hardware network interface device is shown as separate boxes depending on whether commands are for network traffic or storage traffic. For network traffic NDIS miniport driver is in communication with one or more COM devices . Any of a variety of COM devices may be managed by NDIS miniport driver . Examples of such COM devices include without limitation an Ethernet NIC a WiFi device a WiMax device an iWARP device a WSD device an RNDIS device and a TOE device. For storage traffic SCSI miniport driver is in communication with one or more storage interface devices . Any of a variety of storage interface devices may be managed by SCSI miniport driver . Examples of storage interface devices include without limitation an iSCSI device a SCSI device and an FCoE device.

It should be understood that a single IC may be used to provide both a network interface and a storage device interface covering one or more protocols of each of such interfaces. Thus even though two separate boxes are illustratively depicted for one or more COM devices and one or more storage interface devices such two separate boxes may be implemented in a single IC . Such a single IC may have network I O interface and storage I O interface .

PLD of RPU may include a DMA module for communication with DRAM . Again communication between PLD and IC with respect to data is via data only path . Furthermore as previously indicated there may be some address translation or remapping of an SRB with a data buffer to point to DRAM as generally indicated by line spanning port driver and miniport driver as well as pointing to the interface between switch and DRAM . Additionally such remapping at may involve a remap of cleartext logical unit number LUN and logical block addressing LBA SCSI parameters.

User application may be in communication with a file system and a disk driver . For purposes of clarity by way of example and not limitation it shall be assumed that a SCSI protocol is used however other types of storage protocols may be used. Accordingly disk driver may be a SCSI class driver. File system is in communication with disk driver . It should be understood that file system and disk driver may be provided by Microsoft and user application may be any compatible user application. Accordingly user application file system and disk driver are not described in unnecessary detail for purposes of clarity and not limitation.

Lower filter driver is in communication with a RAM disk device driver disk driver SCSI device driver and iSCSI device driver . RAM disk device driver is additionally in communication with secure parser iSCSI device driver and a security application . Secure parser is in communication with security application and RPU . Security application may be application as previously described with reference to .

Lower filter driver may receive an SRB from disk driver as previously described. Lower filter driver may monitor drivers through . SCSI device driver may be in communication with local hardware storage such as one or more storage devices using a SCSI protocol. iSCSI device driver may be in communication with one or more storage interface devices as previously described with reference to . One or more storage interface devices may be for communicating with one or more remotely located hardware storage such as one or more storage devices in a network cloud. It should be understood that device drivers and may be obtained from manufacturers of storage devices.

Secure parser RAM disk device driver and lower filter driver in combination may be operate as previously described with reference to filter driver and device driver but with the addition of a RAM disk operation of DRAM as generally indicated by a dashed line extending between RAM disk device driver and DRAM . Additionally RAM disk device driver may communicate with iSCSI device driver via an M to 1 1 to M M 1 1 M SCSI command bus .

Effectively RAM disk device driver is configured by security Application to treat DRAM like a local RAM disk drive. Thus a read or write request from user application may be provided to RAM disk device driver for writing to DRAM . As previously described such read or write request may involve one or more of encrypting parsing splitting decrypting recombining or restoring data. Thus for example parsed payload data in DRAM may be provided to or be obtained from hardware storage and or hardware storage as generally indicated by dashed lines and respectively. Other details regarding operation of SAN were previously described elsewhere herein and thus are not repeated for purposes of clarity and not limitation.

VM switch such as from Microsoft may include a routing virtual LAN VLAN filtering data copy module and multiple ports such as port 1 P1 and port 2 P2 . Module is in communication with VM buses and of VM bus module such as from Microsoft. VM bus module may be used by VM switch to switch between different VM network blocks such as network virtual machines using VLAN tagging provided by module .

Multiple network virtual machines namely in this exemplary embodiment 128 network virtual machines VM1 through VM128 are coupled to VM bussing of VM bus module . Each network virtual machine such as VM1 for example includes a respective TCP IP module and a respective VM network interface e.g. NIC1 for VM1 and NIC128 for VM128 . VM switch VM bus module and network virtual machines are known and thus are not described in unnecessary detail herein. It should be understood that 128 network virtual machines have switched access to two VM buses namely VM buses and for access to ports P1 and P2 respectively.

Filter driver is a virtualization of filter driver of and device driver is of virtualization device driver of . Miniport driver is a virtualization of a network miniport driver such as miniport driver of . As generally indicated by line filter driver is in communication with module and filter driver is in communication with device driver . Furthermore as generally indicated by line device driver is in communication with a queue of RPU . Thus commands and data may be passed to and from queue to module .

RPU may have one or more encryption and decryption cryptographic engines therein including without limitation instantiated therein in programmable logic coupled to queue . As generally indicated by line queue of RPU is in communication with device driver and device driver is in communication with filter driver . Furthermore as generally indicated by line filter driver is in communication with miniport driver and miniport driver is in communication with queue of network interface . Thus commands and data may be passed to and from queues and .

In addition to queue network interface includes channel switch and a plurality of media access controllers . For purposes of clarity the terms media access control and medium access controller are used interchangeably herein and either or both are referred to as a MAC. Channel switch is for coupling queue to a selected MAC of MACs for communication via Ethernet . Even though four MACs are illustratively depicted fewer or more MACs may be used.

For a secure data storage application data to and from VM switch and queue may be unencrypted however data from queue to queue generally would be encrypted by one or more of cryptographic engines for a transmit direction. In a receive direction encrypted data from queue provided to queue would be decrypted by one or more cryptographic engines for providing to VM switch .

In network I O system is similar to network I O system of and thus generally only the differences between the two systems are described for purposes of clarity and not limitation. In network I O system module is omitted.

VM switch has P1 through P128 ports of ports in communication with Q1 through Q128 queues of queues of RPU . Thus ports correspond to network virtual machines and ports correspond to queues . Furthermore queues correspond to queues .

Ports are in communication with queues through filter driver and device driver . In other words ports are in communication with filter driver through paths filter driver is in communication with device driver through paths and device driver is in communication with queues through paths.

RPU includes multiplexing circuitry for selectively coupling one or more cryptographic engines to a selected queue of queues .

Queues are respectively in communication with queues of network interface through device driver and miniport driver . More particularly Q1 through Q128 of queues are in communication with device driver through paths device driver is in communication with miniport driver through paths and miniport driver is in communication with queues through paths.

Network interface includes Q1 through Q128 queues of queues . One or more of queues are selectively coupled to a MAC of MACs via channel switch .

In network I O system is similar to network I O system of and thus generally only the differences between the two systems are described for purposes of clarity and not limitation. In network I O system VM switch is replaced with a VM monitor having a port P0. Furthermore VM switch is omitted and ports run on management OS directly and not through switch access via a VM switch. Accordingly VM bus module may have respective channels for virtually respectively coupling each of ports to each of virtual machines . VM monitor is in communication with filter driver via port P0 for monitoring such driver.

It should be understood that in each of systems through cryptographic engines encrypt and decrypt all data traffic from and to networking VMs or more particularly to or from a target networking VM . Furthermore even though an example of 128 VMs was used it should be understood that fewer or more networking VMs may be used.

Each VM through VM respectively includes an RPU NIC filter driver an RPU storage filter driver an NIC switch driver and a SCSI switch driver . SCSI switch drivers are in communication with VMWare VM monitor . NIC switch driver of VM is in communication with VMWare VM monitor .

VM includes a PCIe RPU SR secure parser and a PCIe SR NIC . VM includes a PCIe RPU secure parser without SR and a PCIe SCSI HBA without SR. VMs and are in communication with VMWare VM monitor . NIC switch drivers of VMs and are in communication with SCSI HBA . RPU NIC filter drivers of VMs and are in communication with secure parser .

NIC switch drivers of VMs and are in communication with NIC . RPU NIC filter drivers of VMs and are in communication with secure parser .

RPU NIC filter drivers and RPU storage filter drivers are added to VMs through where such VMs through apart from such drivers and are obtained from VMWare Inc. Secure parsers and are added to VMs and respectively where such VMs and apart from such parsers and are obtained from VMWare Inc. VMWare VM monitor is obtained from VMWare Inc. Drivers and as well as an NIC and SCSI HBA are obtained from the vendor or manufacturer of an associated NIC and or SCSI interface. Drivers and as well as secure parsers and may be virtualizations of filter driver and device driver of for used in a VMware server environment.

At one or more SRBs are provided from storage class driver such as storage class driver . For purposes of clarity by way of example not limitation it shall be assumed that a single SRB is processed even though multiple SRBs may be processed at a time.

At such SRB is interrogated to determine whether it is for a write command. For purposes of clarity by way of example not limitation it shall be assumed that a SCSI protocol is used even though in other embodiments other protocols may be used. Thus for example at an SRB is interrogated to determine whether it is a SCSI write command. If at it is determined that such SRB is not a SCSI write command then at it is determined whether such SRB is a SCSI read command. If it is determined at that such SRB is for a SCSI read command then processing of such SCSI read command is described with reference to a read through a filter driver flow of . If however it is determined at that such SRB is not a SCSI read command then at such SRB is provided to one or more lower order filter lower filter drivers.

If however it is determined at that such SRB is for a SCSI write command then at an envelope structure is allocated for such SRB. At such envelope is linked to such a SCSI write SRB allocated from memory mapped adapter DRAM. At such write SRB is enqueued namely added to a queue. At output buffer pointers are initialized for each SRB and a data pointer of such SRB obtained from class driver is passed as a data buffer pointer. At output buffers are allocated from memory mapped DRAM such as DRAM . At MAC digest buffers are allocated and a MAC digest pointer is initialized. At a share stride is initialized. In this example embodiment a stride of eight shares is used however in other embodiments fewer or more than eight shares may be used.

At an encryption key encKey an encryption initialization vector encIV an information dispersal algorithm key idaKey a MAC mode and MAC key and a MAC initialization vector are initialized. At a parse data call for RPU is composed with the envelope structure or envelop initialized or allocated at . At a device driver function call is made by device driver to RPU to perform data encryption and secure parsing operations on such data. As previously described elsewhere herein such secure parsing operations may include parsing and splitting such data into any size data units. For example parsing and splitting operations in accordance with a secure parser as described elsewhere herein may include but are not limited to 1 cryptographically split disperse and securely store data shares in multiple locations 2 encrypt cryptographically split disperse and securely store data shares in multiple locations 3 encrypt cryptographically split encrypt each share then disperse and securely store data shares in multiple locations and 4 encrypt cryptographically split encrypt each share with a different type of encryption than was used in the first step then disperse and securely store the data shares in multiple locations.

At device driver invokes an application programming interface API at for communicating with RPU for such secure parsing operations. At such secure parsing operations having been completed by RPU device driver returns control to filter driver . At filter driver receives an indication that RPU as completed secure parsing operations and updates results from such secure parsing operations such envelope structure allocated at .

At it is determined whether MAC authentication was successful. If at it is determined that MAC authentication was not successful then filter driver flow provides an error status errors out at . If however it is determined that MAC authentication was successful at then at an SRB queue is searched for an envelope matching such envelope updated at .

At it is determined whether an envelope obtained from such search at matches such envelope updated at . If such envelopes do not match as determined at then such searching resumes at until a matching envelope is located. If however a matching envelope is located as determined at then at the matching envelope containing SRB is dequeued from such SRB queue searched at .

At a command to compose a number of new SRBs respectively for each of the shares of securely parsed data is asserted. For purposes of clarity by way of example and not limitation it shall be assumed that there are eight shares. However in other embodiments fewer or more than eight shares may be used.

At a new SRB is constructed for each share. For construction of an SRB for a share a current SRB path identifier namely a path identifier obtained from such SRB provided from storage class driver is set equal to a share new SRB path identifier DrcSrb PathId SRB PathId and a current SRB target identifier is set equal to a new SRB target identifier. Further for this construction a current SRB LUN is set equal to a new SRB LUN. Such newly constructed SRB s data buffer pointer is set equal to such envelope structure s output data buffer pointer indexed by share number e.g. share number 1 of 8 .

At it is determined whether a share number value or share number index has reached 8 namely is less than eight. If it is determined at that the share number is less than eight then composition of another share SRB at is commenced for subsequent construction of another share SRB at . If however it is determined at that a share number index is not less than eight then at the 8 newly constructed share SRBs are sent to one or more lower filter drivers for receipt at . In other embodiments fewer or more than eight new SCSI write commands may be sent at as fewer or more share SRBs may be constructed. Furthermore at DRAM memory may be cleared or otherwise made available when such write commands have completed. In other words such output buffers having such eight SRBs respectively stored may be indicated as being available for reuse.

At one or more SRBs are provided from storage class driver such as class driver . For purposes of clarity by way of example not limitation it shall be assumed that a single SRB is processed even though multiple SRBs may be processed at a time.

At such SRB is interrogated to determine whether it is for a SCSI read command. For purposes of clarity by way of example not limitation it shall be assumed that a SCSI protocol is used even though in other embodiments other protocols may be used. Thus for example at an SRB is interrogated to determine whether it is for a SCSI write command. If such SRB is for a SCSI write command as determined at then such command is processed as previously described with reference to filter driver flow . If however it is determined at that such SRB is not for a SCSI write command then at it is determined whether such SRB is for a SCSI read command.

If at is determined that such SRB is not for a SCSI read command then at such SRB is passed down to a next lower filter driver. If however at it is determined that such SRB is for a SCSI read command then a share number is initialized such as equaling zero for example at .

At it is determined whether such share number is less than eight. Again it should be understood that in other embodiments such share number may be less or more than eight. If such share number is not less than eight as determined at then at eight new SCSI read commands are sent to a next lower filter driver for receipt at . In other embodiments the number of new SCSI read commands sent at may be fewer or more than eight corresponding to the share number.

It should be understood that each share may be associated with any size data unit and shares may be associated with any size data units where such data units have been parsed and split from a single set of data into two or more portions or shares of data as previously described elsewhere herein. If however at it is determined that the share number is less than eight then at memory mapped DRAM is allocated to a share indexed by share number.

At an SRB for such indexed share is constructed. For construction of an SRB for a share a current SRB path identifier namely a path identifier obtained from such SRB provided from storage class driver is set equal to an share new SRB path identifier DrcSrb PathId SRB PathId and a current SRB target identifier is set equal to a new SRB target identifier. Further for this construction a current SRB LUN is set equal to a new SRB LUN. Such newly constructed SRB is passed to a data buffer where such data buffer is as an address space or portion of DRAM allocated at . In other words a share has its own data buffer or buffer address space for storing its SRB as indexed by its share number e.g. share number 1 of 8 .

At a new SCSI read command is composed for a share. After such composition it is determined again at whether or not the share number index is less than eight. This loop continues until it is determined at that the share number is not less than eight. In this example embodiment this loop continues until eight share SRBs have been constructed. In other words after completion of this loop there are eight share SRBs respectively indexed from 1 to 8 respectively allocated a data buffer and each with an associated SCSI read command.

If at is determined that the share number is not less than eight then at such at SCSI read commands composed as previously described are sent to a next lower filter driver at . At control of SCSI reads of such shares is returned to filter driver from such one or more lower filter drivers. It should be appreciated that such one or more lower filter drivers may be for one or more storage devices as previously described herein.

At a SCSI read complete indexed to share number is updated by a share number for each of the shares read using one or more lower filter drivers . At it is determined whether such SCSI read complete index is less than eight. If at it is determined that such SCSI read complete index is less than eight then at nothing is done rather filter driver flow is in a wait state waiting for completion of the last of such SCSI reads.

If however at it is determined that the share number is not less than eight then at an envelope structure for such read shares is allocated. At such envelope structure allocated at is linked to such read SRBs for each of such shares. At such read SRBs are enqueued. At output buffer pointers are initialized for each share SRB for passing as a data buffer pointer.

At pointers for input buffers are initialized for each share of allocated memory mapped DRAM allocated at . At MAC digest buffers are allocated and a MAC digest pointer is initialized. At a share stride is initialized.

At an encryption key an encryption IV an ida key a MAC mode a MAC key and a MAC IV are all initialized. At a restored data call for RPU is composed with such initialized for allocated and share SRB linked envelope. At a function call to device driver is made by filter driver for a restore data function of RPU with a parameter of an envelope structure pointer.

At device driver invokes an API at for communicating with a restorer of RPU for restoring encrypted data to a single unencrypted set of data such as for example unpacking share SRBs by first recombining then decrypting such data obtained therefrom. At such restoring application invoked at is completed by RPU and RPU provides a notice of completion to device driver . Data that is restored by a restorer of RPU may in some applications not be in an encrypted state from which it is restored. Accordingly each portion of parsed data may be secured using any of a variety protocols provided however that such data may be reassembled reconstituted reformed and or decrypted to restored to its original or other usable form. Accordingly restoring data may involve reversing any of a number of operations used to secure such data in accordance with the description herein.

At a return of control to filter driver from device driver is provided as a single data block is restored. At completion of such restoration by RPU is recorded by updating a result in such an envelope structure links at to read share SRBs.

At it is determined whether MAC authentication was successful. If MAC authentication was not successful at then filter driver flow errors out at . If however MAC authentication was successful at then at an SRB queue is search for and envelope matching such envelope updated at . At it is determined whether an envelope obtained from such SRB queue at matches such envelope of . If at it is determined that there is not a match between such envelopes then searching continues at . This loop continues until a match is found.

If however at it is determined that such envelopes match then the matching envelope obtained from such SRB queue at is dequeued from such SRB queue at . At SCSI read control is returned from filter driver to storage class driver at .

At an API for RPU is invoked as previously described. At a spinlock is acquired. At a sequence identifier is incremented such as incremented by one for example. Such sequence identifier may be incremented for each invocation of device driver flow and thus such sequence identifier may be used as a tag for subsequent reference. At an envelope is enqueued for a sequence identifier as incremented at .

At an encryption command is set up. Such set up includes initialization of each of the following a share number an encryption mode an ida mode an MAC mode an encryption key an encryption IV an ida key and a MAC key.

At it is determined whether return status was successful. If return status failed as determined at then device driver flow errors out at and such error status is indicated as a pipeline status at . At it is determined whether a package queue has overflowed. If it is determined that a package queue has overflowed at then an error out is asserted at . If after either assertion of an error out at or a determination that a package queue has not overflowed at at is determined whether such a pipeline is full. If it is determined that such pipeline is full at then an error out is asserted at . After either an error out is asserted at or it is determined that a pipeline is not full as determined at spinlock acquired at is released at . At control is returned to filter driver as previously described.

If however it is determined at that return status was successful then such encryption command set up at is sent at . At it is determined whether return status was successful. If it is determined that that return status was not successful then an error out is asserted at and processing continues as previously described starting from .

If however it is determined at that return status was successful then DMA status is disabled at . Furthermore at an interrupt is disabled. At it is determined whether data length is either greater than a maximum length allowed or equal to zero. If it is determined at that data length is either greater than a maximum length allowed or equal to zero then an error out is asserted at and processing continues as previously described starting from .

If however it is determined at that data length is neither greater than a maximum length allowed or equal to zero then it is determined at whether a share number is not equal to eight. Again the number of shares such as for example the number of portions of data in other embodiments may be less than or greater than eight. If at it is determined that the share number is not equal to eight then at and error out status is asserted and processing continues as previously described starting from .

If however at is determined that the share number does equal eight then at a command to set up RPU to read enciphered or encrypted data after such data has been parsed is sent. At it is determined whether return status was successful. If it is determined at that return status was not successful then at an error out is asserted and processing continues as previously described starting from .

If however at it is determined that return status was successful then at a command is sent to RPU to write data of read share SRBs by RPU as cleartext. At it is determined whether return status was successful. If at it is determined that return status was not successful then at an error out is asserted and processing continues as previously described starting from .

If however at it is determined that return status was successful then at DMA status indication is activated and an interrupt generation is activated. At a command is sent to read a message digest of RPU for writing to a digest memory buffer. Such digest memory buffer may be in system memory such as system memory for example as may be associated with Message Signaled Interrupts MSI .

At it is determined whether return status was successful. If at it is determined that return status was not successful an error out is asserted at and processing continues as previously described starting from . If however it is determined at that return status was successful the encryption and MAC keys set up at are deleted at . After such deletion processing continues as previously described starting from .

At device driver invokes a data restore API for RPU as previously described. Operations through respectively correspond to operations through of except that rather than sending an encryption command at a decryption command is sent at . Accordingly the remainder of the description of operations through is not repeated for purposes of clarity. After disabling DMA status and disabling an interrupt at at a command is sent to RPU to read data where such data is cleartext as having been decrypted at .

At it is determined whether return status was successful. If at it is determined that return status was not successful then an error out is asserted at and an indication of pipeline status is provided at . As operations at through respectively correspond to operations through of description of those operations is not repeated for purposes of clarity.

If however at it is determined that return status was successful then at it is determined whether data length is either greater than a maximum share length allowed or equal to zero. As previously described with reference to a maximum data length was for a single set of data to be parsed. A maximum share length is for each share such as for example a maximum length of a subset of such single data block.

If at it is determined that data length is either greater than a maximum share length allowed or equal to zero then an error out is asserted at and processing continues starting from . If however at it is determined that data length is neither greater than a maximum share length allowed or equal to zero then at is determined whether a share number does not equal eight. Operations at and respectively correspond to operations at and of and thus description of those operations is not repeated for purposes of clarity.

If at it is determined that share number does equals eight then at a command is sent to RPU to write split or parsed shares as a single data block. At it is determined whether return status was successful. Operations through respectively correspond to operations through of except that activating DMA status indication and activating an interrupt generation at is for DMA write operations for writing a single data block. In contrast activating DMA status indication and activating an interrupt generation at of was for DMA read operations for output of parsed encrypted shares to be written to storage devices as described elsewhere herein. Additionally it should be understood that keys deleted at were set up at for device driver flow . The remainder of the description of operations through is not repeated for purposes of clarity.

At an MSI interrupt service routine for RPU RpuMsilsr is initiated. At an MSI interrupt is claimed. At an interrupt DPC is scheduled for RPU . Dashed line generally indicates initiation of such scheduled RPU DPC at .

At control of an MSI ISR portion of flow is returned to an OS. It should be understood that an MSI ISR portion is at a significantly higher priority level than the remainder of flow namely a DPC portion. By separating MSI ISR and DPC portions control for such MSI ISR portion can be returned to a host system OS as quickly while allowing continuation of DPC portion to limit performance impact on such host system.

At a DPC for RPU is initiated. At a spinlock is acquired. At data is processed for secure parsing thereof and such processed data is written as previously described elsewhere herein.

At it is determined whether DMA status has a valid identification and sequence number. In other words although in this embodiment DMA processes only one transaction at a time it is capable of queuing multiple DMA commands. This way DMA can process DMA transactions without gaps to reduce overhead. However the number of multiple DMA commands queued is limited to a maximum number and at it is determined whether such maximum number has been reached. If it is determined at that DMA status is valid then at it is determined whether there is any DMA interrupt queued.

If it is determined at that there is any DMA interrupt queued then at each envelope for each DMA interrupt sequence identifier is dequeued. At a function call is made for secure parsed data completion with a call back with each envelope dequeued at . From it is again determined at whether DMA status is valid.

If at it is determined either that DMA status is not valid at or that there is no DMA interrupt in a queue at then at it is determined whether DMA command entries are less than or equal to a maximum number of commands e.g. a high water mark . If at it is determined that DMA command entries are less than or equal to such a high water mark then at a pipeline is full flag is cleared or left in a clear state. If however at it is determined that DMA command entries are greater than such a high water mark then at such pipeline full flag is set or left in a set state.

After setting or clearing such pipeline full flag as previously described at and respectively at the spinlock acquired at is released. At another spinlock is acquired. It should be understood that the spinlock acquired at is for a data parsing and encrypting portion however the spinlock acquired at is for a data decrypting restore portion.

At a command to read and restore securely parsed data is initiated. Operations at through correspond to operations at through and thus repetition of such description is avoided for purposes of clarity.

After dequeuing at at a share number index is initialized such as set to zero for example. At it is determined whether such share number index is less than eight. Again it should be understood that a share number less than or greater than eight may be used in other embodiments.

At a digest from a restore engine of RPU is copied to an envelope digest buffer for storing therein information on a share. After copying at it is again determined at whether a share number index is less than eight. Accordingly this loop continues until a digest from restore engine of RPU is copied to an envelope digest buffer for storing therein information on each of the shares read.

If at it is determined that a share number index is not less than eight then at a function call is made to indicate completion of read data having been restored. Such function call may include a call back with a dequeued envelope. From it is determined again whether DMA status is valid at .

If it is determined that either DMA status is invalid at or no DMA interrupt is in a queue at then it is determined whether DMA command entries are less than or equal to a high water mark at . Operations through respectively correspond to operations through and thus description of operations through is not repeated for purposes of clarity. After the spinlock acquired at is released at flow may return at such as for example to a host system OS from which it was called.

Programmed computer may be programmed with a known operating system which may be Mac OS Java Virtual Machine Linux Solaris Unix or a Windows operating system among other known platforms. Programmed computer includes a central processing unit CPU memory and an input output I O interface . CPU may be a type of microprocessor known in the art such as available from IBM Intel ARM and Advanced Micro Devices for example. Support circuits not shown may include cache power supplies clock circuits data registers and the like. Memory may be directly coupled to CPU or coupled through I O interface . At least a portion of an operating system may be disposed in memory . Memory may include one or more of the following random access memory read only memory magneto resistive read write memory optical read write memory cache memory magnetic read write memory and the like as well as non transitory signal bearing media as described below.

I O interface may include chip set chips graphics processors and daughter cards among other known circuits. An example of a daughter card may include a network interface card a display interface card a modem card and or a Universal Serial Bus USB interface card. Furthermore I O interface may include a daughter card or as described herein.

I O interface may be coupled to a conventional keyboard network mouse display printer and interface circuitry adapted to receive and transmit data such as data files and the like. Programmed computer may be a server computer or a workstation computer. Thus computer may be coupled to a number of client computers server computers or any combination thereof via a conventional network infrastructure such as a company s Intranet and or the Internet for example allowing distributed use for interface generation.

Memory may store all or portions of one or more programs or data to implement processes in a non transitory machine readable medium in accordance with one or more embodiments hereof to provide any one or more of filter driver device driver lower filter driver RAM disk device driver secure parser filter driver device driver NIC filter driver storage filter driver secure parser secure parser filter driver flow filter driver flow device driver flow device driver flow and or ISR DPC flow as program product . Additionally those skilled in the art will appreciate that one or more embodiments hereof may be implemented in hardware software or a combination of hardware and software. Such implementations may include a number of processors or processor cores independently executing various programs and dedicated hardware or programmable hardware.

One or more program s of program product as well as documents thereof may define functions of embodiments hereof and can be contained on a variety of non transitory signal bearing media such as computer readable media having code which include but are not limited to i information permanently stored on non writable storage media e.g. read only memory devices within a computer such as CD ROM or DVD ROM disks readable by a CD ROM drive or a DVD drive or ii alterable information stored on writable storage media e.g. floppy disks within a diskette drive or hard disk drive or read writable CD or read writable DVD . The above embodiments specifically include information downloaded from the Internet and other networks. Such non transitory signal bearing media when carrying computer readable instructions that direct functions hereof represent embodiments hereof.

While the foregoing describes exemplary embodiment s in accordance with one or more embodiments other and further embodiment s in accordance with the one or more embodiments may be devised without departing from the scope thereof which is determined by the claim s that follow and equivalents thereof. Claim s listing steps do not imply any order of the steps. Trademarks are the property of their respective owners.

