---

title: Systems and methods for instantaneous cloning
abstract: Techniques to clone a writeable data object in non-persistent memory are disclosed. The writeable data object is stored in a storage structure in non-persistent memory that corresponds to a portion of a persistent storage. The techniques enable cloning of the writeable data object without having to wait until the writeable data object is saved to the persistent storage and without needing to quiesce incoming operations (e.g., reads and writes) to the writeable data object.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09529551&OS=09529551&RS=09529551
owner: NETAPP, INC.
number: 09529551
owner_city: Sunnyvale
owner_country: US
publication_date: 20140717
---
This application is a continuation of U.S. patent application Ser. No. 13 781 462 filed on Feb. 28 2013 which is a continuation in part of U.S. patent application Ser. No. 13 098 310 filed on Apr. 29 2011 both of which are incorporated herein by reference in their entirety.

At least one embodiment of the technique introduced here pertains to data storage systems and more particularly to cloning a writeable data object in memory.

A portion of the disclosure of this patent document contains material which is subject to copyright protection. The copyright owner has no objection to the facsimile reproduction by anyone of the patent document or the patent disclosure as it appears in the Patent and Trademark Office patent file or records but otherwise reserves all copyright rights whatsoever. The following notice applies to the software and data as described below and in the drawings hereto Copyright 2011 NetApp Inc. All Rights Reserved.

Typically cloning technology for storage systems works by building a copy of an object that is stored on a persistent storage medium such as a hard disk. The object on disk could be a file a volume or a data object i.e. a logical data container . If the source object for cloning already resides on persistent storage such as a hard disk and is not undergoing any changes the cloning can be done by making an image of the object on disk to another disk or another location on the same disk. One example of this is cloning of a read only file on disk. Cloning a data object in memory can also be done in a simpler fashion if the data object is read only.

Cloning a data object that is actively being modified however typically requires the cloning operation to finish before applying any subsequent incoming operation commands to modify the data object. This creates a delay that slows down the storage system. No specific solutions have been found that resolve this problem adequately.

The techniques introduced here enable instantaneous cloning of a writeable data object in memory. The writeable data object is stored in a storage structure in memory that corresponds to a portion of a persistent storage. Instantaneous cloning means the ability to clone the writeable data object without having to wait until the writeable data object is saved to the persistent storage and without needing to quiesce incoming operations e.g. reads and writes to the writeable data object.

Conventional cloning is done by building a copy of an object in persistent storage. This creates a delay and makes it difficult to clone a writeable data object that can be actively modified at any time.

To allow for instantaneous cloning of the writeable data object in memory the technique introduced here includes a mechanism to manage a write operation to the storage structure in memory. This mechanism enables allocation of a data pointer to the writeable data object to be done without waiting until the writeable data object is saved to the persistent storage. This mechanism also enables freeing of an assigned data pointer to the writeable data object prior to the write operation to execute without waiting until the writeable data object is saved to the persistent storage.

Some embodiments have other aspects elements features and steps in addition to or in place of what is described above. These potential additions and replacements are described throughout the rest of the specification

In the following detailed description of embodiments of the invention reference is made to the accompanying drawings in which like references indicate similar elements and in which is shown by way of illustration specific embodiments in which the invention may be practiced. These embodiments are described in sufficient detail to enable those skilled in the art to practice the invention and it is to be understood that other embodiments may be utilized and that logical mechanical electrical functional and other changes may be made without departing from the scope of the present invention. The following detailed description is therefore not to be taken in a limiting sense and the scope of the present invention is defined only by the appended claims. References in this specification to an embodiment one embodiment or the like mean that the particular feature structure or characteristic being described is included in at least one embodiment of the present invention. However occurrences of such phrases in this specification do not necessarily all refer to the same embodiment.

Referring now to therein is shown an illustration of a storage system for implementing an instantaneous cloning mechanism. The storage system can be for example a storage server or a group e.g. cluster of storage servers. The storage system communicates with client devices to provide storage services to the client devices. The storage system includes a client module a memory and a persistent storage . The client module is configured to manage communication to and from a client device including routing a client I O request. The client device can be for example a computer connected to the storage system . The client device can also be a computer cluster connected to the storage system via a network. For example the client module can be a N Module as described below in . The memory is a volatile memory of the storage system such as a random access memory within a storage server. The memory can be a core or main memory of the storage system . The memory can also be a cache memory of the storage system . For example the memory can be a random access memory RAM .

The persistent storage includes one or more non volatile storage devices. For example such storage devices can be or include disks magnetic tape optical disk flash memory or a solid state drives SSDs .

The memory can include a volume . The volume is a logical data container in the memory that references storage space on persistent storage. Within each of the volume there can be an in memory data object . The in memory data object is a logical data container of reference pointers to data stored elsewhere. The in memory data object can be within the volume . Alternatively the in memory data object can exist within the memory without being inside a volume. Another in memory data object can also be placed within the in memory data object .

Actual data referenced by the in memory data object can be stored within the memory and or on the persistent storage . For example an in memory image of the in memory data object can be stored in the memory . A persistent storage image of the in memory data object can be stored on the persistent storage . The in memory data object can store a logical pointer to portions of the in memory image . The in memory data object can also store a logical pointer to portions of the persistent storage image .

The client module can receive a first client request from the client device where the first client request is a request for a modify write operation. For example the modify write command can be a request to modify a piece of data represented by the in memory data object . When the storage system executes the first client request the in memory image of the in memory data object becomes different from the persistent storage image of the in memory data object . Once modified the in memory data object can be referred to as dirty i.e. where the in memory image and the persistent storage image are not consistent. The storage system normally saves the dirty version of the in memory data object represented by the in memory image to the persistent storage image at a later stage of processing known as a consistency point . Consistency points may occur at predetermined time intervals e.g. every 10 seconds or in response to a specified condition occurring e.g. memory being filled to a certain percentage of its capacity .

The client module can receive a second client request from the client device where the second client request is a request for a clone operation i.e. a request to make a copy of a data object of the storage system such as the in memory data object . The storage system is capable of executing the second client request without waiting for the in memory image of the in memory data object to be saved onto the persistent storage at the later stage of processing i.e. without waiting for the next consistency point.

For example as illustrated the storage system can execute the clone command to create a data object clone of the in memory data object . Creation of the data object clone can be done without waiting for the in memory image as described in detail below. A dirty portion of the in memory image is the difference between the in memory image and the persistent storage image .

The following description associated with describes an extent based architecture which can be used in conjunction with the storage system of to provide instantaneous cloning.

Storage of data in storage units is managed by storage servers which receive and respond to various I O requests from clients directed to data stored in or to be stored in storage units . Data is accessed e.g. in response to the I O requests in units of blocks which in the present embodiment are 4 KB in size although other block sizes e.g. 512 bytes 2 KB 8 KB etc. may also be used. For one embodiment 4 KB as used herein refers to 4 096 bytes. For an alternate embodiment 4 KB refers to 4 000 bytes. Storage units constitute mass storage devices which can include for example flash memory magnetic or optical disks or tape drives illustrated as disks A B . The storage devices can further be organized into arrays not illustrated implementing a Redundant Array of Inexpensive Disks Devices RAID scheme whereby storage servers access storage units using one or more RAID protocols. RAID is a data storage scheme that divides and replicates data among multiple hard disk drives e.g. in stripes of data. Data striping is a technique of segmenting logically sequential data such as a single file so that segments can be assigned to multiple physical devices hard drives. Redundant parity data is stored to allow problems to be detected and possibly fixed. For example if one were to configure a hardware based RAID 5 volume using three 250 GB hard drives two drives for data and one for parity the operating system would be presented with a single 500 GB volume and the exemplary single file may be stored across the two data drives. Although illustrated as separate components for one embodiment a storage server and storage unit may be a part of housed within a single device.

Storage servers can provide file level service such as used in a network attached storage NAS environment block level service such as used in a storage area network SAN environment a service which is capable of providing both file level and block level service or any other service capable of providing other data access services. Although storage servers are each illustrated as single units in a storage server can in other embodiments constitute a separate network element or module an N module and disk element or module a D module . In one embodiment the D module includes storage access components configured to service client requests. In contrast the N module includes functionality that enables client access to storage access components e.g. the D module and may include protocol components such as Common Internet File System CIFS Network File System NFS or an Internet Protocol IP module for facilitating such connectivity. Details of a distributed architecture environment involving D modules and N modules are described further below with respect to and embodiments of a D module and an N module are described further below with respect to .

In yet other embodiments storage servers are referred to as network storage subsystems. A network storage subsystem provides networked storage services for a specific application or purpose. Examples of such applications include database applications web applications Enterprise Resource Planning ERP applications etc. e.g. implemented in a client. Examples of such purposes include file archiving backup mirroring and etc. provided for example on archive backup or secondary storage server connected to a primary storage server. A network storage subsystem can also be implemented with a collection of networked resources provided across multiple storage servers and or storage units.

In the embodiment of one of the storage servers e.g. storage server A functions as a primary provider of data storage services to client . Data storage requests from client are serviced using storage device A organized as one or more storage objects. A secondary storage server e.g. storage server B takes a standby role in a mirror relationship with the primary storage server replicating storage objects from the primary storage server to storage objects organized on storage devices of the secondary storage server e.g. disks B . For example the storage objects can be replicated from the in memory image of to the persistent storage image of . In operation the secondary storage server does not service requests from client until data in the primary storage object becomes inaccessible such as in a disaster with the primary storage server such event considered a failure at the primary storage server. Upon a failure at the primary storage server requests from client intended for the primary storage object are serviced using replicated data i.e. The secondary storage object at the secondary storage server.

It will be appreciated that in other embodiments network storage system may include more than two storage servers. In these cases protection relationships may be operative between various storage servers in system such that one or more primary storage objects from storage server A may be replicated to a storage server other than storage server B not shown in this figure . Secondary storage objects may further implement protection relationships with other storage objects such that the secondary storage objects are replicated e.g. to tertiary storage objects to protect against failures with secondary storage objects. Accordingly the description of a single tier protection relationship between primary and secondary storage objects of storage servers should be taken as illustrative only.

Nodes may be operative as multiple functional components that cooperate to provide a distributed architecture of system . To that end each node may be organized as a network element or module N module A B a disk element or module D module A B and a management element or module M host A B . In one embodiment each module includes a processor and memory for carrying out respective module operations. For example N module may include functionality that enables node to connect to client via network and may include protocol components such as a media access layer Internet Protocol IP layer Transport Control Protocol TCP layer User Datagram Protocol UDP layer and other protocols known in the art. N module can be the client module of .

In contrast D module may connect to one or more storage devices via cluster switching fabric and may be operative to service access requests on devices . In one embodiment the D module implements an extent based storage architecture as will be described in greater detail below. In one embodiment the D module includes storage access components such as a storage abstraction layer supporting multi protocol data access e.g. Common Internet File System protocol the Network File System protocol and the Hypertext Transfer Protocol a storage layer implementing storage protocols e.g. RAID protocol and a driver layer implementing storage device protocols e.g. Small Computer Systems Interface protocol for carrying out operations in support of storage access operations. In the embodiment shown in a storage abstraction layer e.g. file system of the D module divides the physical storage of devices into storage objects. Requests received by node e.g. via N module may thus include storage object identifiers to indicate a storage object on which to carry out the request.

Also operative in node is M host which provides cluster services for node by performing operations in support of a distributed storage system image for instance across system . M host provides cluster services by managing a data structure such as a RDB RDB A RDB B which contains information used by N module to determine which D module owns services each storage object. The various instances of RDB across respective nodes may be updated regularly by M host using conventional protocols operative between each of the M hosts e.g. across network to bring them into synchronization with each other. A client request received by N module may then be routed to the appropriate D module for servicing to provide a distributed storage system image.

It should be noted that while shows an equal number of N modules and D modules constituting a node in the illustrative system there may be different number of N and D modules constituting a node in accordance with various embodiments of instantaneous cloning. For example there may be a number of N modules and D modules of node A that does not reflect a one to one correspondence between the N and D modules of node B. As such the description of a node comprising one N module and one D module for each node should be taken as illustrative only.

The processor is the central processing unit CPU of the storage server and thus controls its overall operation. The processor accomplishes this by executing software stored in memory . For one embodiment multiple processors or one or more processors with multiple cores are included in the storage server . For one embodiment individual adapters e.g. network adapter and storage adapter each include a processor and memory for carrying out respective module operations.

Memory includes storage locations addressable by processor network adapter and storage adapter configured to store processor executable instructions and data structures associated with implementation of an extent based storage architecture such as the extent based storage architecture . Storage operating system portions of which are typically resident in memory and executed by processor functionally organizes the storage server by invoking operations in support of the storage services provided by the storage server . It will be apparent to those skilled in the art that other processing means may be used for executing instructions and other memory means including various computer readable media may be used for storing program instructions pertaining to the inventive techniques described herein. It will also be apparent that some or all of the functionality of the processor and executable software can be implemented by hardware such as integrated currents configured as programmable logic arrays ASICs and the like.

Network adapter comprises one or more ports to couple the storage server to one or more clients over point to point links or a network. Thus network adapter includes the mechanical electrical and signaling circuitry needed to couple the storage server to one or more client over a network. The network adapter may include protocol components such as a Media Access Control MAC layer Common Internet File System CIFS Network File System NFS Internet Protocol IP layer Transport Control Protocol TCP layer User Datagram Protocol UDP layer and other protocols known in the art for facilitating such connectivity. Each client may communicate with the storage server over the network by exchanging discrete frames or packets of data according to pre defined protocols such as TCP IP.

Storage adapter includes a plurality of ports having input output I O interface circuitry to couple the storage devices e.g. disks to bus over an I O interconnect arrangement such as a conventional high performance FC or SAS link topology. Storage adapter typically includes a device controller not illustrated comprising a processor and a memory the device controller configured to control the overall operation of the storage units in accordance with read and write commands received from storage operating system . In one embodiment the storage operating system implements an extent based storage architecture as will be described in greater detail below. As used herein data written by a device controller in response to a write command is referred to as write data whereas data read by device controller responsive to a read command is referred to as read data. 

User console enables an administrator to interface with the storage server to invoke operations and provide inputs to the storage server using a command line interface CLI or a graphical user interface GUI . In one embodiment user console is implemented using a monitor and keyboard.

When implemented as a node of a cluster such as cluster of the storage server further includes a cluster access adapter shown in phantom broken lines having one or more ports to couple the node to other nodes in a cluster. In one embodiment Ethernet is used as the clustering protocol and interconnect media although it will be apparent to one of skill in the art that other types of protocols and interconnects can by utilized within the cluster architecture.

Multi protocol engine includes a media access layer of network drivers e.g. gigabit Ethernet drivers that interface with network protocol layers such as the IP layer and its supporting transport mechanisms the TCP layer and the User Datagram Protocol UDP layer . A file system protocol layer provides multi protocol file access and to that end includes support for one or more of the Direct Access File System DAFS protocol the NFS protocol the CIFS protocol and the Hypertext Transfer Protocol HTTP protocol . A VI layer implements the VI architecture to provide direct access transport DAT capabilities such as RDMA as required by the DAFS protocol . An iSCSI driver layer provides block protocol access over the TCP IP network protocol layers while a FC driver layer receives and transmits block access requests and responses to and from the storage server. In certain cases a Fibre Channel over Ethernet FCoE layer not shown may also be operative in multi protocol engine to receive and transmit requests and responses to and from the storage server. The FC and iSCSI drivers provide respective FC and iSCSI specific access control to the blocks and thus manage exports of logical unit numbers LUNs to either iSCSI or FCP or alternatively to both iSCSI and FCP when accessing data blocks on the storage server.

The storage operating system also includes a series of software layers organized to form a storage server that provides data paths for accessing information stored on storage devices. Information may include data received from a client in addition to data accessed by the storage operating system in support of storage server operations such as program application data or other system data. Preferably client data may be organized as one or more logical storage objects e.g. volumes that comprise a collection of storage devices cooperating to define an overall logical arrangement. In one embodiment the logical arrangement may involve logical volume block number VBN spaces wherein each volume is associated with a unique VBN.

The file system implements a virtualization system of the storage operating system through the interaction with one or more virtualization modules illustrated as a SCSI target module . The SCSI target module is generally disposed between drivers and the file system to provide a translation layer between the data block LUN space and the file system space where LUNs are represented as data blocks. In one embodiment the file system implements a Write Anywhere File Layout WAFL file system having an on disk format representation that is block based using e.g. 4 KB blocks and using a data structure such as index nodes inodes to identify files and file attributes such as creation time access permissions size and block location . File system uses files to store metadata describing the layout of its file system including an inode file which directly or indirectly references points to the underlying data blocks of a file.

For one embodiment the file system includes an extent based architecture as an extension to WAFL. Operationally a request from a client is forwarded as a packet over the network and onto the storage server where it is received at a network adapter. A network driver such as layer or layer processes the packet and if appropriate passes it on to a network protocol and file access layer for additional processing prior to forwarding to file system . There file system generates operations to load retrieve the requested data from the disks if it is not resident in core i.e. in memory . If the information is not in memory file system in cooperation with the extent based architecture accesses an indirect volume to retrieve an extent identifier accesses an extent to physical block map to retrieve a PVBN as described in greater detail with reference to . For one embodiment the file system passes the PVBN to the RAID system . There the PVBN is mapped to a disk identifier and device block number disk DBN and sent to an appropriate driver of disk driver system . The disk driver accesses the DBN from the specified disk and loads the requested data block s in memory for processing by the storage server. Upon completion of the request the node and operating system returns a reply to the client over the network.

It should be noted that the software path through the storage operating system layers described above needed to perform data storage access for the client request received at the storage server adaptable to the teachings of the invention may alternatively be implemented in hardware. That is in an alternate embodiment of the invention a storage access request data path may be implemented as logic circuitry embodied within a field programmable gate array FPGA or an application specific integrated circuit ASIC . This type of hardware implementation increases the performance of the storage service provided by the storage server in response to a request issued by a client. Moreover in another alternate embodiment of the invention the processing elements of adapters may be configured to offload some or all of the packet processing and storage access operations respectively from processor to thereby increase the performance of the storage service provided by the storage server. It is expressly contemplated that the various processes architectures and procedures described herein can be implemented in hardware firmware or software.

When implemented in a cluster data access components of the storage operating system may be embodied as D module configured to access data stored on disk. In contrast multi protocol engine may be embodied as N module to perform protocol termination with respect to a client issuing incoming access over the network as well as to redirect the access requests to any other N module in the cluster. The N module can be the client module of . A cluster services system may further implement an M host e.g. M host to provide cluster services for generating information sharing operations to present a distributed file system image for the cluster. For instance media access layer may send and receive information packets between the various cluster services systems of the nodes to synchronize the replicated databases in each of the nodes.

In addition a cluster fabric CF interface module CF interface modules A B may facilitate intra cluster communication between N module and D module using a CF protocol . For instance D module may expose a CF application programming interface API to which N module or another D module not shown issues calls. To that end CF interface module can be organized as a CF encoder decoder using local procedure calls LPCs and remote procedure calls RPCs to communicate a file system command to between D modules residing on the same node and remote nodes respectively.

Although embodiments of the present invention are shown herein to implement an extent based architecture within the illustrated components and layers of a storage server it will be appreciated that an extent based architecture may be implemented in other modules or components of the storage server in other embodiments. In addition an extent based architecture may be implemented as one or a combination of a software executing processor hardware or firmware within the storage server. As such an extent based architecture may directly or indirectly interface with modules of the storage operating system in accordance with teachings of the present invention.

As used herein the term storage operating system generally refers to the computer executable code operable on a computer to perform a storage function that manages data access and may implement data access semantics of a general purpose operating system. The storage operating system can also be implemented as a microkernel an application program operating over a general purpose operating system such as UNIX or Windows or as a general purpose operating system with configurable functionality which is configured for storage applications as described herein.

In addition it will be understood to those skilled in the art that the invention described herein may apply to any type of special purpose e.g. file server or storage serving appliance or general purpose computer including a standalone computer or portion thereof embodied as or including a storage system. Moreover the teachings of this invention can be adapted to a variety of storage system architectures including but not limited to a network attached storage environment a storage area network and disk assembly directly attached to a client or host computer. The term storage system should therefore be taken broadly to include such arrangements in addition to any subsystems configured to perform a storage function and associated with other equipment or systems. It should be noted that while this description is written in terms of a write anywhere file system the teachings of the present invention may be utilized with any suitable file system including conventional write in place file systems.

The extent based entries of the extent to physical block map provide per aggregate indirection. In contrast virtual volume block numbers VVBN of volume containers provide per volume indirection. A per aggregate extent based entry as used herein refers to an extent being unique across volume boundaries within an aggregate. A per volume indirect entry refers to an entry being unique within volume boundaries. For per aggregate indirection when the storage server copies moves or makes other changes to physical blocks the changes are reflected within the aggregate metadata layer in the extent to physical block map .

These changes however do not need to be propagated into the volume layer because the extent identifier does not need to change. This enables compression decompression sharing and the termination of sharing of extents to occur without communication with the volume layer . Blocks can be easily shared across volume boundaries enabling cross volume deduplication. Segment cleaning and related disk gardening techniques can be performed on the extent to physical block map in a single pass all without having to propagate changes up into the volume layer .

As set forth in further detail above and below embodiments of the extent based architecture include a storage server receiving an I O request including a file block number FBN for an extent. The storage server uses the received FBN as a key to traverse a sorted data structure in the volume layer to an extent identifier. The storage server uses the extent identifier as a key to traverse a second sorted data structure in the aggregate metadata layer to a reference or pointer. The second sorted data structure maps extent identifiers either directly or indirectly to Physical Volume Block Numbers PVBN s. For one embodiment the reference or pointer in the second sorted data structure identifies or refers directly to a PVBN for the extent. Alternatively the reference or pointer identifies another extent identifier that in turn refers directly to a PVBN for the extent. Given that entries in the volume layer are per extent rather than per data block as in previous implementations of WAFL the implementation of an extent based architecture results in a significant decrease in volume layer metadata and in turn performance improvements due to the decrease in metadata involved in common operations. By separating the data stored in the volume layer from the data stored in the aggregate metadata layer the extent based architecture provides the storage server with the ability to write and rearrange data blocks on disk without changing the corresponding extent identifier s . This implies that the sorted data structure in the volume layer does not need to be updated for some manipulation of data blocks on disk. Block extent compression block extent decompression block extent sharing disk gardening etc. can be performed without communicating with the volume layer. Additionally because the sorted data structure in the volume layer is traversed using an FBN which is mapped to an extent identifier the extent based architecture can avoid the misalignment problem of previous implementations of WAFL when the initial offset of data blocks occurs at a different granularity than the data block size e.g. a 512 byte offset for 4 KB blocks .

For one embodiment a PVBN may be referenced directly by only one extent identifier and an extent identifier may not reference a second extent identifier if that second extent identifier references a third extent identifier. This embodiment prevents the level of indirect references in extent map entries from becoming arbitrarily deep and taking an arbitrary amount of time as measured in terms of disk I O operations assuming that each extent entry is likely to be stored within a different disk block and require a separate I O operation .

For one embodiment an extent identifier may include multiple references or pointers. Partial ranges of data blocks within an extent can be overwritten deduplicated compressed etc. Additionally the indirection provided by the extent based architecture allows partial ranges of data blocks within an extent to be overwritten without first reading the extent. In contrast an extent overwrite operation in previous implementations of file systems included reading the data blocks of data from a storage device into a buffer overwriting a portion of the data blocks within the buffer and writing the updated blocks of data back to the storage device.

For one embodiment the extent to PVBN mapping in the aggregate metadata layer is global across volumes. As used herein reference to the extent to PVBN map global across volumes refers to a storage server being able to share or end the sharing of data blocks within extents e.g. via the extent to physical block map across volume boundaries as defined in the volume layer . This is difficult in previous implementations of file systems because file systems typically do not support a layer of indirection that spans a space larger than a single volume.

For one embodiment the lengths of extents vary. For one embodiment the length of an extent is expressed as the number of data blocks of a fixed size within the extent. For example an extent containing only a single 4 KB block would have a length of 1 an extent containing two 4 KB blocks would have a length of 2 etc. For one embodiment extents have a maximum length driven by user I O or write allocation e.g. extents having a maximum length of 64 blocks . For an alternate embodiment the length of extents may be consistently defined e.g. 8 blocks .

For an embodiment utilizing an extent based tree with variable length extents the height of the tree is variable even between two files of the same size. For one embodiment the span of an internal node is also variable. As used herein the span of an indirect block refers to the number of blocks to which that indirect block refers. As a comparison in previous implementations of WAFL the span of an indirect block is fixed the span of a tradvol indirect block is 1024 blocks the span of a flexvol indirect block is 510 blocks e.g. as stored in flexible volume and the span of a 64 bit flexvol indirect block is 255 blocks e.g. as stored in flexible volume .

Additionally in the previous implementations of WAFL a contiguous extent containing N blocks would use the same amount of indirect space as N randomly located blocks because each data block of the extent would be represented by a separate indirect entry in the volume layer. An extent based sorted data structure however greatly reduces the amount of indirect space used because volume layer entries are per extent rather than per block. For example consider a 64 bit flexvol storing a file containing 532 685 800 bytes of data approximately 508 MB as implemented in previous implementations of WAFL. The flexvol includes indirect blocks having 255 entries a span of 255 and each entry refers to a 4 KB block. The flexvol represents the 508 MB file using two level 2 indirect blocks pointing to 510 level 1 indirect blocks pointing to 130050 4 KB level 0 data blocks. In an extent based sorted data structure instead of using one entry for each 4 KB block the storage server uses one entry for each extent. Extents can be longer than a single 4 KB block. For example an extent is a contiguous group of one or more 4 KB blocks. Using an extent based sorted data structure with 16 block long extents and 127 entries per block the storage server represents the 130050 4 KB with only 8129 leaf nodes and 65 internal nodes resulting in an 87 savings in indirect block metadata.

For one embodiment the storage server uses an extent based sorted data structure to implement an indirect volume . For one embodiment the storage server implements each indirect volume as a B tree. shows an exemplary volume layer indirect entry for a leaf node of an extent based data structure used to implement an indirect volume . The volume layer indirect entry stores an FBN a corresponding extent identifier and a length of the extent . The storage server uses the FBN as the primary sorting key to navigate the extent based sorted data structure and find the extent identifier that corresponds to the FBN . For one embodiment the FBN is 48 bits the extent identifier is 48 bits and the length is 8 bits. Alternatively the storage server uses different sizes for one or more of the FBN extent identifier or length . For example the extent identifier may be 64 bits long in an alternate embodiment to e.g. provide for 512 byte granularity in the offsets of blocks. For one embodiment extent lengths vary. For an alternate embodiment extent lengths are fixed.

For one embodiment the FBN is 51 bits to provide for 512 byte granularity in the offsets of blocks where a 48 bit FBN provides for 4 KB byte granularity of FBN offsets . Because the storage server stores indirect blocks using an extent based sorted data structure FBN s do not need to be aligned based upon block size e.g. 512 byte offset alignment and 4 KB blocks . The extent based sorted data structure stores an entry for an entire extent based upon an FBN and length of the extent. The extent based sorted data structure does not store only the block at that FBN and then require subsequent entries to correspond to each subsequent FBN. For example given two adjoining extents that are each 16 blocks in length the entries in the extent based sorted data structure for these two extents will have FBN s that are offset by at least 16 blocks. In traversing the extent based sorted data structure the storage server does not need to assume that each entry is separated by the same offset or that an entry s FBN is offset by a whole number multiple of the block size. Additionally the savings in indirect metadata resulting from using an extent based sorted data structure compensates for the use of three additional bits for each FBN . Providing the 512 byte offset granularity within the volume layer eliminates the previously described complications resulting from misalignment between blocks in FBN space and blocks in aggregate space. Once an FBN is mapped to an extent identifier the extent identifier can be mapped to an extent as described below without concern of misalignment because the aggregate metadata layer maintains a consistent block sized alignment of offsets within the aggregate.

The storage server allocates extent identifiers during write allocation. For one embodiment the storage server allocates extent identifiers from a finite pool. Alternatively extent identifiers are monotonically increasing values that never wrap.

For one embodiment the length of an extent is used for a consistency check as described with reference to below.

The per volume container files of previous implementations of WAFL are not used in an extent based sorted data structure used to implement an indirect volume . Instead of per volume container files the storage server uses an extent to physical block map . As described above the use of the extent to physical block map can result in reduced indirect metadata. The indirect volume blocks however no longer contain cached pointers to PVBN s. Accesses to an extent involves the storage server looking up an extent identifier in the indirect volume and looking up the PVBN e.g. by way of a pointer in the extent to physical block map . The computational overhead of this additional I O look up is offset by some of the features of extent based architecture . For example I O accesses are per extent rather than per block and therefore multiple blocks are accessed by a single I O access of each the indirect volume and the extent to physical block map . Additionally the extent based architecture gains advantages in compression deduplication segment cleaning etc. which can be performed with altering the extent identifier . Actions such as deduplication can easily span the aggregate rather than just a single volume and many changes to blocks e.g. resulting from compression and segment cleaning do not need to be propagated up to the indirect volume e.g. to correct cached indirect pointers as in previous implementations of WAFL .

For one embodiment the storage server uses an extent based sorted data structure to implement an extent to physical block map . For one embodiment the storage server implements an extent to physical block map as a B tree. shows an exemplary extent map entry for a leaf node of an extent based sorted data structure used to implement an extent to physical block map . Leaf nodes of an extent based sorted data structure used to implement an extent to physical block map store extent identifiers references such as a pointers to PVBN s or other extent identifiers offsets for the extents and lengths for the extents . As used herein an offset for an extent is a distance in blocks from the first block of the contiguous group of blocks that make up an extent. For one embodiment the extent identifier is 48 bits the pointer extent identifier is 48 bits the offset is 8 bits and the length is 8 bits. For an alternate embodiment different numbers of bits are used for each portion of an extent map entry .

For one embodiment each extent map entry includes either a pointer or other reference directly to a PVBN or to another extent identifier that directly references a PVBN. For one embodiment each PVBN is owned by only one extent and any other extent that references the PVBN does so by way of referencing the owner extent. As a result the maximum additional look up for a given extent to get to a PVBN should be no more than one. This maximum prevents the level of indirect references in extent map entries from becoming arbitrarily deep and taking an arbitrary amount of time as measured in terms of disk I O operations assuming that each extent entry is likely to be stored within a different disk block . As a result of extents having a single owner the storage server can use the owner extent identifier as a tag unique number or other context for the purpose of lost write detection.

For an alternate embodiment all extent identifiers map directly to a PVBN and PVBN s can be owned by more than one extent. For an embodiment including lost write detection the storage server creates a context tag or unique number e.g. via a separate table that is separate different from the extent identifiers due to the possibility of multiple extent identifiers referencing a single PVBN.

For one embodiment the storage server checks data consistency by comparing the length of an extent as stored in the volume layer with the length of the extent as stored in the aggregate metadata layer .

For one embodiment the storage server utilizes a finite number of extent identifiers. If an extent identifier is a candidate to be reused e.g. upon a request to delete the extent the storage server first determines whether or not other extents refer to that extent identifier. If one or more extents reference the candidate extent identifier the storage server ensures that the one or more extents continue to point to the same data e.g. by altering one of the extents to directly reference the corresponding PVBN and the other extents to reference that altered extent . For one embodiment the storage server maintains e.g. in one or more metafiles reference counts for references by extents to each extent and by extents to each PVBN. Reference counts enable the storage server to be aware of whether or not other extents would be affected by operations performed on an extent PVBN e.g. reallocation segment cleaning etc. . The storage server tracks increments and decrements of the reference count in one or more log files. For example the storage server would increment a reference count when a new extent PVBN is allocated when an extent identifier is shared e.g. via clone creation snapshot creation or deduplication etc. For one embodiment the storage server accumulates increments and decrements using a log file and makes batch updates to reference count metafiles e.g. at a consistency point. For one embodiment the storage server increments a reference count from 0 to 1 for a PVBN directly bypassing the log file when allocating a new extent PVBN and executes all other increments and decrements of the reference counts via the respective reference count log file.

A hierarchical reference counting the volume layer indirect entry can be tracked. Every use of a reference pointer such as the Extent ID and the extent ID is tracked as a single reference count by a reference count metafile which is described later in . For example if three objects reference the same set of data blocks using the extent ID then the reference count for the extent ID is 3. An entire file system tree of objects can be tracked by a hierarchical reference count h refcnt of an extent ID of the root of that tree. In other words a h refcnt on a root node of a data object is the same as a reference count increment of each reference pointer pointed to by walking down the tree rooted at the data object.

The data objects in the extent based data structure are essentially a tree of reference pointers. In a hierarchically reference counted file system based on the extent based data structure any data object can be instantaneously cloned by creating a new h refcnt on the root of the tree of reference pointers that represent the data object such as the extent based data structure . Any data object that has been written out to persistent storage already has a reference pointer assigned to it. Therefore for example creating an instantaneous clone of the extent based data structure is accomplished by taking an extra h refcnt on the reference pointer of the root node of the extent based data structure . Creating a clone of a data object that has not yet been written to persistent storage is done by allocation a reference pointer on the fly.

At processing block if there is a non zero offset the storage server creates a new extent map entry including a newly allocated extent identifier a reference to the existing extent identifier an offset equal to zero and a length of the existing data blocks that are not being overwritten e.g. the value of the offset between the FBN provided with the write request and the FBN for the existing extent . Referring once again to the example in the storage server allocates extent identifier 2 a reference to the entry for extent identifier 1 EID 1 points to P1 an initial offset of zero and a length of four for PVBN s which are not being overwritten.

At processing block if the offset of the FBN provided with the write request from the FBN for the overlapping extent is zero or after creating a new extent map entry for an initial set of blocks not being overwritten the storage server creates a new extent map entry including the newly allocated extent identifier a reference to the stored data blocks provided with the overwrite request an offset from the reference where the newly stored data blocks begin and the length in blocks of the new data. Referring once again to the example in the storage server allocates extent identifier 2 a reference P2 to newly stored PVBN an offset of 0 and a length of 1.

At decision block the storage server determines if the overwrite process has reached the end of the existing extent. For one embodiment the storage server determines if the sum of the offset from the start of the existing extent for the new data blocks and the length of the new data blocks is greater or equal to length of the existing extent to determine if the end of the existing extent has been reached after completing the overwrite portion of the process . Referring once again to the example in the new data block has an offset of 4 and length of 1. Given that the sum of the offset and length five is less than the length of the existing extent eight the remainder of the existing extent would still need to be addressed.

At processing block if the overwrite has not reached the end of the existing extent the storage server creates a new extent map entry including the newly allocated extent identifier a reference to the existing extent identifier an offset equal to the first block of the remainder of existing blocks that are not being overwritten the offset from the beginning of the existing extent to the first block to be overwritten the length of the new data and a length of the remainder of the existing data blocks that are not being overwritten. Referring once again to the example in three blocks PVBN s and are also not being overwritten. The storage server allocates extent identifier 2 a reference to the entry for extent identifier 1 EID 1 points to P1 an offset of five blocks to refer to PVBN from PVBN and a length of three blocks.

At processing block the storage server uses the allocated extent identifier as a key to traverse the aggregate metadata layer extent based sorted data structure and adds the one or more new extent map entries . At processing block the storage server overwrites the existing extent identifier with the allocated extent identifier in the existing entry in the volume layer extent based sorted data structure associated with the FBN for the overlapping extent.

Referring once again to the example in the data block at PVBN has been effectively overwritten with the data block PVBN as illustrated by PVBN being outlined in a dashed line and shaded. The overwrite utilizes block sharing to only write one new data block and use references to access the data blocks that are not overwritten by way of the existing extent. Additionally the data blocks of the existing extent do not need to be read prior to performing an overwrite operation. For one embodiment if an extent map entry identifies or otherwise refers to the existing extent map entry that includes reference to the data block at PVBN e.g. by reference to extent identifier 1 that extent remains unchanged by the overwrite because the existing extent still refers to PVBN with an offset of zero and a length of eight.

For an alternate embodiment when the storage server detects an overwrite request the storage server reads the data blocks that are not being overwritten and rewrites them with the new data blocks as a new extent that can be accessed directly by an extent map entry without relying on block sharing with the existing extent e.g. without referencing the extent identifier of the existing extent . For one embodiment the storage server includes a setting to alternate between the block sharing overwrite described above with reference to and the alternate non sharing overwrite as a trade off between space efficiency block sharing and speed efficiency in subsequent read requests directed at the newly overwritten extent non sharing .

Thus embodiments of an extent based architecture are implemented in a computer system as described herein. In practice the methods and may constitute one or more programs made up of computer executable instructions. The computer executable instructions may be written in a computer programming language e.g. software or may be embodied in firmware logic or in hardware circuitry. The computer executable instructions to implement a persistent cache may be stored on a machine readable storage medium. A computer readable storage medium or a non transitory computer readable storage medium as the terms are used herein include any mechanism that provides i.e. stores and or transmits information in a form accessible by a machine e.g. a computer network device personal digital assistant PDA manufacturing tool any device with a set of one or more processors etc. . A non transitory computer readable storage medium as the term is used herein does not include a signal carrier wave etc. The term RAM as used herein is intended to encompass all volatile storage media such as dynamic random access memory DRAM and static RAM SRAM . Computer executable instructions can be stored on non volatile storage devices such as magnetic hard disk an optical disk and are typically written by a direct memory access process into RAM memory during execution of software by a processor. One of skill in the art will immediately recognize that the terms machine readable storage medium and computer readable storage medium include any type of volatile or non volatile storage device that is accessible by a processor. For example a machine readable storage medium includes recordable non recordable media e.g. read only memory ROM random access memory RAM magnetic disk storage media optical storage media flash memory devices etc. .

The storage system includes a memory and a persistent storage . The memory is a volatile memory such as a RAM or a processor cache register or a combination thereof. The memory can be the memory of . The persistent storage is a collection of one or more non volatile storage devices such as magnetic disks magnetic tapes optical disks a solid state drives SSDs or any combination thereof. The persistent storage can be the persistent storage of .

The memory includes a volume layer and an aggregate metadata layer . The volume layer is a portion of the memory where data objects and other data containers are stored such as indirect flexible volumes. The volume layer is maintained by the operating system of the storage system such as the operating system of . The volume layer can be the volume layer of . The volume layer can include the data object . The data object is a logical data container. The data object can be the in memory data object of .

In some embodiments the data object can be represented by a tree structure . The tree structure has a single root node. The tree structure can be the extent based sorted data structure of . The single root node of the tree structure can be a child node of another reference pointer tree representing a larger data container such as the volume of .

The tree structure has one or more nodes. Each of the nodes is a data container. The leaf nodes can include a reference pointer such as a reference pointer . The reference pointer can be translated to one or more physical blocks on the persistent storage through metadata stored in the aggregate metadata layer further described below. In some embodiments the leaf nodes of the tree structure can be the volume layer indirect entry of . The reference pointer can include an identifier serving as an index to the physical block mapping. The identifier can be the extent ID of or the extent ID of .

The aggregate metadata layer is a portion of the memory configured to store metadata of the storage system . The aggregate metadata layer is responsible for managing the metadata of a storage aggregate. The storage aggregate consists of a collection of physical storage devices. The storage aggregate can host several discrete filesystem volumes therein including the volume of . The aggregate metadata layer is also maintained by the operating system of the storage system . The aggregate metadata layer can be the aggregate metadata layer of . The aggregate metadata layer includes a pointer map . In one embodiment the pointer map can be the extent to physical block map of .

The pointer map is a translation data structure such as a table a dictionary or a tree structure that maps each of the reference pointers in the tree structure such as the reference pointer to one or more physical blocks on the persistent storage . The mapping can be direct or indirect. Instances of the reference pointer can map to other reference pointers. For example reference pointer E1 can map to reference pointer E2 which can map to reference pointers E3 and E4 . In at least one embodiment each reference pointer such as the reference pointer is unique in the pointer map . However each reference pointer can be used in various data objects and logical data containers in the volume layer . The one or more physical blocks can be represented by one or more physical block numbers such as the PVBN the PVBN the PVBN and etc. of . The physical block numbers can then be mapped to a data physical location on the persistent storage . For example this mapping can be described by a physical volume data structure not shown such as the physical volume of or of .

The pointer map can track a reference count of each reference pointer such as a reference count of the reference pointer . The reference count is a count of a number of times the reference pointer is referenced by all logical data containers in the volume layer which thereby indicates the number of logical data containers that include the corresponding block extent. Alternatively the reference count of the reference pointer can be stored on a separate metadata structure in the aggregate metadata layer such as a pointer reference count map .

Referring now to therein is shown a control flow of a storage system . The storage system can be the storage system of . The storage system be a storage server such as the storage server A of or a storage cluster such as the clustered network storage system of . illustrates how the storage system handles an incoming write modify request and how the storage system handles an incoming clone request after the incoming write modify request is received.

The storage system includes a client module . The client module can be the client module of . Each client request received at the client module can be recorded on a request log . For example the request log can store the I O requests as well as other data manipulation and data access requests. In this way even when the storage system experiences a memory component failure or a power failure the request log can serve to restore the state of the storage system prior to the failure.

The client module can receive a modification request . The modification request is a client request to modify a data object such as a data object . The data object is a logical data container such as the in memory data object of or the data object of . When the client module receives the modification request the client module can pass the modification request to a write module .

The write module is configured to write data into data objects of the storage system . When the write module receives the modification request in response the write module executes a free module to free an assigned reference pointer such as the reference pointer of . The assigned reference pointer can be translated to a location of where the data prior to modification is stored. Also in response to the write module receiving the modification request the write module executes an allocation module to allocate a replacement reference pointer for a modified portion of the data object according to the modification request . The write module the free module and the allocation module can be part of a front end operator for the operating system of the storage system . A front end operation in this disclosure is defined as a process that manipulates with data in volatile memory without saving it on persistent storage. A front end operator is a module of the storage system that initiates a front end operation.

The allocation of the replacement reference pointer and the freeing of the assigned reference pointer can be done on the fly. On the fly in this context means in response to the modification request without waiting for another process to finish. An on the fly process therefore is a process executed in response to the modification request without having another process wait for the on the fly process to finish. For example the allocation of the replacement reference pointer and the freeing of the assigned reference pointer can be done without quiescing new incoming client requests to the data object at the write module or at the client module .

For another example the allocation of the replacement reference pointer and the freeing of the assigned reference pointer can be done without waiting for a consistency point module of the storage system to create an image of the data object as modified on a persistent storage . The consistency point module is configured to run an asynchronous process to allocate physical volume block numbers for the storage system . Here asynchronous means that execution of one process is not dependent on the timing of any other process. The operation of the consistency point module is further described below.

The storage system includes an operating system memory . The operating system memory is a working memory of the storage system . For example the operating system memory can be an on chip cache a system cache a RAM a SSD a flash memory or any combination thereof. The operating system memory can be the memory of or the memory of . The operating system memory includes a pointer bucket space a pointer stage space a volume layer an aggregate metadata layer and a memory store . The volume layer is a portion of the operating system memory configured to store logical data containers for the storage system . The volume layer can be the volume layer of . The volume layer can include a reference tree structure representing the data object . The reference tree structure can be the tree structure of .

The memory store is a portion of the operating system memory configured to store data contents of data objects such as the data object . For example the memory store can store an in memory version of the data object such as a dirty in memory image including a changed portion illustrated by the shaded portion in . The aggregate metadata layer is a portion of the operating system memory configured to store metadata of reference pointers and or logical data containers. For example the aggregate metadata layer can include a pointer map for the reference pointers used in the volume layer . The pointer map can be the pointer map of .

The mechanics of on the fly allocation and free processing of the reference pointers can benefit from in memory per processor data structures in a multi processing filesystem where multiple front end operations could be concurrently updating different sections of the same filesystem tree on different processors. Per processor herein can mean per individual processor in a multi processor computing system or per core in a multi core computing system.

The pointer bucket space is a global cache to store per processor structures for reference pointer allocation. For example the pointer bucket space can include a buffer bucket . The buffer bucket represents a set of free reference pointers that are sequentially in the reference pointer number space. The buffer bucket can be created by reading reference count information from the pointer reference count map of . The buffer bucket is generated asynchronously. Asynchronously in this sense means that the process of generating the buffer bucket is not dependent on any other process such as any client request or command. An asynchronous process can be processed by a separate and independent processor in a multi processor system or an independent core in a multi core system.

The allocation module can execute a front end operation running on any CPU to dequeue the buffer bucket from the pointer bucket space and use the available free reference pointers in a multiprocessing safe MP safe fashion. The allocation module can allocate the free reference pointers to any dirtied extents of data. The buffer bucket is returned to the pointer bucket space after use. When the buffer bucket has been fully used the buffer bucket is sent to be applied to a reference count map such as the pointer reference count map of . Once applied the buffer bucket is filled again to be used for allocation.

In some embodiments the buffer bucket tracks free allocated reference pointers in a contiguous set of reference pointers because write allocation can work on ranges of freed reference pointers i.e. Reference pointers with no assigned PVBNs corresponding to data locations on the persistent storage. The buffer bucket can also allow the use bitmasks and can be make memory consumption of the reference pointers more compact.

The pointer stage space is a cache to store a set of per processor structures for buffering freed reference pointers until the freed reference pointers are committed to the reference count metadata map such as the pointer reference count map of . The pointer stage space includes a multi processor stage such as a buffer stage . The buffer stage is a data structure where freed reference pointers can be staged by front end operations such as by the free module . The pointer stage space is maintained as a per processor cache or as a global cache. When maintained as the global cache a lock is used for multi processor processing safety MP safe by restricting access for one processor at a time.

The write module or the free module can allocate as many instances of the buffer stage as needed. The free module can free reference pointers that are dirty into the buffer stage and place the buffer stage back to the pointer stage space . This freeing of reference pointers happens in a MP safe fashion. When the buffer stage is full or an event that waits for all outstanding stages occurs the buffer stage is sent to be asynchronously committed to the reference count metadata map. Reference counts of the freed reference pointers in the buffer stage are decremented when committed to the reference count metadata map. After the buffer stage is emptied when committed to the reference count metadata file the buffer stage is ready to be used again. Unlike the buffer bucket the buffer stage is used by random reference pointer frees and so the buffer stage does not necessarily contain co located reference pointers.

The write module that changes the content of the data object generates an in memory dirty version of the data object such as the dirty in memory image . In some embodiments as a result of the modification request a dirty version of every reference pointer up the ancestry of the reference tree structure of the data object is recursively generated including other logical containers containing the data object . Because the storage system recursively dirties all the nodes in the reference tree structure on every front end modify operation the storage system also allows some of the allocation free processing to be done on the fly by the allocation module and the free module instead of the consistency point module doing the allocation. In some embodiments the free module not only can update the reference count metadata file the free module can also update a physical volume on the fly to free the PVBNs associated with the reference pointer freed by the free module . In some other embodiments some higher parts of the reference tree structure are dirtied asynchronously as part of the consistency point module .

The allocation and free of the reference pointers result in updates to reference count metadata in the aggregate metadata layer that track the reference counts for each of the reference pointers. For example the allocation and freeing of the reference pointer may result in updates to the pointer reference count map of . These updates generate dirty entries to the reference count metadata which also need to be processed by the consistency point module . This is illustrated by . A freeing of an assigned reference pointer to E4 and an allocation of a new reference pointer results in the node E4 being marked as dirty. Other nodes up the reference tree structure are also recursively dirtied. Reference pointers up that points to the dirtied reference point can also be recursively dirtied. For example when E4 is dirtied and re allocated to E4 E1 and E2 are also updated. The updates may result in updates to the pointer map where the dirtied reference pointers point to an in memory object instead of referencing a PVBN.

In one embodiment the pointer reference count map itself does not use reference pointers in its tree. The metafile of the pointer reference count map is outside the volume layer . Therefore the processing of these dirty entries in the pointer reference count map does not require any on the fly processing and can be handled by the consistency point module .

The client module can also receive a clone request . The clone request is a client request to perform a clone operation on a data object such as the data object . The clone operation can be performed by a clone module . The clone module is configured to perform a front end operation to clone data objects. Since every dirty node of the reference tree structure in memory gets a reference pointer on the fly the process of creating a clone of any data object represented by a tree structure of reference pointers such as the reference tree structure reduces to taking a hierarchical reference count on the reference pointer pointing to the root of that tree.

The storage system supports hierarchical reference counting of nodes in a filesystem tree such as the reference tree structure . All data objects in the storage system point to physical blocks using the reference pointers such as ExtentId EId as described in . This means any indirect pointer in a node of the filesystem tree uses a reference pointer to point to its child node. The pointer map captures the mapping between a reference pointer and the actual PVBNs i.e. the physical volume block numbers that address blocks on persistent storage .

Every use of a reference pointer such as an EId in any data object in the storage system is tracked as a single reference count by a reference count metafile such as the pointer reference count map . For example if three objects reference the same set of data blocks using E1 then the reference count for E1 is 3. An entire file system tree of objects can be tracked by a hierarchical reference count h refcnt on that reference pointer of the root of that tree. In other words a h refcnt on a root node of a data object is the same as a reference count increment of each reference pointer pointed to by walking down the tree rooted at the data object.

All data objects in the storage system are essentially a tree of reference pointers. In a hierarchically reference counted file system like the storage system any data object can be instantaneously cloned by creating a new h refcnt on the root of the tree of reference pointers that represent the data object such as the reference tree structure of the data object . Any data object that has been written out to persistent storage already has a reference pointer assigned to it. Therefore for example creating an instantaneous clone of the data object is accomplished by taking an extra h refcnt on the reference pointer of the root node of the reference tree structure of the data object . Creating a clone of a data object that has not yet been written to persistent storage is done by allocation a reference pointer on the fly.

There are two cases of this a a dirty in memory version of an object that has been modified by a client operation such that it is different from its original copy on persistent storage or b a brand new object that has just been created in memory as a result of client operations and is therefore also dirty . For example in case a the aggregate metadata layer can be updated following the method of . For example in case b the aggregate metadata layer can be updated following the method of as further illustrated by .

For the purpose of this disclosure both cases reduce to the same problem. To ensure that a reference pointer tree of a dirtied data object has an updated reference pointer in every node every modify operation or write operation performed at the write module free the outdated assigned reference pointer on the fly and allocate a new reference pointer on the fly with the methods described above. Thus creating a clone of a data object with a dirty in memory version such as the data object can be done also by hierarchically reference counting the reference pointer tree of the data object.

Alternatively in some embodiments the volume layer and the aggregate metadata layer are not separated. For example metadata of the storage system can be stored within the volume layer . In those embodiments reference counts of the reference pointers can be put inside the volume layer . The front end operations can free or allocate reference pointers on the fly in the same manner as when the aggregate metadata layer is separate. These tracking logging mechanisms described above for the aggregate metadata layer can be built inside each flexible volume inside the volume layer such as the volume of .

The storage system includes the consistency point module is configured to run an asynchronous process to allocate physical volume block numbers for the storage system . The allocation module and the free module can allocate and free the reference pointers used by the storage system in the volume layer . In some embodiments the free module can also free the PVBNs associated with the reference pointer being freed. The consistency point module can allocate a new PVBN for each reference pointer marked as dirty by the write module . The allocation of the PVBNs is done asynchronously because it is more efficient to work on a collected number of writes on a file than each individual write. The consistency point module can lay the physical volume blocks out on persistent storage in clever ways to optimize for future IO patterns. In one embodiment the consistency point module can handle both the allocation and freeing of the PVBNs. In other embodiments the freeing of the PVBNs are handled by the free module simultaneously as freeing of the reference pointers in volume layer

Once the consistency point module allocates the physical volume block numbers the consistency point module can also update a persistent storage image of the data object in the persistent storage . For example the consistency point module can update the persistent storage image so that it is consistent with the dirty in memory image including copying the changed portion over to the persistent storage .

Referring now to therein is shown a flow chart of an embodiment of a method of implementing a mechanism to instantaneous clone data objects on a storage system such as the storage system of . The method includes initially receiving a client request for a write operation at a storage system to modify a first dirty portion of an in memory version of a data object represented by a first reference pointer tree at step . The first dirty portion can be a block an extent or a set of blocks of the in memory version of the data object. Step for example can be executed by the client module of . The method continues by marking a node in the first reference pointer tree as dirty wherein the dirty node represents the first dirty portion at step . Step for example can be executed by the write module of .

Next at step an assigned pointer of the dirty node is freed on the fly in response to receiving the client request. In some embodiments freeing the assigned pointer causes freeing of a physical volume block space corresponding to the assigned pointer. The freeing of the physical volume block space on a persistent storage can also be done on the fly in response to receiving the client request. Here on the fly here means that the freeing is in response to the client request for the write operation but without waiting for another operation on the data object to finish. Step for example can be executed by the free module of .

In at least one embodiment freeing the assigned pointer can be done by placing the assigned pointer in a per processor cache stage. When the per processor cache stage is full the per processor cache stage is asynchronously committed to a pointer reference count metadata file in memory. Alternatively in another embodiment freeing the assigned pointer can be done by placing the assigned pointer in a global cache stage where the global cache stage is accessed with a lock for multiprocessor processing safety.

Also after step at step a replacement pointer for the dirty node is allocated on the fly in response to receiving the client request. Here on the fly here means that the allocation is in response to the client request for the write operation but the execution of the allocation is without waiting for another operation on the data object to finish. In some embodiments the replacement pointer and the assigned pointer are reference identifiers that are unique across a metadata map in a storage aggregate metadata layer. However in a volume layer of the storage system multiple references to a same reference identifier can exist in different data objects. The reference identifiers can translate to one or more physical block locations by the metadata map. The physical block locations correspond to one or more persistent storage devices. Step for example can be executed by the allocation module of .

Allocation of the replacement pointer can be executed by a first processor of a multiprocessor system and freeing of the assigned pointer can be executed by a second processor of the multiprocessor system. Allocating the replacement pointer includes storing an extent map entry of the replacement pointer mapped to the physical volume block in a metadata extent map on an aggregation layer of the storage system. In some embodiments allocating the replacement pointer includes allocating the replacement pointer from a per processor cache bucket the per processor cache bucket including freed virtual volume pointers spatially co located in a reference pointer space of the storage system. When the cache bucket is fully used the per processor cache bucket is re allocated by determining co located reference pointers from a pointer reference count metadata file such as the pointer reference count map of in memory.

After step the method continues with creating a first snapshot of the data object where the first snapshot is represented by a second reference pointer tree at step . After step the second reference pointer tree would have the replacement pointer as a node. In some embodiment creating the first snapshot includes determining a hierarchical reference count of a root node of the first reference pointer tree wherein the hierarchical reference count is determined by incrementing a reference count for each node of the first reference pointer tree. In at least one embodiment a second snapshot of the data object can be simultaneously created with the first snapshot. Step for example can be executed by the clone module of .

Next the method continues with allocating a physical volume block corresponding to the replacement pointer after the first snapshot is created at step . Allocating the physical volume block includes storing a mapping of the replacement pointer to the physical volume block in a metadata map on an aggregate metadata layer of the storage system. Allocation of the physical volume block is asynchronous to the creating of the first snapshot. Here asynchronous refers to that execution of one process is not dependent on the timing of any other process. Hence execution of the allocation of the physical volume block is not in response to the first snapshot being created. Step for example can be executed by the consistency point module of .

Referring now to therein is shown a flow chart of an embodiment of a method of operating a storage system such as the storage system of . Initially the method includes storing a writeable data object in non persistent memory of a storage system at step . Step for example can be executed by the write module of .

Next the method continues by cloning the writeable data object in non the non persistent memory at step . Step for example can be executed by the clone module of . In one embodiment cloning the writeable data object can be done without having to wait until a first operation associated with the writeable data object and performed outside of the non persistent memory is executed. For example cloning the writeable data object in memory is performed without having to wait until the writeable data object is saved to persistent storage. In at least one other embodiment cloning the writeable data object in memory is performed without pausing a second operation on the storage system until the cloning is completed. For example cloning the writeable data object in memory is performed without pausing incoming operations on the writeable data object until the cloning is completed.

After step the method then saves the writeable data object to the persistent storage at step . Step for example can be executed by the consistency point module of .

The step includes receiving a reference pointer such as the extent identifier of and or the reference pointer of to dirty. The reference pointer representing the data object can be a target node of a volume layer tree structure such as the reference tree structure of or an aggregate metadata layer pointer map such as the pointer map of . For example the receiving of the reference pointer to dirty can be in response to receiving a first front end operation to modify the data object. The first front end operation can specify a file block number that corresponds to the target node in the tree structure. The target node corresponds to the extent identifier.

The method continues with a step of determining whether the reference pointer is already dirtied. When the reference pointer is not already dirtied the method continues to a step of marking the reference pointer as dirty. When the reference pointer is already dirtied the method continues to a step of determining whether an ancestor reference pointer has a hierarchical reference count such as the hierarchical reference count of greater than 1. When at least one ancestor reference pointer of the reference pointer i.e. a reference point that directly or indirectly references the reference pointer is determined to have a hierarchical reference count greater than one the method continues to the step . Otherwise when none of the ancestor reference pointers has a hierarchical reference count greater than one the method terminates. The step ensures that the reference pointer is re dirtied when a clone of the data object represented by the reference pointer has been made and that both modified versions of the clone and the original have to be stored onto persistent storage by the consistency point module.

Dirtying the reference pointer includes marking the reference pointer as dirty in an in memory metadata structure such as in the volume layer or the aggregate layer of . The dirty marking allows for the consistency point module of on a subsequent consistency point from the time the first front end operation is received to process the data object represented by the reference pointer for storage on a physical storage such as the persistent storage of .

Following the step the method continues to a step of sending a parent reference pointer of the reference pointer to be dirtied when the reference pointer is not a root node of the reference tree. The step represents the recursive mechanism of the method to recursively traverse up the reference tree to dirty the reference tree nodes until a root node is reached or until a reference node that has already been dirtied is reached.

In it is illustrated that a dirtied node of the tree structure has a dirty tag associated with it. The dirty tag signals to a consistency point module such as the consistency point module of that a front end operation has modified an extent map entry such as the extent map entry of and the data referenced by the dirtied node may need to be saved to persistent storage. The dirtied node EID can have other child nodes represented by reference pointers not illustrated by .

The execution of the second front end operation results in generation of a second dirtied tree structure . The chain of ancestry of the third reference pointer are all dirtied in accordance with the recursive dirtying method described above. After the second front end operation is executed the hierarchical reference count map is updated once again.

Although the present invention has been described with reference to specific exemplary embodiments it will be recognized that the invention is not limited to the embodiments described but can be practiced with modification and alteration within the spirit and scope of the appended claims. Accordingly the specification and drawings are to be regarded in an illustrative sense rather than a restrictive sense.

Therefore it is manifestly intended that embodiments of this invention be limited only by the following claims and equivalents thereof.

