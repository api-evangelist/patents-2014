---

title: Systems and methods for remote graphics processing unit service
abstract: Aspects of the present disclosure involve systems and methods for providing remote graphics processing unit (GPU) availability to one or more computing components of a data center. In particular, the present disclosure provides the remote location of one or more GPUs within a computing environment for use by one or more computing devices within the computing environment. Thus, each computing device may utilize the remotely located GPUs to perform the tasks of the computing device associated with a GPU, without the need for the GPU to be located within the computing device itself or within the same rack of the computing device. In this manner, one or more GPUs of a computing environment may provide GPU services to any number of computing devices, even though the GPUs are remote from the computing devices.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09576332&OS=09576332&RS=09576332
owner: VCE IP Holding Company LLC
number: 09576332
owner_city: Richardson
owner_country: US
publication_date: 20140602
---
Aspects of the present disclosure relate to configuration of various computing components among other functions and more particularly to providing remote graphics processing unit GPU availability to one or more computing components of a computing environment.

Graphic Processing Units GPUs are becoming increasingly important to many server applications particularly for performing Virtual Desktop Infrastructure VDI and big data analysis implementations. However providing and consuming GPUs in a computing environment creates a unique set of issues particularly in a highly agile virtualized environment. GPUs are typically not available for blade server type arrangements of computing environments generally requiring organizations to implement non standard rack mount servers alongside their blade servers to provide the GPU services for the computing environment. Further GPUs are expensive investments so an organization will often deploy GPUs to a selective subset of their servers such that workloads requiring GPU support have to be specifically scheduled on those rack mount servers that are provisioned with GPUs. However this reduces flexibility of workload deployment and once a virtualized workload has started running on a particular server it is pinned to the server and cannot be migrated to an alternate server for performance or power management reasons. Furthermore even though a typical server may be capable of hosting a hundred or more VDI sessions a GPU dependency for a VDI workload will often reduce this capability to around 30 sessions resulting in a substantial reduction in the efficiencies of the VDI capability.

Implementations described and claimed herein address the foregoing problems among others by providing systems and methods for a computing system. The computing system includes at least a first networking device connected to a network and configured to host a virtual machine the first networking device comprising an intercept driver program configured to intercept a graphics processing unit GPU call from the virtual machine and a second networking device connected to the network the second networking device comprising at least one GPU and an intercept driver target program the intercept driver target program configured to receive a communication from the intercept driver program on the network. The communication from the intercept driver comprises a representation of the GPU call from the virtual machine and wherein the second networking device executes the GPU call on the at least one GPU to provide GPU services to the virtual machine over the network.

Aspects of the present disclosure involve systems and methods for providing remote graphics processing unit GPU availability to one or more computing components of a computing system. In particular the present disclosure provides the remote location of one or more GPUs which may be within a computing environment for use by one or more computing devices within the computing system. In one particular embodiment the computing system may be deployed in a computing system. Thus each computing device such as a blade server can utilize the remotely located GPUs to perform the tasks of the computing device associated with a GPU without the need for the GPU to be located within the computing device itself or within the same rack of the computing device. Further the computing device may be unaware of the remote location of the utilized GPU such that the computing device is not required to alter the calls to the utilized GPU. In this manner one or more GPUs of a computing environment may provide GPU services to any number of computing devices even though the GPUs are remote from the computing devices.

By providing a system and method for remote GPU services several advantages may be obtained. For example remote location of the GPUs allows for mounting of the GPUs in a purpose built enclosure using a network fabric rather than a PCI bus enabling the use of blade type compute systems for the main applications. In addition remote location of the GPUs allows a GPU cage or rack to be located where there is adequate power and cooling for the GPUs. Also because each GPU is accessible by each computing device and is not necessarily tied to a particular device the remote GPU service system provides a wait feature of a GPU enabling the originating computing device to be placed in a paused state releasing the infrastructure resources while the GPU is processing data and re connecting the device to the selected GPU when the device is un paused. Finally for Virtual Desktop Infrastructure VDI applications connecting the remote clients directly to the GPU avoids hair pinning of the resulting display through the computing device hosting the VDI session thereby reducing update latency.

The various systems and methods disclosed herein provide for remote GPU services in a computing environment context. However it will be appreciated that although some of the example implementations described herein involve a data center the presently disclosed technology may be utilized in any computing or networking system or environment where at least one computing device utilizes at least GPU for processing. For example aspects of the present disclosure may be integrated within a converged infrastructure. Generally speaking converged infrastructures also referred to as integrated infrastructure and other terms involve multiple computing components pre integrated into an optimized computing solution. The computing components of a converged infrastructure may include computer storage networking components and software for managing the integrated components. While some examples disclosed herein reference converged infrastructures also sometimes referred to as unified computing systems fabric based computing systems and dynamic infrastructures systems and method described herein may be applied to other computing environments.

For a detailed description of an example system for providing remote GPU services to one or more computing devices reference is made to . As depicted in the system may be implemented in a computing environment or other computing or networking environment. Further the system includes at least one computing device associated with a server of the computing environment such as a blade type server. In one example the computing devices are virtual machines running on the server and utilizing the server to perform one or more computations of the virtual machine. In other examples the computing devices may be laptops personal digital assistants or any other computing device that may utilize the performance of a GPU for computations. In one embodiment these devices may connect directly to the rack mount server as explained in more detail below with relation to . In another embodiment the computing devices may be a converged infrastructure system as described above.

To facilitate computations each of the computing devices may utilize a GPU in addition to a processor associated with the device. GPUs provide parallel processing capabilities for computing devices and are typically used by such devices to render the pixels of a display associated with the device and or to perform massive data processing for the device. However due to the nature of GPU design GPUs often require direct connection to a Peripheral Component Interconnect PCI bus associated with the computing device. Also GPUs often consume large amounts of power and expend large amounts of heat during use of the GPU. In virtual machines the virtual machine may connect to a GPU of a server or other computing device that acts as the GPU for the virtual machine. However blade type servers such as that shown in typically do not include a GPU. Thus many computing environments will install a computing device with a GPU in the same rack or near the blade servers and host the virtual machines that utilize a GPU on that rack server instead. Such an arrangement typically provides a one to one relationship between virtual machine and utilized GPU or blade server and available GPUs. Thus as explained above computing environments will often deploy GPUs to a selective subset of their servers such that workloads requiring GPU support have to be specifically scheduled on those rack mount servers that are provisioned with GPUs. However this reduces flexibility of workload deployment and once a virtualized workload has started running on a particular server it is pinned to the server and cannot be migrated to an alternate server for performance or power management reasons.

Alternative arrangements to such typical computing systems are now discussed. In particular the present disclosure provides for remotely located GPUs to be used by the computing devices . This is illustrated in as the virtual GPUs associated with each computing device of the system . Thus while each blade server may not have a physical GPU present within the computing device to be used by the computing device a virtual GPU is available to each computing device through the system . In particular each computing device may be associated with one or more GPUs of an array of GPUs embodied in a server of the computing environment. As explained in more detail below each computing device may utilize any number of the GPUs in the GPU array such that more than one GPU may be associated with a single computing device. Communication between the computing devices and the GPU array is performed through a network associated with the computing environment or between two or more computing environments. In such an environment the network which may be without limitation the Internet an Intranet an Ethernet network a wired network a wireless network or the like is used by one or more computing and or data storage devices e.g. one or more computing devices for implementing the system . In another implementation a single GPU may simultaneously offer GPU functionality to two or more computing devices .

To facilitate the usage of a remote GPU from the GPU array by one or more of the computing devices the blade server may utilize a hypervisor . In particular an intercept driver program of the hypervisor is used to intercept calls or commands to a GPU from the computing device . In general a hypervisor is a software program operating on a host machine that creates and runs virtual machines such as the computing device of . Another name for the computing device of may be a guest virtual machine being hosted by the host machine of the blade server . The hypervisor program of the server then runs the virtual guest machines associated with the server. As explained in more detail below with relation to the hypervisor may include an intercept driver program that is configured to intercept calls or commands to a GPU and transmit those commands through the network to a rack mount server that includes one or more GPUs available to the computing devices .

The rack mount server of the system includes the array of GPUs and an operating system OS . The OS is a computer program that is configured to manage the hardware and services of the rack mount server . In particular the OS includes an intercept driver target program that is configured to receive the commands transmitted by the intercept driver of the blade server . In other words the intercept driver transmits the commands over the network to the intercept driver target program. Once received the OS executes the commands to control the GPUs of the GPU array associated with the rack mount server to provide GPU services to the one or more computing devices of the system .

As should be appreciated the components illustrated in the system of are just some of the components that may be included in the system. For example the network may include any number of components to facilitate transmission of data and communications through the network of which are not shown in . Further the computing environment discussed above may include many blade servers hosting many computing devices all or many of which may have access to the GPU array of the rack mount server . However for simplicity the components of are used to disclose the remote GPU services system .

Another embodiment of the computing environment is illustrated in . Many of the components in the environment of are similar or the same as the components illustrated in the environment of . Thus the reference numbers associated with these components are similar from to . In addition the operation of the components is also similar or the same as described above. However in this environment the computing devices such as laptop computers personal digital assistants etc. connect directly to the network instead of connecting through a blade server. Regardless of if the blade server is executing a virtual machine or one or more computing devices connect directly to the network the operations and features described herein remain.

Beginning in operation the system intercepts one or more application programming interface API calls to a GPU from an application being executed on one of the computing devices . In one embodiment the intercept driver program of the hypervisor of the blade server hosting the computing device is configured to intercept the API call to the GPU. In other words the program executed by the computing device is configured to call a GPU associated with the computing device. Because the computing device is being hosted by the blade server this call is received by the hypervisor and more specifically by the intercept driver program. In one embodiment the intercept driver program is a GPU driver for a market available GPU modified to perform one or more of the operations of the method of . Thus the API calls from the executing program of the computing device may be typical calls to access a GPU from a computing device without need for alteration of the API call. As such the computing device and or executing program are unaware of the interception of the API call to the GPU.

In operation the hypervisor repackages the intercepted call to the GPU into a transmission message. The transmission message is configured to be transmitted through the network to a destination address of a device connected to the network. In this particular example the transmission message includes the destination address of the rack mount server and the intercepted call to the GPU. Upon repackaging the intercepted call into the transmission message the hypervisor then transmits the message in operation through the network to the destination address of the message namely the rack mount server .

The transmitted message including the intercepted command is received at the rack mount server that includes the GPU array in operation . In particular an intercept driver target program executed by the OS of the rack mount server with GPUs receives the transmitted message from the network . In addition the OS or intercept driver target program unpacks the received communication in operation to obtain the command included in the message. Upon obtaining the intercepted command from the transmitted message the OS transmits the command to one or more GPUs of the GPU array of the rack mount server in operation . The one or more GPUs receiving the command are thus unaware that the command was received over the network .

In operation the GPUs that receive the command from the OS execute the command. In other words the one or more GPUs execute the GPU call from the computing device as if the GPUs are directly connected to a PCI bus of the computing device. Thus through the operations of remotely located GPU services may be provided to the computing devices . Also information provided by the GPU in response to the received command may be transmitted back to the requesting computing device across the network in a similar manner as shown in operation .

Through the remote GPU system and method described above a computing environment may obtain several advantages over a traditional structure where the GPU for a computing device is directly connected or closely connected to the device. For example remote location of the GPUs allows for mounting of the GPUs in a purpose built enclosure rather than through a PCI bus enabling the use of blade type computing systems which typically do not include a GPU for the main applications. In addition remote location of the GPUs allows a GPU cage or rack to be located where there is adequate power and cooling for the GPUs. Thus a computing environment may locate the GPU cage near a cooling structure of the computing environment to alleviate the heat generation of the GPUs. Previous designs with a GPU in the same rack as the computing device may require additional cooling be provided to each rack individually.

In addition through the system and method described above a computing device may utilize more than one GPU to process data. In particular the OS of the rack mount server that includes the GPU array may be configured or programmed to load balance or otherwise account for requests for the use of the GPUs. In some instances the OS may recognize that the more than one GPU may be used in response to the request by the computing device . In such cases the OS may provide received commands to more than one of the GPUs of the GPU array to accommodate the GPU request from the computing device. Because each GPU is available to each computing device of the system rather than being in a one to one structure that is typical of computing devices more than one GPU is available to the computing devices to execute GPU related processes.

Further one or more of the GPUs of the GPU array may provide processing capabilities for multiple computing devices simultaneously. For example a single GPU may receive GPU instructions from a first computing device and a second computing device of the multiple computing devices and execute those instructions to provide GPU processing capabilities for both devices. Also these instructions may be executed by the GPU in an interwoven manner so that the processing is performed by the GPU without the need for one computing device to wait until the processing on the first set of instructions is complete. Rather it appears to the computing device that the GPU is providing the GPU processing with little to no delay. In one embodiment the interweaving of the instructions from the plurality of computing devices is handled by the operating system of the rack mount server . In particular the operating system may receive and schedule the GPU instructions from the computing devices over the network such that one GPU may provide GPU capabilities to multiple computing devices.

In VDI environments the remote GPU service of the system and method may also provide reduced latency in updating the terminal screen of the VDI. In particular connecting the remote clients directly to the GPU avoids hair pinning of the resulting display through the computing device hosting the VDI session thereby reducing update latency. The reduced latency may be obtained by configuring the intercept driver target program to transmit screen updates performed by one or more the GPUs directly to the VDI display terminal rather than back to the computing device or other hosting machine as illustrated in operation of . In particular the terminal of the VDI may be connected to the network such that the screen updates performed by the remote GPU may be packaged into a transmission message and transmitted by the OS through the network to the terminal. Bypassing sending the screen updates back to the hosting machine may reduce the latency of the screen update processed by the remote GPU .

Another advantage utilized by the remote GPU service system is described in the flowchart of . In particular because each GPU is accessible by each computing device and is not necessarily tied to a particular device the remote GPU service system provides a wait feature of a GPU enabling the originating computing device to be placed in a paused state releasing the infrastructure resources while the GPU is processing data and re connecting the device to the selected GPU when the device is un paused. illustrates example operations for releasing a guest machine during processing by a GPU according to one implementation. In general the operations of are performed by one or more of the components of the system illustrated in . Further the operations of may be performed through software hardware or a combination of both software and hardware embodied within any of the components of the system of .

In general the operations of are performed by the system for tasks that may run for a substantial time on one or more of the remote GPUs. Beginning in operation the guest machine connects to a remote GPU through the system and method described above. Further the guest machine may transfer the data to be processed to the GPU. In operation the remote GPU begins processing the task and the guest machine is quiesced. In one embodiment the guest machine is quiesced using a pause feature of the hypervisor of the hosting machine . By releasing the guest machine the infrastructure resources associated with the guest machine may consumed by another workload. At some later time the guest machine may be un paused and reconnected to the remote GPU in operation . Once reconnected the guest machine may obtain the results from the GPU processing from the remote GPU for the assigned task. In this manner the originating computing device utilizing a remote GPU may be released to perform functions during the day and reconnect to the GPU to obtain the results processed by the GPU. For example a financial modeling process which may take 20 hours or more to process can be started at night. However the infrastructure used by the guest machine that began the modeling process can be used for other purposes during local daytime and at night the original guest machine can be un paused to retrieve the results and perhaps restart the task with a new dataset.

I O device may also include an input device not shown such as an alphanumeric input device including alphanumeric and other keys for communicating information and or command selections to the processors . Another type of user input device includes cursor control such as a mouse a trackball or cursor direction keys for communicating direction information and command selections to the processors and for controlling cursor movement on the display device.

System may include a dynamic storage device referred to as main memory or a random access memory RAM or other computer readable devices coupled to the processor bus for storing information and instructions to be executed by the processors . Main memory also may be used for storing temporary variables or other intermediate information during execution of instructions by the processors . System may include a read only memory ROM and or other static storage device coupled to the processor bus for storing static information and instructions for the processors . The system set forth in is but one possible example of a computer system that may employ or be configured in accordance with aspects of the present disclosure.

According to one embodiment the above techniques may be performed by computer system in response to processor executing one or more sequences of one or more instructions contained in main memory . These instructions may be read into main memory from another machine readable medium such as a storage device. Execution of the sequences of instructions contained in main memory may cause processors to perform the process steps described herein. In alternative embodiments circuitry may be used in place of or in combination with the software instructions. Thus embodiments of the present disclosure may include both hardware and software components.

A machine readable medium includes any mechanism for storing or transmitting information in a form e.g. software processing application readable by a machine e.g. a computer . Such media may take the form of but is not limited to non volatile media and volatile media. Non volatile media includes optical or magnetic disks. Volatile media includes dynamic memory such as main memory . Common forms of machine readable medium may include but is not limited to magnetic storage medium e.g. floppy diskette optical storage medium e.g. CD ROM magneto optical storage medium read only memory ROM random access memory RAM erasable programmable memory e.g. EPROM and EEPROM flash memory or other types of medium suitable for storing electronic instructions.

The description above includes example systems methods techniques instruction sequences and or computer program products that embody techniques of the present disclosure. However it is understood that the described disclosure may be practiced without these specific details.

While the present disclosure has been described with reference to various implementations it will be understood that these implementations are illustrative and that the scope of the disclosure is not limited to them. Many variations modifications additions and improvements are possible. More generally implementations in accordance with the present disclosure have been described in the context of particular implementations. Functionality may be separated or combined in blocks differently in various embodiments of the disclosure or described with different terminology. These and other variations modifications additions and improvements may fall within the scope of the disclosure as defined in the claims that follow.

