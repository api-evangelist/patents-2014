---

title: Parallel processing of data for an untrusted application
abstract: An untrusted application is received at a data center including one or more processing modules and providing a native processing environment. The untrusted application includes a data parallel pipeline. Secured processing environments are used to execute the untrusted application.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09477502&OS=09477502&RS=09477502
owner: Google Inc.
number: 09477502
owner_city: Mountain View
owner_country: US
publication_date: 20141030
---
This application is a continuation of and claims the benefit of priority under to U.S. patent application Ser. No. 12 959 022 filed on Dec. 2 2010 now Issued U.S. Pat. No. 8 887 156 which claims priority to U.S. Patent Application Ser. No. 61 311 148 filed on May 4 2010 the entire contents of which are hereby incorporated by reference.

Large scale data processing may include parallel processing which generally involves performing some operation over each element of a large data set. The various operations may be chained together in a data parallel pipeline to create an efficient mechanism for processing a data set.

In one aspect an untrusted application is received at a data center including one or more processing modules and providing a native processing environment. The untrusted application includes a data parallel pipeline. The data parallel pipeline specifies multiple parallel data objects that contain multiple elements and multiple parallel operations that are associated with untrusted functions that operate on the elements. A first secured processing environment is instantiated in the native processing environment and on one or more of the processing modules. The untrusted application is executed in the first secured processing environment. Executing the application generates a dataflow graph of deferred parallel data objects and deferred parallel operations corresponding to the data parallel pipeline. Information representing the data flow graph is communicated outside of the first secured processing environment. Outside of the first secured processing environment and in the native processing environment one or more graph transformations are applied to the information representing the dataflow graph to generate a revised dataflow graph that includes one or more of the deferred parallel data objects and deferred combined parallel data operations that are associated with one or more of the untrusted functions. The deferred combined parallel operations are executed to produce materialized parallel data objects corresponding to the deferred parallel data objects. Executing the deferred combined parallel operations includes instantiating one or more second secured processing environments in the native processing environment and on one or more of the processing modules and executing the untrusted functions associated with the deferred combined parallel operations in the one or more second secured processing environments.

Implementations may include one or more of the following features. For example the first secured processing environment may include a first virtual machine and the one or more second secured processing environments may include a second virtual machine. The first virtual machine and the one or more second virtual machines may be hardware virtual machines. Executing the untrusted functions associated with the deferred combined parallel operations in the one or more second secured processing environments may include communicating an input batch of records into the second secured processing environment from outside of the second secured processing environment the input batch of records including multiple individual input records executing at least one of the untrusted functions associated with the deferred combined parallel operations on each of the individual records in the input batch to generate output records collecting the output records into an output batch and communicating the output batch outside of the second secured processing environment.

An output of the untrusted application may be sent to a client system that sent the untrusted application to the data center. Communicating the information representing the data flow graph outside of the first secured processing environment may include communicating the information representing the data flow graph to an execute graph service outside of the first secured processing environment.

The deferred combined parallel data operations may include at least one generalized mapreduce operation. The generalized mapreduce operation may include multiple parallel map operations and multiple parallel reduce operations and being translatable to a single mapreduce operation that includes a single map function to implement the multiple parallel map operations and a single reduce function to implement the multiple parallel reduce operations. The single map function and the single reduce function may include one or more of the untrusted functions.

Executing the deferred combined parallel operations may include translating the combined mapreduce operation to the single mapreduce operation. Executing the untrusted functions associated with the deferred combined parallel operations in the one or more second secured processing environments may include executing the single map function and the single reduce function in the one or more second secured processing environments.

Executing the untrusted application in the secured processing environment may include executing the untrusted application within a virtual machine in the first secured processing environment. Executing the untrusted functions associated with the deferred combined parallel operations in the one or more secured processing environments may include executing the untrusted functions associated with the deferred combined parallel operations in a virtual machine within the one or more second secured processing environments.

Communicating information representing the data flow graph outside of the first secured processing environment may include communicating information representing the data flow graph outside of the first secured processing environment using a remote procedure call. The remote procedure call may be audited.

In another aspect a system includes one or more processing modules configured to provide native processing environment and to implement a first secured processing environment in the native processing environment a service located outside of the first secured processing environment and in the native processing environment and one or more second secured processing environments in the native processing environment.

The first secured processing environment configured to execute an untrusted application that includes a data parallel pipeline. The data parallel pipeline specifies multiple parallel data objects that contain multiple elements and multiple parallel operations that are associated with untrusted functions that operate on the elements. Executing the application generates a dataflow graph of deferred parallel data objects and deferred parallel operations corresponding to the data parallel pipeline. The first secured processing environment is also configured to communicate information representing the data flow graph outside of the first secured processing environment 

The service is configured to receive the information representing the data flow graph from the first secured processing environment apply one or more graph transformations to the information representing the dataflow graph to generate a revised dataflow graph that includes one or more of the deferred parallel data objects and deferred combined parallel data operations that are associated with one or more of the untrusted functions and cause execution of the deferred combined parallel operations to produce materialized parallel data objects corresponding to the deferred parallel data objects.

The one or more second secured processing environments are configured to execute the untrusted functions associated with the deferred combined parallel operations to result in execution of the deferred combined parallel operations.

Implementations may include one or more of the following features. For example the first secured processing environment may include a first virtual machine and the one or more second secured processing environments may include a second virtual machine. The first virtual machine and the one or more second virtual machines may be hardware virtual machines.

The one or more processing devices may be configured to implement a worker configured to communicate an input batch of records into the second secured processing environment from outside of the second secured processing environment. The input batch of records may include multiple individual input records. To execute the untrusted functions associated with the deferred combined parallel operations the one or more second secured processing environments may be configured to execute at least one of the untrusted functions associated with the deferred combined parallel operations on each of the individual records in the input batch to generate output records collect the output records into an output batch and communicate the output batch to the worker.

The system may include a client system configured to receive an output of the untrusted application. The deferred combined parallel data operations may include at least one generalized mapreduce operation. The generalized mapreduce operation may include multiple parallel map operations and multiple parallel reduce operations and be translatable to a single mapreduce operation that includes a single map function to implement the multiple parallel map operations and a single reduce function to implement the multiple parallel reduce operations. The single map function and the single reduce function may include one or more of the untrusted functions.

The service may be configured to translate the combined mapreduce operation to the single mapreduce operation. The one or more second secured processing environments may be configured to execute the single map function and the single reduce function in the one or more second secured processing environments.

The first secured processing environment may be configured to execute the untrusted application within a virtual machine in the first secured processing environment. The one or more second secured processing environments may be configured to execute the untrusted functions associated with the deferred combined parallel operations in a virtual machine within the one or more second secured processing environments.

In another aspect information representing a dataflow graph of deferred parallel data objects and deferred parallel operations is accessed. The deferred parallel data objects and deferred parallel operations correspond to parallel data objects and parallel operations specified by a data parallel pipeline included in an untrusted application. The parallel data objects contain multiple elements and the parallel operations are associated with untrusted functions that operate on the elements. One or more graph transformations are applied to the information representing the dataflow graph to generate a revised dataflow graph that includes one or more of the deferred parallel data objects and deferred combined parallel data operations that are associated with one or more of the untrusted functions. The deferred combined parallel operations are executed to produce materialized parallel data objects corresponding to the deferred parallel data objects. Executing the deferred combined parallel operations includes instantiating one or more secured processing environments and executing the untrusted functions associated with the deferred combined parallel operations in the one or more secured processing environments.

The untrusted application that includes the data parallel pipeline may be received and an initial secured processing environment may be instantiated. The untrusted application may be executed in the initial secured processing environment. Executing the application may generate the dataflow graph of deferred parallel data objects and deferred parallel operations. The information representing the data flow graph may be communicated outside of the initial secured processing environment such that the graph transformations are applied to the information representing the dataflow graph outside of the initial secured processing environment. The one or more secured processing environments may include a virtual machine.

Implementations of the described techniques may include hardware a system a method or process or computer software on a computer accessible medium.

The details of one or more implementations are set forth in the accompanying drawings and the description below. Other features will be apparent from the description and drawings and from the claims.

In general the techniques described in this document can be applied to large scale data processing and in particular to large scale data parallel pipelines. Such large scale processing may be performed in a distributed data processing system such as a datacenter or a network of datacenters. For example large scale Internet services and the massively parallel computing infrastructure that support such services may employ warehouse sized computing systems made up of thousands or tens of thousands of computing nodes.

The datacenter includes multiple racks . While only two racks are shown the datacenter may have many more racks. Each rack can include a frame or cabinet into which components such as processing modules are mounted. In general each processing module can include a circuit board such as a motherboard on which a variety of computer related components are mounted to perform data processing. The processing modules within each rack are interconnected to one another through for example a rack switch and the racks within each datacenter are also interconnected through for example a datacenter switch.

In some implementations the processing modules may each take on a role as a master or slave. The master modules control scheduling and data distribution tasks amongst themselves and the slaves. A rack can include storage e.g. one or more network attached disks that is shared by the one or more processing modules and or each processing module may include its own storage. Additionally or alternatively there may be remote storage connected to the racks through a network.

The datacenter may include dedicated optical links or other dedicated communication channels as well as supporting hardware such as modems bridges routers switches wireless antennas and towers and the like. The datacenter may include one or more wide area networks WANs as well as multiple local area networks LANs .

The memory stores application software a mapreduce library a pipeline library and an operating system e.g. Linux . The operating system generally includes procedures for handling various basic system services and for performing hardware dependent tasks. The application software performs large scale data processing.

The libraries and provide functions and classes that may be employed by the application software to perform large scale data processing and implement data parallel pipelines in such large scale data processing. The mapreduce library can support the MapReduce programming model for processing massive amounts of data in parallel. The MapReduce model is described in for example MapReduce Simplified Data Processing on Large Clusters OSDI 04 Sixth Symposium on Operating System Design and Implementation San Francisco Calif. December 2004 and U.S. Pat. No. 7 650 331 both of which are incorporated by reference.

In general the MapReduce model provides an abstraction to application developers for how to think about their computations. The application developers can formulate their computations according to the abstraction which can simplify the building of programs to perform large scale parallel data processing. The application developers can employ the MapReduce model with or without using the mapreduce library . The mapreduce library however can manage many of the difficult low level tasks. Such low level tasks may include for example selecting appropriate parallel worker machines distributing to them the program to run managing the temporary storage and flow of intermediate data between the three phases synchronizing the overall sequencing of the phases and coping with transient failures of machines networks and software.

The MapReduce model generally involves breaking computations down into a mapreduce operation which includes a single map operation and a single reduce operation. The map operation performs an operation on each of the logical records in the input to compute a set of intermediate key value pairs. The reduce operation performs an operation on the values that share the same key to combine the values in some manner. Implicit in this model is a shuffle operation which involves grouping all of the values with the same key.

The mapreduce library may implement a map phase a shuffle phase and a reduce phase to support computations formulated according to the MapReduce model. In some implementations to use the mapreduce library a user program or another library such as pipeline library calls the mapreduce library specifying information identifying the input file s information identifying or specifying the output files to receive output data and two application specific data processing operators the map operator and the reduce operator. Generally the map operator specifies a map function that processes the input data to produce intermediate data and the reduce operator specifies a reduce function that merges or otherwise combines the intermediate data values. The mapreduce library then employs this information to implement that map phase the shuffle phase and the reduce phase.

The map phase starts by reading a collection of values or key value pairs from an input source such as a text file binary record oriented file or MySql database. Large data sets may be represented by multiple even thousands of files which may be referred to as shards and multiple file shards can be read as a single logical input source. The map phase then invokes the user defined function the map function or Mapper on each element independently and in parallel. For each input element the user defined function emits zero or more key value pairs which are the outputs of the map phase.

The shuffle phase takes the key value pairs emitted by the Mappers and groups together all the key value pairs with the same key. The shuffle phase then outputs each distinct key and a stream of all the values with that key to the next phase the reduce phase.

The reduce phase takes the key grouped data emitted by the shuffle phase and invokes the user defined function the reduce function or Reducer on each distinct key and values group independently and in parallel. Each Reducer invocation is passed a key and an iterator over all the values associated with that key and emits zero or more replacement values to associate with the input key. The Reducer typically performs some kind of aggregation over all the values with a given key. For some operations the Reducer is just the identity function. The key value pairs emitted from all the Reducer calls are then written to an output sink e.g. a sharded file or database.

To implement these phases the mapreduce library may divide the input pieces into M pieces for example into 64 megabyte MB sized files and start up multiple copies of the program that uses the library on a cluster of machines such as multiple ones of the processing modules . One of the copies may be a master copy and the rest may be worker copies that are assigned work by the master. The master selects idle workers and assigns each one a map task or a reduce task. There are M map tasks one for each input piece . The workers assigned to a map task use the Mapper to perform the mapping operation on the inputs to produce the intermediate results which are divided for example into R sets. When the intermediate results are divided into R sets there are R reduce tasks to assign. The workers assigned to a reduce task use the Reducer to perform the reduce operation on the intermediate values to produce the output. Once all map tasks and all reduce tasks are completed the master returns to the user program or library employing the mapreduce library . As a result the mapreduce operation is implemented as a set of parallel operations across a cluster of processing devices.

For Reducers that first combine all the values with a given key using an associative commutative operation a separate user defined Combiner function can be specified to perform partial combining of values associated with a given key during the map phase. Each map worker can keep a cache of key value pairs that have been emitted from the Mapper and use the Combiner function to combine locally as much as possible before sending the combined key value pairs on to the Shuffle phase. The Reducer may complete the combining step by combining values from different map workers.

By default the Shuffle phase may send each key and values group to arbitrarily but deterministically chosen reduce worker machine with this choice determining which output file shard will hold that key s results. Alternatively a user defined Sharder function can be specified that selects which reduce worker machine should receive the group for a given key. A user defined Sharder can be used to aid in load balancing. The user defined Sharder can also be used to sort the output keys into reduce buckets with all the keys of the ireduce worker being ordered before all the keys of the i 1st reduce worker. Coupled with the fact that each reduce worker processes keys in lexicographic order this kind of Sharder can be used to produce sorted output.

The pipeline library provides functions and classes that support data parallel pipelines and in particular pipelines that include chains or directed acyclic graphs of mapreduce operations. The pipeline library may help alleviate some of the burdens of implementing chains of mapreduce operations. In general many real world computations require a chain of mapreduce stages. While some logical computations can be expressed as a mapreduce operation others require a sequence or graph of mapreduce operations. As the complexity of the logical computations grows the challenge of mapping the computations into physical sequences of mapreduce operations increases. Higher level concepts such as count the number of occurrences or join tables by key are generally hand compiled into lower level mapreduce operations. In addition the user may take on the additional burdens of writing a driver program to invoke the mapreduce operations in the proper sequence and managing the creation and deletion of intermediate files holding the data.

The pipeline library may obviate or reduce some of the difficulty in producing data parallel pipelines that involve multiple mapreduce operations as well as the need for the developer to produce additional coordination code to chain together the separate mapreduce stages in such data parallel pipelines. The pipeline library also may obviate or reduce additional work to manage the creation and later deletion of the intermediate results in between pipeline stages. As a result the pipeline library may help prevent the logical computation itself from becoming hidden among all the low level coordination details thereby making it easier for new developers to understand the computation. Moreover making use of the pipeline library may help prevent the division of the pipeline into particular stages from becoming baked in to the code and difficult to change later if the logical computation needs to evolve.

In general the application software may employ one or both of the libraries or . An application developer may develop application software that employs the mapreduce library to perform computations formulated as a mapreduce operation.

The application developer may alternatively or additionally employ the pipeline library when developing a data parallel pipeline that includes multiple mapreduce operations. As discussed further below the pipeline library may allow the developer to code the computations in a more natural manner using the native programming language in which the pipeline library is implemented without thinking about casting the logical computation in terms of mapreduce operations or building an ordered graph of operations. The pipeline library can formulate the logical computation in terms of multiple mapreduce operations prior to execution and then execute the computation either by implementing the mapreduce operations itself or interfacing with the mapreduce library to implement the mapreduce operations.

Parallel data collection classes and operations present a simple high level uniform abstraction over many different data representations and over different execution strategies. The parallel data collection classes abstract away the details of how data is represented including whether the data is represented as an in memory data structure as one or more files or as an external storage service. Similarly parallel operations abstract away their implementation strategy such as whether an operation is implemented as a local sequential loop as a remote parallel invocation of the mapreduce library as a query on a database or as a streaming computation.

Rather than evaluate the parallel operations as they are traversed when the data parallel pipeline is executed the evaluator defers the evaluation of parallel operations. Instead the evaluator constructs an internal execution plan dataflow graph that contains the operations and their arguments. Once the execution plan dataflow graph for the whole logical computation is constructed the optimizer revises the execution plan for example by applying graph transformations that fuse or combine chains of parallel operations together into a smaller number of combined operations. The revised execution plan may include a generalized mapreduce operation that includes multiple parallel map operations and multiple parallel reduce operations for example the MapShuffleCombineReduce operation described further below but which can be translated to a single mapreduce operation with a single map function to implement the multiple map operations and a single reduce function to implement the multiple reduce operations. The executor executes the revised operations using underlying primitives e.g. MapReduce operations . When running the execution plan the executor may choose which strategy to use to implement each operation e.g. local sequential loop vs. remote parallel MapReduce based in part on the size of the data being processed. The executor also may place remote computations near the data on which they operate and may perform independent operations in parallel. The executor also may manage the creation and cleanup of any intermediate files needed within the computation.

The pipeline library may be implemented in any of a number of programming languages. The following describes examples of aspects of an implementation in the Java programming language.

The pipeline library provides a parallel data collection class referred to as a PCollection which is an immutable bag of elements of type T. A PCollection can either have a well defined order called a sequence or the elements can be unordered called a collection . Because they are less constrained collections may be more efficient to generate and process than sequences. A PCollection can be created by reading a file in one of several possible formats. For example a text file can be read as a PCollection and a binary record oriented file can be read as a PCollection given a specification of how to decode each binary record into an object of type T. When the pipeline library is implemented using Java a PCollection may also be created from an in memory Java Collection.

Data sets represented by multiple file shards can be read in as a single logical PCollection. For example 

In this example recordsOf . . . specifies a particular way in which a DocInfo instance is encoded as a binary record. Other predefined encoding specifiers may include strings for UTF 8 encoded text ints for a variable length encoding of 32 bit integers and pairsOf e1 e2 for an encoding of pairs derived from the encodings of the components. Some implementations may allow users to specify their own custom encodings.

A second parallel data collection class is PTable which represents an immutable multi map with keys of type K and values of type V. PTable may be just an unordered bag of pairs. Some of the parallel operations may apply only to PCollections of pairs and in Java PTable may be implemented as a subclass of PCollection to capture this abstraction. In another language PTable might be defined as a type synonym of PCollection.

The parallel data objects such as PCollections may be implemented as first class objects of the native language in which the library is implemented. When this is the case the objects may be manipulable like other objects in the native language. For example the PCollections may be able to be passed into and returned from regular methods in the language and may be able to be stored in other data structures of the language although some implementations may prevent the PCollections from being stored in other PCollections . Also regular control flow constructs of the native language may be able to be used to define computations involving objects including functions conditionals and loops. For example if Java is the native language 

Implementing the parallel data objects as first class objects in the native language of the library may simplify the development of programs using the library since the developer can use the parallel data objects in the same manner he or she would use other objects.

In addition to the parallel data collection classes the pipeline library can also include a single data collection class PObject to support the ability to inspect the contents of PCollections during the execution of a pipeline. In contrast to a PCollection which holds multiple elements a PObject is a container for a single object of type T for example a single native object e.g. Java object of type T and any associated methods of PObjects are designed to operate on a single element. Like PCollections PObjects can be either deferred or materialized as described further below allowing them to be computed as results of deferred operations in pipelines. Once a pipeline is run the contents of a now materialized PObject can be extracted using getValue .

For example in an implementation using Java an asSequentialCollection operation can be applied to a PCollection to yield a PObject which can be inspected once the pipeline runs to read out all the elements of the computed PCollection as a regular Java in memory Collection 

As another example the combine operation described below applied to a PCollection and a combining function over Ts yields a PObject representing the fully combined result. Global sums and maxima can be computed this way.

The contents of PObjects also may be able to be examined within the execution of a pipeline for example using an operate primitive provided by the pipeline library . The operate primitive takes a list of PObjects and an argument OperateFn which defines the operation to be performed on each PObject and returns a list of PObjects. When evaluated operate extracts the contents of the now materialized argument PObjects and passes them into the argument OperateFn. The OperateFn returns a list of native objects such as Java objects and operate wraps these native objects inside of PObjects which are returned as the results. Using this primitive arbitrary computations can be embedded within a pipeline and executed in deferred fashion. In other words operations other than ParallelDo operations described below which operate on PCollections that contain multiple elements can be included in the pipeline. For example consider embedding a call to an external service that reads and writes files 

This example uses operations for converting between PCollections and PObjects containing file names. The viewAsFile operation applied to a PCollection and a file format choice yields a PObject containing the name of a temporary sharded file of the chosen format where the PCollection s contents may be found during execution of the pipeline. File reading operations such as readRecordFileCollection may be overloaded to allow reading files whose names are contained in PObjects.

In much the same way the contents of PObjects can also be examined inside a DoFn described below by passing them in as side inputs to parallelDo . Normally a DoFn performs an operation on each element of a PCollection and just receives the PCollection as an input. In some cases the operation on each PCollection may involve a value or other data stored in a PObject. In this case the DoFn may receive the PCollection as an input as normal and a PObject as a side input. When the pipeline is run and the parallelDo operation is eventually evaluated the contents of any now materialized PObject side inputs are extracted and provided to the user s DoFn and then the DoFn is invoked on each element of the input PCollection to perform the defined operation on the element using the data from the PObject s . For example 

As described above data parallel operations are invoked on parallel data objects such as PCollections. The pipeline library defines some primitive data parallel operations with other operations being implemented in terms of these primitives. One of the data parallel primitives is parallelDo which supports elementwise computation over an input PCollection to produce a new output PCollection. This operation takes as its main argument a DoFn a function like object defining how to map each value in the input PCollection into zero or more values to appear in the output PCollection. This operation also takes an indication of the kind of PCollection or PTable to produce as a result. For example 

In this code collectionOf strings specifies that the parallelDo operation should produce an unordered PCollection whose String elements should be encoded using UTF 8. Other options may include sequenceOf elemEncoding for ordered PCollections and tableOf keyEncoding valueEncoding for PTables. emitFn is a call back function passed to the user s process . . . method which should invoke emitFn.emit outElem for each outElem that should be added to the output PCollection. Subclasses of DoFn may be included such as MapFn implementing a map and FilterFn implementing a filter to provide simpler interfaces in some cases.

The operation parallelDo can be used to express both the map and reduce parts of a MapReduce operation. The library also may include a version of parallelDo that allows multiple output PCollections to be produced simultaneously from a single traversal of the input PCollection.

DoFn functions may be prevented from accessing any global mutable state of the enclosing program if DoFn functions can be distributed remotely and run in parallel. DoFn objects may be able to maintain local instance variable state but there may be multiple DoFn replicas operating concurrently with no shared state.

A second primitive groupByKey converts a multimap of type PTable which can have many key value pairs with the same key into a uni map of type PTable where each key maps to an unordered collection of all the values with that key. For example the following computes a table mapping URLs to the collection of documents that link to them 

The operation groupByKey corresponds to the shuffle step of MapReduce. There may also be a variant that allows specifying a sorting order for the collection of values for each key.

A third primitive combineValues takes an input PTable and an associative combining function on Vs and returns a PTable where each input collection of values has been combined into a single output value. For example 

The operation combineValues is semantically a special case of parallelDo but the associativity of the combining function allows the operation to be implemented through a combination of a MapReduce Combiner which runs as part of each mapper and a MapReduce Reducer to finish the combining which may be more efficient than doing all the combining in the reducer.

A fourth primitive flatten takes a list of PCollections and returns a single PCollection that contains all the elements of the input PCollections. The operation flatten may not actually copy the inputs but rather just view the inputs as if the inputs were one logical PCollection.

A pipeline typically concludes with operations that write the final resulting PCollections to external storage. For example 

The pipeline library may include a number of other operations on PCollections that are derived in terms of the above described primitives. These derived operations may be the same as helper functions the user could write. For example a count operation takes a PCollection and returns a PTable mapping each distinct element of the input PCollection to the number of times the element occurs. This function may be implemented in terms of parallelDo groupByKey and combineValues using the same pattern as was used to compute wordCounts above. The code above can be simplified to the following 

Another operation join implements a join over two or more PTables sharing a common key type. When applied to a multimap PTable and a multimap PTable join returns a unimap PTable that maps each key in either of the input tables to the collection of all values with that key in the first table and the collection of all values with that key in the second table. This resulting table can be processed further to compute a traditional inner or outer join but it may be more efficient to be able to manipulate the value collections directly without computing their cross product.

4. Apply parallelDo to the key grouped table converting each Collection into a Pair of a Collection and a Collection.

Another derived operation is top which takes a comparison function and a count N and returns the greatest N elements of its receiver PCollection according to the comparison function. This operation may be implemented on top of parallelDo groupByKey and combineValues .

The operations mentioned above to read multiple file shards as a single PCollection are derived operations too implemented using flatten and the single file read primitives.

As described above the pipeline library executes parallel operations lazily using deferred evaluation. To that end the evaluator defers the evaluation of parallel operations and instead constructs an internal execution plan dataflow graph that contains the operations and the arguments of the operations. Each parallel data object such as a PCollection is represented internally either in deferred not yet computed or materialized computed state. A deferred parallel data object for example holds a pointer to the deferred operation that computes the parallel data object. A deferred operation in turn may hold references to the parallel data objects that are the arguments of the deferred operation which may themselves be deferred or materialized and the deferred parallel data objects that are the results of the operation. When a library operation like parallelDo is called the library creates a ParallelDo deferred operation object and returns a new deferred PCollection that points to the operation. In other words as the data parallel pipeline is executed the evaluator converts the parallel data objects and parallel operations into a directed acyclic graph of deferred unevaluated objects and operations. This graph may be referred to as the execution plan or execution plan dataflow graph.

The optimizer fuses chains or subgraphs of parallel operations in the dataflow graph together into a smaller number of operations some of which may be combined operations which the executor can then execute using an underlying primitive or other logic. The optimizer may be written for example as a series of independent graph transformations. In one implementation the optimizer performs a series of passes over the initial execution plan that reduces the number of overall operations and groups operations with the overall goal of producing the fewest MapShuffleCombineReduce MSCR operations.

An MSCR operation includes a combination of ParallelDo GroupByKey CombineValues and Flatten operations. An MSCR operation can be mapped to and run as a single mapreduce operation. An MSCR operation has M input channels each performing a map operation and R output channels each performing a shuffle a combine and a reduce . Each input channel m takes a PCollection as input and performs an R output ParallelDo map operation on that input to produce R outputs of type PTables. Each output channel R flattens its M inputs and then either a performs a GroupByKey shuffle an optional CombineValues combine and a O output ParallelDo reduce which defaults to the identity operation and then writes the results to Ooutput PCollections or b writes the input directly as the output. The former kind of output channel may be referred to as a grouping channel while the latter kind of output channel may be referred to as a pass through channel. A pass through channel may allow the output of a mapper be a result of an MSCR operation.

MSCR generalizes the MapReduce model by allowing multiple mappers and multiple reducers and combiners by allowing each reducer to produce multiple outputs by removing the requirement that the reducer must produce outputs with the same key as the reducer input and by allowing pass through outputs. Thus any given MSCR may include multiple parallel map operations that each operate on different inputs and multiple reduce operations that operate on the outputs of the map operations to produce multiple different outputs. Despite its apparent greater expressiveness each MSCR operation can be implemented using a single mapreduce operation that includes a single map function to implement the map operations on the different inputs and a single reduce function to implement the reduce operations to produce the multiple outputs.

Once the execution plan is revised by the optimizer the executor executes the revised execution plan dataflow graph. In one implementation the pipeline library performs batch execution. In other words the executor traverses the operations in the revised execution plan in forward topological order and executes each one in turn. Independent operations may be able to be executed simultaneously. Alternatively incremental or continuous execution of pipelines may be implemented where incrementally added inputs lead to quick incremental update of outputs. Further optimization may be performed across pipelines run by multiple users over common data sources.

The executor executes operations other than a MSCR by performing the appropriate computations that perform the operation. MSCRs are mapped to a single mapreduce operation which is then executed.

In some implementations the executor first decides whether the mapreduce operation should be run locally and sequentially or as a remote parallel mapreduce operation using for example mapreduce library . Since there is overhead in launching a remote parallel job local evaluation may be used for modest size inputs where the gain from parallel processing is outweighed by the start up overheads. Modest size data sets may be common during development and testing. Using local evaluation for these data sets may therefore facilitate the use of regular IDEs debuggers profilers and related tools easing the task of developing programs that include data parallel computations.

If the input data set appears large e.g. greater than or equal 64 Megabytes the executor may choose to launch a remote parallel MapReduce operation using the mapreduce library . The executor may use observations of the input data sizes and estimates of the output data sizes to automatically choose a reasonable number of parallel worker machines. Users can assist in estimating output data sizes for example by augmenting a DoFn with a method that returns the expected ratio of output data size to input data size based on the computation represented by that DoFn. Estimates may be refined through dynamic monitoring and feedback of observed output data sizes. Relatively more parallel workers may be allocated to jobs that have a higher ratio of CPU to I O.

The executor may automatically create temporary files to hold the outputs of each operation executed. Once the pipeline is completed all of these temporary files may be automatically deleted. Alternatively or additionally some or all of these temporary files may be deleted as soon as they are no longer needed later in the pipeline.

In general the pipeline library may be designed to make building and running pipelines feel as similar as possible to running a regular program in the native language for which the pipeline library was designed. When the native language is Java using local sequential evaluation for modest sized inputs is one way to do so. Another way is by automatically routing any output to System.out or System.err from within a user s DoFn such as debugging prints from the corresponding remote MapReduce worker to the main program s output streams. Likewise any exceptions thrown within a DoFn running on a remote MapReduce worker are captured sent to the main program and rethrown.

The library may support a cached execution mode. In this mode rather than recompute an operation the executor first attempts to reuse the result of that operation from the previous run if it was saved in a internal or user visible file and if the executor determines that the operation s result hasn t changed. An operation s result may be considered unchanged if a the operation s inputs haven t changed and b the operation s code and captured state haven t changed. The executor may perform an automatic conservative analysis to identify when reuse of previous results is guaranteed to be safe. Caching can lead to quick edit compile run debug cycles even for pipelines that would normally take hours to run. This may reduce the amount of time required to find a bug in a late pipeline stage fix the program and then reexecute the revised pipeline from scratch.

Once the evaluator has generated the dataflow graph the optimizer applies one or more graph transformations to the dataflow graph to generate a revised dataflow graph that includes the deferred parallel data objects or a subset and the deferred combined parallel data operations . The deferred combined parallel data operations may include one or more generalized mapreduce operations for example an MSCR which includes multiple map operations and multiple reduce operations but is translatable to a single mapreduce operation that includes a single map function to implement the map operations and a single reduce function to implement the reduce operations.

In one implementation the optimizer performs a series of passes over the dataflow graph applying the following graph transformations or annotations in the following order 1 sink flattens 2 lift CombineValues operations 3 insert fusion blocks 4 fuse ParallelDos and 5 fuse MSCRs.

The sink flattens transformation involves pushing a Flatten operation down through consuming ParallelDo operations by duplicating the ParallelDo before each input to the flatten. In other words h f a g b is equivalent to h f a h g b . This transformation creates opportunities for ParallelDo fusion described below .

The lift CombineValues operations annotation involves marking certain CombineValues operations for treatment as ParallelDos for ParallelDo fusion. If a CombineValues operation immediately follows a GroupByKey operation the GroupByKey records that fact. The original CombineValues is left in place and is henceforth treated as a normal ParallelDo operation and subject to ParallelDo fusion.

The insert fusion blocks annotation involves annotating the ParallelDos connecting two GroupByKey operations. If two GroupByKey operations are connected by a chain of one or more ParallelDo operations the optimizer chooses which ParallelDos should fuse up into the output channel of the earlier GroupByKey and which should fuse down into the input channel of the later GroupByKey. The optimizer estimates the size of the intermediate PCollections along the chain of ParallelDos identifies one with minimal expected size and marks that intermediate PCollection as a boundary blocking ParallelDo fusion that is marks the ParallelDos on either side of that PCollection as not being subject to fusion into one another .

The fuse ParallelDos transformation involves fusing ParallelDos together. One type of ParallelDo fusion that the optimizer may perform is referred to as producer consumer fusion. If one ParallelDo operation performs function and the result is consumed by another ParallelDo operation that performs function g the two ParallelDo operations may be replaced by a single ParallelDo that computes both and g . If the result of the ParallelDo is not needed by other operations in the graph fusion has rendered it unnecessary and the code to produce it may be removed as dead.

Another type of ParallelDo fusion is referred to as sibling fusion. ParallelDo sibling fusion may be applied when two or more ParallelDo operations read the same input PCollection. The ParallelDo operations can be fused into a single multi output ParallelDo operation that computes the results of all the fused operations in a single pass over the input. Both producer consumer and sibling fusion can apply to arbitrary trees of multi output ParallelDo operations.

As mentioned earlier CombineValues operations are special cases of ParallelDo operations that can be repeatedly applied to partially computed results. As such ParallelDo fusion may also be applied to CombineValues operations.

The fuse MSCRs transformation involves creating MSCR operations. An MSCR operation starts from a set of related GroupByKey operations. GroupByKey operations may be considered related if the operations consume possibly via Flatten operations the same input or inputs created by the same ParallelDo operations. The MSCR s input and output channels are derived from the related GroupByKey operations and the adjacent operations in the execution plan. Each ParallelDo operation with at least one output consumed by one of the GroupByKey operations possibly via Flatten operations is fused into the MSCR forming a new input channel. Any other inputs to the GroupByKeys also form new input channels with identity mappers. Each of the related GroupByKey operations starts an output channel. If a GroupByKey s result is consumed solely by a CombineValues operation that operation is fused into the corresponding output channel. Similarly if the GroupByKey s or fused CombineValues s result is consumed solely by a ParallelDo operation that operation is also fused into the output channel if it cannot be fused into a different MSCR s input channel. All the PCollections internal to the fused ParallelDo GroupByKey and CombineValues operations are now unnecessary and may be deleted. Finally each output of a mapper ParallelDo that flows to an operation or output other than one of the related GroupByKeys generates its own pass through output channel.

After all GroupByKey operations have been transformed into MSCR operations any remaining ParallelDo operations are also transformed into trivial MSCR operations with a single input channel containing the ParallelDo and an identity output channel. The final optimized execution plan contains only MSCR Flatten and Operate operations.

Once the revised dataflow graph is generated the executor executes the deferred combined parallel operations to produce materialized parallel data objects corresponding to the deferred parallel data objects . Executing the generalized mapreduce operation for example MSCR can include translating the generalized mapreduce operation to the single mapreduce operation and executing the single mapreduce operation. Before executing the single mapreduce operation the executor may decide whether to execute the single mapreduce operation as a local sequential operation or a remote parallel operation and then execute the single mapreduce accordingly. For example the executor may decide based on the size of the input data set as described above.

As the executor encounters non MSCR operations the executor executes those operations locally using logic included in the pipeline library . On the other hand when the executor encounters an MSCR operation the executor determines whether to execute the MSCR as a local sequential operation or instead as a remote parallel operation using the mapreduce library . For example the executor may determine an estimated size of data associated with the MSCR and determine whether the estimated size exceeds a threshold size. If the estimated size is below the threshold size executor may execute the MSCR as a local sequential operation . Conversely if the estimated size is equal to or exceeds the threshold size the executor may execute the MSCR operation as remote parallel operation by translating the MSCR into a single mapreduce operation and executing that mapreduce operation as a remote parallel operation using the mapreduce library .

For instance in one implementation the executor estimates the size of the input data for each input channel of the MSCR estimates the size of the intermediary data produced by each input channel and estimates the size of the output data from each output channel. If any of these size estimates is equal to or exceeds 64 megabytes MB then the MSCR is executed as a remote parallel operation using the mapreduce library .

When executing the MSCR as a local sequential operation the executor may perform the appropriate operations over the data is a sequential fashion. For example the executor may implement in memory for loops to access the data and perform the appropriate operations on the data.

When executing the MSCR as a remote parallel operation using the mapreduce library the executor may estimate the number of map worker processes and reduce worker processes needed to perform the associated processing based on the configuration of the input and output channels of the MSCR. For instance the executor may estimate the number of map worker processes for each input channel based for example on an estimated or known size of the input data for each input channel and similarly may estimate the number of reduce worker processes based for example on an estimated or known amount of data to be processed by each output channel. The executor may then add up the number of map worker processes and reduce worker processes and cause these worker processes to be invoked using the mapreduce library

Each map worker and each reduce worker is given an index number. For example if the MSCR includes two input channels one with 4 map worker processes and the other with 5 map worker processes then the 9 workers may be given an index number from 1 to 9. The same may occur for the reduce worker processes. These index numbers are used to associate a given map worker process or reduce worker process with a particular input or output channel respectively. Continuing the foregoing example index numbers 1 4 may be associated with the first input channel while index numbers 5 9 may be associated with the second input channel.

The executor also translates the MSCR into a single mapreduce operation by generating a single map function that implements the multiple map operations in the input channels of the MSCR and a single reduce function that implements the multiple reduce operations in the output channels of the MSCR. The map function uses the index of the map worker processes as the basis for selecting which map operation is applied to the input. For example an if then statement may be included as part of the map function with the index numbers of the map workers being the decision points for the if then statement.

Thus as the mapreduce library assigns a map task to a map worker process the worker s associated index is passed into the map function along with an identity of the file to be worked on. The index number then dictates which map operation parallelDo the map function invokes on the elements in the file and thereby which input channel the worker implements.

Similarly the reduce function uses the index of the reduce worker processes as the basis for selecting which reduce operation is applied to the input of the reduce worker process. As a reduce worker function is assigned a reduce task the worker s associated index is passed into the reduce function along with an identity of the file to be worked on which contains a single flattened stream of key grouped inputs . The index number then dictates which reduce operation the reduce function invokes on the elements in the file and thereby which output channel the worker implements. If the reduce worker process implements a grouping output channel the reduce worker process performs the CombineValues combine operation if any and then the ParallelDo reduce operation. If the reduce worker process implements a pass through output channel the reduce worker process performs an ungrouping operation that outputs key value pairs undoing the effect of the mapreduce library s implicit shuffle.

Each of the MSCR operation s input channels can emit key value pairs to any of its R output channels. For example input channel sends one output to output channel and another output to output channel and nothing to output channel .

The mapreduce library handles the shuffle on the data output by the map worker processes and then routes the output to the correct reducer worker. Each of the MSCR operation s input channels can emit key value pairs to any of its R output channels. For example input channel sends one output to output channel and another output to output channel and nothing to output channel . This is handled for example by the pipeline library by using an emitToShard key value shardNum primitive in the mapreduce library which allows the pipeline library to designate which reduce worker process a given output of a map worker process is sent to. When sending an output from a given map worker process to a particular output channel the pipeline library may compute the range of reduce worker indices corresponding to that output channel chooses one of them using a deterministic function and uses the emitToShard function to send the output to the chosen reducer worker. The deterministic function may include a hash on the key associated with the output values with the result of the hash determining which of the reduce worker processes within the range of indices for the output is chosen. This may ensure that all of the data associated with a particular key is sent to the same reduce worker process.

In one implementation the mapreduce library only directly supports writing to a single output. Moreover in one implementation of the mapreduce library if the reduce function s output expects key value pairs the keys written to this output must be the same as the keys passed in to the reduce function. In contrast in an implementation each MSCR output channel can write to zero one or several outputs with no constraints on keys. To implement these more flexible outputs the reduce function may write directly to the outputs bypassing the mapreduce library s normal output mechanism. If any of the MSCR s outputs satisfies the restrictions of a mapreduce library s output then that output can instead be implemented using the mapreduce library s normal mechanisms.

As each of the parallel operations is evaluated the executor populates the deferred objects with the appropriate data to materialize the objects until all operations are completed at which time the executor returns control back over to the application .

Referring to graph the ParallelDos M M and M are incorporated as MSCR input channels . Each of the GroupByKey operations becomes a grouping output channel . GBK s output channel incorporates the CV CombineValues operation and the R ParallelDo operation . The R ParallelDo operation is also fused into an output channel. An additional identity input channel is created for the input to GBK from non ParallelDo Op. Two additional pass through output channels shown as edges from mappers to outputs are created for the M. and M. PCollections that are used after the MSCR operation. The resulting MSCR operation has 4 input channels and 5 output channels .

While described as implemented as a library the functionality of the pipeline library may additionally or alternatively be implemented as a service that allows a client system to access the functionality over a network such as the Internet. For instance the functionality of the pipeline library can be implemented on a server system as a Web Service with a corresponding set of Web Service Application Programming Interfaces APIs . The Web Service APIs may be implemented for example as a Representational State Transfer REST based HTTP interface or a Simple Object Access Protocol SOAP based interface. Alternatively or additionally an interface such as a web page may be provided to access the service over the network.

Using the API or interface a user may send a program developed by the user to the service from a client system. The program for example may include a data parallel pipeline implemented using the parallel collection classes and parallel operations . Using the API or interface the user may designate data for the pipeline and send a message to the service to execute the program. The message optionally may include any arguments needed for the program. Once the message is received the service executes the program and implements the functionality of the evaluator the optimizer and the executor to implement the data parallel pipeline. The service then may return any outputs of the program to the client system. Alternatively or additionally the user program may execute on the client system with the program using the API to implement the data parallel pipeline using the functionality of the evaluator the optimizer and the executor implemented by the service.

As described more fully below in the implementation illustrated in the untrusted data parallel processing code is broken into two logical pieces and each piece is isolated to run inside of a secured processing environment a sandbox or jail . One piece is the executable code that defines the user s data parallel computation from which the dataflow graph is built and the other piece is the executable code that contains the functions that operate on the data.

For instance each piece of untrusted code may run in a hardware virtual machine VM that runs a guest operating system and emulates network and disk connections. The hardware VM may prevent the untrusted code from accessing the host or native environment directly and may provide communication outside the virtual machine only through specific audited mechanisms. The hardware VM may prevent the user s code from having access to the details of how data is stored and transmitted within the data center.

This architecture may allow the untrusted code to run on top of the data center infrastructure which provides the native processing environment. The file system and interconnection network used to store and move data within the data center may be able to run at full speed with the untrusted code for consuming the data running inside the security sandbox which may slow the untrusted code down. In some cases the bulk of the time spent performing data parallel computations is occupied by moving data. In this case the execution of the code may experience only a modest degradation in overall execution time compared to running directly in the native processing environment provided by the data center infrastructure. Placing only the untrusted code inside a security sandbox may be more efficient in some instances than placing the code and the implementation of the file system and network inside the sandbox. Placing only the untrusted code inside the sandbox may also make the overall system easier to secure since there s a limited channel for the untrusted code to communicate with the trusted host in contrast to the much wider communication channels that may be used to support the whole file system and network inside the sandbox.

The system includes a client system and a data center which may be similar to data center . The client system and the data center can communicate with one another over a network such as the Internet. The client system stores a user program or application that includes a data parallel pipeline implemented for example using the parallel data collection classes and one or more parallel operations described above which are supported by the pipeline processing service provided by the data center . As described further below the user application can be uploaded to and executed by the data center .

The data center includes processing modules and that are similar to processing module and include a variety of computer related components such as memory CPU s and network interface to perform data processing. The data center also includes an externally accessible storage .

Processing module s implement a service interface that provides the mechanisms for the client system to interact with the pipeline processing service. For instance the service interface may provide an API for interacting with the service. As described above such an API may be implemented for example as a Representational State Transfer REST based HTTP interface or a Simple Object Access Protocol SOAP based interface. Alternatively or additionally the service interface may provide a user interface such as a web page that can be displayed on the client system and used to access the service. The API or user interface can be used by a user of the client system to for example upload the user application to the data center designate data for example stored by storage for the user program and send a message to the service to execute the program with message optionally including any arguments needed for the program .

Processing module s implement an execution plan service and a virtual machine . The virtual machine may be a hardware virtual machine that runs a guest operating system and emulates network and disk connections. For example the virtual machine may be implemented by the Kernel Virtual Machine KVM technology which virtualizes Linux on an x86 architecture and may be run as a user process on the trusted host data center. KVM is described for example in the KVM Whitepaper available at http www.redhat.com f pdf rhev DOC KVM.pdf. In other implementations the virtual machine may be implemented as a process virtual machine. In general a process virtual machine runs as an application in an operating system supports a single process and provides a platform independent processing environment.

The virtual machine hosts and executes the uploaded user program and an execution plan library . When the user program is executed the execution plan library builds a dataflow graph based on the parallel data objects and parallel operations that form the pipeline in the user program . To that end the execution plan library implements an evaluator which similarly to evaluator constructs an internal execution plan dataflow graph of deferred objects and deferred operations corresponding to the data parallel pipeline. In addition to implementing the evaluator the execution plan library may implement other functions to communicate a representation of the dataflow graph to the execution plan service . For example in one implementation the execution plan library includes functions to communicate across the virtual machine boundary to the execution plan service using a remote procedure call RPC . These calls may be monitored to insure that appropriate calls are made. For instance the execution plan service may check the arguments to a given function for validity and check the function calls against a white list of function calls that are permitted. The auditing may be performed to detect for example requests probes to services or other data center resources e.g. files or other RPC based services that are not allowed to be accessed by the entrusted code.

In some implementations the virtual machine may be configured such that these RPC calls are the only way for code inside the virtual machine to interact with environments outside the virtual machine . The execution plan library also may implement so functions to receive information enabling materialization of the objects for example versions of the materialized objects themselves or representations of the materialized objects from the execution plan service once the dataflow graph has been optimized and executed. The execution plan library then may use this information to materialize the data objects in the internal dataflow graph so that those materialized objects may be used by the user program .

The execution plan service handles the optimization and execution of the data flow graph sent from the execution plan library . In one implementation the execution plan service is a trusted process that does not execute any untrusted user code and that has full access to the data center infrastructure. The execution plan service accepts the information representing an untrusted execution plan and validates the graph structure. The execution plan service then handles optimizations of the graph that is applies graph transforms as described above and execution of the optimized graph.

To handle the optimizations the execution plan service implements an optimizer . The optimizer like optimizer applies graph transformations such as those described above to generate a revised dataflow graph that includes the deferred parallel data objects or a subset and deferred combined parallel data operations which may include MSCRs. In one implementation the optimizer like optimizer performs a series of passes over the initial execution plan that reduces the number of overall operations and groups operations with the overall goal of producing the fewest MSCR operations.

To handle executing the revised data flow graph the execution plan service implements an executor . If the revised data flow graph includes operations in the dataflow graph that are not MSCRs for example PObject operate functions then the executor may communicate with a virtual machine such as virtual machine to implement the untrusted operations in a NM within the virtual machine.

For MSCRs the executor like executor translates a given MSCR in the graph into a single mapreduce operation. To do so for a given MSCR for example the executor generates a single map function that implements the multiple map operations in the input channels of the MSCR and a single reduce function that implements the multiple reduce operations in the output channels of the MSCR. The executor then executes the single mapreduce operation as a remote parallel operation. For instance similarly to the example described with reference to the executor may cause a number of map workers and reduce workers to be invoked with an associated index number that controls the data each receives and which of the map or reduce operations each performs on the data.

Processing module s implements a given one of the map or reduce workers one copy may be designated as a master which coordinates the various worker copies and an associated virtual machine . Generally in one implementation the untrusted user functions e.g. the map or reduce operations in the single map or reduce function are only executed in the virtual machine while the worker is a trusted process that does not execute any untrusted user code and has full access to the data center infrastructure. That is rather than each worker directly executing the code in the single map function or the single reduce function these functions which contain the untrusted user code are executed in the virtual machine . The trusted worker generally handles the interaction with the rest of infrastructure including data access including external data storage access and communication and coordination with masters and other workers.

Like virtual machine virtual machine may be a hardware virtual machine and may be implemented using KVM technology. In other implementations the virtual machine may be implemented as a process virtual machine. Other processing modules may implement the other parallel workers and associated virtual machines.

In addition to hosting and executing the user functions the virtual machine also hosts and executes an unbatcher batcher process which is used to handle input and output data for the user functions . In general the input and output data is passed between the worker and the virtual machine as batches of records. For instance rather than passing each record to the virtual machine for delivery to the user functions the worker accesses the input data for example stored in storage extracts batches of records from the input and sends the input batch of records to the unbatcher batcher in the virtual machine using for example an RPC. In one example the worker is configured to extract 64 MB of records and send those records to the unbatcher batcher .

The unbatcher batcher is configured to break the batch up into individual records invoke the user functions and pass the individual records to the user functions for processing. The unbatcher batcher is also configured to collect the output of the user functions into an output batch of records and communicate the output batch to the worker which can then arrange to have the results written to an output file or files that can be accessed by for example the execution plan service or other components of system . The execution plan service may be responsible for communicating the results back to the user program through the execute plan library . Passing the inputs and outputs between the worker and the virtual machine in batches may reduce the impact of the processing overhead needed to cross the virtual machine boundary for example the overhead needed to implement an RPC .

As with the RPCs between the execution plan service and the execution plan library the RPC calls between the worker and the unbatcher batcher may be monitored to ensure that appropriate calls are made. In some implementations the virtual machine may be configured such that these RPC calls are the only way for code inside the virtual machine to interact with environments outside the virtual machine .

While the implementation shown includes unbatcher batcher and the sending of records and results across the virtual machine boundary in batches other implementations may not perform batching to communicate across the virtual machine boundary.

The externally accessible storage may be configured as a hosted storage service that is accessible to the client system such that the client system can store data such as input data for the user program in storage and or retrieve output data from the user program . For instance the storage can be implemented as a Web Service with a corresponding set of Web Service APIs. The Web Service APIs may be implemented for example as a Representational State Transfer REST based HTTP interface or a Simple Object Access Protocol SOAP based interface. In a REST based interface a data object is accessed as a resource uniquely named using a URI and the client system and storage exchange representations of resource state using a defined set of operations. For example requested actions can be represented as verbs such as by HTTP GET PUT POST HEAD and DELETE verbs. While shown as part of data center the storage may be implemented by a separate data center or other facility.

In one implementation the results of the user program are stored in a file in the storage . The filename of the file may be designated by the execution plan service or specified in the user program . Once the user program has completed execution and the results are stored in the storage an indication of the successful execution may be sent back to the client system and may optionally include the filename for instance if the filename is generated by the execution plan service or otherwise not known to the client system . The client system then may use the filename and APIs to retrieve the results.

To begin the untrusted application is received at the data center . For example an untrusted user uploads the user program expressed as a Java .jar file to the service interface using for instance the provided API or interface. Also using the API or interface the user then instructs the service interface to start the user program running with some arguments controlling the run. For example the arguments may designate specific input files in the storage .

A first virtual machine is instantiated . For instance the service interface instantiates a slave hardware virtual machine for example using the KVM technology. The service interface populates the virtual machine s virtual local disk with the Java runtime including Java VM the user s jar file the user program the execution plan library jar file and any other standard .jar files and other data files used to implement the user program and the execution plan library .

The untrusted application is then executed in the first virtual machine . For instance the service interface may start up a Java VM and execute the user s untrusted program in the Java VM which is in the virtual machine . The user s untrusted program invokes the execution plan library such that the evaluator builds a dataflow graph in the memory of the virtual machine based on the parallel data objects and parallel operations that form the pipeline in the user program .

Information representing the data flow graph is communicated outside of the first virtual machine . For instance the execution plan library builds a representation of the data flow graph and communicates the representation to the execution plan service using an RPC call.

Outside of the first virtual machine one or more graph transformations are applied to the information representing the dataflow graph to generate a revised dataflow graph that includes one or more of the deferred parallel data objects and deferred combined parallel data operations . For instance the execution plan service accepts the representation of the data flow graph and validates the graph structure. The optimizer performs optimizations of the graph to generate a revised dataflow graph that includes the deferred parallel data objects or a subset and deferred combined parallel data operations which may include MSCRs.

The deferred combined parallel operations are then executed to produce materialized parallel data objects corresponding to the deferred parallel data objects . To that end for instance the executor translates the MSCRs in the graphs into a single mapreduce operation which includes a single map function that implements the multiple map operations and a single reduce function that implements the multiple reduce operations. The executor then executes the single mapreduce operation as a remote parallel operation which results in a number of parallel map workers and reduce workers being invoked and passed the single map or reduce functions as well as an indication of the appropriate input file s . One of the workers is designated as a master which coordinates the activity of the other workers.

One or more second virtual machines are then instantiated for the untrusted user functions. For example a given invoked worker instantiates a slave hardware virtual machine and populates the virtual machine s local disk with the Java runtime including Java VM a copy of the single map or reduce function the unbatcher batcher code and any other standard jar files and other data files needed to implement the user functions and unbatcher batcher . The worker starts up a Java VM in the virtual machine and runs the unbatcher batcher in the Java VM. The unbatcher batcher controls the invocation of the user functions in the Java VM.

Once the virtual machine is instantiated and the unbatcher batcher is running the worker accesses an input file from the storage . The worker extracts a batch of records from the input file and sends the input batch of records to the unbatcher batcher inside the VM using an RPC. The unbatcher batcher breaks up the input batch into individual records invokes the user s function to process each record in turn collects the output records into an output batch and finally communicates the output batch back to the worker in a reply to the RPC.

Once the worker receives the output batch the worker arranges to have the results written to an output file or files that can be accessed by for example the execution plan library or other components of system . Based on the output files the execution plan service generates information enabling materialization of the objects such as materialized versions of the objects or representations of the materialized objects. The execution plan service then communicates this information to the execution plan library . The execution plan library uses this information to materialize the data objects in the internal dataflow graph so that those materialized objects may be used by the user program .

In one implementation once the user program finishes running the outputs or results of the user program are communicated to the service interface . The service interface then communicates the outputs directly to the client system.

Alternatively or additionally the outputs or results may be stored in a file in the storage so as to be made accessible to the client system . In this case for example the filename of the file may be designated by the execution plan service and provided to the client system with an indication that the user program successfully executed. The client system then may use the filename to retrieve the results. In another example the user program may designate the filename and an indication of the successful execution of the user program may be sent to the client system without the filename. However because the filename was designated by the user program the client system may have access to the filename and be able to retrieve the results.

Using the virtual machines to implement the untrusted user code may provide a heterogeneous defense in depth strategy. To compromise the underlying data center nefarious untrusted code must penetrate several audited barriers. For instance when a hardware virtual machine is used the untrusted code must compromise the standard process model implemented by the guest operating system on the hardware virtual machine and gain root access in the guest. The untrusted code must then compromise the sandbox provided by the emulator of the virtual machine. In addition when a programming language that also employs a virtual machine for the runtime is employed the untrusted code must first compromise the innermost virtual machine.

While the architecture shown segregates the user program into a sandboxed area in the data center other processing environments with controlled access to the data center infrastructure may be used. For example in other implementations the user program may execute on client system . A library on the client system similar to the execution plan library may generate a data flow graph and communicate a representation of the data flow graph to the execution plan service either directly or through the service interface . The execution plan service can then validate the data flow graph create a revised graph and execute the revised graph as described above with the user functions still being executed in a virtual machine or other sandbox to prevent direct access by the user functions to the underlying data center infrastructure.

The techniques described above are not limited to any particular hardware or software configuration. Rather they may be implemented using hardware software or a combination of both. The methods and processes described may be implemented as computer programs that are executed on programmable computers comprising at least one processor and at least one data storage system. The programs may be implemented in a high level programming language and may also be implemented in assembly or other lower level languages if desired.

Any such program will typically be stored on a computer usable storage medium or device e.g. CD Rom RAM or magnetic disk . When read into the processor of the computer and executed the instructions of the program cause the programmable computer to carry out the various operations described above.

A number of implementations have been described. Nevertheless it will be understood that various modifications may be. Accordingly other implementations are within the scope of the following claims.

