---

title: Automated load balancing across the distributed system of hybrid storage and compute nodes
abstract: A distributed storage system that performs automated load balancing is described. In an exemplary embodiment, a storage controller server determines if there is duplicative data in a distributed storage system. In this embodiment, the storage controller server detects a load balancing event in the distributed storage system, where the distributed storage system includes a plurality of virtual nodes distributed across a plurality of physical nodes. In response to detecting the load balancing event, the storage controller server determines that a current virtual node is to move from a source physical node to a destination physical node. In addition, the current virtual node is one of the plurality of virtual nodes and the source and destination physical nodes are in the plurality of physical nodes. The storage controller server further moves the current virtual node from the source physical node to the destination physical node.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09378067&OS=09378067&RS=09378067
owner: Springpath, Inc.
number: 09378067
owner_city: Sunnyvale
owner_country: US
publication_date: 20140508
---
This invention relates generally to a storage system and more particularly to automated load balancing in a distributed storage system.

Enterprise storage systems currently available are proprietary storage appliances that integrate the storage controller functions and the storage media into the same physical unit. This centralized model makes it harder to independently scale the storage systems capacity performance and cost. Users can get tied to one expensive appliance without the flexibility of adapting it to different application requirements that may change over time. For small and medium scale enterprise this may require huge upfront capital cost. For larger enterprise datacenters new storage appliances are added as the storage capacity and performance requirements increase. These operate in silos and impose significant management overheads.

Due to non deterministic and non uniform nature of workloads running on a the enterprise storage system each storage node of a multi node system has a potential of being imbalanced in storage capacity or input output I O servicing capacity in comparison to other nodes in the cluster. This could result in some storage nodes becoming full before others and thereby leading to uneven usage of storage. Alternatively some compute nodes could bear the I O load more than other nodes.

A distributed storage system that performs automated load balancing is described. In an exemplary embodiment a storage controller server detects a load balancing event in the distributed storage system that services user input outputs IOs where the distributed storage system includes a plurality of virtual nodes distributed across a plurality of physical nodes. In response to detecting the load balancing event the storage controller server determines that a current virtual node is to move from a source physical node to a destination physical node without a disruption to the user IOs. In addition the current virtual node is one of the plurality of virtual nodes and the source and destination physical nodes are in the plurality of physical nodes. The storage controller server further moves the current virtual node from the source physical node to the destination physical node without disrupting the user IOs.

A distributed storage system called StorFS that performs automated load balancing is described. In the following description numerous specific details are set forth to provide thorough explanation of embodiments of the present invention. It will be apparent however to one skilled in the art that embodiments of the present invention may be practiced without these specific details. In other instances well known components structures and techniques have not been shown in detail in order not to obscure the understanding of this description.

Reference in the specification to one embodiment or an embodiment means that a particular feature structure or characteristic described in connection with the embodiment can be included in at least one embodiment of the invention. The appearances of the phrase in one embodiment in various places in the specification do not necessarily all refer to the same embodiment.

In the following description and claims the terms coupled and connected along with their derivatives may be used. It should be understood that these terms are not intended as synonyms for each other. Coupled is used to indicate that two or more elements which may or may not be in direct physical or electrical contact with each other co operate or interact with each other. Connected is used to indicate the establishment of communication between two or more elements that are coupled with each other.

The processes depicted in the figures that follow are performed by processing logic that comprises hardware e.g. circuitry dedicated logic etc. software such as is run on a general purpose computer system or a dedicated machine or a combination of both. Although the processes are described below in terms of some sequential operations it should be appreciated that some of the operations described may be performed in different order. Moreover some operations may be performed in parallel rather than sequentially.

The terms server client and device are intended to refer generally to data processing systems rather than specifically to a particular form factor for the server client and or device.

A distributed storage system called StorFS that performs automated load balancing is described. As described above due to non deterministic and non uniform nature of workloads running on a distributed file system each storage node has a potential of being imbalanced in storage capacity or input output I O servicing capacity in comparison to other nodes in the cluster. This could result in some storage nodes becoming full before others and thereby leading to uneven usage of storage or some compute nodes being accessed for I O more than others. In one embodiment the automated load balancing avoids storage and compute nodes from becoming imbalanced.

In this embodiment the physical storage associated with each node in the cluster is partitioned into logical storage vNodes. On a high level dynamic load balancing is achieved by dealing directly with virtual nodes vNodes that are distributed over the multiple physical nodes pNodes of the StorFS system. In one embodiment a pNode is a storage server that includes one or more storage medium e.g. a pNode is a storage node A C described below . In one embodiment the StorFS system aims at keeping the in use vNodes balanced so as to minimize the overuse of one or more pNodes and to increase the parallelization of the use of the pNodes. The Cluster Resource Manager Master CRM Master is responsible for monitoring cluster resources e.g. pNodes and their disks and dynamically invoking load balancer at appropriate times to allocate or rebalance vNodes. The StorFS system further does dynamic resource allocation as opposed to large static allocation over all the resources available. By employing dynamic resource allocation load balancing optimizes allocation of vNodes based on available resources at a given time rather than doing statically. In one embodiment the load balancing methods described herein do not require the distributed storage system to be taken offline or user I Os to be quiesced. As described below once a vNode requiring rebalance is identified a new mirror for that vNode is created in a new location e.g. a different disk and or pNode and one of the old minors is retired. During this process at least one of the mirror is online and therefore the cluster and the user I O can continue to make progress.

In one embodiment the design of the StorFS system distributes both the data and the metadata and this system does not require storing a complete global map for locating individual data blocks in our system. The responsibility of managing metadata is offloaded to each individual storage nodes A C. In one embodiment a cluster manager CRM resides on each SC Server maintains some global metadata which is small compared to the local metadata. In one embodiment each logical file or entity is partitioned into equal sized stripe units. The location of a stripe unit is determined based on a mathematical placement function Equation 1 Virtual Node Hash Entity Stripe Unit Total Vitual Nodes

In one embodiment the StorFS system receives the Entityand offset as input for each requested storage operation from an application A C. In this embodiment the StorFS system uses the offset to compute a stripe unit number Stripe Unit based on the stripe unit size Stripe Unit Size and the number of virtual nodes that the entity can be spread across Stripe Unit Per Stripe. Using the stripe unit number and the entity identifier Entity the StorFS system computes the virtual node identifier. As described below the StorFS system uses a hash function to compute the virtual node identifier. With the virtual node identifier the StorFS can identify which physical node the storage entity is associated with and can route the request to the corresponding SC server A C.

In one embodiment each vNode is a collection of either one or more data or metadata objects. In one embodiment the StorFS system does not store data and metadata in the same virtual node. This is because data and metadata may have different access patterns and quality of service QoS requirements. In one embodiment a vNode does not span across two devices e.g. a HDD . A single storage disk of a storage node A C may contain multiple vNodes. In one embodiment the placement function uses that a deterministic hashing function and that has good uniformity over the total number of virtual nodes. A hashing function as known in the art can be used e.g. Jenkins hash murmur hash etc. . In one embodiment the Stripe Unit Per Stripe attribute determines the number of total virtual nodes that an entity can be spread across. This enables distributing and parallelizing the workload across multiple storage nodes e.g. multiple SC servers A C . In one embodiment the StorFS system uses a two level indexing scheme that maps the logical address e.g. offset within a file or an object to a virtual block address VBA and from the VBAs to physical block address PBA . In one embodiment the VBAs are prefixed by the ID of the vNode in which they are stored. This vNode identifier ID is used by the SC client and other StorFS system components to route the I O to the correct cluster node. The physical location on the disk is determined based on the second index which is local to a physical node. In one embodiment a VBA is unique across the StorFS cluster where no two objects in the cluster will have the same VBA.

In one embodiment the cluster manager CRM maintains a database of virtual node vNode to physical node pNode mapping. In this embodiment each SC client and server caches the above mapping and computes the location of a particular data block using the above function in Equation 1 . In this embodiment the cluster manager need not be consulted for every I O. Instead the cluster manager is notified if there is any change in vNode to pNode mapping which may happen due to node disk failure load balancing etc. This allows the StorFS system to scale up and parallelize distribute the workload to many different storage nodes. In addition this provides a more deterministic routing behavior and quality of service. By distributing I Os across different storage nodes the workloads can take advantage of the caches in each of those nodes thereby providing higher combined performance. Even if the application migrates e.g. a virtual machine migrates in a virtualized environment the routing logic can fetch the data from the appropriate storage nodes. Since the placement is done at the stripe unit granularity access to data within a particular stripe unit goes to the same physical node. Access to two different stripe units may land in different physical nodes. The striping can be configured at different level e.g. file volume etc. Depending on the application settings the size of a stripe unit can range from a few megabytes to a few hundred megabytes. In one embodiment this can provide a good balance between fragmentation for sequential file access and load distribution.

In another embodiment the StorFS system uses the concept of virtual node vNode as the unit of data routing and management. In one embodiment there are four types of vNode 

In one embodiment the cluster resource manager CRM maintains a database of virtual node vNode to physical node pNode mapping. In this embodiment each SC client and server caches the above mapping and computes the location of a particular data block using the above function in Equation 1 . In this embodiment the cluster manager need not be consulted for every I O. Instead the cluster manager is notified if there is any change in vNode to pNode mapping which may happen due to node disk failure load balancing etc. This allows the StorFS system to scale up and parallelize distribute the workload to many different storage nodes. In addition this provides a more deterministic routing behavior and quality of service. By distributing I Os across different storage nodes the workloads can take advantage of the caches in each of those nodes thereby providing higher combined performance. Even if the application migrates e.g. a virtual machine migrates in a virtualized environment the routing logic can fetch the data from the appropriate storage nodes. Since the placement is done at the stripe unit granularity access to data within a particular stripe unit goes to the same physical node. Access to two different stripe units may land in different physical nodes. The striping can be configured at different level e.g. file volume etc. Depending on the application settings the size of a stripe unit can range from a few megabytes to a few hundred megabytes. In one embodiment this can provide a good balance between fragmentation for sequential file access and load distribution.

As described above the physical storage associated with each node in the cluster is partitioned into logical storage vNodes. On a high level dynamic load balancing is achieved by dealing directly with vNodes in the cluster by aiming at keeping the in use vNodes balanced at all times as much as possible. The Cluster Resource Manager Master CRM Master is responsible for monitoring cluster resources pNodes and their disks and invoking load balancer at appropriate times to allocate or rebalance these vNodes. However the StorFS system does dynamic resource allocation as opposed to large static allocation over all the resources available. By employing dynamic resource allocation load balancing optimizes allocation of vNodes based on available resources at a given time rather than doing statically. In one embodiment vNode allocation and rebalancing is handled at cluster creation and dynamically during the cluster operation.

At the time of cluster creation the StorFS system creates some static vNodes that store the metadata associated with the cluster and the data to be stored in the StorFS system. These static vNodes are created out of the resources available at the time of cluster creation once CRM establishes that the policy requirements are met after consulting with the policy engine. In one embodiment the policy engine is responsible for making decisions based on current system resources and the state of CRM Master whether auto load balancing is required at this time. The policy engine is described in below. In one embodiment the policy engine is responsible for making decision whether some actions are required and if so what action is required. In one embodiment the actions the policy engine may take are one of the following i static vNode allocation required ii cache vNode movement required iii data vNode movement required iv unsafe to allow cluster operations or v No action required.

At block process uses the policy engine to determine what action should be performed in response to one of the events detected at blocks and above. In one embodiment process uses the policy engine to perform resource addition resource deletion or processing a monitored timer. In one embodiment performing a resource addition is further described in below. In one embodiment performing a resource deletion is further described in below. In one embodiment monitoring a timer is further described in below. In one embodiment process can determine what various actions are required. In this embodiment actions that can be performed are static vNode allocation cache vNode movement and or data vNode movement. If a static vNode allocation is required execution proceeds to block below. If a cache vNode movement is required execution proceeds to block below. If a date of vNode movement is required execution proceeds to block below. If no action is required process stops at block .

At block process performs a static vNode allocation. In one embodiment a static vNode allocation is performed if a resource is added if policy constraints are met. The policy could be user specified. For example and in one embodiment a policy constraint is that there are N pNodes having at least 1 HDD and 1 SSD in the system. At block process performs a rebalance cache vNode. In one embodiment the rebalance cache vNode is performed when process determines that a cache vNode movement is required. A rebalance cache vNode is further described in below. At block process determines if more cache vNodes are to be moved. If more cache vNodes are to be moved process moves to vNodes at block . Execution proceeds to block above. If there are no more cache vNodes to be moved process performs a rebalance metadata with cache at block . In one embodiment this action moves the metadata vNodes along with the cache vNodes from one pNodes to another pNode. The rebalance metadata with cache is further described in below. At block process determines if there are more metadata vNodes to be moved. If there are more metadata vNodes to be moved execution proceeds to block above where the process moves those vNodes. If there are not any more metadata be notes to be moved process stops at block .

At block process performs a rebalance of data vNodes. In one embodiment a rebalance data vNodes is performed by process if one or more data vNodes need to be moved. The rebalance of data vNode is further described in below. At block process determines if there are more data vNodes to be moved. If there are more data vNodes to be moved process moves those data vNodes at block . Execution proceeds to block above. If there are no more data vNodes to be moved execution proceeds to block above.

In process receives a resource deletion request at block . At block process determines if there are vNodes that have degraded more than the user required compliance. For example and in one embodiment a vNode may degrade because each vNode can have replicas. For example and in one embodiment a vNode may have two or three wide configuration parameters for data redundancy to account for failures. These replicas are chosen such that they come from different pNode and disk on that pNode to keep the fault domains of replicas separate such that if one goes bad data could still be retrieved from other replica . A pNode Disk failure causes replicas to become unavailable and thus the vNode to become degraded. If there are not any of the vNodes that have degraded more than the user required compliance process stops at block . In one embodiment a user required compliance is the number of replicas that the user would want to be available at all times. Some users may require just one replica. Others may need more than one replica to be available at all times. If there is a vNode that is degraded more than the user required compliance process determines if these the vNodes can be moved to a different pNode to regain compliance at block . If these pNodes can be moved process performs a cache vNode movement at block . If the vNodes cannot be moved process determines that it is unsafe to allow cluster operations at block . In one embodiment process returns an error code indicating that cluster is unstable.

In process begins by monitoring the timers of block . In one embodiment process monitors timers that are used for statistics collection. In this embodiment process collects statistics regarding the StorFS system usage and uses these statistics to determine if there is an imbalance in the system that could trigger a rebalancing action. At block process finds the disk usage in the cluster. Process additionally sorts the usage of each disk into the array d sorted and sets the variable current equals zero. Process additionally sets d first equal to d sorted current and sets current equal to current 1. At block process determines if d first usage is greater than the mean. In one embodiment the mean is the ratio of the total disk usage to the total capacity. If d first usage is greater than the mean process determines that a data vNode movement is required at block . In one embodiment the required data vNode movement moves a data vNode from the disk d first usage to an underutilized disk. If d first usage is not greater than the mean process stops at block . In one embodiment disk first is the disk with minimum usage. In addition disk first usage is the value of that usage the disk usage value of the disk which is minimally used in the cluster . For example and in one embodiment if there are 3 disks in the system with usage as 10 GB occupied used for Disk1 15 GB occupied used for Disk2 18 G occupied used for Disk3. Then 10 GB 15 GB 18 GB are the disk usage for Disk1 Disk2 and Disk3 respectively. In this example disk first is Disk1 since its usage is least and disk first usage is 10 G. In addition the cluster mean or just mean in this case is 10 15 18 3 43 3 14.33 GB

In one embodiment the Master employs load balancing algorithms part of vNode Mapper engine at the time of cluster creation to evenly distribute the vNodes across the hosts and disks. In one embodiment the load balancing happens as a result of the Policy Engine coming out with an action of static vNode allocation required. The overall process is described in below. In one embodiment there are two types of static vNodes used metadata vNodes and cache vNodes. In this embodiment metadata vNodes are allocated from persistent class storage medium HDD SSD etc. . Cache vNodes caches the information in the metadata vNodes for faster access and also cache the associated user data temporarily until it is flushed to the persistent medium . Because of the caching nature of the cache vNodes cache vNodes are allocated from the caching class storage medium SSD Flash etc. . In one embodiment the load balancing algorithms are used at the time of cluster creation. The uniform distribution of cache vNodes allows for load balanced nodes in terms of compute needs and uniform distribution of metadata vNodes ensure load balanced nodes in terms of storage.

In one embodiment different techniques can be employed for distributing metadata vNodes as opposed to cache vNodes because of a difference in how persistent medium and caching medium disks are formatted. In one embodiment the load distribution algorithm for metadata vNodes tries to collocate e.g. are located on the same physical node the metadata vNodes with corresponding cache vNode for improved read performance and write throughput. If such vNodes cannot be collocated then they are allocated on a next best possible pNodes.

At block process finds for each metadata vNode V to be assigned the mirror set of its corresponding cache vNode V such that V . . . . In one embodiment M which is the mirror are for cache vNode V is . Process finds the disk D on Pwhich has the maximum number of empty segment stores at block . Assume the empty segment store number is S. Process assigns the M as . If no disks with empty segment stores are found on P M is set to an unassigned segment store. At block process determines is M is unassigned. If M is unassigned process stops at block .

If M is assigned process sorts through the pNodes in the system based on the weights of the pNodes. In one embodiment a pNode weight is defined to be a metric based on characteristics that order these pNodes in a priority order. A pNode with better weight will be higher in priority for example. In one embodiment the pNode weight is equal to the of empty segment stores on disk d i . At block process assigns M such that P is the pNode with the maximum weight having a disk D having an empty segment store S. If no such pNode and disk are found M is set as unassigned. At block process determines if M is unassigned. If M is unassigned at block process determines that the system is out of space. In one embodiment process returns an error code indicating that the system is out of space. If M is assigned at block process determines that the metadata of the vNode allocation is done. In this embodiment process returns an indication that the vNode metadata was allocated and that this process is completed.

As and when more data is written into the cluster the load balancing algorithm partitions additional space in the cluster for new data and uses this additional space to allocate new data vNodes. In one embodiment data vNodes are allocated in a persistent medium e.g. SSD HDD etc. . In this embodiment new data vNodes are allocated in such a way that the existing storage nodes are kept in balance in terms of their overall disk usage. is a flow diagram of one embodiment of a process to allocate data vNodes. In process receives a data vNode allocation request a block . In one embodiment process receives the data vNode allocation as the result of a cluster imbalance as described in above. At block process aggregates the persistent medium storages that are reposted in the system. In one embodiment each disk in the StorFS system reposts the partitions as a segment store. In this embodiment the mediums reporting such segment stores are used.

At block process finds for data vNode V to be assigned the mirror set of its corresponding metadata vNode V such that V . In one embodiment which is the mirror for metadata vNode V is . Process finds the disk D on Pwhich has the maximum capacity at block . In one embodiment the maximum capacity is measured in terms of disk usage. Assume the empty segment store number is S. Process assigns the M as . If no disks with empty segment stores are found on P M is set to an unassigned segment store. At block process determines is M is unassigned. If M is unassigned process stops at block .

If M is assigned process sorts through the pNodes in the system based on the weights of the pNodes. In one embodiment a pNode weight is a defined to be a metric based on characteristics that order these pNodes in a priority order. A pNode with better weight will be higher in priority for example. In one embodiment the pNode weight and is equal to the unused capacity on disk d i . At block process assigns M such that is the pNode with the maximum weight having a disk D with the maximum unused capacity and an unused segment store S. If no such pNode and disk are found M is set as unassigned. At block process determines if M is unassigned. If M is unassigned at block process determines that the system is out of space. In one embodiment process returns an error code indicating that the system is out of space. If M is assigned at block process determines that the data vNode allocation is done. In this embodiment process returns an indication that the vNode data was allocated and that this process is completed.

In the operation of the StorFS system the cluster storage usage can become imbalanced. For example and in one embodiment some pNodes may be over utilized whereas other pNodes are underutilized. In this situation if the use of these vNodes on the over utilized vNodes were to bring the cluster storage usage in imbalance e.g. as more and more data gets written in non deterministic manner an automated load balancing triggers again to bring the system back in balance. As another example and in another embodiment as and when new nodes with additional storage capacity are added to the cluster the cluster can be imbalanced because the new nodes may not be occupied as much as the existing ones. In this embodiment if capacity on newly added nodes were to be used for data vNodes this may keep the storage in the cluster balanced. For example and in one embodiment until the new nodes also become as occupied as the older ones the load balancing algorithm will keep picking the newly added nodes and their disks for new data vNode allocation. The cluster could still be imbalanced in terms of compute resources because the static vNodes e.g. cache vNodes that are allocated at the time of cluster creation were created off the old nodes. To be able to use the caching ability and compute of the newly added nodes the StorFS system would transfer some of the static vNodes from existing nodes to the newly added nodes. In this embodiment resource addition resource unavailability time monitoring or user request events can trigger the load balancing actions.

In one embodiment a resource addition after cluster formation can trigger a load balancing action. In this embodiment the load balancing that occurs as a result of this event aims at improving the cluster performance. Resource addition can be the addition of a new pNode to the cluster addition of additional storage medium to an existing pNode and or a combination thereof. When a new resource is added by load balancing static vNodes to the new node will help direct that I Os to the newly added node. Balancing the static vNodes can potentially increase the parallelization of the I Os for better performance. Moreover the caching medium is brought to its use as early as possible. This is performed by a load balancing action called RebalanceCacheVNodes and is further described in below. Once cache vNodes are rebalanced the corresponding metadata vNodes are migrated as to bring back the collocation and hence improve the performance. This is further described as RebalanceMetadataWithCache in below.

In another embodiment resource unavailability after cluster formation can trigger another load balancing action. For example and in one embodiment when a resource is unavailable for a pre determined time the resource is considered gone or lost which would affect the availability factor of the data that resides on that resource. Load balancing techniques are employed at this point to repurpose available storage on remaining nodes such that the replica of data on lost node could be recreated and improve the reliability of the system. This is performed by load balancing action described as RebalanceCacheVNodes in below.

In a further embodiment the StorFS system uses load balancing techniques to rebalance the cluster from an imbalance state when the system determines that a given node is being used in capacity much more than other nodes e.g. one of the disk becomes full . In this embodiment the StorFS system determines this condition by monitoring statistics of node in the system. In addition load balancing techniques can be used when there are certain number of vNodes that are in non compliant mode e.g. nodes are not satisfying service level guarantees like IOps or replication . This is performed by the load balancing actions described as RebalanceDataVNodes in below.

In another embodiment a user could directly trigger the load balancing techniques if the user decides to evacuate a given node or move one type of workload off a given node for any reason. In such cases the load balancing algorithm used will be a combination of above approaches.

In the above cases of load balancing during normal operation of cluster the load balancing is a two step process i a rebalancing step and a ii a movement step. In one embodiment the rebalancing step determines which vNodes are to be rebalanced from source pNodes to destination pNodes. In this embodiment this is an output from one of the above rebalance action with the vNode mapper. With the rebalancing mapped the movement step is carried out by sending a movement request to the current primary of a vNode. In one embodiment after the movement the primary of vNode may or may not be same as the current one. For example the possible outcomes of such a move for a given vNode is V i P primary P mirror1 P mirror2 P  primary P  mirror1 P  mirror2 where each of P primary P mirror1 and P mirror2 could be same or different than P  primary P  mirror1 P  mirror2 respectively. Which in turn results in partial or complete change of vNode and hence resulting in load balancing.

Process inserts a move into a move list for this vNode based on its old and new assignment in the move list at block . In one embodiment the move list is a set of vNode moves that are to be performed. Execution proceeds to block above. At block process processes the move list. In one embodiment process moves the cache vNode. A block process determines if the staggers are complete. If the stagger is not complete process continues on to block below. If the stagger is not complete execution proceeds to block above.

Process continues in at block in which process computes the number of moves required based on an old view and a new view. In one embodiment a view is equal to L C R where L is the load on the CAMS C equals the number of CAMS and R number of cache vNodes minus L C. In addition an old view is a view before the move are complete and the new view is the view after the moves are completed. In one embodiment process computes the number of used CAMS and the total number of CAMS. In one embodiment a used CAMS is a CAMS that is allocated and currently being used by the StorFS system. At block process computes a new CAMS for the cache vNodes. Process computes a load on the pNode at block . In one embodiment the load is the number of primary cache vNodes on this pNode. Process sorts the pNodes based on this load at block . At block process determines if the total number of moved cache vNode is less than vNode number of moves. If the total number of moved cache vNode is greater than or equal to the number of moves execution proceeds to block below. If the total number of move cache vNode is less than the number of moves at block process picks up a pNode from the sorted list in a round robin fashion. At block process picks a cache vNode on the primary pNode. Process picks a CAMS in a round robin fashion a block . At block process changes the write log group based on the cache vNode primary and the new CMS for this cache vNode. Execution proceeds to block above. At block process performs a rebalance metadata with cache. In one embodiment a rebalance metadata with cache is further described in below.

At block process determines if there are any minors that are collocated for this cache vNode. If there are no minors that are collocated process re maps the mirror to match the cache vNode at block . Execution proceeds to block above. If any of the minors are collocated process determines if any mirror of this vNode is collocated with the cache vNode primary at block . If so at block process determines if the collocated mirror is a primary. If the collocated minor is not a primary execution proceeds to block above. If the collocated minor is a primary process collocates one of the non collocated mirrors at block . Execution proceeds to block above. If none of the minors of the vNode are collocated with the cache vNode primary process collocates a vNode s primary with the cache vNode primary at block . Execution proceeds to block above.

In one embodiment the resource mod deletion module includes vNode degraded module move vNode module unsafe module and move cache vNode module . In one embodiment of vNode degraded module determines if there are vNodes which have degraded as described above in block . The move vNode module determines if the vNodes can be moved to a different pNode as described above in block . The unsafe module indicates to the cluster that is unsafe to allow operations as described above in block . The move cache vNode module moves to cache vNodes described above in block .

In one embodiment the monitoring timer module includes a disk usage module disk usage check module and move data vNode module . In one embodiment the disk usage module finds the disk usage in the cluster as described above in block . The disk usage check module determines if the D first usage is greater than the mean as described above in block . The move data vNode module moves the data vNode as described above and block .

As shown in the computer system which is a form of a data processing system includes a bus which is coupled to a microprocessor s and a ROM Read Only Memory and volatile RAM and a volatile memory . The microprocessor may retrieve the instructions from the memories and execute the instructions to perform operations described above. The bus interconnects these various components together and also interconnects these components and to a display controller and display device and to peripheral devices such as input output I O devices which may be mice keyboards modems network interfaces printers and other devices which are well known in the art. Typically the input output devices are coupled to the system through input output controllers . The volatile RAM Random Access Memory is typically implemented as dynamic RAM DRAM which requires power continually in order to refresh or maintain the data in the memory.

The mass storage is typically a magnetic hard drive or a magnetic optical drive or an optical drive or a DVD RAM or a flash memory or other types of memory systems which maintain data e.g. large amounts of data even after power is removed from the system. Typically the mass storage will also be a random access memory although this is not required. While shows that the mass storage is a local device coupled directly to the rest of the components in the data processing system it will be appreciated that the present invention may utilize a non volatile memory which is remote from the system such as a network storage device which is coupled to the data processing system through a network interface such as a modem an Ethernet interface or a wireless network. The bus may include one or more buses connected to each other through various bridges controllers and or adapters as is well known in the art.

Portions of what was described above may be implemented with logic circuitry such as a dedicated logic circuit or with a microcontroller or other form of processing core that executes program code instructions. Thus processes taught by the discussion above may be performed with program code such as machine executable instructions that cause a machine that executes these instructions to perform certain functions. In this context a machine may be a machine that converts intermediate form or abstract instructions into processor specific instructions e.g. an abstract execution environment such as a process virtual machine e.g. a Java Virtual Machine an interpreter a Common Language Runtime a high level language virtual machine etc. and or electronic circuitry disposed on a semiconductor chip e.g. logic circuitry implemented with transistors designed to execute instructions such as a general purpose processor and or a special purpose processor. Processes taught by the discussion above may also be performed by in the alternative to a machine or in combination with a machine electronic circuitry designed to perform the processes or a portion thereof without the execution of program code.

The present invention also relates to an apparatus for performing the operations described herein. This apparatus may be specially constructed for the required purpose or it may comprise a general purpose computer selectively activated or reconfigured by a computer program stored in the computer. Such a computer program may be stored in a computer readable storage medium such as but is not limited to any type of disk including floppy disks optical disks CD ROMs and magnetic optical disks read only memories ROMs RAMs EPROMs EEPROMs magnetic or optical cards or any type of media suitable for storing electronic instructions and each coupled to a computer system bus.

A machine readable medium includes any mechanism for storing or transmitting information in a form readable by a machine e.g. a computer . For example a machine readable medium includes read only memory ROM random access memory RAM magnetic disk storage media optical storage media flash memory devices etc.

An article of manufacture may be used to store program code. An article of manufacture that stores program code may be embodied as but is not limited to one or more memories e.g. one or more flash memories random access memories static dynamic or other optical disks CD ROMs DVD ROMs EPROMs EEPROMs magnetic or optical cards or other type of machine readable media suitable for storing electronic instructions. Program code may also be downloaded from a remote computer e.g. a server to a requesting computer e.g. a client by way of data signals embodied in a propagation medium e.g. via a communication link e.g. a network connection .

The preceding detailed descriptions are presented in terms of algorithms and symbolic representations of operations on data bits within a computer memory. These algorithmic descriptions and representations are the tools used by those skilled in the data processing arts to most effectively convey the substance of their work to others skilled in the art. An algorithm is here and generally conceived to be a self consistent sequence of operations leading to a desired result. The operations are those requiring physical manipulations of physical quantities. Usually though not necessarily these quantities take the form of electrical or magnetic signals capable of being stored transferred combined compared and otherwise manipulated. It has proven convenient at times principally for reasons of common usage to refer to these signals as bits values elements symbols characters terms numbers or the like.

It should be kept in mind however that all of these and similar terms are to be associated with the appropriate physical quantities and are merely convenient labels applied to these quantities. Unless specifically stated otherwise as apparent from the above discussion it is appreciated that throughout the description discussions utilizing terms such as receiving determining moving computing detecting performing reading writing transferring updating collocating or the like refer to the action and processes of a computer system or similar electronic computing device that manipulates and transforms data represented as physical electronic quantities within the computer system s registers and memories into other data similarly represented as physical quantities within the computer system memories or registers or other such information storage transmission or display devices.

The processes and displays presented herein are not inherently related to any particular computer or other apparatus. Various general purpose systems may be used with programs in accordance with the teachings herein or it may prove convenient to construct a more specialized apparatus to perform the operations described. The required structure for a variety of these systems will be evident from the description below. In addition the present invention is not described with reference to any particular programming language. It will be appreciated that a variety of programming languages may be used to implement the teachings of the invention as described herein.

The foregoing discussion merely describes some exemplary embodiments of the present invention. One skilled in the art will readily recognize from such discussion the accompanying drawings and the claims that various modifications can be made without departing from the spirit and scope of the invention.

