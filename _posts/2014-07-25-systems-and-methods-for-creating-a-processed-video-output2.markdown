---

title: Systems and methods for creating a processed video output
abstract: Certain embodiments provide systems and method for creating a processed video stream. A method for creating a processed video stream includes receiving project instructions and media at a computing device executing a socially interactive application. The project instructions can include a project type. The method also includes uploading the project instructions and media to a server connected to the computing device. The method includes creating a project script based on the project type at a processor of the server. The method includes encoding, at the processor, a video stream based on the project script. The method also includes sharing the video stream at the socially interactive application.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09620169&OS=09620169&RS=09620169
owner: DREAMTEK, INC.
number: 09620169
owner_city: San Francisco
owner_country: US
publication_date: 20140725
---
The present application claims priority under 35 U.S.C. 119 e to provisional application Ser. No. 61 859 063 filed on Jul. 26 2013. The above referenced provisional application is hereby incorporated herein by reference in its entirety.

The present invention relates to systems and methods supporting access to an application for users to input process and share media. More specifically certain embodiments of the present invention relate to systems and methods that provide a socially interactive application for inputting processing compiling and sharing video streams comprising images video clips and or audio clips.

Phones cameras and video cameras are commonly used to capture memories. Typically when returning from a major event like a vacation or birthday party photographs may be shared as an online album with friends and family or uploaded to a social media site like Facebook. A problem with standalone photographs of an event is that the story associated with the images may not be apparent from the photograph stills alone. In order to tell a story video editing software can be used to bring pictures videos and music to life. Existing consumer video editing software is expensive and complicated to use because it comes from roots in professional video editing software. For example existing video editing software commonly uses separate parallel lanes also called tracked or swim lanes for video and image clips subtitles audio tracks transitions between clips and the like. Although the separate parallel lanes enable professional video editors to have complete control of the video editing process even the most computer savvy consumers can find the process of editing video to be complicated tedious and time consuming. As such even after a user successfully creates an edited video stream using existing video editing software the user may put off and or be reluctant to create edited video streams in the future due to the time and energy needed to create edited video streams using existing video editing software.

Some existing video editing software has attempted to simplify the video editing process. However these simplified existing video editing software solutions do not provide users with control over the appearance of the output. For example typical simplified video editing software merely allows for user selection of a project template that is used to automatically create the video stream. Users of typical simplified video editing software may not be able to control the order pictures or videos are displayed and what parts of videos are used. Further existing simplified video editing software does not allow for audio ducking and does not provide tools for filtering video audio or colors.

Further limitations and disadvantages of conventional and traditional approaches will become apparent to one of skill in the art through comparison of such systems with some aspects of the present invention as set forth in the remainder of the present application.

A system and or method that provides a socially interactive application for inputting processing compiling and sharing video streams comprising images video clips and or audio clips substantially as shown in and or described in connection with at least one of the figures as set forth more completely in the claims.

These and other advantages aspects and novel features of the present invention as well as details of an illustrated embodiment thereof will be more fully understood from the following description and drawings.

The foregoing summary as well as the following detailed description of embodiments of the present invention will be better understood when read in conjunction with the appended drawings. For the purpose of illustrating the invention certain embodiments are shown in the drawings. It should be understood however that the present invention is not limited to the arrangements and instrumentality shown in the attached drawings.

Aspects of the present invention are related to systems and methods supporting access to an application for users to input process and share media. More specifically certain embodiments of the present invention relate to systems and methods that provide a socially interactive application for inputting processing compiling and sharing video streams comprising images video clips and or audio clips.

A representative embodiment of the present invention aids users of internet connected desktop laptop and or mobile devices with creating and sharing video streams.

As utilized herein the terms exemplary or example means serving as a non limiting example instance or illustration. As utilized herein the term e.g. introduces a list of one or more non limiting examples instances or illustrations

The user input device s may include any device s capable of communicating information from a user and or at the direction of the user to the processor of the computing device for example. The user input device s may include button s a touchscreen motion tracking voice recognition a mousing device keyboard camera and or any other device capable of receiving a user directive. In certain embodiments one or more of the user input devices may be integrated into other components such as the display for example.

The display may be any device capable of communicating visual information to a user. For example a display may include a liquid crystal display a light emitting diode display and or any suitable display. The display can be operable to display information from a video editing application or any suitable information. In various embodiments the display may display information provided by the processor for example.

The processor may be one or more central processing units microprocessors microcontrollers and or the like. The processor may be an integrated component or may be distributed across various locations for example. The processor may be capable of executing software applications receiving input information from user input device s and generating an output displayable by a display among other things. The processor may be capable of executing any of the method s and or set s of instructions discussed below in accordance with the present invention for example. In certain embodiments the processor may execute one or more video editing applications available at server s and or stored at the computing device s in response to user inputs received from user input device s for example.

In various embodiments the information provided by the user input device s to the processor may be processed by the processor to control one or more applications for media uploading media processing media compiling and video sharing for example. As an example button depressions touchscreen selections mousing inputs keyboard inputs and or voice commands among other things may be received from the user input device s and processed by the processor to input media process media compile media and or share video streams for example.

The memory may be one or more computer readable memories for example such as a hard disk floppy disk CD CD ROM DVD compact storage flash memory random access memory read only memory electrically erasable and programmable read only memory and or any suitable memory. The memory may include databases libraries sets of information or other storage accessed by and or incorporated with the processor for example. The memory may be able to store data temporarily or permanently for example. The memory may be capable of storing data generated by the processor and or instructions readable by the processor among other things. In various embodiments the memory stores one or more software applications.

The method for specifying the video editing project may be a Windows application Mac application web based application mobile device e.g. iOS Android or any suitable application for arranging and uploading pictures videos and music. The method of illustrates how the end user may organize media clips add clip options like subtitles or trimming clips and add main project options like the main event title and filters such as video and audio filters for example. In various embodiments the end user may use computing device to upload the media to server s which may include cloud storage for example.

At step the method of is started. At step a user may select a project type such as beat sequence flip swivel or slide sequence image stack or any suitable project type. In various embodiments the project type selection may drive what a user can add or select during the process.

At step a user may add and arrange media and set clip options such as by adding titles or trimming video clips. For example a user can add photos videos music and or the like. In various embodiments media may be added by dragging and dropping by clicking the button and using a standard file dialog to find the media or using any suitable selection mechanism. In certain embodiments a user can arrange the media as desired with a user input device such as a mousing device or by selecting the item and using arrow buttons displayed on the display for example. Video clips can be trimmed removing the start and or end of a video clip to shorten it . If a video clip is present a trim link may be displayed. When selected the trim link can display a trim popup that displays the video clip and start and end sliders that can be color coded e.g. green and red so the user can easily distinguish between what is being kept and what is being trimmed.

At step a user sets the event title and selects or sets an event date. The user may optionally select other projection options such as adding video audio and or color filters among other things. For example the user may select a video filter such as old film makes it look like 60 70 s video among other things. The user can optionally select a color filter color grading such as Super8 sepia new film or any suitable color filter. The color grading filters change vibrancy up or down add grain sharpen and even slow down frame rate for example. The user may optionally select an audio filter such as vinyl noise or projector for example. The user can optionally select convert to black and white which will make the entire output grayscale black and white. Combining a black and white color filter with something like an old film video filter may take it from looking like 60 70 s content to looking like 1920 s content for example.

At step a user may select to upload the project to the server . In response to the upload selection project instructions are created including a user authentication token clip metadata clip options like title trim information and overall project information like event title event date selected filters and the like. The project instructions can be sent to server such as via REST request and parameters e.g. cloud storage parameters may be returned to the client. In various embodiments the server checks to see if the user has already uploaded any of the files and returns only the remaining files needed to complete the upload.

At step the media files related to the project can be uploaded to cloud storage or any suitable storage for example. At step after the project instructions and media are uploaded at steps and the project is queued for processing. At step the method of ends.

Referring to the method for performing the video editing project is the high level process of service methods that take projects from the queue set up the projects for processing and call the main video engine as described below in connection with for example. In various embodiments the method for performing the video editing project is responsible for project setup calling the video engine which compiles the media based on the project type and uploaded instructions creating the final video output creating metadata about projects creating compilation entity data after the video engine completes and creating thumbnails among other things for example.

The method begins at step . At step a next available ready for processing project is picked up from the queue and its status is set to locking then processing for example. At step the media clips are downloaded from storage such as cloud storage to processing server s or network storage. In certain embodiments if a user had selected files that were already uploaded and the files were skipped during the upload process so the file is stored once and the upload process speed is increased these files may still be download during this step. At step the processing server determines whether clips should be re ordered and or re numbered based on the project type e.g. a beat sequence project type . At step if the processing server determines that the clips should be re ordered and or re numbered the re ordering re numbering operation is performed.

At step project scripts may be setup based on the project type. For example a main encoding script can be created based on the project type as described below in connection with main script and exemplary project type scripts . At step the processing server determines whether the project has a new album new files that have not been uploaded before . Albums are created for new media files uploaded. In many cases files will be new but there may be a few media files that the user already uploaded that would be a part of an old album for example. Typically there will be a new album because at least some files will be new. As discussed in more detail below at step thumbnails are created for new clips when a new album is being created for example.

At step the processing server creates large and normal sized thumbnails of the original picture and video files if the project has a new album. In various embodiments the created thumbnails may be used for a picture viewer and for album selection and album content detail in a cloud mixer. For example the thumbnails may be used so a user can later select and work with their uploaded media. As another example large thumbnails can be used so that the output shared by a user may be viewed as a picture slide show with the picture viewer.

At step the video engine begins encoding the media based on the created project script as illustrated in and discussed in more detail below. At step the final video output that may be uploaded to for example cloud storage and viewed by end users is created. After encoding is complete at step standard sized thumbnails may be created for the main video thumbnails for example and small and spliced small thumbnails can be created for main wall image rain for example at step . After successful project completion a compilation entity is created and persisted in a data repository at step . The compilation entity may comprise for example a compilation title date thumbnail URLs video URLs user full name and other metadata about the compilation.

At step project and compilation output metadata such as file total bytes total processing time and or any suitable stats are stored with the project and compilations entities and persisted. In various embodiments the compilation entity can then be added to a user timeline and if public to a main video wall of the socially interactive application. The method ends at step .

The method begins at step . At step a script header is generated. The script header comprises a preset which may be the theme font and or caching and multi threading core count instructions for example. These settings can be used by subsequent video engine function calls for example as described in detail below.

At step a main script body is generated based on a project type. Examples of project types can include a multi clip project as described in single clip project as described in image stack project as described in and the like.

At step a script footer is generated. Script footers may comprise references to video filters e.g. color filters grading e.g. audio filters e.g. and setting to black and white among other things. If music has been added the music may be mixed with the compilation ducking with video audio as well as fading in and out e.g. among other things. Below are examples of filters backgrounds and music for example 

A grayscale function call may be added to the script footer if a black and white filter was selected.

A color filter function call as illustrated in and described below may be added to the script footer if a color filter such as Super 8 Technicolor etc. was selected.

A video filter function call as illustrated in and described below may be added to the script footer if a video filter such as old film among other things was selected.

An audio mixer function call as illustrated in and described below may be added to the script footer if music was added to the project and the project type does not have special audio settings such as the beat sequence or silent movie project types described below. For example the audio mixer may mix the music with the video output ducking when video audio is present repeating music clips to match video output length or trimming music audio to match video output length. The audio mixer can also fade music audio in and out at the beginning and end of video output between different music clips and before and after audio ducking when video audio is present for example. The audio mixer may seamlessly mix music audio with video output. In various embodiments music is handled differently for specific projects types such as beat sequence and silent movie project types for example.

An audio filter function call as illustrated in and described below may be added to the script footer if an audio filter like vinyl noise or projector was selected.

The method begins at step . At step an audio filename input line may be set. At step a title pause list may be generated. For example an image title can be matched with image filenames that may be specifically generated for the beat sequence project and a numeric pause list may be generated based on an image filename number along with the title. In various embodiments although clips usually move quickly to the tempo of the music titles are paused so the clip title can be read for example. When a clip is paused beat effects that move to the music are maintained for the clip. A next clip is transitioned to once the pause duration has been met for example. In various embodiments the pause duration such as 4 seconds is configurable.

At step a main beat sequence function call may be generated as illustrated in and described below. At step a function call for a beat tap effect may be added. The beat tap effect can be based on an audio track for example. The beat tap effect symbolizes the beat of the music making the clip look like it has been tapped or hit with a drum stick to symbolize the beat for example. In various embodiments the clip may be indented revealing a border around it and then snap back.

At step a function call for a beat zoom effect may be added. The beat zoom effect can be based on an audio track for example. The beat zoom effect can zoom into or out of an image to the beat of the music. In contrast to the beat tap effect the clip may not be indented to reveal the border. Instead the beat zoom effect may zoom into and out of the clip for example. At step a function call for a beat rotate effect may be added. The beat rotate effect can be based on an audio track for example. The beat rotate effect rotates the clip left or right to the beat of the music. At step an audio dub function call may be added. The audio dub function can dub music over a compilation for example. The method ends at step .

The method for generating video engine project instructions for an image stack video editing project begins at step . At step a title line e.g. an event title and date can be generated for later use in a main function call. The title may be placed on a blank stack card for example such that the project event title is placed at the beginning of the output video. At step the processing server may loop through images and video clips generating image stack function calls for every n clips image or video file where n is a configurable number such as 8 for example. The processing server can add titles which may be drawn on the bottom of a stack card in marker font and video trims if they exist for example. The method ends at step .

Referring to the method for generating video engine project instructions for a multi clip sequence video editing project may be used for projects with multi clip screens such as a slide sequence as illustrated in and described below a flip sequence as illustrated in and described below and a swivel sequence as illustrated in and described below for example. The multi clip sequence video editing project types can have screens that have multiple e.g. 2 5 image and or video clips on the screen at once as well as single clip screens for images with titles or videos for example. In various embodiments images with titles or videos may be a part of a multi clip screen but can be displayed after the multi clip screen on a single screen so the title can be displayed or the video can be played in its entirety for example.

The method begins at step . At step an array of flip swivel multi clip source lists or slide multi clip source lists may be initialized. In various embodiments the multi clip source lists have how many clips are in the multi clip screen such as 3 4 or 5 for example and what position each of the clips is in on the screen. The arrays may then be shuffled so that when subsequent multi clip screens are taken the screens are randomized so that even if the same content is re processed the output is different. For example the order of the users clips may not change from a per screen perspective but the selection of how many clips are on a multi clip screen and where the clips are placed on the screen is random such that even if the same clips are used each output is different.

At step a random main title screen type may be selected. For example a next random multi clip source list shuffled in above at step can be selected based on a remaining clip count. If there are no remaining multi clip source lists the array can be re initialized shuffled and one of the array can be selected. At step a title screen script line is generated. The title screen script line can be an event title and date line generated with two clips images or videos for example. In various embodiments if either of the two clips for example has subtitles the subtitles are not displayed on this main title screen but are displayed at step as described below.

At step a processing server can determine whether the clips in the main title screen have an image with a subtitle or a video. At step if the main title screen has images with subtitles or videos in the title a single screen for each image with a subtitle and video is generated which allows an important clip to be seen in its entirety on a single screen instead of on a multi clip screen for example. In various embodiments the generation of the single clip screens may include generating trim line if the video was trimmed.

At steps a loop of the remaining images and video clips is performed. At step a multi clip source list is retrieved based on remaining clip count. If there are no remaining multi clip source lists the multi clip source lists array is re initialize and the next random multi clip source list is retrieved. At step a multi clip screen without clip subtitles is generated. The subtitles are generated when the clip is displayed on a single screen as described below at step . At step a processing server may determine whether the generated multi clip screen has images with subtitles or videos in the title. At step if the screen has images with subtitles or videos in the title a single screen for each image with a subtitle and video is generated which allows an important clip to be seen in its entirety on a single screen instead of on a multi clip screen for example. This may also include generating trim line if the video was trimmed. At step the loop is repeated for any remaining images or videos.

At step if there are remaining clips that cannot make up the next multi clip screen e.g. if at least three clips are needed but there are only two clips left single clip lines can be generated to display the last clips. In various embodiments titles subtitles and trim lines may be added as discussed above for example. At step a main function call e.g. a slide sequence as illustrated in and described below a flip sequence as illustrated in and described below or a swivel sequence as illustrated in and described below with the above main title multi clip and single screen lines is generated. The method ends at step .

Referring to a method for generating video engine project instructions for a single clip sequence video editing project may be used for projects with single clip screens such as a focus and silent movie projects for example. In various embodiments the single clip screens can have a title subtitle and the videos can be trimmed. The method begins at step . At step a title screen script line that may include an event title and date for example is generated for later use.

At steps a loop of the images and video clips is performed. At step a single clip screen is generated. The single clip screen may comprise a subtitle if the clip included a title. The single clip screen can comprise a video trim line if the clip is a video that was trimmed. At step the loop is repeated for any remaining images or videos.

At step a main function call is generated with the above main title and single screen lines. For example a main function call for a focus project silent movie project or any suitable project can be generated. The method ends at step .

The method for encoding video begins at step . At step a processing server calls an encoding tool such as x264.exe with a project script. The encoding tool generates an output file which can be a MPEG4 output file for example. The execution of the project script may initiate the functions and the video engine calls for example. At step the processing server determines whether the project has music audio or video audio. At step audio in uncompressed WAV format is extracted using a tool such as WAVI.exe for example. At step the WAV file is encoded to M4A using a tool such as NeroAcenc.exe for example so that internet based media players can decode the audio. At step the M4A and MP4 from step above are multiplexed combined using a tool such as MP4Box for example to create the final output MP4 that can be viewed at the socially interactive application or any suitable application service. The video encoding method ends at step . The final video output that end users consume has been created.

Referring to the method for loading and conforming images and video clips opens images and video clips that can be in a variety of different formats and allows for simplified handling of source clips. The method allows the processing server to treat media as a generic clip instead of having to account for specific media format. The method for loading and conforming images and video clips begins at step . At step defaults are loaded according to a selected preset. In various embodiments defaults can be overridden by options selected by a user or administrator for example. At step an input source string is parsed and a file type is determined by a file extension.

At step a file loader is chosen based on the file type. In various embodiments various types of media are handled automatically based on an assumed file type. At step digital still image formats are loaded. At step AVI audio video files may be opened with FFMpegSource or any suitable tool. For example if unable to open the AVI audio video file using FFMpegSource DirectShowSource filter or any suitable mechanism may be attempted. If AVI audio video file cannot be opened an error is returned. At step transport streams such as MPEG2 and H.264 transport streams are opened using MpegAutoIndexSource or any suitable mechanism. At step Quicktime MOV files are loaded by QtSource or any suitable mechanism.

At step by default video is conformed to consistent dimensions at step depending on the selected preset. In various embodiments defaults can be overridden by options such that video dimension conforming is skipped and the method proceeds to step . If video dimension conforming is not skipped video is resized to a target width and height. For example the video may be conformed to pixel dimensions of either 1280 720 or 640 480 for example depending on the selected preset at step . By default video is letterboxed or pillar boxed as needed to fill the target width and height. The background color may be determined by preset. Optionally the video can be cropped to fit the target frame size instead.

At step by default frame rate conforming is performed at step . In various embodiments defaults can be overridden by options such that frame rate conforming is skipped and the method proceeds to step . If frame rate conforming is not skipped the video frame rate is conformed to the NTSC standard at step . More specifically the video stream is conformed to 30000 1001 29.97 frames per second . In certain embodiments a high quality frame rate conversion option can be chosen to improve smoothness at expense of processing time and some risk of artifacts. If frame rate conforming is skipped at step the method proceeds to step where video is converted to YUV colorspace with YV12 Chroma subsampling for example. After the video is converted at step signal limiting may be applied to the video stream at step . For example a video luminance range may be limited to 16 235 and chrominance can be limited to 16 240 for CCIR 601 compliance.

At step options can be set for custom audio sample rate and word length. At step by default audio is converted to 48 kHz 16 bit. In various embodiments defaults can be overridden by options. For example at step audio may be converted to a specific sample rate and word length specified by a user or system administrator. After converting the audio at step or multichannel audio is mixed to stereo and monaural signals are replicated to 2 channel. Blank audio matching the default or custom audio settings can be inserted on media that lack an audio stream. The method for loading and conforming images and video clips ends at step .

The method for queued caching to generate pre processed video clips is one by which cache enabled custom functions can replicate the parameters needed to create their output. The cache enabled custom functions are written to .avs scripts. The cache .avs scripts can then be queued for multi threaded rendering as illustrated in and as described below using a variety of options including interleaved and segmented processing. The rendered output is encoded to files referred to herein as cache media files. The method provides some unique advantages. For example method allows the output of multiple functions to be processed concurrently in a multithreaded environment. As another example the method allows an output of a single function to be multithreaded by assigning portions of the output to different threads when interleaved or segmented processing is used. As another example once the output of function is cached to disk the cache media files can be loaded by the system and used in place of the output of the functions to avoid reprocessing which saves time and computer processing unit CPU resources.

In various embodiments interleaved processing generates several scripts for a given sequence. Each script may process every Nth frame of the output. For example if 4 threads are used for interleaved processing of a sequence 4 scripts can be generated. More specifically a first script may process frames 0 4 8 etc. A second script may process frames 1 5 9 etc. A third script may process frames 2 6 10 etc. A fourth script may process frames 3 7 11 etc. Once the cache media files for these scripts are rendered and encoded the files can be loaded and recombined by a process known as interleaving as described below with regard to for example.

In certain embodiments segmented processing may generate several scripts for a given sequence. Each script may process a range of frames from the output of the function. For example if the output of a given source file and function is 300 frames and 4 threads are used for segmented processing 4 scripts may be generated. More specifically a first script may process frames 0 74. A second script may process frames 75 149. A third script may process frames 150 224. A fourth script may process frames 225 299. Once the cache media files for these scripts are rendered and encoded they can be loaded and recombined as described below with regard to for example.

At step a list of sources is received as an input. In various embodiments the list of sources may be paths to media files or plain text used for specifying on screen titling text for example. At step each item in the list of sources is processed in a loop for example. At step as each source is processed a global variable is incremented so that a unique index can be used to identify and reference each cached media file. At step a cached enabled function is invoked which generates an .avs script via the method for generating a script illustrated in and described below. Cache enabled functions can replicate the parameters used to create the function output. The cache enabled functions may be written to .avs scripts so that the functions can be rendered by other threads for example. At step a cache identifier associated with the source being processed may be added to an index of a queue identification list. The queue identification list may be sent to the rendering method A illustrated in and described below or can be preserved for future rendering for example. At step the processing server determines whether each item in the list of sources has been processed. If items remain for processing the method returns to step . Otherwise the method proceeds to step .

At step the processing server determines whether rendering and encoding is being deferred in which case the queue identification list is returned at step . If rendering and encoding are not being deferred the method proceeds to step where the cache media files are rendered and encoded by method A illustrated in and described below for example. At step if there is only a single source in the input sources list the encoded cache media file can be read and returned as a clip and the method proceeds to step . If multiple sources are used however the queue identification list may be returned instead at step . Other functions and processes can load pre rendered output from the output of method for example. At step rendered cache media files are loaded via the method illustrated in and described below for example. If there is only a single source in the input sources list the encoded cache media file can be read and returned as a clip. The method ends at step where either the queue identification list or the clip is returned.

The method for generating a script is called by custom cache enabled functions. The method replicates the parameters and logic required to create the output of the parent function and writes these to a script so that the script can be loaded rendered and encoded by an external process and or at a later time as described below. At step the method for generating a script is initiated when an input is received that comprises a list of parameter names and values corresponding to those of the parent function. The global cache identifier variable that uniquely identifies the script and its output is also input. At step the processing server determines whether an .avs script for the cache identifier index exists. The cache identifier index value is resolved to an actual file path pointing to the target .avs script. If the script does not exist at step it is generated. At step the processing server determines whether interleaved processing has been specified. At step if interleaving has been specified a separate script is generated for each interleaving step. If interleaving has not been specified the method proceeds to step where the processing server determines whether segment processing has been specified. At step if segment processing has been specified a separate script is generated for each segment. At step if neither interleaved nor segmented processing is used a single .avs script is generated. The method ends at step .

The method for interfacing with a rendering and encoding engine allows a list of indexes representing the cache identifiers of previously generated scripts for example to be passed to a rendering and encoding engine so that the scripts may be processed and stored as cache media files. At step the method for interfacing with a rendering and encoding engine takes as input a list of indexes representing the cache .avs scripts and the respective outputs cache media files . At step the processing server checks whether output for all the indexes has already been completed. If so the method terminates. At step the processing server sends the list of indexes to the rendering an encoding engine. At step cache media files are indexed using FFIndex or any suitable indexing mechanism. The method ends at step

The method for rendering and encoding scripts takes a list of cache indexes and assigns rendering of the .avs script that corresponds with each index to a thread. In various embodiments a maximum number of concurrent threads can be set. The output of each thread is a cache media file. At step the method for rendering and encoding scripts takes as input a list of indexes representing the cache .avs scripts and respective outputs cache media files . At step each index from the input list is processed in a loop. At step a file path referencing an .avs script is determined by the index. At step the .avs script file path determined at step is added to a queue. At step the processing server determines whether each script file path from the input list has been processed. If .avs script files remain for processing the method returns to step . Otherwise the method proceeds to step

At step each script from the queue is processed in a loop. At step the next script from the scripts queue is retrieved. At step the processing server determines whether any threads are available for processing. If all threads are busy assigned the processing server waits for the next available thread at step . At step a cache media file is generated for each script if the cache media file does not already exist. In various embodiments special encoding options can be included in the script. At step the processing server determines whether all queued scripts have been processed. If scripts remain for processing in the queue the method returns to step . Otherwise the method ends at step

At step the method for generating a single video clip from one or more cache media files takes as input an index referring to a specific cache media file. At step the processing server determines whether the specified index refers to a series of interleaved cache media files. If so the method proceeds to step where the cache media files are loaded and interleaved such that a clip is returned by the method illustrated in and described below. If the specified index does not refer to a series of interleaved cache media files the method proceeds to step where the processing server determines whether the specified index refers to a series of segmented cache media files. If so the method proceeds to step where the cache media files are loaded and concatenated such that a clip is returned by the method illustrated in and described below. If the specified index does not refer to a series of segmented cache media files the method proceeds to step where the cache media files are loaded and a clip is returned by the method illustrated in and described below. The method for generating a single video clip ends at step

At step the method for interleaving a series of cache media files to generate a single video clip takes as input an integer index referring to a specific cache media file. In various embodiments a number of interleaving steps may also be specified. Based on the input remaining indexes may be determined. For example an index value of 24 with 8 steps may use indexes 24 25 26 27 28 29 30 31 . At step each of the indexes is processed in a loop. At step a cache media file associated with each index is loaded and a clip is returned by the method illustrated in and described below. At step the processing server determines whether all indexes have been processed. If indexes remain for processing the method returns to step . Otherwise the method proceeds to step where the clips derived from the cache media files are interleaved to recreate the original output of a custom cache enabled function.

At step the processing server determines whether a companion audio file exists for the interleaved cache media files. If this audio file exists the method proceeds to step where the audio file is loaded and returned as an audio clip. At step the audio clip is dubbed to the video clip. The method for interleaving a series of cache media files to generate a single video clip ends at step where a clip object comprising a video stream and optionally an audio stream is returned.

At step the method for using a segmented series of cache media files to generate a single video clip takes as input an integer index referring to a specific cache media file. In various embodiments a number of segments may also be specified. Based on the input remaining indexes may be determined. For example an index value of 24 with 8 segments may use indexes 24 25 26 27 28 29 30 31 . The first segment in the index list may be different from the other segments for example in that the first segment may only include an audio track. At step a first index in the list is skipped and each of the remaining indexes is processed in a loop. At step a cache media file associated with each index is loaded and a clip is returned by the method illustrated in and described below. At step the processing server determines whether all indexes have been processed. If indexes remain for processing the method returns to step . Otherwise the method proceeds to step where the clips are concatenated to recreate an original output.

At step the audio file corresponding with the first index is loaded and returned as an audio clip. At step the audio clip is dubbed to the video clip. The method for using a segmented series of cache media files to generate a single video clip ends at step where a clip object comprising video and audio streams is returned.

At step the method for generating a single video clip takes as input an integer index referring to a specific cache media file and or a string including the file path for example. At step the processing server determines whether the input is a filename. At step the processing server determines whether the input is an integer index. If the processing server determines that the input is not an integer or path string or the path is invalid the method may terminate with an error.

At step if the input is an integer the actual file path the .avs script referenced by the index is determined. At step if the input is a filename the input file is indexed as needed. For example a third party plugin such as FFindex or any suitable indexing mechanism may be run on the input file if an index file does not exist. At step a video stream from the input file is loaded using FFVideoSource or any suitable mechanism for loading and returning the video stream as a clip.

At step the processing server determines whether the cache media file was encoded in MJPEG format. At step if encoded in MJPEG format the video color range is adjusted. For example MJPEG files may be stored using a different luminance range and step may restore the standard levels. At step the processing server determines whether to attempt to include an audio stream. By default audio is included if the cache media file includes audio. In various embodiments the default may be overridden for example. At step the processing server determines whether the cache media file contains an audio track. If so the method proceeds to step where the audio stream is loaded from the input file and returned as an audio clip. For example FFAudioSource or any suitable mechanism may load the file and return the audio stream as a clip. At step the audio clip is dubbed to the video clip. The method for generating a single video clip ends at step where a clip object comprising a video stream and optionally an audio stream is returned.

Referring to the method for processing video for a beat sequence video editing project loops and synchronizes a series of user uploaded images over a user uploaded music track. The images are timed to appear rhythmically to match the beat of the music. The tempo of the music is tracked throughout so that the appearance of images speeds up or slows down with the beat of the music. Visual effects such as tap zoom and rotate can be applied to the image sequence on particular beats which further correlates the resulting video to the underlying music.

The method for processing video for a beat sequence video editing project begins at step . At step defaults are loaded according to a selected preset. In various embodiments defaults can be overridden by options. At step the system parses a list of input sources and returns a video clip. The list of input sources may include paths to user supplied images or video files for example. The individual images and or video frames of the sequence are loaded as individual frames comprising an NTSC frame rate 29.97 frames per second video stream. If video files are specified individual frames from each video may be chosen at intervals so that a shortened time lapse version of the video clip is converted to a series of images which may be inserted between other images or videos in an input list to form the sequence for example. In various embodiments the frames of the sequence are resized and may be letterboxed or pillar boxed to fill the target width and height. The background color can be determined by the selected preset or option for example. At step titles may be generated for one or more of the individual frames. For example overlay titles as specified in the input parameters for specific frames are rendered to contiguous frames in a 32 bit RGBA 29.97 frames per second video stream. At step a digital music file is loaded. At step beat detection is performed on the digital music file. For example the music file may be analyzed to determine the temporal location of musical beats quarter notes in order set the timing of cuts and effects.

At step the function re times the image sequence to match the tempo of the music. By default images appear for the duration of a music quarter note eighth note or sixteenth note depending on the average tempo quarter note beats per second . The automatic beat subdividing can be disabled by specifying a particular note length or pattern of note lengths. Images for which pauses or titles have been specified are extended to fill multiple beats the number of which can either be specified or automatically determined according to the tempo.

At step if the re timed sequence is of shorter duration than the music the method proceeds to step where the re timed sequence is looped to fill the duration. More specifically at step the basic image sequence duplicates of what was created in step is repeatedly appended to the video stream until the video stream is at least as long as the audio stream. At step the additional instances of the image sequence are matched the tempo of the music. By default images appear for the duration of a music quarter note eighth note or sixteenth note depending on the average tempo quarter note beats per second . The automatic beat subdividing can be disabled by a specifying a particular note length or pattern of note lengths. After re timing the image sequence at step the processing server determines whether the sequence is complete at step . If there are no more frames remaining in the basic image sequence the method continues to step as described above.

If the processing server determines that there are more frames for the image sequence at step the method proceeds to step where the next frame of the image sequence is isolated and prepared for further processing. At step the processing server determines whether the user has specified a pause or a title for the current frame image . By default images appear for the duration of a music quarter note eighth note or sixteenth note depending on the average tempo quarter note beats per second . If a pause or title is not specified for the current frame the method proceeds to step where the duration of the fill beat or partial beat may be extended. The automatic beat subdividing can be disabled by a specifying a particular note length or pattern of note lengths. If a pause or title is specified for the current frame the method proceeds to step where the current frame is looped to fill multiple beats. The duration in beats can either be specified or automatically determined according to the tempo. At step the processing server determines whether titling has been specified for the current frame. If titling was specified the titling is applied by for example superimposing a rendered title overlay over the image at step .

If the processing server determines that the re timed sequence is not of shorter duration than the music at step the method proceeds to step where the processing server determines whether visual effects were selected for application to the video. For example the user can select various effects e.g. zoom rotate stretch tap that are applied at musical intervals according to the tempo and various options. If visual effects were selected at step zoom rotate stretch and or tap effects may be applied at musical intervals according to the tempo and various options at step . At step the audio music and video images streams are synchronized and multiplexed. At step the video is output. The method ends at step .

Referring to an image stack video editing project animates stacks of images that have a solid colored border and may have a thicker bottom border that allow for their titles to be written on them when such titling is specified. In various embodiments titles can be added in a permanent marker font to look hand written for example or any suitable font. Stacks of eight images may animate in to look for example like a random pile of images was dropped onto a table. If a video is present it may play within the walls of the image frame and look similar to image counterparts just with video.

At step input sources are provided as paths to media files which may be video clips with or without audio or still images. Titling text can also be specified for each source or titling text can appear alone as a main title without a video clip or still image source behind it. At step defaults of the function are loaded and overridden as needed by user set options. At step the function loops through the sources list and using the method for generating a script as illustrated in and discussed above creates .avs scripts for each source and or titling text string as illustrated in and described below.

At step the scripts generated in step are batch processed with the method for threading to concurrently render and encode multiple scripts as illustrated in and discussed above resulting in 32 bit RGB transparency cache media clips rendered and encoded as AVI files. In various embodiments the cache identifiers of these cache media clip files are stored in memory so that they can be later read and processed more efficiently. Depending on the unstack option the sequence may either begin with all images stacked on top of each other which then are peeled off one by one revealing the background or begin with the background onto which the images are successively stacked.

At step each 32 bit cache media clip created at step is loaded and each is used as a source in another series of .avs scripts generated by the method for generating a script as illustrated in and discussed above. Each script applies a flatten process as illustrated in and described below to the source.

At step the scripts generated at step are batch processed with the method for threading to concurrently render and encode multiple scripts as illustrated in and discussed above resulting in 24 bit RGB cache media clips rendered and encoded as AVI files for example. In various embodiments the cache identifiers of these cache media clip files are stored in memory so that they can be later used efficiently to contribute to the final output video audio streams.

At step if a loopable sequence has been specified the method proceeds to step where the processing server determines whether the final loopable output contains an additional stacking or unstacking animation to achieve seamless looping. The loopable option ensures that the final sequence starts and ends with either a solid background color or background image if specified by the user. If the output includes an additional unstacking animation an introduction intro cached media clip is generated at step . More specifically the first 10 frames which may include an in transition animation applied to a freeze frame of the source s first frame of each flattened source may be shown in rapid succession then frame blended and frame decimated to give the appearance of fast motion. In various embodiments the intro clip begins with either background color or background image as specified by the user. If the output includes an additional stacking animation a conclusion outro cached media clip is generated at step . More specifically the final 10 frames which may include an out transition animation applied to a freeze frame of the source s last frame of each flattened source may be shown in rapid succession then frame blended and frame decimated to give the appearance of fast motion. In various embodiments the outro clip ends with either background color or background image as specified by the user. The intro or outro cached media clip allows the multiple image stack video editing project sequences to be concatenated seamlessly.

At step the cached source clips with animation and any titling applied and any intro or outro clips are added sequentially to the main output video audio stream. The clips are then converted to YV12 colorspace and audio is converted to 48 kHz 16 bit stereo. At step a single video clip with audio is returned.

At step a single source path to a media file to a digital video file or still image file is provided as input along with optional titling text. If titling text is provided without a source path a main title will be generated. At step function defaults are loaded and overridden as needed by user set options. At step if a media file is provided as input it is loaded and the processing server determines whether the input source is a still image. At step still images are looped to fill the default or user specified duration. At step the processing server determines whether a media file has been specified as input. If not the title text is rendered by itself as a main title .

At step depending on the intended output dimensions either a 16 9 or 4 3 background frame image is loaded into memory. In various embodiments the function supports images of 16 9 4 3 3 2 1 1 2 3 or 3 4 aspect ratios ratio of width to height . At step on main titles the text is rendered at a larger bolder size and placed in the center of the background frame image. At step source dimensions are analyzed and the background frame that most closely matches its aspect ratio is loaded into memory. In various embodiments the function supports images of 16 9 4 3 3 2 1 1 2 3 or 3 4 aspect ratios ratio of width to height . At step the source is cropped to fit within the background frame separated by a margin. Depending on whether titling text has been specified the bottom margin may be extended to allow room for the text to appear on the background frame.

At step the processing server determines whether titling text has been specified for the source. If so the titling text is superimposed over the bottom of the background frame at step . At step the cropped source clip is superimposed over the background frame leaving margins on the sides. In various embodiments the bottom margin s height is larger if titling text has been specified. At step each clip transitions in an out. The direction of the animation and rotation angle is cycled to give the appearance of randomness such as what would result when someone tossed a bunch of photographs into a pile. At step the first frame of the processed input is held for an additional 10 frames. The last frame is held for an additional 10 frames. These additional frames are used so that the in out animations do not subtract from the natural length of the clip. At step the in and out transitions sliding and rotating with simulated motion blur are applied to the composite source titling background frame input. At step a single 32 bit RGB transparency video clip with or without audio is returned.

At step the flatten process takes a cache media identifier as input. Optionally a background image can be specified. At step function defaults are loaded and overridden as needed by user set options. At step the AVI file corresponding to the input cache identifier is loaded into memory as a video clip with or without audio . At step the processing server determines whether the user has specified a background image to be used. If so the processing server determines whether the specified background image has been cached at step . At step the background image is conformed to 24 bit RGB and scaled and or cropped as needed to fill the intended output dimensions for example. At step the method for generating a script as illustrated in and discussed above is used to generate an .avs script and store the resulting cache video clip as an AVI to improve performance on subsequent instances of the flatten process.

At step when no background image is specified a blank clip will be used as the background. The background color is determined by preset and can be overridden by user option. At step the cached version of the background image is loaded into memory as a video clip. At step the input media clip is superimposed over the background. At step a single 24 bit RGB video clip with or without audio is returned.

Referring to swivel sequence and flip sequence projects have multi clip screens where 2 5 images and videos for example appear on the screen at once and the individual clips flip or swivel in to show new images and videos. If images have titles or there are videos present in the multi clip screen they are also displayed on their own after their multi clip screen in their entirety.

At step both the swivel sequence and flip sequence projects take lists of paths to digital video or still images files as input. Titling text can be specified for any source or without a source resulting in a main title. Each list can create a different track. In various embodiments up to 4 tracks can be specified. The multiple tracks are tiled and play simultaneously in the final output which can be either 640 480 or 1280 720 pixels in size for example. In various embodiments the swivel sequence project animates each source around a central axis either vertically or horizontally. In various embodiments the flip sequence project simulates a folding effect along a central horizontal axis. Since the flip sequence project is designed to simulate different images appearing on each side of a board it operates on adjacent pairs of sources. At step function defaults are loaded and overridden as needed by user set options.

At step the processing server determines whether there are multiple tracks. For example up to 4 tracks can be specified. The multiple tracks may be tiled and play simultaneously in the final output. At step if the processing server determines multiple tracks are present at step the first track becomes the master track. In various embodiments only the master track can also have audio. In certain embodiments videos played on the master track maintain their original duration unless trimming is specified while the video on slave tracks are treated similarly to still images trimmed to a common duration audio muted . At step the timing of each cut on the master track may be predicted. At step the total time of the master track is used to determine how long each video or still image appears in the slave tracks. Since each track can have a different number of sources and the master track can have long playing videos this is done to ensure that all tracks have the same duration.

At step one or more track video clips are created as illustrated in and described below. The multiple tracks are tiled and play simultaneously in the final output. The dimensions of the tracks depend on intended output size and the number of tracks. For example if there is one track a single track is generated. If there are two tracks two tracks having identical dimensions may be generated. If there are three tracks one larger track and two smaller tracks can be generated. If there are four tracks four tracks of identical dimensions may be generated.

At step the processing server determines whether multiple tracks are specified. At step in multi track output the tracks are placed side by side and or above and below each other depending on the number of tracks. Since the effects applied to the sources simulate motion and perspective the tracks are cut and layered and recombined so that the currently animating track appears on top of static tracks. The track timings are used to determine sequencing of the layering order. In various embodiments track layering is performed once for 2 or 3 track layouts. Track layering may be performed twice for 4 track layouts since there is an additional possibility of the swivel animations overlapping. Track layering operates in different ways depending on whether a horizontal or vertical swivel axis has been specified.

At step solid colored background may be created. At step the track or multi track output is superimposed over the background. For example if there is one track a single track tile is generated. If there are two tracks two tracks having identical dimensions may be positioned side by side and optionally separated by a border. If there are three tracks two smaller track tiles can be arranged vertically and optionally separated by a border. The two tiles may then be positioned to the left or right of a larger track tile and optionally separated by a border. If there are four tracks each of the four tracks tiles can have identical dimensions and optionally be separated by a border. In various embodiments the combined tiles are then centered on background and optionally separated by a larger border. The borders can reveal the background color.

At step the video output is converted to YV12 colorspace for example. Audio is conformed to 48 kHz 16 bit stereo for example. At step a single YV12 video clip with audio is returned.

At step a list of paths to media files still images or video is received as an input. At step function defaults are loaded and overridden as needed by user set options. At step a swivel sequence effects script is generated for each source which will later be rendered and encoded as cached video clip AVI files as illustrated in and described below. For swivel sequence project operations each source is individually passed to the method illustrated in and described below. For flip sequence project operations adjacent pairs of sources are passed to the method illustrated in and described below.

At step the swivel sequence effects scripts are batch encoded to cache video clip AVI files using the method for threading to concurrently render and encode multiple scripts as illustrated in and described above. At step the cached AVI file output of the swivel sequence effects scripts is loaded using the method for generating a single video clip as illustrated in and described above. At step the video clips are concatenated to form a continuous video track comprised of the sources. At step a single 32 bit RGB transparency video clip with 48 kHz audio is returned for example.

At step either a single source for swivel sequence project operations or adjacent pairs of sources for flip sequence project operations is received. At step function defaults are loaded and overridden as needed by user set options.

At step the processing server determines whether a media file has been specified as input. If not the title text is rendered by itself as a main title and the method proceeds to step . If the media file was specified as the input the method skips to step . At step a solid color different than the main background color is specified for main title backgrounds. At step the main titling is rendered and superimposed over the solid background generated at step . At step the duration of the main title is matched to the duration specified for still images.

At step if the media file was specified as the input at step the source media is loaded and returned as a video clip with or without audio depending on the source file itself. At step the processing server determines whether the source file is a still image or video file. At step if the source file is a video file the processing server determines whether the video clip is to be treated as a still image. This may be specified by the user for example. In various embodiments videos appearing on slave tracks of multi track output are treated as still images. At step if the video clip is being treated as a still image audio attached to the video clip if any is removed. At step the duration of the video clip is matched to the duration specified for still images.

At step the processing server determines whether titling text has been specified for the source. At step if titling text was specified the titling text is rendered and superimposed on the source clip. At step the processing server determines whether the corners of the clip were specified to be rounded. If so a user specified corner radius may be applied to the edges of the clip and the edges can be made transparent at step .

At step extra transparent pixels are added to the edges of the resulting clip to allow room for the simulated perspective when the clip is swiveled in step for example. At step quadrilateral transformations are applied the source to simulate its spinning toward or away from the viewer either horizontally or vertically clockwise or counterclockwise. Simulated motion blur may also be applied for greater realism. In the case of a flip sequence project the effects are applied to two sources at once except for the first and last clips on the track to simulate revealing source B on the flip side of source A . In various embodiments the flip sequence project may include brightness adjustments during the animation to give the illusion of light hitting a shiny surface. The swivel effects are applied to beginning and end of the clip leaving it static for most if its duration Swivel In Static Swivel Out . At step a single 32 bit RGB transparency video clip with or without audio is returned.

Referring to a slide sequence project has multi clip screens where 2 5 images and or videos for example appear on the screen at once and the individual clips slide on and off the screen to show new images and or videos. In various embodiments if images have titles or there are videos present in the multi clip screen they are also displayed on their own after their multi clip screen in their entirety.

At step a list of paths to media files still images or video is input to the method for processing video for a slide multi clip sequence video editing project. A list of screen types may also be input. The screen type determines how many sources appear on screen simultaneously and the geometric arrangement. In various embodiments there may be nine possible screen types numbered 0 8 . At step function defaults are loaded and overridden as needed by user set options.

At step the processing server determines if main title text has been specified by the user. If main title text was specified the method proceeds to step where a slide sequence screen script is generated based on the main title text and additional sources. More specifically main titles may call for 2 additional sources. The source paths and title text are passed to a method for processing main title text and or media sources as illustrated in and described below and rendered as a 3 tile screen where one title is occupied by the rendered main title text and the other two by the media clips.

At step the function loops though the user supplied sources and screen type lists. In various embodiments each given screen type expects a particular number of sources and each creates a different geometric arrangement of the source clips for example 

Screens may also be used with half of the above number of sources so that type 3 may take 4 sources type 7 may take 4 sources etc. for example in the event that there are not enough sources available such as at the end of the sources list. Note that screen type 0 is special in that it uses one source clip.

At step the next screen type is extracted from the list. The source paths that correspond to the screen type are extracted from the sources list. The number of sources consumed per loop depends on the value of this screen type. If there are no more screen types in the list but sources still remain screen types are chosen automatically for the remaining sources.

At step the current screen type and corresponding sources are used to generate a slide sequence screen script as illustrated in and described below via the method for generating a script as illustrated in and described above. The cache identifier of the script is stored so that it can be later rendered and encoded to the cache video clip AVI file. At step the processing server determines whether all sources have been exhausted. If not the method loops back to step .

At step the slide sequence screen scripts are batch encoded to cache video clip AVI files using the method for threading to concurrently render and encode multiple scripts as illustrated in and described above. At step the cached video clip file corresponding to each slide sequence screen script is loaded via the method for generating a single video clip as illustrated in and described above and returned as a video clip. At step the video clips are spliced end to end resulting in single media clip. At step a single media clip with YV12 colorspace video and 48 kHz 16 bit stereo audio is returned for example which can then be further edited processed and or encoded.

At step a single screen type argument and a list of source media paths is received as an input. At step function defaults are loaded and overridden as needed by user set options. At step each of the possible screen type arguments expects a particular number of sources and creates a single clip composed of the sources arranged geometrically for example 

If too many sources are provided for the specified screen type the additional ones are ignored. If too few are provided for optimal usage upper table various embodiments provide attempting to use half the optimal number lower table for example.

At step the processing server determines if the number of supplied sources is adequate for the specified screen type. If the number of sources is not compatible with the screen type a more appropriate screen type is chosen automatically at step . In certain embodiments additional sources are discarded such as when 5 sources are supplied for screen type 2 etc. . At step each source is loaded by one of several source filters depending on file type and returned as a video clip with or without audio. At step each source is assigned an animation direction left right up or down that is used for transitions. Also the panning animation direction alternates for each source.

At step the processing server determines whether the number of sources is optimal double the requirement. This is true when 4 sources have been specified for a 2 tile screen type 8 sources for a 4 tile screen type etc. If the number of sources is optimal each source is scaled to fit the geometry of one of the screen tiles at step . In cases where the aspect ratio of the source does not match the target tile cropping is animated resulting in panning across the image or video. At step for double lists sources are paired together so that given sources 1 2 3 4 for a 2 tile screen sources 1 3 will appear in one tile and 2 4 in the other for example. The paired clips play in succession overlapped by a sliding transition of varying direction. At step in and out sliding transitions are applied to the resulting clip s .

If the number of sources determined at step is not optimal the processing server determines whether main titling text has been supplied at step . At step if main titling text was supplied the main title text is rendered to a 32 bit RGB transparency video clip for example. At step if main titling text was not supplied the processing server determines whether screen type 0 has been specified and if the clip s source was a digital video file. At step for any case where screen type 0 video clips are treated as still images and conformed to common duration without audio. At step each source is scaled to fit the geometry of one of the screen tiles. In cases where the aspect ratio of the source does not match the target tile cropping is animated resulting in panning across the image or video.

At step the processing server determines whether titling text has been specified for each source. At step if titling text has been supplied for any source clips that text string is rendered and superimposed over the corresponding source clip. At step in and out sliding transitions are applied to the resulting clip s .

At step the resulting tiles from steps and or are arranged geometrically to fill the screen optionally separated from each other by an inner border and from the edges of the resulting screen by a larger outer border for example. In various embodiments borders may reveal the background color which is determined by preset and can be overridden by user option. At step the resulting video is converted to YV12 colorspace and audio is conformed to 48 kHz 16 bit stereo for example. At step a single media clip with YV12 colorspace video and 48 kHz 16 bit stereo audio is returned which can then be further edited processed and or encoded.

Referring to the method may be used to match a music file or music files can be many length with video length. For example if the music file is longer than the video it is trimmed to match. As another example if the music file is shorter than the video it is looped with some logic to only do so if X seconds are remaining to match the video length. The method can be used to fade in and out between tracks and loop if necessary to match video length if there are multiple music files. The method may be used to fade in and fade out music file s with the video. The method can be used to call audio ducking functionality to seamlessly fade the music audio in and out around video audio so that the music and video audio do not interfere with each other. In various embodiments the above mentioned exemplary functions of method are performed automatically such that a user does not have to perform the extremely difficult and tedious tasks for example.

The method begins at step . At step function defaults are loaded and overridden as needed by user set options. At step a loader loop is started such that a next music clip is loaded at step . At step the processing server determines whether to normalize a volume of the music clip. If the processing server determines normalization is needed the normalization is performed at step .

At step the processing server may fade in and or out of the music based on selected options. At step a main audio clip is appended with the current audio clip. At step if all music clips have not been loaded the method returns to step . After all music clips have been appended to the main audio clip the method proceeds to step where the processing server determines whether the music clip is longer than the video clip. If the music clip is longer the method proceeds to step where the audio is looped until audio is longer than the video clip. At step the audio clip is trimmed to the video duration.

At step the processing server determines whether to perform audio ducking. If audio ducking was selected the method proceeds to step where the method for audio ducking is performed as illustrated in and described below. Otherwise the method proceeds to step where the audio track of the video and the audio track of the music are mixed. The resulting audio clip is dubbed to the video by synchronizing and multiplexing the audio music and video images streams. At step the audio is conformed to the sample rate and word length as specified in the options or the default of 48 kHz 16 bit . The method ends at step .

Referring to audio ducking is a complex function that maps points in an audio stream of a video clip where silence changes to noise or noise changes to silence and then smoothly mutes or reduces or increases the volume of the music audio e.g. with fade in and fade out so the music audio and the video audio do not collide. As an example if there is music playing over images and then a video with a corresponding audio soundtrack appears the music smoothly fade outs while that video is playing so that the soundtrack of the video is more clearly audible and then fades back in once the video has completed.

The method begins at step . At step function defaults are loaded and overridden as needed by user set options. At step the audio track of the video is written to a WAV file so that it can be analyzed and changes between noise and silence can be detected and logged to a file. At step noise gating is applied to the WAV by a noise gating tool such as those available within sox.exe or any suitable noise gating tool for example. The processing server may log onsets representing point in time at which the soundtrack changes from silence to noise or from noise to silence to an output file using a tool such as Sonic Annotator or any suitable tool for example. The processing server can convert the decimal seconds into frame numbers at 29.97 frames per second fps for example using a tool such as onsets.exe or any suitable tool.

At step the onsets file is loaded. The onsets file includes a list of the temporal location of onsets in integer frames at 29.97 frames per second fps . The first onset is the first point at which silence changes to noise or 0 representing the very beginning in cases where the audio does not begin with silence . The music is faded out at the silence noise onsets and faded in at the noise silence onsets . For example if the audio includes 5 seconds of a subject speaking then 5 seconds of silence followed by 5 seconds of conversation the onsets List would be 0 150 300. As another example if the audio consists of 5 seconds of silence followed by 10 seconds of noise followed by 10 seconds of silence the onsets list will be 150 450.

At step the processing server determines whether the onset list is empty. An empty onsets list indicates that the entire video audio track is silent. No fade in or fade out occurs in these cases and the music is dubbed to the video replacing the audio track of the video at step . At step if the onset list includes one onset there is no silence in the audio track of the video. If there is no silence in the audio track of the video the music audio is mixed with the video audio at a reduced fade out volume. If the onset list includes more than one onset the method proceeds to step and the list of onsets is looped through so that fade in and fade out events can be triggered on the music track at the corresponding video frame numbers.

At step the value of a current onset is compared to the value of the next onset to determine an interval. At step the duration of the interval is compared to the fade in out duration to avoid fading out on very short silences. If the duration of the interval is greater than twice the fade in out duration for example the method may proceed to step . Otherwise the method may skip to step such that the short silence is filtered out and not added to the points list described below.

At step the fade duration is subtracted from the next onset so that the fade is completed by the point specified by the value of the onset. The computed difference is added to a points list which includes the points in time at which the music is faded in and out. After the processing server determines that all onsets were compared at step the method proceeds to step where the processing server determines if the points list is empty. An empty points list may indicate that all silences were too short to be considered significant and were filtered out. If the points list is empty the method proceeds to step where the audio track of the video and the audio track of the music are mixed at levels specified by the options such as a reduced fade out volume for example. The resulting audio clip is dubbed to the video by synchronizing and multiplexing the audio music and video images streams.

At step if the points list is not empty the processing server begins looping through the points list. The first point in the list is 0 zero based indexing . At step the processing server determines whether the index is an odd number. For example since indexing on the loop is zero based and the first event represents a silence to noise transition an odd numbered index triggers a music fade in event while and even numbered index is a fade out. At step the music is faded in at the current point if the processing server determines that the index is an odd number at step . The length of the transition may be determined by the duration option or default of 30 frames one second . At step the music is faded out at the current point if the processing server determines that the index is an even number at step . The length of the transition is determined by the duration option or default of 30 frames one second .

At step the processing server determines whether the points list has been completed. If not the index of the points list is incremented and the method returns to step . Once the processing server determines that the points list is completed the method proceeds to step where the audio track of the video and the audio track of the music which may have been modified to include more or one of fade in out fade out effects are mixed. The resulting audio clip is dubbed to the video by synchronizing and multiplexing the audio music and video images streams. At step the audio is conformed to the sample rate and word length as specified in the options or the default of 48 kHz 16 bit . At step the video and processed audio are returned from the function as a clip object. For example the clip object can be returned to step of as described above and or step of as described below.

Referring to the method for editing and mixing a music file with a video output is a variation of the method that may be used for example when handling one input music file. The method begins at step . At step function defaults are loaded and overridden as needed by user set options. At step if the music parameter is a string the music track is loaded at step . At step the processing server determines whether to perform audio ducking. If audio ducking was selected the method proceeds to step where the method for audio ducking is performed as illustrated in and described above. Otherwise the method proceeds to step where the audio track of the video and the audio track of the music are mixed. The resulting audio clip is dubbed to the video by synchronizing and multiplexing the audio music and video images streams. At step the audio is conformed to the sample rate and word length as specified in the options or the default of 48 kHz 16 bit . The method ends at step .

Referring to the method for filtering and providing an overlay to a video output applies filters to video and overlays custom or stock film grain over the video to provide an old film look. In various embodiment if applied by itself the output resembles 60 70 s footage and if applied with black and white the output resembles 1950 s or 1920 s footage for example. The method begins at step . At step function defaults are loaded and overridden as needed by user set options. At step the processing server determines whether to decimate the frame rate. At step the frame rate may be decimated based on the determination at step .

The processing server determines whether to provide various effects such as colorized video at step film dirt effect at step vignette effect at step film grain at step and or flicker effect at step among other things. Based on the determination at step the processing server may colorize the video with specified colors at step . For example the specified colors may provide sepia effects black and white or any suitable selected effect. If the processing server determines to provide film dirt effects at step the method proceeds to step where an overlay corresponding with film dirt is applied to the video clip. At step a vignette overlay may be applied based on the determination of the processing server at step . Based on the determination at step a film grain effect may be applied at step . In various embodiments a third party plugin may be used to provide one or more of the effects for example. If the processing server determines to provide flicker effect at step lighting and darkening may be applied respectively to each pair of adjacent video frames for example to provide the flicker effect at step .

At step if the frame rate was decimated at step the original input frame rate may be restored by duplicating frames at step . At step the processing server determines whether to conform the video and audio. If video and audio conformance was selected and or specified the method proceeds to step where the video output is converted to YV12 colorspace for example. At step the video output is converted to the NTSC standard frame rate of 29.97 frames per second for example. At step the video output may be conformed to pixel dimensions of either 1280 720 or 640 480 for example depending on a selected preset among other things. At step the audio is conformed to 48 kHz 16 bit stereo for example.

If video and audio conformance was not selected and or specified the method proceeds to step where the processing server determines whether the input video includes an audio track. If so an old film audio filter as described in the method of may be applied at step . The method ends at step .

The method for color filtering a video output begins at step . At step function defaults are loaded and overridden as needed by user set options. At step the processing server applies user specified or default brightness and contrast adjustments to a video clip. At step the processing server determines whether any color filter effects are enabled. For example the processing server may colorize a video clip using a user specified or default color parameter at step . At step the processing server can apply a sepia effect based on a user specified or default color parameter. As another example the processing server may apply a Technicolor effect color channel specified by a channel parameter or a default such as red among other things at step .

At step the processing server determines whether to conform the video and audio. If video and audio conformance was selected and or specified the method proceeds to step where the video output is converted to YV12 colorspace for example. At step the video output is converted to the NTSC standard frame rate of 29.97 frames per second for example. At step the video output may be conformed to pixel dimensions of either 1280 720 or 640 480 for example depending on a selected preset. At step the audio is conformed to 48 kHz 16 bit stereo for example. The method ends at step .

Referring to the method for color filtering a video output may be the function called in the footer of the project as illustrated in step of for example if the user has selected a color filter. Color filters such as sepia new film and the like are applied to the entire video output or individual video clips for example. The method for color filtering a video output begins at step . At step the processing server determines whether a color filter preset was specified.

If the processing server determines that a Super 8 filter was selected at step a video frame rate is converted to 15.985 frames per second at step . At step a strong film grain effect is added to the video. At step a brightness of the video may be decreased by for example 33 while a contrast of the video is increased by 33 . At step a vibrancy of the video may be reduced by 75 and a saturation of the video can be increased by 300 for example. At step a flicker effect may be applied by for example respectively lighting and darkening each pair of adjacent video frames. At step the video frame rate may be converted to 29.97 frames per second.

At step if the processing server determined that an old film filter was selected a strong film grain effect can be added to the video. At step a brightness of the video may be decreased by for example 33 while a contrast of the video is increased by 33 . At step a vibrancy of the video may be reduced by 75 and its saturation can be increased by 300 for example. At step a Gaussian blur effect may be applied to soften color channels.

If the processing server determines that a new film filter was selected at step a brightness of the video may be decreased by for example 33 and a contrast of the video can be increased by 50 at step . At step a vibrancy of the video may be increased by 75 and a saturation of the video can be reduced by 25 for example. At step moderate sharpening can be applied to a luminance channel of the video. At step a mild film grain effect may be added to the video.

At step if the processing server determined that a Technicolor filter was selected the processing server may apply a Technicolor effect color channel specified by a channel parameter or a default such as red among other things. At step if the processing server determined that a sepia filter was selected the processing server may apply a sepia effect based on a user specified or default color parameter. If the processing server determines that a vibrant filter was selected at step a vibrancy of the video may be increased by 100 and its saturation can be reduced by 33 for example at step . At step if the processing server determined that a mellow filter was selected the processing server may increase a vibrancy of the video by 300 and decrease its saturation by 75 for example. At step if the processing server determined that a spot color filter was selected the processing server may provide an extreme vibrancy increase and an extreme saturation decrease for example.

If the processing server determines that a pop art filter was selected at step a video gamma may be decreased and a contrast can be mildly increased for example at step . At step the processing server may reduce a vibrancy of the video by 38 and increase its saturation by 400 for example. At step a mild softening may be applied to the video. At step a strong sharpening may be applied to the video. The method for color filtering a video output ends at step .

Referring to the vinyl noise filter adds old phonograph record style noise to audio. In various embodiments the old phonograph record style noise may be overlaid over an entire video output which may have video audio and music audio for example. In certain embodiments a user can specify vinyl noise strength like normal heavy or extreme for example. The method for applying a vinyl noise audio filter to a video output begins at step . At step function defaults are loaded and overridden as needed by user set options. At step additional options are set based on a profile parameter. At step a silent audio clip such as a silent 18 second 48 kHz 16 bit stereo audio clip is created and stored in memory. At step if the processing server determines that a crackle effect was selected a crackle WAV file is loaded at step and mixed with the silent audio clip at a level specified by a user selected or default crackle parameter.

At step if the processing server determines that a dust noise effect was selected a dust noise WAV file is loaded at step and mixed with the output audio from step at step . At step if the processing server determines that a click effect was selected a click WAV file is loaded at step and mixed with the output audio from step at step . At step if the processing server determines that a mechanical noise effect was selected a mechanical noise WAV file is loaded at step and mixed with the output audio from step at step . At step if the processing server determines that an equalizer effect was selected a midrange frequency boost may be applied to the input audio at a level specified by an equalization strength parameter specified by a user or default.

At step the output audio from step is looped to match the input audio duration. The output audio from step is mixed into the input audio at step the audio levels are normalized at step and the audio is conformed to 48 kHz 16 bit stereo. The method for applying a vinyl noise audio filter to a video output ends at step .

Referring to an old film audio filter adds old projector style audio noise and effects to audio. For example an end user may add the old film audio filter as projector noise from the audio filters. In various embodiments the projector style audio noise and effects are overlaid over the entire video output which may have video audio and music audio for example. The method for applying a projector noise audio filter to a video output begins at step . At step function defaults are loaded and overridden as needed by user set options. At step a noise clip is looped to match the duration of the input video. The noise clip may be a stock audio clip of noise from an old fashioned film projector for example. At step the noise clip is mixed with the input audio at a specified level. The specified level may be user specified or a default such as 25 noise for example. At step the resulting mixed audio clip is dubbed to the video by synchronizing and multiplexing the audio and video streams. The method for applying a projector noise audio filter to a video output ends at step .

The present invention may be embedded in a computer program product which comprises all the features enabling the implementation of the methods described herein and which when loaded in a computer system is able to carry out these methods. Computer program in the present context means any expression in any language code or notation of a set of instructions intended to cause a system having an information processing capability to perform a particular function either directly or after either or both of the following a conversion to another language code or notation b reproduction in a different material form.

Accordingly the present invention may be realized in hardware software or a combination of hardware and software. The present invention may be realized in a centralized fashion in at least one computer system or in a distributed fashion where different elements are spread across several interconnected computer systems. Any kind of computer system or other apparatus adapted for carrying out the methods described herein is suited.

Although devices methods and systems according to the present invention may have been described in connection with a preferred embodiment it is not intended to be limited to the specific form set forth herein but on the contrary it is intended to cover such alternative modifications and equivalents as can be reasonably included within the scope of the invention as defined by this disclosure and appended diagrams.

While the present invention has been described with reference to certain embodiments it will be understood by those skilled in the art that various changes may be made and equivalents may be substituted without departing from the scope of the present invention. In addition many modifications may be made to adapt a particular situation or material to the teachings of the present invention without departing from its scope. Therefore it is intended that the present invention not be limited to the particular embodiment disclosed but that the present invention will include all embodiments falling within the scope of the appended claims.

