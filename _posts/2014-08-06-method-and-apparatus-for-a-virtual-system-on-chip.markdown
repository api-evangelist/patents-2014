---

title: Method and apparatus for a virtual system on chip
abstract: A virtual system on chip (VSoC) is an implementation of a machine that allows for sharing of underlying physical machine resources between different virtual systems. A method or corresponding apparatus of the present invention relates to a device that includes a plurality of virtual systems on chip and a configuring unit. The configuring unit is arranged to configure resources on the device for the plurality of virtual systems on chip as a function of an identification tag assigned to each virtual system on chip.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09378033&OS=09378033&RS=09378033
owner: Cavium, Inc.
number: 09378033
owner_city: San Jose
owner_country: US
publication_date: 20140806
---
This application is a continuation of U.S. application Ser. No. 12 769 463 filed Apr. 28 2010. The entire teachings of the above application are incorporated herein by reference.

A virtual machine is a software implementation of a physical machine that operates similar to a physical machine. Multiple virtual machines may be implemented on a physical machine to allow for sharing of the underlying physical machine resources.

An example embodiment of the present invention relates to a device that includes a plurality of virtual systems on chip and a configuring unit. The configuring unit is arranged to configure resources on the device for the plurality of virtual systems on chip as a function of an identification tag assigned to each virtual system on chip.

Another embodiment of the present invention relates to a processor device that includes a plurality of virtual systems on chip a plurality of operating system applications and a configuring unit. The configuring unit is arranged to exclusively assign the plurality of virtual systems on chip to each operating system application as a function of an identification tag assigned to each virtual system on chip.

In the view of the foregoing the following description illustrates example embodiments and features that may be incorporated into a system for configuring resources on a processor device where the term system may be interpreted as a system a subsystem apparatus method or any combination thereof with regard to embodiments of the present invention.

The system may include a plurality of processing cores and each virtual system on chip may relate to a subset of the plurality of processing cores. The configuring unit may partition the plurality of processing cores into subsets of the plurality of the processing cores that relate to corresponding virtual systems on chip. The configuring unit may assign an identification tag corresponding to the identification tag of a virtual system on chip to the subset of the one or more processing cores that relate to the virtual system on chip. The configuring unit may partition work requests for the one or more processing cores into subsets having identification tags associated with the plurality of virtual systems on chip and the plurality of virtual systems on chip may process the work requests as a function of the identification tags.

The configuring unit may assign subsets of dynamic random access memory to each of the plurality of virtual systems on chip and each assigned subset of the dynamic random access memory may be further assigned the identification tag associated with its corresponding virtual system on chip. The processor may assign the identification tags to each subset of the dynamic random access memory. The configuring unit may configure access to dynamic random access memory resources as a function of the identification tag. The configuring unit may prevent access to a partition of the dynamic random access memory having an assigned identification tag by a virtual system on chip having a different identification tag. The configuring unit may issue a violation signal in an event an attempt to access the partition of the dynamic random access memory having the assigned identification tag by the virtual system on chip having the different identification tag is made. The configuring unit may assign a shared portion of the dynamic random access memory resources to all virtual systems on chip independent of their identification tags. The configuring unit may assign a priority for accessing the dynamic random access memory resources to each of the plurality of virtual systems on chip as a function of the identification tag.

The system may include a level two cache. The level two cache may provide access to dynamic random access memory resources as a function of the identification tag.

The system may include a free pool unit. The free pool unit may allocate processing memory to work requests as a function of the identification tags.

The configuring unit may configure input and output resources for the plurality of virtual systems on chip. The system may include an inbound interconnect unit. The inbound interconnect unit may assign work requests to corresponding virtual systems on chip as a function of the identification tag. The inbound interconnect unit may assign the work requests as a function of a preconfigured database relating the identification tag to a corresponding virtual system on chip.

The system may include a packet input processing unit. The packet input processing unit may allocate one or more buffers from address space of a virtual system on chip to work requests assigned to the virtual system on chip as a function of the identification tag.

The system may include a packet order and work unit. The packet order and work unit may schedule work requests as a function of the identification tags. The packet order and work unit may generate an input queue entry for the work requests assigned to the virtual system on chip as a function of the identification tag. The packet order and work unit may schedule work requests within each of the plurality of virtual systems on chip. The packet order and work unit may schedule work requests among the plurality of virtual systems on chip as a function of the identification tag.

The plurality of virtual systems on chip may be coupled with a packet output processing unit having an output entry queue. Each virtual system on chip may be coupled with a corresponding packet output processing unit that includes one or more output entry queues exclusively assigned to the virtual system on chip. Each output entry queue may be assigned to the virtual system on chip as a function of the identification tag. The system may include a security unit in a level two cache that prevents a virtual system on chip from access to output entry queues of other virtual systems on chip. At least one output entry queue may have a higher priority than other output entry queues.

The one or more virtual systems on chip may be coupled with an output entry queue. The output entry queue may receive output entries having the identification tag corresponding to originating virtual system on chip.

The one or more virtual systems on chip may be coupled with a packet output processing unit. The packet output processing unit may process each output entry as a function of the identification tag.

The system may include an output processing unit. The output processing unit may release processing memory being used by a work request from address space of a virtual system on chip upon completion of the work request as a function of the identification tag. Each of the one or more virtual systems on chip may include an associated priority. The packet output processing unit may process each output entry as a function of the associated priority.

A virtual system on chip VSoC is an implementation of a machine that allows for sharing of underlying physical machine resources between different virtual systems. Under a VSoC implementation multiple operating system applications can co exist on the same physical machine with complete isolation from one another and protection.

Certain example embodiments of the present invention relate to providing hardware support for isolation protection priority configuration and resource allocation of memory and I O accesses from multiple operating systems running on different cores in a multi core processing system. The example embodiments also support sharing of input and output resources among multiple virtual systems on chip VSoC running on multiple groups of cores with no software overhead.

The configuring unit may further configure input and output resources for the plurality of virtual systems on chip . For example in the ingress direction the configuration unit may assign work requests to corresponding virtual systems on chip as a function of the identification tag. In some embodiments an incoming work request may be processed to extract directly or indirectly using information from packet header or payload its corresponding identification tag. Specifically a preconfigured database relating the identification tag to a corresponding virtual system on chip may be employed.

The configuring unit may partition the dynamic random access memory into subsets assigned to each virtual system on chip . Each assigned dynamic random memory partition is further assigned an identification tag associated with its corresponding virtual system on chip. The identification tag may be assigned to the partitions by the configuring unit .

The configuring unit may provide access to dynamic random access memory partitions as a function of the identification tag. In some embodiments the configuring unit prevents access to a partition of the dynamic random access memory having an assigned identification tag by a virtual system on chip having a different identification tag. In an event an attempt to access the partition of the dynamic random access memory having an assigned identification tag by a virtual system on chip with a different identification tag is made the configuring unit may issue a warning sign. The dynamic random access memory may include a shared partition that can be accessed by all virtual systems regardless of their identification tag.

A level two cache or any other central module that encounters memory accesses from units such as cores hardware assist engines input output block etc. may further be configured based on the identification tag. In some embodiments the level two cache provides access to dynamic random access memory resources based on the identification tag. The level two cache may include a security unit that prevents a virtual system on chip from access to other virtual systems on chip.

In certain embodiments level two cache may include a security unit that restricts memory accesses to different virtual systems on chip with low level granularity.

In certain embodiments the virtual systems on chip may be configured to have priority in terms of input and output entry processing and accessing the dynamic access memory such that one virtual system on chip may have strict or weighted priority over other virtual systems on chip.

In certain embodiment level two cache may include a priority unit that allows priority based memory access by different virtual systems on chip. The priority may be strict weighted round robin or a mix of them.

The example embodiment divides a single physical system on chip into multiple virtual systems on chip using a combination of hardware and software features not shown . By doing so the example embodiment provides for low overhead sharing of resource on chip while providing isolation and protection between the multiple virtual systems on chip . The example embodiment also provides hardware support to eliminate performance overheads. Specifically the example embodiment provides the ability to group the processing cores into corresponding subsets included in each virtual system on chip. The example embodiment further provides for partitioning of dynamic random access memory DRAM into subsets assigned to each virtual system on chip. Input output blocks and co processor not shown may be portioned between the multiple virtual systems on chip as well.

By partitioning the system resources into multiple virtual systems on chip the example embodiment provides for executing multiple embedded applications and operating systems on the same chip while enabling sharing of the chip resources and protection between among the multiple virtual systems on chip .

The example embodiment may include a configuring unit which provides support for partitioning the resources of the chip e.g. processing cores DRAM input output blocks coprocessors etc. into the multiple virtual systems on chip . The configuring unit may include two components namely a configuring unit kernel and configuring unit user interface .

The virtual systems serve as virtualized execution environments for executing operating systems and embedded applications. The configuring unit user interface configures the virtual systems on chip to ensure that each virtual system on chip maintains complete ownership of its CPU cores. Each virtual system on chip includes the input output blocks defined by a specific device tree configured by the configuring unit Kernel . Further each device in the device tree of the virtual system on chip may be directly mapped to a physical device or shared with other virtual systems on chip.

Applications and operating systems i.e. software running inside a virtual system on chip employ virtualized device drives as configured by the configuring unit Kernel . Given the protection provided by the multiple virtual systems on chip applications and operating systems running on a virtual system on chip may not be aware of other applications and operating systems running inside other virtual systems on chip.

The configuring unit initializes and controls all platform hardware e.g. CPU cores devices interrupt controllers not shown etc. . The configuring unit further supports creation and management of multiple virtual systems on chip . In doing so the configuring unit exports a partition specific device tree of virtual systems to each partition and exports a user interface for partitions to program virtual systems. The configuring unit further multiplexes the virtual systems on physical devices.

The configuring unit provides hardware support for partitioning of DRAM by using unique identification tags VMID assigned to each virtual system on chip. All cores belonging to the virtual system on chip are also assigned the same VMID. A level two cache provides access control to DRAM segments based on the VMID assigned to each virtual system on chip. Specifically the configuring unit Kernel programs the level two cache to protect DRAM segments belonging to a virtual system on chip from being accessed by other virtual systems on chip. Each memory request from a core is also associated with a VMID. Therefore any memory access e.g. work request to a memory segment not permitted by a VMID assigned to a core causes an access violation.

The configuration unit Kernel also programs the level two cache controller to provide DRAM access priority for various virtual system on chip. Access to DRAM from a virtual system on chip may be prioritized based on the VMID of the request.

The configuring unit user interface provides the interface for virtual system on chip management. The configuring unit user interface may run as a part of the configuring unit kernel or be hosted on a privileged virtual system on chip . The configuring unit user interface assigns resources e.g. DRAM and input output devices to individual virtual systems on chip. Each newly launched virtual system on chip includes a partition control unit . The partition control unit includes elements such as a virtual system on chip specific device tree memory descriptors for memory belonging to the virtual system on chip and one or more application programming interfaces for input output i.e. virtualized device drivers messaging and shared memory .

Such processors may be used in a wide variety of networking and storage equipment including routers switches security appliances content aware switches triple play gateways aggregation devices and gateways storage arrays storage networking equipment and servers.

The processor may include one or more CPU cores . The CPU cores are full functionality and high performance integer implementations and directly support industry standard C C and other programming environments. These cores have all the necessary requirements to boot and run full functionality operating systems. Shared main memory may be implemented via the level two cache and the DRAM.

A packet is received for processing by an ingress interface e.g. XAUI . The processor performs pre processing of the received packet by checking various fields in the headers included in the received packet and then forwards the packet to the packet input unit . In the ingress direction packets i.e. work requests are classified based on VMID. In certain embodiments upon arrival the work requests are processed and the VMID is assigned and or extracted. The VMID may be assigned based on fields extracted from the packet e.g. a metadata field of the work requests and or from a preconfigured lookup table. Based on the assigned VMID the work request is assigned to a corresponding virtual system on processor. For example using the preconfigured lookup table a certain VMID may be assigned to a packet having a certain destination MAC address.

The packet input unit performs further pre processing of the headers included in the received packet. The pre processing may include checksum checks for Transmission Control Protocol TCP User Datagram Protocol UDP L3 network protocols .

The packet input PIP IPD may also be configured to support virtualization as a function of VMID. Based on the VMID the PIP IPD employs different FPA pools groups input queues for different type of traffic identified by the input port. The PIP IPD may further add a VMID to every submitted work request.

A Free Pool Allocator FPA maintains pools of pointers to free memory in level two cache memory and DRAM . The input packet processing unit IPD uses one of the pools of pointers to store received packet data in level two cache memory or DRAM and another pool of pointers to allocate work queue entries for the processor cores.

The free pool allocator FPA may also be configured to support virtualization as a function of VMID. Additional free pools may be added to support the virtual systems on processor. The FPA adds a VMID onto every generated work request and employs the VMID to determine if free buffers are available for allocation.

The packet input unit writes packet data into buffers in the level two cache or DRAM in a format that is convenient to higher layer software executed in at least one processor core for further processing of higher level network protocols.

Level two cache memory and DRAM memory is shared by all of the processor cores and I O co processor devices. Each processor core is coupled to the level two cache memory by a coherent memory bus. The coherent memory bus is the communication channel for all memory and I O transactions between the processor cores the I O Bridge IOB and the Level 2 cache and controller A.

The level 2 cache memory controller maintains memory reference coherence. It returns the latest copy of a block for every fill request whether the block is stored in the L2 cache in DRAM or is in flight. It also stores a duplicate copy of the tags for the data cache in each processor core . It compares the addresses of cache block store requests against the data cache tags and invalidates both copies a data cache tag for a processor core whenever a store instruction is from another processor core or from an I O component via the I O Bridge .

Example embodiments of the present invention implement features outside of the core which identify and provide control for the core. These features include assignment of a VMID to each core. Specifically the VMID is appended to all the memory and IO requests initiated by core. An interrupt may be generated by the external logic if a request generated by the core has an assigned VMID that is different from the VMID of the core.

The level two cache is shared by all of the CPU cores instructions data and hardware device direct memory accesses. The level two cache can be partition bypassed on a reference by reference basis. The input output I O devices can have direct memory access into the level two cache. Example embodiments of the present invention implement features for assignment of level two cache partitions. For example exclusive partitions of the cache may be assigned to each virtual system on processor. The number of level two cache partitions depends on the number of virtual systems on processor supported level of quality of service and the ability to set the quality of service on a virtual systems on processor basis. Each work request includes a VMID and DRAM access is restricted based on the VMID. In some embodiments the VMID is 6 bits. Some embodiments allow for a programmed maximum number of VMIDs. The granularity of DRAM protection may be found as a function of maximum number of VMIDs NVID and a programmed level two cache dram size SZD . For example in some embodiments the granularity of the DRAM protection may be found using 

Given that all memory accesses from cores input output blocks hardware assist engines need to pass through the level two cache the level two cache serves as a central location for all memory accesses inside a system on chip. Therefore a level two cache controller can be expected to control all memory access.

Certain embodiments minimize the required number of entries needed to be maintained in the level two cache controller per VMID access by increasing the granularity.

In certain embodiments the size of number of bits stored in the level two cache controller for access control may be fixed so that a table of access rules may be maintained on the chip. Each stored bit represents a particular location in the DRAM and indicates whether a particular VMID has access i.e. read or write access to that location. Since the bit size is fixed and number of virtual systems on chip and actual external physical DRAM size may vary from system to system the granularity of access control for each VMID may be varied using the above granularity equation.

The processor may also include application specific co processors that offload the processor cores so that the processor achieves high throughput. For example the DFA module includes dedicated DFA engines to accelerate pattern and signature match necessary for anti virus AV Intrusion Detection Systems IDS and other content processing applications at up to 4 Gbps.

The I O Bridge IOB manages the overall protocol and arbitration and provides coherent I O partitioning. The IOB includes a bridge A and a Fetch and Add Unit FAU B. Registers in the FAU B are used to maintain lengths of the output queues that are used for forwarding processed packets through the packet output unit . The I O bridge includes buffer queues for storing information to be transferred between the I O bus coherent memory bus the packet input unit and the packet output unit .

The high performance I O bridge IOB may also be configured to support virtualization as a function of VMID. The IOB adds a VMID to each request and restricts IO access as a function of VMID.

The Packet order work POW module queues and schedules work for the processor cores . Work is queued by adding a work queue entry to a queue. For example a work queue entry is added by the packet input unit for each packet arrival. The timer unit is used to schedule work for the processor cores .

Processor cores request work from the POW module . The POW module selects i.e. schedules work for a processor core and returns a pointer to the work queue entry that describes the work to the processor core .

The packet order and work POW register may also be configured to schedule work requests as a function of the VMIDs. For example the number of POW queues may be increased to handle multiple virtual systems on processor. The actual number of the queues depends on the number of virtual systems on processor that need to be supported. Additionally each virtual system on processor may be assigned an exclusive configuration and status register CSR to allow for POW access for regular functionality. The POW distributes the work requests based on VMID to corresponding virtual systems on processor. In certain embodiments the number of input queues may be increased to 128 or higher to meet the needs of virtual systems on processor.

After the packet has been processed by the processor cores a packet output unit PKO reads the packet data from memory performs L4 network protocol post processing e.g. generates a TCP UDP checksum forwards the packet through the GMX SPC unit and frees the L2 cache DRAM used by the packet.

The processor may further include a miscellaneous input output unit including a central interrupt unit a USB 2.0 a compression decompression unit a RAID De Dup unit a timer unit a random number generator and a key memory .

While this invention has been particularly shown and described with references to example embodiments thereof it will be understood by those skilled in the art that various changes in form and details may be made therein without departing from the scope of the invention encompassed by the appended claims.

It should be understood that elements of the block flow and network diagrams described above may be implemented in software hardware or firmware. In addition the elements of the block flow and network diagrams described above may be combined or divided in any manner in software hardware or firmware. If implemented in software the software may be written in any language that can support the embodiments disclosed herein. The software may be stored on any form of non transitory computer readable medium such as random access memory RAM read only memory ROM compact disk read only memory CD ROM and so forth. In operation a general purpose or application specific processor loads and executes the software in a manner well understood in the art.

