---

title: Systems and methods of vector-DMA cache-XOR for MPCC erasure coding
abstract: System and method embodiments are provided for managing storage systems. In an embodiment, a network component for managing data storage includes a storage interface configured to couple to a plurality of storage devices; and a vector-direct memory access (DMA) cache-exclusive OR (XOR) engine coupled to the storage interface and configured for a multiple parities convolution codes (MPCC) erasure coding to accelerate M parities parallel calculations and the erasures cross-iterations decoding, wherein a single XOR-engine with caches and a vector-DMA address generator is shared by the MPCC erasure coding engine for pipelining external dual data rate (DDR4) memory accesses, where M is a positive integer greater than two.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09571125&OS=09571125&RS=09571125
owner: Futurewei Technologies, Inc.
number: 09571125
owner_city: Plano
owner_country: US
publication_date: 20141002
---
The present application claims the benefit of U.S. Provisional Patent Application No. 61 886 480 filed Oct. 3 2013 and entitled System and Method of Vector DMA cache XOR for MPCC Erasure Coding which is incorporated herein by reference as if reproduced in its entirety.

The present invention relates to a system and method for data storage clusters and in particular embodiments to a system and method of vector DMA cache XOR for m Parities Convolution Codes MPCC erasure coding.

The use of 3 parity convolution codes for digital tape track error check correction generally was developed in 1978. 5 parity data erasure recovery by 5 parallel exclusive or XOR engines generally was developed in 2004.

The 5 parallel independent Random Array of Independent Discs RAID 5 kind XOR engines use a lot of silicon die area for five dedicated 1 megabyte MB first in first out FIFO buffers and ten Direct Memory Access DMA engines in order to reach better Hard Disk read performances but the five XOR engines conflict at the same external memory bus then they cannot be efficiently utilized. Also it is not programmable and does not work for more than five erasures. Many RAID5 6 controller chips have four XOR engines in parallel but they are idle in most of time waiting for data to be ready by the host central processing units CPUs especially in cluster EC usages.

In an embodiment a network component for managing data storage includes a storage interface configured to couple to a plurality of storage devices and a vector direct memory access DMA cache exclusive OR XOR engine coupled to the storage interface and configured for a multiple parities convolution codes MPCC erasure coding to accelerate M parities parallel calculations and the erasures cross iterations decoding wherein a single XOR engine with caches and a vector DMA address generator is shared by the MPCC erasure coding engine for pipelining external dual data rate DDR4 memory accesses where M is a positive integer greater than two.

In an embodiment a method for utilizing programmable vector direct memory access DMA cache exclusive or XOR engine VDXe for convolution erasure coding includes determining with the VDXe a plurality of parities in a round robin fashion by reading N fragments from one local buffer and N 1 fragments received from a remote direct access rDMA packet scatter around a plurality of buffer pools and pointed to by a plurality of rDMA scatter gathering SG chains where N is a positive integer usually greater than M and recovering data loss with the VDXe in a redundant array of independent disk RAID storage clusters or geo distributed sites in the clouds by a zero copy packet by packet exclusive OR XOR operation on the received rDMA packets pointed to by the SG chains.

In an embodiment a method for utilizing programmable vector direct memory access DMA cache exclusive or XOR hardware for convolution erasure coding includes computing by the vector DMA cache XOR hardware parities for the convolution erasure encoding computing by the vector DMA cache XOR hardware syndromes for the convolution erasure decoding and performing by the vector DMA cache XOR hardware cross syndrome iterations to recover rebuild erasure lost data in a unit by unit fashion rather than a block by block of RAID5 6 XOR engine and the processing unit is a cache line of 16 bytes or double cache line of 32 bytes.

The making and using of the presently preferred embodiments are discussed in detail below. It should be appreciated however that the present invention provides many applicable inventive concepts that can be embodied in a wide variety of specific contexts. The specific embodiments discussed are merely illustrative of specific ways to make and use the invention and do not limit the scope of the invention.

Disclosed herein in an embodiment is a Vector Direct Memory Access DMA XOR for MPCC Erasure Coding engine VDXe design to accelerate the M parities parallel calculations and M erasures cross iteration decoding by sharing a single XOR engine with caches and a Vector DMA engine for fast computation pipelining external double data rate DDR dynamic random access memory DRAM DDR3 DDR4 memory accesses and low application specific integrated circuit ASIC cost.

Disclosed herein has an embodiment network component for managing data storage. The network component includes a storage interface configured to couple to a plurality of storage devices and a vector direct memory access DMA cache exclusive OR XOR engine coupled to the storage interface and configured for a multiple parities convolution codes MPCC erasure coding to accelerate M parities parallel calculations and erasures cross iterations decoding wherein a single XOR engine with caches and a vector DMA address generator is shared by an MPCC erasure coding engine for pipelining external dual data rate DDR4 memory accesses where M is a positive integer greater than two. In an embodiment the network component includes a processor a level 1 L1 cache a level 3 L3 cache a zero copy receiver parsing component an L1 vector DMA controller and a packet spread transmitter. In an embodiment the vector DMA cache XOR engine is configured to perform zero copy packet by packet XOR operations to recover data loss in the storage devices. In an embodiment the vector DMA cache XOR engine is configured to write a plurality of data blocks to external dual in line memory modules DIMMs by reading at least some of the storage devices. In an embodiment the vector DMA cache XOR engine is configured to send a plurality of data blocks to other network components by reading the DIMMs. In an embodiment the vector DMA cache XOR engine is configured to receive data packets from other network components and write the data packets to the DIMMS with an remote direct memory access rDMA scatter gathering SG chains. In an embodiment the network component includes a level 1 L1 vector DMA controller and address generator with a map that builds all offset addresses with proper cross correlations among the data wherein the linear map supports non equal length parities. In an embodiment the network component includes a non liner map controlled by firmware that supports equal length parities. In an embodiment the vector DMA cache XOR engine comprises a level 3 L3 cache that pre fetches two data packets in a round robin fashion from each data block. In an embodiment the vector DMA cache XOR engine includes an XOR Register that reads buffers 1 cache line at a time and an XOR unit that computes a plurality of parities writes to L1 cache as 2 times the number of parities times the number of storage device read write operations. In an embodiment the vector DMA cache XOR engine includes a plurality of XOR units operating in parallel and that writes to L1 cache read write operations equivalent to the number of XOR units operating in parallel.

Disclosed herein is an embodiment method for utilizing programmable vector direct memory access DMA cache exclusive or XOR engine VDXe for convolution erasure coding. The method includes determining with the VDXe a plurality of parities in a round robin fashion by reading N fragments from one local buffer and N 1 fragments received from an rDMA packet scatter around a plurality of buffer pools and pointed to by a plurality of rDMA scatter gathering SG chains where N is a positive integer and recovering with the VDXe data loss in a redundant array of independent disk RAID storage cluster by a zero copy packet by packet exclusive OR XOR operation on the received rDMA packets pointed to by the SG chains. In an embodiment recovering the lost data includes parsing all received rDMA packets pointed to by the SG chains as a zero copy Rv and XOR computing the received rDMA packets in data fragments in a round robin fashion. In an embodiment each of a plurality of nodes writes a plurality of equal sized data blocks to external dual in line memory modules DIMMs by reading the RAID storage cluster. In an embodiment the method also includes generating offset addresses to fetch data for a plurality of XOR operations cache line by cache line. In an embodiment the method uses shared caches to perform XOR operations. In an embodiment the method includes pausing XOR operations of multi threading tasks in response to a pause or an end command and saving all the shared caches to external memories for future continuation of the XOR operations. In an embodiment the shared caches include at least two of address cache scratch cache and input cache. In an embodiment a done code causes all read write threads to close and releases all the shared cache resources. The RAID storage cluster can be geo distributed site cloud storage clusters. In an embodiment determining the plurality of parities includes performing a vector DMA for vector memory iterations without a digital processor.

Disclosed herein is an embodiment method for utilizing programmable vector direct memory access DMA cache exclusive or XOR engine VDXe for convolution erasure coding. The method includes computing by the vector DMA cache XOR hardware parities for the convolution erasure coding computing syndromes for the convolution erasure coding and performing cross syndrome iterations to recover erasure lost data. In an embodiment the method includes pausing the cross syndrome iterations and saving all cache data into external memories for future continuation of cross syndrome iterations. In an embodiment the method includes spreading N 1 of 1 megabyte MB remote direct memory access rDMA sequential block transfers into 128 interleaved 8 kilobyte KB packets scattering among N 1 or more of 1 MB rDMA block transfer threads to stream emulate concurrent rDMA transfers whereby a buffer size at rDMA receiver nodes for cache exclusive or XOR is reduced. In an embodiment the method also includes a programmable DMA cache XOR engine VDXe that builds a fine scatter of 128 8 KB rDMA scatter gathering SG chains effectively as 128 of concurrent rDMA transfers to support the cache XOR without a need for large buffers.

Cloud storage erasure coding EC clusters generally have the following system requirements. For clusters there are at least N 6 cloud storage nodes N 24 or more . There are 1 MB block buffers for better hard disk drive HDD access performances. There are 1000 or more concurrent read threads per cluster node. For six erasures there are six parities to fix six failures of HDDs cluster nodes or Data Center Ethernet DCE links. For XOR operations there are six 1 MB accumulative XOR engines to process N of 1 MB data sequentially for fewer buffers. For a client to write data into a node this node XOR computes 6 or M parities from the N data blocks then rDMA writes distributes them into N M nodes and calls the metadata server to record new entries of data segments to read from a node to egress data blocks this node calls the metadata server to setup all related nodes to egress data packets from each good nodes to the client orderly x good nodes with M parities blocks are ready for rebuilding failed data blocks caused by either HDDs failures or networking troubles with proper rDMA queue pairs and VDXe SG chains where parities nodes are rotated as RAID5 fashion for load balancing. More external memories could support more threads.

The current RAID5 XOR engine is by accumulative XOR with a fixed size FIFO 64 KB plus DMA read and DMA write engines. The intermediate XOR result block is held in the FIFO then XOR computed with input data blocks 64 KB one by one accumulatively. Then five such XOR engines in parallel can compute five parities for encoding plus a complicated DSP iterative decoder for five erasure recoveries. It introduces a lot of data movements from small FIFOs to from external memories to support the N of 1 MB sequential cluster data blocks from N 1 different cluster nodes.

There are various issues with the above approaches. FIFO based XOR engines cannot support 1000 threads of N sequential 1 MB data blocks by remote direct memory access rDMA transfers cross N cluster nodes then XOR operations ops among those data. The accumulated XOR causes a big performance penalty with external DIMMs to emulate 1 MB FIFOs. To fix six of 1 MB erasure blocks there are two kind external DIMMs accesses to prepare 24 of 1 MB data and to accumulative XOR 6 of 1 MB as follows 

 a 24 block write wr from HDDs to DIMMs 23 read rd and 23 write by NIC Tx Rv to and from other 23 nodes 2 23 local DMA to parse randomly arrived packets into 23 1 MBs in DIMMs.

 b 24 block 2 rd wr 6 parities 288 times external DIMMs reads writes per 6 of 1 MB EC rebuilds plus 24 4 23 116 times reads writes by a ops.

 c 8 GB sec external DIMMs can only support 8 GBps 288 116 about 10 MBps EC performance if only 50 of the external DIMMs bandwidth is allocated for EC rebuilding usages.

 d Meanwhile most of 6 XOR engines hardware are in idle to wait for external DIMMs bus. An ARM NEON firmware solution puts the CPU core in idle to support single instruction multiple data SIMD XOR vector operations. The hardware capacity is wasted in both of current methods as waiting external DIMMs for data.

A vector direct memory access vector DMA with bus caching mechanism has been used in high performance multi core digital signal processors DSPs . A vector DSP may use a vector DMA cache scheme between the external memory bus and the vector memory.

An embodiment provides a vector DMA cache XOR for a multiple parities convolution codes MPCC erasure coding engine to accelerate the M parities parallel calculations and the M erasures cross iterations decoding by sharing a single XOR engine with caches and a vector DMA address generator for low cost fast computation pipelining external dual data rate DDR4 memory accesses.

An embodiment vector DMA cache XOR engine includes a single DMA and XOR module plus address cache scratch cache input caches for efficient burst accessing the external DDR4 memories or packet by packet in storage clusters. An embodiment has three operation modes to compute the M parities M checksums and M erasure iterations recovery.

vector DMA L1 cache vector DMA control and address generators to support two concurrent 24 XOR in 8 KB fragments by rDMA scatter gathering SG chains.

cache XOR L1 cache for 6 8 KB parities 24 reads and 6 writes to external DIMMs. L3 cache for two 24 8 KB data packets with rDMA SG chains.

zero copy Rv to parse all received rDMA pkts pointed by SG chains from 23 nodes as zero copy Rv then to XOR them in 8 KB fragments round robin fashion such that XOR workflow replaced old local DMA parsing randomly arrived 8 KB packets into 23 of 1 MB buffers ops.

Packet spreading Tx scatters twenty three of the 1 MB sequential rDMA packet streams 1 MB by 1 MB ops into 128 of 8 KB packets interleaved from twenty three of 1 MB buffers in round robin fashion to minimize buffer requirements at rDMA receiver nodes in order to make cache XOR possible with much less external memory accesses. It is also possible to synchronize rDMA Tx SG chains with the same source destination addresses among 128 properly spread 8 KB packets thread by thread.

 a each node writes 1000 of 1 MB to its external DIMMs by reading related HDDs and most of those data packets are delivered to clients directly.

 b it rDMA writes x N M of 1000 data blocks to other x nodes holding good parity blocks by reading DIMMs as 8 KB per packet where x is less or equal M of faulty nodes.

 c it may also receives rDMA pkts from other 23 nodes then writes them to DIMMs with properly built rDMA SG chains if it holds related good parity blocks.

 d as long as one set 24 1 MB data is ready vector DMA can be initialized then fired up to compute M parities or one set 24 8 KB packets segments to compute syndromes then to iterate rebuild failed data unit by unit.

 e L1 vector DMA controller and address generator with map builds all offset addresses with proper cross correlations among 24 data. A linear map will support non equal length parities and special non liner map controlled by firmware can support equal length parities to eliminate the drawbacks in the pre arts.

 g XOR Regx reads buffers 1 cache line XOR unit computes six parities writes to L1 cache as 2 6 24 rd wr ops it reduces to only six wr ops by 6 XOR units in parallel.

A current DMA XOR controller has N source address registers pointers ptrs to sequentially read 1 MB data one by one from N ptrs to accumulate one of 1 MB parity. It also has one length register one count register to generate the offset address that is shared by all N addresses and it has one destination address to write the final accumulated parity in the FIFO to external DIMMs.

 a one XOR engine computes 6 parities in round robin by reading N of 8 KB fragments from one local 1 MB buffer and N 1 fragments that are received rDMA packets scattered around all buffer pools and pointed by 23 rDMA scatter gathering chains .

 b L1 cache vector DMA controller and address generator will hold following data structures of data source ptrs and destination parity ptrs then dynamically generate all offset addresses with map to fetch 8 KB data for six parity XOR ops cache line by cache line. One data unit is XOR by six parity units with proper cross correlation offsets from different 8 KB partial parity fragments.

 c L 1 MB for output parity buffers Lto Lare aggregated from each SG chain with fragment size Sin 8 KB increment only one 1 MB bufis for local data .

 d L3 data cache supports one data read and L1 XOR cache will handle six reads and six writes for six parity XOR ops.

 e L3 data cache could pre fetch at least two 24 8 KB data fragments to support six concurrent parity XOR ops.

In this parity XOR mode M parities are computed sequentially unit by unit 256 bit unit upon memory bus and cache line from N data inputs as shown in . Each of N inputs is loaded into data caches in 1 KB 9 KB programmable packets associated with a DMA descriptor chain that includes M parities destination addresses and size of burst write to external memories the sequence of input data packets source address with burst read size and time spacing among parities. The data packet size is up on the Data Center Ethernet DCE switch fabric as 1.5 KB 8 KB or 9 KB.

 a L1 cache vector DMA address generator will hold data structures of data source ptrs and syndrome ptrs 6 syndrome ptrs point to 6 8 KB temporary tmp circular buffers after every 6 of 8 KB syndromes done cross iteration is called.

 b L3 data cache fetches 24 8 KB data fragments there may be 1 6 data erasures with 1 6 parities even piecewise erasures such that a pbuf 1 set to skip Serasure bytes and next null marked for end of SG chain where 8 KB S Lmarked for syndrome mode.

 c Firmware sets up all addresses zeros tmp buffers loads initial values then starts 24 8 KB cache XOR ops in the following fashion.

 d syndrome XOR iteration XOR write 6 8 KB rebuilt data fragments to external DIMMs update all addresses repeat step d until reach end of SG where all vector DMA data fetch and cache XOR ops are the same as in parity XOR mode. However parity XOR DMA descriptors are circular chains up to 24 1 MB data decoder DMA descriptors are interleaved for syndrome XOR then iteration XOR ops.

In this syndrome XOR mode x checksums or syndromes are computed sequentially from N x data inputs where x indicates the missing data inputs as shown in . The DMA descriptor chains include x cache addresses pointing to scratch cache area with a mapping vector to mask out the missing parities the sequence of input data packets source addresses with burst read size and time spacing among parities. The x will be equal to 1 in most cases and x 6 for worst case of rebuilding six erasure failures.

 c one difference from the previous two modes is the XOR source and destination partners to the same 6 8 KB scratch buffers with proper iteration partners to previous iteration results setup by firmware or application programming interfaces APIs as map cross correlations among N x good data and x parities by the XOR initialization process.

 d after one round of syndrome XOR and iteration XOR ops the vector DMA controller copies the 6 8 KB fragments of rebuilt data to the final 6 1 MB data buffers in external DIMMs.

 e this hardware design uses all 1 MB buffers from common buffer pools shared by HDD reads writes rDMA Tx Rv buffer rings and vector DMA cache XOR engine.

In this erasure iteration mode the x syndromes generated by syndrome XOR can be further iterated unit by unit with map cross correlations among syndromes as shown in . The DMA descriptor chains include x recovered erasures destination addresses and size of burst write to external memories followed by x 1 iteration spacing values then sequence of checksum descriptor chains. The VDXe reads x 1 syndromes to XOR iterate out 1 recovered erasure unit erasure by erasure where each erasure unit is cache line.

 2 The shared caches are address cache scratch cache input caches for efficient burst accessing external DDR4 memories.

 3 Support for zero copy packet by packet XOR operations to recover the storage clusters and cloud storage erasures.

 4 Support packet spreading Tx for rDMA to scatter out pkts to other N 1 storage clusters or cloud nodes to emulate 128 or 256 concurrent big rDMA transfers and effectively reduce the buffer size in receiver ends.

 5 . There are 3 cross correlation tables map for M parities map for M syndromes and map for M iteration rebuilt data. The simple linear cross correlation maps are for non equal length parities and non linear maps can support equal length parities.

 7 VDXe s descriptor chain can be ended by a pause code or a done code to support multi tasking operations. In the pause case the hardware saves all related VDXe control registers and locks to the related partial parity memories in external DDR4 for future continuing computations. In the done case VDXe will close all the read write threads and release all related resources.

An embodiment utilizes a vector DMA XOR for convolution erasure coding. An embodiment provides fast computation performances with cache management. An embodiment provides lower cost with less die size. An embodiment utilizes hardware accelerating the M parities M checksums M iterations parallel computations for MPCC convolution erasure coding. An embodiment provides a single vector DMA XOR engine with programmable N M for N data streams sources and redundant M parities checksums. Embodiments may be implemented in storage clusters data centers solid state disk SSD HDD erasure controllers virtual storage appliances cloud storages disaster recovery storage solutions and the like.

The bus may be one or more of any type of several bus architectures including a memory bus or memory controller a peripheral bus video bus or the like. The CPU may comprise any type of electronic data processor. The memory may comprise any type of system memory such as static random access memory SRAM dynamic random access memory DRAM synchronous DRAM SDRAM read only memory ROM a combination thereof or the like. In an embodiment the memory may include ROM for use at boot up and DRAM for program and data storage for use while executing programs.

The mass storage device may comprise any type of storage device configured to store data programs and other information and to make the data programs and other information accessible via the bus . The mass storage device may comprise for example one or more of a solid state drive hard disk drive a magnetic disk drive an optical disk drive or the like.

The I O interface may provide interfaces to couple external input and output devices to the processing unit . The I O interface may include a video adapter. Examples of input and output devices may include a display coupled to the video adapter and a mouse keyboard printer coupled to the I O interface. Other devices may be coupled to the processing unit and additional or fewer interface cards may be utilized. For example a serial interface such as Universal Serial Bus USB not shown may be used to provide an interface for a printer.

The antenna circuit and antenna element may allow the processing unit to communicate with remote units via a network. In an embodiment the antenna circuit and antenna element provide access to a wireless wide area network WAN and or to a cellular network such as Long Term Evolution LTE Code Division Multiple Access CDMA Wideband CDMA WCDMA and Global System for Mobile Communications GSM networks. In some embodiments the antenna circuit and antenna element may also provide Bluetooth and or WiFi connection to other devices.

The processing unit may also include one or more network interfaces which may comprise wired links such as an Ethernet cable or the like and or wireless links to access nodes or different networks. The network interface allows the processing unit to communicate with remote units via the networks . For example the network interface may provide wireless communication via one or more transmitters transmit antennas and one or more receivers receive antennas. In an embodiment the processing unit is coupled to a local area network or a wide area network for data processing and communications with remote devices such as other processing units the Internet remote storage facilities or the like.

The following references are related to subject matter of the present application. Each of these references is incorporated herein by reference in its entirety 

While this invention has been described with reference to illustrative embodiments this description is not intended to be construed in a limiting sense. Various modifications and combinations of the illustrative embodiments as well as other embodiments of the invention will be apparent to persons skilled in the art upon reference to the description. It is therefore intended that the appended claims encompass any such modifications or embodiments.

