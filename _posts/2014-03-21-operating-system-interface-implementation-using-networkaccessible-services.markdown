---

title: Operating system interface implementation using network-accessible services
abstract: Methods and apparatus for operating system interface implementation using network-accessible services are described. A request to execute a particular program at a distributed computing platform implementing a set of operating system interfaces using resources of network-accessible services of a provider network is received. A compute instance of a computing service is selected for executing operations of a thread of the program. Corresponding to the invocation of a particular operating system interface within the program, at least one operation is performed at a particular service. An overall result of execution of the particular program is determined based at least in part of results of the at least one operation.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09413819&OS=09413819&RS=09413819
owner: Amazon Technologies, Inc.
number: 09413819
owner_city: Reno
owner_country: US
publication_date: 20140321
---
Many companies and other organizations operate computer networks that interconnect numerous computing systems to support their operations such as with the computing systems being co located e.g. as part of a local network or instead located in multiple distinct geographical locations e.g. connected via one or more private or public intermediate networks . For example data centers housing significant numbers of interconnected computing systems have become commonplace such as private data centers that are operated by and on behalf of a single organization and public data centers that are operated by entities as businesses to provide computing resources to customers. Some public data center operators provide network access power and secure installation facilities for hardware owned by various customers while other public data center operators provide full service facilities that also include hardware resources made available for use by their customers.

The advent of virtualization technologies for commodity hardware has provided benefits with respect to managing large scale computing resources for many customers with diverse needs allowing various computing resources to be efficiently and securely shared by multiple customers. For example virtualization technologies may allow a single physical computing machine to be shared among multiple users by providing each user with one or more virtual machines hosted by the single physical computing machine. Each virtual machine may be considered a software simulation acting as a distinct logical computing system that provides users with the illusion that they are the sole operators and administrators of a given hardware computing resource while also providing application isolation and security among the various virtual machines. Furthermore some virtualization technologies are capable of providing virtual resources that span two or more physical resources such as a single virtual machine with multiple virtual processors that spans multiple distinct physical computing systems.

Taken together the establishment of large scale distributed computing environments and virtualization technologies theoretically enable very high levels of scalability availability and data durability for various types of applications. However in many cases in order to maximize the benefits obtained from deployment in the large scale virtualized network environment applications may have to be re designed with such distributed architectures in mind and or may require configuration by administrators who are well versed in various advanced features of the distributed environment. Non trivial programming effort and or configuration expertise may sometimes be required to enable programs that utilize application programming interfaces of traditional operating systems originally designed with uniprocessors or relatively small scale multiprocessors as target execution environments to work well in large scale networks employing modern virtualization technologies.

While embodiments are described herein by way of example for several embodiments and illustrative drawings those skilled in the art will recognize that embodiments are not limited to the embodiments or drawings described. It should be understood that the drawings and detailed description thereto are not intended to limit embodiments to the particular form disclosed but on the contrary the intention is to cover all modifications equivalents and alternatives falling within the spirit and scope as defined by the appended claims. The headings used herein are for organizational purposes only and are not meant to be used to limit the scope of the description or the claims. As used throughout this application the word may is used in a permissive sense i.e. meaning having the potential to rather than the mandatory sense i.e. meaning must . Similarly the words include including and includes mean including but not limited to.

Various embodiments of methods and apparatus for operating system interface implementation using network accessible services supported by provider networks are described. Networks set up by an entity such as a company or a public sector organization to provide one or more services such as various types of multi tenant and or single tenant cloud based computing or storage services accessible via the Internet and or other networks to a distributed set of clients may be termed provider networks in this document. The term multi tenant may be used herein to refer to a service that is designed to implement application and or data virtualization in such a manner that different client entities are provided respective customizable isolated views of the service such that one client to whom portions of the service functionality are being provided using a given set of underlying resources may not be aware that the set of resources is also being used for other clients. A provider network may support single tenant services such as for private cloud implementations in some embodiments either in addition to or instead of multi tenant services. A given provider network may typically include several large data centers hosting various resource pools such as collections of physical and or virtualized computer servers storage devices networking equipment and the like needed to implement configure and distribute the infrastructure and services offered by the provider.

According to some embodiments at least a subset of the resources and services of a provider network may be configured as a distributed computing platform DCP specifically targeted for executing programs that include invocations of various operating system interfaces in a transparent manner so that the programs themselves do not have to be re written to take advantage of the various services of the provider network. An analogy to the way programs are conventionally compiled and executed may help to explain the manner in which the DCP may be used in such embodiments. A given program P may for example be written to conform to a portable operating system interface standard such as a particular version of POSIX. Thus P may include invocations of various OS APIs operating system application programming interfaces or system calls such as memory management system calls storage input output system calls process management or thread management system calls signaling system calls networking system calls and the like defined in the standard. In addition to the operating system calls of course P may include various computations calls to other libraries and applications and so on. The program may have originally been written in accordance with the standard so that it can be compiled for and run at various different processing platforms without requiring changes to the program s source code. Just as respective executable versions of P may be compiled for various traditional uniprocessor and multi processor architectures one or more executable versions of P may be generated for execution using resources of the DCP in some embodiments. When a particular thread of P is to be scheduled for execution at a traditional platform e.g. at program startup or as a result of a fork one or more hardware cores or processors of the traditional platform may be identified for the thread. When a particular thread of P is to be scheduled at the DCP one or more compute instances e.g. virtual machines of a computing service implemented at the DCP may be configured for use by the thread e.g. either by selecting from a pool of pre configured instances or by launching new instance s and then the operations of the thread may be performed using the CPUs or cores allocated to the compute instance or instances. In some cases operations of a single thread of the program may be distributed among several different compute instances e.g. in order to achieve higher scalability than would have been possible if only a single instance were used. The selection of the compute instance or instances may be orchestrated or controlled by an operating system service OSS implemented for the DCP at the provider network in at least some embodiments. In at least one embodiment a client on whose behalf a particular program is to be executed at the DCP may provide an indication of a resource selection policy to the OSS which may be used by the OSS to select specific resources from among the wide variety of resources available for the program. For example a resource selection policy may indicate that the client would prefer to utilize to the extent possible compute instances of a certain performance capability or price range for the program or that the client would prefer to utilize one particular storage service rather than another for storage related operations of the particular program. Clients may not be required to indicate such policies in some embodiments and the OSS may instead utilize various heuristics e.g. a heuristic according to which resources available nearby a compute instance at which a thread of the program is run are generally to be used in preference to remote resources to select specific resources if no client provided policy is available. In some embodiments in which a computing service is implemented at the provider network the OSS may comprise a portion of the back end administrative infrastructure of the computing service which may also be referred to as the control plane of the computing service.

An OSS may be configured to dynamically modify the set of resources such as the number of compute instances or the type of compute instances deployed for a given program in at least some embodiments to implement an automated scaling policy associated with the program. From the perspective of the client on whose behalf the program P is run at the DCP the OSS and or the other services of the provider network may be largely or fully transparent e.g. apart from requesting the compilation and or execution of the program at the DCP and in some cases optionally indicating resource selection policies or limits on resources or budgets as described below the client may not need to interact very much with the OSS or other services. In general the OSS may be considered responsible for translating at least some of the various operating system interfaces invoked by P into corresponding invocations of service programmatic interfaces of the provider network. The relationship between the operating system interfaces and the provider network service interfaces may at least in some cases be more complex than a simple 1 1 mapping for example to implement operations corresponding to a given storage read or write operation of P more than one service API call to one or more storage related services may be required. In addition in at least some embodiments the mappings between the operating system calls and the service API calls may be dynamic rather than static e.g. depending on the state of P or on the workload levels at various resources of the provider network a different set of service interfaces may be used at time T for a given operating system call invocation than is used for the same operating system call invocation at time T.

When a storage related system call is invoked in the program P as it executes at a particular compute instance at least in some embodiments the OSS may have to determine which storage resources of the provider network are to be used for the corresponding operation or operations. At a conventional computing platform several levels of a storage hierarchy may be available to implement a storage operation corresponding to a system call invocation such as various processor caches a volatile main memory storage device caches non volatile storage devices and so on. At the DCP at least in some embodiments a much richer set of choices may be available for storage operations. For example in addition to the hierarchy levels available on a given compute instance which may correspond to those listed above for a conventional platform storage devices of various types of storage services and or database services may be available. In one embodiment for example a block storage service of the provider network may enable access to block storage devices from compute instances such as mountable block level volumes that implement block device programmatic interfaces for I O. Another storage service may support access to unstructured storage objects accessible via web services interfaces rather than block device interfaces. One or more relational or non relational database services and or distributed caching services may also be available for storage related operations in the provider network in at least some embodiments. Any of the various elements of the local storage hierarchy at the compute instances the storage services database services caching services and the like may be used for read and or write operations corresponding to storage related system calls in the program P.

Operations corresponding to networking related system calls invoked by program P may be performed using any of a variety of networking resources of the provider network in various embodiments. For example in one embodiment a particular network address such as an Internet Protocol IP address may be assigned for P by the OSS. As noted above in at least some cases more than one compute instance may be used to perform P s operations. The OSS may initiate or perform the necessary networking configuration to ensure that for any networking transfers directed to P or originating from P the assigned network address is used as the source or destination. The configuration operations may involve for example assigning the network address to one or more virtual or physical network interfaces attached to one or more compute instances used for P making routing changes or activating a redirection mechanism at one or more networking components of the provider network.

In various embodiments synchronization or locking mechanisms may be required to implement some types of program operations as implemented in the DCP. For example a single memory allocation operation invoked from the program running on several different compute instances of the DCP may in some cases result in the allocation of respective portions or blocks of virtual memory at a plurality of compute instances by a distributed memory management component of the OSS. In such a scenario a concurrency control mechanism may be required e.g. to prevent inadvertent overwriting of the allocated memory object from multiple compute instances or to ensure that any one compute instance does not read a partially written value generated by another instance. In some embodiments one or more locking services implemented within the provider network may be used for concurrency control or thread synchronization.

In at least some embodiments not all the operating system interfaces supported at a traditional operating system or defined in a standard may necessarily be implemented using provider network resources. In one such embodiment for example a subset C of operating system calls defined in the standard may be translated or mapped to service API invocations while another subset C of the standard s system calls may be implemented using conventionally acquired local resources e.g. local CPUs or cores local file systems etc. at one or more compute instances . In at least some embodiments while several of the network accessible services used to execute programs at the DCP may be accessible directly by clients for other purposes at least some provider network services may be restricted to internal use only. For example a computing service or a storage service used to schedule threads of a program P or perform I O operations on behalf of P may also be used by clients of the provider network to acquire compute instances and or storage devices for applications unrelated to the use of the DCP. However a locking service used for concurrency control purposes among various compute instances being deployed to execute threads of a program P may not be accessible directly to clients.

In addition to core computing storage and networking services other services of the provider network may also be used to help support operating system interface calls in some embodiments. For example in one embodiment a workflow service may be implemented at the provider network and such a workflow service may be used to schedule pre configuration of compute instances for use by the OSS. A pool of compute instances to be deployed dynamically on an as needed basis to execute programs targeted for the DCP may be established and additional instances may be added to the pool based on workflows implemented using the workflow service. Other examples of services that may be employed for program execution at the DCP may include load balancing services which may be used to ensure that the workload of a program is distributed appropriately among a plurality of compute instances assigned to the program and distributed logging services which may be used to manage log messages generated during execution of a program at several different resources of the provider network .

In at least some embodiments DCP versions of command line and or scripting interfaces typically available at a conventional operating system may be provided in addition to the translations of system calls into service APIs. In one such embodiment for example respective DCP versions of Bash the Bourne Again Shell or ksh the Korn shell and the like may be supported. In one embodiment DCP versions of scripting languages such as Perl AWK or Python may also be supported. In some such embodiments the resources of the provider network may be utilized for various pre existing scripts written using the command line interfaces and or languages without requiring any modifications in addition to being utilized for programs such as P that are specifically compiled for the DCP environment. Thus the scalability theoretically made possible by the provider network s vast resources may become a practical reality achievable by a substantial number of pre existing as well as new scripts and programs without extensive re programming or programmer re training.

In the depicted embodiment the program P may be targeted for execution at a distributed computing platform DCP . The distributed computing platform may implement at least some of the operating system interfaces invoked in P using resources of a plurality of network accessible services of a provider network. The provider network may for example include a variety of computing resources storage resources networking resources as well as various aggregating or configuration services that may select and configure the specific computing storage and or networking services to be used on behalf of various programs similar to P. In particular in some embodiments an operating system service may be implemented to manage resource management for programs such as P targeted for execution at the DCP . The DCP may expose various programmatic interfaces or APIs to the services including the operating system service that manage the computing storage and networking resources such APIs may be referred to herein as DCP APIs . At least a subset of the operating system calls invoked within P such as A and B may in effect be translated into corresponding sets of one or more DCP API calls such as A and B. A given set of DCP API calls in turn may result in the assignment and use of various resources accessible at the DCP to obtain results of the operating system call invocations including selected computing resources storage resources networking resources and so on. The term DCP targeted program may be used herein to refer generally to programs such as P that are to be run in the manner described above i.e. using resources of the provider network to implement operating system primitives invoked within the program. Different sets of resources and or DCP services may be used to perform operations corresponding to respective operating system calls invoked in P e.g. calls A may use computing resources networking resources and aggregating services while calls B may use computing resources storage resources networking resources and aggregating services.

An executable version of program P suitable for the computing resources of the DCP environment may be generated using any of several techniques in different embodiments. In one embodiment a special compiler or a set of compiler flags specific to the DCP may be provided for use by a client or by the programmer responsible for writing the source code for P and the execution of P using the DCP may be initiated at one of the computing resources of the DCP. Compared to the types of compile time optimization opportunities that are typically available for conventional execution platforms a wider variety of compile time or pre compilation optimization opportunities may be available in the DCP context given the vastly larger set of resources and service capabilities that can be used for a given program in such embodiments. In at least one embodiment a client or an administrator of the provider network may specify one or more compiler flags to take advantage of specific features of the DCP for a given program s execution such as extremely high levels of availability or resiliency to infrastructure failures that can be achieved by using resources spread among several different geographically dispersed data centers . In another embodiment a client that wishes to use the DCP for executing a program may simply provide the source code programmatically to the DCP and the DCP may generate the needed executable version. In some embodiments a client may initiate execution of P at a computing platform outside the provider network e.g. within a client owned data center or a client s office premises and a library or execution wrapper program at that platform may interact with various services of the DCP to execute P using provider network resources.

Computing storage networking and other resources of the DCP may be dynamically selected and assigned to perform various subsets or all of the operations of P in various embodiments. For example in one embodiment each thread of execution of P may be mapped to a corresponding computing resource . When P s initial thread is started or when a new thread is created as a result of a fork or similar call the DCP e.g. via an administrative component of its operating system service may identify the specific computing resource to be used and perform the required configuration at the selected computing resource. An executable version of P may for example be loaded from one or more storage resources into one or more memories accessible from the selected computing resources . A file created written and or read by one or more threads of P may be implemented or emulated using storage resources that are accessible over a network from the computing resources and assigned for P s use. Various networking resources of the DCP such as virtual or physical networking interfaces virtual network addresses switches routers gateways and the like may be assigned for use during P s execution. The networking resources may be used for data transfers to from P corresponding to networking calls included in P and for coordination of the various other resources being used for P. In at least some embodiments additional resources may be assigned to P dynamically or resources already in use for P may be freed based on applicable scalability policies and on an analysis of metrics collected. A result of P as a whole which may be based at least in part on the results obtained at the various DCP resources used for the different operating system call invocations may be generated and provided to the client on whose behalf P is executed.

As mentioned above a program P that is compatible with an operating system standard such as POSIX may be run at any of several different execution platforms. illustrates the generation of several different executable versions of a given program including a particular executable version for which resource allocation is managed by an operating system service implemented at a distributed computing platform according to at least some embodiments. As shown compilers A B and C may generate respective executable versions E E and E of the same program P. E may be run at server A that includes one or more processors implementing processor architecture H such as CPUs H H and so on. E may be run at server B that includes one or more processors implementing a different processor architecture H such as CPUs H H etc. E may be run using resources of various services implemented at provider network which together may constitute a distributed computing platform that is compatible with the operating system standard.

The source code of P may be written at a level of abstraction that does not require knowledge of the low level differences in processor or memory implementations between H and H. Similarly the programmer responsible for P need not be aware of the details of various services and resources of the provider network in the depicted embodiment. Just as CPUs or cores may be selected transparently with respect to the programmer to execute various operations of P compute instances such as A B or C of a computing service of the provider network may be selected for E. The compute instances may each comprise virtual machines in some implementations one or more of which may be instantiated at a given hardware server of the computing service called an instance host. A given instance host may implement compute instances for several different clients in some embodiments. In the depicted embodiment operating system service OSS may be implemented at the provider network to orchestrate resource management for E. The OSS may for example comprise its own set of computing devices that are responsible for selecting and configuring compute instances for programs such as P acquiring storage space for P from one or more storage services and so on.

In at least some embodiments networking devices e.g. routers switches gateways or other resources of one or more networking services such as a virtual isolated network service of the provider network may be used for network transfers performed on behalf of P s execution. One or more locking services may be employed for concurrency control operations required during P s execution at the DCP e.g. to coordinate access to shared data from multiple compute instances running respective threads of P. In at least some embodiments the provider network may implement an auto scaling service that may be utilized by the OSS to deploy or decommission resources for P s execution in accordance with a scaling policy. Some or all of the services illustrated in may expose web services APIs to their external i.e. outside the provider network clients and or their internal i.e. within the provider network clients.

As noted above in at least some computing environments shared libraries implementing or acting as wrappers for a variety of commonly used operating system calls may be included as part of a typical operating system installation. For example in some variants of the Unix or Linux operating systems a file called libc.so or lib.so.N where N indicates a version number comprising an implementation of the C standard library or libc may be installed in a well known directory e.g. usr lib of the operating system installation. When a given program is loaded for execution the C standard library may be linked to it and invocations of at least some of the operating system calls within the program may result in the execution of code from the library. In at least some embodiments customized versions of one or more operating system libraries such as libc may be developed and installed to enable the use of the distributed computing platform s services.

As mentioned earlier in at least some embodiments an operating system service OSS may be implemented at a provider network to coordinate resource management at run time on behalf of programs executing at the distributed computing platform. Such an OSS may be implemented using a set of dedicated administrative or control plane resources of the provider network such as computer servers storage devices and the like that may typically not be directly accessible by clients. illustrates example components of an operating system service according to at least some embodiments. Some or all of the OSS components may themselves be implemented in a distributed fashion e.g. using a plurality of software and or hardware components located at one or more data centers of the provider network in at least some embodiments.

As shown the OSS may include an instance pool manager responsible for pre configuring one or more pools of compute instances that can be used for executing programs targeted for the DCP. In some embodiments the instance pool manager may utilize other provider network services such as a workflow service or a job scheduling service to acquire compute instances for the pool or pools and or to release instances from the pool s in response to changing demands for program execution.

In at least some embodiments thread process scheduler may be responsible for determining run time mappings between the program and compute instances e.g. at which specific compute instances various threads of execution of the program are to run and in some cases for determining the number of threads of execution of the program. Depending on the nature of the program and on the performance capabilities of the compute instances available e.g. in the pools populated by the instance pool manager multiple threads may be scheduled for execution at a single compute instance in some embodiments. In some embodiments a given compute instance may be used for threads of more than one program concurrently e.g. compute instances may be used in a multi tenant mode for executing the programs targeted to the DCP while in other embodiments a given compute instance may be restricted to one program at a time.

Distributed memory manager may be configured to determine how memory objects allocated by the program are to be mapped to the respective memories of the one or more compute instances being used for the program execution. For example if a particular data structure DS is to be accessed by two threads T and T of the program with read and or write permissions granted to both threads and the two threads are being run at different compute instances CI and CI the distributed memory manager may have to decide whether DS should be allocated using CI s memory alone CI s memory alone or a combination of CI s and CI s memory. The distributed memory manager may also be responsible for managing concurrency control with respect to the shared data in some embodiments e.g. using a locking service implemented at the provider network or using some other synchronization mechanism. In at least some embodiments non locking concurrency control techniques may be used to coordinate access to memory objects and or storage objects. For example an interleaving technique for update windows may be implemented in one embodiment in which distinct time windows are provided for updates to a memory object from different threads of a program executing at different instances so that no more than one thread is able to modify the memory object at a time. In such a technique a first thread T running at instance I may be allowed to update modify a data structure during a time window W of say a few microseconds or milliseconds a second thread T running at instance may be allowed to update modify the data structure during the next time window W and so on without necessarily employing any locking Such an interleaving technique may be considered analogous to a token passing mechanism in which an update permission token is passed from one thread of the program to another in some selected order and each thread is given the opportunity to update the corresponding object for a short period while the thread has the token. Other non locking concurrency control mechanisms that do not rely on time windows may be employed for coordination of accesses by multiple threads to various memory or storage objects in different embodiments. In at least some embodiments the distributed memory manager may select any of a set of alternative techniques e.g. based on the performance levels achievable by the different techniques and based on the performance requirements of the DCP targeted program to implement memory mapping system calls and shared memory related calls such as calls to mmap shmat and the like in Unix based operating systems.

Distributed storage manager may be responsible for performing read and write I Os on behalf of the program being executed at the DCP in the depicted embodiment. As mentioned above a complex storage hierarchy that includes numerous levels including processor caches of the instance hosts at which the compute instances used for the program are run volatile memory at the instance hosts local non volatile storage devices such as various types of disks network accessible storage devices of various services such as block storage services or unstructured storage services database services archival storage services and the like may be available for storage operations. The distributed storage manager may map various storage objects such as files or volumes accessed by the program to selected levels of the hierarchy select the particular storage devices at which the storage objects are to be stored move the objects between different levels of the hierarchy as needed manage caching of the objects at various levels of the hierarchy and so on.

A networking manager may be configured to perform the networking configuration operations necessary to execute the DCP targeted programs in some embodiments. Such a networking manager may for example obtain one or more IP addresses for a given program configure the necessary virtual and or physical network interfaces at the instances used for the program so that all the instances can receive and transmit traffic with using the IP address es assigned to the program request routing changes configure packet redirector modules and so on. In one embodiment the computing service may assign different IP addresses to each instance host and compute instance by default and redirector modules may be configured within the virtualization management software at various instance hosts and or at network devices to redirect traffic targeted for P to the appropriate set of instances. Inter process communication IPC manager may be responsible for responding to invocations of operating system calls that involve signals semaphores or other IPC primitives by DCP targeted programs in the depicted embodiment. Various IPC operations that may have been implemented entirely within a single system on a conventional platform may require network transfers in the DCP environment and the IPC manager may set up manage the shared use of and dismantle network connections as needed. In some embodiments long term connections may be established between a pair of compute instances being used for DCP targeted programs and the data of several different IPC messages may be transmitted over the same connection.

In at least some embodiments a metrics collector may be used to gather resource utilization throughput response time and other data pertaining to various DCP service API calls employed on behalf of the DCP targeted programs. In many cases more than one implementation approach may be available at the DCP for a given operating system primitive and the metrics collected may be used for example by an optimizer component of the OSS to select the most appropriate implementation. The optimizer may also take into account pricing differences between the different implementations e.g. in view of budget constraints of various clients on whose behalf the DCP targeted programs are run or in view of the service level agreements reached with the clients. In at least some embodiments the optimizer may collect profiling information from a running program e.g. information detailing how much time is spent and which set of resources are used for various sub units of the program and may make placement decisions for the program based on the profiling information. For example one or more of the program s threads may be migrated to a different compute instance with a faster set of processors or a decision to use a different type of storage device for objects read written by the program may be made based on the profiling results.

An availability manager component may be configured to monitor the health of the various resources used for a particular program execution and to replace or restart resources that have failed or are no longer responsive in the depicted embodiment. For example if N compute instances are configured to execute respective threads of the program the availability manager may use a heartbeat mechanism to monitor the status of the N instances. If one or more of the instances becomes unresponsive or unreachable replacement instances may be configured. In some implementations the compute instances may periodically save their state at a persistent storage device e.g. at a storage service of the provider network to enable quick recovery or replacement after a failure. In some embodiments the OSS may also comprise a log manager that collects various log messages from different resources e.g. compute instances storage service nodes networking nodes and the like that are involved in executing the DCP targeted programs. A consolidated view of log messages collected from various resources may be provided in some embodiments which may be helpful in debugging or for performance analysis.

In at least some embodiments different pricing policies may be applied to DCP targeted programs than to other programs executed using the provider network s resources e.g. a premium may be charged for the automated scaling being performed when the DCP is used. In other embodiments no premiums may be charged e.g. when determining the billing amounts associated with the execution of a program on the DCP the sum of the charges that would be incurred as a result of using the various resources individually may be computed. A pricing manager may be responsible for determining the billing amounts to be charged to clients on whose behalf the DCP targeted programs are run. As described below with reference to in at least some embodiments clients may indicate budget limits for the execution of their DCP targeted programs and the pricing manager may be configured to enforce such limits in collaboration with the other components of the OSS. It is noted that in different embodiments not all the components shown in may be included within an OSS and that in at least some embodiments other components not shown in may be incorporated within an OSS.

An initial set of one or more compute instances e.g. from a pre configured pool to be used to implement operations of one or more threads of P may be identified element in response to an explicit or implicit request to execute P. In some embodiments a resource selection policy indicating preferences or requirements of a client on whose behalf P is to be executed may be used to select the types of compute instances as well as other resources to be used for P. The mapping between P and the instances may be determined for example by a thread process scheduler component of the OSS. In one simple implementation each of P s threads or processes may be run at a respective compute instance for example. In some implementations multiple threads may be run at a single compute instance or more than one compute instance may be used for a single thread. A pool of pre configured instances may be created in some embodiments from which instances can be selected on demand for P s execution so that the overhead of starting up new compute instances can be avoided. In some embodiments a network address Addr to be used for P s network communications may be selected and the appropriate networking devices e.g. the virtual and or physical network interfaces routers switches etc. that are used by or for the selected instances may be configured to use Addr for traffic directed to or from P. In one implementation if two compute instances are used to implement respective threads of the P the first instance may be configured to receive network traffic directed to Addr and also to indicate Addr as a source address for outbound network traffic generated by operations of P that are executed at the first instance. Similarly the second instance may also be configured to receive network traffic directed to Addr and to indicate Addr as a source address for outbound network traffic generated by operations of P that are executed at the second instance.

An executable version of P may be loaded into the memory of or into memory accessible from the selected compute instance or instances element . In some cases the compiled version of the program may be stored at a storage service as described above and may be retrieved from the storage service into the compute instance memory or memories. The operations of P may then be initiated at the one or more compute instances element .

In response to an invocation of an operating system call by P the appropriate service s and service resource s to be used to obtain results of the call may be identified element . For example for a storage related system call such as a read or a write to a file the appropriate level of a storage hierarchy that includes volatile memory accessible from the instance s persistent storage devices attached locally to the instance s storage service devices or database service instances may be identified. Operations corresponding to the operating system call may then be performed at a storage location at the selected level of the hierarchy. Similarly for other types of operations requested in the operating system call other provider network resources may be deployed. For a network transfer using a socket API in P for example the appropriate network devices of the provider network such as physical or virtual network interfaces accessible from the compute instances may be used to transmit one or more packets. As indicated above in some embodiments a client supplied resource selection policy may help guide the selection of specific provider network resources and or services to be used for various operating system calls of P.

In at least some embodiments an automated scaling policy may be associated with programs such as P. For example in accordance with one such policy the OSS may be configured to complete the operations of P as quickly as possible using as many compute instances up to some limit as necessary. In accordance with another policy the latency of completion of some set of P s operations may be identified as the metric of greatest interest and efforts to reduce average latency by deploying the fastest available resources may be performed by the OSS. In accordance with the scaling policy associated with P P s performance may be monitored in at least some embodiments element . Additional instances or other resources may be dynamically configured and or currently configured instances or resources may be released or decommissioned as needed on the basis of the metrics and the scaling policy. The overall result of P s execution which may be based at least in part on the results of the various DCP service calls and P s internal computations may eventually be obtained and provided to the client on whose behalf P was executed element .

As mentioned above in some embodiments the OSS may be allowed to dynamically deploy additional resources in accordance with scaling policies or goals associated with DCP targeted programs. In some cases a client may wish to impose limits on the resources deployed e.g. to limit the corresponding billing amounts that the client may be required to pay. is a flow diagram illustrating aspects of operations that may be performed by an operating system service implemented at a distributed computing platform in response to resource limit requests received from clients according to at least some embodiments. As shown in element an indication of a resource limit e.g. a maximum number of instances a maximum amount of storage a maximum amount of network bandwidth or a maximum budget to be used for executing a particular program may be received by the OSS e.g. via one or more programmatic interfaces implemented by the OSS. In some cases the resource or budget limits may be applied collectively to all DCP targeted programs associated with a given client account of the provider network. For example a corporation may set up a client account Corp with the provider network and several different user accounts user user . . . may be set up under the client account. Each user account may be allowed to request execution of DCP targeted programs. In such a scenario a resource limit may be requested at various levels for the client account Corp as a whole for individual user accounts or for a specific program whose execution is requested by a given user.

An initial set of resources e.g. compute instances storage devices network devices and the like may be assigned for the execution of a particular program element . The performance of the program and the utilization of resources for the program may be monitored for some time period element . If an analysis implemented for example by an optimizer component of the OSS suggests that performance could be improved by dynamically deploying additional resources as detected in element and the deployment of additional resources does not violate the requested limit or limits as detected in element the additional resources may be allocated to the program element . If the analysis does not suggest that performance would be improved or if the resource or budget limit would be violated if additional resources were deployed the program may continue execution using its currently allocated set of resources. Whether additional resources were deployed or not the monitoring and analysis steps corresponding to elements onwards may continue until the program eventually terminates.

It is noted that in various embodiments operations other than those illustrated in the flow diagrams of may be implemented to support the various techniques described for implementation of operating system interfaces using provider network accessible service resources and that some of the operations shown may not be implemented or may be implemented in a different order or in parallel rather than sequentially.

The techniques described above of deploying resources of a provider network to implement operating system primitives may be useful in a variety of scenarios. Although provider networks with vary large resource capabilities have been available for a few years a substantial number of programmers are not yet trained to fully exploit the rich feature sets of the services implemented at the provider networks. Furthermore the feature sets of the provider network services keep changing relatively rapidly and new services are brought online every few months. As a result a substantial number of programs may not be able to achieve the kinds of scalability availability and other service characteristics that are theoretically possible at the provider networks. If the equivalent of an operating system standard such as POSIX is implemented as a network accessible service it may become possible to utilize the large resource sets and rich feature sets of provider network services for a far wider range of programs.

In at least some embodiments a server that implements a portion or all of one or more of the technologies described herein including the techniques to implement the various components of the operating system service may include a general purpose computer system that includes or is configured to access one or more computer accessible media. illustrates such a general purpose computing device . In the illustrated embodiment computing device includes one or more processors coupled to a system memory which may comprise both non volatile and volatile memory modules via an input output I O interface . Computing device further includes a network interface coupled to I O interface .

In various embodiments computing device may be a uniprocessor system including one processor or a multiprocessor system including several processors e.g. two four eight or another suitable number . Processors may be any suitable processors capable of executing instructions. For example in various embodiments processors may be general purpose or embedded processors implementing any of a variety of instruction set architectures ISAs such as the x86 PowerPC SPARC or MIPS ISAs or any other suitable ISA. In multiprocessor systems each of processors may commonly but not necessarily implement the same ISA. In some implementations graphics processing units GPUs may be used instead of or in addition to conventional processors.

System memory may be configured to store instructions and data accessible by processor s . In at least some embodiments the system memory may comprise both volatile and non volatile portions in other embodiments only volatile memory may be used. In various embodiments the volatile portion of system memory may be implemented using any suitable memory technology such as static random access memory SRAM synchronous dynamic RAM or any other type of memory. For the non volatile portion of system memory which may comprise one or more NVDIMMs for example in some embodiments flash based memory devices including NAND flash devices may be used. In at least some embodiments the non volatile portion of the system memory may include a power source such as a supercapacitor or other power storage device e.g. a battery . In various embodiments memristor based resistive random access memory ReRAM three dimensional NAND technologies Ferroelectric RAM magnetoresistive RAM MRAM or any of various types of phase change memory PCM may be used at least for the non volatile portion of system memory. In the illustrated embodiment program instructions and data implementing one or more desired functions such as those methods techniques and data described above are shown stored within system memory as code and data .

In one embodiment I O interface may be configured to coordinate I O traffic between processor system memory and any peripheral devices in the device including network interface or other peripheral interfaces such as various types of persistent and or volatile storage devices used to store physical replicas of data object partitions. In some embodiments I O interface may perform any necessary protocol timing or other data transformations to convert data signals from one component e.g. system memory into a format suitable for use by another component e.g. processor . In some embodiments I O interface may include support for devices attached through various types of peripheral buses such as a variant of the Peripheral Component Interconnect PCI bus standard or the Universal Serial Bus USB standard for example. In some embodiments the function of I O interface may be split into two or more separate components such as a north bridge and a south bridge for example. Also in some embodiments some or all of the functionality of I O interface such as an interface to system memory may be incorporated directly into processor .

Network interface may be configured to allow data to be exchanged between computing device and other devices attached to a network or networks such as other computer systems or devices as illustrated in through for example. In various embodiments network interface may support communication via any suitable wired or wireless general data networks such as types of Ethernet network for example. Additionally network interface may support communication via telecommunications telephony networks such as analog voice networks or digital fiber communications networks via storage area networks such as Fibre Channel SANs or via any other suitable type of network and or protocol.

In some embodiments system memory may be one embodiment of a computer accessible medium configured to store program instructions and data as described above for through for implementing embodiments of the corresponding methods and apparatus. However in other embodiments program instructions and or data may be received sent or stored upon different types of computer accessible media. Generally speaking a computer accessible medium may include non transitory storage media or memory media such as magnetic or optical media e.g. disk or DVD CD coupled to computing device via I O interface . A non transitory computer accessible storage medium may also include any volatile or non volatile media such as RAM e.g. SDRAM DDR SDRAM RDRAM SRAM etc. ROM etc. that may be included in some embodiments of computing device as system memory or another type of memory. Further a computer accessible medium may include transmission media or signals such as electrical electromagnetic or digital signals conveyed via a communication medium such as a network and or a wireless link such as may be implemented via network interface . Portions or all of multiple computing devices such as that illustrated in may be used to implement the described functionality in various embodiments for example software components running on a variety of different devices and servers may collaborate to provide the functionality. In some embodiments portions of the described functionality may be implemented using storage devices network devices or special purpose computer systems in addition to or instead of being implemented using general purpose computer systems. The term computing device as used herein refers to at least all these types of devices and is not limited to these types of devices.

Various embodiments may further include receiving sending or storing instructions and or data implemented in accordance with the foregoing description upon a computer accessible medium. Generally speaking a computer accessible medium may include storage media or memory media such as magnetic or optical media e.g. disk or DVD CD ROM volatile or non volatile media such as RAM e.g. SDRAM DDR RDRAM SRAM etc. ROM etc. as well as transmission media or signals such as electrical electromagnetic or digital signals conveyed via a communication medium such as network and or a wireless link.

The various methods as illustrated in the Figures and described herein represent exemplary embodiments of methods. The methods may be implemented in software hardware or a combination thereof. The order of method may be changed and various elements may be added reordered combined omitted modified etc.

Various modifications and changes may be made as would be obvious to a person skilled in the art having the benefit of this disclosure. It is intended to embrace all such modifications and changes and accordingly the above description to be regarded in an illustrative rather than a restrictive sense.

