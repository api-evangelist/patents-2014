---

title: High throughput data modifications using blind update operations
abstract: Update requests that specify updates to a logical page associated with a key-value store are obtained. Updates to the logical page are posted using the obtained plurality of update requests, without accessing the logical page via a read operation.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09514211&OS=09514211&RS=09514211
owner: Microsoft Technology Licensing, LLC
number: 09514211
owner_city: Redmond
owner_country: US
publication_date: 20140720
---
Users of electronic devices frequently need to access database systems to obtain various types of information. Many different techniques have been devised for storage and retrieval of data items. For example some recent hardware platforms have exploited recent hardware developments such as multi core processors multi tiered memory hierarchies and secondary storage devices such as flash in an effort to provide higher performance.

According to one general aspect a system may include an apparatus that includes a computer readable storage medium storing executable instructions the executable instructions including a page update manager that includes an update acquisition module that obtains update requests that specify updates for a logical page associated with a key value store. An update posting engine posts the updates using the obtained update requests without accessing the logical page via a read operation.

According to another aspect update requests that specify incremental updates to a key in a key value store that is associated with a logical page may be obtained. The key may be incrementally updated using the obtained update requests without accessing the logical page via a read operation.

According to another aspect a computer program product including a computer readable storage medium stores executable instructions that cause at least one data processing apparatus to obtain a plurality of update requests that specify updates to a logical page associated with a key value store and post the updates using the obtained plurality of update requests without accessing the logical page via a read operation as part of the posting of the obtained updates to the key.

This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used to limit the scope of the claimed subject matter. The details of one or more implementations are set forth in the accompanying drawings and the description below. Other features will be apparent from the description and drawings and from the claims.

Recent developments in hardware platforms have exploited multi core processors multi tiered memory hierarchies and secondary storage devices such as flash in an effort to provide higher performance. For example central processing unit CPU changes have included multi core processors and main memory access that involves multiple levels of caching. For example flash storage and hard disk vendor recognition that update in place compromises capacity has led to increased use of log structuring. For example cloud data centers increase system scale and the use of commodity hardware puts increased emphasis on high availability techniques.

Many indexing applications may need to sustain rapid writes with high throughput. For such applications the read input output I O that is needed before a write to lookup the existing value of the key may be a bottleneck for index insertion throughput. There is a broad category of applications e.g. inverted index where the value associated with a key may be updated without knowledge of the existing value e.g. adding a new document identifier id to the posting list for a term . For such applications the update need not involve a read I O but the underlying key value store may not support such an optimization. In accordance with example techniques discussed herein a mechanism for blind incremental updates no read needed to perform a write operation may be used within the framework of a generic key value store without making any assumptions about the higher level application. Together with the log structured storage organization of an example key value store as discussed herein key updates may be sustained at a throughput that may approach sequential storage write bandwidth.

In accordance with example techniques discussed herein a mechanism for blind updates no read needed to perform a write operation to records included in logical pages associated with example key value stores is also provided.

In this context a key value store may generally refer to a form of database management system that may store pairs of respective keys or terms and values as well as retrieve values when a respective key is known. For example a key value store may include terms e.g. keys with respective lists of objects associated with the respective terms. For example a key value store may store indexes that include respective lists of document identifiers that are associated with respective terms.

For example in an inverted index environment e.g. LUCENE a common approach is segmented index . Segment indexes are written one by one as documents arrive. Though such writes avoid reads they may complicate the query serving pathway. For example queries may scan multiple segment indexes and hence a user may make a tradeoff between query time and freshness of results.

In this context an inverted index may generally refer to an index data structure storing a mapping from content e.g. words or numbers to its locations in a database file or in a document or a set of documents. For example an inverted index may be used to enable fast full text searches at a cost of increased processing when a document is added to the database. For example an inverted index may include a record level inverted index e.g. an inverted file index or inverted file that stores a list of references to documents for each word or a word level inverted index e.g. a full inverted index or inverted list that may additionally store the positions words within a document.

For example in a generic key value store environment e.g. the Log Structured Merge Tree LSM tree there may be no support for semantic value merge. Hence a read I O is involved to perform a write and this may slow the speed of index insertions.

In accordance with example techniques discussed herein a slim page stub may be held in memory for an index page that resides on storage e.g. disk or flash . This page stub stores information that aids in locating the rest of the page on secondary storage e.g. storing location information indicating locations where the rest of the page resides . For example it may also store some access method specific information. For example it may store key boundaries associated with the index page e.g. minimum and maximum key values of indexes associated with the index page . Thus for example when a blind update is performed the stub may provide information that aids in determining that a particular key belongs with a particular page or not . For example if a page is split during processing the stub has information regarding the boundaries.

In this context a page may refer to an object in storage which may be accessed via a physical storage address. As used herein a page may be associated with a flexible size and may represent a page unit of storage that may be distributed over multiple discontiguously stored segments of storage. The storage may include volatile and or stable storage.

In this context a logical page may include a base page and zero or more delta records indicating updates to the page thus allowing a page to be written to flash in pieces when it is flushed. Thus a logical page on flash may correspond to records potentially on different physical device blocks that are linked together e.g. using file offsets as pointers. Further a physical block may include records from multiple logical pages. Further a logical page may refer to locations that store related information.

In accordance with example techniques discussed herein when an incremental update arrives for a key on the page this may be appended as a delta record to the page e.g. by linking to the existing page stub . A read I O may not be needed as the page stub is sufficient for the update to succeed.

In accordance with example techniques discussed herein these delta records may eventually be flushed to storage using incremental page flushing wherein unflushed portions of a page are copied to a substantially large flush buffer which may be appended to the end of the data log on storage using a single write I O e.g. using LLAMA hence utilizing full storage write bandwidth.

In this context a flush operation may refer to transferring a page from main memory e.g. cache storage to secondary storage by way of copying the page to an output buffer.

In accordance with example techniques discussed herein on the lookup query pathway where read I Os may be acceptable the full page may be read in from storage and the different fragments of the value of a key may be provided to a user defined merge function to obtain the final value the user defined nature of the merge function may allow the key value store to remain generic without any application knowledge e.g. the key value store may be opaque to the application and vice versa . For example a particular user may use a particular technique for storing and maintaining the key value store and the user defined merge function may be used via an invocation of the user s function to perform the merge operation in accordance with the user s particular storage maintenance techniques.

In accordance with example techniques discussed herein a mechanism may be exposed to perform incremental updates to key values.

In accordance with example techniques discussed herein a need to perform read I O before such an incremental update may advantageously be avoided or substantially avoided by appending information describing the update to a page stub existing in memory.

In accordance with example techniques discussed herein incremental flushing of a page storing such key value updates into a large flush buffer that is appended to storage may be performed for example using a single write I O.

In accordance with example techniques discussed herein a user defined merge function may be exposed that is utilized to merge multiple incremental updates to a key value to obtain the final value that is returned on the query pathway.

As shown in and in accordance with example techniques discussed herein a blind incremental update enabled index may be used on a solid state device SSD that stores a base page . As shown in a term index t may initially store values e.g. document identifiers or document ids representing three documents d d and d. As shown in a page stub may be used for attaching requested blind incremental updates to the term index t without reading the base page until they may be further processed. For example the page stub may be prepended to the base page . A first blind update request requesting an addition of a document id d to the term index t may be received for processing. The blind update request may be prepended to the page stub thus updating a current state of the page stub to include the blind update request for the term index t of the base page . Thus the update to the term index t may be performed without reading the old base page.

As shown in a second blind update request requesting a deletion of a document id d from the term index t may be received for processing. The blind update request may be prepended to the page stub thus updating a current state of the page stub to include the blind update request as well as the blind update request for the term index t of the base page .

A lookup may be performed on the term index t which may result in at least a portion of the base page i.e. at least the term index t and the page stub being read into memory. A merge operation may be performed to process the first blind update request and the second blind update request thus adding dto the term index t and deleting dfrom the term index t for use with a consolidated page from the base page . For example the merge operation may be performed via the user defined merge function as discussed above to obtain the final value.

While show only a single page stored in a storage device one skilled in the art of data processing will understand that many pages may be stored in various storage devices without departing from the spirit of the discussion herein.

As shown in and in accordance with example techniques discussed herein blind updates may be performed in association with the solid state device SSD that stores the base page . As shown in the base page may include multiple records that may be updated. For example an UPSERT operation may insert a record if it is not already present and may replace a record with an updated version of the record if it is present e.g. via whole record updates to the base page . In accordance with example techniques discussed herein a page stub may be used for attaching requested blind updates and to the records in the base page without reading the base page until they may be further processed.

For example the page stub may be prepended to the base page . A first blind update request requesting an addition of a record R to the base page may be received for processing. The blind update request may be prepended to the page stub thus updating a current state of the page stub to include the blind update request for the page .

As shown in a second blind update request requesting addition of a record R to the logical page that includes the base page and prior update request may be received for processing. The blind update request may be prepended to the page stub thus updating a current state of the page stub to include the blind update request as well as the blind update request for the base page .

As shown in a third blind update request requesting addition update of a record R e.g. which may include a replacement of already existing record R with regard to the logical page that includes base page and prior update requests and may be received for processing. The blind update request may be prepended to the page stub and prior update requests and thus updating a current state of the page stub to include the blind update request as well as the blind update requests for the base page .

A consolidate operation may be performed on the blind updates with the base page which may result in at least a portion of the base page being read into memory. The consolidate operation may be performed to consolidate the blind updates with the base page to generate a consolidated page from the base page . For example the consolidation operation may be performed via a user defined consolidation operation as discussed herein to obtain the final value.

While shows only a few pages stored in a storage device one skilled in the art of data processing will understand that many pages may be stored in various storage devices without departing from the spirit of the discussion herein.

For example BW TREEs and LLAMA techniques discussed further below may be used for inverted index applications to provide sustained throughput for index inserts updates. For example a log structured store design used with BW TREEs and LLAMA techniques may utilize full storage throughput for writes. However index inserts updates may involve page reads and such reads may interfere with delivery of sustained write throughput particularly on hard disk.

The above two sources of reads during index updates may interfere with sustained index insert update throughput for a database application. Such sources of reads during index updates may be avoided or substantially avoided as follows 

The user layer e.g. application indexing layer may also provide a callback function f e.g. the user defined merge function as discussed above that computes a final posting list representation by combining a base posting list representation with a sequence of incremental updates to the key term . On the query pathway the entire page may be read in and all the fragments for values of the key may be provided to the merge function to compute a final value for that key posting list representation that may be returned to the user layer.

Blind writes i.e. updates that do not involve the reading of a prior version may advantageously include avoidance of reading the prior version and the possibility of concurrent updates at the transactional component TC as discussed further below. Example techniques for handling blind writes and the aggregate objects to which they are applied are discussed below.

In accordance with example techniques discussed herein an example system that may be referred to herein as LLAMA Latch free Log structured Access Method Aware includes a caching and storage subsystem for at least recently developed hardware environments e.g. flash multi core although one skilled in the art of data processing will understand that such example techniques are not limited only to recently developed hardware.

For example LLAMA may support an application programming interface API for arbitrarily selected page oriented access methods that provides both cache and storage management optimizing processor caches and secondary storage. For example caching CL and storage SL layers may use a common mapping table that separates a page s logical and physical location. For example the cache layer CL may support data updates and management updates e.g. for index re organization via latch free compare and swap atomic state changes on its mapping table.

For example the storage layer SL may use the same mapping table to handle the page location changes produced by log structuring on every page flush. For example a latch free BW TREE implementation e.g. an implementation using a BW TREE as an example of an ordered B tree style index may be used. In this context latch free may refer to allowing concurrent access to pages by multiple threads.

Example techniques discussed herein may provide mapping tables that may virtualize both the location and the size of pages. For example such virtualization may be utilized for both main memory designs and stable storage designs e.g. log structured storage designs as discussed further herein.

Example techniques discussed herein may separate an access method layer from cache storage management. As an example techniques discussed herein may be used to enforce a write ahead log protocol. For example before flushing a page a conventional database kernel may check a page log sequence number LSN to determine whether there are updates that are not yet stable in the transactional log. For example LLAMA cache management may exploit example delta updates to swap out a partial page. For example it can drop from the cache the part of the page already present on secondary storage which does not include recent delta updates . For example the access method layer may be regularly flushing for transactional log checkpointing. Thus the cache manager will find sufficient candidate possibly partial pages to satisfy any buffer size constraint.

Example techniques discussed herein may provide a framework that enables a substantial number of access methods i.e. not just a single instance to exploit these techniques by implementing a subsystem layer that provides them. Further a log structured store may be implemented for writing data to secondary storage that provides advantageous efficiency. Hence an access method may focus on the main memory aspects of its index and example techniques discussed herein may provide the framework for achieving performance metrics similar to performance metrics of the BW TREE.

For example a technique such as LLAMA through its API may provide latch free page updating which is accomplished in main memory via a compare and swap CAS atomic operation on the mapping table.

For example in managing the cache a technique such as LLAMA may reclaim main memory by dropping only previously flushed portions of pages from memory thus not involving any input output I O operations even when swapping out dirty pages. Thus a technique such as LLAMA may be able to control its buffer cache memory size without input from its access method user.

For example for effective management of secondary storage a technique such as LLAMA may utilize log structuring. For example a technique such as LLAMA may improve performance compared with conventional log structuring by using partial page flushes and pages with substantially no empty space i.e. substantially 100 storage utilization. These may reduce the number of input output operations I Os and amount of storage consumed per page when a page is flushed and hence may reduce the write amplification that may be experienced when log structuring is used. Further all storage related operations may be completely latch free.

For example a technique such as LLAMA may provide at least a limited form of system transaction. In this sense system transactions are not user level transactions but rather exploiting the log structured store provide atomicity purely for the private use of the access method e.g. for index structure modifications SMOs . For example this may enable indexes to adapt as they grow while concurrent updating continues.

For example the BW TREE structure may include a type of latch free B tree structure. For example updates to BW TREE nodes may be performed based on prepending update deltas to a prior page state. Thus the BW TREE may be latch free as it may allow concurrent access to pages by multiple threads. Because such delta updating preserves the prior state of a page it may provide improved processor cache performance as well.

Example techniques using BW TREEs may further provide page splitting techniques that are also latch free and that may employ B link tree style side pointers. Splits and other structure modification operations may be atomic both within main memory and when made stable. For example atomic record stores may be implemented based on a BW TREE architecture.

One skilled in the art of data processing will appreciate that there may be many ways to accomplish the latch free and log structured storage discussed herein without departing from the spirit of the discussion herein.

At the LLAMA layer see e.g. commonly owned U.S. patent application Ser. No. 13 924 567 entitled Latch Free Log Structured Storage for Multiple Access Methods with inventors David Lomet et al. filed Jun. 22 2013 and Levandoski et al. LLAMA A Cache Storage Subsystem for Modern Hardware Vol. 6 No. 10 39International Conference on Very Large Databases Aug. 26 2013 the page is the abstraction that is manipulated. To support blind writes LLAMA may enable pages to be updated regardless of whether they are in the cache or in stable storage.

In accordance with example techniques discussed herein an example update interface may support at least a delta update UPDATE D and a replacement update UPDATE R e.g. when the entire page is available in cache .

In accordance with example techniques discussed herein LLAMA may advantageously support partial pages including partial page swap outs.

For example a technique such as LLAMA may support a page abstraction supporting access method implementations for cache storage layers. Further a transactional component e.g. a DEUTERONOMY style transactional component may be added on top. is a block diagram of an example architecture for latch free log structured storage for multiple access methods. As shown in a transactional component may support a transactional key value store and may operate with a data component that may include an atomic key value store. As shown in the data component may include a latch free ordered index and or a latch free linear hashing index . As shown in the data component may further include an example latch free log structured access method aware LLAMA storage engine e.g. LLAMA of .

The example API may be data opaque meaning that the example LLAMA implementation does not see e.g. does not examine or analyze or depend on what the access method e.g. of the access method layer is putting into pages or delta records and acts independently of what is provided in the pages or delta records by the access method. Thus example LLAMA implementations may act in response to specific operations where the access method has selected the page upon which to operate and the operation that LLAMA performs is not dependent on the data arguments that are provided.

As shown in a page may be accessed via a mapping table that maps page identifiers PIDs to states e.g. via a physical address stored in the mapping table either in main memory cache or on secondary storage . For example the main memory cache may include random access memory RAM . For example the secondary storage may include flash memory. For example pages may be read from secondary storage into a main memory cache on demand they can be flushed to secondary storage and they may be updated to change page state while in the cache . For example substantially all page state changes both data state and management state may be provided as atomic operations in accordance with example techniques discussed herein. As shown in an example physical address may include a flash memory flag e.g. for 1 bit as shown in the example indicating whether the physical address is associated with flash or memory e.g. cache storage with an address field for at least the address itself e.g. for 63 bits as shown in the example . One skilled in the art of data processing will appreciate that there are many ways of representing a physical address e.g. other than a 64 bit representation without departing from the spirit of the discussion herein.

In accordance with example techniques discussed herein LLAMA through its API may provide latch free page updating via a compare and swap CAS atomic operation on the mapping table e.g. in lieu of a conventional latch that guards a page from concurrent access by blocking threads . For example the CAS strategy may advantageously increase processor utilization and improve multi core scaling.

In accordance with example techniques discussed herein in managing the cache LLAMA may reclaim main memory by dropping only previously flushed portions of pages from memory thus not using any I O even when swapping out dirty pages. Thus an example architecture such as LLAMA may control its buffer cache memory size without a need to examine data stored in pages by its access method user e.g. as an example architecture such as LLAMA is unaware of transactions and write ahead logging .

An example architecture such as LLAMA may use log structuring to manage secondary storage e.g. providing the advantages of avoiding random writes reducing the number of writes via large multi page buffers and wear leveling involved with flash memory . Further an example architecture such as LLAMA may advantageously improve performance e.g. as compared with conventional log structuring with partial page flushes and pages with substantially no empty space i.e. substantially 100 utilization. For example these may reduce the number of I Os and storage consumed per page when a page is flushed and hence may reduce the write amplification that may otherwise be encountered when log structuring is used. Further substantially all storage related operations may be completely latch free.

Additionally an example architecture such as LLAMA may support at least a limited form of system transaction see e.g. D. Lomet et al. Unbundling Transaction Services in the Cloud CIDR 2009 with regard to system transactions . For example system transactions may not be user transactions but rather may provide atomicity purely for the private use of the access method e.g. for index structure modifications SMOs see e.g. C. Mohan et al. ARIES IM An Efficient and High Concurrency Index Management Method Using Write Ahead Logging 1992 SIGMOD 92 1992 pp. 371 380 . For example a property that system transactions recorded separately from the transaction log may be effective is an example of an advantageous insight of the DEUTERONOMY approach to decomposing a database kernel.

In designing an example system such as LLAMA a design goal may include a goal to be as general purpose as possible which may sometimes lead to a goal to be as low level as possible. However for an example system such as LLAMA to be general purpose it may be desirable to operate effectively while knowing as little as possible about what an access method does in using its facilities. Thus operations of an example system such as LLAMA may be primitive targeted at cache management and the updating of pages. For example an example system such as LLAMA may include some additional facilities to support a primitive transaction mechanism that may be advantageously included for SMOs e.g. page splits and merges .

In accordance with example techniques discussed herein an example system such as LLAMA may include nothing in the interface regarding log sequence numbers LSNs write ahead logging or checkpoints for transaction logs. In accordance with example techniques discussed herein an example system such as LLAMA may include no idempotence test for user operations. Further in accordance with example techniques discussed herein an example system such as LLAMA may include no transactional recovery e.g. which may be handled by an access method using an example system such as LLAMA in accordance with example techniques discussed herein .

An example access method may change state in response to user operations. For example a user may want to create C read R update U or delete D a record e.g. CRUD operations . In accordance with example techniques discussed herein an example system such as LLAMA may not directly support these operations. Rather the example access method may implement them as updates to the states of LLAMA pages.

For example there may also be structure changes that are part of example access method operations. For example a BW TREE page split may involve posting a split delta to an original page O so that searchers know that a new page now contains data for a sub range of the keys in O. For example these too may be handled as updates to a LLAMA page O.

In accordance with example techniques discussed herein an example system such as LLAMA may support two forms of update e.g. a delta update and a replacement update. For example an access method may choose to exploit these forms of updates in accordance with a user s wishes. For example a BW TREE may make a series of delta updates and at some point decide to consolidate and optimize the page by applying the delta updates to a base page. For example the BW TREE may then use a replacement update to generate the new base page.

In accordance with example techniques discussed herein an example system such as LLAMA may retain information regarding the physical location of a page in secondary storage throughout update operations and replacement operations as discussed herein so that the system has the secondary storage page location information for re reading the page should it be swapped out of the main memory cache and for garbage collection as further discussed herein. Thus the system may remember previous page locations and stable page state information.

For example a delta update may be indicated as Update D PID in ptr out ptr data . For example the delta update may prepend a delta describing a change to the prior state of the page. For example for the BW TREE the data parameter to Update D may include at least where the lsn enables idempotence. For example the in ptr points to the prior state of the page and the out ptr points to the new state of the page.

For example a replacement update may be indicated as Update R PID in ptr out ptr data . For example a replacement update may result in an entirely new state for the page. The prior state preserved when using an Update D may be replaced by the data parameter. Thus the data parameter contains the entire state of the page with deltas folded in. 

For example a read may be indicated as Read PID out ptr . For example a read may return via out ptr the address in main memory for the page. If the page is not in main memory then the mapping table entry may contain a secondary storage address. For example in that case the page may be read into main memory and the mapping table may be updated with the new main memory address.

In addition to supporting data operations example systems discussed herein e.g. LLAMA may provide operations to manage the existence location and persistence of pages. To adjust to the amount of data stored the access method may add or subtract pages from its managed collections. To provide state persistence an access method may from time to time flush pages to secondary storage. To manage this persistence pages may be annotated appropriately e.g. with log sequence numbers lsns . For example a page manager may be configured to control flush operations allocate operations and free operations on pages.

For example a flush operation may be indicated as Flush PID in ptr out ptr annotation . For example a Flush may copy a page state into the log structured store LSS I O buffer. Flush may be somewhat similar to Update D in its impact on main memory as it prepends a delta with an annotation to the prior state. This delta may be tagged as a flush. In accordance with example techniques discussed herein an example system such as LLAMA may store the LSS secondary storage address where the page is located called the flash offset and the caller annotation in the flush delta. For example a Flush may not ensure a user that the I O buffer is stable when it returns.

For example a buffer manager may be configured to control updates to a log structured secondary storage buffer via latch free update operations. Thus for example multiple threads may simultaneously update the log structured secondary storage buffer via latch free operations.

For example a make stable operation may be indicated as Mk Stable LSS address . For example a MkStable operation may ensure that pages flushed to the LSS buffer up to the LSS address argument are stable on secondary storage. When Mk Stable returns the LSS address provided and all lower LSS addresses are ensured to be stable on secondary storage.

For example a high stable operation may be indicated as Hi Stable out LSS address . For example a Hi Stable operation may return the highest LSS address that is currently stable on secondary storage.

For example a page manager may be configured to initiate a flush operation of a first page in cache layer storage to a location in secondary storage based on initiating a copy of a page state of the first page into a secondary storage buffer and initiating a prepending of a flush delta record to the page state the flush delta record including a secondary storage address indicating a storage location of the first page in secondary storage and an annotation associated with a caller.

For example a buffer manager may be configured to initiate a stability operation for determining that pages flushed to a secondary storage buffer having lower addresses up to a first secondary storage address argument are stable in secondary storage.

For example an allocate operation may be indicated as Allocate out PID . For example an Allocate operation may return the PID of a new page allocated in the mapping table. All such pages may be remembered persistently so Allocate may be included as part of a system transaction as discussed further below which may automatically flush its included operations.

For example a free operation may be indicated as Free PID . For example a Free operation may make a mapping table entry identified by the PID available for reuse. In main memory the PID may be placed on the pending free list for PIDs for a current epoch as discussed further below . Again because active pages may need to be remembered Free may be included as a part of a system transaction.

In accordance with example techniques discussed herein example LLAMA system transactions may be used to provide relative durability and atomicity all or nothing for structure modifications e.g. SMOs . For example an LSS and its page oriented records may be used as log records. For example all operations within a transaction may be automatically flushed to an in memory LSS I O buffer in addition to changing page state in the cache. For example each LSS entry may include the state of a page for an example LSS that is strictly a page store.

In main memory all such operations within a transaction may be held in isolation until transaction commit as discussed further below. For example at commit all page changes in the transaction may be flushed atomically to the LSS buffer. For example on abort all changes may be discarded. For example a system transaction manager may be configured to commit transactions and abort transactions.

For example a transaction begin operation may be indicated as TBegin out TID . For example a transaction identified by a transaction ID TID may be initiated. This may involve entering it into an active transaction table ATT maintained by the example LLAMA cache layer CL manager.

For example a transaction commit operation may be indicated as TCommit TID . For example the transaction may be removed from the active transaction table and the transaction may be committed. For example page state changes in the transaction may be installed in the mapping table and flushed to the LSS buffer.

For example a transaction abort operation may be indicated as TAbort TID . For example the transaction may be removed from the active transaction table changed pages may be reset to transaction begin in the cache and no changes are flushed.

In accordance with example techniques discussed herein in addition to Allocate and Free Update D operations may be permitted within a transaction to change page states. For example Update R might not be used as it may complicate transaction undo as discussed further below.

In accordance with example techniques discussed herein transactional operations may all have input parameters TID and annotation. For example TID may be added to the deltas in the cache and an annotation may be added to each page updated in the transaction e.g. as if it were being flushed . When installed in the flush buffer and committed all updated pages in the cache may have flush deltas prepended describing their location e.g. as if they were flushed independently of a transaction .

The BW TREE see e.g. J. Levandoski et al. The Bw Tree A B tree for New Hardware Platforms 29 ICDE 2013 Apr. 8 11 2013 may provide an example key value store that may enable user transactions to be supported e.g. for the transactional component . For example it may manage LSNs enforce the write ahead log WAL protocol and respond to checkpointing requests as expected by a DEUTERONOMY data component DC see e.g. J. Levandoski et al. Deuteronomy Transaction Support for Cloud Data CIDR January 2011 pp. 123 133 and D. Lomet et al. Unbundling Transaction Services in the Cloud CIDR 2009 . A discussion herein includes addressing how it may accomplish that when using an example system such as LLAMA.

 Data content to the Update D and Update R LLAMA operations may include keys LSNs and the data part of a key value store. For example a BW TREE may thus via these operations implement a key value store provide idempotence via LSNs perform incremental updates via Update D perform its page consolidations via Update R and access pages for read or write using the LLAMA Read or Flush operation. For example the system may include a record manager that may be configured to control updates based on update delta record operations and replacement update operations.

For example an access method may store LSNs in the data it provides to LLAMA via update operations. Further the Flush operation annotation parameter stored in a flush delta may provide additional information to describe page contents. For example these may permit the BW TREE to enforce write ahead logging WAL . For example a Stabilize operation e.g. Mk Stable after flushing a page may make updates stable for transaction log checkpointing.

For example Allocate and Free operations may permit an example BW TREE implementation to grow and shrink its tree. For example BeginTrans e.g. TBegin and Commit Abort e.g. TCommit TAbort may enable the atomicity expected when performing structure modifications operations SMOs .

For example Update operations e.g. Update D Update R may not be limited to user level data. For example a BW TREE may use Update D to post its merge and split deltas when implementing SMOs as discussed further below with regard to system transactions.

In accordance with example techniques discussed herein with respect to cache layer data operations page updating may be accomplished by installing a new page state pointer in the mapping table using a compare and swap operation CAS whether a delta update as shown in or a replacement update. For example a replacement update e.g. Update R PID in ptr out ptr data may include both the desired new state and the location of the prior state of the page in LSS. For example a new update delta e.g. Update D PID in ptr out ptr data points to the prior state of the page which already includes this LSS location.

For example such a latch free approach may avoid the delays introduced by latching but it may incur a penalty of its own as do optimistic concurrency control methods i.e. the CAS may fail and the update will then be re attempted. For example it may be left to an example LLAMA user to retry its operation as appropriate as an example LLAMA implementation may indicate when a failure occurs.

In accordance with example techniques discussed herein while no operation may block when the data is in cache e.g. reading a page from secondary storage may involve waiting for the page to appear in the cache. The mapping table e.g. the mapping table will point to the LSS page even for cached pages as discussed above enabling pages to be moved between cache and LSS for effective cache management.

In accordance with example techniques discussed herein when a page is flushed an example LLAMA implementation may ensure that what is represented in the cache e.g. matches what is in LSS e.g. . Thus the flush delta may include both PID and LSS offset in the flush delta and may include that delta in the LSS buffer and in the cache e.g. by prepending it to the page .

In accordance with example techniques discussed herein because an example LLAMA implementation may support delta updating page state may include non contiguous pieces. Combining this feature with flushing activity may result in an in cache page having part of its state in LSS having been flushed earlier while recent updates may be present only in the cache. When this occurs it may be possible to reduce the storage cost of the next flush.

Thus an example LLAMA implementation may flush such a page by writing a delta that includes only the changes since the prior flush. For example multiple update deltas in the cache may all be made contiguous for flushing by writing a contiguous form of the deltas which may be referred to herein as a C delta with a pointer to the remainder of the page in LSS. Thus the entire page may be accessible in LSS but in possibly several pieces.

In accordance with example techniques discussed herein the Flush operation may observe a cached page state that may have several parts that have been flushed over time in this manner resulting in a cached page in which the separate pieces and their LSS addresses are represented. In accordance with example techniques discussed herein at any time Flush may bring these pieces together in LSS storage by writing the contents of the discontiguous page pieces contiguously and redundantly . For example a user may be willing to leave the pieces separate when LSS uses flash storage while desiring contiguity when LSS uses disk storage due to the differing read access and storage costs.

In accordance with example techniques discussed herein when a page is flushed it may be desirable for a system to know prior to the flush what state of the page is being flushed. For example this may be easily ascertained using latches as a system may simply latch the page and perform the flush. However in a latch free approach the system may have substantial difficulty in ordering page flushes correctly. For example this may pose issues in enforcement of a write ahead log protocol or when the flush occurs as part of a structure modification. For example it may be desirable for inappropriate flushes to fail when they perform their CAS. Thus in accordance with example techniques discussed herein the pointer to the page state to be flushed in the CAS may be used which may then only capture that particular state and may fail if the state has been updated before the flush completes. However this may raise other issues.

In research difficulties have been encountered in determining the kind of strong invariant that may be advantageous when performing cache management and flushing pages to LSS. For example an invariant may include properties such as 

For example the dilemma discussed above may be resolved as discussed below. For example if the CAS is performed early enough then it may be determined whether the flush will be successful or not prior to copying the state of the page to the log buffer. Thus an example flush procedure may be performed as follows 

The result of this example procedure is that the LSS during recovery might not observe pages that are the result of CAS s that have failed. For example this also preserves the property that any page that appears later in the LSS in terms of its position in the log will be a later state of the page than all earlier instances of the page in the LSS log.

In accordance with example techniques discussed herein it may be desirable for an example LLAMA implementation to manage the cache and swap out data so as to meet its memory constraints. For example the example LLAMA implementation may be aware of delta updates replacement updates and flushes and may recognize each of these. However the example LLAMA implementation will know nothing about the contents of the pages if it is to be general purpose. Thus the example LLAMA implementation is unaware whether the access method layer is supporting transactions by maintaining LSN s in the pages. Thus an issue that may be posed includes a potential question regarding how an example LLAMA implementation may provide cache space management including evicting pages when it may not see LSN s and enforce the write ahead log protocol.

For example any data that has already been flushed may be dropped from the cache. For example systems in which pages are updated in place may be prevented from swapping out dropping from the cache any recently updated and dirty page. However because of delta updates an example LLAMA implementation may determine which parts of pages have already been flushed. For example each such part may be described with a flush delta and those flushed parts may be swapped out of the cache.

In swapping out parts of pages it may be disadvantageous to simply deallocate the storage and reuse it as that may leave dangling references to the swapped out parts. Thus in accordance with example techniques discussed herein a delta may be used that describes what parts of a page have been swapped out.

For example for a fully swapped out page its main memory address in the mapping table may be replaced with an LSS pointer from the page s most recent flush delta.

For example a page manager may be configured to initiate a swap operation of a portion of a first page in cache layer storage to a location in secondary storage based on initiating a prepending of a partial swap delta record to a page state associated with the first page the partial swap delta record including a secondary storage address indicating a storage location of a flush delta record that indicates a location in secondary storage of a missing part of the first page.

For example the page manager may be further configured to initiate a free operation for cache layer storage associated with the portion of the first page using an epoch mechanism.

In accordance with example techniques discussed herein this approach may advantageously provide several useful features for users. For example such an example LLAMA implementation s cache layer may reclaim memory e.g. without knowledge regarding the actual content of pages. For example dropping flushed pages and flushed parts of pages may involve no I O operation. For example bringing a partially flushed page back into main memory may involve fewer LSS reads than would be the case for a fully flushed page with multiple parts in LSS.

As discussed herein a logical page may include a base page and zero or more delta records indicating updates to the page thus allowing a page to be written to flash in pieces when it is flushed. Thus a logical page on flash may correspond to records potentially on different physical device blocks that are linked together using file offsets as pointers. Further a physical block may include records from multiple logical pages. illustrates an example log structured storage organization A on flash .

For example a logical page may be read from flash into memory e.g. RAM by starting from the head of the chain on flash whose offset in a sequential log may be obtained from the mapping table and following the linked records. For example an offset may be obtained from the mapping table for accessing a delta record to obtain a current state and a base page for reading the corresponding logical page from flash into memory .

For example an offset may be obtained from the mapping table for accessing a delta record to obtain the delta and link to access a second delta record and subsequently a base page for reading the corresponding logical page from flash into memory .

For example the flush process may advantageously consolidate multiple delta records of the same logical page into a contiguous C delta on flash when they are flushed together. Moreover a logical page may be consolidated on flash when it is flushed after being consolidated in memory which may advantageously improve page read performance.

For example as shown in replacing the prior state of the page with the new state of the page may include consolidating the plurality of delta records into a contiguous C delta which may then be flushed together with the base page .

For example replacing the prior state of the page with the new state of the page may include generating a modified version of the current page or determining another page for replacing the current page and replacing a physical address of the current page with a physical address of the new state of the page e.g. the modified version or the other page for replacement via an atomic compare and swap operation on the mapping table .

For example as a distinction between the features of and when writing a page to secondary storage LLAMA may perform the consolidation illustrated in but it depends upon the access method executing an Update R to perform the consolidation of .

In accordance with example techniques discussed herein an example LLAMA implementation may be entirely latch free. Further dedicated threads might not be used to flush an I O buffer as this may complicate keeping thread workload balanced. Thus all threads may participate in managing this buffer. For example conventional approaches have utilized latches. However such conventional techniques might only latch while allocating space in the buffer releasing the latch prior to data transfers which may then proceed in parallel.

In accordance with example techniques discussed herein an example LLAMA implementation may avoid conventional latches for buffer space allocation instead using a CAS for atomicity as done elsewhere in the example systems discussed herein. For example this involves defining the state on which the CAS executes. For example the constant part of buffer state may include its address Base and size Bsize . For example the current high water mark of storage used in the buffer may be tracked with an Offset relative to the Base. For example each request for the use of the buffer may begin with an effort to reserve space Size for a page flush.

In accordance with example techniques discussed herein to reserve space in the buffer a thread may acquire the current Offset and compute Offset Size. For example if Offset Size Bsize then the request may be stored in the buffer. For example the thread may issue a CAS with current Offset as the comparison value and Offset Size as the new value. If the CAS succeeds Offset may be set to the new value the space may be reserved and the buffer writer may transfer data to the buffer.

In accordance with example techniques discussed herein updates may proceed even when the entire page is not in cache. However there may be times when an update needs the version of the page that has been read to remain unchanged between the read and a subsequent update. For example a user may wish to determine whether the page state has changed since it was previously looked at. i.e. observed .

For example in a non blind update the value of the mapping table pointer which identifies the page state previously read may accompany the update certifying that the state has not changed since it was read. Since a READ operation for a page involves the presence of the page in the cache this may ensure that the update will be made to an in cache full page.

Blind updates may also involve a page state to ensure that possibly conflicting operations have not intervened. For these operations an operation P READ partial page read may be used that reads whatever is currently in the page cache for the state without triggering the read of the full page should the full page not be present. Then the address returned may be used in updates as before only without a need for the full page to be in cache. If the mapping table only stores a flash offset then a P READ may read into cache the first part of the page referenced in flash without a need to bring in the entire page.

Thus an access method having such a PREAD operation may suffice for LLAMA to support blind updates from the access method.

For example a page stub may be provided by using a flush delta with a particular annotation together with its reference to the remainder of the page state in cache and a flash offset stable storage location for a location where the page has been placed in stable storage. For example a stub may be created when the cache manager then swaps out the page via a partial swap up to the last flush delta. However in accordance with example techniques discussed herein the flush delta may be left in place instead of replacing the mapping table address with a flash offset. The remaining flush delta may retain this flash offset plus the annotation for users of LLAMA e.g. the BW TREE . An access method may read a page stub or more of the page if present in the cache with a PREAD to determine what it may do with the page.

As discussed above the BW TREE may be used as an indexed record manager built to execute on top of LLAMA as its page oriented cache manager. At this layer a record manager supports typical CRUD create read update delete operations as a minimum. The CRUD operations may involve knowledge of the prior state of the record store which may involve cache operations at LLAMA that may involve reading the entire page to acquire that knowledge.

To support blind style record operations additional operations may be utilized. For example an UPSERT operation uses a new record version to replace an existing record if it exists or to create insert the record if it does not yet exist. Thus it combines the C of CRUD with the U Create and Update . Within the LLAMA example discussed above an UPSERT may perform a P READ instead of an ordinary read and then proceed with its update e.g. using an UPDATE D .

For example an example BLIND D operation or blind delete the D in CRUD may be used in addition to a normal delete that checks whether a version is present before deleting and hence involving a page READ. This operation may involve only performing a P READ. Such a BLIND D operation may have various different definitions. For example it may delete a prior version when the page is eventually read and rationalized and may be a no op if there is no prior version.

As another example it may linger as a delete request discharged only when a subsequent version is added for the record.

In either event the effect of the BLIND D may not be fully realized until the page is entirely in memory via a READ.

For example an application may support a blind incremental update of a record of the UPSERT form. Incremental record changes as opposed to the complete replacement update model for record updates discussed above may involve intra record semantics that are not conventionally known by a record oriented access method such as the BW TREE.

Incremental record updates UPDATE I which produce changes but not replacements of records may use the same LLAMA operations as blind writes i.e. PREADs instead of READs. Thus they may be treated similarly to UPSERTs. However the BW TREE is not able by itself to understand how to consolidate these updates into a record version. Discussed below are two example techniques for handling this consolidation 

As a first example an application using the BW TREE may issue a record read READ the R in CRUD . For example the BW TREE may gather the pieces of the updated record together via concatenation of the pieces it has received. Subsequent to this read the application presents a consolidated record as a normal update that replaces the prior pieced together record with a record that has been consolidated by the application. No additional functionality aside from UPDATE I may be involved.

As a second example an application may provide to the BW TREE a procedure that understands how to consolidate a record in concatenated pieces format into an application understood consolidated format. There are a variety of ways that this procedure might be provided once in information retained about the table being supported by the BW TREE or by providing a call back point that the BW TREE can call to perform this function e.g. prior to delivering the record as part of a READ or during the page consolidation process done by the BW TREE as it incorporates delta updates into a consolidated page.

Discussed below are example techniques for replacing the record pieces that are the result of UPDATE I with the version that has been consolidated by either example technique discussed above.

Thus it may be desirable to store the consolidated record in the BW TREE replacing the UPDATE I increments that were provided before.

For example a CONSOLIDATE I operation may work on a record when it has been solely updated by prior UPDATE I operations. Such a CONSOLIDATE I operation may identify prior UPDATE I s that are to be replaced and then post this consolidated version into the BW TREE. It may be assumed that each UPDATE I has an LSN that is used for idempotence. When consolidation occurred it is known which of the UPDATE I s went into the consolidated version that was produced. Thus the consolidated record can retain these LSNs when it is used in an update delta for the page. This enables future operations both future consolidates and future BW TREE operations to provide idempotence.

The BW TREE knows how to consolidate a page and may use an UPDATE R LLAMA operation to replace the current form of the page with the new form. During this process the BW TREE may remove from the page all UPDATE I s that are captured in the CONSOLIDATE I operation replacing them with the consolidated record. Any UPDATE I operations that are not captured with the consolidated record may remain as unconsolidated entries for the record e.g. to be consolidated later . The BW TREE does not need to understand the details of the consolidation to accomplish this. It need know only about the LSN information captured in the consolidated record.

One skilled in the art of data processing will appreciate that many other types of techniques may be used for index insertions using blind incremental updates without departing from the spirit of the discussion herein.

Features discussed herein are provided as example embodiments that may be implemented in many different ways that may be understood by one of skill in the art of data processing without departing from the spirit of the discussion herein. Such features are to be construed only as example embodiment features and are not intended to be construed as limiting to only those detailed descriptions.

As further discussed herein is a block diagram of a generalized system for data modifications using blind update operations including both incremental and complete record replacements . The generalized system as shown is merely intended to illustrate various example functionality and or logic that may be included in example techniques as discussed herein and is not intended to be limiting in terms of implementations in various hardware and or software configurations. One skilled in the art of data processing will appreciate that system may be realized in hardware implementations software implementations or combinations thereof. As shown in a system may include a device that includes at least one processor . The device may include a page update manager that may include an update acquisition module that obtains a plurality of update requests that specify updates for a logical page associated with a key value store.

For example page data storage may include any type of page data storage including at least volatile storage such as main memory and more stable storage e.g. more non volatile storage such as secondary storage which may include flash storage as well as other types of disk drives etc. One skilled in the art of data processing will appreciate that there are many types of page data storage that may be used with techniques discussed herein without departing from the spirit of the discussion herein.

According to an example embodiment the page update manager or one or more portions thereof may include executable instructions that may be stored on a tangible computer readable storage medium as discussed below. According to an example embodiment the computer readable storage medium may include any number of storage devices and any number of storage media types including distributed devices.

In this context a processor may include a single processor or multiple processors configured to process instructions associated with a computing system. A processor may thus include one or more processors executing instructions in parallel and or in a distributed manner. Although the device processor is depicted as external to the page update manager in one skilled in the art of data processing will appreciate that the device processor may be implemented as a single component and or as distributed units which may be located internally or externally to the page update manager and or any of its elements.

For example the system may include one or more processors . For example the system may include at least one tangible computer readable storage medium storing instructions executable by the one or more processors the executable instructions configured to cause at least one data processing apparatus to perform operations associated with various example components included in the system as discussed herein. For example the one or more processors may be included in the at least one data processing apparatus. One skilled in the art of data processing will understand that there are many configurations of processors and data processing apparatuses that may be configured in accordance with the discussion herein without departing from the spirit of such discussion.

In this context a component or module may refer to instructions or hardware that may be configured to perform certain operations. Such instructions may be included within component groups of instructions or may be distributed over more than one group. For example some instructions associated with operations of a first component may be included in a group of instructions associated with operations of a second component or more components . For example a component herein may refer to a type of functionality that may be implemented by instructions that may be located in a single entity or may be spread or distributed over multiple entities and may overlap with instructions and or hardware associated with other components.

According to an example embodiment the page update manager may be implemented in association with one or more user devices. For example the page update manager may communicate with a server as discussed further below.

For example one or more databases may be accessed via a database interface component . One skilled in the art of data processing will appreciate that there are many techniques for storing information discussed herein such as various types of database configurations e.g. relational databases hierarchical databases distributed databases and non database configurations.

According to an example embodiment the page update manager may include a memory that may store objects such as intermediate results. In this context a memory may include a single memory device or multiple memory devices configured to store data and or instructions. Further the memory may span multiple distributed storage devices. Further the memory may be distributed among a plurality of processors.

According to an example embodiment a user interface component may manage communications between a user and the page update manager . The user may be associated with a receiving device that may be associated with a display and other input output devices. For example the display may be configured to communicate with the receiving device via internal device bus communications or via at least one network connection.

According to example embodiments the display may be implemented as a flat screen display a print form of display a two dimensional display a three dimensional display a static display a moving display sensory displays such as tactile output audio output and any other form of output for communicating with a user e.g. the user .

According to an example embodiment the page update manager may include a network communication component that may manage network communication between the page update manager and other entities that may communicate with the page update manager via at least one network . For example the network may include at least one of the Internet at least one wireless network or at least one wired network. For example the network may include a cellular network a radio network or any type of network that may support transmission of data for the page update manager . For example the network communication component may manage network communications between the page update manager and the receiving device . For example the network communication component may manage network communication between the user interface component and the receiving device .

For example a cache layer manager may include a mapping table manager that may be configured to initiate table operations on an indirect address mapping table the table operations including initiating atomic compare and swap CAS operations on entries in the indirect address mapping table to replace prior states of pages that are associated with the page data storage with new states of the pages.

For example using such atomic operations may provide full multi threading capability i.e. any thread may access any data thus advantageously providing speed capabilities in processing.

For example the mapping table manager may be configured to initiate the table operations on the indirect address mapping table associated with a data opaque interface wherein the indirect address mapping table is used in common for management of data storage that includes cache layer storage and secondary storage .

For example the indirect address mapping table separates logical locations of pages from corresponding physical locations of the pages wherein users of the page data storage store page identifier values in lieu of physical location address values for the pages elsewhere in data structures referencing the page data storage.

For example an update manager may be configured to control data updates and management updates using latch free compare and swap operations on entries in the indirect address mapping table to effect atomic state changes on the indirect address mapping table .

For example a storage layer may include a log structured storage layer manager that may be configured to control page location changes associated with log structuring resulting from page flushes using latch free compare and swap operations on entries in the indirect address mapping table .

For example a buffer manager may be configured to control updates to a log structured secondary storage buffer via latch free update operations. Thus for example multiple threads may simultaneously update the log structured secondary storage buffer via latch free operations.

For example the buffer manager may be configured to initiate a stability operation for determining that pages flushed to the log structured secondary storage buffer having lower addresses up to a first secondary storage address argument are stable in the log structured secondary storage.

For example a page manager may be configured to control flush operations allocate operations and free operations on pages. For example the page manager may be configured to initiate a flush operation of a first page in cache layer storage to a location in secondary storage based on initiating a copy of a page state of the first page into a secondary storage buffer initiating a prepending of a flush delta record to the page state the flush delta record including a secondary storage address indicating a storage location of the first page in secondary storage and an annotation associated with a caller and initiating an update to the page state based on installing an address of the flush delta record in a mapping table via a compare and swap CAS operation.

For example the page manager may be configured to initiate a swap operation of a portion of a first page in cache layer storage to a location in secondary storage based on initiating a prepending of a partial swap delta record to a page state associated with the first page the partial swap delta record including a main memory address indicating a storage location of a flush delta record that indicates a location in secondary storage of a missing part of the first page.

For example a system transaction manager may be configured to commit transactions and abort transactions.

For example a record manager may be configured to control updates based on update delta record operations and replacement update operations.

For example the page manager may be configured to flush a page state to secondary storage based on installing a pointer to a flush delta record in a mapping table via a compare and swap CAS operation the flush delta record prepended to an existing page state that is replaced in the mapping table via the CAS operation.

For example the page manager may be configured to determine whether the CAS operation succeeds and to initiate a write operation to write the existing page state to a secondary storage flush buffer if it is determined that the CAS operation succeeds.

For example the page manager may be configured to initiate a void operation to storage space previously allocated for the existing page if it is determined that the CAS operation fails.

An update posting engine may post the updates using the obtained update requests without accessing the logical page via a read operation.

For example a page stub manager may initiate a prepending of a page stub to a current state of the logical page via a page stub delta record representing the page stub the page stub including an indication of an address of a current state of the logical page and metadata specifying attributes of the logical page the prepending of the page stub to the current state initiated by installing an address of the page stub delta record representing the page stub in a mapping table via an atomic operation. For example the page stub may include values of key boundaries associated with the logical page.

For example the update acquisition module may obtain a plurality of update requests that specify record updates to the logical page.

For example the update posting engine may post the obtained record updates using the obtained plurality of update requests without accessing the logical page via a read operation.

For example the update posting engine may post the obtained record updates by prepending a plurality of respective delta records to the page stub. The respective delta records may include record update information that specifies the respective updates to the logical page that are specified in the respective plurality of update requests.

For example a consolidation engine may consolidate the obtained record updates via a predefined consolidation function using the plurality of respective delta records that are prepended to the page stub.

For example the update acquisition module may obtain a plurality of update requests that specify incremental updates to a record in the logical page.

For example the update posting engine may incrementally post the obtained specified incremental updates using the obtained update requests without accessing the logical page via a read operation.

For example the update posting engine may incrementally post the obtained specified incremental updates by incrementally prepending a plurality of respective delta records to the page stub wherein the plurality of respective delta records include key update information that specifies the respective updates to the record.

For example a data retrieval engine may initiate a lookup of a key in the key value store to obtain a representation of at least a portion of the logical page in a current state of the logical page via a read operation from storage into memory.

For example a merge engine may merge the obtained specified incremental updates via a predefined merge function using the plurality of respective delta records that are prepended to the page stub. For example the predefined merge function includes a user defined merge function.

For example an incremental flush engine may incrementally flush contents of the logical page with the plurality of respective delta records that are prepended to the page stub to a flush buffer storage area that is appended to storage using a single write operation.

For example the key value store includes keys that represent terms associated with respective documents and respective values that represent respective document identifiers.

One skilled in the art of data processing will appreciate that many different techniques may be used for data modifications using blind updates without departing from the spirit of the discussion herein.

Features discussed herein are provided as example embodiments that may be implemented in many different ways that may be understood by one of skill in the art of data processing without departing from the spirit of the discussion herein. Such features are to be construed only as example embodiment features and are not intended to be construed as limiting to only those detailed descriptions.

The updates may be posted using the obtained plurality of update requests without accessing the logical page via a read operation .

For example a prepending of a page stub to a current state of the logical page may be initiated via a page stub delta record representing the page stub the page stub including an indication of an address of a current state of the logical page and metadata specifying attributes of the logical page the prepending of the page stub to the current state initiated by installing an address of the page stub delta record representing the page stub in a mapping table via an atomic operation .

For example the key value store may include keys that represent terms associated with respective documents and respective values that represent respective document identifiers .

For example a plurality of update requests that specify record updates to the logical page may be obtained in the example of .

For example the obtained record updates may be posted using the obtained plurality of update requests without accessing the logical page via a read operation .

For example the obtained record updates may be posted by prepending a plurality of respective delta records to the page stub wherein the plurality of respective delta records include record update information that specifies the respective updates to the logical page that are specified in the respective plurality of update requests .

For example the obtained record updates may be consolidated via a predefined consolidation function using the plurality of respective delta records that are prepended to the page stub .

For example a plurality of update requests that specify incremental updates to a record in the logical page may be obtained in the example of .

For example the obtained specified incremental updates may be incrementally posted using the obtained plurality of update requests without accessing the logical page via a read operation .

For example the obtained specified incremental updates may be incrementally posted by incrementally prepending a plurality of respective delta records to the page stub wherein the plurality of respective delta records include key update information that specifies the respective updates to the record .

For example a lookup of a key in the key value store may be initiated to obtain a representation of at least a portion of the logical page in a current state of the logical page via a read operation from storage into memory .

For example the obtained specified incremental updates may be merged via a predefined merge function using the plurality of respective delta records that are prepended to the page stub .

For example contents of the logical page with the plurality of respective delta records that are prepended to the page stub may be incrementally flushed to a flush buffer storage area that is appended to storage using a single write operation .

The key may be incrementally updated using the obtained plurality of update requests without accessing the logical page via a read operation .

For example a prepending of a page stub to a current state of the logical page may be initiated via a page stub delta record representing the page stub the page stub including an indication of an address of a current state of the logical page the prepending of the page stub to the current state initiated by installing an address of the update delta record representing the page stub in a mapping table via an atomic operation .

For example incrementally updating the key may include incrementally updating the key using the obtained plurality of update requests by writing out delta update records to secondary storage incrementally without accessing the logical page via a read operation .

For example incrementally updating the key may include incrementally prepending a plurality of respective delta records to the page stub delta record representing the page stub wherein the plurality of respective delta records include key update information that specifies the respective updates to the key that is specified in the respective plurality of update requests in the example of .

For example the prepending of the plurality of respective delta records may be initiated by installing an address of a current respective incremental delta record in a mapping table via an atomic operation .

For example a lookup of the key may be initiated to obtain a representation of at least a portion of the logical page in a current state of the logical page via a read operation from storage into memory .

For example a merge of the obtained incremental updates may be initiated via a predefined merge function using the plurality of respective delta records that are prepended to the page stub .

The updates may be posted using the obtained plurality of update requests without accessing the logical page via a read operation as part of the posting of the obtained updates to the key .

Features discussed herein are provided as example embodiments that may be implemented in many different ways that may be understood by one of skill in the art of data processing without departing from the spirit of the discussion herein. Such features are to be construed only as example embodiment features and are not intended to be construed as limiting to only those detailed descriptions.

For example a system includes an apparatus that includes at least one processor and a computer readable storage medium storing executable instructions that are executable by the at least one processor the executable instructions including a page update manager that includes an update acquisition module that obtains a plurality of update requests that specify updates for a logical page associated with a key value store. An update posting engine posts the updates using the obtained plurality of update requests without accessing the logical page via a read operation.

A page stub manager initiates a prepending of a page stub to a current state of the logical page via an page stub delta record representing the page stub the page stub including an indication of an address of a current state of the logical page and metadata specifying attributes of the logical page the prepending of the page stub to the current state initiated by installing an address of the page stub delta record representing the page stub in a mapping table via an atomic operation.

The update acquisition module obtains a plurality of update requests that specify record updates to the logical page and the update posting engine posts the obtained record updates using the obtained plurality of update requests without accessing the logical page via a read operation.

The update posting engine posts the obtained record updates by prepending a plurality of respective delta records to the page stub wherein the plurality of respective delta records include record update information that specifies the respective updates to the logical page that are specified in the respective plurality of update requests.

A consolidation engine consolidates the obtained record updates via a predefined consolidation function using the plurality of respective delta records that are prepended to the page stub.

The update acquisition module obtains a plurality of update requests that specify incremental updates to a record in the logical page. The update posting engine incrementally posts the obtained specified incremental updates using the obtained plurality of update requests without accessing the logical page via a read operation.

The update posting engine incrementally posts the obtained specified incremental updates by incrementally prepending a plurality of respective delta records to the page stub. The plurality of respective delta records include key update information that specifies the respective updates to the record.

A data retrieval engine initiates a lookup of a key in the key value store to obtain a representation of at least a portion of the logical page in a current state of the logical page via a read operation from storage into memory. A merge engine merges the obtained specified incremental updates via a predefined merge function using the plurality of respective delta records that are prepended to the page stub.

An incremental flush engine incrementally flushes contents of the logical page with the plurality of respective delta records that are prepended to the page stub to a flush buffer storage area that is appended to storage using a single write operation.

The key value store includes keys that represent terms associated with respective documents and respective values that represent respective document identifiers. The logical page is stored on secondary storage.

A plurality of update requests that specify incremental updates to a key in a key value store that is associated with a logical page are obtained.

The key is incrementally updated using the obtained plurality of update requests without accessing the logical page via a read operation.

A prepending of a page stub to a current state of the logical page is initiated via a page stub delta record representing the page stub the page stub including an indication of an address of a current state of the logical page the prepending of the page stub to the current state initiated by installing an address of the update delta record representing the page stub in a mapping table via an atomic operation.

Incrementally updating the key includes incrementally prepending a plurality of respective delta records to the page stub delta record representing the page stub. The plurality of respective delta records include key update information that specifies the respective updates to the key that is specified in the respective plurality of update requests.

The prepending of the plurality of respective delta records is initiated by installing an address of a current respective incremental delta record in a mapping table via an atomic operation.

A lookup of the key is initiated to obtain a representation of at least a portion of the logical page in a current state of the logical page via a read operation from storage into memory.

A merge of the obtained incremental updates is initiated via a predefined merge function using the plurality of respective delta records that are prepended to the page stub.

Incrementally updating the key includes incrementally updating the key using the obtained plurality of update requests by writing out delta update records to secondary storage incrementally without accessing the logical page via a read operation.

A computer program product comprises a computer readable storage medium storing executable instructions that cause at least one data processing apparatus to obtain a plurality of update requests that specify updates to a logical page associated with a key value store and post the updates using the obtained plurality of update requests without accessing the logical page via a read operation as part of the posting of the obtained updates to the key.

One skilled in the art of data processing will understand that there may be many ways of performing data modifications using blind updates without departing from the spirit of the discussion herein.

Customer privacy and confidentiality have been ongoing considerations in data processing environments for many years. Thus example techniques for performing data modifications using blind updates may use user input and or data provided by users who have provided permission via one or more subscription agreements e.g. Terms of Service TOS agreements with associated applications or services associated with such techniques. For example users may provide consent to have their input data transmitted and stored on devices though it may be explicitly indicated e.g. via a user accepted agreement that each party may control how transmission and or storage occurs and what level or duration of storage may be maintained if any. Further identifiers that may be used to identify devices used by a user may be obfuscated e.g. by hashing actual user information. It is to be understood that any user input data may be obtained in accordance with the privacy laws and regulations of any relevant jurisdiction.

Implementations of the various techniques described herein may be implemented in digital electronic circuitry or in computer hardware firmware software or in combinations of them e.g. an apparatus configured to execute instructions to perform various functionality .

Implementations may be implemented as a computer program embodied in signals e.g. a pure signal such as a pure propagated signal . Such implementations will be referred to herein as implemented via a computer readable transmission medium which does not qualify herein as a computer readable storage medium or a computer readable storage device as discussed below.

Alternatively implementations may be implemented via a machine usable or machine readable storage device e.g. a magnetic or digital medium such as a Universal Serial Bus USB storage device a tape hard disk drive compact disk CD digital video disk DVD etc. storing executable instructions e.g. a computer program for execution by or to control the operation of a computing apparatus e.g. a data processing apparatus e.g. a programmable processor a special purpose processor or device a computer or multiple computers. Such implementations may be referred to herein as implemented via a computer readable storage medium or a computer readable storage device and are thus different from implementations that are purely signals such as pure propagated signals and thus do not qualify herein as a computer readable transmission medium as discussed above . Thus as used herein a reference to a computer readable storage medium or a computer readable storage device specifically excludes signals e.g. propagated signals per se.

A computer program such as the computer program s described above can be written in any form of programming language including compiled interpreted or machine languages and can be deployed in any form including as a stand alone program or as a module component subroutine or other unit suitable for use in a computing environment. The computer program may be tangibly embodied as executable code e.g. executable instructions on a machine usable or machine readable storage device e.g. a computer readable medium . A computer program that might implement the techniques discussed above may be deployed to be executed on one computer or on multiple computers at one site or distributed across multiple sites and interconnected by a communication network.

Method steps may be performed by one or more programmable processors executing a computer program to perform functions by operating on input data and generating output. The one or more programmable processors may execute instructions in parallel and or may be arranged in a distributed configuration for distributed processing. Example functionality discussed herein may also be performed by and an apparatus may be implemented at least in part as one or more hardware logic components. For example and without limitation illustrative types of hardware logic components that may be used may include Field programmable Gate Arrays FPGAs Program specific Integrated Circuits ASICs Program specific Standard Products ASSPs System on a chip systems SOCs Complex Programmable Logic Devices CPLDs etc.

Processors suitable for the execution of a computer program include by way of example both general and special purpose microprocessors and any one or more processors of any kind of digital computer. Generally a processor will receive instructions and data from a read only memory or a random access memory or both. Elements of a computer may include at least one processor for executing instructions and one or more memory devices for storing instructions and data. Generally a computer also may include or be operatively coupled to receive data from or transfer data to or both one or more mass storage devices for storing data e.g. magnetic magneto optical disks or optical disks. Information carriers suitable for embodying computer program instructions and data include all forms of nonvolatile memory including by way of example semiconductor memory devices e.g. EPROM EEPROM and flash memory devices magnetic disks e.g. internal hard disks or removable disks magneto optical disks and CD ROM and DVD ROM disks. The processor and the memory may be supplemented by or incorporated in special purpose logic circuitry.

To provide for interaction with a user implementations may be implemented on a computer having a display device e.g. a cathode ray tube CRT liquid crystal display LCD or plasma monitor for displaying information to the user and a keyboard and a pointing device e.g. a mouse or a trackball by which the user can provide input to the computer. Other kinds of devices can be used to provide for interaction with a user as well for example feedback provided to the user can be any form of sensory feedback e.g. visual feedback auditory feedback or tactile feedback. For example output may be provided via any form of sensory output including but not limited to visual output e.g. visual gestures video output audio output e.g. voice device sounds tactile output e.g. touch device movement temperature odor etc.

Further input from the user can be received in any form including acoustic speech or tactile input. For example input may be received from the user via any form of sensory input including but not limited to visual input e.g. gestures video input audio input e.g. voice device sounds tactile input e.g. touch device movement temperature odor etc.

Further a natural user interface NUI may be used to interface with a user. In this context a NUI may refer to any interface technology that enables a user to interact with a device in a natural manner free from artificial constraints imposed by input devices such as mice keyboards remote controls and the like.

Examples of NUI techniques may include those relying on speech recognition touch and stylus recognition gesture recognition both on a screen and adjacent to the screen air gestures head and eye tracking voice and speech vision touch gestures and machine intelligence. Example NUI technologies may include but are not limited to touch sensitive displays voice and speech recognition intention and goal understanding motion gesture detection using depth cameras e.g. stereoscopic camera systems infrared camera systems RGB red green blue camera systems and combinations of these motion gesture detection using accelerometers gyroscopes facial recognition 3D displays head eye and gaze tracking immersive augmented reality and virtual reality systems all of which may provide a more natural interface and technologies for sensing brain activity using electric field sensing electrodes e.g. electroencephalography EEG and related techniques .

Implementations may be implemented in a computing system that includes a back end component e.g. as a data server or that includes a middleware component e.g. an application server or that includes a front end component e.g. a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation or any combination of such back end middleware or front end components. Components may be interconnected by any form or medium of digital data communication e.g. a communication network. Examples of communication networks include a local area network LAN and a wide area network WAN e.g. the Internet.

Although the subject matter has been described in language specific to structural features and or methodological acts it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather the specific features and acts described above are disclosed as example forms of implementing the claims. While certain features of the described implementations have been illustrated as described herein many modifications substitutions changes and equivalents will now occur to those skilled in the art. It is therefore to be understood that the appended claims are intended to cover all such modifications and changes as fall within the scope of the embodiments.

