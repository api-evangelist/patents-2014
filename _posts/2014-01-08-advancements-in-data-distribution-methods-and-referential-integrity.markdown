---

title: Advancements in data distribution methods and referential integrity
abstract: An elastic parallel database system where data distribution is container- and container-context based. Container Based Tables are defined and Container Member Tables achieve co-location of data as needed. A polymorphic key may also establish polymorphic key relationships between rows in one table and rows in many other possible tables.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09064031&OS=09064031&RS=09064031
owner: Tesora, Inc.
number: 09064031
owner_city: Cambridge
owner_country: US
publication_date: 20140108
---
This application is a continuation of commonly assigned copending U.S. patent application Ser. No. 13 966 980 which was filed on Aug. 14 2013 by Mrithyunjaya Annapragada for ADVANCEMENTS IN DATA DISTRIBUTION METHODS AND REFERENTIAL INTEGRITY and claims the benefit of and filing date priority to a U.S. Provisional Patent Application Ser. No. 61 757 809 filed Jan. 29 2013 entitled METHODS AND APPARATUS FOR IMPROVING THE EFFICIENCY OF ELASTIC PARALLEL DATABASE MANAGEMENT SYSTEMS . It also relates generally to a prior U.S. patent application Ser. No. 13 690 496 filed Nov. 30 2012 entitled Mechanism for Co Located Data Placement in a Parallel Elastic Database Management System . The entire contents of each those patent applications are hereby incorporated by reference.

This patent application relates generally to data management systems and more specifically to a parallel and shared nothing relational database management system.

Distributed RDBMS that are implemented according to a Shared Nothing system architecture are sometimes referred to as Parallel Database Servers in contrast to Symmetric Multiprocessing SMP Database Servers and when the number of logical computers in the Parallel Database Server is large this is sometimes referred to as Massively Parallel Processing or MPP architecture.

Conventionally the number of logical computer servers that participate in parallel database architectures remains constant during the life of the system. Elastic Parallel Database Servers are an extension of the static MPP architecture where the number and composition of the computer servers may change during query processing often in response to the demands being placed on the system. We refer to Elastic Parallel Database Management Systems as EPRDBMS.

Relational Database Management Systems RDBMS maintain metadata information about the schema and in the case of EPRDBMS the metadata includes information about the placement of data on the system.

In cases where an RDBMS is used to store data for a Software as a Service SaaS application it is very common that each client of the service has its data and customizations stored in a database on the RDBMS. In this kind of application architecture there is a one to one mapping between clients and databases and therefore a SaaS application with many clients would necessarily have many databases.

In an SMP RDBMS each database resides on a single server and in an MPP RDBMS each database may reside on a plurality of database servers. For the purposes of this preceding statement we refer only to a single copy of the database but multiple copies of the data may be maintained for the purpose of high availability and redundancy.

The Elastic Parallel RDBMS EPRDBMS includes a Dynamic Query Planner that converts queries submitted to the application into query execution plans that consist of a sequence of operations that must be performed by individual nodes in the system in a specified order in order to produce the results expected by the application.

Data in the EPRDBMS is stored in tables that are associated with storage group s and the data in the tables is distributed across a plurality of storage nodes that are members of the storage group using one of many distribution methods some of which are data dependent and some of which are data independent. Tables are grouped into logical collections called databases.

Elasticity in the EPRDBMS is achieved through abstractions like storage nodes grouped into storage nodegroups and compute nodes grouped into compute nodegroups and mechanisms that allow for these to be provisioned dynamically based on the workload being placed on the system at any given instant of time.

The EPRDBMS ensures that applications are guaranteed well understood database ACID properties Atomicity Consistency Isolation and Durability and it also provides applications with the ability to perform groups of operations that are either all completed or all aborted but never a situation where some but not all of the operations are completed.

Unless explicitly described to the contrary this document only refers to the first primary copy of a database. Without loss of generality concepts described here also apply to all copies of the database. This is for simplicity of illustration and so as to not obscure key aspects of the invention.

Commands and operations are submitted to a database in some query language that is mutually understood by the client and the EPRDBMS such as the Structured Query Language SQL on a connection established by the client with the EPRDBMS for this purpose. Persistent state about the connection is maintained by the database along with each connection.

Summary Techniques and methods for improving the storage and processing of data for Software as a Service Applications are provided. SaaS applications implemented according to a Simple SaaS configuration provide each consumer of the service referred to as a client application with an independent database. With a conventional RDBMS this database would reside on a single server and multiple client databases may share the same server. This model has several inefficiencies. First the large number of databases imposes a significant overhead on the RDBMS. Second each RDBMS is confined to a single logical database server and therefore excess capacity needs to be provided on a per database server basis. This leads to low utilizations as one has to provide this headroom for the eventuality that one database on the server experiences higher demands. Thirdly database management operations such as backup and restore need to be done on a per database basis and this is inefficient. Finally when application changes require database changes these changes must be done one application client at a time and this is also inefficient. The techniques and methods presented herein dramatically reduce these overheads and dramatically improve the efficiencies of the RDBMS tier under the SaaS application using techniques of elastic database virtualization and EPRDBMS.

Software as a Service SaaS applications provide the benefits and functionality of the software in an easy to consume online offering. The provider of the SaaS application operates the infrastructure on which the application is run and the customer connects to the application over a network and interacts with it derives the benefits from it all without the burden of managing and operating it. Many SaaS applications store data and customizations in a relational database.

Many SaaS applications are implemented in a Simple SaaS configuration . In this operating configuration data and customizations for each client of the application are stored in a client specific database. In addition there may be some application wide data that is shared by all application clients. In the Simple SaaS configuration each client specific database is stored on a logical database server and for efficiency many client specific databases share the same server. However when one client application sees increased traffic this would result in a higher load being placed on the underlying database server. When one client application sees increased traffic it also has the potential to negatively impact all other client applications sharing the same database server. Therefore excess capacity must be provisioned on a per database server basis and this leads to a low overall database server utilization level.

The customizations by a client may include some actions that could add modify or delete data in the tables in the client specific database add tables to the database schema or modify the schema of some of the tables in the database.

An increase in number of tables per client database and the number of clients results in a very rapid increase in the total number of tables being stored on the database server. Such an increase the associated overhead per table and database have a considerable impact on the performance of an RDBMS. For this and other reasons it is beneficial to reduce the number of tables and databases in the system.

A common operation with multi client SaaS applications is a rolling upgrade where clients are progressively upgraded from one release of the SaaS application to another. These upgrades may in addition to changing the SaaS application require modifications be made to the underlying database including the data and the schema.

According to one aspect herein the EPRDBMS virtualizes the database and therefore while each client believes that it is communicating with a RDBMS with a dedicated database for each client the underlying storage and data management is optimized by among other things reducing the number of actual databases and tables used to store the data.

When implemented in the Simple SaaS Configuration the addition of a new client requires the creation of a new database to contain the information for that client and the creation of the tables required to store information required for that client. The EPRDBMS herein maintains metadata information about each database created for a client application and about each table created in those databases. This metadata information about each database includes such things as the name of the database and any other optional information provided for by a Data Definition Language DDL specification. This metadata information about a table includes such things as the names of the column the data type of the column and any additional optional information about the column as provided for by the DDL specification.

In subsequent paragraphs we use terms such as the list of known databases known tables schema decorations . These are collectively part of the metadata maintained by the EPRDBMS. The namespace within which these lists are maintained may be a global namespace a single global list of known databases tables and decorations or one or more of a number of namespaces the appropriate namespace in each context being determined in some manner such for example as based on the name of the user connected to the database or an explicit command to use a specific database . The use of the terms known database known tables or schema decorations in this description assume that these are referenced in their appropriate namespace.

When the command to create a new database is submitted to the EPRDBMS by the client application this is recorded by the EPRDBMS in the list of known databases .

When a subsequent command to use a named database is submitted by the client application the EPRDBMS consults it s list of known databases and confirms that it knows of the existence of the specified database. If the command to use a named database is found to be in order the connection state associated with the connection will reflect the current database.

When a table is referenced as part of a command that reference may include a specification of the database within which the table resides. If such a table reference is made then the EPRDBMS consults its list of known databases and confirms that it knows of the existence of the specified database and if that is successful it attempts to resolve the reference table in that database context.

When a table is referenced without an explicit qualification indicating the name of the database the current database associated with the connection is used and the EPRDBMS attempts to resolve the reference to the table in the current database context.

When data is stored in a table the EPRDBMS may store additional attributes along with the attributes specified by the client application and the additional attributes being used to identify the database and table context of the row being stored. These additional attributes are referred to as schema decorations .

When a command to create a new table is received by the EPRDBMS it consults its list of known tables to determine whether a table with a compatible schema exists in any database. How this is determined and what constitutes a compatible schema is described in the next paragraph. If it determines that there is a table with a compatible schema then the EPRDBMS records the creation of this new table and the database context in which it was created and the existence of a compatible schema. However should it determine that no compatible schema exists the EPRDBMS records the creation of this new table and the database context in which it was created and the fact that no compatible schema was found.

The schema of one table call it table T is said to be compatible with the schema of another table call it table T for the purposes of the embodiments discussed herein if the data for both table T and table T can be stored together in a single table with some additional columns schema decorations such that filters on the schema decorations can uniquely identify the data that belongs to table T and table T.

Consider the following two tables T A INT B CHAR 30 C DATETIME and T A INT B CHAR 30 C DATETIME . Then the EPRDBMS could create a table T A INT B CHAR 30 C DATETIME D CHAR 2 and store the data from T into T and set the value of D to T in all those rows and store the data from T into T and set the value of D to T for all those rows. To get at the data from T the system would simply query T and add the restriction WHERE D T and the restriction WHERE D T would give the system all the data in T.

Consider the following two tables T A INT B CHAR 30 C DATETIME and T P INT Q CHAR 30 R DATETIME . Then the EPRDBMS could create a table T W INT X CHAR 30 Y DATETIME Z CHAR 2 and store the data from T into T by using column W for all the values of A the column X for all the values of B and column Y for all the values of C and set the value of Z to T in all those rows. It could then store the data from T into T using W X and Y to store the data in P Q and R respectively and set the value of Z to T for all those rows. To get at the data from T the system would simply query T and add the restriction WHERE Z T and alias the columns W X and Y as A B and C. The restriction WHERE Z T and similarly aliasing W X and Y as P Q and R would give the system all the data in T.

For the purposes of the systems discussed herein tables T and T as described above are compatible schemas as the system can easily construct table T and eliminate the duplication of tables T and T in the system.

When a query is received by the EPRDBMS that references reads writes modifies or deletes a table it consults its list of known tables and verifies that the table being referenced is on the list of known tables in the appropriate database context. If the reference is found to be successful i.e. that the referenced table does exist in the appropriate database context then the EPRDBMS consults its metadata and determines whether the appropriate schema decorations would apply to the referenced table. It then can update the query provided to it by the client with the appropriate restrictions consistent with the schema decorations if any to ensure that the query only references data that is germane to the subject query.

As defined above data for tables with compatible schemas may be stored in a single table and the EPRDBMS can distinguish rows from one table from rows from another table by applying appropriate filters on the schema decorations and by aliasing columns if required all based on information it retrieves from the metadata.

When a command to alter a table is received by the EPRDBMS the actions are similar to the actions when the table is being first created namely to verify whether a table with a compatible schema to the new proposed schema already exists and to create a new table with the new proposed schema if one does not exist and to copy all data from the existing table to the new table by the appropriate application of filters based on the schema decorations of the source table. In the event that a table with a compatible scheme with the new proposed schema already exists the data is copied into that table and if that table happened to be a client private table it now becomes a multi client table. If on the other hand no table with a compatible schema is found then the data gets stored in a new client private table.

The EPRDBMS may store some tables with schema decorations and other tables without schema decorations. The EPRDBMS may store data for multiple tables with compatible schemas in a single table utilizing filters on the schema decorations and aliasing to only access the data relevant to a specific query.

The EPRDBMS may further operate in a mode where data for more than one client in the multi client database are referenced in a single query this being done by the inclusion of filters that include schema decorations for more than one client. One example of this would be a query that accesses all data in the database by specifying a filter on the schema decorations that allows the query to process all data from all clients.

When the EPRDBMS stores data for multiple clients in a single table operations that are to execute in the context of a single client are preferably executed with suitable filters based on the schema decorations on the multi client table to ensure that the query only references data that is germane to the subject query.

When a command to drop or truncate a table is received by the EPRDBMS the EPRDBMS verifies that the table is on the list of known tables in the appropriate database context and if the reference is found to be valid it determines whether the data for the table is being stored along with data for other clients. If the EPRDBMS stores data for multiple clients in a single table and the command received was to drop or truncate the table for a single client the EPRDBMS would modify this query to delete the data for the client through the appropriate filters on the schema decorations and if the command was to drop the table then delete the table from the list of known tables in the appropriate context.

The EPRDBMS may not immediately determine that it must store data for two tables with compatible schemas in a single underlying table instead relying on some threshold one of which may be a minimum number of references to a compatible schema before data for these compatible tables is stored together. In tables db04.T and db03.T and are shown with compatible schemas but the system has determined to store them independently.

The EPRDBMS stores data for client applications in tables that are each associated with at least one storage nodegroup with a storage nodegroup consisting of an ordered list of storage nodes. The distribution of the data across the nodes in the storage nodegroup is defined at the time when an association between the table and the storage nodegroup is created. According to this aspect herein the data for a single client while appearing to the client to be in a table that is part of a database dedicated to him may in fact reside in a table shared by multiple clients and in fact distributed over a collection of storage nodes that are part of the EPRDBMS. The storage nodegroup associated with a table may be part of the determination of whether two schemas are compatible or not.

The client application while attempting to create a database or table may specify whether or not the object being created must be considered for consolidation into a multi client object or not. This specification may be provided either as an extension to the Structured Query Language SQL DDL or through some other out of band mechanism such as the out of band DDL specification mechanisms described starting in paragraph 124 below.

According to this aspect herein the client application may stipulate that a table being created should be stored as a client private table even if there is a table with a compatible schema. illustrates tables db04.T and db03.T and . Despite the fact that they have compatible schemas the system is shown as having stored these two as client private tables. One way this can be accomplished is through a DDL specification indicating that the table should be client private.

According to another aspect a client can specify that a table being created should be shared across all clients in a multi client system. A common use of such a table is a table listing system wide information that is of interest to all client applications. Such a table would be considered a system wide table.

According to an aspect herein the system may assume a default behavior when the DDL does not specify whether a table is to be either a client private multi client or system wide. In one implementation herein that default behavior would be to make the table a multi client table.

Accordingly the extension to the SQL DDL in one implementation is provided as above showing the ability to specify client private multi client or system wide tables further depicting that specification as being optional.

Summary Elastic Parallel Database Management Systems operate by storing data on a plurality of storage nodes and processing queries received from client applications by transforming them into a series of operations that the storage nodes perform in conjunction with compute nodes that are used to hold intermediate data. The series of steps that an EPRDBMS performs called a query plan is generated by transforming the incoming query based on the metadata stored in the system. An error in this transformation could result in serious consequences like poor performance data loss or incorrect results. The logic underlying these transformations is extremely complex and therefore error prone. Similar to database optimizers this transformation process is vital to the proper operation of the EPRDBMS. Methods and techniques to improve the reliability of these transformations and the efficiency with which new transformations can be developed are presented.

The EPRDBMS herein stores user data on a plurality of database management servers storage nodes and executes queries received from client applications by rewriting these queries into a series of steps executed by the storage nodes and compute nodes resulting in a result set that is then returned to the client application as the result of the query.

The mechanism used to transform the incoming query into the query execution steps is based among other things on the incoming query the manner in which data in the tables referenced by the query are distributed on the plurality of storage nodes.

Upon receipt of a query from a client application the EPRDBMS herein parses the query and determines the complete meaning of the result set being sought by the client application. It determines the database tables being referenced and consults its metadata to determine how and where the data for those tables is stored including the manner in which the data is distributed across a plurality of storage nodes.

The Query Rewrite Engine QRE transforms queries received into the query execution steps QES that represent an ordered sequence of operations that must be executed in order to produce the desired effects of the query.

The QES consists at least of a a set of operations that must be performed represented in some form such as SQL and b the dependency between the operations described in a above indicating which operations in the QES cannot be performed before some other operations in the QES are completed and c the location and distribution of intermediate tables that are produced as result of the execution of some steps of the QES. In practice the Query Execution Engine QEE which is responsible for executing the QES may choose to execute multiple steps in the in parallel and at the same time if it determines that the dependencies of each of them have been met.

The QRE transforms each input query into an internal representation suitable for its own processing. This internal representation is called a Query Parse Tree QPT . A QPT is a tree data structure where there is a top level node that produces the final results required by the client supplied query. Each node in this tree has an associated operation and some nodes have child nodes as well. Before a node is executed all child nodes must have been completely executed. The DQP generated by the EPRDBMS may therefore consist of a plan where multiple steps are executed at the same time.

Each arrow in the bubble diagram represents a tuple stream a stream of rows and is associated with a geometry the columns in the stream and a distribution that identifies which tuples will be arriving from which storage or compute site and going to which storage or compute site. These are some of the attributes of an arrow.

Each bubble in a bubble diagram has a series of identifiers inside it depicting the kind of bubble. There are many different bubbles performing different database primitive operations. Each bubble is associated with zero or more inputs and may produce zero or one output. Each input and output is an arrow in the bubble diagram. Further each bubble has an execution locus the place where the operation represented by the bubble is executed. These are some of the attributes of a bubble.

Once represented as a bubble diagram it is much easier to visualize the operations of a QRE. The QRE makes transformations of the QPT based on specific rules. These rules may be applied on individual bubbles individual arrows or groups of bubbles and arrows and these rules may be applied based on the attributes of the bubbles and the arrows. Rules may be applied unconditionally or conditionally.

The rules used by the QRE Query Rewriting Rules or QRR are specified to the system in some manner understood by the QRE. These rules may be changed dynamically and a change to the QRR will result in a change in the way the QRE performs rewrites.

The QRR consist of zero or more actions that must be executed in a manner specified in the rule in order to perform the actions intended by that rule.

Each rule has an associated priority and if the QRE determines that it needs to execute multiple QRR s it uses the priority associated with the rule to determine the sequence of execution.

Some actions modify the tree and some actions modify the geometry of a stream input or output . These are some of the attributes of an action.

The QRE executes rules in multiple passes some passes traversing the bubble diagram from the top to the bottom and others from the bottom to the top. The direction of the pass up or down is also used to determine the rules that must be executed. In the top to bottom pass analysis begins at the node that produces the final results of the query or the node that produces no output stream in the case of queries that produce no results . In the bottom to top pass analysis begins at the nodes that have no dependencies and proceeds sequentially through the entire parse tree analyzing nodes whose dependencies have already been analyzed and culminates at the node that produces the final results of the query or the node with no output stream in the case of queries that produce no results .

Having transformed an input query into a QPT the QRE executes the appropriate rules for the QPT and this execution results in the QES.

A top to bottom pass of this QPT would begin at arrow and then go to bubble and then follow arrow to bubble then resume at arrow and follow it to bubble .

A bottom to top pass of this QPT would begin at bubble and traverse arrow but determine that bubble has other pre requisites and therefore resume at bubble and traverse arrow then determine that all prerequisites of bubble have been traversed and therefore resume at bubble and then proceed to arrow .

For the purposes of this paragraph assume that tables T and T are both distributed according to some EDD and co located for the purpose of the join T.P T.Q. A query rewrite rule applied to the join bubble would for example ensure that the distribution of the incoming streams is consistent with the join condition. In this example it would determine that the stream was distributed according to the distribution column of T T.P and that the stream was distributed according to the distribution column of T T.Q and that this was consistent with the join T.P T.Q.

On the other hand assume for the purposes of this paragraph that tables T and T are not co located for the purposes of the join T.P T.Q. A query rewrite rule applied to the join bubble would for example ensure that the distribution of the incoming streams is consistent with the join condition. In this example it would determine that the stream was distributed according to the distribution column of T T.P and that the stream was distributed according to the distribution column of T T.Q and that this was not consistent with the join T.P T.Q as the distribution methods for T and T are not the same. In that case the QRR would perform a translation to the bubble diagram in order to make the inputs to the join consistent with the join condition.

According to the translation in the inputs of the join bubble and are distributed in a manner consistent for the join and is therefore something that the EPRDBMS can execute in parallel.

The translation described in the paragraph above is an example of a translation that modifies the QPT. Similarly translations can change the geometry of a stream when required.

As described above the Query Rewrite Rules may be provided to the QRE in a form understood by the QRE and specifying the actions the priorities and other information required by the QRE in determining the correct rules to be applied and the sequence in which those rules should be applied to the QPT s. For the purposes of illustration a simple text file was used.

An example of the rules for a Full Outer Join bubble is provided as . Rules may include some key attributes including the name prerequisite rules the inputs to the rule the outputs from the rule and attributes of the rule such as whether the rule modifies the tree or the geometry and any external references from the rule. Each rule contains zero or more actions. Each action could specify a priority and the steps to be performed for the action. Comments may also be specified and in the example set of rules comments are depicted by lines where the first non whitespace character is the character. An action in a rule may also reference the current bubble using the special keyword this . The actions are specified in a language that allows for the definition of variables conditional operations branching loops and transfer of control. The language also supports the definition of functions that perform some specified operation. In the illustration a function IS JOINABLE is shown and it operates on two streams in this example the two inputs to the FOJ bubble . The function IS JOINABLE would compare the geometries of the two streams and return the logical values TRUE or FALSE.

The QRE transforms a QPT into QES according to rules defined in the QRR. The EPRDBMS herein provides a mechanism that can be used to instruct the DQP to reload a new set of QRR. One such mechanism is an extension to the SQL language understood by the EPRDBMS. By way of illustration the command could instruct the system to load new rules either for the system as a whole for the current session or for some other group of users sessions or duration.

Summary It is necessary for an EPRDBMS to be able to operate on multiple platforms each with their own peculiarities and specializations and in some instances on multiple platforms at the same time shortcomings in existing database management system architectures are exposed. Current architectures assume homogeneous operating environments and often require highly controlled configurations such as is the case with database appliances and these limitations pose series impediments to cross platform and multi platform operation. Methods and techniques for implementing an EPRDBMS on a plurality of execution environments platforms are presented.

According to one aspect of the system described herein once the QRR has completely transformed the QPT into a QES the Query Execution Engine QEE determines how many compute nodes are required for each of the steps in the QES and provisions the appropriate compute nodes from the compute nodegroup associated with the connection.

Compute Elasticity in the EPRDBMS is achieved through the dynamic provisioning of compute nodes in the compute nodegroups associated with the client connections to the system. The EPRDBMS herein allows a different compute nodegroup to be associated with each connection while also allowing a compute nodegroup to be shared among some or all the connections to the system at any given instant. In response to system load and other preset conditions the system may adapt the membership in compute nodegroups to achieve compute elasticity.

The mechanism for provisioning a new compute nodegroup is dependent on the operating environment platform on which the EPRDBMS is running.

For example when run in an environment such as the Elastic Compute Cloud environment provided by Amazon the mechanism would involve a specific sequence of API calls. And when run in an environment such as the Joyent Cloud provided by Joyent the mechanism would be different. Finally when run in one s own data center the mechanism would be different still.

According to the present invention as depicted in the Query Execution Engine of the EPRDBMS interfaces with the Compute Node Plugin . The Compute Node Plugin provides Application Programming Interfaces API for the Query Execution Engine to invoke and these API s include Node Request and Node Release . When the Query Execution Engine determines that a query requires a compute node it makes the request of the Compute Node Plugin providing information as required by the API exposed by the Plugin. The Compute Node Plugin implements the API s using a Controller a Policy Manager a Monitoring Manager and manages information about nodes using a Node List and Node Utilization . The Compute Node Plugin provisions resources from the Platform by invoking some API s exposed by the Platform and . The Policy Rules implemented by the Policy Manager are specified in some format required by the Policy Manager .

The Controller in the Node Plugin responds to a Compute Node Request which may request one or more nodes and provide additional information about the kind of node s being requested. More particularly the Compute Node Plugin determines which of the nodes that it has already provisioned and is tracking in the Node List and which could be allocated to the requestor. When a Query Execution Engine is provided one or more nodes in response to a request this is recorded by the controller in the Node List.

The Controller in the Node Plugin in response to a Compute Node Release updates its Node List to record the fact that the Query Execution Engine is no longer using the Compute Node s being released.

Platforms may expose API s that allow for the monitoring of utilization and the price of resources at any given instant of time. The Monitoring makes use of API s provided by the platform to gather information about instance utilization and pricing and updates the Node Utilization table.

The term instance used in the previous sentence and in the following paragraphs is very closely related to the term node but not exactly identical. Cloud environments platforms allow requestors to provision machines and these machines are referred to as instances. An instance is therefore a virtual machine or physical machine provisioned by the platform in response to a request from a user of the platform. In the context of an EPRDBMS a Node is a collection of software programs running on one or more machines. In order to instantiate Compute Nodes that are being requested by a Query Execution Engine the Compute Node Plugin manages a pool of instances on which software programs are running and provides these to the Query Execution Engines as Compute Nodes.

For example in the Amazon EC2 cloud environment platform the API call DescribeSpotPriceHistory API call allows a requestor to find out the current price for a node based on specific parameters. Similarly the Amazon EC2 API provides such interfaces as RunInstances StopInstances and StartInstances to control the state of an instance in their cloud. The Monitoring may also inform the platform of the status of a running instance using the ReportInstanceStatus API call. Also the EC2 API provides for mechanisms to monitor such run time utilization elements as CPU Utilization Disk Reads and Writes and Memory utilization.

However other cloud environments platforms may provide different API s for these purposes or in some cases may not provide some of this functionality or may provide additional functionality.

The Controller periodically reviews the utilization of instances that have been provisioned from the platform and based on various Policies determines the optimum number of instances that must be maintained in the Node List at any time. Policy may stipulate such things as a minimum number of Compute Nodes to maintain at any time and a maximum number to maintain at any time. Optionally Policy may stipulate a maximum cost per time period for all compute nodes that are to be maintained and the Controller uses information from Pricing Information to implement this policy. Some platforms further have billing rules such as billing frequency where instances are billed in blocks of some number of minutes hours or days. For example instances in the Amazon Cloud are billed by the hour. Therefore if an instance is provisioned in the Amazon EC2 cloud in the On Demand model the amount that will be paid for this instance is determined by the formula COST CEILING UPTIME IN HOURS RATE where CEILING x is defined as 

Similarly in the Amazon Cloud if instances are provisioned in the Spot Instance model the RATE can change at any time and it is up to the Controller to ensure that the Policy is still respected.

Policy may specify rules in the form of absolutes cost per hour not to exceed a certain amount or in the form of soft conditions target cost per hour not to exceed a certain amount . In the latter case the controller is allowed the flexibility to violate the policy in some circumstances which may also be specified in the policy.

An EPRDBMS may be configured to operate with multiple platforms simultaneously. Each platform may have different API s capabilities and pricing. This is depicted in . The EPRDBMS and its Query Execution Engine interacts with a Compute Node Plugin which has been provided Policy Rules . The Compute Node Plugin has been instructed through Policy Rules to provision instances on Platforms and and .

QEE makes requests to the Compute Node Plugin using the API s provided by the Compute Node Plugin. The various elements of the Compute Node Plugin are analogous to and perform the same functions as their equivalent elements in .

Policy Rules instruct the Compute Node Plugin how and when to provision instances from each of the platforms. The Compute Node Plugin interacts with each platform using the API s exposed by that platform and . As each platform may provide a different API the Compute Node Plugin provides a layer of abstraction between the EPRDBMS and the various platforms and the mechanism to implement a policy across a plurality of different platforms. The QEE may request compute nodes from a particular platform and these requests are provided as part of the API calls supported by the Compute Node Plugin. These preferences from the QEE may be either requirements that the Compute Node Plugin must honor or recommendations that the Compute Node Plugin may honor at its sole discretion.

The EPRDBMS can allow for the specification of the policy rules in some format s such as for the purposes of illustration but not limitation a plain text file or an XML file. Further the invention provides for a mechanism whereby a user may modify the policy rules dynamically through the use of some mechanism s . One such mechanism is an extension to the SQL language understood by the EPRDBMS. A SQL command such as the one shown below may be used to reload policy rules for the system for the present connection or specify some other scope for the new policy rules.

Summary The cost and complexity of managing consistency in a distributed system have led to entire architectures that espouse relaxed consistency or entirely eschew the ability to preserve transaction consistency. Often the CAP Theorem is used to justify these simplifications. In practice however a large class of commercial applications require the ability to perform operations as atomic transactions where either the entire multi step operation is performed or none of the operation is performed but never just a proper subset of the operation. Successful implementation of these semantics in a distributed database management system often come at a high cost. These algorithms are particularly insufficient for application in systems such as elastic parallel database management systems where data is not only distributed but also often stored in a myriad of complex distribution patters for optimum query processing. Methods and techniques are presented to ensure data integrity and transaction consistency in a parallel database management system while dramatically reducing some of the inefficiencies common to conventional algorithms.

The Elastic Parallel Relational Database Management System EPRDBMS herein distributes data and query processing across a plurality of database management systems. When applications submit queries to the EPRDBMS the system translates these queries into a series of steps that must be executed on some or all of the underlying database management systems in a specified sequence in order to accomplish the intent of the input query.

A class of SQL queries modify data in a database these include DELETE INSERT UPDATE and TRUNCATE. Further the semantics of Relational Database Management Systems provide for a logical transaction which is a collection of operations that must be performed on the database and where the guarantee is that either all the operations must be performed or none of the operations are performed but never that some but not all of the operations are performed. This is known as database atomicity and this kind of logical transaction is often referred to as an atomic operation . This is particularly important in the case of systems where multiple operations are performed on different data elements that must be retained consistent within the database.

In a distributed database techniques for distributed transactions are commonly employed and one popular technique for this is the two phase commit 2PC . According to the rules of a 2PC each participating node in the distributed database is informed when a transaction is about to start and then a series of operations that form the transaction are performed. Then a component of the 2PC system called a transaction coordinator informs all nodes that they should prepare to commit . In response to a message to prepare to commit all nodes must perform any and all operations that they require in order to guarantee their ability to commit the operation if so instructed. If a node is not able to guarantee that it can commit the operation it shall respond to the transaction coordinator with a suitable response indicating such inability. Otherwise it may respond with a response indicating that it is prepared to commit . If the transaction coordinator receives a successful response to the prepare to commit command from all participating nodes then it may inform them that they should proceed to commit . Upon receiving a proceed to commit command all nodes should commit the transaction and reply to the transaction coordinator. It is a violation of 2PC for a transaction coordinator to force a proceed to commit if any participating node replied with an error in the prepare to commit phase. It is a violation of 2PC for a participating node to fail to commit when it receives a proceed to commit command if it previously replied with an affirmative response to the prepare to commit .

Distributed transactions come at a premium because of the additional messaging that is required between the various participating entities the transaction coordinator and the nodes participating in the transaction and the fact that there is no opportunity for parallelism amongst the various operations involved the prepare to commit and all responses must complete before the proceed to commit begins.

The EPRDBMS described herein provides several ways in which database tables may be distributed across the plurality of storage nodes that are part of the system. According to one aspect each table is associated with at least one storage group and an association with the first storage is established at the time when the table is created. Furthermore each table may be associated with more than one storage group and the data for the table may be distributed in a different way across the storage nodes that are part of each of the different storage groups the association between a table and its first storage group and the distribution of data on the first storage group being defined at the time when the table is created. At a later time a table may then be associated with the same or different storage groups and those associations would specify the data distribution on the newly associated storage group. Further different tables in the EPRDBMS may be associated with different storage groups.

During query processing the DQP may find it advantageous to use one distribution of a tables data in preference to another one such a determination being made based on the query or queries being processed and the operations that are being performed in those queries. Since the system will maintain a representation of data in the table such that changes are made in a transaction consistent manner the DQP is free to choose the distribution and storage nodegroup that is best and the results of the query would be identical if a different one were chosen. Without loss of generality the same query planning methods used if a table had only a single nodegroup association may be used if there are multiple storage nodegroups associated with the table except that the DQP now has more options to consider in picking the preferred QES.

When a table is associated with multiple storage groups a copy of the data is stored on the nodes that are part of each storage group. If as is the case with table T which is associated with SG and SG a copy of the data is stored on SG distributed according to EDD and a copy of the data is stored on SG Broadcast . In practice this means that Storage Nodes will have a slice of the data for Table T because Storage Node is part of SG. Storage Node will have a slice of data for table T because it is part of SG but it will also in addition have a complete copy of the data for table T because it is part of SG. Finally Storage Node will have a slice of data from T because it is part of SG and will have in addition a complete copy of T because it is part of Storage Group . Finally Storage Node will have a complete copy of T as it is part of SG.

As copies of data for a table may be maintained on a plurality of storage nodes a change to a table may require changes to be made to data on multiple storage nodes in a transaction consistent manner. When a logical transaction is performed by a series of queries the changes that are part of the logical transaction would similarly have to be made to multiple storage nodes in a transaction consistent manner.

When an application connects to the EPRDBMS the transaction state associated with the connection is initialized to an initial state.

The Dynamic Query Planner analyzes each of these operations and based on information obtained from the Catalog Metadata Manager and the Metadata generates Query Execution Steps that are an ordered sequence of operations that must be performed by Query Execution Engine in order to accomplish the intent of the query submitted by the application. When an operation that modifies data on the storage nodes is received by the EPRDBMS the Transaction State associated with the connection is updated to reflect this. During the course of the transaction each operation that modifies data on the various Storage Nodes causes updates to the Transaction State indicating the Storage Nodes where data was modified.

In the case of tables that are distributed according to an Elastic Data Distribution EDD if the SQL queries that modify data include information that help the DQP identify which storage nodes may have data that needs to be updated by the query this information is used in the DQP process in generating the QES on the minimum number of Storage Nodes required for the operation.

In the EPRDBMS tables are associated with one or more storage groups. Data for a single table may therefore be stored on more than one storage node.

For example if an UPDATE is made to table T and the DQP can determine that the change will affect the slice of data stored on Node due to the distribution of data according to EDD on SG the QES would include operations to update the data on nodes and because a broadcast copy of the data is stored on SG which resides on nodes and . This would imply that the transaction state associated with the connection would then be reflected to indicate that the change was made on storage nodes and .

On the other hand if an UPDATE is made to table T the DQP can determine that the change will affect the slice of data stored on Node due to the distribution of data according to EDD on SG the QES would include operations to update the data on nodes and because a broadcast copy of the data is stored on SG which resides on nodes and . This would imply that the transaction state associated with the connection would then be reflected to indicate that the change was made on storage nodes and .

For example if an operation was performed that UPDATED data in tables T and T within a single transaction and the DQP was able to determine that the change to T would affect the data on the slice on storage node and the change to T would affect the data on the slice on storage node then these two operations would both update the transaction state to indicate that the only changes that had been made affected storage node .

If the application indicates that transaction is to be committed the DQP inspects the transaction state associated with the connection and determines the number of Storage Nodes on which data was modified and that needs to be committed in a transaction consistent manner.

If it determines that the transaction need only be committed on a single storage node it can issue a simple directed commit to the single storage node. If it determines that the transaction needs to be committed on multiple storage nodes then it begins the process of distributed transaction on the storage nodes that have modified data.

Once the transaction is successfully committed the transaction state associated with the connection is cleared to an initial state.

If the application indicates that the transaction needs to be aborted or rolled back the DQP inspects the transaction state associated with the connection and determines the number of Storage Nodes on which data was modified and that needs to be rolled back in a transaction consistent manner.

If it determines that the transaction need only be rolled back on a single storage node it can issue a simple directed rollback to the single storage node. If it determines that the transaction needs to be rolled back on multiple storage nodes then it begins the process of distributed transaction on the storage nodes that have modified data.

Once the transaction is successfully rolled back the transaction state associated with the connection is cleared to an initial state.

In the example in paragraph 114 above a commit or rollback from the application would translate into a distributed commit or distributed rollback on storage nodes and . In the example in paragraph 115 above a commit or rollback from the application would translate into a distributed commit or rollback on storage nodes and . Finally in the example in paragraph 116 above a commit or rollback from the application would translate into a directed local or non distributed commit or rollback on only storage node .

Summary Parallel Database Management Systems have traditionally required the specification of some additional information to be used in determining data placement. These specifications take the form of extensions to the Data Definition Language DDL and therefore necessitate a change in the application that wishes to utilize the Parallel Database Management System. Such changes are costly and sometimes infeasible thereby impeding the migration of applications from an SMP database to an EPRBMS. Methods and techniques are presented that allow an unmodified application to be migrated to operate on parallel database management system through the definition of an out of band mechanism for metadata specification.

The Structured Query Language SQL includes a Data Definition Language DDL that is used to define the various objects that are stored in a relational database. For example the CREATE TABLE command is used to create a table and define the columns in the table and their data types and other attributes.

For example the below statement creates a table T with three columns A B and C and A is an integer B is a 10 character string and C is a datetime. Further the DDL defines A and B to be NOT NULL meaning that no row in that table is allowed to exist with a NULL value in either column A or B.

In addition to specifying the geometry of the table the columns their data types and any additional attributes the DDL syntax may be extended to include the specification of other information as described below.

Some Parallel Database Management Systems have extended this syntax by allowing the specification of data distribution. For example the EPRDBMS herein defines the following extension to the SQL Standard DDL Specification for CREATE TABLE as.

In this extended syntax the DDL defines a table T with three columns A B and C and indicates that the data is to be distributed according to an Elastic Data Distribution method based on the values of column A. As no storage group was specified the default storage group associated with the database will be used. This extension to the DDL syntax is an in band specification of the distribution information.

The EPRDBMS herein augments the DDL Specification in one other way and that is through the specification of out of band commands as part of the DDL.

When out of band DDL extensions are enabled with the EPRDBMS the administrator provides the EPRDBMS with these out of band commands in a form and format defined and understood by the EPRDBMS. This may be in the form of a simple text file or maybe some other format.

When a system is first installed and initialized a global set of out of band definitions may be provided to the system. These out of band definitions if specified are consulted on every DDL operation performed on the system. One kind of DDL operation is the CREATE DATABASE command. When the CREATE DATABASE command is executed out of band definitions in the system wide specifications are consulted and if appropriate the CREATE DATABASE command is augmented with some out of band specifications.

The EPRDBMS can further extend the syntax of the CREATE DATABASE command to allow for the specification of a set of out of band definitions to be used when DDL is executed in the context of that database.

When DDL is executed in the context of a database such as the CREATE TABLE command the EPRDBMS herein first consults the system wide out of band definitions if specified and attempts to augment the DDL statement and then consults the database specific out of band definitions if specified and attempts to further augment the DDL statement before arriving at a fully augmented DDL statement that is then executed.

Augmentation rules may be specified in the system wide specifications and in the database wide specifications. Depending on the specifications the EPDBMS may either override one set of specifications in favor of the other or apply some operation to merge the specifications provided in both places or apply some other scheme to determine which set of augmented specifications to apply and which to ignore in each instance. To facilitate this the specifications are defined in a manner that includes an identification such as a name and other attributes like priorities and other guidelines.

Some augmentation specifications may have some prerequisites and required follow on. Augmentation specifications can specify prologues and epilogues. When a DDL command is received by the EPRDBMS and a compatible out of band specification is found if that out of band specification contains a prologue rule the prologue is not immediately executed but rather added to the list of commands to be executed ahead of the present DDL. Similarly if an epilogue is found it is not immediately executed but rather added to the list of commands to be executed after the present DDL. The prologue and epilogue DDL are also processed to determine whether they require additional out of band specifications or not and it is possible to specify that an out of band specification is terminal in that it should not be further augmented. The terminal specification is an example of an attribute of an augmentation that allows the EPRDBMS to determine the exact sequence of commands to execute in response to an incoming DDL when multiple augmentations are specified.

The SPECIFICATION section may define multiple actions such as to REPLACE APPEND ALTER or perform some other modification to the DDL that is being augmented by the out of band specification. The EPRDBMS may assume a default action if none is provided and the default action is defined by the EPRDBMS.

The EPRDBMS herein thus provides for a mechanism whereby a user may modify the out of band metadata specifications dynamically. One such mechanism is an extension to the SQL language understood by the EPRDBMS.

A SQL command such as the one shown above may be used to reload out of band metadata specifications for the system for the present connection or specify some other scope for the new policy rules.

Summary Elastic Parallel Database Management Systems achieve storage elasticity through a technique described as a generational data store. The placement of data on a storage nodegroup is determined by use of this generational mechanism that allows for changes in the enrolment in the storage nodegroup while simultaneously ensuring data co location. Methods and techniques for extending the generational placement algorithm are provided to allow for the optimum placement of data on multiple nodes and providing for the ability to migrate specific data from one node to another while still ensuring co location.

The EPRDBMS utilizes Elastic Data Distribution EDD methods to distribute data over multiple nodes in a group in a deterministic manner to ensure co location.

One aspect of this approach is a generational data structure where each generation includes a Distribution Map DM that is used to determine what DV s were not seen when the subject generation was the current generation of the EDD.

When an EDD is created it is in its first generation and an empty DM is created. To determine whether a particular DV was seen before or not a sequential scan is conducted of the DMs beginning with the first DM and progressing till the current generation of the EDD or the first DM where it cannot be determined with certainty that the subject DV was not seen.

The EPRDBMS may add a new generation at any point in the sequence of generations not necessarily after the current generation. According to this aspect the EPRDBMS may create a new generation ahead of the first generation giving rise to a new first generation . The EPRDBMS may also create a new generation and insert it between two existing generations.

When presented with a row of data to be stored into a table distributed according to an EDD the EPRDBMS can sequentially scan the DM s of the EDD starting with the first generation until it either

According to this aspect the EPDBMS may relocate all instances of a given DV but following the process below 

As depicted in the bits that would be set in the DM bitmap for the subject DV 17 4 are as shown in and the first Generation where it cannot be determined for sure that the subject DV was not seen when the subject generation was the current generation is Generation in the Before scenario. Therefore the EPRDBMS inserts a new Generation before the old Generation now Generation in the After scenario and labeled and associates with the new Generation a DM that simply indicates that the only DV ever seen was the subject DV 17 4 and then proceeds to relocate all data with the subject DV to the desired location. In this manner any subsequent occurrence of the DV 17 4 will be dispatched according to the new Generation .

According to another aspect of the system described herein the EPRDBMS may choose to relocate multiple DV s and it would do so either by repeating the process described above multiple times one for each DV or create a single generation that would dispatch all of the DV s to be relocated using a single new generation and a suitable DM the subject new generation being inserted into the chain of generations at any point ahead of the first generation where it cannot be determined for sure that any one of the subject DV s had not been seen when the subject generation was the current generation. The degenerate case of this insertion would be to always create a new generation ahead of the current first generation thereby creating a new first generation and performing the relocations of the existing data matching the subject DV s.

The newly created generation may have a DM of any form and that the DM in one generation may have a form and format different from the form and format of the DM in any other generation.

Summary Advanced methods for distributing data in a parallel database management system suitable for certain classes of complex schemas are presented. Parallel database management systems have long provided the benefits of parallelism through the distribution of data across a plurality of servers and by having each server process queries based on its subset of the data while having all servers process these queries in parallel and at the same time. Data distribution the algorithms that determined where to store a row of data for optimum query processing invariably depended on some attribute in the row of data a column or a group of columns . In some classes of schema that are commonly in use today this mechanism is insufficient. One class of schemas is the hierarchical schema generally referred to as person child grandchild referring to the relationship between entities in a database that mirror this familial relationship. For optimum query processing performance one would like to store the person entity the child entity and the grandchild entity on the same node in the parallel database but schema optimization techniques such as normalization prevent the grandchild entity from having an attribute for the person entity thereby making it impossible to achieve optimal distribution and a normalized schema at the same time. The techniques described herein create a new class of distribution methods that make this possible.

In an EPRDBMS data in tables is distributed across multiple nodes that are members of a storage group or storage groups associated with a table. In data dependent distribution methods that provide co location rows in tables with identical values for the Distribution Vector DV are stored on the same storage node. This allows for parallelism in query processing.

For the purposes of this illustration assume that columns A and P have identical data types. This implies that if a row in T has a row with A 14 then any row in table T with A 14 will be on the same storage node as the first row. Further any row in T that has P 14 will also be on the same storage node as the rows in T with a value of A 14.

It would be beneficial for the operation of the EPRDBMS if co location of ATTACHMENTS data along with the QUOTATIONS data could be ensured. To achieve this the QUOTATIONS table and the ATTACHMENTS table would have to be distributed according to some EDD on the QUOTATION ID on the same Storage Group or one of the two tables would have to be BROADCAST and the other could be EDD on the same Storage Group. However it would be ideal if all three tables PROJECT QUOTATIONS and ATTACHMENTS could be co located for their respective joins on the foreign key relationships. In order to achieve this the ATTACHMENTS table would have to be distributed according to the PROJECT ID.

Normalization of the schema beyond the second normal form requires that the ATTACHMENTS table not contain the PROJECT ID. This is also good practice to eliminate anomalies in the data model.

To achieve co location of these three tables the ATTACHMENTS data must be distributed according to the PROJECT ID of its parent QUOTATION.

According to one aspect herein a table can be defined to be distributed according to data not present in the tables rows. This is achieved through an abstraction of a CONTAINER. According to this aspect the Distribution Vector for a table may consist of some attributes that are not part of that table or may even consist entirely of attributes that are not part of that table.

Accordingly a CONTAINER is defined to include data for tables that must be co located. illustrates this with a system with four sites site site site and site . The schema contains four tables PROJECT QUOTATIONS ATTACHMENT and ACCOUNTS. In this illustration the PROJECT table is defined as the CONTAINER BASE TABLE CBT and the data for the CBT is stored the four sites in the slices and . The QUOTATIONS table is defined as a CONTAINER MEMBER TABLE CMT and the data for the CMT is stored on the four sites in slices and . The ATTACHMENT table is defined as a CONTAINER MEMBER TABLE CMT and the data for the CMT is stored on the four sites in slices and . Finally the ACCOUNTS table is defined to be distributed according to the BROADCAST distribution model and therefore four identical slices of data are stored in and .

A CBT is a table that forms the basis of the CONTAINER based data distribution model. Data in that table is distributed using some distribution model on storage nodes that form part of an associated storage group. A CONTAINER ELEMENT CE is a row in the CBT. The CONTAINER DISTRIBUTION VECTOR CDV is the DV of the CE. A CONTAINER MEMBER TABLE CMT is a table that participates in container based data distribution according to some CBT. A client connection to the EPRDBMS has connection specific context and one of those items of context is the CONTAINER CONTEXT CC .

According to one aspect herein data for a CMT shall be stored co located along with the data for the appropriate data in the CBT through the mechanism of CONTAINER BASED DATA DISTRIBUTION and the extension of the definition of a DV of a table to include attributes that are not part of the table itself.

According to one aspect of the EPRDBMS herein each connection to the system has associated with it the Container Context CC for each container defined on the system. The CC for each container may be one of the NULL Context the GLOBAL Context the AUTOMATIC Context or a specified Container Context.

We first describe the operation of the system in the NULL GLOBAL and SPECIFIED Container Contexts and then describe the AUTOMATIC Container Context which is a hybrid mode of operation that extends the other three contexts.

According to one aspect herein when a system is operating in the NULL Context the system operates in a manner that reflects that CBT s are distributed according to their stated distributions and that all CMT s are random distributed on the storage group associated with their container. All tables not part of the container system are assumed to be distributed according to their stated distributions.

In the case of more complex schemas it is possible that there may be multiple hierarchies such as the illustration in . In this illustration tables C and D and are related by the relationship R and this is a many to many relationship. The tables C B and A and have a hierarchical N relationship as illustrated by the relationships Q and P . Similarly the tables D E and F and have a N relationship as illustrated by the relationships S and T . The equivalent DDL for this is illustrated on the right with statements through which create each of the tables A through F and define their key relationships.

The ER diagram and DDL as shown in can be converted into a CONTAINER BASED distribution as illustrated in . Two containers cContainer and dContainer are defined and . For the purposes of illustration cContainer was associated with the storage group sg and dContainer was associated with the storage group sg. In practice the two containers could have also been associated with the same storage group. The DDL statements through define the CMT s E F A and B respectively and the DDL statements and define the CBT s C and D respectively.

With this setup queries that only reference tables in a single container queries that reference only tables in the cContainer A B C or queries that reference only tables in the dContainer D E F are handled by the DQP in a manner analogous to the illustrations in and and as described above.

According to one aspect herein a connection to the EPRDBMS can have a CC relative to the cContainer and a CC relative to the dContainer and this CC is stored in the connection state associated with the connection. When a query is received by the EPRDBMS on this connection references to tables in each container group in the illustration of that is cContainer group consisting of table A B C and dContainer group consisting of D E F are planned based on their respective container context.

According to this aspect of the system the container context for each container on the system can be changed independently and without any impact on the other container contexts associated with the connection.

According to another aspect of the system all queries and not just SELECT s as illustrated above accessing data in tables participating in a container are restricted based on the established container context.

According to a further aspect herein the system supports a mechanism for relocation of a row in the CBT. To accomplish this and maintain co location of data one must also relocate all data in CMT s that correspond to the row in the CBT being relocated.

According to another aspect the AUTOMATIC Context is a hybrid context where the system may either operate in the NULL Context or the GLOBAL Context for all queries manipulating existing data and performs automatic detection of context for insertions into the CBT as described above and further performs automatic detection of context of the SPECIFIED CC for addition of data into CMT s.

We now describe the aspects of the system relating to the automatic detection of SPECIFIED CC for addition of data into CMT s.

Consider the relationships between the tables PROJECT QUOTATIONS and ATTACHMENTS described in and above and the CONTAINER based representation in that are described above. Further assume that the system is operating in the AUTOMATIC CC established as below.

Now assume that a user inserted a row into the table QUOTATIONS as illustrated in . The INSERT statement itself is shown on as . The process of performing the INSERT is described below and starts at step . A valid INSERT statement has been received and the DQP process first inspects the statement to determine whether the insertion is into a CMT or not . If not then the row will be dispatched to its correct storage node based on the rules for a non container based table or CBT as appropriate . On the other hand if it is found to be a CMT the system will verify whether a valid DISPATCHING RULE dispatching rules are described in the following sections is found and if not an error is generated . If a dispatching rule is found then the row is sent to the storage node indicated by that rule.

As described above the system relies on a DISPATCHING RULE to determine where to send data during AUTOMATIC CC operation when an insertion is performed in a CMT. According to one aspect herein the EPRDBMS defines an extension to the DDL syntax for table creation that allows for the definition of dispatching rules. According to one aspect these dispatching rules may be specified in band or out of band out of band definitions are described in detail starting in paragraph 124 below . According to another aspect the EPRDBMS defines an extension to the syntax of the SQL statement to allow for the definition of a dispatching rule for the row s being inserted.

A foreign key relationship is a simple example of a dispatching rule. Consider again the example of and the DDL for the various tables involved as described in . The DDL for the QUOTATIONS table included the following FOREIGN KEY PROJECT ID REFERENCES PROJECT PROJECT ID 

In a container based distribution such as this one the row in the QUOTATIONS table should be co located with the row in the PROJECTS table.

In the illustration in the insert statement identifies the PROJECT ID 19 and the EPRDBMS can use this information to determine that the row being inserted must be placed on the same storage node where the row with project id 19 resides. Referring back to we see that the project id 19 resides on site as depicted by the project id s shown in the slice of the project table on that site .

Accordingly therefore the EPRDBMS can dispatch the subject row being inserted into the QUOTATIONS table to site.

As the QUOTATION ID is 74 the same QUOTATION ID inserted in the system determines that the QUOTATION ID is valid and dispatches this row to site the location where the QUOTATIONS row was dispatched in .

According to one aspect the EPRDBMS supports extensions to the INSERT SQL syntax to allow for the definition of a dispatching rule if an FK relationship is not found. One situation where an FK relationship is not sufficient for the definition of a dispatching rule is the case of a polymorphic schema.

When operating in the AUTOMATIC CC if a row were inserted into the table with the value of rel type COMPLAINTS then the dispatching rule would use the polymorphic key a rule to determine that the rel id provided in the insert is to be treated as an FK relationship to the COMPLAINTS table and the row would be dispatched according to the location of the row in the COMPLAINTS table with that same id.

Summary Parallel Database Management Systems provide the benefits of parallelism by distributing data and processing onto a plurality of processing nodes and having each node process the subset of data stored thereon. Optimum data placement called data distribution is important in making this possible. Not all queries that are presented to the system can operate optimally because a data distribution for one set of queries may turn out to be sub optimal for another set of queries. In order to process queries it is sometimes required that data from multiple nodes be brought together first and in parallel database management systems this operation is often referred to as redistribution. Redistribution reduces the benefits of parallelism and therefore a reduction of the amount of data being redistributed is vital to the efficient operation of a parallel database management system. Methods and techniques for reducing data redistribution are presented.

As an EPRDBMS distributes data for user tables across a plurality of Storage Nodes and Parallelism is achieved by having the EPRDBMS generate Query Execution Steps QES that are executed in a specified sequence on a specified node Storage Node or Compute Node for example or nodes in parallel and at the same time it also defines mechanisms for data distribution such as Elastic Data Distribution to ensure the co location of data. When two tables are joined in a SQL query and the data for the tables is co located for the join the join operation can perform in parallel on the storage nodes. However if the data is not co located for the join then the DQP process will introduce redistribution operations on the incoming streams to the join to ensure that the data being joined is in fact co located for the purpose of the join.

An aspect of the EPRDBMS is the use of Compute Nodes as an elastic resource to facilitate these kinds of operations requiring data redistribution.

Assume further that T and T are not distributed in a manner that is co located for the purposes of the join T.X T.Y . This may be for one of many reasons such as that T and T are not on the same Storage Group or that one of T or T is distributed according to a random distribution and so on. In order to execute this query the EPRDBMS must perform a redistribution operation to generate copies of the data required for the join where the join can be performed in parallel on some collection of nodes Storage Nodes or Compute Nodes that are part of the system.

The first REDISTRIBUTE operation performs some deterministic computation on X and redistributes the data in table T columns A and X onto the various nodes compute nodes in the DEFAULT COMPUTE nodegroup associated with the connection. The second REDISTRIBUTE operation performs some deterministic computation on Y and redistributes the data in table T columns B and Y onto the various nodes compute nodes in the DEFAULT COMPUTE nodegroup associated with the connection. The third EXECUTE operation then performs the join between TEMP and TEMP that were created in the above two operations and on the default compute nodegroup associated with the connection. Since the REDISTRIBUTE operations determined where on which compute node in the default compute nodegroup to place each row from tables T and T based on a deterministic operation performed on X and Y the join columns it can be guaranteed that any row in T with a value of X that matched some row in T with the same value of Y would be such that the two rows would be on the same compute node.

Assume that table T has 100 rows and the possible values of X range between 1 and 100 and further assume that table T has 100 000 rows and the possible values of Y range between 1 and 100 000. According to the QES provided above the EPRDBMS would redistribute 100 rows from T into the nodes on the compute nodegroup associated with the connection and then proceed to redistribute 100 000 rows of data from T into the nodes on the compute nodegroup associated with the connection. In total this would result in a redistribution of 100 100 rows of data into the compute group. Then the join would be executed and would not use the rows from TEMP with values of Y between 101 and 100 000 because no rows in TEMP have a corresponding value of X. The cost of redistribution of this unused data includes the cost of unnecessarily reading of the data from table T the cost of moving the data from its storage node of origin to the compute node in the default compute nodegroup the cost of storing it on the compute node and the cost of then reading that data as part of the join between TEMP and TEMP.

According to one aspect of the EPRDBMS herein the amount of data redistributed as part of query processing not necessarily just joins though joins have been used in the illustration above is materially reduced through the use of redistribution inclusion lists. According to this aspect and further using the example provided above during the first REDISTRIBUTE operation on table T the EPRDBMS constructs a list of values of T.X that were redistributed into the compute nodegroup and this list is then used to augment the second REDISTRIBUTE operation to ensure that only those values of T.X which were seen in the first redistribution are now redistributed into TEMP.

Observe that the first REDISTRIBUTE operation generating TEMP from table T also produces the INCLUSION LIST called LIST X which is then used to restrict the values from table T that are REDISTRIBUTED as part of the second operation .

However LIST Y would include 100 000 values and in generating LIST Y the whole table would have been redistributed thereby providing no benefit.

One aspect of the EPRDBMS is that the DQP uses historical data and heuristics to determine the sequence in which to perform the steps in the QES if multiple orderings are possible in order to generate the sequence that is most efficient and this includes the optimization of the order of steps to produce the most effective INCLUSION LISTS.

The INCLUSION LIST is generated during an operation that processes data and may be generated either by the nodes originating the data or the nodes that are consuming the data.

In the example above assume that the table T is on storage group SGconsisting of nodes N N and N and further that the compute nodegroup associated with the connection is CN consisting of nodes N N N and N.

When the Query Execution Step below is executed data on storage nodes N N and N are read and redistributed to N N N and N.

As rows of data are received by nodes N . . . N they can each generate a list of values of X that were received by them and upon notification that all data from T has been read and redistributed the nodes N . . . N can each send the list of values of X that they received to the Query Execution Engine for inclusion in the subsequent step that scans T.

Equally nodes N N and N can generate a list of values of X that they are redistributing and when they have completed scanning their slice of table T they can send their part of the inclusion list to the Query Execution Engine for inclusion in the subsequent step that scans T.

The EPRDBMS thus implements both of these mechanisms for the generation of the INCLUSION LIST where it is generated by the originator or where it is generated by the receivers.

Broadly speaking an INCLUSION LIST is a data structure used by the EPRDBMS that is generated in one step in the QES and used to augment a subsequent step in the QES with additional filtering that will serve to reduce the overall cost of the query execution by identifying rows that should be processed further and those rows that need not be processed further.

The operation utilizing the INCLUSION LIST may utilize the list directly as a filter in the SQL sent to the database on the target nodes where the query execution step is being executed or as a filter applied by the Query Execution Engine as part of the subsequent processing of the data by the EPRDBMS.

The data structure used to implement an INCLUSION LIST may be a simple list enumerating all values that are part of the list. When there are a large number of entries in the list it may be more efficient to use some other data structure than a list. One such mechanism to generate an INCLUSION LIST is through the use of a bitmap where each bit in the bitmap represents some value to include in the list.

For example assume that the values in an inclusion list were integers in the range 1 to 1000 a bitmap of 1000 bits could be used where the i th bit represented the value i .

According to one aspect herein an INCLUSION LIST may include false positives but never include false negatives . In other words an INCLUSION LIST that specifies more values than required is sub optimal but perfectly functional. However an INCLUSION LIST that fails to include some values is not permissible.

According to this aspect an INCLUSION LIST may be a bitmap of some size N . When a value of x needs to be added to the list a deterministic hash CRC for example of x is first computed and then the Modulus operation is used to reduce the value of HASH x to a value in the range of 0 N 1 and that bit in the bitmap is used to represent the value x . The bit may be determined by a formula such as MODULUS HASH x N 

When using this method an initial INCLUSION LIST consisting of N bits is initialized to include all bits of some initial value. To record the inclusion of a particular value in the bitmap the bit corresponding to the value as determined above is set to the non initial value.

Once all values of the list have been set in the bitmap it can be used in a subsequent step in the QES to determine whether a particular value should be processed further or not as follows. For example assume that the value y is being evaluated. We first compute MODULUS HASH y N and determine whether the corresponding bit is set to the non initial value. If it is then that value may have been encountered during the creation of the list and appropriate actions can be taken.

One useful attribute of this mechanism of computing the INCLUSION LIST is that it can be computed in parallel by multiple nodes and then combined by the Query Execution Engine to generate the final inclusion list.

According to one aspect the inclusion list is computed by the receiver nodes of the data. As illustrated above nodes N N N and N would maintain lists as they received data from nodes N N and N as part of the redistribution of table T. By prior agreement nodes N . . . N would use a bitmap of N bits with a common initial value say 0 and set some of the bits in the bitmap to 1 based on the rows of data that they received and as described above. When all data has been received they each send their bitmaps to the Query Execution Engine. Assume that these bitmaps are B B B and B respectively. The Query Execution Engine computes the INCLUSION LIST bitmap B a bitmap of N bits as follows 4567

Therefore bit i in the bitmap B will be set to the value of 1 if and only if it was set to 1 in one of the bitmaps B B B or B.

According to one aspect the data structure maintained by each of the nodes generating a portion of the INCLUSION LIST based on the data they process may not be amenable to this simple mechanism for consolidation. In that case the data structure generated by the Query Execution Engine may be merely a reference to the individual data structures generated. Using the illustration above if nodes N . . . N generated some data structures DS DS DS and DS the Query Execution Engine may generate the INCLUSION LIST merely as DS DS DS DS . When a subsequent step wishes to use this list it would determine whether a given value was seen during the generation of the list by following the same steps that were used by nodes N . . . N in encoding the data structure and then sequentially inspecting the data structures DS . . . DS till a positive indication is found that the subject value was seen in the preceding step.

According to another aspect herein the INCLUSION LIST may be constructed as follows. Each of the participating slices generates M bitmaps each with Nbits. For each value x that they encounter they set multiple bits as follows 

In this scheme with M bitmaps M different hashing algorithms are used. It is not necessary that these be M distinct hashing algorithms but if HASHis the same as HASH for some values i and j then it is required that N N. Assuming that there are l nodes numbered through l then the INCLUSION LIST computed by the Query Execution Engine shall be computed as follows

According to another aspect herein the data structure used to generate the inclusion list may be a bitmap with N bits and where each of the generating nodes produced a bitmap with N bits as follows. For each observed value that is to be included in the list multiple bits are set in the bitmap. Assume that m hashes are used then the occurrence of a single value in the list would be recorded by setting the following bits.

The Query Execution Engine would consolidate the bitmaps received from each of the l participating nodes and generate a single bitmap B as

Through the use of INCLUSION LISTS the EPRDBMS herein reduces the amount of data that must be processed. It finds application in a wide variety of queries including but not limited to joins aggregations and sorting.

The teachings of all patents published applications and references cited herein are incorporated by reference in their entirety.

While this invention has been particularly shown and described with references to example embodiments thereof it will be understood by those skilled in the art that various changes in form and details may be made therein without departing from the scope of the invention encompassed by the appended claims.

