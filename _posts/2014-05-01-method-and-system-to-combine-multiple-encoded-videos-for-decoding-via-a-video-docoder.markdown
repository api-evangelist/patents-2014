---

title: Method and system to combine multiple encoded videos for decoding via a video docoder
abstract: Methods and systems described herein provide for decoding multiple video streams using a single decoder. An example method may include receiving a first data stream that represents encoded frames of a first video and receiving one or more additional data streams that each represent encoded frames of a respective video. For each encoded frame of the received first data stream, the method may also include, combining a respective frame of each of the one or more additional data streams with the encoded first frame such that each encoded frame of the first video includes a respective frame of the first video and a respective frame of each of the one or more additional data streams. The method may then include decoding each combined frame of the first data stream. The method may also include providing for display the decoded frames.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09031138&OS=09031138&RS=09031138
owner: Google Inc.
number: 09031138
owner_city: Mountain View
owner_country: US
publication_date: 20140501
---
Unless otherwise indicated herein the materials described in this section are not prior art to the claims in this application and are not admitted to be prior art by inclusion in this section.

Media such as video and audio is often compressed to reduce the quantity of data needed to represent the media commonly known as the bit rate . By reducing the bit rate a smaller amount of data storage may be needed to store the video and less bandwidth may be needed to transmit the video over a network. Numerous video codecs coder decoders exist as hardware and or software components that enable compression or decompression of digital video. To decode compressed video many types of computing devices have hardware and or software decoding units.

Decoding video can be a CPU intensive task especially for higher resolutions like 1080p. Therefore while video decoders may be implemented as software modules that execute on general purpose processors in many circumstances specialized hardware decoders may be more efficient and or more capable especially with regards to decoding higher resolution video. Some low power devices such mobile computing devices may be equipped with general purpose processors that may have difficulty decoding high resolution video in real time and or within an acceptable power envelope. To assist in decoding such videos some mobile computing devices such as smartphones and tablets may be equipped with one or more hardware video decoders configured to decode compressed video.

In one example a method is provided that includes receiving by a processor a first data stream that represents encoded frames of a first video and receiving a second data stream that represents encoded frames of a second video. For each frame of the received first data stream the method further includes defining in a header of the frame that the frame is a first slice of a respective output frame. And for each frame of the received second data stream the method includes defining in a header of the frame that the frame is a second slice of the respective output frame and that the second slice is stacked vertically underneath the first slice in the respective output frame. The method then includes generating an output data stream that is configured to represent the encoded frames of the first video as respective first slices of encoded output frames and to represent the encoded frames of the second video as respective second slices of the encoded output frames such that a respective output frame is divided into a respective first slice and a respective second slice. The method also includes sending the generated output data stream to a decoder.

In another example a computer readable storage memory is provided that has stored therein instructions that when executed by a processor cause the processor to perform functions. The functions comprise receiving a first data stream that represents encoded frames of a first video and receiving one or more additional data streams that each represent encoded frames of a respective video. For each encoded frame of the received first data stream the functions also comprise combining a respective frame of each of the one or more additional data streams with the encoded first frame such that each encoded first frame includes a respective frame of the first video and a respective frame of each of the one or more additional data streams. The functions further comprise decoding by a particular decoder each combined frame of the first data stream into an output surface format that represents the frames of the first video and the frames of each of the videos of the one or more additional data streams. The functions also comprise providing for concurrent display the decoded frames of the first video and the decoded frames of each of the videos of the one or more additional data streams.

In still another example a device is provided that comprises one or more processors and data storage configured to store instructions that when executed by the one or more processors cause the device to perform functions. The functions comprise receiving a first data stream that represents encoded frames of a first video where each encoded frame of the first video is divided into a respective first macroblock array and receiving a second data stream that represents encoded frames of a second video where each encoded frame of the second video is divided into a respective second macroblock array. The functions also comprise combining the received first data stream and the received second data stream into an output data stream that is configured to represent encoded output frames. Each encoded output frame includes the second macroblock array of a respective encoded frame of the second video vertically concatenated to the first macroblock array of a respective encoded frame of the first video. The functions further comprise decoding the output data stream into an output surface format that includes in respective non overlapping regions decoded frames of the first video and decoded frames of the second video and providing the decoded frames of the first video and the decoded frames of the second video for display.

In yet another example a system is provided that includes a means for receiving a first data stream that represents encoded frames of a first video and a means for receiving one or more additional data streams that each represent encoded frames of a respective video. For each encoded frame of the received first data stream the system further includes a means for combining a respective frame of each of the one or more additional data streams with the encoded frame of the first video such that each encoded frame includes a respective frame of the first video and a respective frame of each of the one or more additional data streams. The system further includes a means for decoding by a particular decoder each combined frame of the first data stream into an output surface format that represents the frames of the first video and the frames of each of the videos of the one or more additional data streams. The system further includes a means for providing for display the decoded frames of the first video and the decoded frames of each of the videos of the one or more additional data streams.

These as well as other aspects advantages and alternatives will become apparent to those of ordinary skill in the art by reading the following detailed description with reference where appropriate to the accompanying figures.

The following detailed description describes various features and functions of the disclosed systems and methods with reference to the accompanying figures. In the figures similar symbols identify similar components unless context dictates otherwise. The illustrative system and method embodiments described herein are not meant to be limiting. It may be readily understood that certain aspects of the disclosed systems and methods can be arranged and combined in a wide variety of different configurations all of which are contemplated herein.

A given computing device may have a video decoder that assists the computing device in decoding encoded videos. In some cases the video decoder may be implemented within the computing device as specialized hardware configured to execute specific code algorithms which function to decode video. The specialized hardware may support decoding a certain number of video streams concurrently. For instance the specialized hardware may include a total of two hardware decoder units each of which may support decoding one video at a time.

At the same time some applications may provide for displaying more videos concurrently than the specialized hardware can decode concurrently. For example a video chat application that supports group chat with multiple computing devices concurrently may provide for concurrent display a video stream from each computing device. As another example a news feed application may provide for display of a preview video next to each news item in a news feed of multiple news items. When an application provides for the concurrent display of more video streams than a given computing device supports in hardware the computing device may decode one or more of the video streams using a software decoder. However decoding in software on a general purpose processor is not typically as efficient as decoding on a hardware decoder. In some cases the general purpose processor on which the software decoder is executing may not be fast enough to decode the one or more video streams in real time. Also in some cases decoding in software may significantly increase the processor utilization which may negatively affect the user experience in various ways such as by causing the UI to feel sluggish or unresponsive. In some cases decoding a video in software may also use relatively more battery charge than used by decoding the video using specialized hardware. As an alternative to decoding with a software decoder manufacturers may include additional decoding units in their computing devices. However this solution may increase the cost and power consumption of the computing devices.

Within examples methods and systems are provided for decoding via a single video decoder multiple compressed video sources for concurrent display. For example a processor may receive two or more data streams that each represent an encoded video. The processor may then combine the two or more data streams into an output data stream that represents frames having content from each of the two or more data streams. The processor may then provide the output data stream as input to a video decoder which may provide decoded frames that include content from each of the two or more video streams. The processor may then divide the outputted frames into their component videos.

It should be understood that arrangements described herein are for purposes of example only. As such those skilled in the art will appreciate that other arrangements and other elements e.g. machines interfaces functions orders and groupings of functions etc. can be used instead and some elements may be omitted altogether according to the desired results. Further many of the elements that are described are functional entities that may be implemented as discrete or distributed components or in conjunction with other components in any suitable combination and location or other structural elements described as independent structures may be combined.

Referring now to the figures illustrates an example computing device by which an example method may be implemented. Computing device may include applications and and an operating system being executed by hardware . Although the example computing device is a smartphone aspects of this disclosure are applicable to other computing devices such as PCs laptops tablet computers etc.

Each of the applications and may include instructions that when executed cause the computing device to perform specific tasks or functions. Applications and may be native applications i.e. installed by a manufacturer of the computing device and or a manufacturer of the operating system or may be a third party application installed by a user of the computing device after purchasing the computing device. A non exhaustive list of example applications includes a media player application that accepts media files as inputs and generates corresponding video and or audio to the output device s a video gallery application a video communication application e.g. a video chat or video call application an e reader application which accepts electronic documents books magazines etc. as input and presents the content of the document via the output device s a feed reader that accepts feeds delivered over the Internet e.g. RSS feeds and or feeds from social network sites as input and presents the feeds via the output device s a map application that displays a map via the output device s a note taking application a bookmarking application and a word processing spreadsheet and or presentation application that accepts specifically formatted files as inputs and presents them via the output devices for viewing and or editing.

The operating system may interact with and manage hardware to provide services for the applications and . For example an application may request that the operating system direct an integrated camera of hardware to capture a visual image and that the hardware store the image to memory.

The hardware may include for example a central processing unit CPU a graphics processor GPU memory an input output I O interface user input device s and output device s . Components of hardware may be controlled by instructions contained in applications and and operating system .

The central processing unit CPU may be operable to effectuate the operation of the computing device by executing instructions stored in memory or disk storage. Such instructions may include the operating system and the applications and . The CPU may for example comprise a single or multi core processor an application specific integrated circuit ASIC field programmable gate array FPGA and or any other suitable circuitry.

The graphics processor may be operable to generate a video stream for output to the screen based on instructions and or data received from the CPU. That is data structures corresponding to images to be displayed on the screen may be stored to and read from the memory or disk storage by the CPU. The CPU may convey such data structures to the graphics processor via a standardized application programming interface API such as for example Standard Widget Toolkit SWT the DirectX Video Acceleration API the Video Decode Acceleration Framework API or other suitable API.

The memory may include program memory and run time memory. The memory may for example comprise non volatile memory volatile memory read only memory ROM random access memory RAM flash memory magnetic storage and or any other suitable memory. Program memory may store instructions executable by the CPU to effectuate operation of the operating system and the applications and . Runtime memory may store data generated or used during execution of the operating system or applications and

The input output I O interface may be operable to receive signals from the input device s and provide corresponding signals to the CPU and or the graphics processor.

The input device s may include for example a mouse a touchpad a motion sensor a trackball a voice recognition device a keyboard or any other suitable input device which enables a user to interact with the computing device .

The output devices may include for example a screen and speakers. The screen may be for example a liquid crystal display LCD screen an OLED screen an e ink screen and or any other suitable device for presenting a graphical user interface.

In some implementations the device may include a device platform not shown which may be configured as a multi layered Linux platform. The device platform may include different applications and an application framework as well as various kernels libraries and runtime entities. In other examples other formats or systems may operate the device as well.

The device may include an interface a wireless communication component a cellular radio communication component sensor s data storage and a processor . Components illustrated in may be linked together by a communication link . The device may also include hardware to enable communication within the device and between the device and another computing device not shown such as a server entity. The hardware may include transmitters receivers and antennas for example.

The interface may be configured to allow the device to communicate with another computing device not shown such as a server. Thus the interface may be configured to receive input data from one or more computing devices and may also be configured to send output data to the one or more computing devices. In some examples the interface may also maintain and manage records of data received and sent by the device . In other examples records of data may be maintained and managed by other components of the device . The interface may also include a receiver and transmitter to receive and send data. In other examples the interface may also include a user interface such as a keyboard microphone touchscreen etc. to receive inputs as well.

The wireless communication component may be a communication interface that is configured to facilitate wireless data communication for the device according to one or more wireless communication standards. For example the wireless communication component may include a Wi Fi communication component that is configured to facilitate wireless data communication according to one or more IEEE 802.11 standards. As another example the wireless communication component may include a Bluetooth communication component that is configured to facilitate wireless data communication according to one or more Bluetooth standards. Other examples are also possible.

The cellular radio component may be a communication interface that is configured to facilitate wireless data communication according to one or one cellular radio standards. For example the cellular radio component may include a cellular radio that is configured to facilitate wireless data communication according to one or more cellular standards such as the Global System for Mobile Communications GSM Code Division Multiple Access Long Term Evolution LTE Worldwide Interoperability for Microwave Access WiMax among others.

The sensor may include one or more sensors or may represent one or more sensors included within the client device . Example sensors include an accelerometer gyroscope pedometer light sensors microphone camera or other location and or context aware sensors.

The data storage may store program logic that can be accessed and executed by the processor . The data storage may also store data that may include data received by any of the wireless communication component the cellular radio communication component the GPS and any of sensors . For instance the data storage may store one or more data streams that represent video.

The processor may be a general purpose processor that may access program logic that can be retrieved and executed by the processor . The processor may include one or more decoder s that may be configured to decode encoded video. A non exhaustive list of example video codecs that may be processed by the one or more decoder s include MPEG e.g. H.264 MPEG 4 AVC and H.265 MPEG H HEVC Windows Media Video WMV On2 e.g. VP8 and VP9 Sorenson Dirac Cinepak and RealVideo.

The device is illustrated to include a graphics processing unit GPU . The GPU may be configured to control other aspects of the device including displays or outputs of the device . The GPU may include one or more decoder s that may be configured to decode video that has been compressed using one or more of the above referenced video codecs among other examples.

The communication link is illustrated as a wired connection however wireless connections may also be used. For example the communication link may be a wired serial bus such as a universal serial bus or a parallel bus or a wireless connection using e.g. short range wireless radio technology communication protocols described in IEEE 802.11 including any IEEE 802.11 revisions or Cellular technology among other possibilities.

Example methods described herein may be performed individually by components of the device or in combination by one or all of the components of the device . In one instance portions of the device may process data and provide an output internally in the device to the processor for example. In other instances portions of the device may process data and provide outputs externally to other computing devices.

In addition for the method and other processes and methods disclosed herein the flowchart shows functionality and operation of one possible implementation of present embodiments. In this regard each block may represent a module a segment or a portion of program code which includes one or more instructions executable by a processor for implementing specific logical functions or steps in the process. The program code may be stored on any type of computer readable medium for example such as a storage device including a disk or hard drive. The computer readable medium may include a non transitory computer readable medium for example such as computer readable media that stores data for short periods of time like register memory processor cache and Random Access Memory RAM . The computer readable medium may also include non transitory media such as secondary or persistent long term storage like read only memory ROM optical or magnetic disks compact disc read only memory CD ROM for example. The computer readable media may also be any other volatile or non volatile storage systems. The computer readable medium may be considered a computer readable storage medium a tangible storage device or other article of manufacture for example. The program code or data for the code may also be stored or provided on other media including communication media such as a wireless communication media for example.

In addition for the method and other processes and methods disclosed herein each block in may represent circuitry that is wired to perform the specific logical functions in the process.

Functions of the method may be fully performed by a processor of a computing device or may be distributed across multiple components of a computing device. In some examples the functions of method may be distributed across multiple computing devices and or a server.

At block the method involves receiving a first data stream that represents encoded frames of a first video and receiving a second data stream that represents encoded frames of a second video. While the functions of receiving the first data stream and receiving the second data stream are discussed herein as one function they may be implemented as separate functions which may be performed in different ways. Also while a first data stream and a second data stream are described by way of example the present method may involve receiving a plurality of data streams such as a third data stream and a fourth data stream.

For instance processor of device in may receive the first data stream and the second data stream. In some examples processor may receive the first data stream and the second data stream from another computing device via interface and communication link for example. In other examples processor may receive the first data stream and the second data stream from data storage via communication link . Computing device may temporarily store i.e. buffer the first data stream and the second data stream in data storage when the first data stream and the second data stream are received via interface . Alternatively computing device may store the first data stream and the second data stream as video files in data storage . Other examples are possible as well.

A video may include a sequence of images also known as frames . As noted above the first data stream may represent encoded frames of a first video and the second data stream may represent encoded frames of a second video. For instance any suitable computing device may have encoded one or more frames of the first video into the first data stream using a video codec. Further any suitable computing device may have encoded one or more frames of the second video into the second data stream. Any suitable video codec may be used to encode the frames including without limitation MPEG developed codecs e.g. H.264 MPEG 4 AVC and H.265 MPEG H HEVC Windows Media Video WMV On2 codecs e.g. VP8 and VP9 Sorenson Dirac Cinepak or RealVideo. Although the processor may receive first video and second videos as encoded data streams encoding the first video and or the second video are not necessary aspects of the invention.

Depending on the video codec used to encode the first video the first data stream and the second data stream may be arranged into different formats. As noted above in some circumstances the videos may be encoded using the H.264 MPEG 4 AVC H.264 codec into a H.264 bitstream. A properly encoded H.264 bitstream contains sufficient information to decode the encoded frames using a decoder. H.264 encoded frames of a video are typically referred to as pictures however for simplicity this description will use the term frame to refer generically to H.264 pictures and frames of a video. The H.264 bitstream is divided into packets known as Network Abstraction Layer NAL packets. Other codecs may also format data streams into packets or may use alternate data formats.

Bitstream includes two types of headers a Sequence Parameter Set SPS and a Picture Parameter Set PPS . The Sequence Parameter Set contains information referring to a sequence of NAL packets e.g. a sequence of slice packets representing frames of a video . The SPS may indicate various information about a sequence such as the height and width of frames represented by the sequence. The Picture Parameter Set PPS contains information referring to one or more frames.

Bitstream also includes instantaneous decoding refresh IDR packet . A IDR packet contains the information necessary to decode one frame. Some H.264 bitstreams use predictive encoding which involves referencing information in other packets to encode or decode frames. A decoder may decode one or more subsequent frames in the bitstream by reference to the IDR packet. IDR packets may also be referred to as key frames or I frames.

Referring back to the representative slice data packets and each slice data packet like other NAL packets may include a slice header and data. Slice data packet is shown by way of example as including a slice header and data . Data is further divided into a macroblock array . The macroblock array includes at least one macroblock but may include a plurality of macroblocks as shown. The H.264 format divides slices into processing units known as macroblocks. Many video codecs use some type of processing unit. The term macroblock is used in this description to refer to H.264 macroblocks as well as macroblocks in other MPEG codecs coding tree units in the H.265 codec megablocks in the VP8 9 codecs and any other similar or equivalent part of a frame in any codec.

In the H.264 codec the slice header contains information about the slice. For instance the slice header may indicate the address of the first macroblock in the slice among other parameters. Under H.264 macroblocks in a frame are addressed in raster scan order i.e. left to right top to bottom . Other codecs may address component parts of frames in different ways all of which are contemplated herein. The slice header may also indicate which slice of a frame that the packet belongs to such as the first slice or the second slice.

While shows the macroblocks of slices and in raster scan order in the bitstream the macroblocks of each slice may be sequential elements of a macroblock array as exemplified by array of . By referencing data in the header such as the height and width of the slice and the address of the first macroblock in the slice a decoder can arrange a sequence of macroblocks into a frame. For instance slice has a height of 4 macro blocks a width of 4 macroblocks and the first macroblock in the slice is at address 0. Therefore under raster scan ordering left to right top to bottom the first macroblock of slice is at address 0 in the top left corner of the frame as shown in . Since the width of the slice is 4 macroblocks the first row includes the first four macroblocks of the frame addressed at macroblock 1 2 and 3 respectively as shown. The second row includes the next four macroblocks the third row includes the next four macroblocks after those and the fourth row includes the last four macroblocks also as shown. As noted above the height of the slice is 4 macroblocks and accordingly there are four rows of macroblocks in the array.

While a H.264 bitstream is described by way of example this disclosure contemplates the different formats used by other codecs. Terms used throughout the description in describing components of H.264 are intended to refer to the H.264 component as well as any equivalent structures in other codecs. For instance header refers to H.264 headers such as the Sequence Parameter Set the Picture Parameter Set and each slice header. And header also refers generically to any portion of a data stream that contains information about the data stream. Frame refers to H.264 pictures or frames and also refers generically to corresponding structures in any other codec. Macroblock refers specifically to H.264 and generically to sub components of a frame as used by other codecs. Other terms used within the specification are intended to refer generally to other codecs as well.

At block the method involves combining the received first data stream and the received second data stream into an output data stream. The output data stream may represent encoded frames of the first video and encoded frames of the second video such that a decoder may decode the output data stream into an output format that represents both the first video and the second video.

The output data stream may be configured to represent encoded output frames. The processor may combine a frame from the first data stream and a frame from the second data stream into an encoded output frame. illustrates an example output frame that includes a first slice and a second slice . The processor may combine a frame from the first data stream e.g. frame which is represented by slice and a frame from the second data stream e.g. frame which is represented by slice into encoded output frame .

The processor may combine frames from the received data streams into different arrangements. In some embodiments the processor may arrange the frames from the received data streams into a vertical stack. For example slice of is vertically stacked onto slice . In other embodiments the processor may arrange the frames from the received data streams into a grid.

For decoding by particular decoders the processor may configure the frames of the received data streams into different arrangements. For instance some decoders may support decoding frames that are no wider than a maximum frame width. Further the maximum frame width supported by some decoders may be relatively narrow such that some horizontal arrangements of frames may exceed the maximum frame width. To arrange frames from the received data streams for such decoders the frames may be vertically stacked upon one another creating a relatively tall and narrow frame. illustrates an example frame that includes slices and in a vertically stacked configuration. Further some decoders may support decoding frames up to relatively narrow maximum frame height which may be exceeded when a certain number of frames are vertically stacked. In such a circumstance the processor may arrange the frames into a grid. illustrates an example frame in which slices and are arranged into a rectangular grid.

The processor may combine frames from the first data stream and the second data stream on a frame by frame basis. As noted above the first video and the second video may be represented by first frames in sequence and by second frames in sequence respectively. Each sequence of frames may include a first frame a second frame a third frame and so on. The processor may combine the first frame of the first data stream and the first frame of the second data stream into the first output frame of the output data stream. The processor may also combine the second frame of the first data stream and the second frame of the second data stream into the second output frame of the output data stream. The processor may repeat this process until it reaches the last frame of either data stream.

The processor may combine the first data stream and the second data stream into the output data stream using a variety of techniques. For example the processor may decode the data streams combine the video into an output video and then encode the output video into the output data stream. Alternatively the processor may combine the first data stream and the second data stream into the output data stream without fully decoding the first data stream and the second data stream. For instance the processor may generate the output stream and arrange the encoded frames within the output stream. Alternatively the processor may modify the first data stream to include the second data stream. Other examples are possible as well.

As noted above in some embodiments a decoder may decode the first data stream and the second data stream before the processor combines frames of the first data stream with frames of the second data stream. For example an application executing on a mobile computing device such as a smartphone or tablet computer may request the first video and the second video from a server that is accessible over a network such as the Internet. The server may then send the first video and the second video as a first data stream and a second data stream respectively to an intermediate computing device e.g. a server having one or more video encoder decoders and at least one processor. The one or more one or more video encoder decoders may decode the first data stream and the second data stream into the first and second video. The processor of the intermediate computing device may then combine the frames of the first video with frames of the second video into an output video. The one or more one or more video encoder decoders may then encode the output video into the output data stream. The intermediate computing device may then send the output data stream to the mobile computing device which may decode the output data stream using its decoder.

In some embodiments combining the received data streams into an output data stream may involve defining in one or more headers of each received data stream that the frames represented by the received data stream are component parts of the output data stream. For instance referring back to frame of the first video may be represented by slice of the first data stream. Within the first data stream slice may be a particular NAL packet of the slice data type. The slice data packet may have a header containing one or more parameters that indicate to a decoder that slice and in turn frame is part of the first data stream. Slice may also have its own header indicating that slice and thus frame is part of the second data stream. When a decoder decodes a slice data packet the decoder may reference the one or more parameters in determining which data stream the slice data packet is in. NAL packets of the same data stream may each have the same indication within their respective header which may group the packets into the same video when decoded.

The processor may use such parameters to combine the first data stream and the second data stream into the output data stream. For each frame of the received first data stream the processor may decode or parse from a header of the first data stream one or more parameters that indicate that the frame is a slice of the first data stream. Then the processor may define or re define the one or more parameters to indicate that the frame is a first slice of a respective output frame. And for each frame of the received second data stream the processor may decode one or more parameters from a header of the second data stream that indicate that the frame is a second slice of the respective output frame. For instance the processor may define in a header of slice that slice is slice of frame and also define in a header of slice that slice is slice of frame . Then for example when frame is decoded the decoder will arrange the content of frame as slice of frame of the output data stream rather than as slice of the first data stream. And the decoder will arranged the content of frame as slice of frame of the output data stream.

The specific parameters in the header used to indicate that the frame is a slice of an output frame may vary based on the codec used to encode the received data streams. In some cases one or more parameters may explicitly define the frame as a slice of an output frame. In other cases the one or more parameters may implicitly define the frame as a slice of an output frame. For instance changing an address in a header may define the frame as a slice of an output frame.

To generate the output data stream the processor may create a header defining the output data stream. For instance with H.264 the processor may create a SPS packet. The header of the output data stream may indicate that the output frames are part of the output data stream. The processor may also define various parameters of the output data stream in the header such as the output frame height and the output frame width. Some of the parameters may be based on aspects of the first data stream and or the second data stream. For instance the processor may decode or parse from the first data stream and the second data stream a first frame height and a second frame height which may indicate the height of frames of the first data stream and the second data stream respectively. Then when vertically stacking frames the processor may define in the header the height of the output frames as the sum of the first frame height and the second frame height. For instance when combining slice and into frame the processor may define the frame height of the output data stream as 8 macroblocks and the frame width as 4 macroblocks as the two slices are each 4 macroblocks high and 4 macroblocks wide.

Alternatively to generate the output data stream the processor may define the first data stream as the output stream and then concatenate frames from the other received data streams to the first data stream. The processor may also decode parameters from one or more headers of the first data stream and re define the one or more parameters as necessary. For instance the processor may parse out the frame height of the first data stream and re define the frame height as the sum of the frame heights of the frames of the first data stream and the heights of the frames of each additional data stream that is combined with the first data stream.

To arrange a frame of the first data stream and a frame of the second data stream within the output frame the processor may define or re define various parameters contained within one or more headers of the received data streams or in a header of the output data stream. As noted above slice headers may contain an indication of the address of the first macroblock in the slice. For instance when combining slice and into frame i.e. a vertically stacked configuration the processor may define the address of the first macroblock of slice as address 16. Address 16 is incremented by a macroblock from the address of the last macroblock of the first slice i.e. address 15 indicating the sixteenth macroblock of slice . As another example referring to to arrange slices and into a vertically stacked configuration as shown the processor may define the address of the first macroblock of the first slice slice as 0. The processor may also define addresses of the first macroblock of slices and as 16 32 and 48 respectively. Each of these addresses is incremented by one macroblock from the address of the last macroblock of the preceding slice. By defining the position of the first macroblock in the slice for slices and in this way the processor may vertically concatenate each macroblock array of each slice to the preceding array.

The processor may also use such techniques in combining a respective frame of each received data stream into an output frame having a grid layout. Consider for example that the processor receives a first data stream a second data stream a third data stream and a fourth data stream. Each received data stream represents frames of a respective video. Frames of each video are represented in the data stream as one four by four slice that is divided into an array of sixteen macroblocks. By way of example slices and of may represent such slices. To arrange slices and into the rectangular grid arrangement of the processor may define the addresses of the first macroblock of slices and as address 0 address 4 address 32 and address 36. In this manner the array of macroblocks in slice is horizontally concatenated to the array of macroblocks in slice . And the array of macroblocks in slice is vertically concatenated to the array of macroblocks in slice and the array of macroblocks in slice is horizontally concatenated to the array of macroblocks in slice .

As noted above some decoders may have maximum frame height and or maximum frame width limitations. In some cases the processor may use a grid layout to avoid these limitations. For instance the processor may arrange two frames into a vertically stacked configuration if the sum of the respective frame heights of each of the two frames is less than or equal to a maximum frame height of a particular decoder. And if the sum of the respective frame heights of each of the two frames is greater than a maximum frame height of a particular decoder the processor may arrange the two frames horizontally. Such a configuration may indicate a preference for a vertically stacked configuration of frames. Some embodiments may prefer a different configuration of frames.

Some codecs require sequencing of the component parts of a frame e.g. slices and macroblocks in decoding order e.g. raster scan order . However some combinations of frames of received data streams into output frames may result in the array of macroblocks of each slice reaching the decoder out of decoding order. The processor and the decoder may then decode the slices out of order. For instance H.264 includes a feature called arbitrary slice ordering that allows the decoder to receive slices of an output frame in any order. For each slice the processor may decode one or more parameters indicating the slice length of the slice i.e. the number of elements in the macroblock array and the macroblock address of the first macroblock in the slice. The processor may then use the one or more decoded parameters to send each slice individually to the decoder. The decoder may then decode the frame after it has received all of the slices of the frame or the decoder may decode the slices in the order in which they come to the decoder.

Some decoders may require that the encoded frames of the output data stream have a consistent frame width. Within examples the processor may decode from a header of the first data stream a first frame width that indicates the width of the encoded frames of the first data stream. The processor may also decode from a header of the second data stream a second frame width that indicates the width of the encoded frames of the second data stream. The processor may then determine that the first frame width is equivalent to the second frame width. Then at some point such as during generation of the output data stream the processor may define in a header of the output data stream that the encoded output frames have a width that is equal to the encoded frames.

However in some cases the encoded frames of the first data stream and the encoded frames of the second data stream may have different frame widths. In some examples the method may then further involve padding the frames of one of the received data streams to make the frames widths equal. For instance referring to slices and may represent a frame of a first data stream and a frame of a second data stream respectively. Slice and have frame widths of 4 macroblocks and 3 macroblocks respectively. The processor may determine that the frame width of slice is less than the frame width of slice . The processor may then pad the slice with additional macroblock data such that the width of slice is equal to the width of slice . The processor may then combine slice and slice into output frame . After output frame is decoded the processor may provide for display the frame of the second data stream without the additional macroblock data.

In some cases the first video represented by the first data stream and the second video represented by the second data stream may have different frame rates. For example referring to data stream may represent a first video having a first frame rate that is three times the second frame rate of a second video that is represented by data stream . Within examples the processor may insert frames into the second data stream so that the frame rates of the data streams are equivalent. For instance the processor may insert two frames into data stream after every frame of the second data stream. Then data stream and data stream are combined into data stream with the same frame rate. So that the content of the second video is not altered the content of each inserted frame may represent content of a frame that neighbors the inserted frame with the second data stream. For instance a type of frame known as a P frame which consists of P SKIP macroblocks may be inserted into the frames such that the content of the inserted frame represents the content of the preceding frame. P frames use data from previous frames to decode the content in the P frame.

Further some codecs may require the key frame intervals of received data streams to be equivalent in order to decode the output data stream when the received data streams are combined. Key frames also known as I frames do not require other video frames to decode. The processor may insert frames to make the key frame intervals of the received data streams equivalent. For instance when the second video has a lower frame rate than the first video the processor may insert after each key frame in the second data stream a particular number of frames. In some examples the inserted frames may be P frames which consist of P SKIP macroblocks. As noted above the particular number of frames may be proportional to the difference in the frames rates of the received data streams.

Other codecs may permit different key frame intervals. With such codecs the processor may combine different types of frame types for decoding by a decoder compatible with the codec. For instance the processor may combine a I frame from the first data stream with a P frame from the second data stream.

At block the method involves decoding the output data stream into an output surface format that includes in respective non overlapping regions decoded frames of the first video and decoded frames of the second video. For instance processor of may decode the output data stream using a decoder that is a component of processor . Alternatively processor may provide output data stream to a different processor such as GPU which may decode the output data stream using decoder . As another example processor may send the output data stream to a decoder on another computing device such as a mobile computing device via interface .

The output surface format may for example represent each frame of the output data stream as an array of pixel data. Each array of pixel data may represent a frame of the first video and a frame of the second video in a different region. The processor may then sample from the different regions to divide the output surface format into the first video and the second video.

At block the method involves providing the decoded frames of the first video and the decoded frames of the second video display. For instance processor may provide the decoded frames to interface . As noted above interface may include a screen such as a touchscreen which may display the decoded frames of each video when they are provided to interface by the processor.

In some examples the processor may provide the first video and the second video for display at different times. For instance at a first presentation time the processor may sample the first video from the output surface format and provide the first video for display. At a second presentation time the processor may sample the second video from the output surface format and provide the second video for display.

In other examples processor may provide the first video and the second video for concurrent display. Concurrent display of the first video and the second video may involve positioning the decoded frames of each video in respective arbitrarily located positions on a display. Concurrent display of the first video and the second video may have various applications. As one example the first video represented in the first data stream and the second video represented in the second data stream may represent a video call from a first device and a video call from a second device respectively. By combining the video calls from the two devices and decoding the video calls as a combined output data stream both video calls may be hardware decoded simultaneously which may enable assorted features of a video chat application. For example the video chat application may support a greater number of concurrent users in a single group chat or display video calls during a group chat at higher frame rates.

In some examples the first video and the second video may represent video previews of respective videos. For instance a news feed application may provide for display of a video preview next to each news item. By application of the present method the processor may combine the video previews into an output data stream and then decode them together by decoding the output data stream. This may enable various features of the news feed application. For instance two or more of the video previews may play concurrently. Alternatively a video gallery application may indicate each video in the gallery using a video preview. Other applications are possible as well.

Within examples the processor may divide received data streams into sets. The processor may then combine the data streams in the set into an output data stream. The processor may select particular data streams for each group based on characteristics of the data stream. For instance as noted above some decoders may require that the key frame intervals of the combined data streams are equivalent. From received data streams the processor may parse one or more parameters that indicate the key frame interval of the data stream. Then the processor may select as a set particular data streams having the same key frame interval. Similarly the processor may select particular data streams having frames of the same frame width as a set. Selection based on other parameters such as frame rate is possible as well.

While various aspects and embodiments have been disclosed herein other aspects and embodiments will be apparent to those skilled in the art. The various aspects and embodiments disclosed herein are for purposes of illustration and are not intended to be limiting with the true scope being indicated by the following claims along with the full scope of equivalents to which such claims are entitled. It is also to be understood that the terminology used herein is for the purpose of describing particular embodiments only and is not intended to be limiting.

