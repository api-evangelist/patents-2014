---

title: Composing and executing workflows made up of functional pluggable building blocks
abstract: A platform that provides a way to automatically compose and execute even complex workflows without writing code is described. A set of pre-built functional building blocks can be provided. The building blocks perform data transformation and machine learning functions. The functional blocks have well known plug types. The building blocks can be composed build complex compositions. Input and output files are converted to a standard data type so that modules are pluggable.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09436507&OS=09436507&RS=09436507
owner: Microsoft Technology Licensing, LLC
number: 09436507
owner_city: Redmond
owner_country: US
publication_date: 20141219
---
This application claims the benefit of U.S. Provisional Patent Application No. 62 023 834 entitled COMPOSING AND EXECUTING WORKFLOWS MADE UP OF FUNCTIONAL PLUGGABLE BUILDING BLOCKS filed Jul. 12 2014 which is hereby incorporated by reference in its entirety.

Instead of just following explicitly programmed instructions some computing systems can learn by processing data. The process whereby a computing system learns is called machine learning. Machine learning can be advantageously employed wherever designing and programming explicit rule based algorithms for data computation is insufficient. Machine learning often is based on a statistical mathematical model. A mathematical model describes a system using mathematical concepts and language. A mathematical model is often used to make predictions about future behavior based on historical data.

Complex functionality that can be delivered as a hosted cloud application can be provided without writing code. Building blocks that provide common functionality can be composed into a workflow a sequence of operations using a drag and connect paradigm. An execution environment can execute the workflow. The workflow can be as complex or as simple as desired and can include numerous stages of computation and various data flows. Computations can include ingesting and transforming data creating optimizers and or applying machine learning ML algorithms. Composed workflows can be automatically operationalized and published as a web service. The workflow can be published as a web end point.

A platform that standardizes the way functional building blocks are composed together in a plug and play manner is described. The platform can compose and execute complex workflows automatically without the need for a user to write program code or provide instructions. A set of pre built pluggable functional building blocks can provide data transformation and or machine learning functions. The functional blocks can use well known plug types. An authoring tool can enable the authoring of workflows using the functional building blocks. A testing tool can enable the testing of the workflows. The composed workflow can be published as an operational web service as a REST representational state transfer end point on a cloud computing platform such as but not limited to Microsoft s Azure ML. A REST API application programming interface is a public URL that can be entered into a browser s address line. It contains no state so that any data needed to produce output is in the URL itself. The interface can be abstracted so that the user does not need to know any specific technology in order to communicate with it. This can enable a user such as but not limited to a developer or data scientist to author their workflows using a simple drag and connect paradigm. The workflows can be tested and provided as production web services without writing any code. Plugability between functional building blocks can be provided by using standardized interfaces for communication between the different functional building blocks. The execution environment for the functional building blocks can automatically convert between different interfaces schemas and data formats.

This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used to limit the scope of the claimed subject matter.

A workflow composition platform e.g. a Predictive Analytics Platform a machine learning platform a non ML platform etc. as described herein can be an extensible cloud based multi tenant service for authoring and executing in testing or production data science workflows. Workflows of any degree of complexity can be created. A composed workflow called an experiment can be executed one or more times using one or more datasets. The workflow can be published as REST representational state transfer end point on a cloud computing platform such as but not limited to Microsoft s Azure ML . A user such as but not limited to a developer or data scientist can author a workflow using a simple drag and connect paradigm. The workflow can be tested and provided as production web services without writing any code.

In accordance with aspects of the subject matter described herein functional building blocks from which workflows are composed can be standard functional components that adhere to particular design patterns data interface standards input output data types and parameter specifications. A functional building block can include a pluggable module and the data e.g. one or more datasets on which the pluggable module operates. Each functional building block can be associated with an interface. The interface describes what type of data the functional building block produces and what type of data the functional building block accepts. Schemas can be standardized to enable plugability of functional blocks. The term plugability as used herein means that the functional building blocks are automatically interoperable without requiring the user to do any conversion coding for schemas data program code or interfaces. When building blocks are pluggable legal connections can be allowed and illegal connections can be disallowed because compatibility between blocks is discernable. Schemas can be standardized so that pluggable modules can interpret and or modify schemas. A standardized data table object can incorporate schemas e.g. ML schemas . The data connection plug in between modules can be the data table object. An execution environment can provide plugability for a spectrum of data types and schemas into the standard data table interface. To run an experiment an application can send the experiment graph e.g. composed workflow to a job execution service JES . The experiment graph can include the dependency structure which can control the order of execution along with pointers to the code and data needed at each graph node. The JES can then schedule execution of each graph node at a resource. Information can be communicated back to the application to de displayed. An authoring tool can provide a user the means to author a workflow using a drag and connect paradigm. A directed acyclic graph DAG manager can parse a workflow the elements of which can be executed by a module execution environment module runtime .

Tool and Platform to Compose and Execute Arbitrarily Complex Workflows Made Up of Functional Pluggable Building Blocks

System can include one or more computing devices such as for example computing device . Contemplated computing devices include but are not limited to desktop computers tablet computers laptop computers notebook computers personal digital assistants smart phones cellular telephones mobile telephones and so on. A computing device such as computing device can include one or more processors such as processor etc. and a memory such as memory that communicates with the one or more processors.

System can include an environment that supports authoring a workflow using a simple drag and connect paradigm. Wrappers around native library elements can be provided for array manipulation slicing and operators. In memory tabular data can be provided in a column oriented data structure. Serialization e.g. from objects to files can be provided. Streaming data support for module connection can be provided. Support for categorical ordinal data types and multi categorical data types can be provided. A categorical data type refers to a data type in which legal values are those values which belong to a specified list of values. Standardized access to machine learning metadata in a dataset can be provided.

Pluggable modules can be provided with thin wrappers around Analytics Framework APIs application programming interfaces with annotations to expose them to the authoring and execution service. APIs can be well layered composable consistent and developer friendly by making use of standard interfaces and basic data types such as N dimensional arrays sparse matrices and data tables by the consistent use of a rich dataset schema that provides access to machine learning specific attributes such as feature channels features labels scores as well as statistics about the data in a uniform manner across all API functions and by conformance to design guidelines e.g. to the .NET Framework Design Guidelines .

System may include one or more computing devices on which any one or any combination of the following reside one or more program modules comprising a workflow composer such as workflow composer one or more program modules comprising a user interface such as user interface a pluggable module library such as library of pluggable modules including one or more functional building blocks such as pluggable module pluggable module . . . pluggable module interfaces one or more descriptions of modules not shown one or more descriptions of datasets one or more descriptions of schemas associated with the datasets one or more descriptions of interfaces one or more program modules comprising a bridge such as bridge that can translate data formats metadata schema and interfaces from a first type of data e.g. a file to a second type of data e.g. an object . Functional building blocks such as pluggable module and its associated dataset can be a standard functional component that meets specified design patterns data interface standards input output data types and parameter specifications. Functional building blocks can be composed into a workflow.

In the machine learning platform represented in system a user can employ a drag and connect paradigm in a user interface e.g. user interface to identify and or select pluggable modules and datasets schemas to be composed into a workflow and to describe how the pluggable modules are connected. An interface associated with the functional building block identifies what the functional building block produces and what the functional building block accepts. Pluggable modules and datasets schemas can be identified by selecting a description of the modules and or datasets schemas. Inputs to the pluggable module can be specified. Outputs from the pluggable module can be specified. Input can be data over which the pluggable module will operate. Data can include one or more datasets and or schemas such as datasets schemas . System can include machine learning schema. Machine learning schema can be standardized. Standardization of machine learning schema can enable functional components to be pluggable. Data can be objects such as DataTable objects. Such an object which is a representation of two dimensional tabular data and its corresponding schema can be a type of representation used by the machine learning platform. Input can be parameters for the module. Output can be results of the computations on the data. System can include one or more program modules that comprise a module loading program. System can include one or more program modules that comprise a dataset schema loading program. Loaded modules can be located in a pluggable module library such as library of pluggable modules . The pluggable module library and datasets may be stored on one or more backend computing devices. Selected pluggable modules and datasets schemas e.g. functional pluggable building blocks such as functional pluggable building block functional pluggable building block etc. can be composed to create a workflow such as workflow . Thus workflow can include a number of tasks where each task is performed by a executing a functional pluggable building block.

When the workflow is executed a DAG directed acyclic graph execution manager such as DAG execution manager can receive a composed workflow from an application and can send the composed workflow e.g. an experiment graph to a job execution service. The DAG execution manager can schedule the tasks that make up the composed workflow to execute on one or more computing resources such as computing resource computing resource . . . to computing resource . Computing resources can be computing devices virtual machines and so on. During workflow execution the workflow interface associated with the executing functional building block pluggable module plus dataset s enables the automatic invocation of the bridge to transform the output of one pluggable module into a form that is acceptable as input to the next pluggable module. The pluggable module itself can be written to one of a small number of standardized pluggable module interfaces. All of the transformations needed to allow communications between pluggable modules is automatically performed outside pluggable module execution by the bridge software. A pluggable module such as pluggable module etc. executing on a computing resource can execute in a pluggable module execution environment or module runtime such as module runtime . The module execution runtime e.g. module runtime can use the appropriate workflow interface to abstract away details such as input and output file location and format by converting input files into DataTable object types such as objects parsing the rest of the arguments calling the pluggable module then serializing output objects into files such as files . Input to the pluggable module execution environment can be in the form of files . A bridge such as bridge can convert the files into objects e.g. DataTable objects and can send the objects to pluggable module . Thus the modules can be written with signatures such as 

Tuple Module DataTable input1 DataTable input2 int parameter1 double parameter2 and so on. The pluggable module can communicate over a bridge such as bridge to another execution environment such as execution environment . Execution environment can be an execution environment operating on another computing resource such as computing resource etc. The DAG execution manager can be notified of the results. The DAG execution manager can wait until all the scheduled tasks are done before sending back results for the entire workflow to the application not shown .

Thus a pluggable module can receive input such as data and a schema for the data. The schema can be standardized so that functional components of system can interpret and or modify the schema. The data and its corresponding schema can be incorporated into a standardized object. The object can provide the data connection plug in between modules. The objects can provide plugability of a spectrum of data types and schema into the standard object interface. A DAG directed acyclic graph execution manager such as DAG execution manager can parse any workflow. DAG execution manager can use a pluggable module runtime such as pluggable module runtime to execute a task.

The pluggable module execution runtime e.g. pluggable module runtime can abstract away details such as input and output file location and format by converting input files into DataTable types parsing the rest of the arguments calling the module then serializing output objects into files.

The trained model input data and any data transformations that are to be performed can be entered into the corresponding flow containers. For example a trained model such as model or model can be selected for use in the scoring experiment by for example clicking and dragging model or model from the experiment list into the model flow container model fc . Test data such as test data can be selected for use in the scoring experiment by for example clicking and dragging test data into test data flow container data fc . Data provided to the experiment can be labeled or unlabeled data. Labeled data is data for which the outcome is known or for which an outcome has been assigned. Unlabeled data is data for which the outcome is unknown or for which no outcome has been assigned. Data provided to the experiment can be test or production data. Data transformation instructions such as for example ignore column 1 can be indicated by for example clicking and dragging saved transformations from the experiment list or entering the desired data transformations in data transformations flow container data trans fc .

The inputs and outputs to the score model module indicated in the score model flow container score model can be indicated by drawing flow connectors such as flow connectors and . For example flow connector indicates that the data indicated in data flow container data fc e.g. test data is input to the data transformation indicated in data flow container data trans fc e.g. ignore column 1 and the transformed data and the model indicated in model flow container model fc e.g. model are input to the score model module score model . The output from the score model module score model can also be designated. The status of the experiment e.g. Draft or Finished can be displayed as the Status Code e.g. status code .

Selecting the RUN option option can trigger the running of the experiment invoking an experiment execution module. After the experiment has been run the experiment can be saved and an option to publish the experiment as a service can be displayed as illustrated by the option publish web service .

Described herein is a system including at least one processor a memory connected to the at least one processor and at least one program module loaded into the memory the at least one program module providing automatic interoperability between a plurality of pluggable functional building blocks comprising at least a first functional building block and a second functional building block of a workflow by executing a second functional building block of the plurality of pluggable functional building blocks in a runtime container wherein the runtime container automatically transforms output produced by the first pluggable functional building block into input acceptable by the second pluggable functional building block. The system can include at least one program module loaded into the memory wherein the at least one program module comprises a bridge that uses a standardized workflow interface for communication between the second pluggable functional building block and the first pluggable functional building block. The system can include at least one program module loaded into the memory wherein the at least one program module comprises a bridge that automatically converts between a workflow interface associated with the first pluggable functional building block to a workflow interface associated with the second pluggable functional building block. The system can include at least one program module loaded into the memory wherein the at least one program module comprises a bridge that automatically converts between a schema associated with the first pluggable functional building block to a schema associated with the second pluggable functional building block. The system can include at least one program module loaded into the memory wherein the at least one program module comprises a bridge that automatically converts between a data format associated with the first pluggable functional building block to a data format associated with the second pluggable functional building block. The system can include at least one program module loaded into the memory wherein the at least one program module comprises a bridge that converts a file produced by the first pluggable functional building block into an object acceptable by a pluggable module of the second functional building block. The system can include at least one program module loaded into the memory wherein the at least one program module comprises a bridge that serializes an object produced by the first functional building block into a file acceptable by a pluggable module of the second pluggable functional building block.

Described is a method in which a processor of a computing device provides automatic interoperability between a plurality of pluggable functional building blocks comprising at least a second pluggable functional building block and a first pluggable functional building block of a workflow by executing a second pluggable functional building block of the plurality of pluggable functional building blocks in a runtime container wherein the runtime container automatically transforms output produced by the first pluggable functional building block into input acceptable by the second pluggable functional building block. Automatic means that the module runtime container performs the transformation conversion programmatically without requiring human interaction. A standardized workflow interface for communication between the second pluggable functional building block and the first pluggable functional building block can be used. Converting between a workflow interface associated with the first pluggable functional building block to a workflow interface associated with the second pluggable functional building block can be automatic. Converting between a data format associated with the first pluggable functional building block to a data format associated with the second pluggable functional building block can be automatic. Converting a file produced by the first pluggable functional building block into an object acceptable by a pluggable module of the second pluggable functional building block can be automatic. An object produced by the second pluggable functional building block can be serialized into a file acceptable by a pluggable module of the first pluggable functional building block.

A computer readable storage medium comprising computer readable instructions which when executed cause at least one processor of a computing device to provide a plurality of pluggable functional building blocks wherein a second pluggable functional building block of the plurality of pluggable functional building blocks is a building block that is automatically interoperable with a first pluggable functional building block and in response to receiving a selection of at least two of the plurality of pluggable functional building blocks and a connection indication between the at least two of the plurality of functional building blocks automatically compose a workflow comprising the first pluggable functional building block and the second pluggable functional building block wherein the output of the first pluggable functional building block is automatically transformed into acceptable input for the second pluggable functional building block when the workflow is executed is described. The computer readable storage medium can include further computer readable instructions which when executed cause the at least one processor to use a standardized workflow interface for communication between the first pluggable functional building block and the second pluggable functional building block. The computer readable storage medium can include further computer readable instructions which when executed cause the at least one processor to automatically convert between a workflow interface associated with the first pluggable functional building block to a workflow interface associated with the second pluggable functional building block. The computer readable storage medium can include further computer readable instructions which when executed cause the at least one processor to convert between a schema associated with the first pluggable functional building block to a schema associated with the second pluggable functional building block. The computer readable storage medium can include further computer readable instructions which when executed cause the at least one processor to automatically convert between a data format associated with the first pluggable functional building block to a data format associated with the second pluggable functional building block. The computer readable storage medium can include further computer readable instructions which when executed cause the at least one processor to automatically convert a file produced by the first pluggable functional building block into an object acceptable by a pluggable module of the second pluggable functional building block. The computer readable storage medium can include further computer readable instructions which when executed cause the at least one processor to automatically publish the workflow as a web service.

In order to provide context for various aspects of the subject matter disclosed herein and the following discussion are intended to provide a brief general description of a suitable computing environment in which various embodiments of the subject matter disclosed herein may be implemented. While the subject matter disclosed herein is described in the general context of computer executable instructions such as program modules executed by one or more computers or other computing devices those skilled in the art will recognize that portions of the subject matter disclosed herein can also be implemented in combination with other program modules and or a combination of hardware and software. Generally program modules include routines programs objects physical artifacts data structures etc. that perform particular tasks or implement particular data types. Typically the functionality of the program modules may be combined or distributed as desired in various embodiments. The computing environment is only one example of a suitable operating environment and is not intended to limit the scope of use or functionality of the subject matter disclosed herein.

With reference to a computing device in the form of a computer is described. Computer may include at least one processing unit a system memory and a system bus . The at least one processing unit can execute instructions that are stored in a memory such as but not limited to system memory . The processing unit can be any of various available processors. For example the processing unit can be a graphics processing unit GPU . The instructions can be instructions for implementing functionality carried out by one or more components or modules discussed above or instructions for implementing one or more of the methods described above. Dual microprocessors and other multiprocessor architectures also can be employed as the processing unit . The computer may be used in a system that supports rendering graphics on a display screen. In another example at least a portion of the computing device can be used in a system that comprises a graphical processing unit. The system memory may include volatile memory and nonvolatile memory . Nonvolatile memory can include read only memory ROM programmable ROM PROM electrically programmable ROM EPROM or flash memory. Volatile memory may include random access memory RAM which may act as external cache memory. The system bus couples system physical artifacts including the system memory to the processing unit . The system bus can be any of several types including a memory bus memory controller peripheral bus external bus or local bus and may use any variety of available bus architectures. Computer may include a data store accessible by the processing unit by way of the system bus . The data store may include executable instructions 3D models materials textures and so on for graphics rendering.

Computer typically includes a variety of computer readable media such as volatile and nonvolatile media removable and non removable media. Computer readable media may be implemented in any method or technology for storage of information such as computer readable instructions data structures program modules or other data. Computer readable media include computer readable storage media also referred to as computer storage media and communications media. Computer storage media includes physical tangible media such as but not limited to RAM ROM EEPROM flash memory or other memory technology CDROM digital versatile disks DVD or other optical disk storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices that can store the desired data and which can be accessed by computer . Communications media include media such as but not limited to communications signals modulated carrier waves or any other intangible media which can be used to communicate the desired information and which can be accessed by computer .

It will be appreciated that describes software that can act as an intermediary between users and computer resources. This software may include an operating system which can be stored on disk storage and which can allocate resources of the computer . Disk storage may be a hard disk drive connected to the system bus through a non removable memory interface such as interface . System applications take advantage of the management of resources by operating system through program modules and program data stored either in system memory or on disk storage . It will be appreciated that computers can be implemented with various operating systems or combinations of operating systems.

A user can enter commands or information into the computer through an input device s . Input devices include but are not limited to a pointing device such as a mouse trackball stylus touch pad keyboard microphone voice recognition and gesture recognition systems and the like. These and other input devices connect to the processing unit through the system bus via interface port s . An interface port s may represent a serial port parallel port universal serial bus USB and the like. Output devices s may use the same type of ports as do the input devices. Output adapter is provided to illustrate that there are some output devices like monitors speakers and printers that require particular adapters. Output adapters include but are not limited to video and sound cards that provide a connection between the output device and the system bus . Other devices and or systems or devices such as remote computer s may provide both input and output capabilities.

Computer can operate in a networked environment using logical connections to one or more remote computers such as a remote computer s . The remote computer can be a personal computer a server a router a network PC a peer device or other common network node and typically includes many or all of the elements described above relative to the computer although only a memory storage device has been illustrated in . Remote computer s can be logically connected via communication connection s . Network interface encompasses communication networks such as local area networks LANs and wide area networks WANs but may also include other networks. Communication connection s refers to the hardware software employed to connect the network interface to the bus . Communication connection s may be internal to or external to computer and include internal and external technologies such as modems telephone cable DSL and wireless and ISDN adapters Ethernet cards and so on.

It will be appreciated that the network connections shown are examples only and other means of establishing a communications link between the computers may be used. One of ordinary skill in the art can appreciate that a computer or other client device can be deployed as part of a computer network. In this regard the subject matter disclosed herein may pertain to any computer system having any number of memory or storage units and any number of applications and processes occurring across any number of storage units or volumes. Aspects of the subject matter disclosed herein may apply to an environment with server computers and client computers deployed in a network environment having remote or local storage. Aspects of the subject matter disclosed herein may also apply to a standalone computing device having programming language functionality interpretation and execution capabilities.

The various techniques described herein may be implemented in connection with hardware or software or where appropriate with a combination of both. Thus the methods and apparatus described herein or certain aspects or portions thereof may take the form of program code i.e. instructions embodied in tangible media such as floppy diskettes CD ROMs hard drives or any other machine readable storage medium wherein when the program code is loaded into and executed by a machine such as a computer the machine becomes an apparatus for practicing aspects of the subject matter disclosed herein. As used herein the term machine readable storage medium shall be taken to exclude any mechanism that provides i.e. stores and or transmits any form of propagated signals. In the case of program code execution on programmable computers the computing device will generally include a processor a storage medium readable by the processor including volatile and non volatile memory and or storage elements at least one input device and at least one output device. One or more programs that may utilize the creation and or implementation of domain specific programming models aspects e.g. through the use of a data processing API or the like may be implemented in a high level procedural or object oriented programming language to communicate with a computer system. However the program s can be implemented in assembly or machine language if desired. In any case the language may be a compiled or interpreted language and combined with hardware implementations.

Although the subject matter has been described in language specific to structural features and or methodological acts it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather the specific features and acts described above are disclosed as example forms of implementing the claims.

