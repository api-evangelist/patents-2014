---

title: Method and system for building text-to-speech voice from diverse recordings
abstract: A method and system is disclosed for building a speech database for a text-to-speech (TTS) synthesis system from multiple speakers recorded under diverse conditions. For a plurality of utterances of a reference speaker, a set of reference-speaker vectors may be extracted, and for each of a plurality of utterances of a colloquial speaker, a respective set of colloquial-speaker vectors may be extracted. A matching procedure, carried out under a transform that compensates for speaker differences, may be used to match each colloquial-speaker vector to a reference-speaker vector. The colloquial-speaker vector may be replaced with the matched reference-speaker vector. The matching-and-replacing can be carried out separately for each set of colloquial-speaker vectors. A conditioned set of speaker vectors can then be constructed by aggregating all the replaced speaker vectors. The condition set of speaker vectors can be used to train the TTS system.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09542927&OS=09542927&RS=09542927
owner: Google Inc.
number: 09542927
owner_city: Mountain View
owner_country: US
publication_date: 20141113
---
Unless otherwise indicated herein the materials described in this section are not prior art to the claims in this application and are not admitted to be prior art by inclusion in this section.

A goal of automatic speech recognition ASR technology is to map a particular utterance or speech sample to an accurate textual representation or other symbolic representation of that utterance. For instance ASR performed on the utterance my dog has fleas would ideally be mapped to the text string my dog has fleas rather than the nonsensical text string my dog has freeze or the reasonably sensible but inaccurate text string my bog has trees. 

A goal of speech synthesis technology is to convert written language into speech that can be output in an audio format for example directly or stored as an audio file suitable for audio output. The written language could take the form of text or symbolic linguistic representations. The speech may be generated as a waveform by a speech synthesizer which produces artificial human speech. Natural sounding human speech may also be a goal of a speech synthesis system.

Various technologies including computers network servers telephones and personal digital assistants PDAs can be employed to implement an ASR system and or a speech synthesis system or one or more components of such systems. Communication networks may in turn provide communication paths and links between some or all of such devices supporting speech synthesis system capabilities and services that may utilize ASR and or speech synthesis system capabilities.

In one aspect an example embodiment presented herein provides a method comprising extracting speech features from a plurality of recorded reference speech utterances of a reference speaker to generate a reference set of reference speaker vectors for each respective plurality of recorded colloquial speech utterances of a respective colloquial speaker of multiple colloquial speakers extracting speech features from the recorded colloquial speech utterances of the respective colloquial speaker to generate a respective set of colloquial speaker vectors for each respective set of colloquial speaker vectors replacing each colloquial speaker vector of the respective set of colloquial speaker vectors with a respective optimally matched reference speaker vector from among the reference set of reference speaker vectors the respective optimally matched reference speaker vector being identified by matching under a transform that compensates for differences in speech between the reference speaker and the respective colloquial speaker aggregating the replaced colloquial speaker vectors of all the respective sets of colloquial speaker vectors into an aggregate set of conditioned speaker vectors providing the aggregate set of conditioned speaker vectors to a text to speech TTS system implemented on one or more computing devices and training the TTS system using the provided aggregate set of conditioned speaker vectors.

In another respect an example embodiment presented herein provides a system comprising one or more processors memory and machine readable instructions stored in the memory that upon execution by the one or more processors cause the system to carry out operations including extracting speech features from a plurality of recorded reference speech utterances of a reference speaker to generate a reference set of reference speaker vectors for each respective plurality of recorded colloquial speech utterances of a respective colloquial speaker of multiple colloquial speakers extracting speech features from the recorded colloquial speech utterances of the respective colloquial speaker to generate a respective set of colloquial speaker vectors for each respective set of colloquial speaker vectors replacing each colloquial speaker vector of the respective set of colloquial speaker vectors with a respective optimally matched reference speaker vector from among the reference set of reference speaker vectors wherein the respective optimally matched reference speaker vector is identified by matching under a transform that compensates for differences in speech between the reference speaker and the respective colloquial speaker aggregating the replaced colloquial speaker vectors of all the respective sets of colloquial speaker vectors into an aggregate set of conditioned speaker vectors providing the aggregate set of conditioned speaker vectors to a text to speech TTS system and training the TTS system using the provided aggregate set of conditioned speaker vectors.

In yet another aspect an example embodiment presented herein provides an article of manufacture including a computer readable storage medium having stored thereon program instructions that upon execution by one or more processors of a system cause the system to perform operations comprising extracting speech features from a plurality of recorded reference speech utterances of a reference speaker to generate a reference set of reference speaker vectors for each respective plurality of recorded colloquial speech utterances of a respective colloquial speaker of multiple colloquial speakers extracting speech features from the recorded colloquial speech utterances of the respective colloquial speaker to generate a respective set of colloquial speaker vectors for each respective set of colloquial speaker vectors replacing each colloquial speaker vector of the respective set of colloquial speaker vectors with a respective optimally matched reference speaker vector from among the reference set of reference speaker vectors wherein the respective optimally matched reference speaker vector is identified by matching under a transform that compensates for differences in speech between the reference speaker and the respective colloquial speaker aggregating the replaced colloquial speaker vectors of all the respective sets of colloquial speaker vectors into an aggregate set of conditioned speaker vectors providing the aggregate set of conditioned speaker vectors to a text to speech TTS system implemented on one or more computing devices and training the TTS system using the provided aggregate set of conditioned speaker vectors.

These as well as other aspects advantages and alternatives will become apparent to those of ordinary skill in the art by reading the following detailed description with reference where appropriate to the accompanying drawings. Further it should be understood that this summary and other descriptions and figures provided herein are intended to illustrative embodiments by way of example only and as such that numerous variations are possible. For instance structural elements and process steps can be rearranged combined distributed eliminated or otherwise changed while remaining within the scope of the embodiments as claimed.

A speech synthesis system can be a processor based system configured to convert written language into artificially produced speech or spoken language. The written language could be written text such as one or more written sentences or text strings for example. The written language could also take the form of other symbolic representations such as a speech synthesis mark up language which may include information indicative of speaker emotion speaker gender speaker identification as well as speaking styles. The source of the written text could be input from a keyboard or keypad of a computing device such as a portable computing device e.g. a PDA smartphone etc. or could be from file stored on one or another form of computer readable storage medium. The artificially produced speech could be generated as a waveform from a signal generation device or module e.g. a speech synthesizer device and output by an audio playout device and or formatted and recorded as an audio file on a tangible recording medium. Such a system may also be referred to as a text to speech TTS system although the written form may not necessarily be limited to only text.

A speech synthesis system may operate by receiving an input text string or other form of written language and translating the written text into an enriched transcription corresponding to a symbolic representation of how the spoken rendering of the text sounds or should sound. The enriched transcription may then be mapped to speech features that parameterize an acoustic rendering of the enriched transcription and which then serve as input data to a signal generation module device or element that can produce an audio waveform suitable for playout by an audio output device. The playout may sound like a human voice speaking the words or sounds of the input text string for example. In the context of speech synthesis the more natural the sound e.g. to the human ear of the synthesized voice generally the better the voice quality ranking of the system. The audio waveform could also be generated as an audio file that may be stored or recorded on storage media suitable for subsequent playout.

In operation a TTS system may be used to convey information from an apparatus e.g. a processor based device or system to a user such as messages prompts answers to questions instructions news emails and speech to speech translations among other information. Speech signals may themselves carry various forms or types of information including linguistic content affectual state e.g. emotion and or mood physical state e.g. physical voice characteristics and speaker identity to name a few.

Speech synthesis based on associating parametric representations of speech with symbolic descriptions of phonetic and linguistic content of text such as enriched transcriptions is customarily referred to as statistical parametric speech synthesis or SPSS . A SPSS system may be trained using data consisting mainly of numerous speech samples and corresponding text strings or other symbolic renderings . For practical reasons the speech samples are usually recorded although they need not be in principle. By construction the corresponding text strings are in or generally accommodate a written storage format. Recorded speech samples and their corresponding text strings can thus constitute training data for a SPSS system.

One example of a SPSS system is TTS based on hidden Markov models HMMs . In this approach HMMs are used to model statistical probabilities associating enriched transcriptions of input text strings with parametric representations of the corresponding speech to be synthesized. One advantageous aspect of HMM based speech synthesis is that it can facilitate altering or adjusting characteristics of the synthesized voice using one or another form of statistical adaptation. For example given data in the form of recordings of a reference speaker the HMM can be adapted to the data so as to make the HMM based synthesizer sound like the reference speaker. The ability to adapt HMM based synthesis can therefore make it a flexible approach.

In another example of a SPSS system a TTS system may use a form of machine learning to generate a parametric representation of speech to synthesize speech. For example a neural network NN may be used to generate speech parameters by training the NN to associated known enriched transcriptions with known parametric representations of speech sounds. As with HMM based speech synthesis NN based speech synthesis can facilitate altering or adjusting characteristics of the synthesized voice using one or another form of statistical adaptation.

In a typical conventional approach SPSS uses homogeneous data from a single speaker with a consistent speaking style recorded under controlled conditions. For example consistency of recorded speech samples can help ensure that a SPSS system learns or is trained to associate a consistent parametric representation of speech sounds with their corresponding enriched transcriptions. Controlled recording conditions can similarly help mitigate potential polluting effects of noise or other extraneous background that can distort parametric representations during training and diminish the quality of the training. In a similar vein the larger the database of recorded samples particularly those recorded under controlled conditions the better the training of a SPSS system and thus the better the accuracy of the TTS performance and the quality of the synthesized speech.

Obtaining large high quality speech databases for training of SPSS systems can be expensive in terms of cost and time and may not scale up well. On the other hand obtaining audio recordings from multiple speakers in diverse recording environments can be considerably cheaper in terms of time and effort and may be a more scalable approach. For example large collections of such diversely recorded speech and associated text are often employed in automatic speech recognition ASR systems where diversity and variability of speakers and recording environments can be a benefit to the training process. However conventional techniques and approaches for merging diverse speech databases for SPSS purposes of training generally require computationally expensive and or complex algorithms that clean and normalize the quality of the audio as well as non trivial speaker normalization algorithms.

In view of these challenges it would be desirable to be able to build high quality SPSS systems using recordings from multiple speakers in different recording environments in a way that overcomes the significant drawbacks of conventional approaches. At a practical level the general availability such diverse speech databases either ones employed by ASR systems or from multiple Internet sources for example warrants devising a technically superior and cost effective technique for merging diverse speech databases for SPSS training.

But there may be additional reasons beyond the availability considerations. In particular there can be a relative paucity of large uniform and high quality speech databases for certain languages spoken by numerous smaller populations that together can account for a very large total number of languages. Such languages are sometimes referred to as long tail languages because the individual populations that speak them occupy the tail of a number or frequency distribution of speakers of all languages any given language in the tail may represent a relatively small population but the total of all languages in the tail can still represent a large total population. One consequence of a relative lack of high quality speech databases for long tail languages can be a reduced number and or diminished quality of TTS based services for the populations that speak these languages.

Accordingly the ability to build high quality SPSS systems using recordings from multiple speakers in different recording environments in a technically superior and cost effective manner could transform the potential scalability offered by the generally availability diverse speaker recordings into practice. And because obtaining large numbers of diverse recordings of long tail languages can be more practical than obtaining large uniform speech databases of these languages overcoming technical and practical challenges of building diverse recording based SPSS can also help make TTS based services more widely available in long tail languages as well as more generally in other circumstances.

Hence example embodiments are described herein for a method and system for building high quality SPSS using recordings from multiple speakers acquired in different recording environments. More particularly recorded speech samples of multiple speakers of a given language acquired in diverse recording environments can be conditioned using a database of recorded speech samples of a reference speaker of a reference language acquired under controlled conditions. Conditioning techniques applied to the recordings of the multiple speakers can enable the diverse recordings to be conditioned and subsequently aggregated into a conditioned speech database that can be used build and train a high quality SPSS system in the given language.

In accordance with example embodiments recorded samples of speech recited in a consistent voice by a reference speaker reading specified text in a reference language can represent a high quality speech database referred to herein as the reference speech database. For example the reference speech database could contain speech samples and associated text of a single reference speaker obtained under controlled recording conditions. Such a database might be obtained specifically for a SPSS system with non trivial emphasis placed on factors that help insure overall quality such as speaking skills and training of the reference speaker.

Also in accordance with example embodiments each of multiple speakers reciting written text in a given language under possibly ad hoc or less controlled recording conditions can be collected in respective ordinary or ad hoc quality speech databases. In acquiring these speech databases more emphasis may be placed on the number of speakers and the total volume of speech samples in all the speech databases than on the quality and speech training of the individual speakers or on the recording conditions under which the speech samples are obtained. For example these speech databases may be obtained from the Internet or simply man to the street recordings. In order to signify a sort of generalized impact of a relatively diminished emphasis on speaker consistency speech quality and or control of recording conditions either intentional or due to circumstances of data acquisition the term colloquial will be used herein as a qualitative descriptor in referring to the multiple speakers the speech samples acquired from them and the databases containing the speech samples. To maintain consistency of terminology the term colloquial language will also be used to refer to the language of a colloquial speaker.

The reference language and the colloquial language need not be the same although they may be lexically related or be characterized as phonetically similar. In an example embodiment the colloquial language could be a long tail language and the reference language could be phonetically similar but more widely spoken. As such a large speech database of the reference language may be readily available or relatively easy to acquire. Applying the conditioning techniques described herein can therefore enable construction of a high quality SPSS system in the long tail language or more generally in the colloquial language .

In accordance with example embodiments the reference speech samples in the reference speech database can be processed into a sequence of temporal frames of parameterized reference speech sounds. The reference text strings associated with each reference speech sample can be processed into a corresponding enriched transcription including a sequence of reference enriched labels. Each temporal frame of parameterized reference speech sound can thus be associated with some number of reference enriched labels. The association can be many to one one to one or one to many. Customarily each such temporal frame of parameterized speech sound is typically referred to as a speaker feature vector. For purposes of the discussion herein feature vectors derived or extracted from speech of the reference speaker will be referred to reference speaker vectors.

Similarly the colloquial speech samples in the colloquial speech databases can be processed into a sequence of temporal frames of parameterized colloquial speech sounds. The colloquial text strings associated with each colloquial speech sample can be processed into a corresponding enriched transcription including a sequence of colloquial enriched labels. Each temporal frame of parameterized colloquial speech sound can thus be associated with some number of colloquial enriched labels. Again the association can be many to one one to one or one to many. For purposes of the discussion herein feature vectors derived or extracted from speech of the colloquial speaker will be referred to colloquial speaker vectors.

In accordance with example embodiments the colloquial speaker vectors from each colloquial speech database can be conditioned using the reference speaker vectors from the reference speech database by replacing each colloquial speaker vector with an optimally matched reference speaker vector. More particularly an analytical matching procedure can be carried out to identify for each colloquial speaker vector a closest match reference speaker vector from among the set of reference speaker vectors. This process is enabled by a novel and effective matching under transform MUT technique and results in determination of reference speaker vectors that most closely parameterize the sounds represented in the colloquial speaker vectors but do so in a way characterized by the voice consistency and controlled recording conditions of the reference speech database. Each colloquial speaker vector can then be replaced with its optimally matched reference speaker vector while at the same time retaining the enriched colloquial labels associated with each colloquial speaker vector. Replacing the colloquial speaker vectors with the identified optimally match reference speaker vectors thereby yields a set of replaced speaker vectors that represent the speech sounds of the colloquial speakers but with the quality and consistency of the reference speech database.

Also in accordance with example embodiments the matching and replacing steps can be carried out separately for each colloquial speech database. Doing so can help mitigate effects of inconsistencies between different colloquial speech databases even if the consistency and or or quality within each colloquial speech database is relatively diminished in comparison with the reference speech database. All of the replaced speaker vectors and their associated enriched colloquial labels can be aggregated into a conditioned aggregate speech database which is of high quality and suitable for training a SPSS system in the colloquial language.

The MUT technique entails a matching procedure that can compensate for inter speaker speech differences e.g. differences between the reference speaker and the colloquial speakers . The matching procedure can be specified in terms of a MUT algorithm suitable for implementation as executable instructions on one or more processors of a system such as a SPSS or TTS system. Taken with additional steps described below MUT can be used to construct a high quality speech database from a collection of multiple colloquial speech databases.

In example embodiments an example method can be implemented as machine readable instructions that when executed by one or more processors of a system cause the system to carry out the various functions operations and tasks described herein. In addition to the one or more processors the system may also include one or more forms of memory for storing the machine readable instructions of the example method and possibly other data as well as one or more input devices interfaces one or more output devices interfaces among other possible components. Some or all aspects of the example method may be implemented in a TTS synthesis system which can include functionality and capabilities specific to TTS synthesis. However not all aspects of an example method necessarily depend on implementation in a TTS synthesis system.

In example embodiments a TTS synthesis system may include one or more processors one or more forms of memory one or more input devices interfaces one or more output devices interfaces and machine readable instructions that when executed by the one or more processors cause the TTS synthesis system to carry out the various functions and tasks described herein. The TTS synthesis system may also include implementations based on one or more hidden Markov models. In particular the TTS synthesis system may employ methods that incorporate HMM based speech synthesis as well as other possible components. Additionally or alternatively the TTS synthesis system may also include implementations based on one or more neural networks NNs . In particular the TTS synthesis system may employ methods that incorporate NN based speech synthesis as well as other possible components.

At step for each respective plurality of recorded colloquial speech utterances of a respective colloquial speaker of multiple colloquial speakers a respective set of colloquial speaker vectors is generated by extracting speech features from the recorded colloquial speech utterances of the respective colloquial. As with the reference speaker vectors each of the colloquial speaker vectors of each respective set corresponds to a feature vector of a temporal frame of a colloquial speech utterance and each colloquial speech utterance can span multiple temporal frames.

At step for each respective set of colloquial speaker vectors each colloquial speaker vector of the respective set of colloquial speaker vectors is replaced with a respective optimally matched reference speaker vector from among the reference set of reference speaker vectors. In accordance with example embodiments the respective optimally matched reference speaker vector is identified by matching under a transform that compensates for differences in speech between the reference speaker and the respective colloquial speaker.

At step the replaced colloquial speaker vectors of all the respective sets of colloquial speaker vectors are aggregated into an aggregate set of conditioned speaker vectors.

At step the aggregate set of conditioned speaker vectors is provided to a text to speech TTS system implemented on one or more computing devices. For example the TTS system can be configured to receive the aggregate set of conditioned speaker vectors as input. As such providing the aggregate set of conditioned speaker vectors to the TTS system can correspond to providing particular input to the TTS system.

Finally at step the TTS system is trained using the provided aggregate set of conditioned speaker vectors. As described below training a TTS system using speaker vectors can entail training the TTS system to associate a transcribed form of text with parameterized speech such as is represented in feature vectors.

In accordance with example embodiments replacing each colloquial speaker vector of the respective set of colloquial speaker vectors with the respective optimally matched reference speaker vector can entail retaining an enriched transcription associated each given colloquial speaker vector that is replaced in each respective set of colloquial speaker vectors. More particularly as described above each given colloquial speaker vector of each respective set of colloquial speaker vectors corresponds to a feature vector extracted from a temporal frame of a particular recorded colloquial speech utterance. In accordance with example embodiments each recorded colloquial speech utterance has an associated text string and each text string can be processed to derive an enriched transcription. By way of example an enriched transcription can include phonetic labels and descriptors of syntactic and linguistic content. Thus each given colloquial speaker vector has an associated enriched transcription derived from a respective text string associated with the particular recorded colloquial speech utterance from which the given colloquial speaker vector was extracted. As each colloquial speaker vector is replaced in accordance with step the associated enriched transcription for the replaced colloquial speaker vector is retained i.e. not replaced .

In further accordance with example embodiments aggregating the replaced colloquial speaker vectors of all the respective sets of colloquial speaker vectors into the aggregate set of conditioned speaker vectors can entail constructing a speech corpus that includes the replaced colloquial speaker vectors of all the respective sets of colloquial speaker vectors and the retained enriched transcriptions associated with each given colloquial speaker vector that was replaced. More particularly the speech corpus can be a training database for a TTS system.

Also in accordance with example embodiments replacing the colloquial speaker vectors of each respective set of colloquial speaker vectors entails doing so one respective set at a time. More particularly all of the colloquial speaker vectors of a given respective set are individually matched and replaced with a respective optimally matched reference speaker vector from among the reference set in a plurality of match and replace operations separate from that applied to the colloquial speaker vectors of any of the other respective sets. As described below carrying out the match and replace operations one respective set at a time helps mitigate possible inconsistencies between respective sets particularly in regards to the matching technique which accounts for statistical characteristics within each respective set.

As described above extracting speech features from recorded reference speech utterances and from the recorded colloquial speech utterances can entail generating feature vectors. More specifically and in accordance with example embodiments extracting speech features from recorded reference speech utterances of the reference speaker can entail decomposing the recorded reference speech utterances of the reference speaker into reference temporal frames of parameterized reference speech units. Each reference temporal frame can correspond to a respective reference speaker vector of speech features. By way of example the speech features can include spectral envelope parameters aperiodicity envelope parameters fundamental frequencies and or voicing of a respective reference speech unit.

Similarly and also in accordance with example embodiments extracting speech features from recorded colloquial speech utterances of the colloquial speaker can entail decomposing the recorded colloquial speech utterances of the colloquial speaker into colloquial temporal frames of parameterized colloquial speech units. Each colloquial temporal frame can correspond to a respective colloquial speaker vector of speech features. Again by way of example the speech features can include spectral envelope parameters aperiodicity envelope parameters fundamental frequencies and or voicing of a respective reference speech unit.

By way of example and in accordance with example embodiments the reference speech units can correspond to one phonemes triphone or other context sequences of phonemes. Similarly and also in accordance with example embodiments the colloquial speech units can correspond to one phonemes triphone or other context sequences of phonemes.

In further accordance with example embodiments replacing each colloquial speaker vector of each respective set of colloquial speaker vectors with the respective optimally matched reference speaker vector from among the reference set of reference speaker vectors can entail optimally matching speech features of the colloquial speaker vectors with speech features of the reference speaker vectors. More specifically for each respective colloquial speaker vector an optimal match between its speech features and the speech features of a particular one of the reference speaker vectors can be determined. In accordance with example embodiments the optimal match can be determined under a transform that compensates for differences in speech between the reference speaker and each respective colloquial speaker. Then for each respective colloquial speaker vector its speech features are replaced with the speech features of the determined particular one of the reference speaker vectors.

In further accordance with example embodiments the spectral envelope parameters of each vector of reference speech features can be Mel Cepstral coefficients Line Spectral Pairs Linear Predictive coefficients and or Mel Generalized Cepstral Coefficients. In addition indicia of first and second time derivatives of the spectral envelope parameters can be included. Similarly the spectral envelope parameters of each vector of colloquial speech features can be Mel Cepstral coefficients Line Spectral Pairs Linear Predictive coefficients and or Mel Generalized Cepstral Coefficients. Again indicia of first and second time derivatives of the spectral envelope parameters can be included as well.

In accordance with example embodiments the recorded reference speech utterances of the reference speaker can be in a reference language and the colloquial speech utterances of all the respective colloquial speakers can all be in a colloquial language. In one example wherein colloquial language can be lexically related to the reference language. In a further example the colloquial language and a lexically related reference language can be different. Then in still further accordance with example embodiments training the TTS system using the provided aggregate set of conditioned speaker vectors can entail training the TTS system to synthesize speech in the colloquial language but in a voice of the reference speaker.

It will be appreciated that the steps shown in are meant to illustrate a method in accordance with example embodiments. As such various steps could be altered or modified the ordering of certain steps could be changed and additional steps could be added while still achieving the overall desired operation.

Methods in accordance with an example embodiment such as the on described above devices could be implemented using so called thin clients and cloud based server devices as well as other types of client and server devices. Under various aspects of this paradigm client devices such as mobile phones and tablet computers may offload some processing and storage responsibilities to remote server devices. At least some of the time these client services are able to communicate via a network such as the Internet with the server devices. As a result applications that operate on the client devices may also have a persistent server based component. Nonetheless it should be noted that at least some of the methods processes and techniques disclosed herein may be able to operate entirely on a client device or a server device.

This section describes general system and device architectures for such client devices and server devices. However the methods devices and systems presented in the subsequent sections may operate under different paradigms as well. Thus the embodiments of this section are merely examples of how these methods devices and systems can be enabled.

Network may be for example the Internet or some other form of public or private Internet Protocol IP network. Thus client devices and may communicate using packet switching technologies. Nonetheless network may also incorporate at least some circuit switching technologies and client devices and may communicate via circuit switching alternatively or in addition to packet switching.

A server device may also communicate via network . In particular server device may communicate with client devices and according to one or more network protocols and or application level protocols to facilitate the use of network based or cloud based computing on these client devices. Server device may include integrated data storage e.g. memory disk drives etc. and may also be able to access a separate server data storage . Communication between server device and server data storage may be direct via network or both direct and via network as illustrated in . Server data storage may store application data that is used to facilitate the operations of applications performed by client devices and and server device .

Although only three client devices one server device and one server data storage are shown in communication system may include any number of each of these components. For instance communication system may comprise millions of client devices thousands of server devices and or thousands of server data storages. Furthermore client devices may take on forms other than those in .

User interface may comprise user input devices such as a keyboard a keypad a touch screen a computer mouse a track ball a joystick and or other similar devices now known or later developed. User interface may also comprise user display devices such as one or more cathode ray tubes CRT liquid crystal displays LCD light emitting diodes LEDs displays using digital light processing DLP technology printers light bulbs and or other similar devices now known or later developed. Additionally user interface may be configured to generate audible output s via a speaker speaker jack audio output port audio output device earphones and or other similar devices now known or later developed. In some embodiments user interface may include software circuitry or another form of logic that can transmit data to and or receive data from external user input output devices.

Communication interface may include one or more wireless interfaces and or wireline interfaces that are configurable to communicate via a network such as network shown in . The wireless interfaces if present may include one or more wireless transceivers such as a BLUETOOTH transceiver a Wifi transceiver perhaps operating in accordance with an IEEE 802.11 standard e.g. 802.11b 802.11g 802.11n a WiMAX transceiver perhaps operating in accordance with an IEEE 802.16 standard a Long Term Evolution LTE transceiver perhaps operating in accordance with a 3rd Generation Partnership Project 3GPP standard and or other types of wireless transceivers configurable to communicate via local area or wide area wireless networks. The wireline interfaces if present may include one or more wireline transceivers such as an Ethernet transceiver a Universal Serial Bus USB transceiver or similar transceiver configurable to communicate via a twisted pair wire a coaxial cable a fiber optic link or other physical connection to a wireline device or network.

In some embodiments communication interface may be configured to provide reliable secured and or authenticated communications. For each communication described herein information for ensuring reliable communications e.g. guaranteed message delivery can be provided perhaps as part of a message header and or footer e.g. packet message sequencing information encapsulation header s and or footer s size time information and transmission verification information such as cyclic redundancy check CRC and or parity check values . Communications can be made secure e.g. be encoded or encrypted and or decrypted decoded using one or more cryptographic protocols and or algorithms such as but not limited to the data encryption standard DES the advanced encryption standard AES the Rivest Shamir and Adleman RSA algorithm the Diffie Hellman algorithm and or the Digital Signature Algorithm DSA . Other cryptographic protocols and or algorithms may be used instead of or in addition to those listed herein to secure and then decrypt decode communications.

Processor may include one or more general purpose processors e.g. microprocessors and or one or more special purpose processors e.g. digital signal processors DSPs graphical processing units GPUs floating point processing units FPUs network processors or application specific integrated circuits ASICs . Processor may be configured to execute computer readable program instructions that are contained in data storage and or other instructions to carry out various functions described herein.

Data storage may include one or more non transitory computer readable storage media that can be read or accessed by processor . The one or more computer readable storage media may include volatile and or non volatile storage components such as optical magnetic organic or other memory or disc storage which can be integrated in whole or in part with processor . In some embodiments data storage may be implemented using a single physical device e.g. one optical magnetic organic or other memory or disc storage unit while in other embodiments data storage may be implemented using two or more physical devices.

Data storage may also include program data that can be used by processor to carry out functions described herein. In some embodiments data storage may include or have access to additional data storage components or devices e.g. cluster data storages described below .

Referring again briefly to server device and server data storage device may store applications and application data at one or more locales accessible via network . These locales may be data centers containing numerous servers and storage devices. The exact physical location connectivity and configuration of server device and server data storage device may be unknown and or unimportant to client devices. Accordingly server device and server data storage device may be referred to as cloud based devices that are housed at various remote locations. One possible advantage of such cloud based computing is to offload processing and data storage from client devices thereby simplifying the design and requirements of these client devices.

In some embodiments server device and server data storage device may be a single computing device residing in a single data center. In other embodiments server device and server data storage device may include multiple computing devices in a data center or even multiple computing devices in multiple data centers where the data centers are located in diverse geographic locations. For example depicts each of server device and server data storage device potentially residing in a different physical location.

In some embodiments each of the server clusters A B and C may have an equal number of server devices an equal number of cluster data storages and an equal number of cluster routers. In other embodiments however some or all of the server clusters A B and C may have different numbers of server devices different numbers of cluster data storages and or different numbers of cluster routers. The number of server devices cluster data storages and cluster routers in each server cluster may depend on the computing task s and or applications assigned to each server cluster.

In the server cluster A for example server devices A can be configured to perform various computing tasks of a server such as server device . In one embodiment these computing tasks can be distributed among one or more of server devices A. Server devices B and C in server clusters B and C may be configured the same or similarly to server devices A in server cluster A. On the other hand in some embodiments server devices A B and C each may be configured to perform different functions. For example server devices A may be configured to perform one or more functions of server device and server devices B and server device C may be configured to perform functions of one or more other server devices. Similarly the functions of server data storage device can be dedicated to a single server cluster or spread across multiple server clusters.

Cluster data storages A B and C of the server clusters A B and C respectively may be data storage arrays that include disk array controllers configured to manage read and write access to groups of hard disk drives. The disk array controllers alone or in conjunction with their respective server devices may also be configured to manage backup or redundant copies of the data stored in cluster data storages to protect against disk drive failures or other types of failures that prevent one or more server devices from accessing one or more cluster data storages.

Similar to the manner in which the functions of server device and server data storage device can be distributed across server clusters A B and C various active portions and or backup redundant portions of these components can be distributed across cluster data storages A B and C. For example some cluster data storages A B and C may be configured to store backup versions of data stored in other cluster data storages A B and C.

Cluster routers A B and C in server clusters A B and C respectively may include networking equipment configured to provide internal and external communications for the server clusters. For example cluster routers A in server cluster A may include one or more packet switching and or routing devices configured to provide i network communications between server devices A and cluster data storage A via cluster network A and or ii network communications between the server cluster A and other devices via communication link A to network . Cluster routers B and C may include network equipment similar to cluster routers A and cluster routers B and C may perform networking functions for server clusters B and C that cluster routers A perform for server cluster A.

Additionally the configuration of cluster routers A B and C can be based at least in part on the data communication requirements of the server devices and cluster storage arrays the data communications capabilities of the network equipment in the cluster routers A B and C the latency and throughput of the local cluster networks A B C the latency throughput and cost of the wide area network connections A B and C and or other factors that may contribute to the cost speed fault tolerance resiliency efficiency and or other design goals of the system architecture.

As shown in client device may include a communication interface a user interface a processor and data storage all of which may be communicatively linked together by a system bus network or other connection mechanism .

Communication interface functions to allow client device to communicate using analog or digital modulation with other devices access networks and or transport networks. Thus communication interface may facilitate circuit switched and or packet switched communication such as POTS communication and or IP or other packetized communication. For instance communication interface may include a chipset and antenna arranged for wireless communication with a radio access network or an access point. Also communication interface may take the form of a wireline interface such as an Ethernet Token Ring or USB port. Communication interface may also take the form of a wireless interface such as a Wifi BLUETOOTH global positioning system GPS or wide area wireless interface e.g. WiMAX or LTE . However other forms of physical layer interfaces and other types of standard or proprietary communication protocols may be used over communication interface . Furthermore communication interface may comprise multiple physical communication interfaces e.g. a Wifi interface a BLUETOOTH interface and a wide area wireless interface .

User interface may function to allow client device to interact with a human or non human user such as to receive input from a user and to provide output to the user. Thus user interface may include input components such as a keypad keyboard touch sensitive or presence sensitive panel computer mouse trackball joystick microphone still camera and or video camera. User interface may also include one or more output components such as a display screen which for example may be combined with a touch sensitive panel CRT LCD LED a display using DLP technology printer light bulb and or other similar devices now known or later developed. User interface may also be configured to generate audible output s via a speaker speaker jack audio output port audio output device earphones and or other similar devices now known or later developed. In some embodiments user interface may include software circuitry or another form of logic that can transmit data to and or receive data from external user input output devices. Additionally or alternatively client device may support remote access from another device via communication interface or via another physical interface not shown .

Processor may comprise one or more general purpose processors e.g. microprocessors and or one or more special purpose processors e.g. DSPs GPUs FPUs network processors or ASICs . Data storage may include one or more volatile and or non volatile storage components such as magnetic optical flash or organic storage and may be integrated in whole or in part with processor . Data storage may include removable and or non removable components.

In general processor may be capable of executing program instructions e.g. compiled or non compiled program logic and or machine code stored in data storage to carry out the various functions described herein. Data storage may include a non transitory computer readable medium having stored thereon program instructions that upon execution by client device cause client device to carry out any of the methods processes or functions disclosed in this specification and or the accompanying drawings. The execution of program instructions by processor may result in processor using data .

By way of example program instructions may include an operating system e.g. an operating system kernel device driver s and or other modules and one or more application programs e.g. address book email web browsing social networking and or gaming applications installed on client device . Similarly data may include operating system data and application data . Operating system data may be accessible primarily to operating system and application data may be accessible primarily to one or more of application programs . Application data may be arranged in a file system that is visible to or hidden from a user of client device .

Application programs may communicate with operating system through one or more application programming interfaces APIs . These APIs may facilitate for instance application programs reading and or writing application data transmitting or receiving information via communication interface receiving or displaying information on user interface and so on.

In some vernaculars application programs may be referred to as apps for short. Additionally application programs may be downloadable to client device through one or more online application stores or application markets. However application programs can also be installed on client device in other ways such as via a web browser or through a physical interface e.g. a USB port on client device .

A TTS synthesis system or more generally a speech synthesis system may operate by receiving an input text string processing the text string into a symbolic representation of the phonetic and linguistic content of the text string generating a sequence of speech features corresponding to the symbolic representation and providing the speech features as input to a speech synthesizer in order to produce a spoken rendering of the input text string. The symbolic representation of the phonetic and linguistic content of the text string may take the form of a sequence of labels each label identifying a phonetic speech unit such as a phoneme and further identifying or encoding linguistic and or syntactic context temporal parameters and other information for specifying how to render the symbolically represented sounds as meaningful speech in a given language. While the term phonetic transcription is sometimes used to refer to such a symbolic representation of text the term enriched transcription introduced above will instead be used herein in order to signify inclusion of extra phonetic content such as linguistic and or syntactic context and temporal parameters represented in the sequence of labels. 

The enriched transcription provides a symbolic representation of the phonetic and linguistic content of the text string as rendered speech and can be represented as a sequence of phonetic speech units identified according to labels which could further identify or encode linguistic and or syntactic context temporal parameters and other information for specifying how to render the symbolically represented sounds as meaningful speech in a given language. As discussed above the phonetic speech units could be phonemes. A phoneme may be considered to be the smallest segment of speech of given language that encompasses a meaningful contrast with other speech segments of the given language. Thus a word typically includes one or more phonemes. For purposes of simplicity phonemes may be thought of as utterances of letters although this is not a perfect analogy as some phonemes may present multiple letters. As an example the phonemic spelling for the American English pronunciation of the word cat is k ae t and consists of the phonemes k ae and t . Another example is the phonemic spelling for the word dog is d aw g consisting of the phonemes d aw and g . Different phonemic alphabets exist and other phonemic representations are possible. Common phonemic alphabets for American English contain about 40 distinct phonemes. Other languages may be described by different phonemic alphabets containing different phonemes.

The phonetic properties of a phoneme in an utterance can depend on or be influenced by the context in which it is or is intended to be spoken. For example a triphone is a triplet of phonemes in which the spoken rendering of a given phoneme is shaped by a temporally preceding phoneme referred to as the left context and a temporally subsequent phoneme referred to as the right context. Thus the ordering of the phonemes of English language triphones corresponds to the direction in which English is read. Other phoneme contexts such as quinphones may be considered as well.

Speech features represent acoustic properties of speech as parameters and in the context of speech synthesis may be used for driving generation of a synthesized waveform corresponding to an output speech signal. Generally features for speech synthesis account for three major components of speech signals namely spectral envelopes that resemble the effect of the vocal tract excitation that simulates the glottal source and prosody that describes pitch contour melody and tempo rhythm . In practice features may be represented in multidimensional feature vectors that correspond to one or more temporal frames. One of the basic operations of a TTS synthesis system is to map an enriched transcription e.g. a sequence of labels to an appropriate sequence of feature vectors.

In the context of speech recognition features may be extracted from a speech signal e.g. a voice recording in a process that typically involves sampling and quantizing an input speech utterance within sequential temporal frames and performing spectral analysis of the data in the frames to derive a vector of features associated with each frame. Each feature vector can thus be viewed as providing a snapshot of the temporal evolution of the speech utterance.

By way of example the features may include Mel Filter Cepstral MFC coefficients. MFC coefficients may represent the short term power spectrum of a portion of an input utterance and may be based on for example a linear cosine transform of a log power spectrum on a nonlinear Mel scale of frequency. A Mel scale may be a scale of pitches subjectively perceived by listeners to be about equally distant from one another even though the actual frequencies of these pitches are not equally distant from one another. 

In some embodiments a feature vector may include MFC coefficients first order cepstral coefficient derivatives and second order cepstral coefficient derivatives. For example the feature vector may contain 13 coefficients 13 first order derivatives delta and 13 second order derivatives delta delta therefore having a length of 39. However feature vectors may use different combinations of features in other possible embodiments. As another example feature vectors could include Perceptual Linear Predictive PLP coefficients Relative Spectral RASTA coefficients Filterbank log energy coefficients or some combination thereof. Each feature vector may be thought of as including a quantified characterization of the acoustic content of a corresponding temporal frame of the utterance or more generally of an audio input signal .

In accordance with example embodiments of HMM based speech synthesis a sequence of labels corresponding to enriched transcription of the input text may be treated as observed data and a sequence of HMMs and HMM states is computed so as to maximize a joint probability of generating the observed enriched transcription. The labels of the enriched transcription sequence may identify phonemes triphones and or other phonetic speech units. In some HMM based techniques phonemes and or triphones are represented by HMMs as having three states corresponding to three temporal phases namely beginning middle and end. Other HMMs with a different number of states per phoneme or triphone for example could be used as well. In addition the enriched transcription may also include additional information about the input text string such as time or duration models for the phonetic speech units linguistic context and other indicators that may characterize how the output speech should sound for example.

In accordance with example embodiments speech features corresponding to HMMs and HMM states may be represented by multivariate PDFs for jointly modeling the different features that make up the feature vectors. In particular multivariate Gaussian PDFs can be used to compute probabilities of a given state emitting or generating multiple dimensions of features from a given state of the model. Each dimension of a given multivariate Gaussian PDF could thus correspond to different feature. It is also possible to model a feature along a given dimension with more than one Gaussian PDF in that dimension. In such an approach the feature is said to be modeled by a mixture of Gaussians referred to a Gaussian mixture model or GMM. The sequence of features generated by the most probable sequence of HMMs and HMM states can be converted to speech by a speech synthesizer for example.

It should be noted that the discussion in this section and the accompanying figures are presented for purposes of example. Other TTS system arrangements including different components different relationships between the components and or different processing may be possible. For example in an alternative embodiment a TTS system could use a machine learning model such a neural network for generating speech features at run time based on learned trained associations between known labels and known parameterized speech.

In accordance with example embodiments the text analysis module may receive an input text string or other form of text based input and generate an enriched transcription as output. The input text string could be a text message email chat input or other text based communication for example. As described above the enriched transcription could correspond to a sequence of labels that identify speech units including context information.

As shown the TTS subsystem may employ HMM based speech synthesis to generate feature vectors corresponding to the enriched transcription . This is illustrated in by a symbolic depiction of a reference HMM in the TTS subsystem . The reference HMM is represented by a configuration of speech unit HMMs each corresponding to a phonetic speech unit of a reference language. The phonetic units could be phonemes or triphones for example. Each speech unit HMM is drawn as a set of circles each representing a state of the speech unit and arrows connecting the circles each arrow representing a state transition. A circular arrow at each state represents a self transition. Above each circle is a symbolic representation of a PDF. In the HMM methodology the PDF specifies the probability that a given state will emit or generate speech features corresponding to the speech unit modeled by the state. The depiction in the figure of three states per speech unit HMM is consistent with some HMM techniques that model three states for each speech unit. However HMM techniques using different numbers of states per speech units may be employed as well and the illustrative use of three states in as well as in other figures herein is not intended to be limiting with respect to example embodiments described herein. Further details of an example TTS synthesis system are described below.

In the example of the TTS subsystem outputs synthesized speech in a voice of a reference speaker. The reference speaker could be a speaker used to train the reference HMM.

In further accordance with example embodiments the HMMs of a HMM based TTS synthesis system may be trained by tuning the PDF parameters using a database of text recorded speech and corresponding known text strings.

For purposes of illustration is depicted in a way that represents two operational modes training time and run time. A thick horizontal line marks a conceptual boundary between these two modes with Training Time labeling a portion of above the line and Run Time labeling a portion below the line. As a visual cue various arrows in the figure signifying information and or processing flow and or transmission are shown as dashed lines in the Training Time portion of the figure and as solid lines in the Run Time portion.

During training a training time text string from the speech database may be input to the text analysis module which then generates training time labels an enriched transcription of the training time text string . Each training time label could be made up of a phonetic label identifying a phonetic speech unit e.g. a phoneme context information e.g. one or more left context and right context phoneme labels physical speech production characteristics linguistic context etc. and timing information such as a duration relative timing position and or phonetic state model.

The training time labels are then input to the HMM module which models training time predicted spectral parameters and training time predicted excitation parameters . These may be considered speech features that are generated by the HMM module according to state transition probabilities and state emission probabilities that make up at least in part the HMM parameters. The training time predicted spectral parameters and training time predicted excitation parameters are then input to the HMM training module as shown.

In further accordance with example embodiments during training a training time speech signal from the speech database is input to the feature extraction module which processes the input signal to generate expected spectral parameters and expected excitation parameters . The training time speech signal is predetermined to correspond to the training time text string this is signified by a wavy dashed double arrow between the training time speech signal and the training time text string . In practice the training time speech signal could be a speech recording of a speaker reading the training time text string . More specifically the corpus of training data in the speech database could include numerous recordings of a reference speaker reading numerous text strings. The expected spectral parameters and expected excitation parameters may be considered known parameters since they are derived from a known speech signal.

During training time the expected spectral parameters and expected excitation parameters are provided as input to the HMM training module . By comparing the training time predicted spectral parameters and training time predicted excitation parameters with the expected spectral parameters and expected excitation parameters the HMM training module can determine how to adjust the HMM parameters so as to achieve closest or optimal agreement between the predicted results and the known results. While this conceptual illustration of HMM training may appear suggestive of a feedback loop for error reduction the procedure could entail a maximum likelihood ML adjustment of the HMM parameters. This is indicated by the return of ML adjusted HMM parameters from the HMM training module to the HMM parameters . In practice the training procedure may involve many iterations over many different speech samples and corresponding text strings in order to cover all or most of the phonetic speech units of the language of the TTS speech synthesis system with sufficient data to determine accurate parameter values.

During run time operation illustrated in the lower portion of below thick horizontal line a run time text string is input to the text analysis module which then generates run time labels an enriched transcription of the run time text string . The form of the run time labels may be the same as that for the training time labels . The run time labels are then input to the HMM module which generates run time predicted spectral parameters and run time predicted excitation parameters again according to the HMM based technique.

The run time predicted spectral parameters and run time predicted excitation parameters can generated in pairs each pair corresponding to a predicted pair of feature vectors for generating a temporal frame of waveform data.

In accordance with example embodiments the run time predicted spectral parameters and run time predicted excitation parameters may next be input to the speech synthesizer module which may then synthesize a run time speech signal . As an example speech synthesize could include a vocoder that can translate the acoustic features of the input into an output waveform suitable for playout on an audio output device and or for analysis by a signal measuring device or element. Such a device or element could be based on signal measuring hardware and or machine language instructions that implement an analysis algorithm. With sufficient prior training the run time speech signal may have a high likelihood of being an accurate speech rendering of the run time text string .

In an alternative embodiment a neural network such as a feed forward neural network can be used for mapping enriched transcriptions to parameterized speech. A neural network can be implemented as machine language instructions such as a software and or firmware program in a centralized and or distributed fashion on one or more computing platforms or systems for example. In algorithmic terms a neural network can be described as having one or more layers each including a set of nodes. Each node can correspond to a mathematical function such as a scalar weighting function having adjustable parameters and by which can be computed a scalar output of one or more inputs. All of the nodes may be the same scalar function differing only according to possibly different parameter values for example. By way of example the mathematical function could take the form of a sigmoid function. The output of each node in a given layer can be connected to the inputs of one or more nodes of the next forward layer. The nodes of a first input layer can receive input data at their respective inputs and the nodes of a last output layer can deliver output data from their respective outputs. There can be one or more hidden layers between the input and output layers.

In the context of a TTS system for example the input layer could receive one or more enriched transcriptions and the output layer could deliver feature vectors or other form of parameterized speech. By appropriately adjusting the respective parameter values of the functions of the nodes during a training process the neural network can learn how to later accurately generate and output run time predicted feature vectors in response to enriched transcriptions received as input at run time.

As with the TTS system illustrated in a training time operational mode and a run time operational mode are represented in . Again a thick horizontal line marks a conceptual boundary between these two modes with Training Time labeled above the line and Run Time labeled below. Data processing flow is represented in dashed lines in the Training Time portion of the figure and in solid lines in the Run Time portion.

Operation of the TTS system in the two modes is largely similar to that described for the HMM based TTS system in except for certain aspects related to the neural network. During training a training time text string from the speech database may be input to the text analysis module which then generates training time labels an enriched transcription of the training time text string . The training time labels are then input to the feature generation module which models training time predicted spectral parameters and training time predicted excitation parameters . These correspond to speech features generated by the neural network . The training time predicted spectral parameters and training time predicted excitation parameters are then input to the neural network training module as shown.

Also during training time a training time speech signal from the speech database is input to the feature extraction module which processes the input signal to generate expected spectral parameters and expected excitation parameters . A correspondence between the training time speech signal and the training time text string is signified by a wavy dashed double arrow between the two. The expected spectral parameters and expected excitation parameters are provided as input to the neural network training module . By comparing the training time predicted spectral parameters and training time predicted excitation parameters with the expected spectral parameters and expected excitation parameters the neural network training module can determine how to adjust the neural network so as to achieve closest or optimal agreement between the predicted results and the known results. For example the parameters of the scalar function in each node of the neural network can be iteratively adjusted to achieve the consistent and accurate agreement between expected and training time parameters.

During run time operation a run time text string can be input to the text analysis module which then generates run time labels . The run time labels are then input to the feature generation module which generates run time predicted spectral parameters and run time predicted excitation parameters according to the trained NN based operation. The run time predicted spectral parameters and run time predicted excitation parameters can be input to the speech synthesizer module which may then synthesize a run time speech signal .

In feature extraction for generating expected spectral and excitation parameters and text analysis for generating training time labels are represented as training time operations. However these operations need not necessarily be carried out during training time. More particularly they can be carried prior to training time and their outputs stored in a training database which can subsequently be accessed during training time to achieve the same purpose at that depicted in . In accordance with example embodiments a training database can be created during a separate phase or operational mode from training and can further be conditioned prior to training to improve the quality of the data and hence improve the accuracy and effectiveness of the subsequent training.

In three operational modes descriptively labeled Training Database Construction Training Time and Run Time are represented in three panels separated by two thick horizontal lines. Data processing flow is represented in dashed lines in the Training Database Construction panel top and the Training Time panel middle and in solid lines in the Run Time panel bottom . Some of the functional components of the TTS system have operational roles in more than one mode and are represented more than once in .

During training database construction a training time text string from the speech database may be input to the text analysis module which then generates training time labels an enriched transcription of the training time text string . Also during training database construction a training time speech signal from the speech database is input to the feature extraction module which processes the input signal to generate expected spectral parameters and expected excitation parameters . A correspondence between the training time speech signal and the training time text string is signified by a wavy dashed double arrow between the two. The expected spectral parameters and expected excitation parameters and the training time labels are all then stored in the training database together with a mapping or association between the parameterize speech and the labels. The training database can then be accessed during training time to train the TTS system .

During training time the training time labels can be retrieved from the training database and input to the HMM module which models training time predicted spectral parameters and training time predicted excitation parameters . The training time predicted spectral parameters and training time predicted excitation parameters are then input to the HMM training module as shown. Also during training time the expected spectral parameters and expected excitation parameters associated with the training time labels can be retrieved from the training database and provided as input to the HMM training module . By comparing the training time predicted spectral parameters and training time predicted excitation parameters with the expected spectral parameters and expected excitation parameters the HMM training module can determine how to adjust the HMM parameters so as to achieve closest or optimal agreement between the predicted results and the known results.

During run time operation a run time text string is input to the text analysis module which then generates run time labels . The run time labels are then input to the HMM module which generates run time predicted spectral parameters and run time predicted excitation parameters . In accordance with example embodiments the run time predicted spectral parameters and run time predicted excitation parameters can then input to the speech synthesizer module which can synthesize a run time speech signal .

Note that while illustrates three separate operational modes for a HMM based TTS system a similar configuration of three modes training database construction training time and run time can be achieved with a NN based TTS system such as the one illustrated in . Explicit description of such a configuration is omitted here for the sake of brevity.

In accordance with example embodiments a training database constructed in a separate operation from actual training such as the training database can be conditioned prior to use in training so as to improve the quality of the training data and thereby improve the accuracy and effectiveness of the subsequent training More particularly conditioning a training database can entail replacing feature vectors e.g. the expected spectral parameters and expected excitation parameters with ones from a known high quality database using an optimal matching technique. Such a conditioning procedure is described below.

The accuracy of a TTS system e.g. how accurately the TTS system maps text to intended speech e.g. as written and the quality of a TTS system e.g. how natural or good the synthesize voice sounds can depend at least in part on the quality and quantity of the speech samples e.g. speech utterances used for training the TTS system. More particularly the quality of record samples can affect the accuracy with which speech utterances can be decomposed into feature vectors used for training And the quality of recorded speech samples together with the quantity can affect the consistency with which mapping numerous recorded instances of the same intended speech sounds e.g. acoustic renderings of speech units such as phonemes can yield similar characteristic parametric representations of those sounds e.g. feature vectors . This can in turn be a factor in how well the TTS system can be trained to reproduce the parametric representations for speech synthesis at run time.

Considering by way of example the TTS system in speech samples used for training can be recorded and stored in the speech database together with their associated text strings. The quality and effectiveness of training a text to speech system such as the TTS system can therefore be tied to the quality and quantity of the speech samples in the speech database since these are among the factors can determine the quality of the feature vectors used in training e.g. the expected spectral parameters and expected excitation parameters in the example of the TTS system .

One conventional approach to assembling a speech database of a large number quantity of high quality recordings is to invest significant effort into acquiring a large number of speech samples from a skilled trained speaker reading from standard or canonical text sources and recording the readings under controlled relatively noise free conditions. While this approach can yield good results for training a TTS system it can pose practical challenges and involve large expense in terms of time and cost in some circumstances. For example the availability of and or demand for trained readers and controlled recording facilities might be relatively less common among speakers of certain long tail languages than among large populations of widely spoken languages. This is just one example of a circumstance that might be an impediment to a conventional approach to building a speech database for TTS training.

By contrast it may be relatively easy and or inexpensive to acquire a large number of samples of multiple different speakers of a given language recorded under diverse and to some extent uncontrolled conditions e.g. noise quality of recording equipment etc. . Such recordings might be acquired on an ad hoc basis such as man on the street or impromptu recording sessions for example. Additionally or alternatively a wide variety of recording collections might be publically and freely available on the Internet or other public networks. While the total quantity of recordings represented in all or even just some such recording collections can be quite large the potential inconsistencies of the recordings both in terms of speaker skill e.g. voice clarity voice quality etc. and recording conditions can diminish the quality of a speech database that includes these recordings. Consequently the accuracy and quality of TTS system trained using such a speech database can suffer.

In accordance with example embodiments a high quality training database such as training database can be constructed from numerous individual recording collections of speech samples of different speakers of the same common language made under diverse recording conditions by applying a conditioning technique to feature vectors derived from the recorded samples. More specifically the conditioning technique entails replacing feature vectors derived from recorded speech samples of multiple different speakers of the same common language with optimally matched speaker vectors derived from recorded speech samples of a reference speaker of a reference language in a quality controlled speech database referred to herein as a reference speech database. Identification of the optimally matched speaker vectors is achieved using a technique that matches speaker vectors of different speakers under a transform that compensates for differences in speech between the different speakers.

The matching technique referred to as matching under transform or MUT enables parameterized representations of speech sounds derived from speech of a reference speaker to be optimally matched to parameterized representations of speech sounds derived from speech of one or more other speakers. When the speech of the reference speaker is of higher quality than that of the one or more other speakers the optimally matched parameterized representations can serve as higher quality replacements of the parameterized representations that were derived from the speech of the one or more other speakers.

In accordance with example embodiments the matching and replacing technique using MUT can be applied separately to each of multiple speech databases acquired from different speakers to create separate sets of replaced conditioned feature vectors. The separate sets of replaced feature vectors can then be aggregated into a single conditioned training database referred to as an aggregated conditioned training database. Carrying out the matching and replacing technique separately on each of multiple speech databases can eliminate the effect of inconsistencies between the different multiple speech databases thereby achieving the best MUT results for each of the multiple speech databases before all the replaced feature vectors are aggregated.

The reference language of the reference speaker need not be the same as the common language of the multiple speakers although this is not excluded by example embodiments. Rather the reference language and the common language may be lexically related. For example they may represent different but related branches or descendants of a single language family. Other relationships based on some form of similarity or commonality between the reference language and the common language are possible as well.

For purposes of convenience in the discussion herein the common language of the multiple speakers will be referred to as a colloquial language. As noted earlier the use of the qualitative descriptor colloquial is meant to signify a generalized impact of a relatively diminished emphasis on speaker consistency speech quality and or control of recording conditions in the process of obtaining the speech databases of the multiple speakers. The qualitative descriptor colloquial will also be adopted to refer to the multiple speakers the speech databases obtained from their recordings as well as aspects and elements related to processing of their speech.

In accordance with example embodiments a respective colloquial speech database can be acquired from each of multiple colloquial speakers of a colloquial language. Each colloquial speech database can contain a respective plurality of colloquial speech utterances speech samples each corresponding to a text string or other form of written text . For purposes of discussion the number of colloquial speech databases will be taken to be K each obtained from one of K colloquial speakers. For example each of the K colloquial speech databases might represent one of K different recording sessions with one of the K colloquial speakers.

Referring again to each of the K colloquial speech databases can be of a form represented by the speech database . In accordance with example embodiments each colloquial speech database can be processed to construct a corresponding respective colloquial training database. For example the process described for constructing the training database can be used to construct K colloquial training databases. Each colloquial training database can be of a form represented by the training database each containing a respective plurality of colloquial speaker vectors and each colloquial speaker vector having an associated enriched transcription. For example still keeping with the example illustrated in each colloquial speaker vector can correspond to a vector of expected spectral parameters and expected excitation parameters the associated transcription can be the associate training time labels .

The number of colloquial speaker vectors and associated enriched transcriptions need not be the same in each of the colloquial training databases. For identification purposes the K colloquial training databases can be indexed by k k 1 . . . K. To signify the possibly different number of colloquial speaker vectors in each colloquial training database the colloquial speaker vectors in each colloquial training database can be indexed by j j 1 . . . J where Jis the number of colloquial speaker vectors in the kth colloquial training database and again k 1 . . . K. The total number N of colloquial speaker vectors in all K colloquial training database is then given by N J.

Also in accordance with example embodiments a reference speech database can be acquired from a reference speaker of a reference language. The reference speech database can contain a plurality of reference speech utterances speech samples each corresponding to a text string or other form or written text . Furthermore the reference speech database can be processed to construct a corresponding reference training database. For example the process described for constructing the training database can also be used to construct the reference training database. The reference training database can be of a form represented by the training database containing a plurality of reference speaker vectors and each reference speaker vector having an associated enriched transcription. For example each reference speaker vector can correspond to a vector of expected spectral parameters and expected excitation parameters the associated transcription can be the associate training time labels .

For purposes of discussion the number of reference speaker vectors in the reference training database will be taken to be M. The individual reference speaker vectors can be indexed by i i 1 . . . M. Note that in general M can be different any or all of J k 1 . . . K. In practice for circumstance in which the reference speech database represents a high quality speech database of a large quantity of reference speech samples and each of the K colloquial speech databases might be represent relatively small speech databases it might be the case that M J k 1 . . . K. This need not be the case however. Furthermore even for M J k 1 . . . K it may be that M

In accordance with example embodiments for each of the Jcolloquial speaker vectors in the kth colloquial training database MUT can be used to identify an optimally matched reference speaker vector from among the M reference speaker vectors in the reference training database. Once the identification is made the colloquial speaker of each match can be replaced in the kth colloquial training database with the identified optimally matched reference speaker vector. As described in more detail below MUT operates jointly over the ensemble of all the Jcolloquial speaker vectors in the kth colloquial training database and the ensemble of all M reference speaker vectors in the reference training database. Thus all of the identifications are made for the colloquial speaker vectors in the kth colloquial training database in a joint operation before each colloquial speaker vectors in the kth colloquial training database is replaced by its identified optimal match.

In further accordance with example embodiments for each colloquial speaker vector replaced by its optimally matched reference speaker vector in the manner described above the enriched transcription associated with the colloquial speaker vector is retained. Thus the respective enriched transcription that represents a symbolic phonetic description of each colloquial speaker vector comes to be associated with a replaced speaker vector. Put another way the parametric representation of speech associated with each enriched transcription can be considered as being updated with a new parametric representation of that speech obtained from parametric representations in the reference training database using MUT.

In further accordance with example embodiments the joint MUT operation is carried out separately for each different colloquial training database. That is the joint MUT operation is carried out separately over each of the k 1 . . . K colloquial training database. However each joint MUT operation matches the Jcolloquial speaker vectors in the kth colloquial training database against the same M reference speaker vectors in the reference training database. By carrying out the MUT separately in this manner any possible inconsistencies between the different colloquial training databases does not enter into any of the joint operations.

Note that the replacement of colloquial speaker vectors in kth colloquial training database the can be carried out after the MUT identifications are made for the kth colloquial training database or after the MUT identifications are made for all K of the colloquial training databases. Either approach can be accommodated by appropriately keeping track of the MUT identifications made in each of the K joint MUT operations.

In accordance with example embodiments the replaced speaker vectors in all the K colloquial training databases can be aggregated into an aggregated conditioned training database. By doing so a high quality training database containing all the N total replaced speaker vectors can be constructed. The aggregated conditioned training database can then be used to train a TTS system. The N replaced speaker vectors can be added to the aggregated conditioned training database all at once following completion first of the replacing J k 1 . . . K colloquial speaker vectors in all the K colloquial training databases before aggregating them. Alternatively the N replaced speaker vectors can be aggregated iteratively by adding the replaced Jcolloquial speaker vectors of the kth colloquial training database before carrying out MUT and replacement of the Jcolloquial speaker vectors in the k 1st colloquial training database and so on for example.

Each respective colloquial speaker vector in is also depicted next to its associated enriched transcription which carries the same index as the respective colloquial speaker vector. For the sake of brevity in the figure the colloquial speaker vectors are simply labeled Colloq. Vector and the associated enriched transcriptions are simply labeled Colloq. Labels. 

The top and bottom middle portion of depicts the i 1 . . . M reference speaker vectors of the reference training database . In accordance with example embodiments the same reference training database is used in MUT and replacement for each of the colloquial training databases. This is indicated by the duplicate depiction of the reference training database in the top and bottom of the figure. The vertical ellipses in the middle portion of represent repeated use of the reference training database for the other MUT and replacement operations. For the sake of brevity in the figure the reference speaker vectors are simply labeled Ref. Vector. Enriched transcriptions that can be associated with the reference speaker vectors are omitted from the illustration since MUT only operates on the parameterized speech representations e.g. feature vectors . Vertical ellipses between the reference speaker vectors within the reference training database represent other possible reference speaker vectors of the reference training databases not explicitly shown in the figure .

In accordance with example embodiments each colloquial speaker vector is replaced by an optimally matched reference speaker vector. Thus each respective replaced speaker vector in is labeled Ref. Vector since it comes from the reference training database. Also in accordance with example embodiment the respective enriched transcription associated with each replaced speaker vector is retained. This is also indicated in by the reuse of the Colloq. Labels from the colloquial training databases . . . K.

Example operation of MUT and replacement illustrated in is represented conceptually by black curved lines connecting colloquial speaker vectors in the colloquial training databases and K with reference speaker vectors in the reference training database at the top and bottom of and by black curved arrows connecting the matched reference speaker vectors in the reference training database at the top and bottom of with the replaced speaker vectors in the replaced training databases and K. By way of example the colloquial speaker vector j 1 is shown to be matched with the reference speaker vector i 3. Also by way of example the colloquial speaker vector j 2 is shown to be matched with the reference speaker vector i M the colloquial speaker vector j 3 is shown to be matched with the reference speaker vector i the colloquial speaker vector j is shown to be matched with the reference speaker vector i 1 and the colloquial speaker vector j Jis shown to be matched with the reference speaker vector i 2. As mentioned above and described in more detail below the optimal matching of the individual colloquial speaker vectors to the reference speaker vectors is carried out jointly over the ensemble of speaker vectors in both training database. The particular matches represented in this example by the thick curved lines are arbitrary and for purposes of illustration only.

As shown by the black curved arrows the reference speaker vectors identified as optimal matches to the colloquial speaker vectors in the colloquial training database the become the replacement speaker vectors in the replaced training database . Thus in the illustrated example the colloquial speaker vector j 1 is replaced by the reference speaker vector i 3. Similarly the colloquial speaker vector j 2 is replaced by the reference speaker vector i M the colloquial speaker vector j 3 is replaced by the reference speaker vector i the colloquial speaker vector j is replaced by the reference speaker vector i 1 and the colloquial speaker vector j Jis replaced by the reference speaker vector i 2. Note that the colloquial labels enriched transcriptions are not replaced. The replaced training database can thus be obtained from the colloquial training database by replacing the colloquial speaker vectors of the colloquial training database with the optimally matched reference speaker vectors.

A similar description of MUT and replacement applies to the colloquial training database K. Again by way of example the colloquial speaker vector j 1 is shown to be matched with the reference speaker vector i the colloquial speaker vector j 2 is shown to be matched with the reference speaker vector i 2 the colloquial speaker vector j 3 is shown to be matched with the reference speaker vector i M the colloquial speaker vector j is shown to be matched with the reference speaker vector i 3 and the colloquial speaker vector j Jis shown to be matched with the reference speaker vector i 1. Once more the optimal matching of the individual colloquial speaker vectors to the reference speaker vectors is carried out jointly over the ensemble of speaker vectors in both training database. However the optimal matching for the colloquial speaker vectors in the colloquial training database K is carried out separately from that for the colloquial speaker vectors in the colloquial training database . The particular matches represented in this example by the thick curved lines are once more arbitrary and for purposes of illustration only.

Again the black curved arrows indicate the replacement operation. In the illustrated example the reference speaker vectors identified as optimal matches to the colloquial speaker vectors in the colloquial training database K become the replacement speaker vectors in the replaced training database K. Thus in the illustrated example the colloquial speaker vector j 1 is replaced by the reference speaker vector i the colloquial speaker vector j 2 is replaced by the reference speaker vector i 2 the colloquial speaker vector j 3 is replaced by the reference speaker vector i M the colloquial speaker vector j is replaced by the reference speaker vector i 3 and the colloquial speaker vector j Jis replaced by the reference speaker vector i 1. Again the colloquial labels enriched transcriptions are not replaced. The replaced training database K can thus be obtained from the colloquial training database K by replacing the colloquial speaker vectors of the colloquial training database K with the optimally matched reference speaker vectors.

Training a TTS system with an aggregated conditioned training database is illustrated in which shows a HMM based TTS system . The functional components of the TTS system include a text analysis module a HMM module that includes HMM parameters a speech synthesizer module a HMM training module and an aggregated conditioned training database . These functional components could be implemented as machine language instructions in a centralized and or distributed fashion on one or more computing platforms or systems such as those described above. The machine language instructions could be stored in one or another form of a tangible non transitory computer readable medium or other article of manufacture such as magnetic or optical disk or the like and made available to processing elements of the system as part of a manufacturing procedure configuration procedure and or execution start up procedure for example.

In accordance with example embodiments the aggregated conditioned training database can be constructed as described above for the aggregated conditioned training database .

Two operational modes are represented in descriptively labeled Training Time and Run Time and separated by a thick horizontal line. Data processing flow is represented in dashed lines in the Training Time panel top and in solid lines in the Run Time panel bottom . During training time the training time labels can be retrieved from the aggregated conditioned training database and input to the HMM module which models training time predicted spectral parameters and training time predicted excitation parameters . The training time predicted spectral parameters and training time predicted excitation parameters are then input to the HMM training module as shown. Also during training time the expected spectral parameters and expected excitation parameters associated with the training time labels can be retrieved from the aggregated conditioned training database and provided as input to the HMM training module . By comparing the training time predicted spectral parameters and training time predicted excitation parameters with the expected spectral parameters and expected excitation parameters the HMM training module can determine how to adjust the HMM parameters so as to achieve closest or optimal agreement between the predicted results and the known results.

During run time operation a run time text string is input to the text analysis module which then generates run time labels . The run time labels are then input to the HMM module which generates run time predicted spectral parameters and run time predicted excitation parameters . In accordance with example embodiments the run time predicted spectral parameters and run time predicted excitation parameters can then input to the speech synthesizer module which can synthesize a run time speech signal . By training the TTS system with the aggregated conditioned training database synthesized speech can be made sound like the voice of the reference speaker even though the initial sources of the training speech samples were the multiple colloquial speakers.

Note that while illustrates two separate operational modes for a HMM based TTS system a similar configuration of two modes training time and run time can be achieved with a NN based TTS system such as the one illustrated in . Explicit description of such a configuration is omitted here for the sake of brevity.

The functional components of the TTS synthesis system include a text analysis module for converting input text into an enriched transcription and a TTS subsystem including a conditioned HMM for generating synthesized speech from the enriched transcription .

In accordance with example embodiments the text analysis module may receive an input text string or other form of text based input and generate an enriched transcription as output. The TTS subsystem may then employ the conditioned HMM to generate feature vectors corresponding to the enriched transcription .

In general terms the replacement of colloquial speaker vectors with reference speaker vectors described above is a form of voice conversion. More particularly voice conversion is concerned with converting the voice of a source speaker to the voice of a target speaker. For purposes of the discussion herein the target speaker is designated X and the source speaker is designated Y. These designations are intended for convenience of discussion and other designations could be used. In the context of speech modeling e.g. recognition and or synthesis feature analysis of speech samples of speaker X could generate a vector space of speech features designated X space. Similarly feature analysis of speech samples of speaker Y could generate a vector space of speech features designated Y space. For example feature vectors could correspond to parameterizations of spectral envelopes and or excitation as discussed above. In general X space and Y space may be different. For example they could have a different number of vectors and or different parameters. Further they could correspond to different languages be generated using different feature extraction techniques and so on.

Matching under transform may be considered a technique for matching the X space and Y space vectors under a transform that compensates for differences between speakers X and Y. It may be described in algorithmic terms as a computational method and can be implemented as machine readable instructions executable by the one or more processors of a computing system such as a TTS synthesis system. The machine language instructions could be stored in one or another form of a tangible non transitory computer readable medium or other article of manufacture such as magnetic or optical disk or the like and made available to processing elements of the system as part of a manufacturing procedure configuration procedure and or execution start up procedure for example.

The following discussion describes a mathematical formalism that can be used to convert the voice of the source speaker represented by Y space vectors to the voice of the target speaker represented by X space vectors. In the context of the colloquial speakers and the reference speaker described above each instance of a colloquial speaker can be taken to be the source speaker of the formalism and the reference speaker can be taken to be the target speaker of the formalism. With this correspondence the computations implied by the formalism can be viewed as being carried out separately for each colloquial speaker as an instance of source speaker. For purposes of discussion the formalism and without loss of generality the terminology of source target X space and Y space is adopted.

By way of example X space may be taken to include N vectors designated right arrow over x n 1 . . . N. Similarly Y space may be taken to include Q vectors designated right arrow over y q 1 . . . Q. As noted N and Q may not necessarily be equal although the possibility that they are is not precluded. In the context of speech modeling N and Q could correspond to a number of samples from speakers X and Y respectively.

In accordance with example embodiments matching under transform MUT uses a transformation function right arrow over y F right arrow over x to convert X space vectors to Y space vectors and applies a matching minimization MM operation within a deterministic annealing framework to match each Y space vector with one X space vector. The transformation function defines a parametric mapping from X space to Y space. At the same time a non parametric association mapping from Y space to X space may be defined in terms of conditional probabilities. Specifically for a given X space vector right arrow over x and a given Y space vector right arrow over y an association probability p right arrow over x right arrow over y may be used to specify a probability that right arrow over y maps to right arrow over x . In this way MUT involves bi directional mapping between X space and Y space parametric in a forward direction X Y via F and non parametric in the backward direction Y X via p right arrow over x right arrow over y .

A goal of MUT is to determine which X space vectors right arrow over x correspond to a Y space right arrow over y vector in the sense that F right arrow over x is close right arrow over y in L2 norm and under the circumstance that F right arrow over x and the probabilities p right arrow over x right arrow over y are not known ahead of time. Rather than searching for every possible mapping between X space and Y space vectors a distortion metric between right arrow over x and right arrow over y may be defined as 1 where Wis a weighting matrix depending on Y space vector right arrow over y . Then taking p right arrow over x right arrow over y to be the joint probability of matching vectors right arrow over y and right arrow over x an average distortion over all possible vector combinations may be expressed as . 2 In the MUT approach the bi directional mapping provides a balance between forward and backward mapping ensuring convergence to a meaningful solution.

In accordance with example embodiments minimizing the average distortion D simultaneously for F right arrow over x and p right arrow over x right arrow over y may be achieved using techniques of simulated annealing. Specifically an uncertainty in probabilistic matching between X space and Y space may be accounted for by an association entropy which can be expressed as H Y X H Y H X Y . Taking

Minimizing D with respect to the association probabilities yields the associations. In the general case of 0 the association probabilities may be expressed in the form of a Gibbs distribution and determined in what is referred to algorithmically herein as an association step. When approaches zero the mapping between Y space and X space becomes many to one many Y space vectors may be matched to one X space vector . It can be shown in this case 0 that the association probabilities may be determined from a search for the nearest X space vector in terms of the distortion metric d right arrow over y right arrow over x in what is referred to algorithmically herein as a matching step. 

Given the associations determined either by an association step or a matching step the transform function can be defined and its optimal parameters determined by solving a minimization of D with respect to the defined form of F . This determination of F right arrow over x is referred to algorithmically herein as a minimization step. 

The purpose of the transform is to compensate for speaker differences between in this example speakers X and Y. More specifically cross speaker variability can be captured by a linear transform of the form right arrow over right arrow over x where right arrow over is a bias vector and is linear transformation matrix of the k th class. The linear transform matrix can compensate for differences in the vocal tract that are related to vocal tract shape and size. Accordingly F right arrow over x may be defined as a mixture of linear regressions function defined as right arrow over 4 where p k right arrow over x is the probability that right arrow over x belongs to the k th class.

Assuming a class of probabilities p k right arrow over x corresponding to a Gaussian mixture model GMM and reformulating right arrow over x using the vector operator vec and the Kronecker delta product to define right arrow over vec it can be shown that F right arrow over x may be expressed as 

Based on the discussion above two algorithms may be used to obtain matching under transform. The first is referred to herein as association minimization and the second is referred to herein as matching minimization. In accordance with example embodiments association minimization may be implemented with the following steps 

Initialization sets a starting point for MUT optimization and may differ depending on the speech features used. For conversion of mel cepstral coefficient MCEP parameters a search for a good vocal tract length normalization transform with a single linear frequency warping factor may suffice. Empirical evidence suggests that an adequate initialization transform is one that minimizes the distortion in an interval 0.7 1.3 of frequency warping factor. The association step uses the Gibbs distribution function for the association probabilities as described above. The minimization step then incorporates the transformation function. Steps 5 and 6 iterate for convergence and cooling.

In further accordance with example embodiments matching minimization may be implemented with the following steps 

Initialization is the same as that for association minimization starting with a transform that minimizes the distortion in an interval of values of 0.7 1.3 in frequency warping factor. The matching step uses association probabilities determined from a search for the nearest X space vector as described above. The minimization step then incorporates the transformation function. Step 5 iterates for convergence. Note that there is no cooling step since matching minimization assumes 0.

While MUT as described is used to replace each source vector of the Y space e.g. each colloquial speaker vector of a given colloquial training database with an optimally matched target vector of the X space e.g. a reference speaker vector of the reference training database in practice the matching can be performed by considering vectors in contexts of temporally earlier and later vectors. For example a context right arrow over y right arrow over y right arrow over y right arrow over y right arrow over y can be matched against a context right arrow over x right arrow over x right arrow over x right arrow over x right arrow over x to obtain the best match of right arrow over x to right arrow over y . Matching in context in this way can help further improve the accuracy of the matching.

In accordance with example embodiments applying MUT to replacement of each colloquial speaker vector of a given colloquial training database with a reference speaker vector of the reference training database can be described as entailing the following algorithmic steps 

An illustrative embodiment has been described by way of example herein. Those skilled in the art will understand however that changes and modifications may be made to this embodiment without departing from the true scope and spirit of the elements products and methods to which the embodiment is directed which is defined by the claims.

