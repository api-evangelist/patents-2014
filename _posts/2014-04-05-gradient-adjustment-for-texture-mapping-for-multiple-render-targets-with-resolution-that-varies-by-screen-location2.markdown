---

title: Gradient adjustment for texture mapping for multiple render targets with resolution that varies by screen location
abstract: In a computer graphics processing unit (GPU) having a shader and a texture unit the pixel shader is configured to receive or generate one or more sets of texture coordinates per pixel sample location. The pixel shader and texture unit between them are configured to calculate texture space gradient values for one or more primitives and generate and apply per-pixel gradient scale factors configured to modify the gradient values to smoothly transition them between regions of a display device having different pixel resolutions.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09652882&OS=09652882&RS=09652882
owner: SONY INTERACTIVE ENTERTAINMENT AMERICA LLC
number: 09652882
owner_city: San Mateo
owner_country: US
publication_date: 20140405
---
This application is related to commonly assigned co pending U.S. patent application Ser. No. 14 246 064 to Tobias Berghoff entitled METHOD FOR EFFICIENT CONSTRUCTION OF HIGH RESOLUTION DISPLAY BUFFERS filed the same day as the present application the entire contents of which are herein incorporated by reference.

This application is related to commonly assigned co pending U.S. patent application Ser. No. 14 246 067 to Tobias Berghoff entitled GRAPHICS PROCESSING ENHANCEMENT BY TRACKING OBJECT AND OR PRIMITIVE IDENTIFIERS filed the same day as the present application the entire contents of which are herein incorporated by reference.

This application is related to commonly assigned co pending U.S. patent application Ser. No. 14 246 068 to Mark Evan Cerny entitled GRADIENT ADJUSTMENT FOR TEXTURE MAPPING TO NON ORTHONORMAL GRID filed the same day as the present application the entire contents of which are herein incorporated by reference.

This application is related to commonly assigned co pending U.S. patent application Ser. No. 14 246 061 to Tobias Berghoff entitled VARYING EFFECTIVE RESOLUTION BY SCREEN LOCATION BY CHANGING ACTIVE COLOR SAMPLE COUNT WITHIN MULTIPLE RENDER TARGETS filed the same day as the present application the entire contents of which are herein incorporated by reference.

This application is related to commonly assigned co pending U.S. patent application Ser. No. 14 246 063 to Mark Evan Cerny entitled VARYING EFFECTIVE RESOLUTION BY SCREEN LOCATION BY ALTERING RASTERIZATION PARAMETERS filed the same day as the present application the entire contents of which are herein incorporated by reference.

This application is related to commonly assigned co pending U.S. patent application Ser. No. 14 246 066 to Mark Evan Cerny entitled VARYING EFFECTIVE RESOLUTION BY SCREEN LOCATION IN GRAPHICS PROCESSING BY APPROXIMATING PROJECTION OF VERTICES ONTO CURVED VIEWPORT filed the same day as the present application the entire contents of which are herein incorporated by reference.

Aspects of the present disclosure are related to computer graphics. In particular the present disclosure is related to adjustment of gradients used for texture mapping.

Graphics processing typically involves coordination of two processors a central processing unit CPU and a graphics processing unit GPU . The GPU is a specialized electronic circuit designed to accelerate the creation of images in a frame buffer intended for output to a display. GPUs are used in embedded systems mobile phones personal computers tablet computers portable game devices workstations and game consoles. A GPU is typically designed to be efficient at manipulating computer graphics. GPU s often have a highly parallel processing architecture that makes the GPU more effective than a general purpose CPU for algorithms where processing of large blocks of data is done in parallel.

The CPU may send the GPU instructions commonly referred to as draw commands that instruct the GPU to implement a particular graphics processing task e.g. render a particular texture that has changed with respect to a previous frame in an image. These draw commands may be coordinated by the CPU with a graphics application programming interface API in order to issue graphics rendering commands that correspond to the state of the particular application s virtual environment.

In order to render textures for a particular program a GPU may perform a series of processing tasks in a graphics pipeline to translate the visuals in the virtual environment into images that can be rendered onto a display. A typical graphics pipeline may include performing certain rendering or shading operations on virtual objects in the virtual space transformation and rasterization of the virtual objects in the scene to produce pixel data suitable for output display and additional rendering tasks on the pixels or fragments before outputting the rendered image on a display.

Virtual objects of an image are often described in virtual space in terms of shapes known as primitives which together make the shapes of the objects in the virtual scene. For example objects in a three dimensional virtual world to be rendered may be reduced to a series of distinct triangle primitives having vertices defined in terms of their coordinates in three dimensional space whereby these polygons make up the surfaces of the objects. Each polygon may have an associated index that can be used by the graphics processing system to distinguish a given polygon from other polygons. Likewise each vertex may have an associated index that can be used to distinguish a given vertex from other vertices. A graphics pipeline may perform certain operations on these primitives to produce visuals for the virtual scene and transform this data into a two dimensional format suitable for reproduction by the pixels of the display. The term graphics primitive information or simply primitive information as used herein is used to refer to data representative of a graphics primitive. Such data includes but is not limited to vertex information e.g. data representing vertex positions or vertex indices and polygon information e.g. polygon indices and information that associates particular vertices with particular polygons.

A GPU may perform rendering tasks of the graphics pipeline by implementing programs commonly known as shaders. A typical graphics pipeline may include vertex shaders which may manipulate certain properties of the primitives on a per vertex basis as well as pixel shaders also known as fragment shaders which operate downstream from the vertex shaders in the graphics pipeline and may manipulate certain values on a per pixel basis before transmitting the pixel data to a display. The fragment shaders may manipulate values relevant to applying textures to primitives. The pipeline may also include other shaders at various stages in the pipeline such as geometry shaders that use the output of the vertex shaders to generate a new set of primitives as well as compute shaders CS which may be implemented by a GPU to perform certain other general computational tasks.

Part of the process of mapping textures to primitives involves calculating gradients in texture space from pixel locations in screen space. The gradient calculation often assumes that the pixel locations are based on a square orthonormal grid.

Although the following detailed description contains many specific details for the purposes of illustration anyone of ordinary skill in the art will appreciate that many variations and alterations to the following details are within the scope of the invention. Accordingly the exemplary embodiments of the invention described below are set forth without any loss of generality to and without imposing limitations upon the claimed invention.

In certain graphics applications bitmapped textures are painted onto the polygon. In such a case each pixel value drawn by the output device is determined from one or more pixels sampled from the texture. As used herein a bitmap generally refers to a data file or structure representing a generally rectangular grid of pixels or points of color on a computer monitor paper or other display device. The color of each pixel is individually defined. For example a colored pixel may be defined by three bytes one byte each for red green and blue. A bitmap typically corresponds bit for bit with data formats supported by device texture sampling units which may typically include a range of options including various bit depths per channel or block compression probably in the same format as it would be stored in the display s video memory or maybe as a device independent bitmap. A bitmap is characterized by the width and height of the image in pixels and the number of bits per pixel which determines the number of colors it can represent.

The process of transferring a texture bitmap to a surface often involves the use of texture MIP maps also known as mipmaps . The letters MIP in the name are an acronym of the Latin phrase multum in parvo meaning much in a small space . Such mipmaps are pre calculated optimized collections of bitmap images that accompany a main texture intended to increase rendering speed and reduce aliasing artifacts.

Each bitmap image of the mipmap set is a version of the main texture but at a certain reduced level of detail LOD . Although the main texture would still be used when the view is sufficient to render it in full detail the graphics hardware rendering the final image switches to a suitable mipmap level or interpolates between the two nearest levels when the texture is viewed from a distance or at a small size. Rendering speed increases since the number of texture pixels texels being processed can be much lower and their distribution in memory more coherent than with simple textures. Artifacts may be reduced since the mipmap images are effectively already anti aliased taking some of the burden off the real time rendering hardware.

The blending between mipmap levels typically involves some form of texture filtering. As used herein texture filtering refers to a method used to map texels pixels of a texture to points on a 3D object. A simple texture filtering algorithm may take a point on an object and look up the closest texel to that position. The resulting point then gets its color from that one texel. This simple technique is sometimes referred to as nearest neighbor filtering. More sophisticated techniques combine more than one texel per point. The most often used algorithms in practice are bilinear filtering and trilinear filtering using mipmaps. Anisotropic filtering and higher degree methods such as quadratic or cubic filtering result in even higher quality images.

Textures are typically square and have side lengths equal to a power of 2. If e.g. a texture has a basic size of 256 by 256 pixels then the associated mipmap set may contain a series of 8 images each half the size of the previous one 128 128 pixels 64 64 32 32 16 16 8 8 4 4 2 2 and 1 1 a single pixel . If for example this texture is mapped onto a 40 40 pixel portion of a screen space then an interpolation of the 64 64 and the 32 32 mipmaps would be used. As used herein the term screen space refers generally to the set of coordinates used by the display buffer in the graphics pipeline.

The key operation in the process of determining the appropriate mipmap level involves determining the area covered in texture coordinate space sometimes referred to as UV coordinate space for a corresponding area of pixel locations from screen space sometimes referred to as XY coordinate space . In general terms screen space gradients of interpolated or computed texture UV coordinates are calculated from U and V values sampled at XY space pixel locations in the relevant portion of the scene. In some implementations a texture coordinate gradient is determined for each screen space direction X and Y by calculating the change in texture coordinates occurring when the screen X coordinate changes and the screen Y is fixed sometimes referred to as du dy dv dx and the change in texture coordinates occurring when the screen Y coordinate changes and the screen X is fixed sometimes referred to as du dy dv dy . This texture coordinate gradient calculation may optionally include corrections for non orthonormality of the sample grid. For non anisotropic texture lookups the gradient with the larger magnitude among these two is used to select level of detail LOD . For anistropic texturing the smaller magnitude gradient is used to select the LOD and the texture is sampled in a line corresponding to the larger magnitude gradient.

Also note that the above calculation can be generalized to 1 2 3 or more texture coordinate dimensions. Typical hardware calculates a 1D gradient in U space or a 2D gradient in UV space or a 3D gradient in UVW space depending on the texture dimensionality. Thus aspects of the present disclosure are not limited to implementations involving two texture coordinate dimensions.

The process of determining the appropriate MIP level of detail is however based on an assumption that the relevant portion of the virtual space onto which the texture to be applied is a regular arrangement of samples i.e. the sample points within screen pixels are evenly spaced across the entirety of screen space in the vertical and horizontal directions. However visual artifacts can arise from discontinuities in the sample pattern for instance at a boundary between screen areas having different sample spacing in screen space. At such boundaries locally correct texture filtering on either side of the boundary may access significantly different MIP levels of detail producing a noticeable change in the appearance of the output image across the boundary. In such situations the gradients must be adjusted on each side of the boundary in order to produce smoothly varying texture filtering and thereby reduce the visibility of the boundary.

Aspects of the present disclosure include graphics processing systems that are configured to implement gradient adjustment in texture mapping. By way of example and not by way of limitation illustrates a block diagram of a computer system that may be used to implement graphics processing according to aspects of the present disclosure. According to aspects of the present disclosure the system may be an embedded system mobile phone personal computer tablet computer portable game device workstation game console and the like.

The system generally may include a central processor unit CPU a graphics processor unit GPU and a memory that is accessible to both the CPU and GPU. The CPU and GPU may each include one or more processor cores e.g. a single core two cores four cores eight cores or more. The memory may be in the form of an integrated circuit that provides addressable memory e.g. RAM DRAM and the like. The memory may include graphics memory that may store graphics resources and temporarily store graphics buffers of data for a graphics rendering pipeline. The graphics buffers may include e.g. vertex buffers for storing vertex parameter values index buffers for holding vertex indices depth buffers e.g. Z buffers for storing depth values of graphics content stencil buffers frame buffers for storing completed frames to be sent to a display and other buffers. In the example shown in the graphics memory is shown as part of the main memory. In alternative implementations the graphics memory could be a separate component possibly integrated into the GPU .

By way of example and not by way of limitation the CPU and GPU may access the memory using a data bus . In some cases it may be useful for the system to include two or more different buses. The memory may contain data that can be accessed by the CPU and GPU . The GPU may include a plurality of compute units configured to perform graphics processing tasks in parallel. Each compute unit may include its own dedicated local memory store such as a local data share.

The CPU may be configured to execute CPU code which may include an application that utilizes graphics a compiler and a graphics API. The graphics API can be configured to issue draw commands to programs implemented by the GPU. The CPU code may also implement physics simulations and other functions. The GPU may be configured to operate as discussed above. In particular the GPU may execute GPU code which may implement shaders such as compute shaders CS vertex shaders VS and pixel shaders PS as discussed above. To facilitate passing of data between the compute shaders CS and the vertex shaders VS the system may include one or more buffers which may include a frame buffer FB. The GPU code may also optionally implement other types of shaders not shown such as pixel shaders or geometry shaders. Each compute unit may include its own dedicated local memory store such as a local data share. The GPU may include one or more texture units configured to perform certain operations for applying textures to primitives as part of a graphics pipeline.

According to aspects of the present disclosure a pixel shader PS and a texture unit are configured to generate one or more texture coordinates UV and potentially also texture space gradient values for each coordinate Gr for one or more corresponding pixel sample locations. These gradient values may potentially be corrected for non orthonormality of the sample grid. While these gradient values Gr provide locally correct texture filtering in some configurations there may be abrupt transitions between screen areas having different pixel sample configurations which may become visible in the output image as an abrupt change in texture filtering appearance. According to aspects of the present disclosure a pixel shader PS may calculate and supply the texture unit with screen space gradient scale factors Sc which the texture unit may apply as a linear screen axis aligned scale transformation to the gradients Gr to obtain adjusted gradient values Gr . These gradient scale factors Sc may be used to modify the gradient values Gr progressively leading up to a screen area boundary in order to smoothly transition them between regions of the display device having different pixel resolutions.

By way of example and not by way of limitation the texture units may be implemented as special purpose hardware such as an application specific integrated circuit ASIC Field Programmable Gate Array FPGA or a system on chip SoC or SOC .

As used herein and as is generally understood by those skilled in the art an application specific integrated circuit ASIC is an integrated circuit customized for a particular use rather than intended for general purpose use.

As used herein and as is generally understood by those skilled in the art a Field Programmable Gate Array FPGA is an integrated circuit designed to be configured by a customer or a designer after manufacturing hence field programmable . The FPGA configuration is generally specified using a hardware description language HDL similar to that used for an ASIC.

As used herein and as is generally understood by those skilled in the art a system on a chip or system on chip SoC or SOC is an integrated circuit IC that integrates all components of a computer or other electronic system into a single chip. It may contain digital analog mixed signal and often radio frequency functions all on a single chip substrate. A typical application is in the area of embedded systems.

These components are connected by either a proprietary or industry standard bus. Direct Memory Access DMA controllers route data directly between external interfaces and memory bypassing the processor core and thereby increasing the data throughput of the SoC.

A typical SoC includes both the hardware components described above and executable instructions e.g. software or firmware that controls the processor core s peripherals and interfaces.

According to aspects of the present disclosure some or all of the functions of the texture units may alternatively be implemented by appropriately configured software instructions executed by a software programmable general purpose computer processor. Such instructions may be embodied in a computer readable medium e.g. memory or storage device .

The system may also include well known support functions which may communicate with other components of the system e.g. via the bus . Such support functions may include but are not limited to input output I O elements power supplies P S a clock CLK and cache . In addition to the cache the GPU may include its own GPU cache and the GPU may be configured so that programs running on the GPU can read through or write though the GPU cache .

The system may optionally include a mass storage device such as a disk drive CD ROM drive flash memory tape drive or the like to store programs and or data. The system may also optionally include a display device to present rendered graphics to a user and user interface unit to facilitate interaction between the system and a user. The display device may be in the form of a flat panel display head mounted display HMD cathode ray tube CRT screen projector or other device that can display visible text numerals graphical symbols or images. The display may display rendered graphic images processed in accordance with various techniques described herein. The user interface may include a keyboard mouse joystick light pen game controller or other device that may be used in conjunction with a graphical user interface GUI . The system may also include a network interface to enable the device to communicate with other devices over a network . The network may be e.g. a local area network LAN a wide area network such as the internet a personal area network such as a Bluetooth network or other type of network. These components may be implemented in hardware software or firmware or some combination of two or more of these.

According to aspects of the present disclosure the system is configured to implement portions of a graphics rendering pipeline. illustrates an example of a graphics rendering pipeline in accordance with aspects of the present disclosure.

The rendering pipeline may be configured to render graphics as images that depict a scene having a two dimensional or preferably three dimensional geometry in virtual space sometime referred to herein as world space . The early stages of the pipeline may include operations performed in virtual space before the scene is rasterized and converted to screen space as a set of discrete picture elements suitable for output on the display device . Throughout the pipeline various resources contained in the graphics memory may be utilized at the pipeline stages and inputs and outputs to the stages may be temporarily stored in buffers contained in the graphics memory before the final values of the images are determined.

The rendering pipeline may operate on input data which may include one or more virtual objects defined by a set of vertices that are set up in virtual space and have geometry that is defined with respect to coordinates in the scene. The early stages of the pipeline may include what is broadly categorized as a vertex processing stage in and this may include various computations to process the vertices of the objects in virtual space. This may include vertex shading computations which may manipulate various parameter values of the vertices in the scene such as position values e.g. X Y coordinate and Z depth values color values lighting values texture coordinates and the like. Preferably the vertex shading computations are performed by one or more programmable vertex shaders. The vertex processing stage may optionally include additional vertex processing computations such as tessellation and geometry shader computations which may be optionally used to generate new vertices and new geometries in virtual space. Once the stage referred to as vertex processing is complete at this stage in the pipeline the scene is defined by a set of vertices which each have a set of vertex parameter values .

The pipeline may then proceed to rasterization processing stages associated with converting the scene geometry into screen space and a set of discrete picture elements i.e. pixels. The virtual space geometry may be transformed to screen space geometry through operations that may essentially compute the projection of the objects and vertices from virtual space to the viewing window or viewport of the scene. The vertices may define a set of primitives.

The rasterization processing stage depicted in may include primitive assembly operations which may set up the primitives defined by each set of vertices in the scene. Each vertex may be defined by an index and each primitive may be defined with respect to these vertex indices which may be stored in index buffers in the graphics memory . The primitives may preferably include at least triangles defined by three vertices each but may also include point primitives line primitives and other polygonal shapes. During the primitive assembly stage certain primitives may optionally be culled. For example those primitives whose indices indicate a certain winding order may be considered to be back facing and may be culled from the scene.

By way of example and not by way of limitation where the primitives are in the form of triangles defined by vertices in three dimensional virtual space the primitive assembly determines where on the screen of the display each triangle is located. Clipping and screen space transformation operations are typically performed by the primitive assembly unit .

After primitives are assembled the rasterization processing stages may include scan conversion operations which may sample the primitives at each pixel and generate fragments sometimes referred to as pixels from the primitives for further processing when the samples are covered by the primitive. The scan conversion operations include operations that take a primitive that has been converted to screen space coordinates and determines which pixels are part of that primitive. In some implementations multiple samples for each pixel are taken within the primitives during the scan conversion operations which may be used for anti aliasing purposes. In certain implementations different pixels may be sampled differently. For example some edge pixels may contain a lower sampling density than center pixels to optimize certain aspects of the rendering for certain types of display device such as head mounted displays HMDs . The fragments or pixels generated from the primitives during scan conversion 144 may have parameter values that may be interpolated to the locations of the pixels from the vertex parameter values of the vertices of the primitive that created them. The rasterization stage may include parameter interpolation operations stage to compute these interpolated fragment parameter values which may be used as inputs for further processing at the later stages of the pipeline.

The pipeline may include further pixel processing operations indicated generally at in to further manipulate the interpolated parameter values and perform further operations determining how the fragments contribute to the final pixel values for display. Some of these pixel processing tasks may include pixel shading computations that may be used to further manipulate the interpolated parameter values of the fragments. The pixel shading computations may be performed by a programmable pixel shader and pixel shader invocations may be initiated based on the sampling of the primitives during the rasterization processing stages . The pixel shading computations may output values to one or more buffers in graphics memory sometimes referred to as render targets or if multiple as multiple render targets MRTs .

MRTs allow pixel shaders to optionally output to more than one render target each with the same screen dimensions but potentially with a different pixel format. Render target format limitations often mean that any one render target can only accept up to four independent output values channels and that the formats of those four channels are tightly tied to each other. MRTs allow a single pixel shader to output many more values in a mix of different formats. The formats of render targets are texture like in that they store values per screen space pixel but for various performance reasons render target formats are becoming more specialized in recent hardware generations sometimes but not always requiring what is called a resolve to reformat the data before it is compatible with being read in by the texture units .

The pixel processing may generally culminate in render output operations which may include what are commonly known as raster operations ROP . Rasterization Operations ROP is simply run multiple times per pixel once for each render target among the multiple render targets MRTs . During the output operations the final pixel values may be determined in a frame buffer which may optionally include merging fragments applying stencils depth tests and certain per sample processing tasks. The final pixel values include the collected output to all active render targets MRTs . The GPU uses the final pixel values to make up a finished frame which may optionally be displayed on the pixels of the display device in real time.

The output operations may also include texture mapping operations which may be performed to some extent by one or more pixel shaders PS and to some extent by the texture units . The pixel shader computations include calculating texture coordinates UV from screen space coordinates XY and sending the texture coordinates to the Texture Operations and receiving texture data TX. The texture coordinates UV could be calculated from the screen space coordinates XY in an arbitrary fashion but typically are calculated from interpolated input values or sometimes from the results of previous texture operations. Gradients Gr are often directly calculated from quads of texture coordinates by the texture units Texture Operations hardware units but can optionally be calculated explicitly by the pixel shader computations and passed to the texture operations rather than relying on the texture units to perform the default calculation.

The texture operations generally include the following stages which can be performed by some combination of a pixel shader PS and a texture unit . First one or more texture coordinates UV per pixel location XY are generated and used to provide a coordinate set for each texture mapping operation. Then texture space gradient values Gr for pixel sample locations are generated potentially including corrections for non orthonormality of the sample grid. Finally the gradients Gr are modified by per pixel screen space gradient scale factors Sc supplied by the pixel shader PS in order to produce the final texture space gradients Gr used for texture filtering operations. The gradient scale factors Sc may be chosen to produce gradient values Gr which smoothly transition between regions of the display device having different pixel resolutions or sample distributions.

In some implementations the pixel shader PS can generate the texture coordinates UV and gradient scale factors Sc per pixel location XY and provide a coordinate set for each texture mapping operation to the texture unit which may generate texture space gradient values Gr and modify them to produce corrected texture space gradient values Gr .

In other implementations the pixel shader PS could calculate the texture space coordinates UV explicit differences Gr from the pixel locations XY and gradient scale factors Sc and pass all of these values to the texture unit and indicate to the texture unit that it must still perform any corrections for non orthonormality normally then apply the gradient scale factors Sc to get the adjusted gradient values Gr .

In other alternative implementations the pixel shader PS could calculate the texture space coordinates UV and explicit corrected gradients Gr and pass those to the texture unit indicating to the texture unit that any required corrections have already been applied in software and that the corrected gradients Gr should be used as is to select the LOD.

Aspects of the present disclosure are directed to adjustment of gradients Gr used by the texture unit to determine the mipmap level LOD for a texture to be applied to a primitive in the graphics pipeline. The basic concept is illustrated in . depicts the texture UV coordinates of four pixel samples in two orthogonal quads E and E . In quad E all samples points in XY space lie on an orthonormal grid with spacing of one screen pixel horizontally and vertically.

In quad E texture gradients Gr in UV space are trivially computed from the texture coordinates UV. The texture gradients Gr may be expressed mathematically in terms of the texture coordinates UV of the upper left upper right and lower left pixels respectively u0 v0 u1 v1 and u2 v2 as follows  1 0  1 0  2 0  2 0

These calculations of du dx u1 u0 etc. can be performed by the texture unit hardware if the pixel shader PS does not choose to override them with software calculated values. The texture gradients Gr are then used by the texture unit to determine the mipmap LOD level to sample.

According to aspects of the present disclosure the gradients Gr du dy dv dx du dy dv dy are adjusted on a per pixel basis to account for discontinuities in pixel resolution across a display. For example in certain display configurations e.g. head mounted display HMD applications it is advantageous to use different resolutions for different parts of the display. In such cases it is very advantageous to scale the gradients on a per pixel basis to smooth out the change in texture filtering at or near boundaries between adjacent regions of different resolution. The basic concept is illustrated in .

As shown in and the display area may be divided into two or more regions and of different pixel resolution. Each region may have a resolution that is related to the solid angle subtended by the region of the display e.g. in the case of a head mounted display HMD . By way of example and not by way of limitation the central region may have a nominal or standard pixel resolution R. Edge regions could have half the standard resolution AR e.g. half the pixels in these regions would be turned off or not rendered on the display. Corner regions and could have on quarter the standard resolution R e.g. three quarters of the pixels in these regions would be turned off or not rendered on the display. shows these different regions drawn to different sizes according to the resolution of each region.

Gradients might need to be adjusted for pixels near the boundaries between adjacent regions. For example in the lines marking the boundaries between the sections have been removed. However discontinuities in texture filtering between neighboring regions make the boundaries visible. The GPU can be configured to adjust gradients for pixels near the boundaries between regions to smooth out the discontinuities making them less visible. For example as shown in horizontal gradients for selected pixels in the full resolution central region in the row D D in can be scaled so as to produce progressively blurrier texture filtering towards the half resolution edge regions such that two pixels immediately adjacent across the border would access approximately the same MIP LOD level.

Alternatively gradients in the half resolution edge regions may also be scaled so as to become progressively sharper towards the full resolution central region .

Generalized adjustment of the gradients to account for differing resolution can be understood with reference to which depicts in UV space the texture coordinates for two adjacent quads of pixels E and E across a boundary in horizontal screen area also indicated by the correspondingly labeled rectangles in screen space in . In the halving of the horizontal density of samples between E and E produces du dx dv dx for E that have approximately twice the length of the du dx dv dx calculated for E. This may in turn result in selection of a lower level of detail for quad E than for E. It should be noted that these gradient calculations for quad E and quad E are locally correct in that each would correctly select a mipmap LOD that would cover each screen pixel with approximately one texture pixel texel but that this abrupt change in texture filtering may produce a visible change in the appearance of the output image across the boundary between quads with sample patterns like E and those with patterns like E. The visibility of this boundary may be reduced by choosing to adjust gradient values in pixels near to the boundary in order to produce a smoothly varying selection of mipmap LOD.

Gradients in UV space du dx dv dx du dy dv dy are calculated normally for each quad which might in some other cases include corrections for non orthonormality of the local sample distribution. The gradients can then be multiplied by per pixel gradient scale factors scaleX scaleY on a per pixel basis to produce the final adjusted gradients du dx dv dx du dy dv dy that the texture unit uses to determine the appropriate mipmap level    scale    scale    scale    scale

The gradient scale factors scaleX and scaleY would be calculated by the pixel shader PS and passed to the texture unit which would use them to calculate the final adjusted gradients.

It should be understood that the above gradient scale correction is applied identically to each texture coordinate U and V and so can be extended trivially to 1 dimensional coordinates U 2 dimensional coordinates UV or 3 dimensional coordinates UVW .

Suitable values for gradient scale factors scaleX scaleY can be determined empirically by iterating the gradient scale factor values applied to selected pixels proximate the boundaries of neighboring regions of different pixel resolution. A range of suitable values for the gradient scale factors can be determined from the relative pixel resolutions of the neighboring regions. For example along row D D the pixel resolution changes from R in region to R in region and again to R in region . The gradient scale value scaleX would transition from roughly 1 to roughly 2 over several pixels in region proximate the boundaries with regions and . In it can be seen that applying a gradient scaleX factor of 2 to quad E from region would result in du dx dv dx which roughly match the gradients calculated for adjacent quad E from region thus creating continuity in the scale corrected gradients and texture filtering LOD selection. Similarly gradient scale values scaleX scaleY would transition from a value of roughly 1 to a value of roughly 2 over several pixels in region proximate the boundaries with regions and . The gradient scale factors may be varied between the two values over several pixels e.g. roughly 4 to 8 pixels.

The texture unit may use the final adjusted gradients to select the appropriate LOD to apply to one or more primitives from the final adjusted gradients.

An additional aspect of the present disclosure include a graphics processing method comprising receiving or generating one or more sets of texture coordinates per pixel sample location calculating texture space gradient values for one or more primitives and generating and applying per pixel gradient scale factors configured to modify the gradient values to smoothly transition them between regions of a display device having different pixel resolutions.

Another additional aspect is a graphics processing system configured to implement the forgoing method.

Yet another additional aspect is a computer readable medium having computer executable instructions embodied therein that when executed implement the foregoing method.

A further aspect is an electromagnetic or other signal carrying computer readable instructions for performing the foregoing method.

Another further aspect is a computer program product downloadable from a communication network and or stored on a computer readable and or microprocessor executable medium characterized in that it comprises program code instructions for implementing the foregoing method.

While the above is a complete description of the preferred embodiment of the present invention it is possible to use various alternatives modifications and equivalents. Therefore the scope of the present invention should be determined not with reference to the above description but should instead be determined with reference to the appended claims along with their full scope of equivalents. Any feature described herein whether preferred or not may be combined with any other feature described herein whether preferred or not. In the claims that follow the indefinite article A or An refers to a quantity of one or more of the item following the article except where expressly stated otherwise. The appended claims are not to be interpreted as including means plus function limitations unless such a limitation is explicitly recited in a given claim using the phrase means for. 

