---

title: Data replication system
abstract: Systems and methods are provided for an asynchronous data replication system in which the remote replication reduces bandwidth requirements by copying deduplicated differences in business data from a local storage site to a remote, backup storage site, the system comprising: a local performance storage pool for storing data; a local deduplicating storage pool for storing deduplicated data, said local deduplicating storage pool further storing metadata about data objects in the system and which has metadata analysis logic for identifying and specifying differences in a data object over time; a remote performance storage pool for storing a copy of said data, available for immediate use as a backup copy of said data to provide business continuity to said data; a remote deduplicating storage pool for storing deduplicated data; and a controller for synchronizing the remote performance storage pool to have the second version of the data object using deduplicated data.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09251198&OS=09251198&RS=09251198
owner: ACTIFIO, INC.
number: 09251198
owner_city: Waltham
owner_country: US
publication_date: 20140919
---
This Application is a Continuation of U.S. application Ser. No. 13 564 502 filed on Aug. 1 2012 the disclosure of which is herein incorporated by reference in its entirety. U.S. application Ser. No. 13 564 502 claims priority under 35 U.S.C. 119 e to U.S. Provisional Application 61 513 747 filed on Aug. 1 2011 the disclosure of which is herein incorporated by reference in its entirety.

This invention relates generally to data management data protection disaster recovery and business continuity. More specifically this invention relates to a system and method for providing replication and business continuity using a data virtualization system providing data deduplication.

The business requirements for managing the lifecycle of application data have been traditionally met by deploying multiple point solutions each of which addresses a part of the lifecycle. This has resulted in a complex and expensive infrastructure where multiple copies of data are created and moved multiple times to individual storage repositories. The adoption of server virtualization has become a catalyst for simple agile and low cost compute infrastructure. This has led to larger deployments of virtual hosts and storage further exacerbating the gap between the emerging compute models and the current data management implementations.

Applications that provide business services depend on storage of their data at various stages of its lifecycle. shows a typical set of data management operations that would be applied to the data of an application such as a database underlying a business service such as payroll management. In order to provide a business service application requires primary data storage with some contracted level of reliability and availability.

Backups are made to guard against corruption or the primary data storage through hardware or software failure or human error. Typically backups may be made daily or weekly to local disk or tape and moved less frequently weekly or monthly to a remote physically secure location .

Concurrent development and test of new applications based on the same database requires a development team to have access to another copy of the data . Such a snapshot might be made weekly depending on development schedules.

Compliance with legal or voluntary policies may require that some data be retained for safely future access for some number of years usually data is copied regularly say monthly to a long term archiving system .

Disaster Recovery services guard against catastrophic loss of data if systems providing primary business services fail due to some physical disaster. Primary data is copied to a physically distinct location as frequently as is feasible given other constraints such as cost . In the event of a disaster the primary site can be reconstructed and data moved back from the safe copy.

Business Continuity services provide a facility for ensuring continued business services should the primary site become compromised. Usually this requires a hot copy of the primary data that is in near lockstep with the primary data as well as duplicate systems and applications and mechanisms for switching incoming requests to the Business Continuity servers.

Thus data management is currently a collection of point applications managing the different parts of the lifecycle. This has been an artifact of evolution of data management solutions over the last two decades.

Current Data Management architecture and implementations such as described above involve multiple applications addressing different parts of data lifecycle management all of them performing certain common functions a make a copy of application data the frequency of this action is commonly termed the Recovery Point Objective RPO b store the copy of data in an exclusive storage repository typically in a proprietary format and c retain the copy for certain duration measured as Retention Time. A primary difference in each of the point solutions is in the frequency of the RPO the Retention Time and the characteristics of the individual storage repositories used including capacity cost and geographic location.

In a series of prior patent applications e.g. U.S. Ser. No. 12 947 375 a system and method for managing data has been presented that uses Data Management Virtualization. Data Management activities such as Backup Replication and Archiving are virtualized in that they do not have to be configured and run individually and separately. Instead the user defines their business requirement with regard to the lifecycle of the data and the Data Management Virtualization System performs these operations automatically. A snapshot is taken from primary storage to secondary storage this snapshot is then used for a backup operation to other secondary storage. Essentially an arbitrary number of these backups may be made providing a level of data protection specified by a Service Level Agreement.

The present application provides enhancements to the above system for data management virtualization.

Systems and methods are provided for an asynchronous data replication system for providing a remote copy of data. In one embodiment remote replication is provided with reduced bandwidth requirements by copying deduplicated differences in business data from a local storage site to a remote backup storage site the system comprising a local performance storage pool for storing data a local deduplicating storage pool for storing deduplicated data said local deduplicating storage pool further storing metadata about data objects in the system and which has metadata analysis logic for identifying and specifying differences in a data object over time a remote performance storage pool for storing a copy of said data available for immediate use as a backup copy of said data to provide business continuity to said data a remote deduplicating storage pool for storing deduplicated data said remote deduplicating storage pool being in communication with said local deduplicating storage pool and with said remote performance storage pool and a controller for in response to a remote replication command causing the local deduplicating storage pool to identify and specify differences between a first version of a data object at a first instant in time and a second version of said data object at a second instant in time for causing the specification of differences in the data object to be provided to the remote deduplicating storage pool for causing the local performance storage pool to retrieve any data specified in the differences specification that is not already stored at the remote deduplicating storage pool for causing the data retrieved by the local performance storage pool to be provided to the remote deduplicating storage pool and for causing the remote deduplicating storage pool to provide a synchronized copy of said data to said remote performance storage pool thereby synchronizing the remote performance storage pool to have the second version of the data object.

The disclosed subject matter also includes an embodiment wherein the remote performance storage pool is kept available for providing prompt access to the data object within a finite length of time the length of time being independent of the size of the data and also of the latency of the data link between the local site and the remote site. The controller is configured to provide one of synchronized iterative and pipelined replication to transfer operational control of the business data to the remote site thereby providing failover capability to cause the remote performance storage pool to transfer data back to the local performance storage pool via the remote deduplicating storage pool and the local deduplicating storage pool thereby providing sync back capability. The local deduplicating storage pool and the remote deduplicating storage pool are each configured to create an organized arrangement of hash structures wherein each structure for a subset of the hash structures includes a hash signature for a corresponding content segment and is associated with a reference to the corresponding content segment wherein the logical organization of the arrangement represents the logical organization of the content segments as they are represented within the data object.

The disclosed subject matter also includes an embodiment wherein the local deduplicating storage pool is configured to identify and specify differences between a first version of a data object at a first instant in time and a second version of said data object at a second instant in time and for causing the specification of differences in the data object to be provided to the remote deduplicating storage pool and further comprising the local performance storage pool configured to retrieve any data specified in the differences specification that is not already stored at the remote deduplicating storage pool. The controller is configured to cause a partial data fingerprint to be generated using a fingerprinting function at the local deduplicating storage pool to cause the partial data fingerprint to be received at the remote deduplicating storage pool and to cause the partial data fingerprint to be compared with an independently generated fingerprint generated from the data object at the remote deduplicating storage pool the independently generated fingerprint reflecting byte regions sparsely sampled from throughout the data object and thereby efficiently providing an indication of the contents of the data object so that comparing data fingerprints for two data objects can determine if the data objects are different without requiring hashing of all bytes in the two compared data objects.

This disclosure pertains to generating a data fingerprint for an object stored in a virtual storage pool that may be used to compare two objects over the life of those data objects.

This disclosure also pertains to a method for improved incremental copy performance using hybrid seeding to perform copies and differencing operations using different virtual storage pools.

This disclosure also pertains to a mechanism for data replication for disaster recovery and business continuity using a pipeline of storage pools.

In the Data Management Virtualization system described below a user defines business requirements with regard to the lifecycle of the data and the Data Management Virtualization System performs these operations automatically. A snapshot is taken from primary storage to secondary storage this snapshot is then used for a backup operation to other secondary storage. Essentially an arbitrary number of these backups may be made providing a level of data protection specified by a Service Level Agreement.

The data management engine is operable to execute a sequence of snapshot operations to create point in time images of application data on a first storage pool each successive point in time image corresponding to a specific successive time state of the application data and each snapshot operation creating difference information indicating which application data has changed and the content of the changed application data for the corresponding time state. The data management engine is also operable to execute at least one back up function for the application data that is scheduled for execution at non consecutive time states and is also full of maintain history information having time state information indicating the time state of the last back up function performed on the application data for a corresponding back up copy of data. The data management engine creates composite difference information from the difference information for each time state between the time state of the last back up function performed on the application data and the time state of the currently scheduled back up function to be performed on the application data and sends the composite difference information to a second storage pool to be compiled with the back up copy of data at the last time state to create a back up copy of data for the current time state.

Data Management Virtualization technology according to this disclosure is based on an architecture and implementation based on the following guiding principles.

First define the business requirements of an application with a Service Level Agreement SLA for its entire data lifecycle. The SLA is more than a single RPO Retention and Recovery Time Objective RTO . It describes the data protection characteristics for each stage of the data lifecycle. Each application may have a different SLA.

Second provide a unified Data Management Virtualization Engine that manages the data protection lifecycle moving data across the various storage repositories with improved storage capacity and network bandwidth. The Data Management Virtualization system achieves these improvements by leveraging extended capabilities of modern storage systems by tracking the portions of the data that have changed over time and by data deduplication and compression algorithms that reduce the amount of data that needs to be copied and moved.

Third leverage a single master copy of the application data to be the basis for multiple elements within the lifecycle. Many of the Data Management operations such as backup archival and replication depend on a stable consistent copy of the data to be protected. The Data Management Virtualization System leverages a single copy of the data for multiple purposes. A single instance of the data maintained by the system may serve as the source from which each data management function may make additional copies as needed. This contrasts with requiring application data to be copied multiple times by multiple independent data management applications in the traditional approach.

Fourth abstracting physical storage resources into a series of data protection storage pools which are virtualized out of different classes of storage including local and remote disk solid state memory tape and optical media private public and or hybrid storage clouds. The storage pools provide access independent of the type physical location or underlying storage technology. Business requirements for the lifecycle of data may call for copying the data to different types of storage media at different times. The Data Management Virtualization system allows the user to classify and aggregate different storage media into storage pools for example a Quick Recovery Pool which may include high speed disks and a Cost Efficient Long term Storage Pool which may be a deduplicated store on high capacity disks or a tape library. The Data Management Virtualization System can move data amongst these pools to take advantage of the unique characteristics of each storage medium. The abstraction of Storage Pools provides access independent of the type physical location or underlying storage technology.

Fifth improve the movement of the data between storage pools and disaster locations utilizing underlying device capabilities and post deduplicated application data. The Data Management Virtualization System discovers the capabilities of the storage systems that include the Storage Pools and takes advantage of these capabilities to move data efficiently. If the Storage System is a disk array that supports the capability of creating a snapshot or clone of a data volume the Data Management Virtualization System will take advantage of this capability and use a snapshot to make a copy of the data rather than reading the data from one place and writing it to another. Similarly if a storage system supports change tracking the Data Management Virtualization System will update an older copy with just the changes to efficiently create a new copy. When moving data across a network the Data Management Virtualization system uses a deduplication and compression algorithm that avoids sending data that is already available on the other side of the network.

One key aspect of improving data movement is recognizing that application data changes slowly over time. A copy of an application that is made today will in general have a lot of similarities to the copy of the same application that was made yesterday. In fact today s copy of the data could be represented as yesterday s copy with a series of delta transformations where the size of the delta transformations themselves are usually much smaller than all of the data in the copy itself. The Data Management Virtualization system captures and records these transformations in the form of bitmaps or extent lists. In one embodiment of the system the underlying storage resources a disk array or server virtualization system are capable of tracking the changes made to a volume or file in these environments the Data Management Virtualization system queries the storage resources to obtain these change lists and saves them with the data being protected.

In the preferred embodiment of the Data Management Virtualization system there is a mechanism for eavesdropping on the primary data access path of the application which enables the Data Management Virtualization system to observe which parts of the application data are modified and to generate its own bitmap of modified data. If for example the application modifies blocks and during a particular period the Data Management Virtualization system will eavesdrop on these events and create a bitmap that indicates that these particular blocks were modified. When processing the next copy of application data the Data Management Virtualization system will only process blocks and since it knows that these were the only blocks that were modified.

In one embodiment of the system where the primary storage for the application is a modern disk array or storage virtualization appliance the Data Management Virtualization system takes advantage of a point in time snapshot capability of an underlying storage device to make the initial copy of the data. This virtual copy mechanism is a fast efficient and low impact technique of creating the initial copy that does not guarantee that all the bits will be copied or stored together. Instead virtual copies are constructed by maintaining metadata and data structures such as copy on write volume bitmaps or extents that allow the copies to be reconstructed at access time. The copy has a lightweight impact on the application and on the primary storage device. In another embodiment where the application is based on a Server Virtualization System such as VMware or Xen the Data Management Virtualization system uses the similar virtual machine snapshot capability that is built into the Server Virtualization systems. When a virtual copy capability is not available the Data Management Virtualization System may include its own built in snapshot mechanism.

It is possible to use the snapshot as a data primitive underlying all of the data management functions supported by the system. Because it is lightweight the snapshot can be used as an internal operation even when the requested operation is not a snapshot per se it is created to enable and facilitate other operations.

At the time of creation of a snapshot there may be certain preparatory operations involved in order to create a coherent snapshot or coherent image such that the image may be restored to a state that is usable by the application. These preparatory operations need only be performed once even if the snapshot will be leveraged across multiple data management functions in the system such as backup copies which are scheduled according to a policy. The preparatory operations may include application quiescence which includes flushing data caches and freezing the state of the application it may also include other operations known in the art and other operations useful for retaining a complete image such as collecting metadata information from the application to be stored with the image.

To serve local backup requirements a sequence of efficient snapshots are made within local high availability storage . Some of these snapshots are used to serve development test requirements without making another copy. For longer term retention of local backup a copy is made efficiently into long term local storage which in this implementation uses deduplication to reduce repeated copying. The copies within long term storage may be accessed as backups or treated as an archive depending on the retention policy applied by the SLA. A copy of the data is made to remote storage in order to satisfy requirements for remote backup and business continuity again a single set of copies suffices both purposes. As an alternative for remote backup and disaster recovery a further copy of the data may be made efficiently to a repository hosted by a commercial or private cloud storage provider.

Application creates and owns the data. This is the software system that has been deployed by the user as for example an email system a database system or financial reporting system in order to satisfy some computational need. The Application typically runs on a server and utilizes storage. For illustrative purposes only one application has been indicated. In reality there may be hundreds or even thousands of applications that are managed by a single Data Management Virtualization System.

Storage Resources is where application data is stored through its lifecycle. The Storage Resources are the physical storage assets including internal disk drives disk arrays optical and tape storage libraries and cloud based storage systems that the user has acquired to address data storage requirements. The storage resources include Primary Storage where the online active copy of the application data is stored and Secondary Storage where additional copies of the application data are stored for the purposes such as backup disaster recovery archiving indexing reporting and other uses. Secondary storage resources may include additional storage within the same enclosure as the primary storage as well as storage based on similar or different storage technologies within the same data center another location or across the internet.

One or more Management Workstations allow the user to specify a Service Level Agreement SLA that defines the lifecycle for the application data. A Management workstation is a desktop or laptop computer or a mobile computing device that is used to configure monitor and control the Data Management Virtualization System. A Service Level Agreement is a detailed specification that captures the detailed business requirements related to the creation retention and deletion of secondary copies of the application data. The SLA is more than the simple RTO and RPO that are used in traditional data management applications to represent the frequency of copies and the anticipated restore time for a single class of secondary storage. The SLA captures the multiple stages in the data lifecycle specification and allows for non uniform frequency and retention specifications within each class of secondary storage. The SLA is described in greater detail in .

Data Management Virtualization Engine manages all of the lifecycle of the application data as specified in SLA. It manages potentially a large number of SLAs for a large number of applications. The Data Management Virtualization Engine takes inputs from the user through the Management Workstation and interacts with the applications to discover the applications primary storage resources. The Data Management Virtualization Engine makes decisions regarding what data needs to be protected and what secondary storage resources best fulfill the protection needs. For example if an enterprise designates its accounting data as requiring copies to be made at very short intervals for business continuity purposes as well as for backup purposes the Engine may decide to create copies of the accounting data at a short interval to a first storage pool and to also create backup copies of the accounting data to a second storage pool at a longer interval according to an appropriate set of SLAs. This is determined by the business requirements of the storage application.

The Engine then makes copies of application data using advanced capabilities of the storage resources as available. In the above example the Engine may schedule the short interval business continuity copy using a storage appliance s built in virtual copy or snapshot capabilities. The Data Management Virtualization Engine moves the application data amongst the storage resources in order to satisfy the business requirements that are captured in the SLA. The Data Management Virtualization Engine is described in greater detail in .

The Data Management Virtualization System as a whole may be deployed within a single host computer system or appliance or it may be one logical entity but physically distributed across a network of general purpose and purpose built systems. Certain components of the system may also be deployed within a computing or storage cloud.

In one embodiment of the Data Management Virtualization System the Data Management Virtualization Engine largely runs as multiple processes on a fault tolerant redundant pair of computers. Certain components of the Data Management Virtualization Engine may run close to the application within the application servers. Some other components may run close to the primary and secondary storage within the storage fabric or in the storage systems themselves. The Management stations are typically desktop and laptop computers and mobile devices that connect over a secure network to the Engine.

Application Specific Module . This module is responsible for controlling and collecting metadata from the application . Application metadata includes information about the application such as the type of application details about its configuration location of its datastores its current operating state. Controlling the operation of the application includes actions such as flushing cached data to disk freezing and thawing application I O rotating or truncating log files and shutting down and restarting applications. The Application Specific module performs these operations and sends and receives metadata in responses to commands from the Service Level Policy Engine described below. The Application Specific Module is described in more detail in connection with .

Service Level Policy Engine . This module acts on the SLA provided by the user to make decisions regarding the creation movement and deletion of copies of the application data. Each SLA describes the business requirements related to protection of one application. The Service Level Policy Engine analyzes each SLA and arrives at a series of actions each of which involve the copying of application data from one storage location to another. The Service Level Policy Engine then reviews these actions to determine priorities and dependencies and schedules and initiates the data movement jobs. The Service Level Policy Engine is described in more detail in connection with .

Object Manager and Data Movement Engine . This module creates a composite object consisting of the Application data the Application Metadata and the SLA which it moves through different storage pools per instruction from the Policy Engine. The Object Manager receives instructions from the Service Policy Engine in the form of a command to create a copy of application data in a particular pool based on the live primary data belonging to the application or from an existing copy e.g. in another pool. The copy of the composite object that is created by the Object Manager and the Data Movement Engine is self contained and self describing in that it contains not only application data but also application metadata and the SLA for the application. The Object Manager and Data Movement Engine are described in more detail in connection with .

Storage Pool Manager . This module is a component that adapts and abstracts the underlying physical storage resources and presents them as virtual storage pools . The physical storage resources are the actual storage assets such as disk arrays and tape libraries that the user has deployed for the purpose of supporting the lifecycle of the data of the user s applications. These storage resources might be based on different storage technologies such as disk tape flash memory or optical storage. The storage resources may also have different geographic locations cost and speed attributes and may support different protocols. The role of the Storage Pool Manager is to combine and aggregate the storage resources and mask the differences between their programming interfaces. The Storage Pool Manager presents the physical storage resources to the Object Manager as a set of storage pools that have characteristics that make these pools suitable for particular stages in the lifecycle of application data. The Storage Pool Manager is described in more detail in connection with .

In one embodiment the system may deploy the Service Level Policy Engine on the same computer system as the Object Manager for ease of implementation. In another embodiment the system may employ multiple systems each hosting a subset of the components if beneficial or convenient for an application without changing the design.

The Object Manager and the Storage Pool Managers are software components that may reside on the computer system platform that interconnects the storage resources and the computer systems that use those storage resources where the user s application resides. The placement of these software components on the interconnect platform is designated as a preferred embodiment and may provide the ability to connect customer systems to storage via communication protocols widely used for such applications e.g. Fibre Channel iSCSI etc. and may also provide ease of deployment of the various software components.

The Object Manager and Storage Pool Manager communicate with the underlying storage virtualization platform via the Application Programming Interfaces made available by the platform. These interfaces allow the software components to query and control the behavior of the computer system and how it interconnects the storage resources and the computer system where the user s Application resides. The components apply modularity techniques as is common within the practice to allow replacement of the intercommunication code particular to a given platform.

The Object Manager and Storage Pool Managers communicate via a protocol. These are transmitted over standard networking protocols e.g. TCP IP or standard Interprocess Communication IPC mechanisms typically available on the computer system. This allows comparable communication between the components if they reside on the same computer platform or on multiple computer platforms connected by a network depending on the particular computer platform. The current configuration has all of the local software components residing on the same computer system for ease of deployment. This is not a strict requirement of the design as described above and can be reconfigured in the future as needed.

Object Manager is a software component for maintaining Data Storage Objects and provides a set of protocol operations to control it. The operations include creation destruction duplication and copying of data among the objects maintaining access to objects and in particular allow the specification of the storage pool used to create copies. There is no common subset of functions supported by all pools however in a preferred embodiment primary pools may be performance optimized i.e. lower latency whereas backup or replication pools may be capacity optimized supporting larger quantities of data and content addressable. The pools may be remote or local. The storage pools are classified according to various criteria including means by which a user may make a business decision e.g. cost per gigabyte of storage.

First the particular storage device from which the storage is drawn may be a consideration as equipment is allocated for different business purposes along with associated cost and other practical considerations. Some devices may not even be actual hardware but capacity provided as a service and selection of such a resource can be done for practical business purposes.

Second the network topological proximity is considered as near storage is typically connected by low latency inexpensive network resources while distant storage may be connected by high latency bandwidth limited expensive network resources conversely the distance of a storage pool relative to the source may be beneficial when geographic diversity protects against a physical disaster affecting local resources.

Third storage optimization characteristics are considered where some storage is optimized for space efficient storage but requires computation time and resources to analyze or transform the data before it can be stored while other storage by comparison is performance optimized taking more storage resources by comparison but using comparatively little computation time or resource to transform the data if at all.

Fourth speed of access characteristics are considered where some resources intrinsic to a storage computer platform are readily and quickly made available to the user s Application e.g. as a virtual SCSI block device while some can only be indirectly used. These ease and speed of recovery is often governed by the kind of storage used and this allows it to be suitably classified.

Fifth the amount of storage used and the amount available in a given pool are considered as there may be benefit to either concentrating or spreading the storage capacity used.

The Service Level Policy Engine described below combines the SLA provided by the user with the classification criteria to determine how and when to maintain the application data and from which storage pools to draw the needed resources to meet the Service Level Agreement SLA .

The object manager creates maintains and employs a history mechanism to track the series of operations performed on a data object within the performance pools and to correlate those operations with others that move the object to other storage pools in particular capacity optimized ones. This series of records for each data object is maintained at the object manager for all data objects in the primary pool initially correlated by primary data object then correlated by operation order a time line for each object and a list of all such time lines. Each operation performed exploits underlying virtualization primitives to capture the state of the data object at a given point in time.

Additionally the underlying storage virtualization appliance may be modified to expose and allow retrieval of internal data structures such as bitmaps that indicate the modification of portions of the data within the data object. These data structures are exploited to capture the state of a data object at a point in time e.g. a snapshot of the data object and to provide differences between snapshots taken at a specific time and thereby enables optimal backup and restore. While the particular implementations and data structures may vary among different appliances from different vendors a data structure is employed to track changes to the data object and storage is employed to retain the original state of those portions of the object that have changed indications in the data structure correspond to data retained in the storage. When accessing the snapshot the data structure is consulted and for portions that have been changed the preserved data is accessed rather than the current data as the data object has been modified at the areas so indicated. A typical data structure employed is a bitmap where each bit corresponds to a section of the data object. Setting the bit indicates that section has been modified after the point in time of the snapshot operation. The underlying snapshot primitive mechanism maintains this for as long as the snapshot object exists.

The time line described above maintains a list of the snapshot operations against a given primary data object including the time an operation is started the time it is stopped if at all a reference to the snapshot object and a reference to the internal data structure e.g. bitmaps or extent lists so that it can be obtained from the underlying system. Also maintained is a reference to the result of copying the state of the data object at any given point in time into another pool as an example copying the state of a data object into a capacity optimized pool using content addressing results in an object handle. That object handle corresponds to a given snapshot and is stored with the snapshot operation in the time line. This correlation is used to identify suitable starting points.

Optimal backup and restore consult the list of operations from a desired starting point to an end point. A time ordered list of operations and their corresponding data structures bitmaps are constructed such that a continuous time series from start to finish is realized there is no gap between start times of the operations in the series. This ensures that all changes to the data object are represented by the corresponding bitmap data structures. It is not necessary to retrieve all operations from start to finish simultaneously existing data objects and underlying snapshots overlap in time it is only necessary that there are no gaps in time where a change might have occurred that was not tracked. As bitmaps indicate that a certain block of storage has changed but not what the change is the bitmaps may be added or composed together to realize a set of all changes that occurred in the time interval. Instead of using this data structure to access the state at a point in time the system instead exploits the fact that the data structure represents data modified as time marches forward. Rather the end state of the data object is accessed at the indicated areas thus returning the set of changes to the given data object from the given start time to the end time.

The backup operation exploits this time line the correlated references and access to the internal data structures to realize our backup operation. Similarly it uses the system in a complementary fashion to accomplish our restore operation. The specific steps are described below in the section for Optimal Backup Restore. 

Primary Storage Pool contains the storage resources used to create the data objects in which the user Application stores its data. This is in contrast to the other storage pools which exist to primarily fulfill the operation of the Data Management Virtualization Engine.

Performance Optimized Pool a virtual storage pool able to provide high performance backup i.e. point in time duplication described below as well as rapid access to the backup image by the user Application

Capacity Optimized Pool a virtual storage pool that chiefly provides storage of a data object in a highly space efficient manner by use of deduplication techniques described below. The virtual storage pool provides access to the copy of the data object but does not do so with high performance as its chief aim in contrast to the Performance Optimized pool above.

The initial deployments contain storage pools as described above as a minimal operational set. The design fully expects multiple Pools of a variety of types representing various combinations of the criteria illustrated above and multiple Pool Managers as is convenient to represent all of the storage in future deployments. The tradeoffs illustrated above are typical of computer data storage systems.

From a practical point of view these three pools represent a preferred embodiment addressing most users requirements in a very simple way. Most users will find that if they have one pool of storage for urgent restore needs which affords quick recovery and one other pool that is low cost so that a large number of images can be retained for a large period of time almost all of the business requirements for data protection can be met with little compromise.

The format of data in each pool is dictated by the objectives and technology used within the pool. For example the quick recovery pool is maintained in the form very similar to the original data to minimize the translation required and to improve the speed of recovery. The long term storage pool on the other hand uses deduplication and compression to reduce the size of the data and thus reduce the cost of storage.

The Object Manager creates and maintains instances of Data Storage Objects from the Virtual Storage Pools according to the instructions sent to it by the Service Level Policy Engine . The Object Manager provides data object operations in five major areas point in time duplication or copying commonly referred to as snapshots standard copying object maintenance mapping and access maintenance and collections.

Object Management operations also include a series of Resource Discovery operations for maintaining Virtual Storage Pools themselves and retrieving information about them. The Pool Manager ultimately supplies the functionality for these.

Snapshot operations create a data object instance representing an initial object instance at a specific point in time. More specifically a snapshot operation creates a complete virtual copy of the members of a collection using the resources of a specified Virtual Storage Pool. This is called a Data Storage Object. Multiple states of a Data Storage Object are maintained over time such that the state of a Data Storage Object as it existed at a point in time is available. As described above a virtual copy is a copy implemented using an underlying storage virtualization API that allows a copy to be created in a lightweight fashion using copy on write or other in band technologies instead of copying and storing all bits of duplicate data to disk. This may be implemented using software modules written to access the capabilities of an off the shelf underlying storage virtualization system such as provided by EMC vmware or IBM in some embodiments. Where such underlying virtualizations are not available the described system may provide its own virtualization layer for interfacing with unintelligent hardware.

Snapshot operations require the application to freeze the state of the data to a specific point so that the image data is coherent and so that the snapshot may later be used to restore the state of the application at the time of the snapshot. Other preparatory steps may also be required. These are handled by the Application Specific Module which is described in a subsequent section. For live applications therefore the most lightweight operations are desired.

Snapshot operations are used as the data primitive for all higher level operations in the system. In effect they provide access to the state of the data at a particular point in time. As well since snapshots are typically implemented using copy on write techniques that distinguish what has changed from what is resident on disk these snapshots provide differences that can also be composed or added together to efficiently copy data throughout the system. The format of the snapshot may be the format of data that is copied by Data Mover which is described below.

When a copy operation is not a snapshot it may be considered a standard copy operation. A standard copy operation copies all or a subset of a source data object in one storage pool to a data object in another storage pool. The result is two distinct objects. One type of standard copy operation that may be used is an initial baseline copy. This is typically done when data is initially copied from one Virtual Storage Pool into another such as from a performance optimized pool to a capacity optimized storage pool. Another type of standard copy operation may be used wherein only changed data or differences are copied to a target storage pool to update the target object. This would occur after an initial baseline copy has previously been performed.

A complete exhaustive version of an object need not be preserved in the system each time a copy is made even though a baseline copy is needed when the Data Virtualization System is first initialized. This is because each virtual copy provides access to a complete copy. Any delta or difference can be expressed in relation to a virtual copy instead of in relation to a baseline. This has the positive side effect of virtually eliminating the common step of walking through a series of change lists.

Standard copy operations are initiated by a series of instructions or requests supplied by the Pool Manager and received by the Data Mover to cause the movement of data among the Data Storage Objects and to maintain the Data Storage Objects themselves. The copy operations allow the creation of copies of the specified Data Storage Objects using the resources of a specified Virtual Storage Pool. The result is a copy of the source Data Object in a target Data Object in the storage pool.

The Snapshot and Copy operations are each structured with a preparation operation and an activation operation. The two steps of prepare and activate allow the long running resource allocation operations typical of the prepare phase to be decoupled from the actuation. This is required by applications that can only be paused for a short while to fulfill the point in time characteristics of a snapshot operation which in reality takes a finite but non zero amount of time to accomplish. Similarly for copy and snapshot operations this two step preparation and activation structure allows the Policy Engine to proceed with an operation only if resources for all of the collection members can be allocated.

Object Maintenance operations are a series of operations for maintaining data objects including creation destruction and duplication. The Object Manager and Data Mover use functionality provided by a Pool Request Broker more below to implement these operations. The data objects may be maintained at a global level at each Storage Pool or preferably both.

Collection operations are auxiliary functions. Collections are abstract software concepts lists maintained in memory by the object manager. They allow the Policy Engine to request a series of operations over all of the members in a collection allowing a consistent application of a request to all members. The use of collections allows for simultaneous activation of the point in time snapshot so that multiple Data Storage Objects are all captured at precisely the same point in time as this is typically required by the application for a logically correct restore. The use of collections allows for convenient request of a copy operation across all members of a collection where an application would use multiple storage objects as a logical whole.

The Object Manager discovers Virtual Storage Pools by issuing Object Management Operations to the Pool Manager and uses the information obtained about each of the pools to select one that meets the required criteria for a given request or in the case where none match a default pool is selected and the Object Manager can then create a data storage object using resources from the selected Virtual Storage Pool.

The Object Manager also provides sets of Object Management operations to allow and maintain the availability of these objects to external Applications. The first set is operations for registering and unregistering the computers where the user s Applications reside. The computers are registered by the identities typical to the storage network in use e.g. Fibre Channel WWPN iSCSI identity etc. . The second set is mapping operations and when permitted by the storage pool from which an object is created the Data Storage Object can be mapped that is made available for use to a computer on which a user Application resides.

This availability takes a form appropriate to the storage e.g. a block device presented on a SAN as a Fibre Channel disk or iSCSI device on a network a filesystem on a file sharing network etc. and is usable by the operating system on the Application computer. Similarly an unmapping operation reverses the availability of the virtual storage device on the network to a user Application. In this way data stored for one Application i.e. a backup can be made available to another Application on another computer at a later time i.e. a restore.

The Data Mover is a software component within the Object Manager and Data Mover that reads and writes data among the various Data Storage Objects according to instructions received from the Object Manager for Snapshot Point in Time Copy requests and standard copy requests. The Data Mover provides operations for reading and writing data among instances of data objects throughout the system. The Data Mover also provides operations that allow querying and maintaining the state of long running operations that the Object Manager has requested for it to perform.

The Data Mover uses functionality from the Pool Functionality Providers see to accomplish its operation. The Snapshot functionality provider allows creation of a data object instance representing an initial object instance at a specific point in time. The Difference Engine functionality provider is used to request a description of the differences between two data objects that are related in a temporal chain. For data objects stored on content addressable pools a special functionality is provided that can provide differences between any two arbitrary data objects. This functionality is also provided for performance optimized pools in some cases by an underlying storage virtualization system and in other cases by a module that implements this on top of commodity storage. The Data Mover uses the information about the differences to select the set of data that it copies between instances of data objects .

For a given Pool the Difference Engine Provider provides a specific representation of the differences between two states of a Data Storage Object over time. For a Snapshot provider the changes between two points in time are recorded as writes to a given part of the Data Storage Object. In one embodiment the difference is represented as a bitmap where each bit corresponds to an ordered list of the Data Object areas starting at the first and ascending in order to the last where a set bit indicates a modified area. This bitmap is derived from the copy on write bitmaps used by the underlying storage virtualization system. In another embodiment the difference may be represented as a list of extents corresponding to changed areas of data. For a Content Addressable storage provider the representation is described below and is used to determine efficiently the parts of two Content Addressable Data Objects that differ.

The Data Mover uses this information to copy only those sections that differ so that a new version of a Data Object can be created from an existing version by first duplicating it obtaining the list of differences and then moving only the data corresponding to those differences in the list. The Data Mover traverses the list of differences moving the indicated areas from the source Data Object to the target Data Object. See Optimal Way for Data Backup and Restore. 

The Object Manager instructs the Data Mover through a series of operations to copy data among the data objects in the Virtual Storage Pools . The procedure includes the following steps starting at the reception of instructions 

Second add Object to Collection. The collection name from above is used as well as the name of the source Data Object that is to be copied and the name of two antecedents a Data Object against which differences are to be taken in the source Storage Resource Pool and a corresponding Data Object in the target Storage Resource Pool. This step is repeated for each source Data Object to be operated on in this set.

Third prepare Copy Request. The collection name is supplied as well as a Storage Resource Pool to act as a target. The prepare command instructs the Object Manager to contact the Storage Pool Manager to create the necessary target Data Objects corresponding to each of the sources in the collection. The prepare command also supplies the corresponding Data Object in the target Storage Resource Pool to be duplicated so the Provider can duplicate the provided object and use that as a target object. A reference name for the copy request is returned.

Fourth activate Copy Request. The reference name for the copy request returned above is supplied. The Data Mover is instructed to copy a given source object to its corresponding target object. Each request includes a reference name as well as a sequence number to describe the overall job the entire set of source target pairs as well as a sequence number to describe each individual source target pair. In addition to the source target pair the names of the corresponding antecedents are supplied as part of the Copy instruction.

Fifth the Copy Engine uses the name of the Data Object in the source pool to obtain the differences between the antecedent and the source from the Difference Engine at the source. The indicated differences are then transmitted from the source to the target. In one embodiment these differences are transmitted as bitmaps and data. In another embodiment these differences are transmitted as extent lists and data.

Data Storage Objects are software constructs that permit the storage and retrieval of Application data using idioms and methods familiar to computer data processing equipment and software. In practice these currently take the form of a SCSI block device on a storage network e.g. a SCSI LUN or a content addressable container where a designator for the content is constructed from and uniquely identifies the data therein. Data Storage Objects are created and maintained by issuing instructions to the Pool Manager. The actual storage for persisting the Application data is drawn from the Virtual Storage Pool from which the Data Storage Object is created.

The structure of the data storage object varies depending on the storage pool from which it is created. For the objects that take the form of a block device on a storage network the data structure for a given block device Data Object implements a mapping between the Logical Block Address LBA of each of the blocks within the Data Object to the device identifier and LBA of the actual storage location. The identifier of the Data Object is used to identify the set of mappings to be used. The current embodiment relies on the services provided by the underlying physical computer platform to implement this mapping and relies on its internal data structures such as bitmaps or extent lists.

For objects that take the form of a Content Addressable Container the content signature is used as the identifier and the Data Object is stored as is described below in the section about deduplication.

A Pool Manager is a software component for managing virtual storage resources and the associated functionality and characteristics as described below. The Object manager and Data Movement Engine communicate with one or more Pool Managers to maintain Data Storage Objects .

Virtual Storage Resources are various kinds of storage made available to the Pool Manager for implementing storage pool functions as described below. In this embodiment a storage virtualizer is used to present various external Fibre Channel or iSCSI storage LUNs as virtualized storage to the Pool Manager .

The storage pool manager has a typical architecture for implementing a common interface to diverse implementations of similar functionality where some functionality is provided by smart underlying resources and other functionality must be implemented on top of less functional underlying resources.

Pool request broker and pool functionality providers are software modules executing in either the same process as the Object Manager Data Mover or in another process communicating via a local or network protocol such as TCP. In this embodiment the providers include a Primary Storage provider Snapshot provider Content Addressable provider and Difference Engine provider and these are further described below. In another embodiment the set of providers may be a superset of those shown here.

Virtual Storage Resources are the different kinds of storage made available to the Pool Manager for implementing storage pool functions. In this embodiment the virtual storage resources include sets of SCSI logical units from a storage virtualization system that runs on the same hardware as the pool manager and accessible for both data and management operations through a programmatic interface in addition to standard block storage functionality additional capabilities are available including creating and deleting snapshots and tracking changed portions of volumes. In another embodiment the virtual resources can be from an external storage system that exposes similar capabilities or may differ in interface for example accessed through a file system or through a network interface such as CIFS iSCSI or CDMI in capability for example whether the resource supports an operation to make a copy on write snapshot or in non functional aspects for example high speed limited capacity such as Solid State Disk versus low speed high capacity such as SATA disk . The capabilities and interface available determine which providers can consume the virtual storage resources and which pool functionality needs to be implemented within the pool manager by one or more providers for example this implementation of a content addressable storage provider only requires dumb storage and the implementation is entirely within content addressable provider an underlying content addressable virtual storage resource could be used instead with a simpler pass through provider. Conversely this implementation of a snapshot provider is mostly pass through and requires storage that exposes a quick point in time copy operation.

Pool Request Broker is a simple software component that services requests for storage pool specific functions by executing an appropriate set of pool functionality providers against the configured virtual storage resource . The requests that can be serviced include but are not limited to creating an object in a pool deleting an object from a pool writing data to an object reading data from an object copying an object within a pool copying an object between pools requesting a summary of the differences between two objects in a pool.

Primary storage provider enables management interfaces for example creating and deleting snapshots and tracking changed portions of files to a virtual storage resource that is also exposed directly to applications via an interface such as fibre channel iSCSI NFS or CIFS.

Snapshot provider implements the function of making a point in time copy of data from a Primary resource pool. This creates the abstraction of another resource pool populated with snapshots. As implemented the point in time copy is a copy on write snapshot of the object from the primary resource pool consuming a second virtual storage resource to accommodate the copy on write copies since this management functionality is exposed by the virtual storage resources used for primary storage and for the snapshot provider.

Difference engine provider can satisfy a request for two objects in a pool to be compared that are connected in a temporal chain. The difference sections between the two objects are identified and summarized in a provider specific way e.g. using bitmaps or extents. For example the difference sections might be represented as a bitmap where each set bit denotes a fixed size region where the two objects differ or the differences might be represented procedurally as a series of function calls or callbacks.

Depending on the virtual storage resource on which the pool is based or on other providers implementing the pool a difference engine may produce a result efficiently in various ways. As implemented a difference engine acting on a pool implemented via a snapshot provider uses the copy on write nature of the snapshot provider to track changes to objects that have had snapshots made. Consecutive snapshots of a single changing primary object thus have a record of the differences that is stored alongside them by the snapshot provider and the difference engine for snapshot pools simply retrieves this record of change. Also as implemented a difference engine acting on a pool implemented via a Content Addressable provider uses the efficient tree structure see below of the content addressable implementation to do rapid comparisons between objects on demand.

Content addressable provider implements a write once content addressable interface to the virtual storage resource it consumes. It satisfies read write duplicate and delete operations. Each written or copied object is identified by a unique handle that is derived from its content. The content addressable provider is described further below .

In operation the pool request broker accepts requests for data manipulation operations such as copy snapshot or delete on a pool or object. The request broker determines which provider code from pool to execute by looking at the name or reference to the pool or object. The broker then translates the incoming service request into a form that can be handled by the specific pool functionality provider and invokes the appropriate sequence of provider operations.

For example an incoming request could ask to make a snapshot from a volume in a primary storage pool into a snapshot pool. The incoming request identifies the object volume in the primary storage pool by name and the combination of name and operation snapshot determines that the snapshot provider should be invoked which can make point in time snapshots from the primary pool using the underlying snapshot capability. This snapshot provider will translate the request into the exact form required by the native copy on write function performed by the underlying storage virtualization appliance such as bitmaps or extents and it will translate the result of the native copy on write function to a storage volume handle that can be returned to the object manager and used in future requests to the pool manager.

Optimal Way for Data Backup is a series of operations to make successive versions of Application Data objects over time while minimizing the amount of data that must be copied by using bitmaps extents and other temporal difference information stored at the Object Mover. It stores the application data in a data storage object and associates with it the metadata that relates the various changes to the application data over time such that changes over time can be readily identified.

Each data object within the destination Virtual Storage Pool is complete that is it represents the entire data object and allows access to the all of the Application Data at the point in time without requiring external reference to state or representations at other points in time. The object is accessible without replaying all deltas from a baseline state to the present state. Furthermore the duplication of initial and subsequent versions of the data object in the destination Virtual Storage Pool does not require exhaustive duplication of the Application Data contents therein. Finally to arrive at second and subsequent states requires only the transmission of the changes tracked and maintained as described above without exhaustive traversal transmission or replication of the contents of the data storage object.

Intuitively the operation of the Optimal Way for Data Restore is the converse of the Optimal Way for Data Backup. The procedure to recreate the desired state of a data object in a destination Virtual Storage Pool at a given point in time includes the following steps 

Access to the desired state is complete it does not require external reference to other containers or other states. Establishing the desired state given a reference state requires neither exhaustive traversal nor exhaustive transmission only the retrieved changes indicated by the provided representations within the source Virtual Storage Pool.

The Service Level Agreement of certain embodiments has been designed to represent all of these complexities that exist in the business requirements. The Service Level Agreement has four primary parts the name the description the housekeeping attributes and a collection of Service Level Policies. As mentioned above there is one SLA per application.

The description attribute is where the user can assign a helpful description for the Service Level Agreement.

The Service Level agreement also has a number of housekeeping attributes that enable it to be maintained and revised. These attributes include but are not limited to the owner s identity the dates and times of creation modification and access priority enable disable flags.

The Service Level Agreement also contains a plurality of Service Level Policies . Some Service level Agreements may have just a single Service Level Policy. More typically a single SLA may contain tens of policies.

Each Service Level Policy includes at least the following in certain embodiments the source storage pool location and type the target storage pool location and type the frequency for the creation of copies expressed as a period of time the length of retention of the copy expressed as a period of time the hours of operation during the day for this particular Service Level Policy and the days of the week month or year on which this Service Level Policy applies.

Each Service Level Policy specifies a source and target storage pool and the frequency of copies of application data that are desired between those storage pools. Furthermore the Service Level Policy specifies its hours of operation and days on which it is applicable. Each Service Level Policy is the representation of one single statement in the business requirements for the protection of application data. For example if a particular application has a business requirement for an archive copy to be created each month after the monthly close and retained for three years this might translate to a Service level Policy that requires a copy from the Local Backup Storage Pool into the Long term Archive Storage Pool at midnight on the last day of the month with a retention of three years.

All of the Service Level Policies with a particular combination of source and destination pool and location say for example source Primary Storage pool and destination local Snapshot pool when taken together specify the business requirements for creating copies into that particular destination pool. Business requirements may dictate for example that snapshot copies be created every hour during regular working hours but only once every four hours outside of these times. Two Service Level Policies with the same source and target storage pools will effectively capture these requirements in a form that can be put into practice by the Service Policy Engine.

This form of a Service Level Agreement allows the representation of the schedule of daily weekly and monthly business activities and thus captures business requirements for protecting and managing application data much more accurately than traditional RPO and RPO based schemes. By allowing hour of operation and days weeks and months of the year scheduling can occur on a calendar basis. 

Taken together all of the Service Level Policies with one particular combination of source and destinations for example source local primary and destination local performance optimized captures the non uniform data protection requirements for one type of storage. A single RPO number on the other hand forces a single uniform frequency of data protection across all times of day and all days. For example a combination of Service Level Policies may require a large number of snapshots to be preserved for a short time such as 10 minutes and a lesser number of snapshots to be preserved for a longer time such as 8 hours this allows a small amount of information that has been accidentally deleted can be reverted to a state not more than 10 minutes before while still providing substantial data protection at longer time horizons without requiring the storage overhead of storing all snapshots taken every ten minutes. As another example the backup data protection function may be given one Policy that operates with one frequency during the work week and another frequency during the weekend.

When Service Level Policies for all of the different classes of source and destination storage are included the Service Level Agreement fully captures all of the data protection requirements for the entire application including local snapshots local long duration stores off site storage archives etc. A collection of policies within a SLA is capable of expressing when a given function should be performed and is capable of expressing multiple data management functions that should be performed on a given source of data.

Service Level Agreements are created and modified by the user through a user interface on a management workstation. These agreements are electronic documents stored by the Service Policy Engine in a structured SQL database or other repository that it manages. The policies are retrieved electronically analyzed and acted upon by the Service Policy Engine through its normal scheduling algorithm as described below.

The Application Specific Module interacts with various components of the application and its operating environment including Application Service Processes and Daemons Application Configuration Data Operating System Storage Services such as VSS and VDS on Windows Logical Volume Management and Filesystem Services and Operating System Drivers and Modules .

The Application Specific Module performs these operations in response to control commands from the Service Policy Engine . There are two purposes for these interactions with the application Metadata Collection and Application Consistency.

Metadata Collection is the process by which the Application Specific Module collects metadata about the application. In some embodiments metadata includes information such as configuration parameters for the application state and status of the application control files and startup shutdown scripts for the application location of the datafiles journal and transaction logs for the application and symbolic links filesystem mount points logical volume names and other such entities that can affect the access to application data.

Metadata is collected and saved along with application data and SLA information. This guarantees that each copy of application data within the system is self contained and includes all of the details required to rebuild the application data.

Application Consistency is the set of actions that ensure that when a copy of the application data is created the copy is valid and can be restored into a valid instance of the application. This is critical when the business requirements dictate that the application be protected while it is live in its online operational state. The application may have interdependent data relations within its data stores and if these are not copied in a consistent state will not provide a valid restorable image.

The exact process of achieving application consistency varies from application to application. Some applications have a simple flush command that forces cached data to disk. Some applications support a hot backup mode where the application ensures that its operations are journaled in a manner that guarantees consistency even as application data is changing. Some applications require interactions with operating system storage services such as VSS and VDS to ensure consistency. The Application Specific Module is purpose built to work with a particular application and to ensure the consistency of that application. The Application Specific Module interacts with the underlying storage virtualization device and the Object Manager to provide consistent snapshots of application data.

For efficiency the preferred embodiment of the Application Specific Module is to run on the same server as Application . This assures the minimum latency in the interactions with the application and provides access to storage services and filesystems on the application host. The application host is typically considered primary storage which is then snapshotted to a performance optimized store.

In order to minimize interruption of a running application including minimizing preparatory steps the Application Specific Module is only triggered to make a snapshot when access to application data is required at a specific time and when a snapshot for that time does not exist elsewhere in the system as tracked by the Object Manager. By tracking which times snapshots have been made the Object Manager is able to fulfill subsequent data requests from the performance optimized data store including for satisfying multiple requests for backup and replication which may issue from secondary capacity optimized pools. The Object Manager may be able to provide object handles to the snapshot in the performance optimized store and may direct the performance optimized store in a native format that is specific to the format of the snapshot which is dependent on the underlying storage appliance. In some embodiments this format may be application data combined with one or more LUN bitmaps indicating which blocks have changed in other embodiments it may be specific extents. The format used for data transfer is thus able to transfer only a delta or difference between two snapshots using bitmaps or extents.

Metadata such as the version number of the application may also be stored for each application along with the snapshot. When a SLA policy is executed application metadata is read and used for the policy. This metadata is stored along with the data objects. For each SLA application metadata will only be read once during the lightweight snapshot operation and preparatory operations which occur at that time such as flushing caches will only be performed once during the lightweight snapshot operation even though this copy of application data along with its metadata may be used for multiple data management functions.

History Store is where historical information about past activities is saved for the use of all data management applications including the timestamp order and hierarchy of previous copies of each application into various storage pools. For example a snapshot copy from a primary data store to a capacity optimized data store that is initiated at 1 P.M. and is scheduled to expire at 9 P.M. will be recorded in History Store in a temporal data store that also includes linked object data for snapshots for the same source and target that have taken place at 11 A.M. and 12 P.M.

These stores are managed by the Service Policy Engine. For example when the user through the Management workstation creates a Service Level Agreement or modifies one of the policies within it it is the Service Policy Engine that persists this new SLA in its store and reacts to this modification by scheduling copies as dictated by the SLA. Similarly when the Service Policy Engine successfully completes a data movement job that results in a new copy of an application in a Storage Pool the Storage Policy Engine updates the History Store so that this copy will be factored into future decisions.

The preferred embodiment of the various stores used by the Service Policy Engine is in the form of tables in a relational database management system in close proximity to the Service Policy Engine. This ensures consistent transactional semantics when querying and updating the stores and allows for flexibility in retrieving interdependent data.

The scheduling algorithm for the Service Policy Scheduler is illustrated in . When the Service Policy Scheduler decides it needs to make a copy of application data from one storage pool to another it initiates a Data Movement Requestor and Monitor task . These tasks are not recurring tasks and terminate when they are completed. Depending on the way that Service Level Policies are specified a plurality of these requestors might be operational at the same time.

The Service Policy Scheduler considers the priorities of Service Level Agreements when determining which additional tasks to undertake. For example if one Service Level Agreement has a high priority because it specifies the protection for a mission critical application whereas another SLA has a lower priority because it specifies the protection for a test database then the Service Policy Engine may choose to run only the protection for the mission critical application and may postpone or even entirely skip the protection for the lower priority application. This is accomplished by the Service Policy Engine scheduling a higher priority SLA ahead of a lower priority SLA. In the preferred embodiment in such a situation for auditing purposes the Service Policy Engine will also trigger a notification event to the management workstation.

For each SLA the schedule engine collects together all of the Service Level Policies that have the same source pool and destination pool the process state at and iterates to the next SLA in the set of SLAs in . Taken together this subset of the Service Level Policies represent all of the requirements for a copy from that source storage pool to that particular destination storage pool.

Among this subset of Service Level Policies the Service Policy Scheduler discards the policies that are not applicable to today or are outside their hours of operation. Among the policies that are left find the policy that has the shortest frequency and based on the history data and in history store the one with the longest retention that needs to be run next .

Next there are a series of checks which rule out making a new copy of application data at this time because the new copy is not yet due because a copy is already in progress or because there is not new data to copy. If any of these conditions apply the Service Policy Scheduler moves to the next combination of source and destination pools . If none of these conditions apply a new copy is initiated. The copy is executed as specified in the corresponding service level policy within this SLA .

Next the Scheduler moves to the next Source and Destination pool combination for the same Service Level agreement . If there are no more distinct combinations the Scheduler moves on to the next Service Level Agreement .

After the Service Policy Scheduler has been through all source destination pool combinations of all Service Level Agreements it pauses for a short period and then resumes the cycle.

A simple example system with a snapshot store and a backup store with only 2 policies defined would interact with the Service Policy Scheduler as follows. Given two policies one stating backup every hour the backup to be kept for 4 hours and another stating backup every 2 hours the backup to be kept for 8 hours the result would be a single snapshot taken each hour the snapshots each being copied to the backup store but retained a different amount of time at both the snapshot store and the backup store. The backup every 2 hours policy is scheduled to go into effect at 12 00 P.M by the system administrator.

At 4 00 P.M. when the Service Policy Scheduler begins operating at step it finds the two policies at step . Both policies apply because a multiple of two hours has elapsed since 12 00 P.M. There is only one source and destination pool combination at step . There are two frequencies at step and the system selects the 1 hour frequency because it is shorter than the 2 hour frequency. There are two operations with different retentions at step and the system selects the operation with the 8 hour retention as it has the longer retention value. Instead of one copy being made to satisfy the 4 hour requirement and another copy being made to satisfy the 8 hour requirement the two requirements are coalesced into the longer 8 hour requirement and are satisfied by a single snapshot copy operation. The system determines that a copy is due at step and checks the relevant objects at the History Store to determine if the copy has already been made at the target at step and at the source at step . If these checks are passed the system initiates the copy at step and in the process triggers a snapshot to be made and saved at the snapshot store. The snapshot is then copied from the snapshot store to the backup store. The system then goes to sleep and wakes up again after a short period such as 10 seconds. The result is a copy at the backup store and a copy at the snapshot store where every even hour snapshot lasts for 8 hours and every odd hour snapshot lasts 4 hours. The even hour snapshots at the backup store and the snapshot store are both tagged with the retention period of 8 hours and will be automatically deleted from the system by another process at that time.

Note that there is no reason to take two snapshots or make two backup copies at 2 o clock even though both policies apply because both policies are satisfied by a single copy. Combining and coalescing these snapshots results in the reduction of unneeded operations while retaining the flexibility of multiple separate policies. As well it may be helpful to have two policies active at the same time for the same target with different retention. In the example given there are more hourly copies kept than two hour copies resulting in more granularity for restore at times that are closer to the present. For example in the previous system if at 7 30 P.M. damage is discovered from earlier in the afternoon a backup will be available for every hour for the past four hours 4 5 6 7 P.M. As well two more backups will have been retained from 2 P.M. and 12 P.M.

The content addressable store implementation provides a storage resource pool that is optimized for capacity rather than for copy in or copy out speed as would be the case for the performance optimized pool implemented through snapshots described earlier and thus is typically used for offline backup replication and remote backup. Content addressable storage provides a way of storing common subsets of different objects only once where those common subsets may be of varying sizes but typically as small as 4 KiBytes. The storage overhead of a content addressable store is low compared to a snapshot store though the access time is usually higher. Generally objects in a content addressable store have no intrinsic relationship to one another even though they may share a large percentage of their content though in this implementation a history relationship is also maintained which is an enabler of various optimizations to be described. This contrasts with a snapshot store where snapshots intrinsically form a chain each storing just deltas from a previous snapshot or baseline copy. In particular the content addressable store will store only one copy of a data subset that is repeated multiple times within a single object whereas a snapshot based store will store at least one full copy of any object.

The content addressable store is a software module that executes on the same system as the pool manager either in the same process or in a separate process communicating via a local transport such as TCP. In this embodiment the content addressable store module runs in a separate process so as to minimize impact of software failures from different components.

This module s purpose is to allow storage of Data Storage Objects in a highly space efficient manner by deduplicating content i.e. ensuring repeated content within single or multiple data objects is stored only once .

The content addressable store module provides services to the pool manager via a programmatic API. These services include the following 

Object to Handle mapping an object can be created by writing data into the store via an API once the data is written completely the API returns an object handle determined by the content of the object. Conversely data may be read as a stream of bytes from an offset within an object by providing the handle. Details of how the handle is constructed are explained in connection with the description of .

Temporal Tree Management tracks parent child relationships between data objects stored. When a data object is written into the store an API allows it to be linked as a child to a parent object already in the store. This indicates to the content addressable store that the child object is a modification of the parent. A single parent may have multiple children with different modifications as might be the case for example if an application s data were saved into the store regularly for some while then an early copy were restored and used as a new starting point for subsequent modifications. Temporal tree management operations and data models are described in more detail below.

Difference Engine can generate a summary of difference regions between two arbitrary objects in the store. The differencing operation is invoked via an API specifying the handles of two objects to be compared and the form of the difference summary is a sequence of callbacks with the offset and size of sequential difference sections. The difference is calculated by comparing two hashed representations of the objects in parallel.

Garbage Collector is a service that analyzes the store to find saved data that is not referenced by any object handle and to reclaim the storage space committed to this data. It is the nature of the content addressable store that much data is referenced by multiple object handles i.e. the data is shared between data objects some data will be referenced by a single object handle but data that is referenced by no object handles as might be the case if an object handle has been deleted from the content addressable system can be safely overwritten by new data.

Object Replicator is a service to duplicate data objects between two different content addressable stores. Multiple content addressable stores may be used to satisfy additional business requirements such as offline backup or remote backup.

These services are implemented using the functional modules shown in . The Data Hash module generates fixed length keys for data chunks up to a fixed size limit. For example in this embodiment the maximum size of chunk that the hash generator will make a key for is 64 KiB. The fixed length key is either a hash tagged to indicate the hashing scheme used or a non lossy algorithmic encoding. The hashing scheme used in this embodiment is SHA 1 which generates a secure cryptographic hash with a uniform distribution and a probability of hash collision near enough zero that no facility need be incorporated into this system to detect and deal with collisions.

The Data Handle Cache is a software module managing an in memory database that provides ephemeral storage for data and for handle to data mappings.

The Persistent Handle Management Index is a reliable persistent database of CAH to data mappings. In this embodiment it is implemented as a B tree mapping hashes from the hash generator to pages in the persistent data store that contain the data for this hash. Since the full B tree cannot be held in memory at one time for efficiency this embodiment also uses an in memory bloom filter to avoid expensive B tree searches for hashes known not to be present.

The Persistent Data Storage module stores data and handles to long term persistent storage returning a token indicating where the data is stored. The handle token pair is subsequently used to retrieve the data. As data is written to persistent storage it passes through a layer of lossless data compression in this embodiment implemented using zlib and a layer of optional reversible encryption which is not enabled in this embodiment.

For example copying a data object into the content addressable store is an operation provided by the object handle mapper service since an incoming object will be stored and a handle will be returned to the requestor. The object handle mapper reads the incoming object requests hashes to be generated by the Data Hash Generator stores the data to Persistent Data Storage and the handle to the Persistent Handle Management Index. The Data Handle Cache is kept updated for future quick lookups of data for the handle. Data stored to Persistent Data Storage is compressed and optionally encrypted before being written to disk. Typically a request to copy in a data object will also invoke the temporal tree management service to make a history record for the object and this is also persisted via Persistent Data Storage.

As another example copying a data object out of the content addressable store given its handle is another operation provided by the object handle mapper service. The handle is looked up in the Data Handle Cache to locate the corresponding data if the data is missing in the cache the persistent index is used once the data is located on disk it is retrieved via persistent data storage module which decrypts and decompresses the disk data and then reconstituted to return to the requestor.

The data for a content addressable object is broken up into chunks . The size of each chunk must be addressable by one content addressable handle . The data is hashed by the data hash module and the hash of the chunk is used to make the handle. If the data of the object fits in one chunk then the handle created is the final handle of the object. If not then the handles themselves are grouped together into chunks and a hash is generated for each group of handles. This grouping of handles continues until there is only one handle produced which is then the handle for the object.

When an object is to be reconstituted from a content handle the copy out operation for the storage resource pool the top level content handle is dereferenced to obtain a list of next level content handles. These are dereferenced in turn to obtain further lists of content handles until depth 0 handles are obtained. These are expanded to data either by looking up the handle in the handle management index or cache or in the case of an algorithmic hash such as run length encoding expanding deterministically to the full content.

This is a standard tree data structure and the module supports standard manipulation operations in particular Add adding a leaf below a parent which results in a change to the tree as between initial state and after add state and Remove removing a node and reparenting its children to its parent which results in a change to the tree as between after add state and after remove state .

The Add operation may be used whenever an object is copied in to the CAS from an external pool. If the copy in is via the Optimal Way for Data Backup or if the object is originating in a different CAS pool then it is required that a predecessor object be specified and the Add operation is invoked to record this predecessor successor relationship.

The Remove operation is invoked by the object manager when the policy manager determines that an object s retention period has expired. This may lead to data stored in the CAS having no object in the temporal tree referring to it and therefore a subsequent garbage collection pass can free up the storage space for that data as available for re use.

Note that it is possible for a single predecessor to have multiple successors or child nodes. For example this may occur if an object is originally created at time T and modified at time T the modifications are rolled back via a restore operation and subsequent modifications are made at time T. In this example state T has two children state T and state T.

Different CAS pools may be used to accomplish different business objectives such as providing disaster recovery in a remote location. When copying from one CAS to another CAS the copy may be sent as hashes and offsets to take advantage of the native deduplication capabilities of the target CAS. The underlying data pointed to by any new hashes is also sent on an as needed basis.

Garbage Collection navigates the tree in order to reduce the cost of the mark phase as described below

Replication to a different CAS pool finds a set of near neighbors in the temporal tree that are also known to have been transferred already to the other CAS pool so that only a small set of differences need to be transferred additionally

Optimal Way for data restore uses the temporal tree to find a predecessor that can be used as a basis for the restore operation. In the CAS temporal tree data structure children are subsequent versions e.g. as dictated by archive policy. Multiple children are supported on the same parent node this case may arise when a parent node is changed then used as the basis for a restore and subsequently changed again.

The CAS difference engine compares two objects identified by hash values or handles as in and produces a sequence of offsets and extents within the objects where the object data is known to differ. This sequence is achieved by traversing the two object trees in parallel in the hash data structure of . The tree traversal is a standard depth or breadth first traversal. During traversal the hashes at the current depth are compared. Where the hash of a node is identical between both sides there is no need to descend the tree further so the traversal may be pruned. If the hash of a node is not identical the traversal continues descending into the next lowest level of the tree. If the traversal reaches a depth 0 hash that is not identical to its counterpart then the absolute offset into the data object being compared where the non identical data occurs together with the data length is emitted into the output sequence. If one object is smaller in size than another then its traversal will complete earlier and all subsequent offsets encountered in the traversal of the other are emitted as differences.

As described under Garbage Collector is a service that analyzes a particular CAS store to find saved data that is not referenced by any object handle in the CAS store temporal data structure and to reclaim the storage space committed to this data. Garbage collection uses a standard Mark and Sweep approach. Since the mark phase may be quite expensive the algorithm used for the mark phase attempts to minimize marking the same data multiple times even though it may be referenced many times however the mark phase must be complete ensuring that no referenced data is left unmarked as this would result in data loss from the store as after a sweep phase unmarked data would later be overwritten by new data.

The algorithm employed for marking referenced data uses the fact that objects in the CAS are arranged in graphs with temporal relationships using the data structure depicted in . It is likely that objects that share an edge in these graphs differ in only a small subset of their data and it is also rare that any new data chunk that appears when an object is created from a predecessor should appear again between any two other objects. Thus the mark phase of garbage collection processes each connected component of the temporal graph.

Next node is processed. As it shares an edge with node which has been marked the difference engine is applied to the difference between the object referenced by and the object referenced by obtaining a set of depth 0 depth 1 and higher hashes that exist in the unmarked object but not in the marked object. In the figure the hash that exists in node but not in node is H so H is marked as referenced. This procedure is continued until all edges are exhausted.

A comparison of the results produced by a prior art algorithm and the present embodiment shows that when node is processed by the prior art algorithm previously seen hashes H and H are emitted into the output stream along with new hash H. Present embodiment does not emit previously seen hashes into the output stream resulting in only new hashes H H H H H being emitted into the output stream with a corresponding improvement in performance. Note that this method does not guarantee that data will not be marked more than once. For example if hash value H occurs independently in node it will be independently marked a second time.

Copying an object from another pool into the CAS uses the software modules described in to produce a data structure referenced by an object handle as in . The input to the process is a a sequence of chunks of data at specified offsets sized appropriately for making depth 0 handles and optionally b a previous version of the same object. Implicitly the new object will be identical to the previous version except where the input data is provided and itself differs from the previous version. The algorithm for the copy in operation is illustrated in a flowchart at .

If a previous version b is provided then the sequence a may be a sparse set of changes from b . In the case that the object to be copied and is known to differ from a previous object at only a few points this can greatly reduce the amount of data that needs to be copied in and therefore reduce the computation and i o activity required. This is the case for example when the object is to be copied in via the optimal way for data backup described previously.

Even if the sequence a includes sections that are largely unchanged from a predecessor identifying the predecessor b allows the copy in procedure to do quick checks as to whether the data has indeed changed and therefore to avoid data duplication at a finer level of granularity than might be possible for the difference engine in some other storage pool providing input to a CAS.

Implicitly then the new object will be identical to the previous version except where the input data is provided and itself differs from the previous version. The algorithm for the copy in operation is illustrated in a flowchart at .

The process starts at step as an arbitrarily sized data object in the temporal store is provided and proceeds to which enumerates any and all hashes depth 0 through the highest level referenced by the hash value in the predecessor object if such is provided. This will be used as a quick check to avoid storing data that is already contained in the predecessor.

At step if a predecessor is input create a reference to a clone of it in the content addressable data store temporal data structure. This clone will be updated to become the new object. Thus the new object will become a copy of the predecessor modified by the differences copied into the CAS from the copying source pool.

At steps the Data Mover pushes the data into the CAS. The data is accompanied by an object reference and an offset which is the target location for the data. The data may be sparse as only the differences from the predecessor need to be moved into the new object. At this point the incoming data is broken into depth 0 chunks sized small enough that each can be represented by a single depth 0 hash.

At step read the predecessor hash at the same offset. If the hash of the data matches the hash of the predecessor at the same offset then no data needs to be stored and the depth 1 and higher objects do not need to be updated for this depth 0 chunk. In this case return to accept the next depth 0 chunk of data. This achieves temporal deduplication without having to do expensive global lookups. Even though the source system is ideally sending only the differences from the data that has previously been stored in the CAS this check may be necessary if the source system is performing differencing at a different level of granularity or if the data is marked as changed but has been changed back to its previously stored value. Differencing may be performed at a different level of granularity if for example the source system is a snapshot pool which creates deltas on a 32 KiB boundary and the CAS store creates hashes on 4 KiB chunks.

If a match is not found the data may be hashed and stored. Data is written starting at the provided offset and ending once the new data has been exhausted. Once the data has been stored at step if the offset is still contained within the same depth 1 object then depth 1 depth 2 and all higher objects are updated generating new hashes at each level and the depth 0 depth 1 and all higher objects are stored at step to a local cache.

However at step if the amount of data to be stored exceeds the depth 1 chunk size and the offset is to be contained in a new depth 1 object the current depth 1 must be flushed to the store unless it is determined to be stored there already. First look it up in the global index . If it is found there remove the depth 1 and all associated depth 0 objects from the local cache and proceed with the new chunk .

At step as a quick check to avoid visiting the global index for each depth 0 depth 1 and higher object in the local cache lookup its hash in the local store established in . Discard any that match.

At step for each depth 0 depth 1 and higher object in the local cache lookup its hash in the global index . Discard any that match. This ensures that data is deduplicated globally.

At step store all remaining content from the local cache into the persistent store then continue to process the new chunk.

Reading an object out of the CAS is a simpler process and is common across many implementations of CAS. The handle for the object is mapped to a persistent data object via the global index and the offset required is read from within this persistent data. In some cases it may be necessary to recurse through several depths in the object handle tree.

As described under the Replicator is a service to duplicate data objects between two different content addressable stores. The process of replication could be achieved through reading out of one store and writing back into another but this architecture allows more efficient replication over a limited bandwidth connection such as a local or wide area network.

A replicating system operating on each CAS store uses the difference engine service described above together with the temporal relationship structure as described in and additionally stores on a per object basis in the temporal data structure used by the CAS store a record of what remote store the object has been replicated to. This provides definitive knowledge of object presence at a certain data store.

Using the temporal data structure it is possible for the system to determine which objects exist on which data stores. This information is leveraged by the Data Mover and Difference Engine to determine a minimal subset of data to be sent over the network during a copy operation to bring a target data store up to date. For example if data object O has been copied at time T from a server in Boston to a remote server in Seattle Protection Catalog Store will store that object O at time T exists both in Boston and Seattle. At time T during a subsequent copy from Boston to Seattle the temporal data structure will be consulted to determine the previous state of object O in Seattle that should be used for differencing on the source server in Boston. The Boston server will then take the difference of T and T and send that difference to the Seattle server.

The process to replicate an object A is then as follows Identify an object A that is recorded as having already been replicated to the target store and a near neighbor of A in the local store. If no such object A exists then send A to the remote store and record it locally as having been sent. To send a local object to the remote store a typical method as embodied here is send all the hashes and offsets of data chunks within the object query the remote store as to which hashes represent data that is not present remotely send the required data to the remote store sending the data and hashes is implemented in this embodiment by encapsulating them in a TCP data stream .

Conversely if A is identified then run the difference engine to identify data chunks that are in A but not in A. This should be a superset of the data that needs to be sent to the remote store. Send hashes and offsets for chunks that are in A but not in A. Query the remote store as to which hashes represent data that is not present remotely send the required data to the remote store.

The Host Agent software implements some of the application specific module described above. It executes on the same servers as the application whose data is under management.

The DMV server software implements the remainder of the system as described here. It runs on a set of Linux servers that also provide highly available virtualized storage services.

These software components communicate with one another via network connections over an IP network . Data Management Virtualization systems communicate with one another between primary site and data replication DR site over an IP network such as a public internet backbone.

The DMV systems at primary and DR sites access one or more SAN storage systems via a fibre channel network . The servers running primary applications access the storage virtualized by the DMV systems access the storage via fibre channel over the fibre channel network or iSCSI over the IP network. The DMV system at the remote DR site runs a parallel instance of DMV server software on Linux server . Linux server may also be an Amazon Web Services EC2 instance or other similar cloud computational resource.

The host computer shown in may serve as an administrative workstation or may implement the application and Application Specific Agent or may implement any and all logical modules described in this specification including the Data Virtualization System itself or may serve as a storage controller for exposing storage pools of physical media to the system. Workstations may be connected to a graphical display device and to input devices such as a mouse and a keyboard . Alternately the active user s workstation may include a handheld device.

Throughout this specification we refer to software components but all references to software components are intended to apply to software running on hardware. Likewise objects and data structures referred to in the specification are intended to apply to data structures actually stored in memory either volatile or non volatile. Likewise servers are intended to apply to software and engines are intended to apply to software all running on hardware such as the computer systems described in .

A data fingerprint is a short binary digest of a data object that may be generated independently regardless of how the data object is stored and is identical when generated multiple times against identical input data with identical parameters. Useful properties for the fingerprint are that it be of fixed size that it be fast to generate for data objects in all storage pools and that it be unlikely that different data objects have identical fingerprints.

A data fingerprint is different from a checksum or a hash. For example a fingerprint is taken for only a sample of the object not the whole object. Obtaining a binary digest of a small percentage of the data object is sufficient to provide a fingerprint for the whole data object. Since a data fingerprint only requires reads and computes on a small percentage of data such fingerprints are computationally cheap or efficient compared to a checksum or hash.

These data fingerprints are also different in that a single data object may have multiple fingerprints. Over the life of a data object multiple fingerprints are stored with the object as metadata one per generation of the data object. The multiple fingerprints persist over multiple copies and generations of the data object.

Data fingerprints may be used to compare two objects to determine whether they are the same data object. If the data fingerprints for two objects differ the two objects can definitively be said to be different. As with checksums data fingerprints may thus be used to provide a measure or test of data integrity between copied or stored versions of a data object. Two data objects with the same data fingerprint may not necessarily be the same object.

As multiple fingerprints are taken of an object data fingerprints may be used to compare two objects with increasing reliability. A fingerprint match on a subsequent revision increases confidence that all the previous copies were accurate. If a fingerprint does not match this indicates that either this copy or previous copies were not accurate. With each next generation of the copy a new fingerprint may be computed and validated against the corresponding fingerprint for that generation or revision.

If two data objects are compared by comparing their corresponding data fingerprints and the corresponding fingerprints do not match it is possible to conclude with certainty that the two data objects are different. However if the corresponding fingerprints do match it is not possible to conclude that the corresponding data objects are necessarily identical. For example given two data objects that represent a digital photograph or image data taking a data fingerprint may include taking a checksum or binary digest of a portion of each image. Comparing the two data objects based on a single portion of each image would not necessarily indicate that they are the same image. However if multiple portions of the two images are identical it is possible to conclude with increased certainty that the two images are the same image.

The calculation of a data fingerprint may require a selection function which may be dynamic that selects a subset or portion of the input data object. Any such function may be used one specific example is described below in connection with certain embodiments. The function may select small portions of the data object that are spread out throughout the entirety of the data object. This strategy for selecting portions of data is useful for typical storage workloads in which large chunks of data are often modified at one time by selecting a relatively large number of non contiguous portions or extents of data that are widely distributed within the data object the selection function increases the probability that a large contiguous change in the data object may be detected. The function may change over time or may base its output on various inputs or parameters.

The choice of a selection function should ideally be done with an awareness of the content of a data object. Portions of the data object that are likely to change from generation to generation should be included in the fingerprint computation. Portions of the data object that are static or tend to be identical for similar objects should not be included in the fingerprint. For example disk labels and partition tables which tend to be static should not generally be included in the fingerprint since these would match across many generations of the same object. The tail end of a volume containing filesystems often tend to be unused space this area should not be used in the computation of the fingerprint as it will add computational and IO cost to the fingerprint without increasing its discriminating value.

It is apparent that as the total size of the subset selected by the selection function increases the probability that the data fingerprint captures all changes to the data increases until the subset is equal to the whole data object at which time the probability is 1. However the selection function may balance the goal of increased probability of detecting changes with the goal of providing a consistently fast fingerprinting time. This tradeoff is expressly permitted as the disclosed system allows for multiple data fingerprints to be taken of the same data object. Multiple fingerprints can provide the increased error checking probability as well as when the number of fingerprints becomes large the number of un checked bytes in the data object decreases to zero.

A data fingerprinting function may operate as follows in some embodiments. A data object is any file stored within any virtual storage pool for example a disk image stored as part of a data protection or archiving workflow. Start is a number representing an offset or location within the file. Period is a number representing a distance between offsets within the file. Data Sample is a subset of data from within the data object. Chunk checksums are the result of specific arithmetic checksum operations applied to specific data within the file. The data fingerprint is a single numerical value derived deterministically from the content of the data object and the parameters start and period . Other parameters and other parametrized functions may be used in certain embodiments.

The data samples are broken into fixed length chunks in this illustration 4 KB. For each chunk a chunk checksum is calculated for the data stream where the checksum includes the data in the chunk and the SHA 1 hash of the data in the chunk. One checksum algorithm used is the fletcher 32 method http en.wikipedia.org wiki Fletcher s checksum . These chunk checksums are then added together modulo 2 and the arithmetic sum of the chunk checksums is the data fingerprint parameterized by Start and Period. Other methods for combining the plurality of hash values or checksums into a single hash value may be contemplated in certain embodiments of the invention. A single hash value is preferred for simplicity. It is not necessary for the single hash value to reveal which data subsets were used in producing the chunk checksums.

In other embodiments a data fingerprint may be performed using other functions that focus on interesting sections of a data object where certain sections are determined to be interesting using various means. Interesting sections may be sections that are determined to change frequently or that are likely to change frequently. A priori information about the content of the data object or the frequency of change of parts of the data object may be used. For example when the system detects that a data object is a disk image the system may ignore the volume partition map as the partition map rarely changes. As another exanple if the system knows that it is storing a Microsoft Word document and that the headers of the document are unlikely to change it may designate the body and text areas of the document as interesting and may choose to fingerprint those areas. Fingerprinting an interesting area may be performed in a manner similar to in some embodiments where the data samples are chosen by first identifying interesting data areas and then identifying areas to sample within the interesting data areas using an algorithm that generates a sparse subset of the interesting data areas.

In a preferred embodiment the described fingerprinting algorithm has a very small overhead and thus fingerprinting may be performed often. However in cases such as when a pool includes offline tapes fingerprinting all data may not have a reasonable overhead.

In addition to the operations described above for the Object Manager an additional operation is defined that of generating a fingerprint for a data object given a set of parameters operation . Every data object that is cataloged is fingerprinted and the fingerprint is stored with all other metadata.

When an object is cataloged Object Manager may make a request for a fingerprint on a data object to each pool. The first fingerprint is generated at the first storage optimized pool or snapshot pool and stored in the catalog store. After a data object is first copied into the Performance Optimized Pool using the lightweight snapshot operation the data movement requestor generates a set of parameters for a fingerprint and uses them to request a fingerprint operation from the object manager . In turn the object manager requests a fingerprint from the performance optimized pool operation . The performance optimized pool is capable of generating the fingerprint. In a preferred embodiment every pool managed by pool manager is capable of generating a fingerprint. The new fingerprint is stored into the protection catalog store along with other metadata for the object as described above operation .

After any subsequent copy request operation such as copy to capacity optimized pool operation the fingerprint is requested from the target pool for the target object operation operation . Once generated the stored fingerprint is then passed on to each subsequent pool where the newly calculated fingerprint is then verified against the stored fingerprint to assure that copying errors have not occurred. Each subsequent pool may calculate the fingerprint again and validate the calculated fingerprint against the stored fingerprint.

To generate a fingerprint the data object is sampled at regular intervals defined by Start and Period parameters. Each sample is a fixed size in this illustration 64 KB. In one embodiment the parameter Period is chosen such that it is approximately 1 1000 of the size of the data object and Start is chosen between 0 and Period according to a pseudo random number generator.

For each new revision or generation of the data object the start parameter may be modified resulting in a data fingerprint of a different region of the data object. The object size however changes only in certain circumstances. If the object size stays constant the period stays constant. If the object size changes the period will change as well. A period of 1 1000 0.001 or another small fraction may be selected to ensure that calculating a fingerprint will take a small time and or a constant time. Note that depending on the function used to generate the subset of the data object used for the data fingerprinting operation other parameters may be modified instead of the start parameter. The result is to cause the data fingerprint to be generated from a different region of the data object such that cumulative data fingerprints result in fingerprinting of an increasing proportion of the data object over time.

Multiple generations of a data object may be created as a result of interactions with service level agreements SLAs as described elsewhere in the present disclosure. For example given a SLA that schedules a snapshot operation once every hour an additional generation of a data object will be created every hour. For each additional generation a new data fingerprint is created and sent. If the data object has not changed from the previous generation to the current generation the data itself need not be sent but a fingerprint is sent to the target data pool regardless to incrementally increase the probability that the sparse data fingerprinting operation has captured all changes to the data throughout the data object.

As different storage pools may support different operations the fingerprint operation may be supported by one or more storage pools in the system. The pools are brokered by the operation manager such as Pool Request Broker . In a preferred embodiment the fingerprint operation is supported by all pools.

Fingerprinting remains with the metadata for the lifetime of the data object. This allows fingerprinting to also be used during restore as well as during copy or other phases of data storage access and recovery which provides true end to end metadata from a data perspective. Fingerprinting during restore is performed as follows. When a restore operation is requested by Object Manager a fingerprint operation may take place on the restored data. This fingerprint operation may take place before or after the restore operation. By using the fingerprint operation all previously stored revisions of the data object are used to verify the currently restored copy of the data according to the fingerprint verification method described above. This leverages incremental knowledge in a way different from that of typical I O path CRC protection.

As disclosed above each copy of an object between virtual storage pools is incremental transferring only data from the source object known to be absent in the target pool. It follows from this that any errors in copying in one generation of an object will still be present in subsequent generations. Indeed such errors may be compounded. The use of a data fingerprint provides a check that copies of an object in different virtual storage pools have the same data content.

The choice of data fingerprint method also controls the level of confidence in the check as the Period is made smaller the cost of generating the fingerprint goes up as more data needs to be read from the pool but the chance of generating a matching fingerprint despite the data containing copying errors decreases.

However for successive generations of a single object different values may be used for the parameter Start . This ensures that with repeated copying of successive generations of single object the chance that any copying error might not be eventually caught reduces asymptotically to zero.

When data management virtualization is used it becomes possible to separate different operations required during of a data object s lifecycle between pools thereby utilizing the best qualities of each pool to provide superior performance. Just as objects are stored in multiple places because each pool has different characteristics other operations may also be based on the best characteristics of each place the data is stored. In the method described below if it is faster to transfer data out of a given pool that pool can be used to transfer the data. This is effectively performing a type of multiplexing based on attributes of the pool. Different pools may be selected based on the pool that is best for the specific situation.

The procedure described below is called hybrid seeding and is named according to the fact that the data is copied or seeded to a plurality of storage pools which are later used to copy the data to another pool. The greatest value is when there is a large amount of data to be transferred. However the cost for performing this method is low so this method may be used for both large and small transfers.

Performance Optimized Pool is a virtual storage pool with the property that retrieving data and metadata from an object in the pool is quick for example a snapshot pool.

Capacity Optimized Pool is a virtual storage pool with slower data and metadata retrieval characteristics for example a content addressable store. Target Pool is a virtual storage pool that is a target for copying data from pool for example a content addressable store on a remote system. Each pool has different performance characteristics the capacity optimized pool performs well at performing differences and the performance optimized pool performs well at retrieving bulk data. If one pool performs better than another pool at a particular task it may be said to have relatively high performance at that task relative to the other pool.

Object A is a data object that has previously been copied into storage pool . Object A is the result of a previous copy of object A into storage pool . Object B is a data object in storage pool that is the result of changes to an object A that is it is a newer generation or version of A. Object B is the result of a previous copy of object B into storage pool . Object B is the intended result of a copy of object B into target pool .

Object Manager not shown acts as a controller to direct a command to copy data objects to the data stores which contain the data objects. In this case the copy operation for Object B being copied from capacity optimized pool to capacity optimized pool is optimized using hybrid seeding.

In operation the Difference Engine is instructed by the Object Manager to compute differences between objects A and B prior to copying the differences to target pool to be applied to an object there. This results in the execution of logic that determines that storage pool is capable of higher performance differencing than storage pool and therefore that storage pool should perform the differencing operation. This logic may reside in storage pool or may also reside in Object Manager or may reside elsewhere in the storage virtualization system. Since A has previously been copied to storage pool as A and since B has previously been copied from storage pool to pool as B the result of operation is also a delta of object A and B. This delta or difference set may be characterized as a type of differences specification.

In operation the delta of A and B not shown is requested by the Object Manager to be copied from storage pool to storage pool to be applied to object A in accordance with differences computed previously at storage pool . The copy may be performed via direct connection between storage pool and storage pool in some embodiments. This division of the differencing operation and the bulk data copy operation results in a higher performance differencing operation at capacity optimized pool than would have been possible at performance optimized pool . The division also results in a higher performance copy from performance optimized pool than would have been possible had the copy been performed from capacity optimized pool .

In certain embodiments the logic for each storage pool may be provided by a single centralized controller in which case the messaging described here may occur within the controller in other embodiments logic for one or more storage modules may be executed on computing resources at the storage pools themselves. Copy operations may be requested by the Data Mover or by capacity optimized pool or other pools in some embodiments.

This completes the copy of all data that has changed since A from pool to pool and results in objects B B and B being synchronized between pools and .

This method is applicable at least under the following preconditions Object A has identical copies in two pools copy A in a capacity optimized pool and copy A in another target capacity optimized pool . These may for example be the result of previously copying an object A from storage pool via pool to pool . Object B is a newer version of A and has identical copies in two pools copy B in the first performance optimized pool and copy B in the capacity optimized pool . For example B has been copied already from pool to pool as a backup. And Object B is to be copied from the first capacity optimized pool to the second the target pool . The retrieval time for data from the first pool is much better than that for the second pool . For example pool is based on enterprise class primary storage while pool is a lower cost or higher latency device more suited to archiving or backup.

This method improves the overall object copy time from the second pool to the third pool. As described previously the copy is executed by invoking the differencing engine in pool to provide a set of differences between A and B. Specifically the differences generated by the difference engine are a description of the changed sections of the object not the data themselves. For example the differences may include a set of offset length pairs describing extents within the data object that have changed.

In prior implementations the differences between sections would be generated by reading from object B and object B and then applying the differences to object B to achieve the required copy. However in the method now illustrated the differences are generated by the difference engine in pool but the data are read from the object in pool that is the sections are read from object B and B and then applied to object B to achieve the required copy.

Since one precondition was that retrieval time from pool be better than from pool this affords improved copy rate.

A similar method may also be used during restore operations. When performing a restore differencing and bulk data copy may be separated and bulk copying may be performed from the data store that is fastest rather than from the store that performs the differencing operation.

In another general sense this separation of operations by pool constitutes a hybridization of operations. Different operations are performed at different storage pools and these operations are combined into a single operation by the virtualization layer. Further such applications may exist. For example comparing two objects in the performance pool may be a difficult task but the comparison operations may be performed at two content addressable pools containing the same data objects.

Replication and failover are well understood operations in which business logic and data are maintained at a hot backup at a remote site. Failover transfers operation over to the remote site. Replication transfers data over to the remote site. A new replication method is described below by which a pipeline of storage pools is used in conjunction with data management virtualization to reduce the amount of data that gets transferred. This method also enables bidirectional continuous deduplicated replication.

Business continuity and disaster recovery are well established practices in the IT industry. For operation they depend on having data from a primary location replicated to a secondary location regularly. The replica at the secondary location must be consistent that is it must represent a state of the data as it was at some moment in time at the primary location such that it can be used to start a secondary application server at the secondary location. The replica needs to be quick to access so that a secondary application server can be started up using the data very quickly in the event of the primary application server becoming unavailable. And the replica needs to be low latency i.e. when data is made available to an application server at the secondary location it should represent a consistency point at a time on the primary server that is as recent as possible typically measured in minutes.

In addition to replication a sync back operation may also be supported. Sync back is the operation that is performed when data is transferred back from the backup site to the main site. In other words the direction of the data copy arrow points in the opposite direction. Sync back supports and enables fail back which is an operation that reverses the fail over and transfers operation back to the primary server.

It is necessary to be able to perform multiple sync backs because typically business applications keep running while the sync is occurring. It is therefore necessary for the sync back to be efficient.

Primary Application Server is a server at location representative of one or more such servers delivering business services that may be consumed locally or remotely over some network interface.

Standby Location is a location where the business may alternatively supply application or data services in some event that causes disruption to the availability of service at the primary location. For example a power outage at the primary location might cause a web server to be unavailable in which case a web server at the backup location could be configured to respond to the requests that would normally be directed to the primary location.

Primary Data A is the live data being read and written for the operation of the primary application server. This might for example include a database that is servicing transactions for a web application interface. In a preferred embodiment this is a LUN exported to a the primary application server over a storage network. In another preferred embodiment this is a disk image for a virtual machine.

At primary location Primary Pool is the primary storage resource pool from which storage for the operation of the primary application server is allocated. This would typically be an enterprise class SAN storage array. Performance Optimized Pool is a storage pool for data protection as described previously which supports the lightweight snapshot operation and differencing. In a preferred implementation this is a snapshot pool based on low cost networked storage. Capacity Optimized Pool is a storage pool that supports the differencing operation. In a preferred implementation this is a deduplicating content addressable store.

At standby location Capacity Optimized Pool is a storage pool at the standby location that in turn supports the differencing operation. Again in a preferred implementation this is a deduplicating content addressable store. Performance Optimized Pool is a storage pool at the standby location that has faster access times than is typically the case for a content addressable store. In a preferred implementation this is a snapshot pool based on low cost networked storage. Primary Pool is a storage pool from which storage can be allocated for execution of a standby application server. This could for example be an enterprise class SAN storage array.

Copies A A A A A of the primary data object A are exact data copies to be created within each of the storage pools as described. The sequential copy operations will be described below in greater detail. These operations are issued by the Service Policy Engine and brokered to each storage pool by the Object Manager as described previously. Logic implementing the Service Policy Engine Object Manager and other controllers may be implemented in a centralized server or may be distributed across the network.

The purpose of this method of combining operations and components as described previously is to meet these goals for business continuity while reducing the load on a network connection and not requiring any dedicated network bandwidth for business continuity in addition to replication for data protection. Stated differently the purpose may be to provide the effect of asynchronous mirroring of data from a primary location to a second location.

The Service Policy Engine is responsible for marshalling operations in sequence such that virtual copies of Primary Data A are created in each pool in the sequence. As the Service Policy Engine issues copy requests to the Object Manager the Object Manager brokers these requests to lightweight snapshot or efficient incremental copy operations between pairs of pools. Thus the first operation executed is to make a lightweight snapshot of the current state of Primary Data A the second operation executed is to copy just the changed extents within Snapshot A into the Capacity Optimized Pool generating a new content addressed object A the third operation is to use efficient replication between content addressable stores to generate a new content addressed object A with minimized data transfer due to data deduplication in the first capacity optimized pool the fourth operation is to apply just the changes between A and its previous revision in order to update a previous object in the second Performance Optimized Pool to A.

At the time the virtual A has been completely defined and in a preferred implementation verified using a fingerprint matching mechanism as described previously some previous revisions of the same object may now be removed from the second Performance Optimized Pool and from other intermediary pools. In the preferred implementation the latest revision and one revision back are retained while older revisions are removed. This allows the differencing engine implementations in each pool to find adequately close matches each time a new virtual copy is to be transferred.

In some embodiments capacity optimized pool at the standby location may receive metadata from capacity optimized pool at the primary location and data from performance optimized pool at the primary location thus providing faster throughput for the bulk data transfer. In other embodiments capacity optimized pool may receive data from the primary location and may immediately send it to performance optimized pool . In other embodiments one or both of capacity optimized pool or performance optimized pool at the primary location may send the metadata and data to the remote location with both capacity optimized pool and performance optimized pool as the destination. Designating performance optimized pool as the destination and immediately copying the data to performance optimized pool provides updated data to the remote performance pool in as short a time as possible reducing the time window between backups during which data loss can occur and allowing the remote failover location to resume operation with as little lost data as possible.

Since the remote performance optimized pool always has at least one complete older copy of the data on the original system access to data on the remote pool can be provided near instantaneously. The data on the remote site does not depend on the data at the local site and is stored in a native format that is readily usable by the business application. In the event of a failover the data on the remote performance optimized pool is available within a finite length of time and the length of time is independent of the size of data stored and of the latency or availability of the data link between the local site and the remote site. Since data is stored in a native format at the remote site it is possible to copy data directly in a native format between the local site and the remote site in some embodiments.

In a preferred implementation these virtual copy operations are scheduled to execute successively or serially with a delay or with no delay between them the entire sequence to be initiated at a regular interval which may be selected by an operator to vary from minutes to hours.

In another implementation the operations are pipelined tightly so operations near the start of the sequence overlap operations near the end of the sequence. This reduces latency while incurring the greatest resource consumption.

In other implementations the system operates in a parallel fashion so that multiple operations may occur simultaneously.

In the event that the Primary Application Server becomes damaged or otherwise unavailable the standby application server must be brought into operation with known good data from the primary side that is as recent as possible. In the mechanism described here a virtual copy is made of the most recent data object in the standby Performance Optimized Pool to obtain a data object which can once again be used as a disk image or logical unit number LUN that may be referenced and modified by the standby application server. In the preferred implementation the virtual copy is made into a standby primary pool using the lightweight copy operation.

Typically the standby site is not as well provisioned with resources or connectivity as the primary site so once the primary site is available once more it is preferred that the business service in question be once again provided at the primary site. However any changes made to data while the secondary site was providing the business service must themselves be replicated back to the primary site.

In this mechanism this is achieved by a sequence of efficient virtual copy operations from the modified version of A thru to a new version of primary data A thus 

B a space efficient snapshot of B the modified version of A is made in the standby performance optimized pool .

B a content addressed object in the standby capacity optimized pool is made by efficient copy of changed extents only in B.

B a copy of B is made in the primary capacity optimized pool by minimal transfer of deduplicated data.

Bi in the primary performance optimized pool is generated efficiently by efficient copy using the difference engine.

Bi is then available for an operator to restore to the primary application server in a variety of ways. In a preferred implementation a virtual copy of B is made using a lightweight snapshot operation and exposed as a LUN or disk image to the primary application server. In another implementation the data from B is fully copied over LUN or disk image A. In another implementation a virtual copy of B is made using a lightweight snapshot operation and exposed as a LUN or disk image to a new primary application server which may be a physical server or virtual machine.

This method effectively allows the failover replication site primary pool to be a high performance data store providing instant access or very short access time and efficient and recent transfer of primary pool data while still providing the benefits of the data management virtualization system. The optimization is enabled by the use of the intelligent deduplication and other methods described above.

The foregoing has outlined some of the more pertinent features of the subject matter. These features should be construed to be merely illustrative. Many other beneficial results can be attained by applying the disclosed subject matter in a different manner or by modifying the subject matter as will be described.

