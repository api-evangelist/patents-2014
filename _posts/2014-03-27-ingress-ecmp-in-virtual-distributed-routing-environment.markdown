---

title: Ingress ECMP in virtual distributed routing environment
abstract: A logical routing element (LRE) having multiple designated instances for routing packets from physical hosts (PH) to a logical network is provided. A PH in a network segment with multiple designated instances can choose among the multiple designated instances for sending network traffic to other network nodes in the logical network according to a load balancing algorithm. Each logical interface (LIF) of an LRE is defined to be addressable by multiple identifiers or addresses, and each LIF identifier or address is assigned to a different designated instance.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09413644&OS=09413644&RS=09413644
owner: NICIRA, INC.
number: 09413644
owner_city: Palo Alto
owner_country: US
publication_date: 20140327
---
In a network virtualization environment one of the more common applications deployed on hypervisors are 3 tier apps in which a web tier a database tier and app tier are on different L3 subnets. This requires IP internet protocol packets traversing from one virtual machine VM in one subnet to another VM in another subnet to first arrive at a L3 router then forwarded to the destination VM using L2 MAC media access control address. This is true even if the destination VM is hosted on the same host machine as the originating VM. This generates unnecessary network traffic and causes higher latency and lower throughput which significantly degrades the performance of the application running on the hypervisors. Generally speaking this performance degradation occurs whenever any two VMs in two different network segments e.g. different IP subnet different L2 segments or different overlay logical networks communicate with each other.

U.S. patent application Ser. No. 14 137 862 filed on Dec. 20 2013 describes a logical router element LRE that operates distributively across different host machines as a virtual distributed router VDR . Each host machine operates its own local instance of the LRE as a managed physical routing element MPRE for performing L3 packet forwarding for the VMs running on that host. The LRE therefore makes it possible to forward data packets locally i.e. at the originating hypervisor without going through a shared L3 router.

Furthermore an LRE as described by U.S. patent application Ser. No. 14 137 862 not only performs L3 routing for VMs operating in host machines that operate the LRE but also performs L3 routing for physical routers hosts or other network nodes that do not operate the LRE. One particular host machine operating the LRE is selected as the designated host machine and its MPRE is the designated instance of the LRE for handling L3 routing of traffic from the physical routers.

In some embodiments a logical routing element LRE includes one or more logical interfaces LIFs that each serve as an interface to a corresponding segment of a logical network. Each network segment has its own logical interface to the LRE and each LRE has its own set of logical interfaces. In some embodiments at least one of the LIFs of a LRE is defined to be addressable by two or more identifiers e.g. IP addresses . Some embodiments allow each LIF identifier to serve as a destination address for network traffic. In some embodiments a network segments can encompass multiple IP subnets and a LIF interfacing such a network segment is addressable by IP addresses that are in different IP subnets. In some embodiments a network segment that is an overlay encapsulation network e.g. VXLAN or VLAN includes multiple IP subnets.

A physical host PH is a network node that belongs to a logical network but does not operate a local instance of the logical network s LRE. In some embodiments network traffic from a PH to a VM is routed by a designated host machine that does operate a local instance of the LRE i.e. MPRE . The local instance of the LRE running on such a designated host is referred as a designated instance or DI in some embodiments. In some embodiments a logical network or an LRE has multiple designated instances for some or all of the network segments. A PH in a network segment with multiple designated instances can choose among the multiple designated instances for sending network traffic to other network nodes in the logical network for load balancing purposes. In order to support multiple designated instances per network segment a corresponding LIF in some embodiments is defined to be addressable by multiple identifiers or addresses e.g. IP addresses where each LIF identifier or address is assigned to a different designated instance. In some embodiments each LIF identifier serves as a destination address for network traffic. Each designated instance DI assigned to a particular LIF identifier in turn handles network traffic for that particular assigned LIF identifier.

Some embodiments advertise the IP addresses of the LIF of that particular network segment as a list of available next hops. Once a list of designated instances is made available to a physical host the physical host is able to select any one of the designated instances as a next hop into the logical network. Such selection can be based on any number of criteria and can be made for any number of purposes. In some embodiments a physical host selects a designated instance as the next hop based on current network traffic information in order to balance the traffic load between the different designated host machines. In some embodiments a PH uses the list of designated instances to perform ECMP Equal Cost Multi path Routing algorithms on ingress network traffic to the logical network.

In some embodiments packets coming from physical hosts PHs rely on routing table entries in designated instances for routing. In some embodiments these entries are filled by address resolution protocols ARP initiated by PHs or by DIs themselves. In some embodiments a PH that has received a list of IP addresses as next hops performs ARP operation to translate the received L3 IP address into L2 MAC addresses in order to ascertain the PMAC addresses of the designated instances. In some embodiments the designated instances not only resolve IP addresses for packets that come from external PHs but also for packets coming from VMs running on host machines having a local instance of the LRE. The routing utilizes routing table entries in the available designated instances of a particular LIF.

In some embodiments each MPRE select a designated instance for requesting address resolution based on the destination IP address. Such address resolution requests and address resolution replies are UDP messages in some embodiments. In some embodiments an MPRE would make such an address resolution request to a designated instance that is associated with a LIF address that is in a same IP subnet as the destination IP address. In some embodiments each designated instance is for resolving IP addresses that are in the same subnet as its assigned LIF IP address. In some embodiments when a designated instance is not able to resolve a destination IP address upon receiving an address resolution request it will perform an ARP operation in order to resolve the unknown IP address.

The preceding Summary is intended to serve as a brief introduction to some embodiments of the invention. It is not meant to be an introduction or overview of all inventive subject matter disclosed in this document. The Detailed Description that follows and the Drawings that are referred to in the Detailed Description will further describe the embodiments described in the Summary as well as other embodiments. Accordingly to understand all the embodiments described by this document a full review of the Summary Detailed Description and the Drawings is needed. Moreover the claimed subject matters are not to be limited by the illustrative details in the Summary Detailed Description and the Drawings but rather are to be defined by the appended claims because the claimed subject matters can be embodied in other specific forms without departing from the spirit of the subject matters.

In the following description numerous details are set forth for the purpose of explanation. However one of ordinary skill in the art will realize that the invention may be practiced without the use of these specific details. In other instances well known structures and devices are shown in block diagram form in order not to obscure the description of the invention with unnecessary detail.

In some embodiments a logical routing element LRE includes one or more logical interfaces LIFs that each serve as an interface to a corresponding segment of the network. Each network segment has its own logical interface to the LRE and each LRE has its own set of logical interfaces. In some embodiments at least one of the LIFs of a LRE is defined to be addressable by two or more identifiers e.g. IP addresses . Some embodiments allow each LIF identifier to serve as a destination address for network traffic. In some embodiments a network segments can encompass multiple IP subnets and a LIF interfacing such a network segment is addressable by IP addresses that are in different IP subnets. In some embodiments a network segment that is an overlay encapsulation network e.g. VXLAN or VLAN includes multiple IP subnets.

For some embodiments conceptually illustrates a virtualized network environment that uses LREs to implement L3 packet forwarding between network nodes. As illustrated the virtualized network environment is a multi tenancy network environment that serves two different tenants X and Y and is implementing two different logical networks and for these two different tenants. The logical network includes segments A B C and D for tenant X while the logical network includes network segments E F G and H for tenant Y. Each segment includes one or more network nodes that are each labeled either as VM virtual machine or PH physical host . The logical network has a LRE for handling L3 routing between network segments A B C and D while the logical network has a LRE for handling L3 routing between network segments E F G and H.

In some embodiments the virtualized network environment is implementing the logical networks and over a virtualization infrastructure that includes several host machines interconnected by a physical network as described in more detail below. Some of these host machines are operating virtualization software or hypervisors that allow them to host one or more VMs. Some of these host machines are also operating local instances of the LREs as managed physical routing elements MPREs that allow the host machines to distributively perform L3 routing between network nodes in different network segments. Each MPRE i.e. a local instance of an LRE running on a host machine functions as the local physical router for the VMs operating on that host machine. Logical routing elements LRE or virtual distributed routers VDR are described in U.S. patent application Ser. No. 14 137 862 now published as U.S. Patent Publication 2015 0106804 which is hereby incorporated by reference.

Each network segment includes one or more individually addressable network nodes that consumes generates or forwards network traffic. In some embodiments a network segment is a portion of the network e.g. an IP subnet . In some embodiments a network segment is defined by a L2 logical switch and includes network nodes interconnected by that logical switch. In some embodiments a network segment is an encapsulation overlay network such as VXLAN or VLAN. Such a network segment can span multiple data centers and or include multiple IP subnets in some embodiments. In some embodiments a logical network can include different types of network segments e.g. a mixture of VLANs and VXLANs . In some embodiments network nodes in a same segment are able to communicate with each other by using link layer L2 protocols e.g. according to each network node s L2 MAC address while network nodes in different segments of the network cannot communicate with each other with a link layer protocol and must communicate with each other through network layer L3 routers or gateways.

As illustrated in some of these network nodes are virtual machines VM running on host machines while others are stand alone network nodes such as physical routers or physical host machines PH . For some embodiments a VM is a network node that is hosted by a host machine. Each of the host machines also operates a local instance of a LRE as its MPRE such that packets from the VMs can be routed locally at that host machine to other segments of its logical network. Conversely a PH is a network node that is not hosted by such a host machine. A PH does not have a local instance of a LRE to locally route its packet to other segments of its logical network. In some embodiments a PH belonging to a network segment uses a MPRE of another host machine i.e. an LRE instance local to another host machine for routing within the logical network. Routing for PH network nodes will be further described in Section II below.

The LREs and are the logical routers for the logical networks and respectively. The LRE handles routing only for the traffic of tenant X while the LRE handles routing only for the traffic of tenant Y. Consequently the network traffic of tenant X is entirely isolated in the logical plane from the network traffic of tenant Y although they may share physical resources as further described below.

As mentioned an LRE operates distributively across the host machines in its logical network as a virtual distributed router VDR where each host machine operates its own local instance of the LRE as a MPRE for performing L3 packet forwarding for the VMs running on that host. In the LRE LRE for tenant X is illustrated as encompassing MPREs while the LRE LRE for tenant Y is illustrated as encompassing MPREs . In other words each of the MPREs is a local instance of the LRE running on a different host machine for tenant X while each of MPRE is a local instance of the LRE running on a different host machine for tenant Y.

As illustrated each of LREs and includes a set of logical interfaces LIFs that each serves as an interface to a particular segment of the network. The LRE has LIF A LIF B LIF C and LIF D for handling packets to and from the network segments A B C and D respectively while the LRE has LIF E LIF F LIF G and LIF G for handling packets to and from the network segments E F G and H respectively. Each logical interface is assigned its own set of identifiers e.g. IP address or overlay network identifier that is unique within the network virtualization environment . For example LIF A of LRE assigned IP addresses 1.1.1.251 1.1.1.252 and 1.1.1.253 and LIF F is assigned IP addresses 4.1.2.251 4.11.2.252 and 4.11.2.253. Each of these LIF identifiers can serve as a destination address for network traffic in other words the multiple IP addresses or identifiers of a LIF allows the LIF to appear as multiple different network traffic destinations. For example in some embodiments each LIF IP address serves as an address of a default gateway or ARP proxy for network nodes of its particular network segment. Having multiple IP addresses per LIF provides the network nodes in the corresponding network segments a list of gateways or proxies to choose from.

In some embodiments a network segments can encompass multiple IP subnets and a LIF interfacing such a network segment is addressable by IP addresses that are in different IP subnets. In some embodiments a network segment that is an overlay encapsulation network e.g. VXLAN or VLAN includes multiple IP subnets. illustrates LIFs that interface network segments that include one or more IP subnets. Specifically illustrates LIFs A H of the LREs and and their corresponding network segments A H.

As illustrated some of the network segments e.g. network segments A and E include only one IP subnet. A LIF interfacing such a network segment have all of its LIF addresses in one IP subnet. For example the network segment A only includes network nodes in IP subnet 1.1.1.x and the LIF addresses for its corresponding LIF LIF A are also all in the IP subnet 1.1.1.x i.e. 1.1.1.251 1.1.1.252 1.1.1.253 . On the other hand some of the network segments include multiple IP subnets. For example the network segment B includes IP subnets 1.1.2.x and 1.1.12.x while the segment C includes IP subnets 1.1.3.x 1.1.13.x and 1.1.23.x. In some embodiments a LIF of a network segment also has LIF IP addresses in those multiple subnets of the network segments. For example LIF B has IP addresses in IP subnet 1.1.2.x 1.1.2.251 as well as in IP subnet 1.1.12.x 1.1.12.252 and 1.1.12.253 . In some of these embodiments network nodes in a particular IP subnet uses only LIF addresses in the same IP subnet when accessing the LIF. For example in some embodiments VMs in subnet 1.1.14.x of segment D uses only the addresses 1.1.14.252 or 1.1.14.253 to address LIF D but not 1.1.4.251 even though 1.1.4.251 is also an address of the same LIF.

In some embodiments the IP addresses of a LIF need not correspond exactly with the IP subnets in the LIF s network segment. For example a LIF may have an IP address that is not in any of the network segment s subnets e.g. the network segment E does not have IP subnet that encompasses the LIF address 4.10.1.253 in LIF E or a LIF may have a subnet that does not have at least one LIF address that is in that subnet e.g. LIF H does not have a LIF address in the subnet 4.1.14.x .

Several figures below e.g. use the IP address and network segment assignment of . One of ordinary skill would understand that the values of IP addresses and labels of network segments of are arbitrarily chosen for purposes of illustration and that the various embodiments described in those figures as well as other figures are entirely independent of the specific names or numerical values chosen.

Several more detailed embodiments of the invention are described below. Section I describes distributed routing using LREs in virtualized network environment. Section II describes various applications of a LIF that has multiple LIF identifiers. Section III describes the control and configuration of LRE. Finally section IV describes an electronic system with which some embodiments of the invention are implemented.

As mentioned some embodiments use logical routing elements LREs for routing packets between network nodes in different network segments. These LREs operate in a distributed manner across multiple host machines each of these host machines operating a local instance of the LRE as its managed physical routing element MPRE . In some embodiments each of these host machines is also operating a virtualization software or a hypervisor that allows it to host one or more virtual machines VMs and to provide network access to those VMs. In some embodiments the host machines running the LREs are in a network virtualization infrastructure over a physical network. Such a network virtualization infrastructure in some embodiments includes physical network nodes such as external edge routers that belong to a network segment that is served by one of the LREs and yet does not operate the LRE itself.

As illustrated the host machine is hosting VMs the host machine is hosting VMs and the host machine is hosting VMs . These VMs belong to different network segments. Namely the VM belongs to segment A the VM belong to segment B the VM belong to segment C the VMs belong to segment D the VMs belong to segment E the VMs and belong to segment F the VMs belong to segment G and the VMs belong to segment H.

Each host machine is operating two MPREs for the two different LREs and . Specifically the host machine is operating MPREs and the host machine is operating MPREs and and the host machine is operating MPREs and . The MPREs are local instances of the LRE operating in the host machines respectively for the logical network of tenant X. The MPREs are local instances of the LRE operating in the host machines respectively for the logical network of tenant Y.

A MPRE residing on a host machine has a set of LIFs i.e. the LIFs of the LRE for interfacing with the VMs operating on that host machine. For example the MPRE has LIFs A B C and D as the local instance of the LRE . The LIF A of the MPRE serves the VM a segment A VM the LIF B of MPRE serves the VM a segment B VM and the LIF C of MPRE serves the VM a segment C VM . As illustrated an MPRE of a LRE logical network may reside on a host machine that does not have VMs in all network segments and the MPRE therefore may have LIFs that are inactive. For example the host machine does not have a VM belonging to segment D and the LIF D of its MPRE is therefore not activated illustrated with dashed borders .

Each MPRE of a host machine handles the L3 routing of packets coming from the VMs that are served by the MPRE s LIFs. In other words each MPRE handles the L3 routing of the VMs belonging to network segments that form the logical network of its parent LRE. For example the MPRE performs L3 routing for VMs belonging to network segments A B C of the logical network while the MRPE performs L3 routing for VMs belonging to network segments E and G of the logical network .

Each host machine is also operating a managed physical switching element MPSE for performing L2 level switching between the VMs and the MPREs on that host machine. The MPSE of each host machine also has an uplink connection to the physical network so the VMs and the MPREs in the host machine can exchange packets with network nodes outside of the host machine e.g. VMs in other host machines and PHs over the physical network . For example packets can arrive at the MPSE of the host from the physical network through the uplink from one of the MPREs or or from one of the VMs . Packets that require L3 level routing are forwarded by the MPSE to one of the MPREs or and the routed packet are sent back to the MPSE to be forwarded to their L2 destination within the host machine or outside of the host machine reachable by the physical network .

In some embodiments all MPREs are addressable within its host machine i.e. by the MPSE of the host machine by a same virtual MAC address VMAC while each MPRE is addressable from network nodes outside of its host machine by a physical MAC address PMAC that uniquely identifies the MPRE. Such a PMAC in some embodiments distinguishes a MPRE operating in one host machine from another MPRE operating in another host machine even when those MPREs are instances of a same LRE. In some embodiments though MPREs of different tenants on a same host machine are addressable by a same MAC either VMAC or PMAC at the MPSE of the host machine the MPREs are able to keeps packets of different logical networks and of different clients separate by using network segment identifiers e.g. VNI VXLAN ID or VLAN tag or ID . For example the LIFs A B C and D of MPRE ensures that the MPRE receives only packets with identifiers for network segments A B C or D while the LIFs E F G and H of MPRE ensures that the MPRE receives only packets with identifiers for network segments E F G and H. The operations of MPSE are described in U.S. patent application Ser. No. 14 137 862.

Physical hosts PH are network nodes that though belonging to logical networks or do not operate a local instance of either the LRE or the LRE . Specifically the PH belongs to network segment A the PHs and belong to network segment C and the PH belong to network segment G. In some embodiments a PH is a physical host machine that does not run virtualization software at all and does not host any VMs. In some embodiments some physical host machines are legacy network elements such as filer or another non hypervisor non VM network stack built into the underlying physical network which used to rely on standalone routers for L3 layer routing. In some embodiments a PH is an edge router or a routing gateway that serves as an interface for the logical networks or with other external networks. In some embodiments such an edge router is a VM running on a host machine that operates hypervisor virtualization software but the host machine of the edge router does not operate an LRE for either logical network or . In order to perform L3 layer routing for these PH network nodes some embodiments designate one or more MPREs running in the host machines of the network virtualization infrastructure to act as a dedicated routing agent designated instance or designated MPRE for these PHs. In some embodiments L2 traffic to and from these PHs are handled by local instances of MPSEs in the host machines without having to go through a designated MPRE. Designated instances will be further described in Section II.a below.

In some embodiments a LRE operates within a virtualization software e.g. a hypervisor virtual machine monitor etc. that runs on a host machine that hosts one or more VMs e.g. within a multi tenant data center . The virtualization software manages the operations of the VMs as well as their access to the physical resources and the network resources of the host machine and the local instantiation of the LRE operates in the host machine as its local MPRE. For some embodiments illustrates a host machine running a virtualization software that includes a MPRE of an LRE. The host machine connects to e.g. other similar host machines through a physical network . This physical network may include various physical switches and routers in some embodiments.

As illustrated the host machine has access to a physical network through a physical NIC PNIC . The host machine also runs the virtualization software and hosts VMs . The virtualization software serves as the interface between the hosted VMs and the physical NIC as well as other physical resources such as processors and memory . Each of the VMs includes a virtual NIC VNIC for accessing the network through the virtualization software . Each VNIC in a VM is responsible for exchanging packets between the VM and the virtualization software . In some embodiments the VNICs are software abstractions of physical NICs implemented by virtual NIC emulators.

The virtualization software manages the operations of the VMs and includes several components for managing the access of the VMs to the physical network by implementing the logical networks to which the VMs connect in some embodiments . As illustrated the virtualization software includes several components including a MPSE a MPRE a controller agent a VTEP and a set of uplink pipelines .

The controller agent receives control plane messages from a controller or a cluster of controllers. In some embodiments these control plane message includes configuration data for configuring the various components of the virtualization software such as the MPSE and the MPRE and or the virtual machines. In the example illustrated in the controller agent receives control plane messages from the controller cluster from the physical network and in turn provides the received configuration data to the MPRE through a control channel without going through the MPSE . However in some embodiments the controller agent receives control plane messages from a direct data conduit not illustrated independent of the physical network . In some other embodiments the controller agent receives control plane messages from the MPSE and forwards configuration data to the router through the MPSE . The controller agent and the configuration of the virtualization software will be further described in Section III below.

The VTEP VXLAN tunnel endpoint allows the host to serve as a tunnel endpoint for logical network traffic e.g. VXLAN traffic . VXLAN is an overlay network encapsulation protocol. An overlay network created by VXLAN encapsulation is sometimes referred to as a VXLAN network or simply VXLAN. When a VM on the host sends a data packet e.g. an ethernet frame to another VM in the same VXLAN network but on a different host the VTEP will encapsulate the data packet using the VXLAN network s VNI and network addresses of the VTEP before sending the packet to the physical network. The packet is tunneled through the physical network i.e. the encapsulation renders the underlying packet transparent to the intervening network elements to the destination host. The VTEP at the destination host decapsulates the packet and forwards only the original inner data packet to the destination VM. In some embodiments the VTEP module serves only as a controller interface for VXLAN encapsulation while the encapsulation and decapsulation of VXLAN packets is accomplished at the uplink module .

The MPSE delivers network data to and from the physical NIC which interfaces the physical network . The MPSE also includes a number of virtual ports vPorts that communicatively interconnects the physical NIC with the VMs the MPRE and the controller agent . Each virtual port is associated with a unique L2 MAC address in some embodiments. The MPSE performs L2 link layer packet forwarding between any two network elements that are connected to its virtual ports. The MPSE also performs L2 link layer packet forwarding between any network element connected to any one of its virtual ports and a reachable L2 network element on the physical network e.g. another VM running on another host . In some embodiments a MPSE is a local instantiation of a logical switching element LSE that operates across the different host machines and can perform L2 packet switching between VMs on a same host machine or on different host machines.

The MPRE performs L3 routing e.g. by performing L3 IP address to L2 MAC address resolution on data packets received from a virtual port on the MPSE . Each routed data packet is then sent back to the MPSE to be forwarded to its destination according to the resolved L2 MAC address. This destination can be another VM connected to a virtual port on the MPSE or a reachable L2 network element on the physical network e.g. another VM running on another host a physical non virtualized machine etc. .

As mentioned in some embodiments a MPRE is a local instantiation of a logical routing element LRE that operates across the different host machines and can perform L3 packet forwarding between VMs on a same host machine or on different host machines. In some embodiments a host machine may have multiple MPREs connected to a single MPSE where each MPRE in the host machine implements a different LRE. MPREs and MPSEs are referred to as physical routing switching element in order to distinguish from logical routing switching elements even though MPREs and MPSE are implemented in software in some embodiments. In some embodiments a MPRE is referred to as a software router and a MPSE is referred to a software switch . In some embodiments LREs and LSEs are collectively referred to as logical forwarding elements LFEs while MPREs and MPSEs are collectively referred to as managed physical forwarding elements MPFEs .

In some embodiments the MPRE includes one or more logical interfaces LIFs that each serves as an interface to a particular segment of the network. In some embodiments each LIF is addressable by its own IP address and serve as a default gateway or ARP proxy for network nodes e.g. VMs of its particular segment of the network. As described in detail below in some embodiments all of the MPREs in the different host machines are addressable by a same virtual MAC address while each MPRE is also assigned a physical MAC address in order indicate in which host machine does the MPRE operate.

The uplink module relays data between the MPSE and the physical NIC . The uplink module includes an egress chain and an ingress chain that each performs a number of operations. Some of these operations are pre processing and or post processing operations for the MPRE . The operations of the uplink module are described in U.S. patent application Ser. No. 14 137 862.

As illustrated by the virtualization software has multiple MPREs from multiple different LREs. In a multi tenancy environment a host machine can operate virtual machines from multiple different users or tenants i.e. connected to different logical networks . In some embodiments each user or tenant has a corresponding MPRE instantiation in the host for handling its L3 routing. In some embodiments though the different MPREs belong to different tenants they all share a same vPort on the MPSE and hence a same L2 MAC address VMAC or PMAC . In some other embodiments each different MPRE belonging to a different tenant has its own port to the MPSE.

The MPSE and the MPRE make it possible for data packets to be forwarded amongst VMs without being sent through the external physical network so long as the VMs connect to the same logical network as different tenants VMs will be isolated from each other .

A MPRE running on a host machine allows L3 routing of packets between VMs running on a same host machine to be done locally at the host machine without having to go through the physical network. illustrates the use of MPREs for performing distributed L3 routing for VMs in different host machines. Specifically illustrates MPREs performing L3 routing between VMs in a same host machine and between VMs in different host machines.

As illustrated a physical network supports network communications between host machines the host machine is illustrated in . The host machines are operating MPREs respectively. The MPREs are local instances of a same LRE. Each MPRE has a corresponding routing table for MPREs respectively for mapping L3 IP addresses into L2 MAC addresses. The LRE and hence the MPREs have LIFs for network segments A B C and D. The host machine is hosting a VM which is a network node in network segment B. The host machine is hosting a VM which is a network node in network segment D. The host machine is hosting VMs and which are network nodes in network segments A and C respectively.

At operation 3 the MPRE realizes that destination address 1.1.4.4 is in a subnet in network segment D and therefore uses its LIF D to send out the packet with MAC4 as the destination MAC address. Though not illustrated the packet is forwarded out by an MPSE in the host machine . The MPSE recognizes that MAC4 is not in the host machine and sends it out to the physical network .

At operation 4 the packet reaches the host machine . Since the packet is already routed i.e. having a routed MAC address the MPSE of the host machine in operation 5 forward the packet to L2 address MAC4 i.e. the VM without going through the MPRE .

At operation 8 the MPRE realizes that destination address 1.1.3.3 is in a subnet belonging to network segment C and therefore uses its LIF C to send out the packet with MAC5 as the destination MAC address. Though not illustrated the packet is forwarded by an MPSE in the host machine . The MPSE recognizes that MAC5 is in the host machine so it forwards the packet directly to the VM without going through the physical network .

As mentioned a physical host PH is a network node that belongs to a logical network but does not operate a local instance of the logical network s LRE. In some embodiments network traffic from a PH to a VM is therefore routed by a designated host machine that does operate a local instance of the LRE i.e. MPRE . However in some embodiments the converse is not true. Namely network traffic from VMs to a PH is always routed locally in a distributed fashion by each host machine s own MPRE without relying on a designated host.

In operations 1 3 and 4 the MPREs are performing L3 routing operations since the PH is on a different network segment than the VMs and . The IP address of the PH is 1.1.2.10 which makes the PH part of network segment B. The IP address of VM is 1.1.1.1 which is in network segment A. The IP address of VM is 1.1.3.3 which is in network segment C. The IP address of VM is 1.1.4.4 which is in network segment D. Operation 2 on the other hand illustrates the forwarding of a packet to the PH from a VM that is in the same segment B as the PH the VM is at IP address 1.1.2.2 which is also in segment B . If the packet has already specified the destination MAC address i.e. MAC10 in some embodiments the MPSE of the host machine would directly forward the packet to the PH via the physical network without routing. If the destination MAC address is unknown the MPRE in some embodiments would perform a bridging operation to map the destination IP address 1.1.2.10 to the destination MAC address MAC10. MPREs performing bridging operations are described in U.S. patent application Ser. No. 14 137 862.

As mentioned a physical host PH is a network node that belongs to a logical network but does not operate a local instance of the logical network s LRE. In some embodiments network traffic from a PH to a VM is therefore routed by a designated host machine that does operate a local instance of the LRE i.e. MPRE . The local instance of the LRE running on such a designated host is referred as a designated instance or DI in some embodiments because it is a designated MPRE instance used to handle traffic from physical hosts that do not have their own MPREs.

In some embodiments a logical network or an LRE has multiple designated instances for some or all of the network segments. A PH in a network segments with multiple designated instances can choose among the multiple designated instances for sending network traffic to other network nodes in the logical network for say load balancing purposes. In order to support multiple designated instances per network segment a corresponding LIF in some embodiments is defined to be addressable by multiple identifiers or addresses e.g. IP addresses where each LIF identifier or address is assigned to a different designated instance. In some embodiments each LIF identifier serves as a destination address for network traffic. Each designated instance DI assigned to a particular LIF identifier in turn handles network traffic for that particular assigned LIF identifier.

The logical network also includes two PHs and . The PHs do not run their own local instances of the LRE and therefore rely on designated instances for L3 routing within the logical network . The IP address of the PH is 1.1.2.10 and the IP address of the PH is 1.1.2.11 which indicates that both PH and the PH are in the network segment B and interfaces the LRE by using LIF B.

In the example of the LIF B has three IP addresses 1.1.2.251 1.1.2.252 and 1.1.2.253. The logical network has three designated host machines and three DIs for these three LIF addresses the MPRE running on the host machine is the DI for the LIF address 1.1.2.251 the MPRE running on the host machine is the DI for the LIF address 1.1.2.252 and the MPRE running on the host machine is the DI for the LIF address 1.1.2.253. The MPREs in host machines are not DIs for the LIF B though not illustrated they can be DIs for other LIFs . Thus the host machines and can all serve as designated host machines for performing L3 routing on packets from the PHs and .

As mentioned earlier each MPRE is addressable from network nodes outside of its host machine by a physical MAC address PMAC which uniquely identifies the MPRE from other MPREs in other host machines. In some embodiments the PHs use the PMAC of a designated instance as its first hop L2 destination. In other words to send a packet to be routed by a DI a PH would first send the packet to the DI by using the DI s PMAC address. In the example of the DI in the host machine has PMAC address PMAC100 the DI in the host machine has PMAC address PMAC200 and the DI in the host machine has PMAC address PMAC300 .

Operations labeled 1 through 4 illustrates the routing of the packet . At operation 1 the PH sends the packet on to the physical network . The packet specifies that it is destined for IP 1.1.3.3 while its first hop MAC address is PMAC300 . At operation 2 the packet reaches the MPRE in the host based on the MAC address PMAC300 which is the PMAC of the MPRE . The packet enters the MPRE through LIF B since the PH is in network segment B IP address 1.1.2.10 . At operation 3 the MPRE uses its routing table to translates the destination IP address 1.1.3.3 to destination MAC address MAC3 . At operation 4 the MPSE not illustrated of the host machine recognizes that MAC3 is the MAC address of a VM running within the host machine . The MPSE then forwards the packet to the VM .

Operations labeled 5 through 9 illustrates the routing of the packet . At operation 5 the PH sends the packet on to the physical network . The packet specifies that it is destined for IP 1.1.4.4 while its first hop MAC address is PMAC100 . At operation 6 the packet reaches the MPRE in the host based on the MAC address PMAC100 which is the PMAC of MPRE . The packet enters the MPRE through its LIF B since the PH is in network segment B IP address 1.1.2.10 . At operation 7 the MPRE uses its routing table to translates the destination IP address 1.1.4.4 to destination MAC address MAC4 . At operation 8 the MPSE of the host machine realizes that MAC4 is not an address for any network node within the host machine and forwards the routed packet out onto the physical network . At operation 9 the routed packet with destination MAC4 reaches the host machine whose MPSE not illustrated recognize it as the L2 address of a VM running on that host machine. The MPSE of the host machine then forwards the routed packet to the VM whose IP address is 1.1.4.4.

In some embodiments different LIFs of a LRE have different sets of IP addresses and each IP address of a LIF has a corresponding designated instance. illustrates conceptually illustrates a LRE in which each LIF has multiple IP addresses and each IP address has its own corresponding designated instance. The LRE LRE X has four LIFs for four different network segments. The LIFs and are for VLAN network segments VLAN100 and VLAN200 . The LIFs and are for VXLAN network segments VXLAN500 and VXLAN600 .

As illustrated each of LIFs has multiple IP addresses and each IP address is associated with a host machine that is operating a local instance of the LRE X i.e. MPRE as the designated instance for that IP address. In some embodiments each IP address of a LIF is associated with a different host machine. As mentioned earlier in some embodiments a PMAC of a MPRE is an address that is used to uniquely identify one MPRE in one host machine from other MPREs in other host machines therefore IP addresses associated with different PMAC addresses indicates designated instances in different host machines. For example the LIF has IP addresses 2.1.2.251 2.1.2.252 and 2.1.2.253. The LIF IP addresses 2.1.2.251 has a designated instance with PMAC address 11 11 11 11 12 01 or PMAC4 the LIF IP addresses 2.1.2.252 has a designated instance with PMAC address 11 11 11 11 12 02 or PMAC5 and the LIF IP addresses 2.1.2.253 has a designated instance with PMAC address 11 11 11 11 12 01 or PMAC6 . The three IP addresses of the LIF are therefore assigned to MPREs in three different host machines.

In some embodiments one host machine can serve as the designated host machine and its MPRE as the designated instance for multiple different IP addresses from multiple different LIFs. For example the PMAC address PMAC1 corresponds to both IP address 2.1.1.251 of the LIF and IP address 2.1.3.251 of the LIF i.e. the MPRE having PMAC1 is serving as the designated instance for both of these LIF IP addresses. Likewise the PMAC address PMAC6 corresponds to both IP address 2.1.2.253 of the LIF and IP address 2.1.4.253 of the LIF . In other words the MPRE having PMAC1 is a designated instance and its host machine the designated host machine for both VLAN100 and VXLAN500 while the MPRE having PMAC6 is a designated instance for both VLAN200 and VXLAN600.

The network virtualization infrastructure also includes PH which are not operating a local instance of the LRE . The PH are in VLAN100 the PH are in VLAN200 the PH are in VXLAN500 and the PH are in VXLAN600.

Some of the host machines namely host machines are operating MPREs that serve as designated instances for handling traffic from the PHs . Specifically the host machines and are serving as designated host machines for VLAN100 for handing traffic from PHs and the host machines and are serving as designated host machines for VLAN200 for handing traffic from PHs and the host machines and are serving as designated host machines for VXLAN500 for handing traffic from PHs and and the host machines and are serving as designated host machines for VXLAN500 for handing traffic from PHs and . Though not illustrated in some embodiments some of the network segments are inherently distributed so there would be no need for designated instances for handling traffic from physical hosts of those network segments. For example in some embodiments some VXLAN network segments have physical hosts that are capable of distributed routing and therefore do not need MPREs in other host machines as designated instances.

Each network segment and the LIF for that network segment has its multiple LIF IP addresses assigned to different host machines. For example the LIF for VLAN200 has three IP addresses 2.1.2.251 2.1.2.252 and 2.2.253 and each of these IP addresses is assigned to a different host machine 2.1.2.251 is assigned to the host machine 2.1.2.252 is assigned to the host machine and 2.1.2.253 is assigned to the host machine . As mentioned earlier by reference to some host machines serve as designated host machine for different IP addresses from different LIFs network segments. As illustrated in the host machine PMAC1 is serving as designated host machine i.e. hosting a designated instance MPRE for both 2.1.1.251 of VLAN100 and 2.1.3.251 for VXLAN500. The host machine PMAC6 is serving as designated host machine for both 2.1.2.253 of VLAN200 and 2.1.4.253 of VXLAN600.

As mentioned in some embodiments having multiple designated instances per LIF gives a physical host using that LIF a list of choices when selecting a next hop. A physical host having such a list is able to select one designated instance as destination for say to balance the load across different designated instances. To provide such a list to the physical hosts of a particular network segment some embodiments advertise the IP addresses of the LIF of that particular network segment as a list of available next hops.

The controller also selects the host machines to serve as the designated instances designated host machines for those advertised LIF IP addresses. As illustrated the controller selects the host machine as the designated host i.e. its MPRE as the designated instance for the LIF IP address 2.1.1.251 the host machine as the designated host for the LIF IP address 2.1.1.252 and the host machine as the designated host for the LIF IP address 2.1.1.253. When the physical hosts subsequently request address resolution for their received next hop IP addresses some embodiments provide the PMACs of the selected designated instances designated hosts as the resolved L2 MAC addresses to the requesting physical hosts. Address resolution of LIF IP addresses will be described further below in Section II.c.

Once a list of designated instances is made available to a physical host the physical host is able to select any one of the designated instances as a next hop into the logical network. Such selection can be based on any number of criteria and can be made for any number of purposes. In some embodiments a physical host selects a designated instance as the next hop based on current network traffic information in order to balance the traffic load between the different designated host machines. In some embodiments a PH uses the list of designated instances to perform ECMP Equal Cost Multi path Routing algorithms on ingress network traffic to the logical network.

Each of the core routers performs ECMP algorithms to select one of the edge routers as the next hop for traffic flowing from the client site towards the network virtualization infrastructure . Each of the edge routers in turn performs its own ECMP algorithm to select one of the designated instances as the next hop for traffic into the network virtualization infrastructure . In some embodiments at least some of the routers perform the ECMP algorithms in order to balance the traffic and or computation load among downstream routers. In some embodiments such an ECMP algorithm is based on dynamic network traffic status where the selection of the next hop is cognizant of the current traffic load on each of the designated instances. In some embodiments the ECMP algorithm selects a next hop by blindly hashing the ingress data packet without regard to any real time network traffic status.

The edge router has a list and the edge router has a list . Both the lists and are derived from the advertised list of LIF IP addresses that includes 2.1.1.251 2.1.1.252 and 2.1.1.253. Each of the routers selects a next hop from uses its list of IP addresses. For example the edge router uses its list to perform ECMP and determines that 2.1.1.252 is a better next hop than 2.1.1.251 and 2.1.1.253 for a particular data packet. The edge router then selects 2.1.1.252 as the destination IP. In the example of the MPRE running on the host machine has been selected as the designated instance for the IP address 2.1.1.252 which has L2 address PMAC2 . The particular data packet destined to the IP address 2.1.1.252 will therefore be sent to the host machine by using the L2 address PMAC2 .

At the process updates network information. The process then selects at an IP address as the next hop. Some embodiments select a next hop based on real time network information update in order to achieve load balancing. Some embodiments do not use such network information update but rather rely on random selection e.g. simple hashing to achieve load balancing. Some embodiments use other types of ECMP algorithms for selecting a next hop.

The process next determines at whether the selected next hop IP address has a corresponding resolved L2 address. The resolved L2 address is the actual MAC address of the host machine that is chosen as the designated host and hosting the designated LRE instance for the next hop IP address. If the selected next hop has a resolved L2 address the process proceeds to to forward the packet. Otherwise the process performs at address resolution operation in order to resolve the selected next hop IP address e.g. by sending ARP request for the selected next hop IP address .

Once the next IP address has been resolved into an L2 address the process forwards the packet by using the resolved L2 address. The process then returns to to see if there is another packet to be forwarded by the LRE. The resolution of addresses by designated instances will be further described in Section II.c below.

The process then assigns at a set of IP addresses for a LIF. Next the process assigns at a designated instance to each IP address of the LIF. Each designated instance is an MPRE residing on a host machine. The process then advertises at the list of IP address for the LIF as a list of available next hops to external host machines e.g. edge routers connected to that LIF. The process then repeats through until it determines at that all LIFs in the LRE have a set of IP addresses and a set of corresponding designated instances. In some embodiments each LIF is assigned a unique set of IP addresses and no two LIFs share a same IP address. In some embodiments an MPRE of a host machine can serve as the designated instance for two or more different IP addresses from different LIFs.

Once the designated instances for the LIF IP addresses have been chosen the process produces at a configuration for the LRE. The process then pushes the LRE configuration to each of the host machines in the network virtualization infrastructure. Some of the host machines receiving the configuration would learn that it has been chosen as a designated host machine i.e. having a designated instance MPRE and perform the functions of a designated instance. The configuration of an LRE will be described further in Section III below. The process then ends.

The routing operations illustrated in rely on routing table entries in MPREs for translating L3 IP addresses into L2 MAC addresses. Packets coming from physical hosts PHs in particular rely on routing table entries in designated instances for routing. In some embodiments these entries are filled by address resolution protocols ARP initiated by PHs or by DIs themselves. Furthermore a PH that has received a list of IP addresses as next hops such as the routers and in also performs ARP operation to translate the received L3 IP address into L2 MAC addresses in some embodiments. In other words in order to use the received LIF IP addresses as next hops a PH in some embodiments performs ARP in order to ascertain the PMAC addresses of the designated instances.

For some embodiments illustrates ARP operations for resolving LIF IP addresses advertised to the PHs. The figure illustrates ARP operations by PHs and PH . The PHs and have each received a list of next hops from a logical network . The PHs and are both network nodes in a network segment VLAN100 and the list provides a list of IP address for the LIF of VLAN100 which includes 2.1.1.251 2.1.2.252 and 2.1.1.253. The PH is maintaining a routing table and the PH is maintaining a routing table .

The logical network is implemented over an array of host machines including host machines and . The logical network is implementing an LRE and the host machines of the logical network including the host machines and are each running a local instance of the LRE as its MPRE. The PMAC address of the host machine is PMAC1 and its MPRE has been chosen as the designated instance for the LIF address 2.1.1.251. The PMAC address of the host machine is PMAC2 and its MPRE has been chosen as the designed instance for the LIF address 2.1.2.252.

At operation 4 the PH selects the IP address 2.1.2.252 as a next hop but its routing table does not have an entry for the 2.1.2.252. The PH in turn broadcast an ARP query message for the IP address 2.1.2.252. At operation 5 the host machine receives the ARP query broadcast. Realizing that it is the designated instance for the IP address 2.1.2.252 it sends an ARP reply to the PH indicating that the MAC address for the IP addresses is PMAC2 . At operation 6 the PH receives the ARP reply and updates its routing table entry for 2.1.2.252 with PMAC2 . After operations 1 through 6 the router will be able to use the MPREs of the host machines and for routing.

At operation 7 the PH also selects the IP address 2.1.2.252 as a next hop but its routing table does not have an entry for the 2.1.2.252. The PH in turn broadcast an ARP query message for the IP address 2.1.2.252. At operation 8 the host machine receives the ARP query broadcast. Realizing that it is the designated instance for the IP address 2.1.2.252 it sends an ARP reply to the PH indicating that the MAC address for the IP addresses is PMAC2 . At operation 9 the PH receives the ARP reply and updates its routing table entry for 2.1.2.252 with PMAC2 . After operations 7 through 9 router will be able to use the MPRE of the host machine for routing.

In some embodiments the designated instances also serve as ARP proxies. In some embodiments a designated instance performs ARP of its own if it is not able to resolve a destination IP address. illustrates the designated instances and acting as ARP proxies when they receive data packets with unknown destination IP addresses from the PH . As illustrated the PH has already resolved its next hop LIF IP addresses 2.1.1.251 and 2.1.2.252 into PMAC1 and PMAC2 from previous ARP operations i.e. the operations illustrated in . The PH is therefore able to select either PMAC1 or PMAC for routing. In some embodiments such a selection is based on ECMP algorithm for load balancing purposes as discussed above in Section II.b.

In operations labeled 1 to 12 illustrates the routing of packets and to VMs and through designated instances in host machines and . At operation 1 the PH sends packet . The packet has PMAC1 as its destination address and 2.1.2.101 as its destination IP address. The MAC address PMAC1 corresponds to the MPRE of the host machine . The PH at this operation has selected 2.1.2.101 PMAC1 over 2.1.3.102 PMAC2 according to a selection algorithm e.g. ECMP for load balancing even though both IP addresses of the LIF for VLAN100 has been resolved.

At operation 2 the host machine receives the packet based on the MAC address PMAC1 but its routing table cannot resolve the IP address 2.1.2.101. At operation 3 the MPRE of the host machine broadcast an ARP query for the destination IP address 2.1.2.101.

At operation 4 the MPRE of a host machine replies to the ARP query because the host machine is hosting a VM whose IP address is 2.1.2.101. The ARP reply indicates that the MAC address for 2.1.2.101 is MAC21 . At operation 5 the host machine receives the ARP reply and updates its routing table for the entry for 2.1.2.101. At operation 6 having resolved the destination IP address 2.1.2.101 for the packet the host machine sends the data packet to the host machine and to the VM by using MAC21 as the destination address.

At operation 7 after sending the packet to the designated instance for 2.1.1.251 PMAC1 the PH sends the packet to the designated instance for 2.1.2.252 PMAC2 . The packet has PMAC2 as its destination address and 2.1.3.102 as its destination IP address. The MAC address PMAC2 corresponds to the MPRE of the host machine . The PH at this operation has selected 2.1.3.102 PMAC2 over 2.1.2.101 PMAC1 according to a selection algorithm e.g. ECMP for load balancing even though both IP addresses of the LIF for VLAN100 has been resolved.

At operation 8 the host machine receives the packet based on the MAC address PMAC2 but its routing table cannot resolve the IP address 2.1.3.102. At operation 9 the MPRE of the host machine broadcast an ARP query for the destination IP address 2.1.3.102. At operation 10 the MPRE of a host machine replies to the ARP query because the host machine is hosting a VM whose IP address is 2.1.3.102. The ARP reply indicates that the MAC address for 2.1.3.102 is MAC34 . At operation 11 the host machine receives the ARP reply and updates its routing table for the entry for 2.1.3.102. At operation 12 having resolved the destination IP address 2.1.3.102 for the packet the host machine sends the data packet to the host machine and to the VM by using MAC34 as the destination address.

Once the routing table of a designated instance has an MAC address resolution for a destination IP address any subsequent data packet having the same destination IP address can use the resolved MAC address and would not cause the designated instance to initiate another ARP request for that same destination IP address. illustrates the designated instance using its existing routing table entry to route a data packet from the other PH without initiating an ARP operation. As illustrated the routing table of the host machine already has an address resolution entry for 2.1.2.101 as MAC21 from a previous ARP operation i.e. the operations illustrated in . In operations labeled 1 to 3 the figure illustrates the routing of the packet from the PH to the VM .

At operation 1 the PH sends the packet . The packet has PMAC1 as its destination address and 2.1.2.101 as its destination IP address. The MAC address PMAC1 corresponds to the MPRE of the host machine . At operation 2 the host machine receives the packet based on the MAC address PMAC1 and its routing table already has an entry for resolving the IP address 2.1.2.101 into MAC21 . The routing table also adds an entry based on the packet s source IP address and MAC address i.e. 2.1.2.11 and MAC11 of the PH for future use. At operation 3 the host machine sends the data packet to the host machine and to the VM by using MAC21 as the destination address.

In some embodiments the designated instances not only resolve IP addresses for packets that comes from external PHs but also for packets coming from host machines running a local instance of the LRE. illustrates the routing of a packet from a VM in a host machine operating a MPRE to a physical host that is not operating a MPRE. The routing utilizes routing table entries in the available designated instances and for the LIF VLAN100. The routing table of the host machine already has an entry for 2.1.2.11 as MAC11 from a previous routing operation i.e. the routing of the packet from the PH in .

In operations labeled 1 through 6 illustrates the routing of the packet from the VM to the PH . At operation 1 a VM running on a host machine is sending the data packet which has a destination IP 2.1.2.11 and destination MAC address VMAC . As mentioned earlier VMAC is the MAC address used by a VM when addressing its own local MPRE. Since the MPRE in the host machine is not able to resolve the destination IP address 2.1.2.11 the host machine sends out a request for resolution to the designated instances and at operations 2 and 3 respectively.

In some embodiments an MPRE that needs to resolve a destination IP address would make a request for address resolution to a designated instance. In some embodiments an MPRE would make such an address resolution request to a designated instance that is associated with a LIF address that is in same IP subnet as the destination IP address. In the example of the host machine is a designated instance for the IP address 2.1.2.252 which is in the same IP subnet as destination address 2.1.2.11. The MPRE in the host machine therefore makes the address resolution request to the designated instance rather than to whose IP address 2.1.1.251 is in a different IP subnet. In some embodiments each designated instance is for resolving IP addresses that are in the same subnet as its assigned LIF IP address.

The host machine at operation 4 examines its routing table and found an entry for the IP address 2.1.2.11 as MAC11 and replies to the MPRE in the host machine in operation 5 . Finally at operation 6 the MPRE of the host machine sends the data packet to the PH by using the MAC address MAC11 which is the MAC address of the PH .

In some embodiments the address resolution requests to designated instances and address resolution replies from designated instances are UDP messages. In the example of one of the designated instances has a routing table entry for the destination IP address and was therefore able to reply to the address resolution request with its own routing table entry. In some embodiments when a designated instance is not able to resolve a destination IP address upon receiving an address resolution request it will perform an ARP operation in order to resolve the unknown IP address. illustrates an ARP operation performed by a designated instance when it is unable to resolve an IP address upon receiving an address resolution request.

In operations labeled 1 through 8 illustrates the routing of a packet from the VM to the PH . At operation 1 the VM running on a host machine is sending the data packet which has a destination IP 2.1.1.12 and destination MAC address VMAC . As mentioned earlier VMAC is the MAC address used by a VM when addressing its own local MPRE. Since the MPRE in the host machine is not able to resolve the destination IP address 2.1.1.12 it sends out a request for resolution to the designated instances and in operations 2 and 3 respectively. In the example of the host machine is a designated instance for the IP address 2.1.1.251 which is in the same IP subnet as destination address 2.1.1.12. The MPRE in the host machine therefore makes the address resolution request to the designated instance rather than 1602 whose IP address 2.1.2.252 is in a different IP subnet.

At operation 4 the host machine designated instance examines its routing table and realizes that it does not have an entry for resolving IP address 2.1.1.12. It therefore broadcasts an ARP request for the IP address 2.1.1.12. At operation 5 the PH whose IP address is 2.1.1.12 replies to the ARP request with its MAC address MAC12 . At operation 6 the designated instance receives the ARP reply from the PH and updates its own routing table . At operation 7 the designated instance sends address resolution reply message to the MPRE in the host machine informing the MPRE that the MAC address for the IP address 2.1.1.12 is MAC12 . At operation 8 the MPRE in the host machine forwards the packet to the PH by using MAC12 as the destination MAC address.

In the examples of the packets being routed and are sourced by VMs operating on host machines that are not designated instances VMs and running on host machines and . However one of ordinary skill would understand that the operations illustrated in can also be performed for a VM that is operating on a designated instance host machine.

For some embodiments conceptually illustrates a process for processing a data packet at an MPRE. In some embodiments the process is performed by MPREs that are designated instances as well as MPREs that are not designated instances. The process starts when it receives at a packet based on a destination MAC address. The destination MAC address can either be a broadcast MAC address e.g. ffffffffffff or the MAC address of the receiving MPRE i.e. its PMAC address or the generic VMAC address of all MPREs . The process then determines at whether the packet is an ARP query for an IP address. If the packet is an ARP query the process proceeds to . Otherwise the process proceeds to .

At the process examines if this MPRE is a designated instance of the IP address being ARP queried. If this MPRE is the designated instance for the IP address being ARP queried the process responds at to the ARP query with its own unique PMAC address and ends. Otherwise the process ignores at the ARP query and ends.

At the process determines if the destination IP address is in the routing table of the MPRE. If the destination IP address is not in the routing table the process proceeds to . If the destination IP is in the routing table the process routes at the packet by using the routing table entry for the destination IP address to find the corresponding MAC address. The packet then forwards at the packet by using the MAC address as the destination address for the packet. This forwarding operation is performed by using the MPSE of the host machine in some embodiments. The process then ends.

At the process selects a designated instance for resolving the IP address. As mentioned in some embodiments each LIF has multiple IP addresses and each of the IP addresses is assigned to a designated instance. In some embodiments the process would make the address resolution request to a designated instance that corresponds to a LIF IP address that is in the same IP subnet as the destination IP address. The process then determines at if this MPRE is itself the selected designated instance. If this MPRE is the selected designated instance itself process proceeds to . If this MPRE is not the selected designated instance or is not a designated instance at all the process requests at address resolution from the selected designated instance. The process then receives at the address resolution from the designated instance. In some embodiments such address resolution requests and replies are transmitted as UDP messages between the designated instance and the host machine requesting the address resolution. The process then updates at the routing table of the MPRE based on the received address resolution and proceeds to to route the data packet.

At the process performs ARP operation to resolve the IP address since the MPRE is the selected designated instance but cannot resolve destination IP address from its existing routing table entries. After making the ARP request and receiving the reply for the ARP the process proceeds to to update its routing table route at the data packet forwards at the data packet and ends.

For some embodiments conceptually illustrates a process for performing address resolution at a designated instance MPRE. The process starts when it receives at an address resolution request message from a host machine e.g. from an MPRE performing the process for a destination IP address with unknown MAC address. The process then determines at if it is able to resolve the requested destination IP address locally i.e. if the requested address is in the MPRE s own routing table. If so the process proceeds to . If the process cannot resolve the requested address it proceeds to to broadcast an ARP request for the requested destination IP address. The process then updates at its routing table after it receives the corresponding ARP reply bearing the resolved MAC address. The process then replies at to the address resolution request by informing the requester of the resolved MAC address. The process then ends. In some embodiments the address resolution request message and the address resolution reply messages are UDP messages.

In some embodiments the LRE instantiations operating locally in host machines as MPREs either for routing and or bridging as described above are configured by configuration data sets that are generated by a cluster of controllers. The controllers in some embodiments in turn generate these configuration data sets based on logical networks that are created and specified by different tenants or users. In some embodiments a network manager for a network virtualization infrastructure allows users to generate different logical networks that can be implemented over the network virtualization infrastructure and then pushes the parameters of these logical networks to the controllers so the controllers can generate host machine specific configuration data sets including configuration data for LREs. In some embodiments the network manager provides instructions to the host machines for fetching configuration data for LREs from the controllers.

For some embodiments illustrates a network virtualization infrastructure in which logical network specifications are converted into configurations for LREs in host machines to be MPREs bridges . As illustrated the network virtualization infrastructure includes a network manager one or more clusters of controllers and host machines that are interconnected by a physical network. The host machines includes host machines though host machines are not illustrated in this figure.

The network manager provides specifications for one or more user created logical networks. In some embodiments the network manager includes a suite of applications that let users specify their own logical networks that can be virtualized over the network virtualization infrastructure . In some embodiments the network manager provides an application programming interface API for users to specify logical networks in a programming environment. The network manager in turn pushes these created logical networks to the clusters of controllers for implementation at the host machines.

The controller cluster includes multiple controllers for controlling the operations of the host machines in the network virtualization infrastructure . The controller creates configuration data sets for the host machines based on the logical networks that are created by the network managers. The controllers also dynamically provide configuration update and routing information to the host machines . In some embodiments the controllers are organized in order to provide distributed or resilient control plane architecture in order to ensure that each host machines can still receive updates and routes even if a certain control plane node fails. In some embodiments at least some of the controllers are virtual machines operating in host machines.

The host machines operate LREs and receive configuration data from the controller cluster for configuring the LREs as MPREs bridges. Each of the host machines includes a controller agent for retrieving configuration data from the cluster of controllers . In some embodiments each host machine updates its MPRE forwarding table according to a VDR control plane. In some embodiments the VDR control plane communicates by using standard route exchange protocols such as OSPF open shortest path first or BGP border gateway protocol to routing peers to advertise determine the best routes.

In operation 3 the controller agents operating in the host machines send requests for LRE configurations from the cluster of controllers based on the instructions received at operation 2 . That is the controller agents contact the controllers to which they are pointed by the network manager . In operation 4 the clusters of controllers provide LRE configurations to the host machines in response to the requests.

The LRE for tenant X includes LIFs for network segments A B and C. The LRE for tenant Y includes LIFs for network segments D E and F. In some embodiments each logical interface is specific to a logical network and no logical interface can appear in different LREs for different tenants.

The configuration data for a host in some embodiments includes its VMAC which is generic for all hosts its unique PMAC and a list of LREs running on that host. For example the configuration data for the host would show that the host is operating a MPRE for the LRE while the configuration data for the host would show that the host is operating MPREs for the LRE and the LRE . In some embodiments the MPRE for tenant X and the MPRE for tenant Y of a given host machine are both addressable by the same unique PMAC assigned to the host machine.

The configuration data for an LRE in some embodiments includes a list of LIFs a routing forwarding table and controller cluster information. The controller cluster information in some embodiments informs the host where to obtain updated control and configuration information. In some embodiments the configuration data for an LRE is replicated for all of the LRE s instantiations i.e. MPREs across the different host machines.

The configuration data for a LIF in some embodiments includes the name of the logical interface e.g. a UUID its set of IP addresses its MAC address i.e. LMAC or VMAC its MTU maximum transmission unit its destination info e.g. the VNI of the network segment with which it interfaces whether it is active or inactive on the particular host and whether it is a bridge LIF or a routing LIF. The configuration data for LIF also includes a designated instance criteria field .

In some embodiments the designated instance criteria is an external facing parameters that indicate whether a LRE running on a host as its MPRE is a designated instance and needs to perform address resolution for physical hosts. In some embodiments such criteria for designated instances is a list e.g. of the IP address for the LIF and the corresponding identifiers for the host machines selected to serve as the designated instance designated host machine for those IP addresses. In some embodiments a host machine that receives the configuration data determines whether it is a designated host machine i.e. operating a MPRE that is the designated instance for one of the LIF IP addresses by examining the list . A host machine e.g. host 2 knows to operate its MPRE as a designated instance for a particular LIF IP address e.g. 2.1.2.252 when it sees its own identifier associated with that particular LIF IP addresses in the designated instance criteria .

In some embodiments the LREs are configured or controlled by APIs operating in the network manager. For example some embodiments provide APIs for creating a LRE deleting an LRE adding a LIF and deleting a LIF. In some embodiments the controllers not only provide static configuration data for configuring the LREs operating in the host machines as MPRE bridges but also provide static and or dynamic routing information to the local LRE instantiations running as MPREs. Some embodiments provide APIs for updating LIFs e.g. to update the MTU MAC IP information of a LIF and add or modify route entry for a given LRE. A routing entry in some embodiments includes information such as destination IP or subnet mask next hop information logical interface metric route type neighbor entry or next hop or interface etc. route control flags and actions such as forward blackhole etc. .

Some embodiments dynamically gather and deliver routing information for the LREs operating as MPREs. illustrates the gathering and the delivery of dynamic routing information for LREs. As illustrated the network virtualization infrastructure not only includes the cluster of controllers and host machines it also includes a host machine that operates a virtual machine edge VM for gathering and distributing dynamic routing information. In some embodiments the edge VM executes OSPF or BGP protocols and appears as an external router for another LAN or other network. In some embodiments the edge VM learns the network routes from other routers. After validating the learned route in its own network segment the edge VM sends the learned routes to the controller clusters . The controller cluster in turn propagates the learned routes to the MPREs in the host machines .

Many of the above described features and applications are implemented as software processes that are specified as a set of instructions recorded on a computer readable storage medium also referred to as computer readable medium . When these instructions are executed by one or more processing unit s e.g. one or more processors cores of processors or other processing units they cause the processing unit s to perform the actions indicated in the instructions. Examples of computer readable media include but are not limited to CD ROMs flash drives RAM chips hard drives EPROMs etc. The computer readable media does not include carrier waves and electronic signals passing wirelessly or over wired connections.

In this specification the term software is meant to include firmware residing in read only memory or applications stored in magnetic storage which can be read into memory for processing by a processor. Also in some embodiments multiple software inventions can be implemented as sub parts of a larger program while remaining distinct software inventions. In some embodiments multiple software inventions can also be implemented as separate programs. Finally any combination of separate programs that together implement a software invention described here is within the scope of the invention. In some embodiments the software programs when installed to operate on one or more electronic systems define one or more specific machine implementations that execute and perform the operations of the software programs.

The bus collectively represents all system peripheral and chipset buses that communicatively connect the numerous internal devices of the electronic system . For instance the bus communicatively connects the processing unit s with the read only memory the system memory and the permanent storage device .

From these various memory units the processing unit s retrieves instructions to execute and data to process in order to execute the processes of the invention. The processing unit s may be a single processor or a multi core processor in different embodiments.

The read only memory ROM stores static data and instructions that are needed by the processing unit s and other modules of the electronic system. The permanent storage device on the other hand is a read and write memory device. This device is a non volatile memory unit that stores instructions and data even when the electronic system is off. Some embodiments of the invention use a mass storage device such as a magnetic or optical disk and its corresponding disk drive as the permanent storage device .

Other embodiments use a removable storage device such as a floppy disk flash drive etc. as the permanent storage device. Like the permanent storage device the system memory is a read and write memory device. However unlike storage device the system memory is a volatile read and write memory such a random access memory. The system memory stores some of the instructions and data that the processor needs at runtime. In some embodiments the invention s processes are stored in the system memory the permanent storage device and or the read only memory . From these various memory units the processing unit s retrieves instructions to execute and data to process in order to execute the processes of some embodiments.

The bus also connects to the input and output devices and . The input devices enable the user to communicate information and select commands to the electronic system. The input devices include alphanumeric keyboards and pointing devices also called cursor control devices . The output devices display images generated by the electronic system. The output devices include printers and display devices such as cathode ray tubes CRT or liquid crystal displays LCD . Some embodiments include devices such as a touchscreen that function as both input and output devices.

Finally as shown in bus also couples electronic system to a network through a network adapter not shown . In this manner the computer can be a part of a network of computers such as a local area network LAN a wide area network WAN or an Intranet or a network of networks such as the Internet. Any or all components of electronic system may be used in conjunction with the invention.

Some embodiments include electronic components such as microprocessors storage and memory that store computer program instructions in a machine readable or computer readable medium alternatively referred to as computer readable storage media machine readable media or machine readable storage media . Some examples of such computer readable media include RAM ROM read only compact discs CD ROM recordable compact discs CD R rewritable compact discs CD RW read only digital versatile discs e.g. DVD ROM dual layer DVD ROM a variety of recordable rewritable DVDs e.g. DVD RAM DVD RW DVD RW etc. flash memory e.g. SD cards mini SD cards micro SD cards etc. magnetic and or solid state hard drives read only and recordable Blu Ray discs ultra density optical discs any other optical or magnetic media and floppy disks. The computer readable media may store a computer program that is executable by at least one processing unit and includes sets of instructions for performing various operations. Examples of computer programs or computer code include machine code such as is produced by a compiler and files including higher level code that are executed by a computer an electronic component or a microprocessor using an interpreter.

While the above discussion primarily refers to microprocessor or multi core processors that execute software some embodiments are performed by one or more integrated circuits such as application specific integrated circuits ASICs or field programmable gate arrays FPGAs . In some embodiments such integrated circuits execute instructions that are stored on the circuit itself.

As used in this specification the terms computer server processor and memory all refer to electronic or other technological devices. These terms exclude people or groups of people. For the purposes of the specification the terms display or displaying means displaying on an electronic device. As used in this specification the terms computer readable medium computer readable media and machine readable medium are entirely restricted to tangible physical objects that store information in a form that is readable by a computer. These terms exclude any wireless signals wired download signals and any other ephemeral signals.

While the invention has been described with reference to numerous specific details one of ordinary skill in the art will recognize that the invention can be embodied in other specific forms without departing from the spirit of the invention. In addition a number of the figures including and conceptually illustrate processes. The specific operations of these processes may not be performed in the exact order shown and described. The specific operations may not be performed in one continuous series of operations and different specific operations may be performed in different embodiments. Furthermore the process could be implemented using several sub processes or as part of a larger macro process. Thus one of ordinary skill in the art would understand that the invention is not to be limited by the foregoing illustrative details but rather is to be defined by the appended claims.

