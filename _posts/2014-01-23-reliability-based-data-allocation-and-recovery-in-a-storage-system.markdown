---

title: Reliability based data allocation and recovery in a storage system
abstract: A storage system provides highly flexible data layouts that can be tailored based on reliability considerations. The system allocates reliability values to logical containers at an upper logical level of the system based, for example, on objectives established by reliability SLOs. Based on the reliability value, the system identifies a specific parity group from a lower physical storage level of the system for storing data corresponding to the logical container. After selecting a parity group, the system allocates the data to physical storage blocks within the parity group. In embodiments, the system attaches the reliability value information to the parity group and the physical storage units storing the data. In this manner, the underlying physical layer has a semantic understanding of reliability considerations related to the data stored at the logical level. Based on this semantic understanding, the system has the capability to prioritize data operations on the physical storage units according to the reliability values attached to the parity groups.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09477553&OS=09477553&RS=09477553
owner: NetApp, Inc.
number: 09477553
owner_city: Sunnyvale
owner_country: US
publication_date: 20140123
---
This application is a continuation of U.S. application Ser. No. 13 086 267 filed Apr. 13 2011. The content of the above identified application is incorporated herein by reference in its entirety.

At least one embodiment of the present invention pertains to data storage systems and more particularly to a technique for data allocation and recovery in a storage system based on reliability values associated with the data.

Network based storage or simply network storage is a common approach to backing up data making large amounts of data accessible to multiple users and other purposes. In a network storage environment a storage server makes data available to client host systems by presenting or exporting to the clients one or more logical containers of data. There are various forms of network storage including network attached storage NAS and storage area network SAN . In a NAS context a storage server services file level requests from clients whereas in a SAN context a storage server services block level requests. Some storage servers are capable of servicing both file level requests and block level requests.

The technology marketplace has been experiencing several trends that impact existing network storage technologies. For example the density of magnetic storage media continues to grow in network storage systems but performance of such media measured as input output operations per second IOPS per spindle has not exhibited a similar growth rate. That is magnetic media have increased in density at a rate greater than the rate at which their speed has increased. As a result data operations such as backup maintenance recovery of failed drives etc. take longer to complete resulting in myriad performance and reliability issues. For example the longer recovery time of a failed drive presents a window of vulnerability during which the ability to protect new data is compromised. Moreover the processing cycles spent in the longer recovery time also affects the overall performance of the storage system.

Presently the underlying physical layer of a storage system does not have a semantic understanding of the stored data. That is for example the underlying physical layer does not distinguish data corresponding to important business information versus data corresponding to scratch space information. Consequently the physical layer does not make any effort to distinguish or otherwise prioritize the data for the various data operations. This lack of semantic understanding further exacerbates the performance and reliability issues associated with the various data operations.

Introduced below is a layout and file system architecture for a storage system and associated methods and apparatus collectively called the system introduced here or simply the system in the discussion which follows. The system provides highly flexible data layouts that can be tailored to numerous different applications and use cases. Among other features the system is capable of allocating physical storage units for data corresponding to logical storage entities based on for example reliability service level objectives SLOs as discussed below.

The system in at least one embodiment assigns a reliability value to each logical container of data e.g. a volume located at an upper logical layer of the storage system. In one embodiment the reliability value is assigned according to objectives dictated by reliability SLOs. In other embodiments the reliability value may be assigned by the user via a management interface of the storage system or may automatically be assigned based on the type of data stored in the logical container.

Based on the reliability value the system identifies a particular parity group from the underlying physical storage layer e.g. a RAID layer of the storage system for storing data corresponding to the logical container. A parity group is a collection of storage areas from one or more physical storage devices sharing one or more common protection level attributes e.g. parity protection level type of storage medium etc. . In an illustrative embodiment each parity group is used to denote storage areas or storage slices selected from one or more physical storage devices e.g. a collection of slices from different disk drives that use a common parity based protection scheme against data loss. The parity group for a given logical container of data is chosen based on the reliability value as dictated for example by the reliability SLOs. For example a logical container with a high reliability requirement is assigned a parity group with the highest protection level. The system then allocates data for the logical container within physical storage blocks selected from the assigned parity group. In embodiments the system attaches the reliability information of the logical container to the parity group and also to the physical storage devices in which the data is stored. For example the reliability level information is attached as metadata to the parity group.

In this manner the underlying physical storage layer has semantic understanding of the importance of the data stored in the physical storage devices. That is the underlying physical layer has the capability to distinguish and prioritize data stored in the physical storage devices. Consequently the underlying physical storage layer can prioritize various data operations e.g. backup operations data recovery operations based on the values of the attached reliability information. An illustrative example is the recovery of a failed physical data element e.g. a failed disk drive . The physical data element may comprise storage blocks or slices belonging to different parity groups. Accordingly prior to recovery of a failed physical data element e.g. a disk drive the system identifies the parity groups that the failed element participated in. The system then prioritizes the list of parity groups and performs the recovery process first on the physical storage device belonging to the parity groups with the highest reliability values as indicated for example by reliability SLOs .

In this manner the system has the ability to for example efficiently postpone or otherwise de prioritize data recovery operations on physical storage devices with lower reliability values. Prioritizing the data recovery operations on the parity groups with high reliability values results in significant performance and reliability improvement. For example in the reconstruction or failure recovery scenario the reliability based prioritization minimizes the window of vulnerability to data loss due to failure of another physical storage element and also lowers the interference with system s foreground primary workload.

Other aspects of the technique will be apparent from the accompanying figures and from the detailed description which follows.

References in this specification to an embodiment one embodiment or the like mean that the particular feature structure or characteristic being described is included in at least one embodiment of the present invention. Occurrences of such phrases in this specification do not necessarily all refer to the same embodiment.

A storage system provides highly flexible data layouts that can be tailored based on reliability considerations. The system allocates reliability values to logical containers at an upper logical level of the system based for example on objectives established by reliability SLOs. Based on the reliability value the system identifies a specific parity group from a lower physical storage level of the system for storing data corresponding to the logical container. After selecting a parity group the system allocates the data to physical storage blocks within the parity group. In embodiments the system attaches the reliability value information to the parity group and the physical storage units storing the data. In this manner the underlying physical layer has a semantic understanding of reliability considerations related to the data stored at the logical level. Based on this semantic understanding the system has the capability to prioritize data operations e.g. recovery operations maintenance operations etc. on the physical storage units according to the reliability values attached to the parity groups.

The storage server or servers may be for example one of the FAS family of storage server products available from NetApp Inc. The client systems . . are connected to the storage server via the computer network which can be a packet switched network for example a local area network LAN or wide area network WAN . Further the storage server is connected to the disks via a switching fabric which can be a fiber distributed data interface FDDI network for example. It is noted that within the network data storage environment any other suitable numbers of storage servers and or mass storage devices and or any other suitable network technologies may be employed.

The storage server can make some or all of the storage space on the disk s available to the client systems . .. For example each of the disks can be implemented as an individual disk multiple disks e.g. a RAID group or any other suitable mass storage device s . The storage server can communicate with the client systems . . according to well known protocols such as the Network File System NFS protocol or the Common Internet File System CIFS protocol to make data stored on the disks available to users and or application programs. The storage server can present or export data stored on the disk as volumes to each of the client systems . .. A volume is an abstraction of physical storage combining one or more physical mass storage devices e.g. disks or parts thereof into a single logical storage object the volume and which is managed as a single administrative unit such as a single file system. A file system is a structured e.g. hierarchical set of stored logical containers of data e.g. volumes logical units LUNs directories files . Note that a file system does not have to include or be based on files per se a file system can be any structured set of logical containers of data such as files directories LUNs etc. A block as the term is used herein is the smallest addressable unit of contiguous data used by a given storage system to manipulate and transfer data. In conventional storage systems a block is commonly though not necessarily 4 KB in length.

Various functions and configuration settings of the storage server and the mass storage subsystem can be controlled from a management station coupled to the network . Among many other operations a data object migration operation can be initiated from the management station .

The environment in includes a plurality of client systems . .M a clustered storage server system and a computer network connecting the client systems and the clustered storage server system . As shown in the clustered storage server system includes a plurality of server nodes . .N a cluster switching fabric and a plurality of mass storage devices . .N which can be disks as henceforth assumed here to facilitate description. Alternatively some or all of the mass storage devices can be other types of storage such as flash memory SSDs tape storage etc. Note that more than one mass storage device can be associated with each node .

Each of the nodes is configured to include several modules including an N module a D module and an M host each of which can be implemented by using a separate software module and an instance of a replicated database RDB . Specifically node . includes an N module . a D module . and an M host . node .N includes an N module .N a D module .N and an M host .N and so forth. The N modules . .M include functionality that enables nodes . .N respectively to connect to one or more of the client systems over the network while the D modules . .N provide access to the data stored on the disks . .N respectively. The M hosts provide management functions for the clustered storage server system . Accordingly each of the server nodes in the clustered storage server arrangement provides the functionality of a storage server.

The RDB is a database that is replicated throughout the cluster i.e. each node includes an instance of the RDB . The various instances of the RDB are updated regularly to bring them into synchronization with each other. The RDB provides cluster wide storage of various information used by all of the nodes including a volume location database VLDB not shown . The VLDB is a database that indicates the location within the cluster of each volume in the cluster i.e. the owning D module for each volume and is used by the N modules to identify the appropriate D module for any given volume to which access is requested.

The nodes are interconnected by a cluster switching fabric which can be embodied as a Gigabit Ethernet switch for example. The N modules and D modules cooperate to provide a highly scalable distributed storage system architecture of a clustered computing environment implementing exemplary embodiments of the present invention. Note that while there is shown an equal number of N modules and D modules in there may be differing numbers of N modules and or D modules in accordance with various embodiments of the technique described here. For example there need not be a one to one correspondence between the N modules and D modules. As such the description of a node comprising one N module and one D module should be understood to be illustrative only.

The storage controller can be embodied as a single or multi processor storage system executing a storage operating system that preferably implements a high level module such as a storage manager to logically organize the information as a hierarchical structure of named directories files and special types of files called virtual disks hereinafter generally blocks on the disks. Illustratively one processor can execute the functions of the N module on the node while another processor executes the functions of the D module .

The memory illustratively comprises storage locations that are addressable by the processors and adapters for storing software program code and data structures associated with the present invention. The processor and adapters may in turn comprise processing elements and or logic circuitry configured to execute the software code and manipulate the data structures. The storage operating system portions of which is typically resident in memory and executed by the processors s functionally organizes the storage controller by among other things configuring the processor s to invoke storage operations in support of the storage service provided by the node . It will be apparent to those skilled in the art that other processing and memory implementations including various computer readable storage media may be used for storing and executing program instructions pertaining to the technique introduced here.

The network adapter includes multiple ports to couple the storage controller to one or more clients over point to point links wide area networks virtual private networks implemented over a public network Internet or a shared local area network. The network adapter thus can include the mechanical electrical and signaling circuitry needed to connect the storage controller to the network . Illustratively the network can be embodied as an Ethernet network or a Fibre Channel FC network. Each client can communicate with the node over the network by exchanging discrete frames or packets of data according to pre defined protocols such as TCP IP.

The storage adapter cooperates with the storage operating system to access information requested by the clients . The information may be stored on any type of attached array of writable storage media such as magnetic disk or tape optical disk e.g. CD ROM or DVD flash memory solid state disk SSD electronic random access memory RAM micro electro mechanical storage and or any other similar media adapted to store information including data and parity information. However as illustratively described herein the information is stored on disks . The storage adapter includes a plurality of ports having input output I O interface circuitry that couples to the disks over an I O interconnect arrangement such as a conventional high performance Fibre Channel FC link topology.

Storage of information on disks can be implemented as one or more storage volumes that map to a collection of physical storage devices which can be divided into one or more RAID groups.

The storage operating system facilitates clients access to data stored on the disks . In certain embodiments the storage operating system implements a file system that cooperates with one or more virtualization modules to virtualize the storage space provided by disks . In certain embodiments a storage manager logically organizes the stored data that described further below. In one embodiment the storage operating system implements write anywhere and copy on write functionality that is any data or metadata can be written to any free physical data block and a modification to any logical data block is always written to a new physical data block rather than overwriting the original physical data block.

In addition the storage operating system includes a set of layers organized to form a backend server that provides data paths for accessing information stored on the disks of the node . The backend server in combination with underlying processing hardware also forms the D module . To that end the backend server includes a storage manager module that manages any number of volumes a RAID system module and a storage driver system module .

The storage manager primarily manages a file system or multiple file systems and serves client initiated read and write requests. In at least one embodiment the storage manager implements the volumes regions extents slabs based storage techniques introduced here. The RAID system module manages the storage and retrieval of information to and from the volumes disks in accordance with a RAID redundancy protocol such as RAID 4 RAID 5 RAID DP or declustered RAID discussed below while the disk driver system implements a disk access protocol such as Serial ATA SATA SCSI or FC protocol FCP .

The backend server also includes a CF interface module to implement intra cluster communication with N modules and or other D modules. The CF interface modules and can cooperate to provide a single file system image across all D modules in the cluster. Thus any network port of an N module that receives a client request can access any data container within the single file system image located on any D module of the cluster.

The CF interface modules implement the CF protocol to communicate file system commands among the modules of cluster over the cluster switching fabric . Such communication can be effected by a D module exposing a CF application programming interface API to which an N module or another D module issues calls. To that end a CF interface module can be organized as a CF encoder decoder. The CF encoder of e.g. CF interface on N module can encapsulate a CF message as i a local procedure call LPC when communicating a file system command to a D module residing on the same node or ii a remote procedure call RPC when communicating the command to a D module residing on a remote node of the cluster. In either case the CF decoder of CF interface on D module de encapsulates the CF message and processes the file system command.

In operation of a node a request from a client can be forwarded as a packet over the network and onto the node where it is received at the network adapter . A network driver of layer processes the packet and if appropriate passes it on to a network protocol and file access layer for additional processing prior to forwarding to the storage manager . At that point the storage manager interprets the request and generates operations to load retrieve the requested data from the RAID system if it is not resident in memory . The storage manager determines in which extent and in which region the data resides. The region receives a request for that portion of extent and in turn determines the slab s containing the requested data. The request is then handed to the RAID system module for further processing and the determination of which storage device s hold the data before issuing requests to the appropriate storage device driver s . The storage device driver s access es the data from the specified device s and loads the requested data block s in memory for processing by the node. Upon completion of the request the node and operating system returns a reply to the client over the network .

The data request response path through the storage operating system as described above can be implemented in general purpose programmable hardware executing the storage operating system as software or firmware. Alternatively it can be implemented entirely or partially in specially designed hardware. That is in an alternate embodiment of the invention some or all of the storage operating system is implemented as logic circuitry embodied within for example one or more field programmable gate arrays FPGAs application specific integrated circuits ASICs programmable logic devices PLDs or some combination thereof.

The N module and D module each can be implemented as processing hardware configured by separately scheduled processes of storage operating system however in an alternate embodiment the modules may be implemented as processing hardware configured by code within a single operating system process. Communication between an N module and a D module is thus illustratively effected through the use of message passing between the modules although in the case of remote communication between an N module and D module of different nodes such message passing occurs over the cluster switching fabric . A known message passing mechanism provided by the storage operating system to transfer information between modules processes is the Inter Process Communication IPC mechanism. The protocol used with the IPC mechanism is illustratively a generic file and or block based agnostic CF protocol that comprises a collection of methods functions constituting a CF API.

The following is a description of the internal functionality and architecture of an illustrative system that can be used to practice the techniques described with reference to discussed below. Of course it is understood that the following system architecture and functionality is defined only for convenience in understanding an exemplary mechanism by which the techniques may be practiced. Other mechanisms with different architectures and internal functionalities may also be used to practice the techniques discussed herein as long as the overall physical to logical storage hierarchy such as the generic architecture discussed further below with reference to is maintained.

The system described here includes a file system and layout engine. As shown in in one embodiment the system provides several distinct data layers including an aggregate layer also called slab allocation layer which sits on top of the RAID layer a region layer which sits on top of the aggregate layer and a volume layer also called file system layer which sits on top of the region layer . In one embodiment these layers are implemented by the storage manager in the D module of each node in a storage cluster as discussed further below.

In one embodiment the system divides RAID groups into two dimensional arrays of data blocks and then further divides up those arrays into slabs . Slabs can be defined along RAID stripe boundaries. The block arrays and slabs are contained within the aggregate layer . The system further defines multiple regions to contain data in the region layer . Each region can include one or more logical extents not shown in the region layer in for simplicity . Allocated to each extent is at least a portion of one or more slabs that are allocated to the region that includes the extent.

The system also maintains multiple volumes or file systems as logical containers of data in the volume layer. Each volume includes one or more of the logical extents from one or more of the regions . Notably the region layer hides the layouts of the logical extents within the various regions from the volume layer and therefore from the clients and users .

The various slabs can be defined from a heterogeneous pool of physical storage devices and any given region can include extents built from slabs of two or more different types of physical storage device such as flash memory solid state drives SSDs HDDs etc. By their nature these types of physical storage devices have different reliability characteristics. Accordingly the slabs defined from the different physical storage devices have different reliability characteristics. In embodiments the various slabs are defined using a block of storage area or storage slices selected from multiple physical storage devices where the storage areas use a common parity based protection scheme against data loss. Accordingly in such embodiments the various slabs would each correspond to a particular parity group. Here each slab may be envisioned as an abstraction of a parity group to corresponding upper layers.

The volume layer is created and managed by the volume manager . The volume manager handles the creation and deletion of volumes. The storage manager includes a volume layout manager for each type of volume in the volume layer . Each volume layout manager implements the internal organization of the corresponding volume type and is responsible for assembling externally visible data objects e.g. files or LUNs from extents. It also implements whatever naming scheme is appropriate for the volumes objects e.g. hierarchical pathnames for traditional file systems LUN IDs for a LUN volume etc. .

Below the volume manager are a region manager and a region layout manager which provide and manage regions. The region manager manages the overall population of regions associated with the storage manager. It decides on the assignment of individual extents to a suitable region e.g. one that includes slabs of physical storage of particular characteristics . The region manager also decides when new regions are needed and what type s they should be and it creates them. It also monitors size and free space of regions. For example the region manager might decide that a region has grown too large and split it into two smaller regions or it might ask a region with a lot of free space to return one or more slabs to the slab allocation manager . When the volume layer needs to create new extents the region manager decides in which region s to place the extents.

The storage manager creates a separate region layout manager for each region in the region layer . Each region layout manager is responsible for managing the internal functionality of the corresponding region and in particular for determining the actual physical placement of data within the region. More specifically a region layout manager determines the allocation of the individual extents to the physical storage blocks within the slabs that make up the corresponding region i.e. it makes layout decisions for the extents stored in the region corresponding . Each region layout manager also manages and determines format and storage locations for its region internal metadata. Each region layout manager provides a block I O interface to the RAID layer.

The region extents interface provides communication between the volume manager on one hand and the region manager and region layout manager on the other hand. The slab allocation manager sits below the region manager and region layout manager and above the RAID system module which implements the RAID layer and is responsible for creating and allocating slabs. The slab allocation manager allocates slabs in response to requests from region layout managers . It has the global knowledge of how many slabs of each type exist and it can inform the region manager when it is low on a particular type of slab causing the region manager to identify regions that are underutilizing and can therefore release slabs of that type. The slab allocation manager requests parity groups from the RAID system module from which it carves out slabs.

In one embodiment the RAID layer is implemented as declustered RAID. Declustered RAID is a RAID implementation that slices individual physical devices in the heterogeneous pool into storage areas or slices and then assembles the slices from different devices into different parity groups where each parity group comprises slices or storage areas that have at least a common parity based protection scheme against data loss. In embodiments the slices within each parity group may have other common physical characteristics e.g. type of physical storage device in addition to the common parity based protection scheme characteristic. The size of the parity groups are not tied to the physical size of the storage devices in the pool.

In one illustrative embodiment as shown in parity groups are constructed from selected storage devices or elements from the various RAID groups. For example a RAID aggregate may include a first group that is comprised of SSD RAID elements . A second RAID group may be comprised of HDD RAID elements. Disks or other such storage elements are selected from the various RAID groups and selectively built in to various parity groups. For example parity group A may include slices of disks selected from the various RAID groups of the RAID aggregate that have a particular parity based protection scheme e.g. 3 1 RAID 4 against data loss. Similarly parity group B includes slices of disks selected from RAID groups where the selected slices have a second type of parity protection scheme. As illustrated in the exemplary embodiment in the slices of disks having a common parity protection scheme from the different parity groups are slid in to form different parity groups e.g. parity group A parity group B etc. .

Above the parity groups the slab allocation layer takes the two dimensional arrays of blocks and carves them along stripe boundaries into many much smaller slabs of storage. The number of stripes in a slab is related to the underlying physical storage type for example HDD slabs may be at least few tracks long while SSD slabs may be at least an erase block long. At the same time slabs are kept relatively small because they are the basic unit of space allocation to the next higher level in the system i.e. the regions.

A region holds logical virtualized extents of data. Each extent is simply a range of bytes of data or metadata stored in a region and accessed via an extent identifier ID . Reference counts for extents are maintained within the region allowing for external sharing of extents. The layout and location of an extent within a region is hidden from the users of the extent i.e. from volumes clients end users .

The virtualization of extents within regions is an architectural advantage for the system. Traditional file systems manage the performance space efficiency and reliability of an extent of data through direct control of the layout of the data. In the system described here expectations are expressed for example through the SLO of an extent. A region completely hides the details of the location of the data and how the SLO is honored. This gives the region the latitude to implement algorithms such as compression or storing very similar extents together sharing most of their data blocks and the few divergences.

The isolation of the internal structure of regions allows for the implementation of multiple internal region layout manager entities which optimize the organization of the internals of the region for specific workloads. Different regions can be optimized for different purposes including different internal layouts and algorithms as well as dynamically shifting mixes of underlying storage. Extents with very different SLOs can be stored in different regions. For example in there are shown three types of extents namely LUN data L small file data S and metadata M . These three types of extents are stored in three specialized regions A B and C each with its own internal format to map each extent ID to its storage. The different regions A B and C are also using different mixes of storage slabs as dictated by their need to satisfy the SLOs on their extents.

The top layer of the system is the volume layer . As shown in volumes can be structured as trees of variably sized extents. Bottom level extents hold the data of the volume while higher level extents store the metadata that organizes the lower level extents. In the example of three volumes A B and C in an aggregate are each made up of a tree of extents where the extents are maintained in three separate regions A B and C. Each volume contains data as well as metadata. Further it can be seen that volume B includes two different classes of data as well as metadata. The different classes of data and the metadata have different SLOs and so are stored in different types of regions. Administrators can express their preferences for data sets through options in data management software the details of which are not germane to the techniques introduced here . These preferences are translated into objectives expectations on the particular volumes A B and C and data objects within them and eventually to objectives on the different data and metadata extents. At the region level all objectives with regard to the performance space efficiency and reliability of the extents are conveyed through the SLO of the extent.

Referring again to the system allows for different volume types. All data and metadata is stored in files and each file is a tree of extents rooted at the file s inode primary metadata container . The inode itself can be stored in the data extents of a separate inode file.

As is discussed in greater detail below the SLOs of metadata allow a volume to specially treat high level metadata such as volume level metadata blocks. The system can store the high level metadata needed to boot the aggregate in special purpose regions allowing rapid boot takeover and high level repair. By storing critical metadata in storage with a high level RAID redundancy the system can reduce the exposure to repair related downtime.

An example of a hierarchy of all of these layers is illustrated in . The RAID system module has assembled slices of HDDs and SSDs into virtualized RAID groups and assembled those RAID groups into an aggregate. The slab allocation layer sees these parity groups as large arrays of blocks which it cuts into slabs. Hence the system has sliced the parity groups into slabs and allocated some of those slabs into two regions . The two regions are holding different classes of data extents from the two volumes above them. Finally the data in the volumes is exported through the CF interface .

The access path to a data container can be similar to that in a conventional file system. For example each container can be identified in the storage cluster by its unique ID. The N modules route data to the appropriate D module using the container s unique ID stored in a system wide map e.g. the VLDB .

In one embodiment the RAID layer or more precisely the RAID system module communicates a list of its parity groups to the storage manager . For each parity group the RAID layer informs the storage manager of the parity groups width i.e. the number of slices mapped to different devices that can hold data number of blocks block size type of physical device e.g. HDD SSD and potentially subtypes e.g. RPM inside tracks etc. . Since there are generally massive commonalities between parity groups within a RAID group or aggregate this can be achieved with a single type field and a table lookup. The system can tag blocks written to RAID with the triplet parity group ID slice number block offset .

As described above the slab allocation layer takes the parity groups supplied by RAID and carves them into slabs which are smaller subarrays of blocks allocated from the parity group s larger array. Slabs inherit their performance and reliability traits from their parity group. They are allocated as a range of parity stripes so the width of all slabs on a parity group is uniform within the group. The degree of freedom in slab allocation is the number of stripes in the slab. Slabs can be created in a range of standard sizes and can be subdivided or combined as needed. On spinning media e.g. HDDs a minimum slab length may be chosen to approximate a small multiple of track size while on SSDs or storage attached flash the minimum slab length may be an erase block for example.

Regions are virtual logical storage containers that use a collection of slabs to hold logical extents of reference counted data. A region will know at least some physical and reliability characteristics of each slab that is allocated to it including 

A file within a volume is made up of one or more extents which are contained within one or more different regions. An extent is a logical piece of data. Different extents can be of different sizes. In one embodiment extents can be relatively large e.g. on the order of many tens of MB. For each extent the region also stores an extent descriptor which is a collection of metadata about the extent similar to an inode . The extent descriptor will keep information such as the extent s size when it was created and its SLO. The extent descriptor is also used by the region layout manager to translate from logical addresses in the extent to physical addresses in the storage managed by the region. This can be done by using a buffer tree similar to that used by conventional block oriented file systems.

The above description provided the internal functionality and architecture of an illustrative system that can be used to practice the techniques described with reference to discussed below. As indicated above it is understood that the above system architecture and functionality is illustrative and other systems with different architectures and internal functionalities may also be used to practice the techniques discussed in the following sections as long as the overall physical to logical storage hierarchy such as the generic architecture discussed further below with reference to is maintained.

As noted above the system described here can dynamically manage data allocation in the physical layer e.g. the RAID layer according to reliability information associated with higher level logical data e.g. volume level data . The major principle here is that toward implementing a service level storage system the system can use for example SLOs to determine the type of reliability required for allocating a particular logical entity e.g. a volume a file a directory etc. and then allocate physical data storage blocks for the logical entity based on the reliability.

In embodiments the system allocates data corresponding to the logical entity to a particular parity group based on the reliability value associated with the logical entity. For example SLOs may define a high reliability requirement for logical entities representing an important project workspace e.g. a directory or a volume corresponding to a company s vital employee profile management project and a low reliability requirement for logical entities representing unimportant and temporary storage areas e.g. a directory designated as scratch space for holding temporary files . The system in embodiments allocates physical storage blocks for each logical entity based on the reliability definitions. For example data corresponding to the project workspace is allocated to physical storage blocks from for example a parity group that has high protection level e.g. RAID DP . Data corresponding to the scratch space is allocated to physical storage blocks from for example a parity group that has a lesser protection level e.g. RAID 1 .

In particular such reliability SLO based allocation allows the system to make perform efficient data operations. For example the system prioritizes the execution of background maintenance operations e.g. disk consistence checks based on the reliability level associated with the various parity groups. In an illustrative embodiment the reliability level of the various parity groups are encoded within the metadata associated with the parity groups. Prior to running a background maintenance operation on a given physical storage element e.g. a hard disk the system retrieves a list of parity groups the physical storage element participated in and orders the list according to the reliability levels associated with the parity groups. Subsequently the system prioritizes operations according to the ordered list. In one example of a prioritized operation the system may run more frequent background maintenance operations on parity groups i.e. storage blocks comprised in parity groups with high reliability values and occasional background maintenance operations on parity groups with lower reliability values. This way the system s resources are freed up and used more effectively in performing maintenance operations on important blocks instead of uniformly spending the resources across all parity groups.

In another example of such reliability SLO based data operations the system prioritizes reconstruction of failed physical storage elements e.g. a failed drive based on the reliability levels or values associated with the various parity groups of the system. For example upon detecting a failed drive the system identifies the list of parity groups the drive participated in and orders the parity groups according to their associated reliability values. Storage blocks in the parity groups with the highest reliability values are reconstructed first. The remaining parity groups are reconstructed according to the prioritized order based on the reliability values. In some instances the reconstruction may also be dynamically scheduled such that the high reliability parity groups are reconstructed first and the lower reliability parity groups are scheduled for reconstruction during system idle times.

It is understood that such reliability based prioritized execution of operations based on reliability levels attached to the parity groups according to for example reliability SLO targets of corresponding logical entities may be extended to other data operations e.g. snapshot creation data deduplication or other such operations as understood by a person of ordinary skill in the art. related to the physical storage blocks. The above reliability based operations are further illustrated in detail with reference to below.

The system first attaches the reliability value information to an entity e.g. volume in the logical layer. In embodiments the reliability value information is stored along with metadata attached to the logical entity. The reliability value may be assigned in one of several ways. In one embodiment the reliability value is determined based on objectives or targets established by storage SLOs as indicated in block . While many different characteristics can be captured in storage SLOs e.g. latency throughput reliability availability etc. to simplify description the following discussion will only cover reliability SLOs. An SLO can be embodied as a key value pair. SLOs can express reliability values based on particular data types or based on where the data is stored. For example SLOs may define data stored in project spaces or personal work directories as gold reliability standards and may define data stored in temporary scratch spaces as bronze reliability standards. Alternately or in addition to such standards SLOs may define ranking of volumes based on user assigned settings to the volumes or even the logical data e.g. files directories etc. stored in volumes. In one embodiment therefore a storage SLO assigns a reliability level for each high level logical entity e.g. volume of the storage system. The reliability value indicated by the SLO is stored or attached to the metadata of the high level logical entity .

In embodiments the reliability values may be assigned to a high level logical entity based on inputs received from a user of the storage system as indicated in block of . For example an administrator establishing or allocating the various logical entities for various tasks or users may categorically assign reliability values to each of the logical entities. In instances the administrator may assign the values in the form of SLO inputs that in turn get stored as reliability values in the metadata of the logical entities. In instances the administrator may review the type of logical entity being constructed and assign a reliability value according to the type of data associated with the logical entity. For example the administrator may establish a volume for a new project for maintaining employee payroll records. Given the importance of the data to be stored in such a volume the administrator may assign a gold reliability value for the volume and such a reliability value gets attached to the metadata of volume . In examples the administrator may accomplish such reliability value assignment through a user interface of the storage system. Such an interface may be afforded for example by the M Host of the storage system.

In other embodiments the reliability values may be automatically assigned to the volume based on the type of data being stored in a volume as indicated in block . Again this may be accomplished in conjunction with storage SLOs or by means of inputs provided by a user. For example the storage system when establishing a logical entity e.g. volume determines a type of data to be stored in the entity. The type of data may be determined for example based on information that may be supplied a request is received to create the entity. Based on the type of data the system may look up information provided by storage SLOs or input provided by the user to determine a reliability value to be attached to the logical entity e.g. volume .

Using one of the above mechanisms the reliability values are attached to the high level logical entities. In the illustration in volumes and are assigned gold standard reliability values because they are respectively to be used to store data relating to a project workspace or the user s personal workspace. Volume designated as scratch space is assigned a bronze standard reliability value. The eventual allocation of physical storage blocks is now based on the reliability values assigned to the logical entities. On a high level data from each logical entity is assigned to physical storage blocks selected from a particular parity group based on the reliability levels. For example volumes and with their gold reliability values are assigned to storage blocks from a parity group with a high reliability coefficient. In one embodiment the reliability coefficient is a function of the protection level afforded by the parity group. Accordingly data corresponding to volumes ad are allocated to physical storage blocks selected from for example parity group with double parity e.g. a RAID DP parity group or other such high protection characteristics. Volume may in turn be allocated to physical storage blocks from parity group that has lower protection characteristics e.g. a RAID 0 parity group .

In embodiments the reliability value is propagated to the parity groups based on the data stored in physical storage blocks selected from the parity groups. To illustrate this physical storage blocks that store data corresponding to the gold reliability value are selected from parity group in the above example. Accordingly a gold standard reliability value is attached to parity group . In one embodiment such reliability value information may be attached to metadata associated with the parity group. In this manner by virtue of reliability values propagated to the parity groups from the logical entity reliability values are explicitly attached to the parity groups. In embodiments the reliability values may also be attached to the individual storage devices e.g. storage disks that are inherent to each parity group.

An illustrative manner by which the reliability information may be propagated from the logical entity to the physical storage blocks is explained with reference to the exemplary architecture discussed above with reference to . Reliability values are first assigned and attached to the individual volumes in volume layer . In one embodiment such reliability values are attached to metadata associated with the volumes . The volume layer in addition to mapping the individual volumes to individual regions and placing logical extents within each region also translates the volume level reliability values to corresponding regions of the volumes . The reliability value of each region is basically the same as the volume that the region corresponds to. In embodiments the reliability information is stored in metadata associated with each region and also attached to the logical extents defined by the regions. When the regions apportion storage by acquiring slabs from the aggregate layer the system selects slabs based on certain attributes related to the storage SLOs. For example metadata associated with the slabs specify attributes of the storage blocks specific to the slabs. The attributes may specify attributes such as for example device type e.g. SSD high capacity SATA drive etc. I O bandwidth specifications protection level RAID 0 RAID 1 RAID DP etc. and other such parameters. The system described here focuses on the protection level attribute to select a slab that has a protection level commensurate with the reliability level specified for a volume corresponding to the selected slab s . For example a slab with a RAID DP protection level attribute is selected for volume which has a gold reliability value . In embodiments the system may also take into account other attributes e.g. disk type in addition to the protection level attribute in determining the selection of particular slabs.

The selection of the slabs based at least on the protection level attribute effectively allows the system to select physical storage devices from a parity group that corresponds to the protection level attribute of the selected slabs. For example a slab with protection level attribute RAID DP comprises physical storage blocks that have a RAID DP protection level. If in some instances a slab with a protection level attribute RAID DP and a disk type attribute SSD is selected such a slab would comprise physical storage blocks that satisfy both attributes. Accordingly in this manner the selection of the protection level attribute dictates the selection of physical storage devices from a specific parity group. The system stores data corresponding to the logical entity e.g. a volume from volume layer in the physical storage blocks selected from a specific parity group based for example on the SLO reliability value associated with the logical entity .

As indicated above in addition to allocating parity groups based on the reliability value the system also encodes e.g. by incorporating as metadata information the reliability value of a logical entity to both the parity group from which the physical storage blocks are selected as well as to the physical storage units as well. In this manner the reliability information is propagated and explicitly encoded at the lower levels e.g. RAID layer level of the storage system. Using this explicit reliability information the lower levels of the storage system can efficiently optimize data operations such as for example integrity checking backup reconstruction schedules etc.

The process then returns to step where the system identifies a specific parity group in the physical storage entity layer from which physical storage blocks should be selected. In embodiments the system selects such a parity group based on the reliability level associated with the logical entity. For example a RAID DP parity group is selected for a logical container with a gold reliability value and a RAID 0 parity group is selected for a logical container with a bronze reliability value. Subsequent to selecting the specific parity group the process continues to step where data corresponding to the logical container is allocated to physical storage blocks within the selected parity group. Further at step the system attaches the reliability value to metadata associated with the parity group. Additionally in some instances the system also attaches the reliability value to the individual physical storage units that store the information associated with the logical entity.

As discussed above allocation of the physical storage blocks in this manner has several benefits. For example the system now has the capability to perform data operations e.g. maintenance operations backup operations reconstruction operations etc. in a prioritized order based on the reliability values of the parity groups. now illustrates an exemplary process by which the system performs such prioritized data operations. At step the system receives a request e.g. as a user input as a system initiated command as an interrupt operation as a failure recovery operation etc. to perform the data operation relating to physical storage units or even to an entire physical data element e.g. a failed drive .

In response to receiving the request the process proceeds to step where the system identifies a list of parity groups that the physical store units or the physical data element participated in. Subsequently at step the system determines e.g. by analyzing metadata associated with the parity groups whether the parity groups have associated reliability value information. If such information is not available as indicated in step the process shifts to step where the system identifies the reliability value associated with the parity groups by traversing the storage hierarchy. For example the system may query metadata associated with slabs to which the parity groups are attached to determine the reliability value. If that is unsuccessful the system traverses up the chain all the way for example to the volume layer to identify the reliability value associated with the logical entity. The system identifies the identified reliability value to the associated parity groups and the process then shifts to step .

At step the list of parity groups is sorted to generate a prioritized ordered list of parity groups. In an illustrative example the ordered list first lists all the parity groups with gold reliability values then lists the silver parity groups and finally the bronze reliability values. Finally at step the system performs the data operation on the physical storage units according to the prioritized list. For example the data operation is first performed on physical storage units belonging to gold rated parity groups. The operation is then performed on the physical storage units belonging to the silver rated parity groups and so on. The data operation may also be staggered or selectively enabled based on user preferences to further increase efficiency of the data operations. For example during recovery of a failed drive the physical storage units corresponding to the gold parity groups are immediately reconstructed so as to avoid interruption of data access to information with high reliability requirements. However the reconstruction of the physical storage units belonging to the bronze parity groups may be pushed out indefinitely or performed only during the system s idle cycles. In this manner both reliability and efficiency objectives are met by ensuring that high reliability data is prioritized for data operations while potentially performance intensive operations on a wide latitude of low reliability data is pushed out to idle cycles.

The techniques introduced above can be implemented by programmable circuitry programmed or configured by software and or firmware or entirely by special purpose circuitry or in a combination of such forms. Such special purpose circuitry if any can be in the form of for example one or more application specific integrated circuits ASICs programmable logic devices PLDs field programmable gate arrays FPGAs etc.

Software or firmware to implement the techniques introduced here may be stored on a machine readable medium and may be executed by one or more general purpose or special purpose programmable microprocessors. A machine readable medium as the term is used herein includes any mechanism that can store information in a form accessible by a machine a machine may be for example a computer network device cellular phone personal digital assistant PDA manufacturing tool any device with one or more processors etc. . For example a machine accessible medium includes recordable non recordable media e.g. read only memory ROM random access memory RAM magnetic disk storage media optical storage media flash memory devices etc. etc.

The term logic as used herein can include for example special purpose hardwired circuitry software and or firmware in conjunction with programmable circuitry or a combination thereof.

Although the present invention has been described with reference to specific exemplary embodiments it will be recognized that the invention is not limited to the embodiments described but can be practiced with modification and alteration within the spirit and scope of the appended claims. Accordingly the specification and drawings are to be regarded in an illustrative sense rather than a restrictive sense.

