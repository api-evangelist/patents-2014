---

title: Semantic clustering and user interfaces
abstract: Semantic clustering techniques are described. In various implementations, a conversational agent is configured to perform semantic clustering of a corpus of user utterances. Semantic clustering may be used to provide a variety of functionality, such as to group a corpus of utterances into semantic clusters in which each cluster pertains to a similar topic. These clusters may then be leveraged to identify topics and assess their relative importance, as for example to prioritize topics whose handling by the conversation agent should be improved. A variety of utterances may be processed using these techniques, such as spoken words, textual descriptions entered via live chat, instant messaging, a website interface, email, SMS, a social network, a blogging or micro-blogging interface, and so on.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09275042&OS=09275042&RS=09275042
owner: VirtuOz SA
number: 09275042
owner_city: Paris
owner_country: FR
publication_date: 20140124
---
This application is a continuation of U.S. application Ser. No. 12 748 150 filed Mar. 26 2010 now U.S. Pat. No. 8 694 304 issued Apr. 8 2014. The entire teachings of the above patent are incorporated herein by reference.

Companies continue to develop an ever increasing variety of techniques to interact with customers. For example a company may provide a website that includes details about products and or services of the company. Additionally the website may include support information or functionality to purchase products and services from the company. A customer for instance may interact with the website to find information about a prospective purchase and later after the purchase to find information regarding use of the purchase. Consequently the amount of information that is made available via these techniques is ever increasing which may make it difficult for customers to locate desired information using traditional techniques.

One such traditional technique that has been employed by the companies involves the use of search technologies. For example the company may include search technologies on a website to allow customers to hunt for answers to their questions. This may work well for certain types of queries and issues but may fail as questions become increasingly complex as issue resolution may require personalized information and so on. As a result users may walk away from the website frustrated may make a time consuming call to a human customer service representative CSR and so on. Therefore traditional search techniques may have a negative impact on user experience with the website and consequently on the user s view of the company as a whole.

Semantic clustering techniques are described. In various implementations a conversational agent is configured to accept natural language input from a user utterances and then perform deep linguistic analysis of these utterances. Semantic clustering may be applied to the output of such analysis to provide a variety of functionalities such as grouping a corpus of utterances into semantic clusters in which each cluster pertains to a similar topic. These semantic clusters may then be leveraged to identify topics and assess their relative importance in order to for example prioritize topics that occur frequently or topics whose handling by the conversational agent should be improved. A variety of utterances may be processed using these techniques such as spoken words or textual descriptions entered via instant messaging a website interface SMS email a social networking blogging or micro blogging service and so on.

This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used as an aid in determining the scope of the claimed subject matter.

Users may have access to an ever increasing variety of information from an ever increasing variety of sources such as via a website mobile communications device email instant messaging and so on. Consequently it has become increasingly difficult for a user to locate desired information from within this variety of information which may lead to user frustration with the traditional techniques used to access the information as well as the provider of the information e.g. the company itself.

Conversational agent techniques are described which include semantic clustering and other functionalities that are described in the following sections. In various implementations conversational agents are implemented using one or more modules to engage in an interactive natural language dialog with a user via a textual chat. Thus use of conversational agents may provide automated assistance to users to help them resolve issues without directly interacting with a human agent e.g. a customer support representative in a call center . This may help a company to efficiently utilize resources and provide additional functionality to a user that was not available via traditional search techniques. The textual chat may be inputted using a variety of mechanisms such as transcripts of spoken words such as telephone calls text inputs e.g. instant messages live chat email SMS blogging and micro blogging services and so on automatic speech recognition and so forth.

Through use of linguistic analysis techniques the conversational agent may map user inputs henceforth called utterances to semantic representations. Such representations may be graphs the nodes of which represent concepts and the edges of which represent semantic roles. Such graphs will henceforth be called semantic graphs .

The conversational agent may represent a user intent by an intent graph pattern or a plurality of intent graph patterns. Thus a user utterance may be formed into a semantic graph and compared with intent graph patterns henceforth called graph patterns . If there is a match then the utterance likely involves the intent represented by the graph pattern or plurality of graph patterns.

Semantic graphs representing utterances may be grouped into semantic clusters using various techniques referred to as semantic clustering techniques. Each of the semantic clusters may pertain to a similar topic. These semantic clusters can then be leveraged to determine dominant or active topics in a corpus of utterances for a variety of purposes. For example semantic clusters may be used to improve the quality of a conversational agent. Further discussion of semantic clustering techniques and conversational agents may be found in relation to the following sections.

In the following discussion an example environment is described along with example procedures that may be implemented in the example environment as well as in other environments. Accordingly the example procedures are not limited to implementation in the example environments and the example environments are not limited to implementation of the example procedures.

Likewise the network may assume a variety of configurations. For example the network may include a wide area network WAN a local area network LAN a wireless network a public telephone network an intranet a telephone network and so on. Further although a single network is shown the network may be configured to include multiple networks. For instance the client device configured as a desktop computer and the service provider may be communicatively coupled via the Internet and the client device configured as a wireless phone may be communicatively coupled to the service provider via a telephone network. A wide variety of other instances are also contemplated.

The service provider is illustrated as being implemented by one or more servers or other computing devices that are accessible to the client devices via the network . Additionally the conversational agent is illustrated as a module that is implemented by the service provider . For example the conversational agent may include a user experience that is accessible via a webpage output by the service provider to the client device configured as a desktop computer. In another example the conversational agent may include a user experience that is accessible via a spoken input received by the client device configured as a wireless phone. Thus user experience of the conversational agent may be accessed through a wide variety of techniques. A variety of other examples are also contemplated such as instant messaging email user generated content in conjunction with a social network blogging and micro blogging services and so on.

Generally any of the functions described herein can be implemented using software firmware hardware e.g. fixed logic circuitry manual processing or a combination of these implementations. The terms module and functionality as used herein generally represent software firmware hardware or a combination thereof. In the case of a software implementation the module and or functionality represents instructions e.g. program code that perform specified tasks when executed on a processing system that may include one or more processors or other hardware. The program code can be stored in a wide variety of types and combinations of memory may be employed such as random access memory RAM hard disk memory removable medium memory and other types of computer readable media. The features of the semantic clustering techniques described below are platform independent meaning that the techniques may be implemented on a variety of commercial computing platforms having a variety of processors.

The conversational agent is configured to engage in an interactive natural language dialog with a human user via textual chat to complete a specific task for or on behalf of that user. For example text entered by a user through interaction with the client device configured as a desktop computer may be provided to the conversational agent . In another example a voice input provided by the client device configured as a wireless phone may be converted to text and processed by the conversational agent the response of the conversational agent can then be converted back to speech before being sent to the client device .

Tasks may include providing information to the user answering the user s questions helping the user solve a problem support agent proposing new products and services to the user sales and or marketing agent and so on.

The conversational agent may embed complex logic flows to aid interaction with the user using natural language. The conversational agent may also interact with various application programming interfaces APIs and backend systems of a company that offers use of the agent e.g. the service provider . For example the conversational agent may be offered as a visual avatar on a company web site or a specific section of the site but other interaction channels such as instant messaging systems mobile phones email social networking sites or blogging and micro blogging services are also contemplated. The conversational agent may respond to user questions and also drive the conversation to solicit specific information to better understand the user s situation.

Utterances that are received e.g. spoken or typed by a user are parsed by a linguistic analysis module of the conversational agent and may be matched by a comparison module against a number of possible intents that are part of one or more decision trees . Based on the identified intent the conversational agent may then generate a reply. A conversation between the user and the agent may include one or more of these interactions between the user and the conversational agent .

A user s intent can be expressed in a variety of ways. For example the user s intent may be configured as a single information request may include a set of several potential information requests and so on. In the latter case the conversational agent may ask for clarification until a specific information request is identifiable and may be satisfied.

In one or more implementations conversations are modeled as paths through a set of decision trees which may be configured as circuit like structures that describe possible conversation flows. The root of each decision tree may describe an initial state before a user intent has been identified. Leaves of the decision tree may be thought of as answers to a specific request. Accordingly a path from the root to a leaf of the decision tree may represent a sequence of utterances e.g. speech acts that may lead to identification of the information requested by the user and thus completion of the conversational agent s task. In addition to a simple traversal of the decision tree the conversational agent may offer increasingly complex dialog strategies that allow the user to switch between tasks or decision trees flexibly.

The set of intents that can be matched to user utterances at a particular point in time relates to a current position of a conversation in the decision tree . For example a customer of a telecommunications company might initiate a conversation by asking Can I access my voice mail from the web Upon recognizing the intent of the question the conversational agent moves from the decision tree s root node to one of the root s child nodes. Assuming that the company delivers phone services through a cellular network landlines and VoIP the conversational agent may consult the information that is relevant to proceed in the decision tree and respond with a clarifying question e.g. What type of phone service do you use for your voice mail 

Assuming the user answers with an utterance that includes sufficient information and is recognized by the agent the conversational agent has identified the user s intent and moves to a leaf node which contains an answer to the user s question. It should be noted that a user utterance such as VoIP may be associated with a different intent when produced at the beginning of the conversation at the root of the decision tree as opposed to later in the conversation at the node corresponding to web access to voicemail.

In addition to the current position in the decision tree the conversational agent may have knowledge of pieces of information that were obtained earlier during the conversation. For example this additional information may be represented as variable value pairs which may act to limit the user from asking a same question multiple times asking for information that was already provided by user and so on. Additionally the conversational agent may implement complex and sophisticated conversation strategies. For example the conversational agent may proactively ask questions and volunteer related pieces of information based on information known about the user from the conversation or other data collected about the user e.g. via an API provided by the service provider offering the conversational agent .

The main trait of a node is the concept it represents. In an implementation concept traits e.g. modify in are abstracted over lexical variations and spelling mistakes. For example replacing the word change by alter modify or even moddify sic in the user input does not affect the structure of the semantic graph shown in . Likewise representing my and I by the concept Interlocutor makes the interpretation of the semantic form insensitive to the form used in the user utterance. For example the utterance You would like to modify your order may be parsed by the linguistic analysis module to form the graph shown in . Similarly We would like to change our order may also be parsed by the linguistic analysis module into the graph shown in .

In implementations constructions such as would like to are represented by a modal trait on the modify node and not a concept trait. Additionally this particular value may be present on one or more of the utterances I d like to I want to I wanna and so on. In this way a single representation may be provided for a variety of synonymous constructions. On the other hand use of a dedicated modal trait rather than creating a node with a want to concept trait may help to simplify the semantic graphs and thus facilitate pattern matching further discussion of which may be found later in the Pattern Matching section of the description.

The graph edges that are drawn in may be referred to as function edges with the respective labels representing semantic roles. For example in Order is the theme of Modify i.e. the object acted upon. In this is denoted by the Theme label on the edges between the two nodes denoting the theme semantic role. The Theme label may still fulfill this role even if the same relation were expressed in the passive e.g. Has my order been modified where order is a syntactic subject. Thus semantic abstraction may be used to provide a straightforward identification of common ideas in different utterances which may increase the efficiency and accuracy of a pattern matching process to be discussed later.

In the semantic graph function edges and their incident nodes form a tree. In implementations the root of the tree may be used as a placeholder that does not represent a particular concept of the utterance. For example the concept trait may be set to a value Top which is representative of the most general concept.

It should be noted that parsing may focus on extracting dependencies between words which may then be mapped to dependencies between concepts. This approach known generically as a dependency grammar does not make assumptions on phrase structure. Therefore incomplete sentences and ungrammatical sentences may be handled and mapped to a semantic graph much in the way a human may extract meaning from ungrammatical or incomplete sentences. This approach allows a conversational agent to be robust and able to understand real user utterances which are often grammatically incorrect may contain typos and spelling mistakes and may use slang words or phrases.

Because there may be a variety of spelling suggestions for a word and a lexical entry may include several words for example credit card or bill of sale the lexical module of the conversational agent may map a word sequence of the user utterance to one or more flexion sequences. A flexion is a lexical entry that includes a lemma i.e. an uninflected form and possibly grammatical marks such as tense number person mood and so on. For example the lemma agent may have the flexions that include agent and agents. 

In an implementation the lexicon that is used to match words to flexions is language dependent. Additionally some of the entries contained therein may be specific to a business area a conversational agent and so on. For example lexical entries may include names of forms specific to a business area or commercial names specific to the conversational agent . Accordingly lexicon lookup may be filtered by word spaces where a word space characterizes a conversational agent or a business area.

At the syntactic module level information that is common to the flexions of a given lemma is stored in a dictionary . This information may include 1 construction information and 2 ontology information. Ontology information pertains to the semantic level and provides the concept traits which are further mentioned in the Parsing and semantic representations of input sentences Section and . Construction information includes possible part of speech assignments to a lemma For example format may be assigned both verb and noun constructions. Construction information may also include syntactic patterns linking the dictionary item to other items. For example the construction for format as a verb may show that the item relates to a noun with a subject link and to another noun with an object link.

A unification based algorithm may be employed to unify available constructions of the lemmata i.e. a plurality of lemma in a sequence to yield one or more syntactic graphs. In addition to part of speech information linearity information e.g. in English a tendency of objects to occur after verbs and the confidence assigned to the recognition of particular constructions may be taken into account to score the graphs.

At the semantic module level a highest scoring syntactic graph is mapped to a semantic graph . As a result of this process a semantic graph having increased abstraction is obtained in which nodes represent ontology concepts and edges represent logical relations between the concepts.

Ontology may be represented as a language independent concept hierarchy. This hierarchy may be represented using a directed graph with two types of edges is a kind of and subsumes. In the example shown in for instance a password is a kind of certificate or credentials and is a kind of secret or arcanum . In turn secret or arcanum also subsumes esoterica and kabbalah while certificate or credentials subsumes login name identity card and diploma .

For illustration purposes suppose the conversational agent has been designed to help users change their password on the web site that embeds the conversational agent s user experience . A user may express this request in a variety of ways. Consider for example the user utterances How does one change one s password How to change password How should I go about changing my password Need to change my password. How do I do that and Would you be so kind as to tell me how to modify my password Each of these wordings contain the concepts how and change password with substantial variation in the exact way these two concepts are linked to each other as well as in the use or omission of pronouns.

One way to capture an intent common to these utterances is through use of semantic representations that contain graph fragments. depicts an example implementation of a graph pattern that includes graph fragments for 1 change password with password functioning as the theme of change and 2 how . These fragments form a graph pattern which for purposes of the discussion that follows may be simply referred to as a pattern . An utterance is considered as matching this pattern if each of the pattern fragments occurs in a semantic graph of the utterance. It should be noted that this is a condition that is considered sufficient for matching further discussion of this and other conditions may be found in relation to the following section.

For example semantic graphs for how to change password and need to change my password. How do I do that both contain these fragments examples of which are illustrated in the implementation of . In this implementation examples of semantic graphs that match the two graph fragments of are shown.

Suppose the conversational agent has been created to explain how to change credentials i.e. user ID and or password rather than a password specifically. Accordingly a pattern may be defined to match questions about how to change one s password as well as a user ID or other credentials. This is an example of a situation in which information to be provided by the conversational agent may be described by a general concept that subsumes a number of more specific concepts that are likely to occur in user utterances. For example the conversational agent may deliver generic information about connecting an Internet router but requests for this information are likely to mention specific router brands and models.

Concept subsumption may provide flexibility to the conversational agent . In implementations the conditions that are to be met for a match to be considered between a semantic graph and a pattern are stated as follows A pattern matches a semantic graph if and only if a subgraph of the semantic graph subsumes the pattern. Continuing with the previous example a simple example of graph subsumption would be the semantic graph for change credentials as subsuming the graph for change password an example of which is shown in the implementation of .

In given that ontology defines password as a child of credentials as previously discussed in relation to Graph subsumes Graph in this implementation . More generally a graph g subsumes a graph g if and only if g can be transformed into g by zero or more applications of the following operations 

Trait subsumption has been illustrated in for concept traits. However it should be noted that trait subsumption may be defined on a variety of traits including function labels on edges. Here are other examples 

Subsumption for modal values is based on sets of possible values. Either a trait takes its value in a hierarchy e.g. edge labels ontology concepts or in a collection of sets. For example the modal value MUST is really a singleton set that includes a single instance of MUST. 

In addition to capturing stylistic variations on a question matching also helps capture logically distinct but equivalent ways of expressing the same intent. For example a user might ask how she can change her password by typing How can I change my password or by typing Can you help me change my password Therefore a single intent is not usually captured by a single graph pattern. Accordingly several graph patterns may be used. This set of patterns forms a logical disjunction meaning that in order to match the intent a user utterance matches at least one of the patterns.

A set of possible intents may be associated with each position in a conversational agent s decision tree . This set is the union of the intents of the child nodes at that position in the decision tree . Each of the possible intents at the current decision tree position is represented by a set of graph patterns. The set of patterns collectively representing each of the possible intents at a current position are referred to as the active patterns in the following discussion.

Given an utterance and a current position in the conversational agent s decision tree the conversational agent may perform the following steps to determine user intent 

If no successful match can be found in step 2 above we say that the utterance is unmatched. In such a case the conversational agent may not have the linguistic knowledge to assign an intent to this utterance.

A number of metrics may be used to measure a distance between a graph of an utterance and a matching graph pattern in the conversational agent s knowledge. These metrics may combine one or more of the following quantities algebraically 

The matching distance between two trait values quantity 2 henceforth called subsumption distance may be computed as a function of 

The conversational agent may also leverage indirect patterns that are assigned low confidence and may be used in cases when the conversational agent is not sure of the user s intent. Exclusive or direct patterns may take precedence over non exclusive or indirect patterns when identifying a user s intent by the conversational agent . If the user s utterance does not match one or more direct patterns each indirectly matching intent may be considered as potentially relevant. The conversational agent may then offer the user a list of question rewordings or a list of potentially relevant topics or questions. This may occur when a user has entered several keywords but not a full sentence or phrase that more fully describes what is being requested.

For example a user may type cashback which might mean How does cashback work or I never received my cash back. A designer of the conversational agent may address this situation in a variety of ways examples of which include the following 

The first method may be useful in specific situations for conversational agents where several keywords or ideas are used throughout by the agent in a wide variety of contexts. Therefore more precise information is to be gathered to differentiate between them. The second method that relies on indirect patterns makes it possible to deal with intent ambiguity with minimal demands on designer time.

Semantic clustering may be performed to provide a variety of functionality. For example semantic clustering may be performed to group a corpus of user utterances into zero or more semantic clusters where each cluster corresponds to utterances that pertain to the same or to similar topics. This similarity of utterances inside each cluster may be used for a variety of purposes further description of which may be found below. Furthermore this similarity may be used as an input to a human reviewer to a machine learning algorithm to an A B testing algorithm to improve performance of the conversational agent or to an alerting system which may identify or react to new or existing user issues among other applications.

A designer of the conversational agent also called reviewer may improve the quality of a conversational agent by examining the user utterances that are not matched to a direct intent pattern and or that get matched to indirect patterns exclusively. Review of these utterances by the designer may indicate whether new intent patterns or new topics are to be added to the agent s content. This review may be performed in a variety of ways further discussion of which may be found in relation to the following sections.

Traditional ad hoc approaches that selected random utterances or conversations were inefficient and could miss issues. By using semantic clustering however the quality of the conversational agent may be increased by 

In some instances user utterances may be matched to a wrong intent. Typically this occurs because patterns that were previously inserted into the conversational agent s configuration are too general or are simply wrong. Traditional ad hoc techniques that were developed to address these challenges would rely on reviews or testing of randomly selected utterances or conversations and detection of manifestations of disinterest disapproval or hostility from the user. This ad hoc approach suffers from lack of prioritization and amount of time required. Further many users ask their question receive an answer and simply close the conversational agent thereby giving no indication whether the answer provided really answered their question.

Through use of semantic clustering described herein however a reviewer may spot erroneous intent identification by 

Identification of the most salient topics in a body of utterances may be employed in a variety of areas. For example such identification may be conducted for marketing purposes for identifying strong and weak points of a service for identifying new topics that are being discussed by users for analyzing the evolution of topics over time for creating a new conversational agent using content from pre existing sources and so on.

Additionally the utterances can originate from text based chats with the conversational agent text based chats with live people e.g. live chat interactions or instant messaging from emails SMS blogging or micro blogging sites social networking sites phone transcripts or any other form of communication that may result in the production of a body of utterances that may be parsed into semantic graphs.

One such area that may employ the identification techniques is to identify evolution of a topic profile of a communication channel over time. In particular topic evolution may be tracked in near real time and be used to assess the significance of topics as topics come into prominence in order to react to the situation that caused the topics to become prominent.

Another such area is to compare two or more distinct utterance corpora with respect to their respective major topics e.g. to provide identification of common and distinct topics. In addition to the qualitative information about topics quantitative information regarding the occurrence frequency or the occurrence time of each topic may also be provided.

On the other hand topic identification offers the possibility of using analytics data to improve a conversational agent. For example an analytics tool such as a web analytics tool or BI tool may produce various metrics such as measures of conversation success based on the combination of elementary criteria e.g. whether the user thanked the agent at the end of the conversation or whether the interaction resulted in a sale and filter these results by time of the day or time period. However unless such measures can be viewed per topic analytics results are generally not actionable for improving a conversation agent because the results may be aggregated across a multitude of conversations and topics without an indication of content areas that are to be improved.

By allowing a reviewer to look at conversation metrics aggregated per cluster semantic clustering makes it possible to identify topics having a relatively higher priority for pattern matching improvement. This identification may be performed based on the values of the aggregated metrics and on various conversational agent specific criteria.

Once a set of semantic clusters for a corpus is available attaching a given utterance to a cluster can provide information about the utterance s topic and more especially about the topic s importance. This would be otherwise difficult for a reviewer to do without reading a relatively large sample of the utterances in the corpus.

For instance consider a conversational agent that helps users with selling their items online. If a reviewer looks at the utterance I would like to sell a model car the reviewer have no direct way of knowing how frequent this topic actually is note this topic is very different from the one denoted by utterances such as I would like to sell a car or I would like to sell a Ford . Cluster attachment can provide this information in addition to the particular utterance at which the reviewer happens to be looking. In this particular example cluster attachment may help the reviewer understand at a glance that I would like to sell my model car is not a frequent topic for this conversational agent.

Since cluster sizes are a good indication of the prevalence of a topic cluster attachment enables a reviewer to assess the seriousness of failure by the agent to correctly respond to a particular utterance. Without the ability to reliably carry out such assessment contingent feedback from reviewers and other interested parties are prone to lead to futile or even detrimental improvement efforts. The latter occur when remedying an issue in a minor area may cause a regression in an important area.

In addition to providing frequency information about the topic underlying an utterance cluster attachment also leads to topic identification which has useful applications for displaying utterances in a UI and allowing an agent to respond helpfully to sentences whose intent has not been identified by other means than topic identification.

Clustering in general may involve an array of techniques that use a distance metric in order to subdivide sets of objects into one or more groups of closely related objects.

Semantic clustering is an approach that is based on the clustering of semantic graphs that are produced by a linguistic analysis process such as the one previously described in the Matching User Input Utterances to User Intents section using one or more of the distance metrics described below or elsewhere. The objects that are clustered in this context are the semantic graphs produced by linguistic analysis. The clustering of semantic graphs induces a clustering of the corresponding utterances that generated these graphs thus use the term semantic clustering may refer to both clustering of semantic graphs and clustering of utterances.

Provided a suitable distance metric is used semantic clustering amounts to grouping utterances by user intent or topic even though users may describe the same user intent or topic in different utterances using different words and grammatical structures.

When applied to a corpus of user utterances that have failed to be matched to a specific direct and or indirect intent henceforth called unmatched utterances semantic clustering enables reviewers to 

In an implementation a corpus having a size between 10 000 to 100 000 unmatched user utterances is used for semantic clustering. However fewer unmatched utterances may be used for cases where the volume of data is relatively small and larger numbers of unmatched utterances may be clustered for conversational agents or other text corpora that have a sufficient volume of conversations. The number of utterances may also be chosen depending on whether the semantic clustering is to be done on a near real time basis.

Sentences and other user utterances are represented by their semantic graphs which may be a result of the parsing process described in the Matching User Input Utterances to User Intents section previously. It should be noted that this in itself may provide an initial level of clustering. For example in a corpus of 50 000 utterances the same semantic graph may be used to represent 10 to 20 different utterance forms.

Semantic clustering may be used to create groups of utterances that for a human reviewer are intuitively close to each other because they are related to the same or to similar topics. To quantify this intuitive notion of semantic closeness a number of proximity metrics may be defined to compare two or more semantic graphs. For example concept set proximity and graph intersection proximity may be used. Both types involve taking a ratio of an aggregate specificity of semantic objects common to the two graphs to the aggregate specificity of each of the semantic objects found in the two graphs. In other words these metrics represent specificity of common concepts divided by overall specificity.

Specificity will be further discussed in the following section. However generally the less frequently a term occurs in a reference corpus the more specific the term may be considered. Therefore the inverse of a term s frequency may be used as a reliable measure of specificity. The aggregate specificity of a number of different concepts may be computed by taking the sum of their specificities.

The concept set metric may be used to map each of the semantic graphs to a set of concepts and metaconcepts that it contains. The concepts are the concept traits of the graph nodes. Metaconcepts are the values found for modal traits and referent traits e.g. affirmative negative interrogative interrogative negative .

The concept set proximity of two graphs g and g may be defined as the ratio of the aggregate specificity of common concepts and metaconcepts to the aggregate specificity of each of the concepts and metaconcepts found in g and g.

In an implementation the graph intersection metric assigns a proximity to a pair of graphs g and g in the following way 

Two example heuristics have been defined to carry out the intersection step 1 as detailed below in the Intersection Heuristic and Intersection Heuristic sections.

The intersection heuristics use a concept of ontology distance between two concepts. The ontology distance between two concepts c and c is defined as a pair d h where d is the length of the path in the concept hierarchy from c to c via their closest common ancestor and h is the larger of the distance between c and the closest common ancestor and the distance between c and the closest common ancestor .

For example the ontology distance between a concept and itself is 0 0 . Between a concept and a parent concept the ontology distance is 1 1 while between two sibling concepts the ontology distance is 2 1 .

In addition for the purpose of defining the intersection heuristics in the following discussion the following may hold true 

In an example implementation the subsumption specificity of two trait values u and u and a subsuming value v has been defined as the product of 

The following is an example initial call to IntersectionHeuristic IntersectionHeuristic root root resultRoot where root is the root of g root is the node of g and resultRoot is a newly created node with concept trait Top i.e. the most general concept in the concept hierarchy .

The following is an example of use of IntersectionHeuristic After IntersectionHeuristic terminates resultRoot is the root of g g. In order to compute the aggregate specificity of g g sum the subsumption specificities associated with all the nodes of g g.

Intersection heuristic yields a maximally specific intersection except in cases where two very dissimilar nodes dominate very similar lists of nodes. In so far as in semantic graphs the semantics of a node determines the semantics of the nodes it can dominate this problem does not occur frequently.

On the other hand intersection heuristic takes into account each of the concepts in sub trees dominated by nodes c and c prior to pairing them and is therefore robust with respect to the risk of finding similar node lists under dissimilar nodes.

In terms of robustness intersection heuristic may be considered close to the optimal intersection algorithm which would rank pairs based on their graph based proximities. But while the running time of the optimal algorithm even if implemented with dynamic programming is exponential in the product of the depth of the shallower graph and the largest child list heuristic is linear in the number of graph nodes. Therefore from a computational complexity point of view heuristic is suitable for computing an intersection of a set or corpus of graphs rather than just two graphs as described in the Precision Feedback section of the discussion.

Use of inverse frequency as a specificity metric generally provides a sufficient approximation to measure the specificity of a concept i.e. its ability to characterize a specific topic. However topic selectivity may be measured in a more direct way namely by comparing a unigram distribution of concepts in a corpus with a conditional distribution of terms in utterances containing the concept to measure. For example a neutral concept like interlocutor typically occurs in a subcorpus whose concept distribution does not essentially differ from the distribution of terms in the whole corpus. Conversely a specialized concept like spark plug typically occurs in utterances forming an automotive subcorpus meaning that in this subcorpus automotive terms will be overrepresented and terms representative of other specialized topics e.g. cooking or finance are underrepresented with respect to the overall corpus. A variety of heuristics may be used to perform such a comparison between distributions examples of which include vector space sine and the Kullback Leibler divergence.

Concept set proximity generally yields results that are less precise than graph intersection proximity because the former does not take into account the shape of the graphs.

In one implementation clustering may be based primarily on a concept set based proximity metric in an alternate implementation clustering may be based on a graph proximity metric. However two level hierarchical clustering as described in the Two Level Hierarchical Clustering section may make use of the same or different proximity metrics for each level for instance in one implementation the first level clustering may use concept set proximity while the second level may use graph proximity.

On the other hand graph intersection may be used in its own right to produce a graph to be used as a decision tree pattern as described in the Pattern Generation section. In this context intersection heuristic is primarily used. However in an alternate implementation the results of both heuristics may be compared for added precision as described in the Precision Feedback section.

The semantic clustering algorithm is based on an algorithm named Quality Threshold or QT clustering. It can be outlined as follows 

The value of QT may be determined through experimentation to strike a good balance between cluster homogeneity and low cluster fragmentation. Cluster fragmentation is defined as the scattering across several clusters of utterances pertaining to the same topic. Cluster homogeneity is defined as the extent to which different utterances that have been clustered together actually pertain to the same topic. For example when clusters are not sufficiently homogeneous a smaller number of clusters may be generated however the clusters may contain user utterances that are not highly related. On the other hand if there is a high level of cluster fragmentation a relatively larger number of clusters may be generated that are not easily reviewable since similar utterances may not be grouped together in the same cluster.

A variety of proximity metrics may be used in the above algorithm examples of which may be found in the Proximity Metrics section. Since these metrics yield the same value for pairs of semantic graphs s s regardless of the exact membership of SG the basic QT clustering algorithm described above can be optimized as follows 

The following variations on the definition of cluster size provide different ways of identifying the largest cluster candidate.

In addition in an alternate implementation the following variation may be made on the threshold criterion in Step A above 

In addition in one implementation the following variation called transitive clustering may be made to Step B 

The constant QT may be chosen to be a lower threshold than QT. Transitive clustering strategy may reduce cluster fragmentation without impairing cluster homogeneity.

An implementation may use the radius criterion with transitive clustering and a quality threshold QT 0.65. These parameters may be particularly useful when clustering is combined with automatic pattern generation which is described in the next section.

Because clusters of unmatched utterances may include utterances for whose intent no direct or indirect pattern has been defined an addition to the semantic clustering algorithm may be made to add functionality to automatically generate patterns for a cluster. The patterns can be presented for inspection to a human reviewer e.g. the designer of the conversational agent who may then validate them and determine the desired intent for these utterances before adding the patterns to the conversational agent. Alternatively or additionally the agent designer may modify the patterns before adding them may decide to discard them altogether and so on. In yet another embodiment the association between pattern and intent can be performed automatically e.g. without user intervention through execution of software such as machine learning or A B testing.

The graph intersection heuristics described in the Intersection Heuristic and Intersection Heuristic sections detail basic building blocks for a pattern generation algorithm. For purposes of the following discussion it should be noted that by virtue of the way in which it is constructed the intersection g g of two graphs g and g when considered as a pattern matches both g and g. In addition g g is likely to match other graphs that are similar to g and g.

To ease the task of the agent designer the pattern generation algorithm may yield patterns in order of decreasing specificity. For example it may be easier to modify specific graphs e.g. graphs with a multitude of nodes and traits by the designer so as to capture the desired intent with the appropriate level of specificity. On the other hand a pattern that is too generic is essentially useless because it can be easily overtaken by more specific patterns or matched incorrectly to utterances for which no appropriate patterns exist. Therefore a designer reviewing a cluster s patterns in decreasing specificity order may find a point beyond which it is not desirable to continue the review. Once that point has been reached the agent designer may decide to discard the subsequent suggestions.

In an implementation members of m are marked for exclusion from further intersections to avoid generating patterns that only include insignificant differences.

A further application of generating patterns for clusters is that the pattern projections form homogeneous sub clusters thereby providing useful hierarchical clustering. In implementations clusters individually represent one or more closely related topics while sub clusters may each focus on a single topic. Viewing instances of closely related topics as identified by sub clusters close to each other as part of a single cluster may be useful to assess missing content in the agent or risks of interaction between patterns identifying similar intents.

If the reviewer decides to exclude semantic graphs the following approach may be used to generate a new pattern. If the two semantic graphs that have been intersected to generate the original pattern are still in the reviewer s selection the process stops without a pattern being generated. If however one or both of the generating utterances have been excluded the following algorithm may be launched to compute a more precise match than previously 

To compute the intersection of k graphs g g . . . gusing a binary intersection operation any bracketing is valid for instance g g . . . g . . . provided the implementation of the binary operation is associative. Since the intersection operations are heuristic approximations to the optimum this is not guaranteed but using intersection heuristic which is close to optimal provides a relatively high probability that the results will be bracketing invariant.

This approach to pattern re computation viz. an intersection heuristic may help to avoid reducing the precision of traits. In particular unequal concepts may result in a closest common ancestor in the generated pattern. This means that if after taking the intersection of the retained projection in its entirety unwanted semantic graphs are still matched either the choice of utterances to retain and exclude turns out to be contradictory or the intersection heuristic has failed to optimally pair up nodes. The risk of the second case may be minimized by applying intersection heuristic rather than the slightly faster but less robust intersection heuristic which is used for the initial match generation in an implementation.

In order to allow a designer to weigh the consequences of adding a generated pattern as is or after modifications functionality may be implemented to evaluate based on an existing corpus of utterances the impact of adding a pattern to the conversational agent s decision tree . This impact is determined by the matching extent of the pattern which will be described below.

An observation that supports this functionality is that the impact of a pattern on future utterances may be estimated by evaluating the impact of the pattern on a corpus of past utterances i.e. on the available corpus of utterances . This is true for patterns that reoccur in user utterances over time which is the case for patterns that have been obtained from semantic clustering.

Furthermore adding a pattern that has no impact on past utterances only makes sense if the reviewer has foreknowledge of new utterances that are likely to be made by the users of the agent in the future. Lacking such foreknowledge it is not likely that adding such a pattern will have an impact for example a pattern that is overly specific would have little to no measurable impact on the past or future behavior of the conversational agent.

To estimate the matching extent of a pattern based on a corpus of past utterances the pattern can be simply matched against the semantic graphs of these utterances. Estimating the matching extent provides agent reviewers with heuristics to predict whether the most optimal patterns are being added to improve the agent s performance efficiently and without adding unneeded patterns.

In one implementation matching extent estimation can be performed for suggested patterns. In an alternate implementation matching extent estimation can be performed on either suggested patterns suggested patterns that have been modified by the reviewer or patterns that have been generated through other means for instance manually .

The matching extent of a pattern p is the subset S of semantic graphs of the utterance corpus so that for every s S 

In order to be able to compute S the utterance corpus contains the matching distance for each matched utterance. Alternatively or additionally the identities of patterns involved in direct matches may be stored together with each semantic graph in the corpus and the matching distance may be computed on the fly. 

The matching extent is characterized quantitatively using the following definition of a matching percentile 

For example the pattern or patterns with the largest matching extent has or have percentile 100 on the other hand the patterns with the smallest matching extent have a matching percentile of 100 s L where s is the smallest extent size for any pattern in the decision tree .

The potential matching percentile of a prospective pattern p can be estimated as described in the section Matching extent of a pattern above. This matching percentile may be shown using appropriate visualization such as the meter in for a designer to assess the impact of adding the prospective pattern p to the decision tree of the conversational agent .

For increased precision when computing the matching percentile of a prospective pattern p the extents of existing patterns may be recomputed to take into account cases in which an utterance potentially matches p instead of the existing pattern p that it was matching with previously.

In addition the potential matching extent of the prospective pattern p may be presented to the reviewer by displaying a small subset of potentially matching utterances for instance on the order of 10 . The subset includes utterances with the shortest matching distance to the new pattern. A selectable control e.g. via a user interface may be used to allow this set to be expanded to show the whole matching extent if desired by the reviewer.

When an utterance that was matched to a pattern q in the log becomes potentially matched to a prospective pattern p this may be referred to as p stealing this utterance from q. In addition p may be said to steal this utterance from the node in the decision tree to which q is attached.

Sometimes this stealing is desirable as in the case of a new more specific pattern stealing user utterances inappropriately matched to a wrong answer. In other cases however a new pattern may steal user utterances that were correctly matched previously. Therefore in an implementation the features described herein may promote human oversight to understand whether a proposed pattern would improve or worsen the agent s overall performance.

At the start of a review process that examines the results of semantic clustering it is desirable to provide the reviewer with a quantitative estimate of the impact that the review process may have on improving the conversational agent. For instance if the review examines the results of clustering user utterances that have failed to be matched to specific intents with the goal of adding new patterns to the decision tree it is desirable to provide an estimate of what the improvement of the conversational agent s matching performance may be after the review process has been completed.

In an implementation the following information may be shown to the reviewer an example of which is illustrated in 

This number or proportion provides a quantitative estimate of the potential improvement of the matching performance of the conversational agent that can be achieved during the current semantic clustering round.

The minimum aggregate frequency k is a parameter of the configuration. The aggregate frequency of a cluster is the total number of occurrences in the corpus of the utterances it contains. In implementations semantic clusters that are less frequent than k are not worked on by a reviewer due to providing minimal leverage although other implementations are also contemplated.

In an implementation example when clustering corpora containing approximately 10 000 unmatched utterances a value of k 5 typically results in approximately 200 clusters to review. In an alternate implementation when a reviewer is concerned primarily with the most salient issues setting k to the low teens may provide an optimal cluster selection.

The contents of a cluster may be epitomized by displaying an utterance corresponding to the semantic graph having the least specificity in the cluster. The reason for this choice is that such an utterance primarily contains information that played a role while building the cluster. Choosing an utterance with a richer semantic graph may run a risk of displaying incidental information that is not representative of the cluster s topic.

In an alternative implementation the cluster center may be defined as a most central utterance where centrality is measured by the average of proximities to each member of a cluster. In case there is more than one most central utterance the most frequent utterance among such most central utterances is selected to represent the cluster s topic.

Clustering criteria includes the quality threshold the threshold criterion radius vs. diameter a building method e.g. transitive vs. intransitive and a proximity metric.

Tuning semantic clustering includes the choosing of clustering criteria such that in most cases utterances matched by different patterns in a specified agent will go to different clusters.

In practice there are a variety of practices for defining graph patterns in agents so that it is necessary to tune clustering criteria to match the specificity level of a particular conversational agent.

In an implementation two level hierarchical clustering includes applying a QT algorithm to a corpus of semantic graphs to obtain clusters and then applying the QT with more stringent clustering criteria on the set of semantic graphs in each cluster obtained in the first step to obtain subclusters.

A variation on the semantic clustering technique described above may be applied to a corpus representing a set of utterances that have been assigned an intent by the agent e.g. a set of matched utterances. In this case semantic clustering attempts to classify by topic utterances that have already been assigned precise topics namely intents in the decision tree by the conversational agent.

Two cases may arise either each member of a cluster is assigned the same intent by the agent convergent assignment or members are assigned multiple intents divergent assignment . The terms convergent cluster and divergent cluster may also be used to describe these cases.

Divergent assignment may mean that a cluster spans several different intents heterogeneous cluster and those intents have been correctly identified by the agent in which case no corrective action is desirable. However provided the quality threshold of the clustering process is tuned to the level of distinction the agent makes between intents a majority of cases of divergence may arise when a heterogeneous cluster contains some utterances with incorrect intent assignment. Therefore each case of divergent assignment may be presented for review.

Errors in intent determination may result in convergent clusters with a single missed assignment. Reviewing single convergent cluster for correctness may amount to reviewing one utterance representing an entirety of the cluster namely the cluster center due to homogeneity and convergence .

However reviewing each convergent cluster could still prove too time consuming in a complex conversational agent. Accordingly in implementations the following review and semantic clustering scheme is implemented on a corpus of matched utterances 

This approach leaves clusters that contain convergent sub clusters out of the review as it is likely that such subclusters have the least probability of containing erroneous intent assignments. Conversely it is likely that if a subcluster has been misassigned to a wrong intent then there is a chance that some other sub cluster of the same cluster may be partially misassigned and thus form a divergent cluster.

When in the course of the review intent misassignments are discovered a variety of corrective actions can be undertaken by the reviewer which include fixing or removing the pattern that mismatched adding a more specific pattern to the correct intent in the decision tree in order to allow this intent to capture the utterances in the subcluster and so on.

One or more implementations use concept set proximity with a quality threshold QT 0.65 and a diameter criterion. On the other hand one implementation of 2 level hierarchical clustering uses a diameter criterion with a quality threshold QT 0.65 at the upper level cluster construction and a diameter criterion with quality threshold QT 0.80 at the lower level subcluster construction .

Semantic clustering based on natural language parsing and the proximity metrics outlined in the Proximity Metrics section including 2 level hierarchical clustering as described in the Two level Hierarchical Clustering section may be used to feed clustering results to a graphical user interface. The graphical user interface may be employed by a reviewer to examine clusters and subclusters if desirable in order to understand what are the most popular topics for a corpus of utterances e.g. user inputs performed via live chat IM SMS email social networking sites blogging and micro blogging services etc. as well as to obtain a quantitative indication of the prevalence of those topics in the corpus. The following lists techniques that may be employed to support this functionality.

A cluster can be represented by its center as defined in the Cluster Center section. Additionally a user selectable i.e. clickable user interface control associated with the cluster center may be used to expand the cluster to display its content.

For relatively large clusters the graphs of its members may be sorted by proximity to the center so as to expand the center by the k closest graphs where typically k 10 the graphical user interface may show the respective content through a specific command. Proximity to the center may be computed using a variety of metrics examples of which were described in the Proximity Metrics section e.g. concept set proximity.

If multiple semantic clustering rounds are performed e.g. on corpora corresponding to different time intervals techniques may be employed to match clusters from the two sets that correspond to the same topic. Such clusters may be referred to as same topic clusters in the following discussion. It should be noted that a simple comparison of cluster member utterances or semantic graphs may not be sufficient in some instances to determine if two clusters relate to the same topic. This is because the actual utterances and semantic graphs making up a cluster may vary from one corpus to the next.

Once same topic clusters are identified between two corpora a determination may then be made as to what clusters are present in the more recent corpus and what clusters are absent from that corpus. This may correspond to the occurrence of new topics or the disappearance of old topics respectively.

While the clustering algorithm relies on the availability of a proximity metric and may not involve the definition of a vector space in which to locate the objects to cluster concept set proximity provides the ingredients for defining a vector space.

To map a semantic graph g to a vector each concept may be considered as a dimension and the coordinates may be set as follows 

The distance between the centroids of two clusters C and C may be computed and a decision made that C and C represent the same topic if this distance is below a specified threshold.

The center of a cluster may be defined as its least specific member an example of which was previously discussed in the Cluster Center section. A decision may be made for example that two clusters C and C represent the same topic if and only the set based proximity of the respective centers is above a specified threshold.

In an implementation the threshold is set between 0.95 and 1.0 although other implementations may also be considered. The center based heuristic may be used in place of the centroid based heuristic insofar as 1 the centroid has increased sensitivity to the presence of nonessential concepts than the center and 2 concept set proximity is perceptibly more accurate than Euclidean distance.

To compare two cluster sets C and C their intersections inter C inter C and inter C C may be computed as follows.

The sets inter C and inter C contain same topic clusters from C and C respectively determined based on the proximity identification criterion as described above . However for the purpose of displaying cluster members they may be used as follows respectively 

If all same topic clusters are to be displayed the set of shared clusters inter C C may be shown. In an alternate implementation if it is desirable to show more recent clusters inter C may be used instead.

Clustering of a relatively large number of utterances may allow reviewers to identify prominent topics based on the size of the resulting semantic clusters. This may have a variety of applications such as to make sense of large volumes of utterances efficiently and automatically. For example an implementation may contain one or more of the following applications of semantic clustering Voice of the Customer analysis display of hottest topics for the benefit of social networking online or micro blogging communities identification of the top issues on support channels or real time user feeds such as from social networks or micro blogging sites and so on. A variety of techniques may be employed to provide this analysis an example of two of which is described as follows.

For example a company may seek knowledge of what is utmost on its customers minds and therefore filter a corpus of utterances consisting of messages or product opinions to retain those utterances that contain references to specific products or topics prior to identifying dominant topics in the retained corpus using semantic clustering.

To track the appearance and disappearance of topics over time in a communication channel a technique may be employed to cluster corpora built during time windows that may be close and or overlapping. For instance an implementation may use corpora of utterances built each day by taking into consideration all utterances that have occurred over the last 5 days.

For example a customer support department may be interested in knowing the top issues encountered by customers in real time so as to efficiently allot customer support operators. If this department offers a high throughput automated chat a one hour sliding window may be used to track the main topics on an hourly basis. Topic appearance and disappearance may be brought to the attention of specialists by using automated topic tracking i.e. without manual topic marking or a predetermined list of topics the specialists may then decide whether these topics correspond to an underlying issue or to an issue resolution respectively.

Automated topic tracking can be achieved by means of the approach described in the section Cluster identification in binary comparison of cluster sets. If C is the set of clusters from a previous time window and C is the set of clusters from the current time window then C inter C may represent outdated clusters while C inter C may represent new clusters.

Automated topic tracking is also applicable across corpora that are not successors in a time series. For example a company may be interested in comparing the dominant topics in user utterances obtained through different channels such as blog contents blog comments micro blogging sites SMS or conversations with humans and conversations with conversational agents during a given time period. On the other hand a company with operations or subsidiaries in several countries may be interested in identifying common topics across these operations from customer support logs. In this case the relevant output of the algorithm described in the section Cluster identification in binary comparison of cluster sets will be inter C C which yields common topics.

An analytics tool offers the possibility of associating other quantitative data beside frequency data with utterances in a cluster. For instance the results of semantic clustering could be joined against the metrics derived by an analytics system such as a web analytics solution or a business intelligence solution .

Furthermore analytics data can be aggregated per cluster. For example it could be of interest to a company to know which topics are most or least conducive to commercial orders in this case checkout completion and or shopping cart value could be the relevant analytics metrics that are joined with the result of semantic clustering or on the contrary cancelations in which case shopping cart abandonment may be the relevant metric .

Some analytics measures for example measures of conversation success correlate with the quality of an agent s content and pattern matching Accordingly aggregating such measures per semantic cluster and displaying the results to a reviewer provides straightforward clues to areas the reviewer needs to act upon in order to improve a conversational agent.

The following lists some supporting techniques that may be used to implement the identifying features.

DiameterProximity s C average proximity of s to each member of C where proximity may be one of the proximity metrics that have been defined previously.

Given a set of clusters S an utterance s is said to be attached to cluster CS if and only if C is the cluster that maximizes DiameterProximity s C for any C S. If several clusters satisfy this definition the cluster C among these several clusters that maximizes TopicFrequency s C may be defined to be the attachment cluster.

Another technique for computing cluster attachment involves the same definitions as in the preceding sections except that RadiusProximity defined as follows is substituted for DiameterProximity 

where the center of C is defined as in the Cluster Center section i.e. as the cluster member with least specificity.

The diameter method may be used for analytical applications as in assessing the relative importance of issues reported in the conversational agent whereas the radius method may be used when cluster attachment is used in the conversational agent to identify the intent of a user utterance.

In an implementation a hybrid approach is used by the conversational agent . In this approach diameter proximity is used except for clusters whose size exceeds a certain threshold e.g. 500.

The above techniques may be leveraged in a variety of different applications. For example the importance of an individual case of bad quality response may be accessed. The ability to sort utterances by attachment frequency makes it possible to report an individual issue not as a request to remedy that particular issue but as information on what topics may be improved or to decide that improvement is not desirable should it be found that this particular utterance does not correspond to a frequent topic. This approach allows a prioritized approach to agent improvement.

In another example corpora may be displayed by topic. While nontrivial clusters i.e. clusters of more than 1 utterance may account for a varying proportion of a given corpus cluster attachment may make it possible to consider each single utterance in a corpus as a member of an enlarged cluster. This makes it possible to use summarization techniques that use the center of a cluster or the k most central utterances of an enlarged cluster to represent potentially hundreds or thousands of sentences thus enabling efficient browsing of a corpus. Such efficient browsing may be further facilitated by sorting enlarged clusters by the sum of the attachment frequencies of their members thus giving most prominence to the most popular topics.

In a further example online exploitation of cluster attachment may be performed. Determining the attachment cluster of a single sentence may be performed with sufficient speed to be used as part of a conversational agent s matching process. In cases where a conversational agent cannot determine a specific intent for a user input identifying its topic through cluster attachment can enable the agent to 

The choice between options 1 and 2 is a matter of level of confidence which is determined by the attachment proximity of the utterance. In an implementation a proximity value of 0.33 is used as a cut off point between low confidence and high confidence.

Although the invention has been described in language specific to structural features and or methodological acts it is to be understood that the invention defined in the appended claims is not necessarily limited to the specific features or acts described. Rather the specific features and acts are disclosed as example forms of implementing the claimed invention.

