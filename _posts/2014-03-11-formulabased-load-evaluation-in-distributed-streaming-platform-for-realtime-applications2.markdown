---

title: Formula-based load evaluation in distributed streaming platform for real-time applications
abstract: Software for a distributed streaming platform receives an application that runs on a streaming platform. The application is structured as a directed acyclic graph (DAG) with instances of operators as nodes and streams as edges between nodes. The application is associated with a pre-defined hint that is a key-value pair. The software launches the application by assigning the instances of operators to containers provided by the streaming platform and initiating the streams. Then the software reads a value for the pre-defined hint and transmits the value to the application through an application programming interface (API) exposed by the streaming platform. The software receives a request from the application through the API to make a dynamic adjustment. And the software makes the dynamic adjustment and re-launches the application using a recovery policy.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09563486&OS=09563486&RS=09563486
owner: DataTorrent, Inc.
number: 09563486
owner_city: Santa Clara
owner_country: US
publication_date: 20140311
---
This application claims priority to U.S. Provisional Patent Application Ser. No. 61 776 545 entitled Real Time Streaming Platform for Hadoop filed on Mar. 11 2013 U.S. Provisional Patent Application Ser. No. 61 838 870 entitled A Distributed Streaming Platform for Real Time Applications filed on Jun. 24 2013 and U.S. Provisional Patent Application Ser. No. 61 957 267 entitled Distributed Streaming Platform for Real Time Applications filed on Jun. 25 2013. This application is related to U.S. patent application Ser. No. 13 927 108 entitled Distributed Streaming Platform for Real Time Applications filed on Jun. 25 2013. This application is also related to U.S. patent application Ser. No. 13 928 357 entitled Dynamic Partitioning of Instances in Distributed Streaming Platform for Real Time Applications and U.S. patent application Ser. No. 13 928 351 entitled Checkpointing in Distributed Streaming Platform for Real Time Applications and U.S. patent application Ser. No. 13 928 363 entitled Dynamic Adjustments in Distributed Streaming Platform for Real Time Applications all of which were filed on Jun. 26 2013.

Additionally this application is related to U.S. patent application Ser. No. 14 203 551 entitled Thread Local Streams in Distributed Streaming Platform for Real Time Applications which was filed on Mar. 10 2014. And this application is related to U.S. application Ser. No. 14 205 320 entitled Scalable Local Cache in Distributed Streaming Platform for Real Time Applications and U.S. application Ser. No. 14 205 357 entitled Partitionable Unifiers in Distributed Streaming Platform for Real Time Applications both of which were contemporaneously filed.

The disclosures of all of the applications identified in the above paragraphs are incorporated herein by reference.

Streaming applications operate on input data which is not retrieved from persistent storage but which arrives as one or more continuous sequence of items. Such input data might be streaming media such as streaming audio or streaming video. Or such input data might be other than streaming audio or streaming video e.g. real time streaming text. Examples of the latter type of input data include real time electronic stock tickers published by financial websites such as Yahoo Finance CNBC Bloomberg or NASDAQ and real time content streams published by websites such as Twitter and Facebook which leverage interest and or social graphs.

As the sources of streaming data proliferate scalability has become an issue for streaming applications that process such data and the platforms which run the streaming applications. Outside of the area of streaming applications scalability has been addressed by distributed batch processing platforms based on the Map Reduce or similar frameworks. However these platforms typically operate on input data originating in persistent storage e.g. the persistent storage of the commodity servers that make up a Hadoop cluster. That is to say in terms of a stock and flow model these platforms operate on a stock rather than a flow or stream .

Performance is also an issue for streaming applications and their platforms since it is often desirable that a streaming application operate in real time or near real time. In the past streaming applications achieved real time performance by sacrificing data integrity or data completeness. For distributed batch processing platforms based on Map Reduce and similar frameworks real time performance is often limited to accessing e.g. using Pig Scalding Dremel Drill etc. a store of indexed results that were generated offline.

Complicating matters still further streaming applications tend to be non stop almost by definition. And consequently fault tolerance is an important issue for streaming applications and the platforms on which they run.

In an example embodiment a method is described. The method is executed by one or more processors in real time or near real time rather than offline. According to the method software for a distributed streaming platform receives an application that runs on a streaming platform. The application is structured as a directed acyclic graph DAG with instances of operators as nodes and streams as edges between nodes. The application is associated with a pre defined hint that is a key value pair. The software launches the application by assigning the instances of operators to containers provided by the streaming platform and initiating the streams. Then the software reads a value for the pre defined hint and transmits the value to the application through an application programming interface API exposed by the streaming platform. The software receives a request from the application through the API to make a dynamic adjustment. And the software makes the dynamic adjustment and re launches the application using a recovery policy.

In another example embodiment an apparatus is described namely computer readable storage media which persistently store a program. The program might be software for a distributed streaming platform. The program is executed by one or more processors in real time or near real time rather than offline. The program receives an application that runs on a streaming platform. The application is structured as a directed acyclic graph DAG with instances of operators as nodes and streams as edges between nodes. The application is associated with a pre defined hint that is a key value pair. The program launches the application by assigning the instances of operators to containers provided by the streaming platform and initiating the streams. Then the program reads a value for the pre defined hint and transmits the value to the application through an API exposed by the streaming platform. The program receives a request from the application through the API to make a dynamic adjustment. And the program makes the dynamic adjustment and re launches the application using a recovery policy.

Another example embodiment involves a method. The method is executed by one or more processors in real time or near real time rather than offline. According to the method software for a distributed streaming platform receiving an application that runs on a streaming platform. The application is structured as a directed acyclic graph DAG with instances of operators as nodes and streams as edges between nodes. The application is associated with a pre defined hint. The software launches the application by assigning the instances of operators to containers provided by the streaming platform and initiating the streams. The software reads a value for the pre defined hint and transmits the value to the application through an API exposed by the streaming platform. The software receives a request from the application through the API to make a dynamic adjustment. And the software makes the dynamic adjustment and re launches the application wherein the dynamic adjustment includes a dynamic partition of an instance of an operator into a plurality of instances.

Other aspects and advantages of the inventions will become apparent from the following detailed description taken in conjunction with the accompanying drawings which illustrates by way of example the principles of the inventions.

In the following description numerous specific details are set forth in order to provide a thorough understanding of the example embodiments. However it will be apparent to one skilled in the art that the example embodiments may be practiced without some of these specific details. In other instances process operations and implementation details have not been described in detail if already well known.

In an example embodiment the website might be composed of a number of servers connected by a network e.g. a local area network LAN or a WAN to each other in a cluster or other distributed system which might run website software e.g. web server software database software etc. and distributed computing software. In an example embodiment the website might also be composed of a number of servers connected by a network to each other in a cluster or other distributed system which might run website software e.g. web server software database software etc. and distributed computing and or cloud software such as Hadoop Hadoop Distributed File System HDFS Pig CloudBase etc. The servers at website are also connected e.g. by a storage area network SAN to persistent storage . Persistent storage might include a redundant array of independent disks RAID and or flash memory. Persistent storage might be used to store data related to the data streamed by website e.g. financial data content data for social interest networks etc.

In an alternative example embodiment the servers in website and website and the persistent storage might be hosted wholly or partially off site in the cloud e.g. as a platform as a service PaaS or an infrastructure as a service IaaS .

In turn the distributed computing framework might be supported by distributed storage which might be Hadoop Distributed File System HDFS in an example embodiment. And the distributed computing framework and distributed storage might run on a networked cluster of servers e.g. commodity servers or other hardware computation units e.g. the hardware computation units emanating from Facebook s Open Compute Project .

In an example embodiment logic and logic might include model view controller MVC logic for displaying the results of some or all of the operators in a dashboard that is part of a graphical user interface GUI . In an example embodiment if the origin of the streaming data is a stock ticker the dashboard might display statistics related to stock prices and stock sales. Or if the origin of the streaming data is a content stream broadcast by a social interest network the dashboard might display statistics related to social signals e.g. likes favorites shares etc. related to posts to the content stream.

As depicted in the software e.g. the STRAM receives a specification e.g. whose location is identified in a user or script command on a command line interface CLI for an application that is streaming in operation . In an example embodiment the specification might be a source program in Java created in an integrated development environment IDE such as Eclipse or NetBeans. In another example embodiment the specification might be a Hadoop style properties file. Or the application might be specified at the CLI e.g. through user input such as macros as described below. In operation the software converts the specification into a logical plan that includes a directed acyclic graph DAG or other precedence graph with operators as nodes and streams as edges. One might think of the logical plan as special form of a data object model DOM . The operators are program instructions and the streams are unbound sequences of streaming windows that are ordered in terms of time. In an example embodiment the sequence might originate from web services interface e.g. a web API exposed by Yahoo Finance or Twitter accessed by an input adapter for the distributed streaming platform. It will be appreciated that a Hadoop Map Reduce application can be represented as a DAG though such applications tend to operate on batches of data rather than streams of data.

In operation the software translates the logical plan e.g. the DAG into a physical plan using any stream modes specified in the specification e.g. in line in node in rack or other and with one or more of instances of the operators per the static partitioning e.g. as expressed in partition counts in the specification. In operation the software obtains a number of containers or processes running on a networked cluster of servers or other physical computational units . In an example embodiment the software might obtain these containers from the YARN Resource Manager. One might regard a container as a permission from YARN to run a process on a particular server or other physical computation unit in the networked cluster. And one might regard the YARN Resource Manager as a distributed operating system OS that manages processes memory and persistent storage for the networked cluster. One slave e.g. a STRAM Child might execute in each container in an example embodiment. Then in operation the software assigns the instances of the operators to the slaves for execution according to an execution plan that depends on the physical plan and the number of containers obtained. In operation the software launches the execution plan using input adapters which convert external input into tuples grouped into streaming windows. And each slave monitors the throughput of streaming windows through the instances in its container e.g. by monitoring the ordinal streaming window identifiers along other statistics related to service level and or performance in general e.g. latency between servers e.g. as reported by the container network usage e.g. as reported by the container CPU usage e.g. as reported by the container memory usage e.g. as reported by the container uptime errors including data loss e.g. as reported by error tuples ports streams size of message queues managed by buffer servers throughput other than operator throughput such as stream throughput or message queue throughput operator skew time delay with respect to external system components etc. and reports the results e.g. in conjunction with a heartbeat protocol in an example embodiment. In an example embodiment the software might also monitor service level and or performance in general using statistics e.g. latency between servers network usage CPU usage memory usage uptime etc. received from or reported by the Resource Manager or other components of the distributed streaming platform.

In operation the software makes one or more dynamic adjustments based on the results of the monitoring e.g. to reduce traffic of streaming windows through message queues in containers through server NICs and other bottlenecks or more generally to improve performance as measured by the reported statistics . In an example embodiment the dynamic adjustments might include updating the physical plan by adding new instances of operators or deleting existing instances of operators. Or the dynamic adjustments might update the execution plan by returning a container to the YARN Resource Manager and or obtaining a new container from the YARN Resource Manager for a rack and or server or other physical computational unit and or moving instances of operators from one container or server to another. As described in further detail below the making of dynamic adjustments e.g. failover and dynamic partitioning includes re initiating the streams in accordance with a recovery policy e.g. at least once at most once exactly once through commands e.g. from the STRAM to the slave e.g. STRAM Child which in turn controls the operators in the container and the buffer server in the container. Also in an example embodiment the dynamic adjustments might originate from commands entered by a user or the application at a CLI that supports runtime modifications to the logical plan e.g. a macro code an interface or API a GUI text input etc. the physical plan and or the execution plan.

In an example embodiment the distributed streaming platform might support a macro that is a set of CLI instructions that insert a sub DAG which might be reusable and therefore a candidate for a library consisting of multiple operators and streams into an application at runtime. The distributed streaming platform might also support hierarchical operators that are reusable sub DAGs which are inserted into logical plans prior to runtime.

Then in operation the software outputs structured tuples e.g. using output adapters from some or all of the instances to a display e.g. a graphical user display or GUI dashboard for decision support to persistent storage e.g. using HDFS for subsequent use by another application to another system etc. As noted on each of the operations in this process might be executed in real time or near real time rather than offline in an example embodiment. Moreover some of the operations described in the process e.g. the monitoring making dynamic adjustments and output operations might be continuous or non stop operations in an example embodiment.

In an example embodiment each container might be a multi threaded process that includes one thread for each operator instance one thread for the container s buffer server and one thread for the slave e.g. STRAM Child . In an example embodiment each container has a single buffer server which manages for the operator instances in the container a message queue FIFO not priority of streaming windows on a per port basis e.g. the buffer server keeps track of which port has read which tuple . Each of these threads might perform its computations in memory spilling over to persistent storage such as HDFS only in the event that memory is insufficient. It will be appreciated that by using a single thread for each operator each operator can execute asynchronously without creating memory storage incoherency.

In an example embodiment each tuple might be a Plain Old Java Object POJO structured according to a schema or data type. In an example embodiment each stream might have one upstream operator and one downstream operator. In that event the schema for a tuple also defines a schema for a stream e.g. by defining a schema for the output port of the stream s upstream operator that is the same as the schema for the input port of the stream s downstream operator. In an example embodiment each operator might have one output port but one or more input ports which are mapped to the operator s one output port by the logic in the operator. For example the input to an instance of an operator that computes an average price might be a streaming window that consists of a begin window e.g. a control tuple with a field for an identifier say 70 followed by data tuple with a field for a price say 221.00 followed by an end window e.g. a control tuple with a field for an identifier also . The instance might re compute an existing average using the new price to obtain a new average of say 230.00 and then output or emit to an output port a begin window e.g. with an identifier a data tuple with a field for an average price set to 230.00 and an end window e.g. with an identifier . It will be appreciated that the tuple input to the instance might already have a field for an average price which is set to 230.00 by the instance. Or alternatively the instance might dynamically allocate the tuple with the field for an average price and set it to 230.00 the tuple might then be de allocated by a downstream operator e.g. a downstream operator that is an output adapter that displays the average price of 230.00 in a GUI dashboard. In an example embodiment an instance of an operator might be used to change the schema of a tuple without making changes to any values in the schema s fields. In operation the DAG includes operators that are program instructions. In an example embodiment these program instructions might relate to the business logic for the application e.g. computing a financial statistic e.g. such as the high or low price for a stock within a period of time for display in a GUI dashboard for an application fed by a stock ticker e.g. through a web API . Or the program instructions might be more generic along the lines of the C runtime library or the C template library. In that regard a library of reusable common or standard operator templates e.g. for use by developers of applications for the distributed streaming platform might include operator templates with functionality for 1 matching tuples and emitting results where the output might be tuples that matched tuples that did not match a Boolean flag etc. 2 recording tuples 3 counting items such as keys frequency of keys unique counts etc. 4 filtering such things as streams with input schema using keys or rates e.g. sampling rates 5 filtering log file lines from Apache and Tomcat servers 6 joining and sorting items 7 indexing or mapping including hash mapping for such operations as search indexing word counting etc. 8 consolidating schemas e.g. to consolidate multiple streams into one schema 9 inputting data into the application e.g. an input adapter and outputting data from the application e.g. an output adapter including adapters using Hadoop Distributed File System HDFS MemCache MySQL MongoDB console HTTP Apache ActiveMQ RabbitMQ ZeroMQ Kafka Kestrel Redisetc. Websocket LocalFile etc. 9 performing mathematical operations such as compare max min average sum quotient range except margin change etc. 10 managing streams without changing tuples or schema e.g. stream duplicator stream merger array list splitter hash map splitter dev null counter etc. 11 generate load for testing e.g. event generator random generator filter event generator etc. 12 computing over application windows that are sliding 13 generating data for charts e.g. in conjunction with CLI macros that are inserted dynamically at runtime through the CLI 14 allow the usage of languages such as Python JavaScript Bash etc. 15 issue alerts using SMTP Simple Mail Transfer Protocol and 16 utility functions that are building blocks for other operator templates including those listed above. In operation the DAG includes edges that are streams made up of streaming windows. In an example embodiment each streaming window is an atomic microbatch of sequential tuples that is associated with a recovery policy for an application. In an example embodiment the length of the sequence of tuples in a streaming window is variable in length e.g. configurable by the user directly or indirectly a streaming window begins with a special begin window tuple e.g. a control tuple and ends with a special end window tuple e.g. a control tuple . In an example embodiment a streaming window might be specified in terms of time as approximately 0.5 seconds. An alternative example embodiment might use only a single control tuple e.g. begin window or some other form of timestamp ordering for concurrency control e.g. isolation within the meaning of the ACID or Atomicity Consistency Isolation Durability properties for reliable data processing .

Other control tuples might include checkpoint tuples that are inserted into the streaming data periodically per the checkpointing interval or frequency specified by the user or application e.g. directly or indirectly through the recovery policy . In an example embodiment checkpoint tuples might be inserted by an input adapter triggering checkpoints as they work their way through all of the application s operators and be removed by an output adapter. It will be appreciated that checkpoint tuples can be used to achieve checkpointing at the end of streaming windows e.g. align checkpoints with boundaries of streaming windows .

In an example embodiment an instance of an operator might report errors e.g. counted per streaming window using an error tuple that is emitted through an error port e.g. an output port for an error stream to a log file e.g. in HDFS . Also in an example embodiment an input adapter might use a sample operator to perform bucket testing on new application logic on a relatively small subset of a stream before deployment to the application.

In an example embodiment an application window might be specified as in terms of streaming windows or using a period of time. In an example embodiment an application window specified in terms of time might range from 5 to 15 minutes. Also in an example embodiment the default application window might be a single streaming window. An application window is associated with an operator thus an application might have multiple application windows. Also in an example embodiment an application might be either an aggregate application window or a sliding application window.

An aggregate application window is constructed by combining a number of consecutive streaming windows without overlapping. That is to say the next application window begins only after the current application window ends in an example embodiment. Aggregate application windows are used for stateless application operators e.g. application operators that operate solely on data in the streaming windows without resort to data read into memory from persistent storage. It does not follow that the operator instances in the physical plan are stateless they are stateful in an example embodiment. In an example embodiment the distributed streaming platform might enhance performance of an aggregate application window by using one begin window tuple e.g. aligned with the window boundary of the aggregate application window and one end window tuple e.g. also aligned with the window boundary of the aggregate application window for all of the streaming windows in the aggregate application window that is to say the intervening control tuples e.g. begin window tuples and end window tuples might not be processed by the operator associated with the aggregate application window though they might be used for monitoring purposes e.g. by the STRAM child and or buffer server . Also in an example embodiment the distributed streaming platform e.g. STRAM might use the last streaming window in an aggregate application window when making a purge determination as described in further detail below. An example of an operator that might be used with an aggregate application window in a financial application is an operator that charts stock ticker data on a per minute basis.

A sliding application window is constructed by combining a number of consecutive streaming windows with overlapping. That is to say the current sliding application window is formed by dropping a streaming window from the previous sliding application window and adding a new streaming window in an example embodiment e.g. sliding by one streaming window . Sliding application windows are used for stateful application operators and the operator instances in the physical plan are also stateful in an example embodiment. An example of an operator that might be used with a sliding application window in a financial application is an operator that counts the top 10 trades in terms of volume on a stock ticker over the past minute e.g. starting from now .

Again the use cases are many and financial applications are mentioned because they are a type of process that benefits from real time or near real time processing. So therefore the types of applications that can benefit from the processing described herein can be large and without limitation and for purposes of example only such applications can be for processing technical computing data computing statistics data processing statistics advertising statistics gaming statistics hospital resource management traffic statistics application load managing distributed processing load balancing of servers and processes inventory statistics data distribution statistics and other types technology driven processes.

Also in an example embodiment the recovery policy might be configurable by the user of the application or the application itself. Such configuration might occur prior to launch or during runtime e.g. through the CLI. The recovery policy might be one of at least once at most once or exactly once as described in further detail below. The recovery policy might impact performance of the distributed streaming platform because it can affect of the frequency of instance checkpointing e.g. when the recovery policy is exactly once instance checkpointing will occur at the end of every streaming window. In an example embodiment instance checkpointing involves a pausing an instance of an operator at the end of a streaming window b serializing the instance to persistent storage e.g. using functionality such as Kryo to serialize the instance to a file in a file system such as Hadoop Distributed File System HDFS and c notifying the STRAM of the last window completed. Also in an example embodiment instance checkpointing might occur at a specified time period e.g. every 30 seconds which might be configurable by the user of the application e.g. when the recovery policy is other than exactly once.

It will be appreciated that the statefulness of the instance might determine the amount of data to be serialized in an example embodiment. For example if an operator is stateless e.g. it operates solely on the tuples in a streaming window without resort to data of its own read into memory from persistent storage serialization of the operator might be skipped.

In an example embodiment the recovery policy might be specified on a per operator basis or a per instance basis. That is there might be different recovery policies for different operators or for different instances of the same operator. So for example a stateful instance might have a recovery policy of at least once or exactly once whereas a stateless instance might have a recovery policy of at most once.

Traditionally the state of a streaming application is defined as the state of all operators and the state of all streams. In an example embodiment the state of the streaming application might be defined as the state of all operator instances e.g. one or more serializations of the instance obtained through checkpointing and the set of all streaming windows in the message queues maintained by the buffer servers. It will be appreciated that in such an embodiment the state of an operator instance is associated with an identifier for a streaming window. In an example embodiment the checkpointing might be asynchronous insofar as the latest serialization or checkpoint for one instance of an operator might be at the end of a streaming window whose identifier differs from that of the latest serialization or checkpoint for another instance. Also in an example embodiment if multiple serializations are stored for an operator instance STRAM might purge earlier serializations on a FIFO basis consistent with the recovery policy.

In an example embodiment the STRAM might dump the current state of all operator instances including additions deletions movements to other containers etc. to a change file e.g. in HDFS . It will be appreciated that the distributed streaming platform might then use this change file to create an updated logical plan which might be used to re launch the application e.g. in the event of a grid outage in the networked cluster. Such a change file might be updated a at a recurring time period that is configurable by the user or the application or b as a result of a command at the CLI e.g. by the user or an application.

In operation the slaves e.g. STRAM Childs might report the results of their monitoring e.g. to the STRAM in conjunction with a heartbeat protocol. Also in an example embodiment the heartbeat interval or period might be configurable by the user of the application e.g. either in terms of units of time or number of streaming windows. In an example embodiment the heartbeat protocol might use YARN RPC remote procedure call . It will be appreciated that a heartbeat interval that is too short might add considerable network traffic and resultant computation to the distributed streaming platform.

In any event the reporting of the results of the monitoring might be aligned with a streaming window boundary e.g. through an end window. That is to say the reporting of the results might take place during the period of time between an end window and the next begin window in an example embodiment. This period of time might also be used for restarting operators e.g. during server outages checkpointing checksumming and other statistics generation etc. In an example embodiment class method calls might be associated with begin window tuples and end window tuples. And in such an example embodiment the class method call for end window might perform some or all of the reporting of the results of the monitoring restarting operators checkpointing checksumming and other statistics generation.

It will be appreciated that each streaming window is identified by an ordinal identifier that increases as the application runs e.g. 1 2 3 etc. . In an example embodiment the results of the monitoring in operation might include a the identifier for the last processed streaming window per operator in the container b the identifier for the last checkpoint streaming window per operator in the container and c the identifier for the committed streaming window. The committed streaming window is the streaming window that has been computed by all output adapters e.g. operators which write to console or persistent storage . In an example embodiment the STRAM uses b and c to determine which streaming windows can be purged from the buffer server s message queue in each container and which checkpoint serializations or files can be purged from persistent storage e.g. HDFS . In an example embodiment the user of the application might configure the period at which this purge determination is performed by the STRAM e.g. every 30 seconds.

In another example embodiment the results of the monitoring might also include some statistics related to streams e.g. tuples consumed at each input port of an operator per second tuples emitted to or by each output port of an operator per second etc. Also each buffer server might also report monitoring results related to the streaming windows in its message queue the identifier of the last streaming window in the message queue confirmation of the purge of a streaming window in the message queue etc.

In operation the software re initiates the streams in accordance with a recovery policy when making dynamic adjustments. In an example embodiment the recovery policy might be one of at least once at most once exactly once by analogy to the delivery assurances in the WS Reliable Messaging Protocol. These recovery policies are described in greater detail in in a context where a dynamic adjustment results from failure of a container or its server e.g. failover .

In an example embodiment the distributed streaming platform e.g. the STRAM might ignore all or part of the static partition treating it as a hint rather than a command. In that event the software might issue a diagnostic message e.g. through the CLI or to a log file to the user who submitted the application.

As depicted in the software e.g. the STRAM determines that a container or its server has failed e.g. based on notification from YARN Resource Manager a time out on heartbeat to slave an exception repeatedly thrown by an instance in the container etc. in operation . In operation the software obtains a new container e.g. from the YARN Resource Manager and assigns instances of operators to the container per the original execution plan or per an updated execution plan based on the monitoring results reported by slaves e.g. STRAM Childs . Then in operation the software restarts the application according to the following recovery policies A if the recovery policy is at most once e.g. data loss is acceptable then the software instructs e.g. through the STRAM Child each instance in the new container to subscribe to the next streaming window in the upstream buffer server s message queue or alternatively instructs the upstream buffer server through a STRAM Child to transmit that streaming window to each of those instances B if the recovery policy is at least once e.g. data loss is not acceptable but extra computations are then 1 the software determines the latest viable checkpoint for each instance in the new container using streaming window identifiers of checkpoints for that instance and for downstream instances e.g. the streaming window identifier of the latest viable checkpoint is less or older than the streaming window identifiers of the checkpoints for the downstream instances 2 restarts e.g. through the STRAM Child each instance in the new container using the latest viable checkpoint and restarts e.g. through the STRAM Childs the downstream instances using each of their latest checkpoints e.g. with each of the downstream instances being instructed to subscribe to the streaming window in each of their upstream buffer server s message queues with a streaming window identifier greater or newer than each of their latest checkpoints and 3 instructs e.g. through the STRAM Child each instance in the new container to subscribe to the streaming window in the upstream buffer server s message queue with a streaming window identifier that is greater or newer than the streaming window identifier of the latest viable checkpoint or C if the recovery policy is exactly once e.g. data loss is not acceptable and neither are extra computations then the software restarts e.g. through the STRAM Child each instance in the new container using the last checkpoint its streaming window identifier will be less or older by one and instructs e.g. through the STRAM Child each of those instances to subscribe to the streaming window in the upstream buffer server s message queue that was lost not checkpointed .

It will be appreciated that the recovery policy of at most once can be processed faster than a recovery policy of at least once and exactly once at the cost of data loss. And while a recovery policy of exactly once might be processed faster than a recovery policy of at least once the former recovery policy might significantly impact performance of the distributed streaming platform since it requires the checkpointing of an instance at the end of every streaming window.

It will also be appreciated that operations and might also be used during other dynamic adjustments as described above e.g. adding new containers to an updated execution plan based on monitoring results from the slaves or a command e.g. a macro entered by a user or script at the CLI while the application is continuously executing.

When the stream mode is in node shown in as the arrow with medium thickness the streaming windows going between the output port of an upstream operator on the stream and the input port of the downstream operator on the stream pass through the message queue managed by the container s buffer server. Passing through the message queue might entail serialization of tuples into bytes at the output port of the upstream operator on the stream and de serialization of bytes into tuples at the input port of the downstream operator on the stream according to a stream codec e.g. for stream sockets or other network sockets such as Kryo. Consequently the throughput of streaming windows when the stream mode is in node will be lower than the throughput of streaming windows when the stream mode is in line.

When the stream mode is in rack shown in as the thin arrow the streaming windows going between the output port for an upstream operator on the stream and the input port for the downstream operator on the stream pass through both the message queue managed by the container s buffer server and a network interface controller NIC or other hardware component that connects one server or other physical computation unit with another. Consequently the throughput of streaming windows when the stream mode is in rack will be significantly lower than the throughput of streaming windows when the stream mode is in line or in node. And when the steam mode is other not shown in the throughput of streaming windows might be significantly lower than the throughput of streaming windows when the stream mode is in line in node and in rack in an example embodiment.

In an example embodiment the software might ignore some or all of the stream modes treating them as a hints rather than commands. For example a user or code might submit a specification in which all streams are specified as in line in order to obtain fast throughput of streaming windows though such an approach would result in a process that exceeded the capacity of a container. In that event the software might issue a diagnostic message e.g. through the CLI or to a log file to the user who submitted the application.

As depicted in the software e.g. the STRAM determines that an instance of an operator in a container is creating a bottleneck based on monitoring results received from container s slave in operation . For example the instance might be an upstream instance to a downstream instance with two input ports. And the input port connected to the upstream instance might have significantly lower throughput than the other input port. In operation the software pauses the instance of the operator e.g. after its last checkpointing e.g. serialization to HDFS . The software then assigns multiple instances of the operator to the container and connects the instances to the upstream operators e.g. using a stream duplicator and to the downstream operator e.g. using a stream merger or a unifier in operation . In operation the software starts the instances of the operator e.g. through the slave using the last checkpoint and a recovery policy e.g. at most once at least once or exactly once as explained in detail above with respect to operation in .

In an example embodiment a stream codec might be used to split the tuples in a streaming window between multiple instances of the same operator that result from static partitioning in the specification or dynamic partitioning at run time. For example a hash function might be applied to a tuple to obtain a hash code and the lower bits of the hash code might determine which instance of the operator receives the tuple e.g. if there are two instances the lower bit of the hash code would be sufficient to split the tuples between the instances . It will be appreciated that such an approach which might be referred to as sticky key differs from a round robin approach where the first tuple would go to the first instance the second tuple would go to the second instance the third tuple would go to the first instance the fourth tuple would go to the second instance etc.

In such an example embodiment the sticky key approach might result in a skewed distribution where one instance of the same operator receives many more tuples than the other instances e.g. 4 tuples received by the one instance to every 1 tuple received by each of the other instances. In that event the STRAM might lessen the skew by applying at runtime a ratio e.g. 2 1 of maximum load to minimum load as configured by the user of the application or the application itself e.g. through the CLI . In that event the one instance receiving more tuples would receive at most 2 tuples for every 1 instance received by each of the other instances.

In an example embodiment security for the distributed streaming platform might be provided by Kerberos where the access points are the STRAM and each of the buffer servers. In that embodiment the STRAM might obtain a security token and pass it to the STRAM Child e.g. a thread which in turn passes it to the buffer server e.g. also a thread that it monitors and controls in their shared container or process . The buffer server could then use the security token to verify the security of any new connection to the container. Also in an example embodiment security for the distributed streaming platform s graphical user interfaces GUIs might be provided by Simple and Protected GSSAPI Negotiation Mechanism SPNEGO .

In an example embodiment a reservoir buffer e.g. a thread associated with an instance of an operator might be used to synchronize streaming windows for operator instances with multiple input ports. In an example embodiment the reservoir buffer might monitor the input ports to determine when a begin window tuple e.g. a control tuple with a new window identifier has been received by one of the input ports. The reservoir buffer might then emit a begin window tuple with that window identifier on the output port for the instance e.g. using the container s message queue or another FIFO queue in an example embodiment. But the reservoir buffer might emit an end window tuple with that window identifier on the output port for the instance e.g. using the container s message queue or another FIFO queue only after the reservoir buffer determines that an end window tuple with that identifier has been received by all of the input ports for the instance. It will be appreciated that in such an example embodiment the instance s propagation of a begin window tuple and the processing of the data tuples that follow the begin window tuple is non blocking with respect to the instance whereas the instance s propagation of an end window tuple is blocking with respect to the instance except in an example embodiment when performing the operations in a recovery policy as described below . Further the completion of a streaming window in an instance of an operator e.g. propagation of an end window through an instance in an example embodiment only occurs after all upstream instances have finished processing the streaming window. That is the window identifier of the streaming window in an upstream instance is greater than or equal to the window identifier of the streaming window in the instance and the window identifier of the streaming window in a downstream instance is less than or equal to the window identifier of the streaming window in the instance. Also in an example embodiment the reservoir buffer might merge data tuples received on multiple input ports from different instances of the same upstream operator into a single queue e.g. using the container s message queue or another FIFO queue through a first come first served approach to aggregation. This is illustrated in the following figure.

Operator 1 Stock Tick Input also transmits the price stream to operator 5 High Low which computes a stream of high low price data and transmits it to operator 7 Chart . Operator 1 Stock Tick Input also transmits the volume stream to operator 6 Minute Vol which computes a stream of volume per minute data and transmits it to operator 7 Chart . Operator 7 Chart computes a stream of chart data and transmits it to operator 8 Console e.g. for display in a GUI. Operator 1 Stock Tick Input also transmits the price stream to operator 9 SMA or simple moving average which computes a stream of sma price data and transmits it to operator 10 Console e.g. for display in a GUI.

In an example embodiment operator instances in physical plans and execution plans might be identified as integers rather than strings chars etc. for purposes of performance efficiency.

GUI view is a tuple viewer associated with a recording whose name is container 1370542662205 0007 01 000007 3 1370896978891 . GUI view also includes a window identifier whose value is 8816 that shows the streaming window that is the source of the tuples shown in the GUI view and processed by the selected instance e.g. operator 3 as shown at the top left of the view . The tuples themselves are shown as a scrollable stream with the earliest tuple at the top of the stream and the latest tuple at the bottom of the stream. Tuple is an input tuple data in terms of table whose identifying number is 5804 and whose value is a URL namely http twcm.me MLwbd . Tuple is an output tuple count in terms of table whose identifying number is 5808 and whose value includes http twcm.me MLwdb 100 which is a count of the number of times that the URL http twcm.me MLwbd has been seen. shows GUI view after a user has clicked control to stop the recording of data related to tuples processed by the selected instance.

As depicted in the software e.g. the STRAM launches a real time streaming application that runs on a distributed streaming platform in operation . In an example embodiment the real time streaming application might be structured as a directed acyclic graph DAG with instances of operators as nodes and streams as edges between nodes as described earlier. In operation the software receives an indication that a stream is I O bound where a stream connects one operator instance to another operator instance in a single container provided by the distributed streaming platform. In an example embodiment the indication is a platform measurement as to resource use e.g. throughput of streaming windows or tuples for the stream. Or the indication might be a platform measurement as to a pre defined hint as described in further detail below. Then in operation the software transmits the platform measurement to the real time streaming application e.g. through an API exposed by the distributed streaming platform and receives a request e.g. a request setting an attribute of a stream from the real time streaming application to combine the operator instances connected by the stream into a single operator instance. And in operation the software creates a single operator instance e.g. through compiler in lining and re initiates the stream e.g. using a recovery policy such as at least once at most once exactly once etc. .

In an example embodiment each of the operations in this process might be executed in real time or near real time rather than offline. Moreover some of the operations described in the process e.g. the operations following the original launch of the real time streaming application might be continuous or non stop operations in an example embodiment.

In operation the software receives an indication that a stream is I O bound. Recall that in an example embodiment an operator instance might be a thread and a server buffer managing a message queue for stream might be a thread. It will be appreciated that the term I O bound is a term of art in the field of computer science and refers to a condition where the time period to complete a computation e.g. the operations performed by a thread is primarily determined by time spent waiting for read write operations to be completed e.g. reading writing to storage e.g. to volatile storage such as main memory or to persistent storage such as a hard disk or flash memory . By contrast the term CPU bound refers to a condition where the time period to complete a computation e.g. the operations performed by a thread is primarily determined by time spent processing data. It will be appreciated that a stream might become I O bound for example if the streaming windows of tuples processed by a thread involved in the stream e.g. an operator instance or a server buffer spill over from main memory to persistent storage. In an example embodiment the software might determine that a stream is I O bound if throughput increases as a result of a partition of one or both of the stream s operator instances e.g. when the software performs automated sensitivity analysis with respect to partitioning and throughput. It will be appreciated that such partitioning might be avoided by combining the operator instances connected by a stream into a single operator instance which in turn might free up resources such as containers.

In operation the software from the real time streaming application receives a request to combine the operator instances connected by the stream into a single operator instance. In an example embodiment the request might involve the setting of an attribute on the stream e.g. STREAM LOCALITY ThreadLocal. In the same or other embodiments the request might take the form of a change to the stream mode as described earlier e.g. from in node to in line or from in rack to in line. In the request occurs at run time e.g. after the launch of the real time streaming application. Alternatively the request might occur prior to launch in the logical plan e.g. the specification for the real time streaming application.

It will be appreciated that in a throughput statistic provides the indication that a stream is I O bound. However in the same or an alternative embodiment the indication might involve s a platform measurement of another statistic or b a platform measurement as to a pre defined hint e.g. number of hash table map keys.

As depicted in the software e.g. the STRAM receives a real time streaming application that runs on a distributed streaming platform in operation . In an example embodiment the real time streaming application is structured as a directed acyclic graph DAG with instances of operators as nodes and streams as edges between nodes. The real time streaming application is associated with a pre defined hint that is a key value pair e.g. the number of entries in a hash table map . In operation the software launches the real time streaming application by initiating streams and assigning instances of operators to containers provided by the distributed streaming platform. In operation the software reads the value for a pre defined hint e.g. the number of entries in hash table map and transmits the value to the real time streaming application through an application programming interface API exposed by the distributed streaming platform. Then in operation the software receives a request from the real time streaming application through the API to make a dynamic adjustment e.g. partition an instance of an operator reading from and or writing to a hash table map into multiple instances of the operator . And in operation the software makes the dynamic adjustment and initiates re initiates the corresponding streams using a recovery policy as described above.

Here again each of the operations in this process might be executed in real time or near real time rather than offline in an example embodiment. Moreover some of the operations described in the process e.g. the operations following the original launch of the real time streaming application might be continuous or non stop operations in an example embodiment.

The process described above uses as an example a key value pair that is number of entries in a hash table map. This example is not meant to be limiting. For example in the same or other embodiments the key value pair might be a count associated with another data structure e.g. the number of entries in a linked list.

In operation the software makes a dynamic adjustment e.g. partitioning an instance of an operator reading from and or writing to a hash table map into multiple instances e.g. 3 instances of the operator. In an example embodiment this dynamic partitioning might result in the creation of multiple instances e.g. 3 instances of the hash table map for example if the hash table map is local variable e.g. on the thread stack for the operator instance. Or the dynamic partitioning might result in no new additional instances of the hash table map for example if the hash table map is a global variable e.g. container data for the operator instance. In an example embodiment the dynamic adjustment might not involve partitioning an operator instance. Rather the dynamic adjustment might involve setting unsetting an operator attribute e.g. a customization of the operational behavior of an operator . Or the dynamic adjustment might involve setting or unsetting an operator property e.g. a customization of the formal definition of an operator . Also in an example embodiment the dynamic adjustment might involve setting the locality attribute of a stream to ThreadLocal to combine two operators into one. Or the dynamic adjustment might involve flushing a hash table map to disk.

Here again in an example embodiment the dynamic partitioning in operation 7 might result in the creation of multiple instances e.g. 3 instances of the hash table map for example if the hash table map is local variable e.g. on the thread stack for the operator instance. Or the dynamic partitioning might result in no new additional instances of the hash table map for example if the hash table map is a global variable e.g. container data for the operator instance.

As depicted in the software e.g. the STRAM receives a real time streaming application that runs on a distributed streaming platform in operation . The real time streaming application is structured as a directed acyclic graph DAG with instances of operators as nodes and streams as edges between nodes. In an example embodiment multiple e.g. partitioned instances of the same operator access the same database. In operation the software receives a pre defined hint associated with the real time steaming application where the pre defined hint sets a maximum period of time e.g. approximately 8 hours for local caching of a result from a query of the database by each of the multiple instances. Alternatively the maximum period of time for local caching might be a default setting for the real time streaming application. In operation the software launches the real time streaming application by assigning instances of operators to containers provided by the distributed streaming platform and initiating streams. In an example embodiment each container is associated with a local cache which is resizable upward or downward e.g. from approximately 4 GB . Then in operation the software monitors a performance statistic e.g. throughput such as number of bytes read per ms number of bytes written per ms number of keys read per ms number of keys written per ms etc. for accesses to the database by each of the multiple instances and transmits the results of the monitoring to the real time streaming application through an application programming interface API exposed by the distributed streaming platform. In an example embodiment the number of keys read written per ms might pertain to the keys in a key value table map e.g. a social security number or SSN used as a key for a person s address . In operation the software receives a request from the real time streaming application through the API to make a dynamic adjustment that increases the maximum period of time e.g. to approximately 24 hours or more for local caching of a result from a query of the database by each of multiple instances. And in operation the software makes the dynamic adjustment.

Here again each of the operations in this process might be executed in real time or near real time rather than offline in an example embodiment. Moreover some of the operations described in the process e.g. the operations following the original launch of the real time streaming application might be continuous or non stop operations in an example embodiment.

In an example embodiment YARN might be used to create the local cache in each container on an as needed basis. It will be appreciated that such a local cache e.g. a compute local memory cache differs from the traditional implementation of Memcached which employs a client server relationship with a relatively small number of servers providing memory caching services to a relatively large number of clients.

As depicted in the software e.g. the STRAM receives a real time streaming application that runs on distributed streaming platform in operation . The real time streaming application is structured as directed acyclic graph DAG with operators as nodes and streams as edges between nodes. In an example embodiment the real time streaming application includes an operator that receives values e.g. values in key value table map counts values that are unique or distinct and emits the unique values in a stream. In operation the software partitions e.g. statically in a specification for the real time streaming application or dynamically based on monitoring the operator into multiple partitioned instances. In an example embodiment the partitioning results in a round robin distribution of values to the multiple partitioned instances. In operation the software assigns each unique value emitting from a partitioned instance to one of a group of unifiers according to a pre defined scheme e.g. alphabetic ASCII or numeric . In an example embodiment each partitionable unifier creates a count of the unique values received by that partitionable unifier. Then in operation the software transmits the counts from each of the partitionable unifiers to a downstream instance of an operator that aggregates the counts into a sum of unique values. And in operation the software causes the sum of unique values to be displayed in a graphical user interface GUI such as a dashboard.

Here again each of the operations in this process might be executed in real time or near real time rather than offline in an example embodiment. Moreover some of the operations described in the process e.g. the operations following the original launch of the real time streaming application might be continuous or non stop operations in an example embodiment.

Use case 2 shown in solves the scalability problem by using multiple instances labeled Inst 1 Inst 2 and Inst 3 of the same operator executing in parallel which transmit counts of unique values to a downstream operator e.g. a unifier labeled Unif that sums the counts. In an example embodiment the values are distributed between the three operator instances using a round robin scheme that results in each operator instance receiving approximately 33.3 of the values. It will be appreciated that an accurate sum of unique values might not be obtained in this use case. For example if the values include three values of the surname Smith and each of these values is distributed by the round robin scheme to a different operator instance this value will be reported as unique or distinct even though it is not.

Use case 3 shown in also solves the scalability problem by using multiple instances Inst 1 Inst 2 and Inst 3 of the same operator executing in parallel. In an example embodiment the values are distributed between the three operator instances using a sticky key partition based on the first letter e.g. an ASCII char value or char in a surname e.g. a string array of ASCII char values or chars . So surnames whose first letter is between A H inclusive go to operator instance Inst 1 surnames whose first letter is between I P inclusive go to operator instance Inst 2 and surnames whose first letter is between R Z inclusive go to operator instance Inst 3. It will be appreciated that an accurate sum of unique values can be obtained in this use case. However skewing might occur in this use case as shown by the bold arrows going from the input to Inst 1 and from Inst 1 to the unifier if the values being input show a skewed distribution e.g. most of the input surnames have first letters between A H inclusive .

In the example uses surnames e.g. a string array of ASCII char values or chars as the values input to the partitioned operator instances. This example is not meant to be limiting. In the same or an alternative embodiment the values input to the partitioned operator instances might be integers floating point numbers enumerated values or any other ordinal values whether individually or in a collection such as an array or list. It will be appreciated that partitionable unifiers as described in allow the computation of unique or distinct values in a scalable and distributed fashion.

In an example embodiment the operations described in might be implemented as platform functionality. In such an embodiment the sticky key partition amongst the partitionable unifiers might be performed using a function call e.g. Get Partionable Unifier Key.

Though some of the embodiments described above have involved a stock ticker they are intended as illustrative rather than limiting. In another example embodiment some or all of the operations described above might be used with online machine learning including online active learning where predictions are compared with subsequent feedback received from a data stream e.g. a stock ticker or a human classifier labeler. Or some or all of the operations described above might be used to provide pricing in real time to a stock exchange an advertising exchange or other online market. Also some or all of the operations described above might be used for analyzing websites targeting ads recommending goods or services providing search results or other responses to queries geo positioning including geo location inventory analysis online gaming including social gaming network routing including routing in wireless networks etc. Or some or all of the operations described above might be used for security including fraud detection outage detection e.g. in a data center or other analyses of event data including sensor data in real time.

In an example embodiment advertising models that use bidding mechanisms can also benefit from near real time performance analysis which enables buyers and sellers to make faster changes to ad pricing or inventory adjustments.

Returning to personal computer and the servers in website and website might include 1 hardware consisting of one or more microprocessors e.g. from the x86 family the PowerPC family the ARM family etc. volatile storage e.g. RAM and persistent storage e.g. a hard disk or solid state drive and 2 an operating system e.g. Linux Windows Server Mac OS Server Windows Mac OS etc. that runs on the hardware. Similarly in an example embodiment mobile device might include 1 hardware consisting of one or more microprocessors e.g. from the ARM family the x86 family etc. volatile storage e.g. RAM and persistent storage e.g. flash memory such as microSD and 2 an operating system e.g. Symbian OS RIM BlackBerry OS iPhone OS Palm webOS Windows Mobile Android Linux etc. that runs on the hardware.

Also in an example embodiment personal computer and mobile device might each include a browser as an application program or as part of an operating system. Examples of browsers that might execute on personal computer include Internet Explorer Mozilla Firefox Safari and Google Chrome. Examples of browsers that might execute on mobile device include Safari Mozilla Firefox Android Browser and Palm webOS Browser. It will be appreciated that users of personal computer and mobile device might use browsers to communicate e.g. through a graphical user interface or GUI with website software running on the servers at website . Alternatively a users of personal computer and mobile device might communicate with website directly or indirectly e.g. using a script through a command line interface CLI .

It will be appreciated that the above example embodiments include functionality that 1 enables a slave e.g. STRAM Child to monitor operator instances in the slave s container and effectuate dynamic adjustments ordered by the STRAM 2 generates streaming windows using control tuples inserted by an input adapter that creates data tuples from an external data stream through the application of a schema 3 displays data from data tuples in a GUI view using an output adapter that removes control tuples and 4 supports checkpointing on streaming window boundaries using checkpoint tuples inserted by an input adapter.

With the above embodiments in mind it should be understood that the inventions might employ various computer implemented operations involving data stored in computer systems. Any of the operations described herein that form part of the inventions are useful machine operations. The inventions also relate to a device or an apparatus for performing these operations. The apparatus may be a general purpose computer selectively activated or configured by a computer program stored in the computer. In particular various general purpose machines may be used with computer programs written in accordance with the teachings herein or it may be more convenient to construct a more specialized apparatus to perform the required operations.

The inventions can also be embodied as computer readable code on a computer readable medium. The computer readable medium is any data storage device that can store data which can thereafter be read by a computer system. Examples of the computer readable medium include hard drives network attached storage NAS read only memory random access memory CD ROMs CD Rs CD RWs DVDs Flash magnetic tapes and other optical and non optical data storage devices. The computer readable medium can also be distributed over a network coupled computer systems so that the computer readable code is stored and executed in a distributed fashion.

Although example embodiments of the inventions have been described in some detail for purposes of clarity of understanding it will be apparent that certain changes and modifications can be practiced within the scope of the following claims. For example some or all of the processes described above might be used with streaming media such as streaming audio or streaming video. Or the hardware for the distributed streaming platform might include a quantum computer e.g. D Wave System s quantum computer along with or instead of traditional servers e.g. in the x86 or ARM families . Moreover the operations described above can be ordered modularized and or distributed in any suitable way. Accordingly the present embodiments are to be considered as illustrative and not restrictive and the inventions are not to be limited to the details given herein but may be modified within the scope and equivalents of the following claims. In the following claims elements and or steps do not imply any particular order of operation unless explicitly stated in the claims or implicitly required by the disclosure.

The example embodiments described in this disclosure include the following 1 a method comprising the operations of launching an application that runs on a streaming platform wherein the application is structured as a directed acyclic graph DAG with instances of operators as nodes and streams as edges between nodes receiving an indication that a first instance of an operator is I O bound wherein a stream connects the first instance of an operator to a second instance of another operator in a single container provided by the streaming platform transmitting the indication to the application and receiving a request to combine the first instance with the second instance into a single third instance of an operator and creating the third instance and re initiating the stream using a recovery policy wherein each of the operations is executed by one or more processors in real time or near real time rather than offline 2 a method as in 1 wherein the instances of operators are program instructions and the streams are unbound sequences of streaming windows that are ordered in terms of time 3 a method as in 2 wherein each streaming window is an atomic microbatch of sequential structured tuples 4 a method as in 1 wherein the indication includes a platform measurement as to resource use 5 a method as in 4 wherein the platform measurement is throughput of streaming windows 6 a method as in 4 wherein the platform measurement is throughput of tuples 7 a method as in 1 wherein the third instance is created at least in part through compiler in lining 8 a method as in 1 wherein the request sets an attribute of the stream 9 one or more computer readable media persistently storing one or more programs wherein the one or more programs when executed instruct one or more processors to perform the following operations launch an application that runs on a streaming platform wherein the application is structured as a directed acyclic graph DAG with instances of operators as nodes and streams as edges between nodes receive an indication that a first operator instance is I O bound wherein a stream connects the first operator instance to a second operator instance in a single container provided by the streaming platform transmit the indication to the application and receive a request to combine the first operator with the second operator instance into a single third operator instance and create the third operator instance and re initiate the stream using a recovery policy wherein each of the operations is executed in real time or near real time rather than offline 10 computer readable media as in 9 wherein the operators are program instructions and the streams are unbound sequences of streaming windows that are ordered in terms of time 11 computer readable media as in 10 wherein each streaming window is an atomic microbatch of sequential structured tuples 12 computer readable media as in 9 wherein the indication includes a platform measurement as to resource use 13 computer readable media as in 12 wherein the platform measurement is throughput of streaming windows 14 computer readable media as in 12 wherein the platform measurement is throughput of tuples 15 computer readable media as in 9 wherein the third instance is created at least in part through compiler in lining 16 computer readable media as in 9 wherein the request sets an attribute of the stream 17 a method comprising the operations of launching an application that runs on a streaming platform wherein the application is structured as a directed acyclic graph DAG with instances of operators as nodes and streams as edges between nodes receiving an indication that a first operator instance is I O bound wherein a stream connects the first operator instance to a second operator instance in a single container provided by the streaming platform transmitting the indication to the application and receiving a request to combine the first operator with the second operator instance into a single third operator instance wherein the request sets an attribute on a stream and creating the third operator instance and re initiate the stream wherein each of the operations is executed by one or more processors in real time or near real time rather than offline 18 a method as in 17 wherein the indication includes a platform measurement as to resource use 19 a method as in 18 wherein the platform measurement is throughput of tuples 20 a method as in 17 wherein the third instance is created at least in part through compiler in lining.

The example embodiments described in this disclosure further include the following 1 a method comprising the operations of receiving an application that runs on a streaming platform wherein the application is structured as a directed acyclic graph DAG with instances of operators as nodes and streams as edges between nodes and wherein multiple instances of an operator access a shared database receiving a pre defined hint associated with the application wherein the pre defined hint sets a maximum period of time for local caching of a result from a query of the database by each of the multiple instances launching the application by assigning the instances of operators to one or more containers provided by the streaming platform and initiating the streams wherein each container is associated with a local cache monitoring a performance statistic for accesses to the database by each of the multiple instances and transmitting results of the monitoring to the application through an application programming interface API exposed by the streaming platform receiving a request from the application through the API to make a dynamic adjustment that increases the maximum period of time for local caching of a result from a query of the database by each of the multiple instances and making the dynamic adjustment and re launching the application using a recovery policy wherein each of the operations is executed by one or more processors in real time or near real time rather than offline 2 a method as in 1 wherein the instances of operators are program instructions and the streams are unbound sequences of streaming windows that are ordered in terms of time 3 a method as in 2 wherein each streaming window is an atomic microbatch of sequential structured tuples 4 a method as in 1 wherein the local cache is resizable upward or downward 5 a method as in 1 wherein the performance statistic measures throughput 6 a method as in 1 wherein the performance statistic measures keys read 7 a method as in 1 wherein the performance statistic measures keys written 8 one or more computer readable media persistently storing one or more programs wherein the one or more programs when executed instruct one or more processors to perform the following operations receive an application that runs on a streaming platform wherein the application is structured as a directed acyclic graph DAG with instances of operators as nodes and streams as edges between nodes and wherein multiple instances of an operator access a shared database receive a pre defined hint associated with the application wherein the pre defined hint sets a maximum period of time for local caching of a result from a query of the database by each of the multiple instances launch the application by assigning the instances of operators to one or more containers provided by the streaming platform and initiating the streams wherein each container is associated with a local cache monitor a performance statistic for accesses to the database by each of the multiple instances and transmit results of the monitoring to the application through an application programming interface API exposed by the streaming platform receive a request from the application through the API to make a dynamic adjustment that increases the maximum period of time for local caching of a result from a query of the database by each of the multiple instances and make the dynamic adjustment and re launch the application using a recovery policy wherein each of the operations is executed in real time or near real time rather than offline 9 computer readable media as in 8 wherein the instances of operators are program instructions and the streams are unbound sequences of streaming windows that are ordered in terms of time 10 computer readable media as in 9 wherein each streaming window is an atomic microbatch of sequential structured tuples 11 computer readable media as in 8 wherein the local cache is resizable upward or downward 12 computer readable media as in 8 wherein the performance statistic measures throughput 13 computer readable media as in 8 wherein the performance statistic measures keys read 14 computer readable media as in 8 wherein the performance statistic measures keys written 15 a method comprising the operations of receiving an application that runs on a streaming platform wherein the application is structured as a directed acyclic graph DAG with instances of operators as nodes and streams as edges between nodes and wherein multiple instances of an operator access a shared database receiving a pre defined hint associated with the application wherein the pre defined hint sets a maximum period of time for local caching of a result from a query of the database by each of the multiple instances launching the application by assigning the instances of operators to one or more containers provided by the streaming platform and initiating the streams wherein each container is associated with a local cache which is resizable upward or downward monitoring a performance statistic for accesses to the database by each of the multiple instances and transmitting results of the monitoring to the application through an application programming interface API exposed by the streaming platform receiving a request from the application through the API to make a dynamic adjustment that increases the maximum period of time for local caching of a result from a query of the database by each of the multiple instances and making the dynamic adjustment and re launching the application wherein each of the operations is executed by one or more processors in real time or near real time rather than offline 16 a method such as 15 wherein the instances of operators are program instructions and the streams are unbound sequences of streaming windows that are ordered in terms of time 17 a method such as 16 wherein each streaming window is an atomic microbatch of sequential structured tuples 18 a method such as 15 wherein the performance statistic measures throughput 19 a method such as 15 wherein the performance statistic measures keys read and 20 a method such as 15 wherein the performance statistic measures keys written.

And the example embodiments described in this disclosure include the following 1 a method comprising the operations of receiving an application that runs on a streaming platform wherein the application is structured as a directed acyclic graph DAG with operators as nodes and streams as edges between nodes and wherein the application includes an operator that receives a plurality of values counts the values that are unique and emits the unique values in a stream partitioning the operator into at least two partitioned instances assigning a unique value emitting from a partitioned instance to one of a plurality of unifiers according to a pre defined scheme wherein each unifier creates a count of the unique values received by the unifier transmitting the counts from each of the unifiers to a downstream instance of an operator that aggregates the counts into a sum and displaying the sum in a graphical user interface GUI wherein each of the operations is executed by one or more processors in real time or near real time rather than offline 2 a method as in 1 wherein the instances of operators are program instructions and the streams are unbound sequences of streaming windows that are ordered in terms of time 3 a method as in 2 wherein each streaming window is an atomic microbatch of sequential structured tuples 4 a method as in 1 wherein the partitioning is static partitioning in a specification for the application 5 a method as in 1 wherein the partitioning is dynamic partitioning based on results of monitoring the partitioned instances 6 a method as in 1 wherein the pre defined scheme is ASCII 7 a method as in 1 wherein the pre defined scheme is numerical 8 a method as in 1 wherein the partitioning results in a round robin distribution of values to the partitioned instances 9 a method as in 1 wherein the values are values in a key value table or a key value map 10 one or more computer readable media persistently storing one or more programs wherein the one or more programs when executed instruct one or more processors to perform the following operations receive an application that runs on a streaming platform wherein the application is structured as a directed acyclic graph DAG with operators as nodes and streams as edges between nodes and wherein the application includes an operator that receives a plurality of values counts the values that are unique and emits the unique values in a stream partition the operator into at least two partitioned instances assign a unique value emitting from a partitioned instance to one of a plurality of unifiers according to a pre defined scheme wherein each unifier creates a count of the unique values received by the unifier transmit the counts from each of the unifiers to a downstream instance of an operator that aggregates the counts into a sum and display the sum in a graphical user interface GUI wherein each of the operations is executed in real time or near real time rather than offline 11 computer readable media as in 10 wherein the instances of operators are program instructions and the streams are unbound sequences of streaming windows that are ordered in terms of time 12 computer readable media as in 11 wherein each streaming window is an atomic microbatch of sequential structured tuples 13 computer readable media as in 10 wherein the partitioning is static partitioning in a specification for the application 14 computer readable media as in 10 wherein the partitioning is dynamic partitioning based on results of monitoring the partitioned instances 15 computer readable media as in 10 wherein the pre defined scheme is ASCII 16 computer readable media as in 10 wherein the pre defined scheme is numerical 17 computer readable media as in 10 wherein the partitioning results in a round robin distribution of values to the partitioned instances 18 computer readable media as in 10 wherein the values are values in a key value table or a key value map 19 a method comprising the operations of receiving an application that runs on a streaming platform wherein the application is structured as a directed acyclic graph DAG with operators as nodes and streams as edges between nodes and wherein the application includes an operator that receives a plurality of values counts the values that are unique and emits the unique values in a stream partitioning the operator into at least two partitioned instances wherein the partitioning is static partitioning in a specification for the application and the partitioning results in a round robin distribution of values to the partitioned instances assigning a unique value emitting from a partitioned instance to one of a plurality of unifiers according to a pre defined scheme wherein each unifier creates a count of the unique values received by the unifier transmitting the counts from each of the unifiers to a downstream instance of an operator that aggregates the counts into a sum and displaying the sum in a graphical user interface GUI wherein each of the operations is executed by one or more processors in real time or near real time rather than offline 20 a method as in 19 wherein the pre defined scheme is ASCII.

