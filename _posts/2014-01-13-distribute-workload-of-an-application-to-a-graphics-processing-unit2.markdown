---

title: Distribute workload of an application to a graphics processing unit
abstract: Systems, methods, and techniques of distributing a workload of an application to a GPU are provided. An example method includes obtaining an intermediate representation of a source code portion of an application and compiling the intermediate representation into a set of instructions that is native to the GPU. The set of instructions includes a binary representation of the source code portion executable on the GPU, and execution of the set of instructions on the GPU includes processing a workload of the application. The method also includes transforming data associated with the source code portion into one or more data types native to the GPU and sending to the GPU a communication including the set of instructions executable on the GPU and the one or more data types native to the GPU.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09632761&OS=09632761&RS=09632761
owner: Red Hat, Inc.
number: 09632761
owner_city: Raleigh
owner_country: US
publication_date: 20140113
---
The present disclosure generally relates to computing devices and more particularly to distributing a workload of an application.

A computation may be performed on one or more computing devices. Certain computations e.g. parallel processing or parallel programming may benefit from using processing units of the computing device. For example a complex computation may run faster if the computation is divided into portions and the portions are run on a number of processing units. Graphics processing units GPUs are leading the trend for performing computationally intensive tasks for applications.

It may be desirable to parallelize computations especially when working with complex data and large data sets. This disclosure relates to distributing a workload of an application for execution on a graphics processing unit GPU . Methods systems and techniques for distributing the workload to one or more GPU s and or to one or more GPU core s are provided.

According to an embodiment a system for distributing a workload of an application to a graphics processing unit GPU includes a computing device coupled to a plurality of processing units. The plurality of processing units includes a central processing unit CPU and a GPU. Each of the CPU and GPU executes a workload of an application. The system also includes a scheduler that obtains an intermediate representation of a source code portion of the application. The scheduler transforms data associated with the source code portion into one or more data types native to the GPU. The system further includes a compiler that compiles the intermediate representation into a set of instructions that is native to the GPU. The set of instructions includes a binary representation of the source code portion executable on the GPU. Execution of the set of instructions on the GPU includes processing a workload of the application. The CPU sends to the GPU the set of instructions executable on the GPU and the one or more data types native to the GPU. The communication causes the GPU to execute the set of instructions using the one or more data types native to the GPU.

According to another embodiment a method of distributing a workload of an application to a GPU includes obtaining an intermediate representation of a source code portion of an application. The method also includes compiling the intermediate representation into a set of instructions that is native to the GPU. The set of instructions includes a binary representation of the source code portion executable on the GPU. Execution of the set of instructions on the GPU includes processing a workload of the application. The method further includes transforming data associated with the source code portion into one or more data types native to the GPU. The method also includes sending to the GPU a communication including the set of instructions executable on the GPU and the one or more data types native to the GPU. The communication causes the GPU to execute the set of instructions using the one or more data types native to the GPU.

According to another embodiment a non transitory machine readable medium includes a plurality of machine readable instructions that when executed by one or more processors are adapted to cause the one or more processors to perform a method including obtaining an intermediate representation of a source code portion of an application compiling the intermediate representation into a set of instructions that is native to the GPU where the set of instructions includes a binary representation of the source code portion executable on the GPU and where execution of the set of instructions on the GPU includes processing a workload of the application transforming data associated with the source code portion into one or more data types native to the GPU and sending to the GPU a communication including the set of instructions executable on the GPU and the one or more data types native to the GPU where the communication causes the GPU to execute the set of instructions using the one or more data types native to the GPU.

It is to be understood that the following disclosure provides many different embodiments or examples for implementing different features of the present disclosure. Some embodiments may be practiced without some or all of these specific details. Specific examples of components modules and arrangements are described below to simplify the present disclosure. These are of course merely examples and are not intended to be limiting.

It may be advantageous to distribute the workload of an application across multiple processing units. A computing device may be equipped with one or more high performance GPUs that provides high performance with regard to computations. The GPU may be equipped with one or more GPU cores. It may be advantageous to enable an application to harness the power of a GPU to perform computations.

In an embodiment a system for distributing a workload of an application to a GPU includes a computing device coupled to a plurality of processing units. The plurality of processing units includes a central processing unit CPU and a GPU. Each of the CPU and GPU executes a workload of an application. The system also includes a scheduler that obtains an intermediate representation of a source code portion of the application. The scheduler transforms data associated with the source code portion into one or more data types native to the GPU. The system further includes a compiler that compiles the intermediate representation into a set of instructions that is native to the GPU. The set of instructions includes a binary representation of the source code portion executable on the GPU. Execution of the set of instructions on the GPU includes processing a workload of the application. The CPU sends to the GPU the set of instructions executable on the GPU and the one or more data types native to the GPU. The communication causes the GPU to execute the set of instructions using the one or more data types native to the GPU.

Although an application may be described as being compiled once into an intermediate representation which is then compiled into code that is native to a GPU it should be understood that other embodiments in which the intermediate representation is compiled into another intermediate representation multiple times before being compiled into code that is native to the GPU are within the scope of the disclosure. For example application may be compiled into a first intermediate representation which is compiled into a second intermediate representation which may be further compiled into another intermediate representation which is finally compiled into code that is native to a GPU and executable on the GPU.

Computing device is coupled to hardware which includes a plurality of processing units. The plurality of processing units includes a CPU and a GPU . Although one CPU and one GPU are illustrated other embodiments including more than one CPU and or more than one GPU are within the scope of the present disclosure. Further each CPU may include one or more CPU cores and each GPU may include one or more GPU cores. In an example a GPU may include multiple processing cores that share memory. In such an example each GPU core may have its own L1 cache and the L2 cache may be common to all GPU cores which connect to the L2 cache via a shared bus.

Computing device includes a node that is a software program that runs and communicates with applications. Node runs application and may be an operating system or a virtual machine running on a computing device. A virtual machine is a software implementation of a machine that may execute programs. In an example the virtual machine may be a JAVA virtual machine JVM . Trademarks are the property of their respective owners.

In a source code compiler compiles application into an intermediate representation that may be distributed to different nodes e.g. node . Node provides the runtime environment for intermediate representation . Intermediate representation may be machine independent binary code that is not directly executable on a computing device. In an example the output of source code compiler is not executable code. In such an example intermediate representation may represent an instruction set for a CPU or GPU that does not exist in reality. Intermediate representation may be compiled into instructions that are native to and directly executed by computing device .

Compilation of the source code program into an intermediate representation that is machine independent may be advantageous because the source code program may be portable and used on various hardware platforms. As such distribution of the intermediate representation may make it easier to run a program e.g. application in a wide variety of environments because any platform that implements node may run the program. The intermediate representation may be a highly optimized set of instructions designed to be executed by node . Further issues with application security may be reduced because the application is executed by node . For example node may prevent the program from generating side effects outside of the runtime environment and that spill over into the rest of the system.

The present disclosure provides techniques to extend the concept of parallel execution of an application to distributing a workload of the application to a GPU. A task that can be split into multiple workloads can be divided and executed in parallel on multiple processing units thus increasing the program throughput. It should be understood that a processing unit may refer to CPU a CPU core a GPU or a GPU core. The design of a CPU is optimized for sequential code performance and makes use of sophisticated logic to allow instructions from a single thread of execution to execute in parallel or even out of their sequential order while maintaining the appearance of sequential execution. In contrast the design of a GPU is optimized for computations and typically has a highly parallel structure. In an example the GPU may perform a massive number of floating point calculations multiple times faster than a CPU. One or more GPUs and or GPU core s may execute a workload of an application in parallel. In an example application computes prime numbers and the application developer splits up the calculation of prime numbers such that a file includes lines that each perform some work of the computation and may be processed by separate CPU s CPU core s GPU s and or GPU core s . Some work of the computation may be performed concurrently on the CPU s CPU core s GPU s and or GPU core s .

Application may include indications that one or more particular portions of the source code should be sent to the GPU for processing. In an example application includes an annotation that specifies a source code portion of application that is executable on a GPU. In the example illustrated in application includes a first source code portion A and a second source code portion B. A binary representation of first source code portion A may be executed on CPU and a binary representation of second source code portion B may be executed on GPU . In particular second source code portion B may be marked by the application developer who determines that it may be advantageous to execute second source code portion B on a GPU rather than a CPU.

In an example the application developer uses metadata e.g. an annotation associated with a class to mark it as executable on a GPU. Table A at line 1 illustrates an example Java annotation in a Java class definition indicating that the class is executable on a GPU.

Table A illustrates an example class definition of a Computation class. The class definition of the Computation class defines the particular class and includes at line 1 an annotation GPUExecutable indicating that the particular class e.g. Computation class is executable on a GPU. The Computation class is a public class that computes a power of a number. The class may contain one or more methods that is executable on the GPU.

Node may include an annotation processor not shown that processes annotations. The annotation processor may be for example a compiler plug in. The annotation processor may recognize the annotation GPUExectuable as being an indication that the Computation class is executable on a GPU. Other ways to indicate a class as being executable on a GPU are within the scope of the disclosure. In an example node has a list that includes classes that are executable on a GPU. When node executes the intermediate representation of application node may determine whether the particular class is executable on a GPU by checking the list.

Node obtains intermediate representation of application . Intermediate representation includes a first intermediate representation A and a second intermediate representation B. First intermediate representation A is an intermediate representation of first source code portion A and second intermediate representation B is an intermediate representation of second source code portion B. In an example node obtains intermediate representations A and B from the same file. In another example node obtains intermediate representations A and B from different files that are received at different times.

In an example application is a Java application that is written in the Java programming language intermediate representation is Java bytecode and node is a JVM. In such an example source code compiler compiles the Java application into Java bytecode which is converted to code that is executable on a GPU. Although the present disclosure may describe the application as being a Java application intermediate representation as being Java bytecode and or node as being a JVM it should be understood that the techniques in the disclosure apply to any application a portion or all that is compiled into an intermediate representation and may benefit from the techniques provided in the disclosure. For example an application that is written in ColdFusion JRuby Jython Groovy and Scala may be compiled into an intermediate representation and may benefit from the techniques provided in the disclosure.

Node may have its own implementation of the threading model depending on its vendor and the platform for which it is written. In an example a thread is an abstraction within node . In such an example the operating system executing not shown in computing device does not know anything about the threads that exist in node . For example from the perspective of the operating system node may be a single process and a single thread.

Node includes a scheduler and communications module . Scheduler schedules code to be run on the CPU and or GPU. Scheduler may create a CPU thread that executes on the CPU and may create a GPU thread that executes on the GPU. In an embodiment scheduler schedules the execution of threads on the CPU and or GPU. Node may create a thread and hold all of its information related to the thread within the thread object itself. Such information may include the thread s stack a program counter to track the currently executing instruction and other data. Scheduler may schedule the concurrent execution of one or more CPU threads and or one or more GPU threads. Scheduler may use a parallel application programming interface API that is available via the Java Development Kit MK . For example scheduler may make API calls to schedule the concurrent execution of a plurality of threads.

In an example scheduler schedules one or more CPU threads to execute in parallel with one or more GPU threads. In such an example application is executing on computing device in parallel with a workload of application being executed on GPU . In another example scheduler schedules GPU threads to execute on a first GPU and a second GPU in parallel. In another example scheduler schedules GPU threads to execute on multiple GPU cores of GPU in parallel.

Allowing one or more CPU threads and or one or more GPU threads to run concurrently may eliminate or greatly reduce user observable pause times. A CPU thread and GPU thread run concurrently when execution of the CPU thread and the GPU thread has some overlap. In an example a time period in which CPU executes a first set of instructions may overlap with a time period in which GPU executes a second set of instructions. For example a CPU thread may run on CPU during a period of time in which a GPU thread is running on GPU . In another example a CPU thread may start running on CPU and stop before completion a GPU thread may start running on GPU and stop before completion and the CPU thread may resume running on CPU and finish. In another example a GPU thread may start running on GPU and stop before completion a CPU thread may start running on CPU and stop before completion and the GPU thread may resume running on GPU and finish.

Similarly GPU threads may run concurrently when execution of a first GPU thread and a second GPU thread has some overlap. In an example a time period in which a first GPU core of GPU executes a first set of instructions may overlap with a time period in which a second core of GPU executes a second set of instructions. For example a first GPU thread may run on the first core of GPU during a period of time in which a second GPU thread is running on the second core of GPU . In another example a first GPU thread may start running on the first core of GPU and stop before completion a second GPU thread may start running on the second core of GPU and stop before completion and the first GPU thread may resume running on the first core of GPU and finish.

Node includes a compiler specific to CPU that compiles intermediate representation A into a first set of instructions that is native to CPU . The first set of instructions includes a binary representation of source code portion A executable on CPU . The first set of instructions may be directly executed on CPU . Node also includes a compiler specific to GPU that compiles intermediate representation B into a second set of instructions that is native to GPU . The second set of instructions includes a binary representation of source code portion B executable on GPU . The second set of instructions may be directly executed on GPU and execution of the second set of instructions on GPU includes processing a workload of application . In an example the instruction sets of CPU and GPU are different.

The following is a description of a compiler that is specific to a GPU. This description may apply as well to a compiler that is specific to a CPU. Compiler specific to GPU is specific to GPU and native code may refer to code that is created to run directly only on the type of GPU for which the compiler is designed. For example any GPU may be used to process a workload of an application as long as a compiler that is specific to the particular GPU is used to compile the intermediate representation into code that is native to the GPU. Node may include one or more compilers specific to one or more GPUs.

A compiler that is specific to a GPU is dependent on the architecture of the GPU. For example GPUs that are provided by different vendors may have different instruction sets and different compilers may be specific to each of the GPUs. In an example node is a JVM and GPU is based on an ARM instruction set architecture that enables the JVM to execute Java bytecode. Traditionally JVMs may easily run on the ARM architecture. In another example node is a JVM and GPU is based on an instruction set architecture that is different from the ARM instruction set architecture. In such an example compiler specific to GPU may be a special compiler that compiles intermediate representation B to a code that is executable on the particular GPU.

Compiling intermediate representation B into a set of instructions includes generating a stream of commands for execution on GPU . CPU may generate a stream of commands including the set of instructions for execution on GPU and send the stream of commands to GPU . The intermediate representation may be compiled into the set of instruction all at once before application begins executing on computing device or may be compiled on the fly. In an example before application executes on computing device compiler specific to GPU compiles intermediate representation B into the binary representation of the source code portion executable on GPU . In another example while application is executing on computing device compiler specific to GPU compiles intermediate representation B into the binary representation of the source code portion executable on GPU .

The data types used in application may be different from data types understood by GPU . In an embodiment scheduler transforms data associated with source code portion B into one or more data types native to GPU . In an example scheduler transforms data associated with source code portion B into one or more data types native to GPU .

Application may be written in a high level programming language and the data associated with the source code portion may include data including one or more data types specific to the high level programming language. In an example application is a Java application and the data associated with the source code portion includes first data including one or more data types specific to Java. A data type specific to Java may be a Java primitive type e.g. long or int . Scheduler may transform a Java primitive type into a binary representation of a data type that is native to GPU . Java is an object oriented programming language that defines classes. A data object may be a self contained entity that has state and behavior and represents an instance of a class. The data object s state may be described as its fields and properties and the data object s behavior may be described by its methods and events. Application may include code that creates instances of a class and an instance of a particular class may expose methods and properties defined by that class.

In an embodiment transformation between data that are specific to Java and data that are specific to the GPU may be done via a library . Library may be part of the memory space for node . Scheduler obtains the first data including one or more data types specific to Java. Scheduler may transform the first data into second data including one or more data types native to GPU by passing the first data to library . In an example library is a Java library that accepts as parameters a Java data object and converts the data object to an object that is native to GPU . Responsive to passing the first data to library scheduler may obtain the second data including the one or more data types native to GPU . A data type in the first data corresponds to a data type in the second data. Additionally the data representation may be different for each GPU e.g. different implementation of floating point numbers . Accordingly library may be a library that is written for a particular GPU. As such if two GPUs having different data representations are used node may include a library specific to the data representation of the first GPU and a different library specific to the data representation of the second GPU.

As discussed the intermediate representation may be compiled into the set of instruction all at once before application begins executing on computing device or may be compiled on the fly. If compiler specific to GPU compiles intermediate representation B into the binary representation of the source code portion executable on GPU before application executes on computing device scheduler may load the compiled binary representation and send this binary representation to GPU for execution. GPU may then execute the compiled binary representation. If compiler specific to GPU compiles intermediate representation B into the binary representation of the source code portion executable on GPU while application is executing on computing device scheduler may access compiler specific to GPU to request that it compile intermediate representation B to a set of instructions that is native to GPU . In an example after application starts executing on computing device compiler specific to GPU compiles intermediate representation B into the binary representation of the source code portion executable on GPU . Scheduler may access library via an API call including the data associated with the source code portion. Responsive to the API call scheduler may receive the one or more data types native to GPU where a data type that is specific to the high level programming language and included in the API call corresponds to one or more data types native to GPU .

Scheduler schedules the set of instructions to be run on GPU and interacts with communications module to send to GPU a communication including the set of instructions executable on GPU along with the transformed data native to GPU . The data transformation may be performed when scheduler passes the data to GPU . In an example GPU is a single instruction multiple data SIMD processor and scheduler reuses the set of instructions executable on GPU . In an example scheduler sends to GPU the set of instructions native to GPU and sends to a first core of GPU a first set of data native to GPU and sends to a second core of GPU a second set of data native to GPU . Accordingly different sets of data may be executed by different GPU cores of GPU while using the same set of instructions.

In an embodiment communications module communicates with a graphics card including GPU and sends a communication to the graphics card that causes GPU to execute code. Communications module may send to GPU a communication including the set of instructions executable on GPU and the one or more data types native to GPU . The communication causes GPU to execute the set of instructions using the one or more data types native to the GPU. The set of instructions and the one or more data types native to GPU is represented in a binary format including for example parameters intended for consumption by GPU .

Although computing device is illustrated as including one GPU this is not intended to be limiting and other embodiments in which the computing device includes a plurality of GPUs are within the scope of the disclosure. In an example computing device includes a first GPU and a second GPU. Scheduler may transform the data associated with second source code portion B and a third source code portion not shown of application into one or more data types native to the first and second GPUs respectively. The data may include a first set of data and a second set of data where each of the first and second sets of data includes one or more data types specific to Java. Transforming the data may include transforming the one or more data types of the first set of data into one or more data types native to the first GPU and transforming the one or more data types of the second set of data into one or more data types native to the second GPU. As discussed scheduler may use a library e.g. library to obtain the one or more data types native to the first GPU and the one or more data types native to the second GPU. Compiler specific to GPU may compile second source code portion B into a first set of instructions and may compile the third source code portion into a second set of instructions.

A communication including the first set of instructions native to the first GPU along with the one or more transformed data types native to the first GPU may be sent to the first GPU and a communication including the second set of instructions native to the second GPU along with one or more transformed data types native to the second GPU may be sent to the second GPU. The first GPU may execute the first set of instructions using the one or more transformed data types native to the first GPU in parallel with the second GPU executing the second set of instructions using the one or more transformed data types native to the second GPU. Additionally the first and second sets of instructions executed by the first and second GPUs may be the same or may be different.

In another example GPU of computing device includes a first GPU core and a second GPU core. Scheduler may transform the data associated with second source code portion B and a third source code portion not shown of application into one or more data types native to the first and second GPU cores respectively. The data include a first set of data and a second set of data where each of the first and second sets of data includes one or more data types specific to the application. Transforming the data may include transforming the one or more data types of the first set of data into one or more data types native to the GPU and transforming the one or more data types of the second set of data into one or more data types native to the GPU. As discussed scheduler may use library to obtain the one or more data types native to the GPU. Compiler specific to GPU may compile second source code portion B into a first set of instructions and may compile the third source code portion into a second set of instructions.

A communication including the first set of instructions native to the GPU along with the one or more transformed data types native to the GPU may be sent to the first GPU core and a communication including the second set of instructions native to the GPU along with one or more transformed data types native to the GPU may be sent to the second GPU core. The first GPU core may execute the first set of instructions using the one or more transformed data types native to the GPU in parallel with the second GPU core executing the second set of instructions using the one or more transformed data types native to the GPU. Additionally the first and second sets of instructions executed by the first and second GPU cores may be the same or may be different.

Each of the GPUs may receive the communication including the set of instructions executable on the GPU and the one or more data types native to the GPU and execute the set of instructions using the one or more data types native to the GPU to determine one or more results. In an example GPU receives the communication including the set of instructions executable on GPU and the one or more data types native to GPU . Responsive to the communication GPU may execute the set of instructions using the one or more data types native to GPU to determine one or more results.

Communications module may receive a response to the communication where the response includes a result of the GPU processed workload e.g. from each of the GPUs . In an example the result includes third data including one or more data types native to GPU . In keeping with the above example scheduler may transform the third data including the one or more data types native to GPU into fourth data including one or more data types specific to Java. A data type in the third data corresponds to a data type in the fourth data. Scheduler may perform this transformation for each of the results that scheduler receives.

As discussed transformation between data that are specific to Java and data that are specific to the GPU may be done via library . Scheduler obtains the third data including one or more data types native to GPU . Scheduler may transform the third data into fourth data including one or more data types specific to Java by passing the third data to library . In an example library is a Java library that accepts as parameters a data object that is native to GPU and converts the data object to a Java data object. Responsive to passing the third data to library scheduler may obtain the fourth data including the one or more data types specific to Java. A data type in the third data corresponds to a data type in the fourth data. Communications module may send the fourth data to application .

As discussed above and further emphasized here is merely an example which should not unduly limit the scope of the claims. In an example although block diagram is described herein with one application running on computing device other embodiments including more than one application running on computing device are within the scope of this disclosure. Further each of scheduler compiler specific to CPU compiler specific to GPU and communications module may be implemented on a computing device that includes a memory and a processor. Further scheduler compiler specific to CPU compiler specific to GPU and communications module may be implemented on different computing devices and may be separated by a network. The network may include various configurations and use various protocols including the Internet World Wide Web intranets virtual private networks wide area networks local networks private networks using communication protocols proprietary to one or more companies Ethernet WiFi and HTTP and various combinations of the foregoing.

The processing units may reside in a common computing device or may reside in distributed computing devices. For example CPU and GPU may reside in different computing devices. In such an example CPU may send over a network a message including a set of instructions executable on GPU and the one or more data types native to GPU . GPU may execute the set of instructions using the one or more data types native to the GPU and may send a response including a result of the GPU processed workload to computing device .

Method includes blocks . In a block an intermediate representation of a source code portion of an application is obtained. In an example node obtains intermediate representation B of source code portion B of application . In a block the intermediate representation is compiled into a set of instructions that is native to the GPU where the set of instructions includes a binary representation of the source code portion executable on the GPU and where execution of the set of instructions on the GPU includes processing a workload of the application. In an example compiler specific to GPU compiles intermediate representation B into a set of instructions that is native to GPU where the set of instructions includes a binary representation of source code portion B executable on GPU and where execution of the set of instructions on GPU includes processing a workload of application .

In a block data associated with the source code portion are transformed into one or more data types native to the GPU. In an example scheduler transforms data associated with source code portion B into one or more data types native to GPU . In a block a communication including the set of instructions executable on the GPU and the one or more data types native to the GPU is sent to the GPU where the communication causes the GPU to execute the set of instructions using the one or more data types native to the GPU. In an example communications module sends to GPU a communication including the set of instructions executable on GPU and the one or more data types native to GPU where the communication causes GPU to execute the set of instructions using the one or more data types native to GPU .

It is also understood that additional processes may be inserted before during or after blocks discussed above. It is also understood that one or more of the blocks of method described herein may be omitted combined or performed in a different sequence as desired.

Computer system includes a bus or other communication mechanism for communicating information data signals and information between various components of computer system . A CPU which may be a micro controller digital signal processor DSP or other processing component processes these various signals such as for display on computer system or transmission to other devices e.g. GPU via communication link . CPU may generate a stream of commands to send to GPU via bus . Components of computer system also include a system memory component e.g. RAM a static storage component e.g. ROM and or a disk drive . Computer system performs specific operations by CPU and other components by executing one or more sequences of instructions contained in system memory component .

In an example memory includes a CPU memory space and a GPU memory space. The CPU memory space may be separate from the GPU memory space. In such an example to enable CPU and GPU to access data stored in the memory space of the other an application is executed that includes instructions that manually copy data back and forth between the separate memory spaces. In another example the CPU memory space and GPU memory space are unified memory spaces that relieve the application from including instructions that manually copy data back and forth between the memory spaces. In an example bus is a PCI bus that delivers data between CPU and GPU . Although bus is illustrated as being the path that delivers data between CPU and GPU this is not intended to be limiting and other embodiments using a different mechanism to deliver data between CPU and GPU are within the scope of the disclosure. In an example an accelerated graphics port AGP which is an interface specification is used to provide a dedicated point to point channel so that GPU may directly access memory .

Components include an input output I O component that processes a user action such as selecting keys from a keypad keyboard selecting one or more buttons or links etc. and sends a corresponding signal to bus . I O component may also include an output component such as a display and an input control such as a cursor control such as a keyboard keypad mouse etc. . An optional audio input output component may also be included to allow a user to use voice for inputting information by converting audio signals into information signals. Audio I O component may allow the user to hear audio. A transceiver or network interface transmits and receives signals between computer system and other devices via a communication link to a network. In an embodiment the transmission is wireless although other transmission mediums and methods may also be suitable. In an example computer system sends a workload of an application to another GPU that is couple to computer system over a network via network interface .

Logic may be encoded in a computer readable medium which may refer to any medium that participates in providing instructions to CPU for execution. Such a medium may take many forms including but not limited to non volatile media volatile media and transmission media. In various implementations non volatile media includes optical or magnetic disks or solid state drives volatile media includes dynamic memory such as system memory component and transmission media includes coaxial cables copper wire and fiber optics including wires that include bus . In an embodiment the logic is encoded in non transitory computer readable medium. In an example transmission media may take the form of acoustic or light waves such as those generated during radio wave optical and infrared data communications.

Some common forms of computer readable media include for example floppy disk flexible disk hard disk magnetic tape any other magnetic medium CD ROM any other optical medium punch cards paper tape any other physical medium with patterns of holes RAM PROM EEPROM FLASH EEPROM any other memory chip or cartridge or any other medium from which a computer is adapted to read. In various embodiments of the present disclosure execution of instruction sequences to practice the present disclosure may be performed by computer system . In various other embodiments of the present disclosure a plurality of computer systems coupled by communication link to the network e.g. such as a LAN WLAN PTSN and or various other wired or wireless networks including telecommunications mobile and cellular phone networks may perform instruction sequences to practice the present disclosure in coordination with one another.

Where applicable various embodiments provided by the present disclosure may be implemented using hardware software or combinations of hardware and software. Also where applicable the various hardware components and or software components set forth herein may be combined into composite components including software hardware and or both without departing from the spirit of the present disclosure. Where applicable the various hardware components and or software components set forth herein may be separated into sub components including software hardware or both without departing from the spirit of the present disclosure. In addition where applicable it is contemplated that software components may be implemented as hardware components and vice versa.

Application software in accordance with the present disclosure may be stored on one or more computer readable mediums. It is also contemplated that the application software identified herein may be implemented using one or more general purpose or specific purpose computers and or computer systems networked and or otherwise. Where applicable the ordering of various blocks described herein may be changed combined into composite blocks and or separated into sub blocks to provide features described herein.

The foregoing disclosure is not intended to limit the present disclosure to the precise forms or particular fields of use disclosed. As such it is contemplated that various alternate embodiments and or modifications to the present disclosure whether explicitly described or implied herein are possible in light of the disclosure. Changes may be made in form and detail without departing from the scope of the present disclosure. Thus the present disclosure is limited only by the claims.

