---

title: System and method for implementing traffic optimization for overlay networks
abstract: A proxy apparatus includes a processor and a memory storing instructions executed by the processor to determine whether a received packet has a corresponding application proxy and, if so, apply application proxy processing optimizations to the packet plus overlay network optimizations to the packet. Wherein the application proxy processing optimizations include header reduction for header fields that remain static from transmission to transmission.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09455950&OS=09455950&RS=09455950
owner: Blue Coat Systems, Inc.
number: 09455950
owner_city: Sunnyvale
owner_country: US
publication_date: 20140314
---
This application claims priority to U.S. Provisional Patent Application 61 800 347 filed Mar. 15 2013 the contents of which are incorporated herein by reference.

The present invention relates generally to traffic optimization for overlay networks and in particular for such optimization as may be used in IPv6 over IPv4 overlay networks such as those built on top of the Intra Site Automatic Tunnel Address Protocol ISATAP .

Many enterprises have started transitioning towards Internet Protocol version 6 IPv6 for their networks. As this migration to IPv6 takes place however many enterprises are operating and will operate both IPv4 and IPv6 networks at the same time often on the same infrastructure. These enterprises still demand access to application acceleration and protocol optimization techniques that have been deployed for their IPv4 equipment and now require that these optimization techniques be available over IPv6 over IPv4 overlay networks such as those built on top of the Intra Site Automatic Tunnel Address Protocol ISATAP .

A proxy apparatus includes a processor and a memory storing instructions executed by the processor to determine whether a received packet has a corresponding application proxy and if so apply application proxy processing optimizations to the packet plus overlay network optimizations to the packet. Wherein the application proxy processing optimizations include header reduction for header fields that remain static from transmission to transmission.

Example embodiments of the present invention are discussed below with reference to the various figures. However those skilled in the art will readily appreciate that the detailed description given herein with respect to these figures is for explanatory purposes as the invention extends beyond these embodiments. Described herein are systems and methods for carrying out application acceleration and protocol optimization techniques over overlay networks. Much of the following discussion will focus on IPv6 over IPv4 overlay networks such as ISATAP overlay networks but this is simply for convenience in order to simplify the discussion. Thus readers should recognize that the solutions provided herein apply to many other types of overlay networks and tunnels such as generic routing encapsulation GRE IP in IP and other such overlay networks .

ISATAP facilitates the automatic creation of tunnels that can carry IPv6 traffic over IPv4 routing infrastructures. Such tunnels can be created between a host and a router or between two hosts. An ISATAP overlay network is symmetrical because a pair of ISATAP capable devices one at each end of the tunnel is necessary for traffic encapsulation. With ISATAP an IPv4 address of a tunnel endpoint device is embedded into the last 32 bits of an IPv6 address. This address combined with a designated prefix e.g. 00 00 5E FE forms a 64 bit interface identifier of an IPv6 address.

Problems associated with this type of overlay network include packet encapsulation or tunnel overhead the potential for packet fragmentation and inefficient bandwidth utilization due to the transmission of protocol packets that have minimal packet content variation. The present invention addresses these issues and more generally accomplishes application acceleration and optimization in the presence of packet encapsulation. Since common proxies already deal with application protocols that run over the transmission control protocol TCP or the user datagram protocol UDP but cannot accommodate overlay networks that utilize packet encapsulation we propose herein a generic proxy that processes and accelerates non TCP and non UDP traffic such as ICMPv6 on the ISATAP overlay network. The generic proxy may also be utilized to accelerate TCP or UDP flows that are not intercepted thus bypassed by the application proxies.

Packet header compression is a well known optimization process. Our approach for dealing with headers in overlay networks utilizes a similar principle in that for header fields that remain static from transmission to transmission we provide a high compression ratio. Unlike other schemes that focus mainly on protocol headers however our approach performs dictionary based compression on the overall packet. For TCP and UDP traffic that has an associated application proxy the header compression takes place first followed by application and protocol optimization through the proxy and optionally dictionary compression on the proxy processed payload to obtain additional bandwidth savings. Taking the overall packet into account for compression is necessary because there are multiple layers of packet headers including both IPv4 and IPv6.

Our general solution architecture is symmetrical and is shown in . Between a pair of communicating end nodes two optimization tunnels are established across a wide area network WAN . The first optimization tunnel call it a proxy ISATAP tunnel is used to carry traffic that would require processing by an application proxy in addition to processing by the ISATAP overlay optimization module. The second optimization tunnel call it a pure ISATAP tunnel is used to carry non TCP non UDP traffic and bypassed TCP and UDP traffic and is only processed by the ISATAP overlay optimization module.

The level of optimization performed in or for each tunnel is different. In the proxy ISATAP tunnel protocols such as HyperText Transfer Protocol HTTP Common Internet File System CIFS or Message Application Programming Interface MAPI are optimized through the actions of one or more associated application proxies. When appropriate these application proxies may also employ object caching. In the pure ISATAP tunnel a generic proxy is constructed to intercept packets in order to perform header reduction and overall packet compression based optimization functions. Each pair of optimization tunnels is created for a pair of communicating nodes based on the IPv4 addresses of the encapsulation layer as described in detail below.

It should be appreciated that proxy is associated with a first network while proxy is associated with a second network. The optimization tunnel created by the generic proxy is a TCP connection. L2 L3 traffic is passed into this pipe for optimization and L2 L3 traffic is reproduced on the other end of the pipe. Consequently the first network and the second network are treated as one big virtual network. Any traffic that is generated is optimized across the WAN . The L2 L3 protocols were designed to operate over a LAN not a WAN. Without this disclosed feature not only the WAN bandwidth utilization would be saturated with a high cost but the performance demanded by the L2 L3 protocol operations would not be met. For any L4 to L7 traffic if there is no specific proxy built for the traffic then the traffic is optimized by the generic proxy. Thus in accordance with an embodiment of the invention protocol optimization is based upon a specially built proxy or a generic proxy. In other words an embodiment of the invention detects traffic or application type and then applies a proper optimization system.

The communication between a client and an optimizer appliance and the functions performed at the optimizer are illustrated through an example shown in . In this example client issues an HTTP request on the ISATAP overlay network. The request includes a TCP SYN packet and reaches the first optimizer that is deployed inline. Once the optimizer receives the TCP SYN packet it uses an internal traffic classification engine to identify the traffic as potential HTTP traffic verifies an associated HTTP proxy is present and then terminates the TCP connection as if the optimizer were the content server . This process is sometimes known as server spoofing. The client then issues an HTTP request on this intercepted TCP connection.

The intercepted TCP connection is made up of a series of packets such as packet which includes a TCP header and payload . In the case of IPv6 inIPv4 there will also be respective protocol headers and . The optimizer receives the packets recognizes them as TCP6 in TCP4 e.g. by examining the different layers of protocol headers and as a first step in the optimization process removes the IPv4 overlay header . The optimizer then passes the remaining portions of the original packet to an HTTP proxy . The HTTP proxy first checks its object cache to determine if a local copy of the requested content is present and if so returns that cached content to the client see . If the local cache does not have the requested content then the HTTP proxy forwards the request to an ISATAP overlay module. The ISATAP overlay optimizer performs dictionary based compression on the packet and transmits a fully optimized packet over the optimization channel to its peer optimizer on the other end of the WAN. Further TCP traffic is processed in a similar fashion on a connection by connection basis.

The above assumes that the traffic received at the optimizer is TCP or UDP traffic with an associated application proxy an HTTP proxy in the above example . In the case of non TCP or non UDP traffic or for TCP UDP traffic with no dedicated application proxy a different processing regime one which includes a generic proxy rather than a dedicated application proxy is used. In such instances when a packet is received a check is made to determine if an existing pure ISATAP tunnel for the traffic exists. This may be done for example based on the IPv4 source and destination addresses included in the IPv4 header. If no existing tunnel is present a new pure ISATAP tunnel is created as soon as the generic proxy detects the new ISATAP tunnel endpoints. Subsequently received packets will be communicated via this newly fashioned optimization channel on a packet by packet basis. All data communication that takes place between the same IPv4 pair of ISATAP tunnel end points is transferred using the same optimization channel.

Returning to the example of an HTTP request issued by a client once the peer optimizer receives the fully optimized packet it decompresses and reproduces the original packet as shown in . This includes reconstructing the ISTAP overlay header so that the resulting packet is identical to the packet that was originally generated by the client. At this point the peer optimizer issues the HTTP request to the content server as if it were the client. This process is sometimes known as client spoofing omits the TCP exchange phase to simplify discussion .

The server responds to the HTTP request and returns the content that is encapsulated inside an ISATAP overlay . The peer optimizer then has to perform similar optimization steps as those performed on the client request including overlay header removal handoff to the HTTP proxy and overall packet optimization and transmits the optimized response frames back to the client side optimizer . As can be seen in the client side optimizer including the HTTP proxy then performs similar functions as those performed by the server side optimizer which also includes the HTTP proxy . The HTTP proxy as described here is sometimes known as a split proxy and is described in detail in commonly owned U.S. Patent Publication 2007 0245090 incorporated herein by reference. For both the proxy ISATAP tunnel and the pure ISATAP tunnel the IPv6 headers that include any IPv6 extension headers that are present in the original packet and the payload are preserved.

As shown in when the client side optimizer receives the optimized packet over the optimization channel it hands the packet off to the HTTP proxy and then reconstructs the ISATAP overlay header before returning the HTTP response to the client . These operations ensure that the returned packet is identical to the packet that was originally generated by the content server. One additional function performed by the optimizer is to cache the retrieved content if possible so that future requests for the same content can be fetched from the local cache instead of reissuing the same request over the WAN.

For a pure ISATAP tunnel since the data is processed on a per packet basis the server to client flow is identical to the client to server flow. For example an ICMPv6 reply will be processed in the same way as a ICMPv6 request with the exception that it is now intercepted on the server side optimizer instead of the client side optimizer. The client side and server side optimizers may share the optimization channel information and reuse the channel if necessary.

As indicated above in the case of non TCP or non UDP traffic or for TCP UDP traffic that does not have a dedicated application proxy at the optimizer a generic proxy is used. The generic proxy and a companion ADN work in a split proxy model which means that for a given traffic flow there is a component of the generic proxy and ADN on each of a pair of optimizer appliances on respective sides of a WAN as shown in . Referring briefly to each generic proxy maintains a table of tunnels connection table indexed by source and destination addresses from the IPv4 header . These tunnels are only conceptual and identify a flow between IPv4 endpoints based on a unique source and destination address combination. The actual tunnel over the WAN is achieved using ADN tunnels . So these generic proxy tunnels have a one to one mapping to ADN tunnels.

When an IPv4 encapsulated packet is received the generic proxy performs a table lookup to find an existing tunnel. If the lookup fails it means that this is a new flow and hence a tunnel has to be formed for it. The generic proxy invokes the ADN to create a tunnel for this new flow. A successful lookup indicates that there is an existing ADN tunnel that services this source and destination IPv4 address.

The ADN is informed of the IP encapsulation header and optimization policy byte cache compression etc. during set up of the tunnel. In order to setup the ADN tunnel to find its peer ADN at the server side optimization appliance known as the concentrator the ADN performs an ADN route lookup on the destination address in the IP encapsulation header. On a successful route lookup the ADN exchanges the IP encapsulation header with the concentrator during a tunnel setup handshake. This setup need only be performed once. Once the tunnel has been setup the ADN treats this as any other tunnel and only the data is exchanged e.g. after byte caching and compression .

As part of the tunnel setup the ADN on the concentrator also informs hands off to the generic proxy of the new tunnel thus letting it populate its table of tunnels with the new source and destination IP tuple. By doing so the generic proxy on the ADN concentrator is able to match the reverse packets to the existing ADN tunnel.

The generic proxy and the ADN tunnel exchange data via a socket pair that is created during the ADN tunnel setup. Thus each unique generic proxy tunnel and ADN tunnel combination has a socket pair between them one on the client side optimization appliance the branch and one on the concentrator. Only the encapsulated data including the IPv6 header for an IPv6 in IPv4 packet is exchanged over the socket pair. The ADN treats this data read from the socket pair as any other data like a read from a traditional proxy protocol performing byte caching and compression if the configuration policy so dictates of the whole data including the IPv6 header for an IPv6 in IPv4 packet . The ADN maintains the message boundary for data received from each read from its end of the socket pair. This message boundary is maintained as this data is transferred across the tunnel and delivered to the corresponding generic proxy tunnel on the peer optimization device on the other side of the WAN link. The generic proxy tunnel maintains the message boundary for the data it reads from its client server side socket and the write onto its end of the socket pair and also for the data it reads from the socket pair and writes to its client server socket. The protocol stacks at the optimizer appliances ensure this message boundary is maintained when data is delivered across the ADN. In one embodiment the generic proxy may time out a flow within a certain predetermined time interval e.g. two minutes if no activity is observed and then proceeds to clean up the associated tunnel records.

The ADN s behavior to support IP encapsulated packets that are serviced by a proxy on the optimization appliance is similar to what it does for the generic proxy. This scenario does not require a split proxy model unless the proxy service itself demands it. The IP encapsulation header is exchanged between the proxy and ADN and a bit flag is set. When ADN observes this bit flag it uses the destination address in the IP encapsulation header to perform a route lookup. This is in lieu of using a destination address specified by the protocol proxy which continues to act as though it is connecting to the encapsulated IPv6 address in case of IPv6 in IPv4 deployments .

When the ADN finds a concentrator that fronts the IPv4 address in the encapsulation header it forms an explicit tunnel to it exchanging the IP encapsulation header information during a tunnel connection handshake. Subsequent data on that tunnel need not transfer the IP encapsulation header once the tunnel is established. When the concentrator connects to the content server it stores the header information for the connection and recreates the encapsulated packet for all subsequence packets on the flow. Thereafter the ADNs may treat this connection as any other from the protocol proxy. Even in case of proxies that operate in a split proxy mode and it is a protocol proxy on the concentrator that connects to the content server the connection to the content server is first made by the concentrator ADN and only when the connection is successful is the connection handed off to the split proxy s protocol proxy. This way the split proxy protocol proxy does not need to separately accommodate the IP encapsulation.

To evaluate the performance of our solution we measured header reduction and dictionary based compression for several test scenarios. In this analysis we focused on the HTTP and CIFS protocols because they are the ones most often encountered with applications deployed over a WAN in an enterprise environment.

The test setup consisted of two hosts two peer optimization appliances and one WAN simulator to simulate a T1 link. Table 1 lists values for various link parameters. The topology is identical to the deployment scenario illustrated in . The test hosts were IPv4 and IPv6 dual stack capable and each was configured with an ISATAP interface.

Eleven files randomly selected with sizes distributed between 3.5 MB 51 MB were used in the test runs. The basic test sets and data collection are described in Table 2.

Throughput degradation td was used to measure the performance gain from header reduction. It is calculated as the difference of effective throughput et between IPv4 and ISATAP. Effective throughput is calculated as file size fs divided by transferring time tt . IPv4ISATAP IPv4

Effective throughput is used to measure the performance gain from dictionary based compression. As the performance of dictionary based compression is not related to the protocol headers we measured the performance on ISATAP.

Our experiments determined that for HTTP transfers the throughput degradation introduced by ISATAP is about 5 without any optimization. After performing header reduction over our WAN optimizer the throughput degradation was completely eliminated.

Using CIFS to transfer the test files the throughput degradation introduced by ISATAP is approximately 3.5 without any optimization. After performing header reduction over our WAN optimizer the throughput degradation was reduced to 0.5 .

We determined that effective throughput for both HTTP and CIFS increased significantly with the use of our optimization techniques because of the high compression ratio achieved by the dictionary compression. The test results indicated tremendous bandwidth savings when the same traffic was transferred and optimized by the dictionary based compression.

The example computer system includes a processor e.g. a central processing unit CPU a graphics processing unit GPU or both a main memory and a static memory which communicate with each other via a bus . The computer system may further include a video display unit e.g. a liquid crystal display LCD . The computer system may also include an alphanumeric input device e.g. a keyboard a user interface UI controller e.g. a mouse hard buttons etc. a disk drive unit a signal generation device e.g. a speaker and one or more network interface devices e.g. a WiFi and or other transceiver . The disk drive unit includes a machine readable medium on which is stored one or more sets of instructions and data structures e.g. software embodying or used by any one or more of the methodologies or functions illustrated herein. The software may also reside completely or at least partially within the main memory and or within the processor during execution thereof by the computer system with the main memory and the processor also constituting machine readable media. As used herein the term machine readable medium should be taken to include a single medium or multiple media e.g. a centralized or distributed database and or associated caches and servers that store the one or more sets of instructions. The term machine readable medium should also be taken to include any tangible medium that is capable of storing encoding or carrying a set of instructions for execution by the machine and that cause the machine to perform any of the one or more of the methodologies illustrated herein. The term machine readable medium shall accordingly be taken to include but not be limited to solid state memories and optical and magnetic medium.

The embodiments described herein may generally be implemented as hardware and or software logic embodied in a tangible i.e. non transitory medium that when executed is operable to perform the various methods and processes described above. That is logic may be embodied as physical arrangements modules or components. A tangible medium may be substantially any computer readable medium that is capable of storing logic or computer program code which may be executed e.g. by a processor or an overall computing system to perform methods and functions associated with the embodiments. Such computer readable mediums may include but are not limited to including physical storage and or memory devices. Executable logic may include but is not limited to including code devices computer program code and or executable computer commands or instructions.

The steps associated with the methods of described above may vary in different embodiments of the invention. Therefore the present examples are to be considered as illustrative and not restrictive and the examples is not to be limited to the details given herein but may be modified within the scope of the appended claims.

