---

title: Systems and methods for providing synchronized content
abstract: Systems and methods for providing synchronized content are disclosed. The synchronized content may be content related to visual content captured by an imager of a mobile computing device. The mobile computer device may send the captured visual content to a content synchronization server that may identify the captured visual content, find relevant content associated with the captured visual content, synchronize the relevant content with the captured visual content, and send the synchronized relevant content to the client device.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09628837&OS=09628837&RS=09628837
owner: AudioStreamTV Inc.
number: 09628837
owner_city: New York
owner_country: US
publication_date: 20140807
---
This application claims the benefit of U.S. Provisional Patent Application No. 61 863 238 filed Aug. 7 2013 the disclosure of which is incorporated by reference herein in its entirety.

The advance of mobile computing has resulted in unprecedented levels of access to information. A significant number of people use mobile computing devices on a daily basis to communicate with others navigate the physical world conduct business and access information as well as to provide entertainment and personal assistance. While mobile computing devices tend to be deliberately compact to promote portability the relatively small display sizes on these devices may diminish the user experience in a number of ways. For instance it is notoriously more difficult to provide user input to mobile computing devices using tiny keypads and touch sensitive displays than it is to provide user input to a desktop computing device using a full sized keyboard and mouse.

Small displays can also diminish the viewing experience for a user who wishes to use the mobile computing device to watch content such as television movies and video clips for example. On the other hand large display devices in public places enable a rich visual experience but are often muted to avoid disturbing those who are not interested in listening to the displayed content. Such displays may also be muted if there are several displays in the same location providing different content. Large displays in public spaces may use closed captioning or subtitling to give users a visual rendering that represents a transcription verbatim or edited of the audio portion of content being shown on the display. Closed captions however are visually distracting and fail to provide the user with a fully satisfying experience as rich auditory features such as music and voice inflection cannot be satisfactorily rendered via text.

Systems and methods are disclosed for providing synchronized content that may combine the advantages of mobile computing devices and relatively larger digital displays. The disclosed embodiments may facilitate recognition of visual content being presented on a digital display and provision of synchronized audio or other content related to the visual content to a mobile computing device. The mobile computing device referred to herein as a client device may therefore enable users to listen to audio that matches and is synchronized with video content presented on a digital display such as a television a digital sight or other media display even when that display is muted or its audio signal is inaudible to the user. Additional content related to the video content such as live blogs social media posts and advertisements for example may additionally or alternatively be provided to the client device.

Other aspects and advantages of the invention will become apparent from the following drawings detailed description and claims all of which illustrate the principles of the invention by way of example only.

The various embodiments disclosed herein may facilitate the provision of content synchronized with a content source to an Internet connected client device. For example the a content synchronization server may identify video content screened on a digital display physically close to the client device e.g. a TV or outdoor digital sign and stream back the relevant matching synchronized audio. To capture the video content an application can be installed on the client device which may include an imager capable of recording video. The content synchronization server can receive the captured video content from the client device and potentially matching video content and the corresponding audio content from a third party content source.

Using the client device the user can record a short video off the nearby digital display. The video may then be sent manually or automatically to the content synchronization server. The server side can analyze the video sent by the client side and will match it with the video content it receives as input from the third party content source. The results of this matching may include identifying the video that was screened on the digital display and recorded by the user and the determining the timing of that video relative to the video content from the third party content source. The content synchronization server can then stream the synchronized audio and or any other synchronized content to the client device enabling the user to hear the audio and or receive the other relevant content.

Third party display can be any display device capable of presenting visual content to a user. Thus third party display may be for example a television a computer monitor a mobile electronic device an electronic or nonelectronic sign. The visual content presented by third party display may therefore range from video signals e.g. live or prerecorded television signals movies video or clips to static or semi static digital or analog images e.g. storefront advertisements billboards or works of art .

The visual content presented by third party display may be captured by client device . Client device may be any electronic device capable of capturing visual content such as an audio player a video player a music recorder a game player a video recorder a camera a radio a cellular telephone or other wireless communication device a personal digital assistant a programmable remote control a pager a laptop computer a desktop computer or combinations thereof. In some cases client device may be an electronic device such as a smartphone for example that can perform multiple functions e.g. play music display video store pictures and receive and transmit telephone calls .

Client device can include among other components processor s memory communications module and imager . Components and may all be part of client device or alternatively individual components may be connected to client device in any suitable manner. For example imager may be a removable image capturing device that can be coupled to client device wirelessly or using a cable not shown .

Processor s may be connected to the other components of client device e.g. via a communications bus to control and operate electronic device . In some embodiments processor s may execute instructions stored in memory which may include one or more different types of memory such as one or more of several caches flash memory RAM ROM and or hybrid types of memory. Processor s may include for example one or more a microcontrollers and or microprocessors that can execute instructions from one or more software or firmware applications stored in memory . Processor s may also control one or more input output I O modules of client device . Examples of I O modules may include for example a digital display an audio output device such as an audio out jack or a speaker a touch screen interface and or one or more peripheral devices such as a keyboard or mouse.

Imager may include any device or devices capable of capturing visual content from third party display and formatting the digital content in a format suitable for transmission to content synchronization server . Thus imager may include one or more digital image sensors e.g. CCD or CMOS sensors that can capture visual content as still images or videos. The still images or videos may then be sent to content synchronization server using communications module . Client device may also send relevant information about client device such as its physical location and or orientation and details about imager the device type and or the user for example.

In some embodiments software or firmware may be installed on or otherwise be executable by client device may enhance the ability of imager to capture visual content that may be easily recognized by content synchronization server . For example client device may provide automatic control of zoom and or focus control for imager in order to capture a clear and crisp version of the video content use a high dynamic range in order to improved contrast of the video content captured and or apply image stabilization in order to ensure consistent results for successive captured frames. Client device may also perform one or more pre processing operations on the captured video or images such as changing brightness levels or colors cropping or extracting key features for example. Pre processing the captured visual content may significantly reduce the computing resources required for content synchronization server to identify the visual content.

Communications module may include circuitry that enables client device to be communicatively coupled to another device e.g. a computer or an accessory device . Communications module may allow client device to connect to a communications network using any suitable communications protocol. For example communications module may create a short range communications network using a short range communications protocol to connect to other devices or systems located close to client device . For example communications module may be operative to create or connect to a local communications network using the Bluetooth protocol to couple with a Bluetooth headset. Communications module may also include a wired or wireless network interface card NIC configured to connect to the Internet or any other public or private network. For example client device may be configured to connect to the Internet via a wireless network such as a packet radio network an RF network a cellular network or any other suitable type of network.

As described in detail below content synchronization server may receive visual content e.g. in the form of still images or videos from client device identify the visual content find relevant content associated with the visual content such as an audio signal associated with the visual content synchronize the related content and send the related content to client device . Client device may receive the related content via communications module and present it to the user via one or more of the I O modules available on or connected to client device .

Thus in some exemplary embodiments imager may record visual content being displayed by a television in the vicinity of client device . The television may be muted too far away for the user to hear well playing audio in a language unknown to the user or otherwise providing to the user a less than optimal auditory experience. Upon capturing the visual content client device can send the visual client to content synchronization server which may identify the visual content being displayed on the television e.g. as being broadcast by a particular television channel find an audio signal associated with the visual content synchronize the audio signal with the visual content and send the audio signal to client device . Client device may then play the audio signal via an audio output device of client device such as a speaker audio output jack for example.

Content I O module may generally speaking be responsible for interfacing with client device e.g. receiving captured visual content from and sending synchronized content to the client device receiving audio visual content from a third party content source calculating a lag between the captured visual content and the third party content source extracting frames from the captured visual content and the content from the third party content source and transmitting the extracted frames to content detector module . Content I O module may also receive a matching decision from content detector module receive content matching the matched content from the third party content source synchronize the matched content with the captured visual content and send the synchronized matched content to client device .

Captured visual content from client device may first be received at gateway server of content I O module . Gateway server may use an Application Programming Interface API such as REST API for example to implement the transfer of the captured visual client from client device to content synchronization server . Along with the captured visual content gateway server may also receive various other information from client device such as geolocation one or more timestamps image properties camera state and or user identifying information for example. In some embodiments gateway server may also include clock synchronization API which may be an API that receives information about a local clock running on client device and compares that clock with a local clock of content synchronization server . Clock synchronization API may continuously or periodically compare the local clocks to ensure that content synchronization server always has the most current version of the local clock of client device .

Gateway server may route the data received from client device to various other components of content synchronization server . In some embodiments data other than the captured video data and the local clock data may be stored in database . Content detector module may consult the data stored in database to improve the content matching process. In some embodiments database may also store captured visual content and or local clock data received from client device .

The captured visual content may be sent from gateway server to channel synchronization server . Channel synchronization server may store the captured visual content in a format suitable for processing by content detector module . For example while the captured visual content may be stored in the same format e.g. as a set of still images or as a video with a particular frame rate received from client device the captured visual content may be converted to a different frame rate or a different file format to be compatible with content detector module .

Channel synchronization server may also receive content from a third party content source . Third party content source may be any content source likely to be able to deliver relevant content which may be according to various embodiments the same content similar content or supplemental content to the captured visual content received from client device . Accordingly third party content source may be a cable interface card that receives a cable television signal or a content streaming server capable of providing a wide variety of content on demand for example. In some embodiments channel synchronization server may identify the type of captured visual content received from client device e.g. as visual content captured from a television signal and choose an appropriate third party content source from a number of available third party content sources.

A V extractor may receive content from third party content source and extract video and audio from the content source. Extraction may occur continuously or upon request such as when captured visual content is received at channel synchronization server for example. One benefit of continuously extracting content from third party content source may be that if the captured visual content is delayed with respect to live content received from third party content source the captured visual content may have to be compared with content from the past to find matching content from third party content source . Such past data may not be available however if A V extractor only begins to extract content upon captured visual content reaching channel synchronization server . Thus if third party content source is a cable interface card content from each available channel may be extracted on a continuous basis. On the other hand if third party content source is an on demand content source A V extractor may wait to extract content until captured visual content reaches channel synchronization server .

A V extractor may extract video from third party content source by sampling the video signal at a suitable sampling rate and storing the sampled frames in channel synchronization server . In some embodiments the sampling rate may match the frame rate of the video signal e.g. 25 frames per second such that each frame is sampled and stored in channel synchronization server . In other embodiments A V extractor may sample the video signal at a lower sampling rate in order to reduce the amount of data to be stored in channel synchronization server . The number of frames sampled per second may also be chosen as a function of an acceptable error in audio synchronization and or the number of frames per second in the captured visual content received from client device . Accordingly the sampling rate may be chosen to match the frame rate of the captured visual content and or the sampling rate may be chosen to be as sparse as possible while ensuring that the time delay between the captured visual content and the extracted video is within a maximum acceptable offset between the captured visual content and an audio signal to be synchronized with the captured visual content.

In the case of a live video signal the first to last frames stored from a particular channel in channel synchronization server may cover a defined time period. For example the defined time period may cover at least the maximum known delay for a television broadcast. Accordingly if broadcasts of a television signal are known to differ by up to 30 s e.g. among different cable providers nationwide or worldwide 30 s of sampled video frames for each channel may be stored in channel synchronization server . Stored frames that correspond temporally with captured visual content received at channel synchronization server may be stored until content detector module returns a content matching result.

Video data typically has a frame rate upwards of 25 frames per second which may mean that much of the video data stored in channel synchronization server may be redundant. For computational efficiency key frame detector may be used to reduce the number of frames to be processed and compared during the content matching process. Generally speaking key frame detector can divide the frames stored in channel synchronization server into shots where each shot is a sequence of continuous frames that describe a single action. A key frame may be chosen that is representative of the frames in that shot such as the first frame of a new shot for example. Key frames may be chosen from a sequence of video frames with any suitable level of granularity but ideally each key frame should represent its shot while nontrivially reducing the number of frames to be processed. Because A V extractor may continuously extract video from third party content source key frame detector may detect key frames on an ongoing basis.

Key frame detector may use one or more known methods and or one or more of the methods described below to detect key frames. In accordance with various embodiments key frame detector may detect a new key frame when a new key frame is suggested by one or more of the following methods.

In some embodiments key frames may be detected using a histogram correlation process. The process may begin by calculating a histogram representation of the color distribution of the pixels in a first frame with each bin in the histogram representing a particular color or a range of colors. In other embodiments the frame may first be converted to gray scale and then a histogram of the image may be generated where the bins e.g. 256 bins of the histogram represent the distribution of the pixels in the image over the gray scale. In still other embodiments the frame may be converted to a 2 bit e.g. black and white image and key frame detector may generate a 2 bin histogram representing the pixels in the 2 bit image. In a like manner key frame detector can generate a histograms for each frame received from channel synchronization server .

Once the histograms are generated key frame detector may compare the histogram generated for a given frame with those generated for one or more of its neighboring frames e.g. the frame immediately following the given frame . Comparing the histograms may involve calculating a correlation coefficient between the histograms with the expectation that frames that are part of the same shot will be highly correlated while frames that are part of different shots will be lowly correlated or uncorrelated. The correlation coefficient calculated during the comparing may be compared to a threshold value and key frame detector may determine that if the correlation coefficient is less than the threshold value the neighboring frame is a key frame e.g. the first frame of the next shot in the video .

In some embodiments rather than the threshold being a fixed value the correlation coefficient may be compared to a threshold value that adapts to an average correlation value of the last N e.g. 5 10 15 or 30 frames. In this manner key frame detector can avoid detecting excess key frames even for single shots with relatively low correlation between successive frames. In further embodiments key frame detector can calculate the correlation coefficient between the histogram generated for the current frame with the histogram generated for the previous key frame. If the correlation coefficient is below a threshold value which may be a lower threshold than that used to compare neighboring frames for example the current frame may be designated as a key frame.

Key frame detector may additionally or alternatively detect key frames on the basis of analysis of a frame s keypoints. As used herein a keypoint may denote a notable feature of an object in an image that may be extracted and used to identify the object in another image despite changes in scale translation rotation illumination and distortion. Keypoints may typically be found in high contrast portions of an image such as edges.

For each frame key frame detector can extract local key points and extract local feature descriptors using a known keypoint algorithm such as Kaze Features Speeded Up Robust Features SURF or Scale Invariant Feature Transform SIFT for example. Each frame may then be compared to the previous frame or previous N frames using a suitable method such as Random Sample Consensus RANSAC algorithm which can determine the correspondence between keypoints in the frames being compared. Key frame detector may analyze the correspondence data to determine whether a change has occurred between the two frames. For example if a keypoint has moved by a threshold number of pixels e.g. 50 between the two frames or if a keypoint does not exist in both frames being compared key frame detector can declare that the keypoint has changed. If the percentage of changed keypoints exceeds a certain threshold value e.g. 30 key frame detector can consider the new frame a key frame.

Additionally key frame detector may recognize semantic features located in each frame and use the changes in the recognized features to determine whether or not to designate a particular frame as a key frame. The semantic features may include but are not limited to a face detector a commercial logo detector a text detector a car detector a sky detector a scene classifier and the number of segments in an image after it is segmented by a segmentation algorithm. Details regarding the recognition and extraction of semantic features are discussed in more detail below.

Key frame detector can group the output of each detector into a feature vector and store the feature vector for each frame. The feature vectors for each consecutive set of frames or the last group of N frames may then be compared. If the number of changes between the feature vectors e.g. the Hamming distance between the feature vectors is larger than a threshold value e.g. 3 key frame detector may consider the new frame a key frame.

The key frames designated by key frame detector and the frames from the captured visual content may be sent to content detector module which can match the captured visual content to content available via third party content source . Reference is also made to which shows a detailed schematic diagram of content detector module in accordance with some embodiments.

Captured visual content frames may be sent from content I O module to digital display detector of content detector module . Digital display detector can be used to detect a digital display within the captured visual content frames and extract the digital display from the frames. Extracting the digital display from the captured visual content frames may enable content detector module to more easily compare the captured visual content with the frames received from key frame detector and thereby identify the captured visual content. Digital display detector may use one or more known methods and or one or more of the methods described below to detect the digital display. In accordance with various embodiments digital display detector may detect a digital display in a frame when the coordinates of the digital display in the frame are suggested by one or more of the methods.

The Hough Transform has been widely used in computer vision applications to identify geometric shapes. Because digital displays of the kind under consideration typically have distinct quadrilateral shapes which can vary depending on the viewing angle the Hough Transform may be used to identify a digital display in a video frame. However if the frame contains multiple quadrilateral objects or when a part of the digital display in the frame is occluded applying the Hough Transform alone may not consistently detect the digital display in the frame.

In order to confidently identify the digital display in a given frame the Hough Transform may be combined with a salient object detection algorithm which can serve to find the most interesting object in an image namely in this case the instance of a digital display in an image. Using salient object detection an input image i.e. a frame of captured video content can be assumed to contain a background and an object which may be anywhere in the image and may have an arbitrary shape. The goal of salient objection detection can be to separate the object from the background. Combining the Hough Transform with salient object detection can help to overcome limitations attendant to using approach separately.

According to some embodiments digital display detector may include rectangular object detector salient object detector weak contours filter and rectangle similarity threshold module . Rectangular object detector and salient object detector may both output one or more bounding boxes potentially detecting the digital display in the frame. Digital display detector can output a binary i.e. yes no output that indicates whether or not a digital display has been identified based at least on the number of contours detected in the saliency map described in detail below and whether the outputs of the two detectors match one another within a threshold.

Reference is now made to which shows a flowchart of an exemplary process for detecting a digital display using salient object detection. Process may begin at step in which a digital display detector e.g. digital display detector of can receive a set of captured visual content frames. The captured visual content frames may be frames of a video captured using a client device e.g. client device of from a digital display e.g. third party display of . The number of frames received at step may depend upon a number of factors including the length and frame rate of the visual content assuming the captured visual content is a video and whether or not the visual content was edited e.g. shortened or compressed by removing some frames .

At step the digital display detector may detect salient features in each of the frames. Any known method may be used to detect salient features in a given frame or salient features may be detected using the methods disclosed below. The results of a salient feature detection process for a given frame may be combined with the results for each other frame received in step in order to detect salient objects that exist in all or most of the frames. At step frames may be combined on the basis of a bitwise or operation that may effectively integrate the output of a salient feature detection process on a pixel by pixel basis.

In some implementations digital display detector can generate a histogram representation of the color distribution of the pixels in a first frame with each bin in the histogram representing a particular color a range of colors or a range of grayscale values. In the case where the bins in the generated histogram do not faithfully represent the actual color of the pixel the histogram may be considered a modified histogram. Each bin in the modified histogram may be assigned a color probability as a function of the likelihood that the bin represents a portion of a salient object. The modified histogram may then be back projected on the original image and each pixel can be assigned with a probability based on the corresponding histogram bin in the image. Pixels with probabilities exceeding a threshold may be considered as part of a salient object while pixels with probabilities less than the threshold may be ignored.

Detecting features may additionally or alternatively involve calculating the saliency of each pixel in a frame by comparing its color contrast to all other pixels in the image. For example the saliency of a particular pixel I may be calculated as 1 where D I I is the color distance metric between Iand Iin the L a b space. In general the higher the saliency calculated in Eq. 1 the higher the probability that the pixel is part of a salient object. In some embodiments before determining the saliency of each pixel in a frame the colors in the frame may be quantized into bins where each bin represents a continuous range of colors in the real color e.g. RGB color space to reduce the computational demands of the saliency calculation.

Still further recognizing that the salient feature to be recognized in process is a digital display the digital display detector may analyze brightness levels of pixels in each frame. Accordingly the digital display detector may combine the luminance channel from all received frames together using a bitwise or operation. Salient features may then be detected upon determining that a set of pixels defining an object was consistently brighter than other pixels in the frames.

Still further yet digital display detector may rely on background detection to detect a potential digital display in the captured visual content frames. Background detection may rely on the fact that the pixels representing the digital display will change at a much higher rate than pixels representing the background. Accordingly pixels that undergo a high rate of change in comparison to other pixels in the frames may be considered as being part of a potential digital display. Recognizing that the pixels corresponding to the digital display may shift from one frame to the next due to the imager moving or shaking for example the pixels may be remapped so that they correspond to the same location in each frame.

At step the detected salient features may be combined into a saliency map using mean shift segmentation. After the saliency map is generated an edge detection algorithm e.g. Sobel Canny or Prewitt may be applied to the saliency map at step . Next the detected edges may be compared to a threshold to eliminate weak edges at step and contours may be extracted at step .

At step the extracted contours may be filtered using assumed facts about the shape of the digital display. For instance because displays usually have well defined aspect rations such as 4 3 or 16 9 for example contours with dimensions that depart from those ratios by a threshold amount may be ignored although because the aspect ratio may change as a function of the angle between the camera and the display contours with aspect ratios close to those typically associated with digital displays may be maintained through the filtering step. Furthermore contours that enclose an area smaller than a defined threshold may also be discarded during the filtering process.

At step the digital display detector can extract one or more contours located close to the center of the saliency map. Because a user attempting to capture video from a digital display is likely to place the digital display in the center of the video capturing window filtering out salient objects that are located away from the center of the saliency map may facilitate location of the digital display over other detected salient features.

Reference is now made to which shows a flowchart of an exemplary process for detecting a digital display using a Hough Transform algorithm in accordance with some embodiments. Process may begin at step in which a digital display detector e.g. digital display detector of can receive a set of captured visual content frames. Step may correspond to step of for example.

At step the digital display detector can preprocess each frame in order to enhance the image and increase its contrast. Increasing the contrast may be accomplished by linearly multiplying each pixel s value. Increasing the contrast in this manner may facilitate locating a digital display in the frames because digital display frames are typically black and increasing the contrast in the frames can facilitate differentiation of the digital display s frame from its contents and from the background. At step the digital display detector can apply an edge detection algorithm such as the Sobel Canny or Prewitt edge detection schemes for example to each frame to generate an edge map.

At step the digital display detector may apply the Hough Transform to find and mathematically define lines in the edge map. Once the lines are defined the digital display detector may identify the corners in each frame at step . Identifying the corners may involve finding the intersections of all of the lines defined by the Hough Transform. At step the identified corners may be filtered such that groups of identified corners that are close to one another e.g. within a radius of 50 pixels may be de duplicated. In some embodiments all but one corner of a group of closely spaced corners may be filtered out in step .

At step the digital display detector may analyze each group of three corners to find a minimum area bounding rectangle for each group. Inspecting every three corners allows one corner of the digital display to be missing e.g. due to occlusions or bad lighting conditions making for a more robust analysis than would be possible if four corners were required. These rectangles may then be filtered if they do not match a set of defined parameters. The parameters may be defined to filter out rectangles that are not likely to represent a digital display. Accordingly the digital display detector may filter out rectangles that are rotated more than a defined angle from a reference line parallel to the upper and lower bounds of the frame or rectangles that do not have an aspect ratio within a certain range.

At step the digital display detector may select a single rectangle based on brightness level. The brightness level for each rectangle may be calculated by analyzing the luminance value for each pixel in each frame and combined e.g. integrated summed or averaged over all frames in the set of captured video content frames. The brightest rectangle may then be chosen as the digital display.

As noted above outputs of process may be compared with outputs of process to increase that probability of finding the digital display in a set of captured visual content frames. In particular the digital display detector may indicate that it has found a digital display in the set of frames upon determining that 1 there are only a few e.g. 5 or fewer contours in the saliency map and or 2 the results of the two detectors are similar within a threshold. It should be understood however that the digital display detector may rely on either process or process or any process known in the art to detect a digital display in a set of video frames.

Attention is now returned to which depicts the extracted key frames and the captured visual content frames being received at feature extractor . Information regarding the location of the digital display within the captured visual content frames may also be received from digital display detector in order to ensure that features extracted from these frames are from the digital display and not from its surrounding environment. Feature extractor may generally identify one or more features from each of the extracted key frames and at least one of the captured visual content frames. Extracted key frames that are missing the features extracted from the captured visual content frame may be discarded as representing an unlikely match to the captured visual content. Discarding as many key frames as possible using feature extraction may significantly lessen the burden on content synchronization server during the more computationally intensive matching process which will be described in detail below.

The feature extraction algorithms disclosed below may be particularly well suited for the task of culling large numbers of extracted key frames because of their ability to filter large number of frames with relative computational efficiency and with a very low false negative rate. That is feature extraction may facilitate eliminating key frames that are very different from one or more reference captured video content frames based on a detected unsimilarity between the key frame and the reference frame the number of which can amount to a significant portion of the total number of key frames received at feature extractor .

Feature extractor may detect any suitable feature types from among the captured frames and the key frames including a TV channel logo text faces and dominant colors for example the detection of which are described in detail below. It should be understood that other types of features such as the existence of a corporate logo in a frame for example may be recognized in a similar manner and the feature types disclosed herein should not be understood as a complete list of extractable features.

Key frames may be eliminated under a number of circumstances which may be chosen to reduce the false negative rate. For instance if a captured visual content frame contains either a TV channel logo text or a face but a key frame does not the key frame may be eliminated. Similarly a key frame may be eliminated if the captured visual content frame contains a dominant color not present in the key frame. On the other hand the absence of a feature in a captured visual content frame might not be used to eliminate key frames as it is more likely that features be present but undetected in the captured visual content frames than in the key frames extracted from third party content source .

In some embodiments one particular frame may be chosen from the captured visual content frames to serve as a reference frame. The reference frame may be chosen in order to eliminate the highest possible number of unlikely frames from among the set of key frames. Thus the reference frame may be chosen based on the existence of a large number of extracted features e.g. a frame having a face a TV logo text and a particularly distinct dominant color such that key frames lack even one of the extracted features may be discarded.

Feature extractor may first attempt to detect and extract a logo such as a TV channel logo from the reference frame and each of the key frames using logo detector . While any suitable process may be used to detect and extract a logo the following method may be particularly advantageous because its ability to detect a logo that is not statically placed in a video as well as one that is viewed from many different angles and under varied lighting conditions.

First logo detector may build a database of logos such as a database of TV channel logos with one logo for each channel e.g. CNN ESPN etc. . The database may include affine transformations of each logo generated to supply views of each logo from various angles and with various width height scaling. Next logo detector can train a Histogram of Gradients detector to extract the contours of the logos in the database.

Turning to the extracted key frames logo detector may filter out pixels not belonging to the three most dominant colors in a target candidate logo i.e. one of the logos in the database of logos . Filtering the key frames by color in this manner may help to narrow the search area for finding a logo in the key frame. In particular logo detector may narrow the search area to a polygon bounding an area that contains all of the three most dominant colors in the target logo. Once the search area is so narrowed the Histogram of Gradients detector can extract the contours of the features in that area to search for a match from among the logos in the database of logos using a sliding window for example.

If there is a match for a specific logo the output for that specific logo detector may be designated a 1 otherwise it can be designated a 0. In this manner logo detector can give a binary result for the existence of a particular logo in each key frame.

Furthermore logo detector can assign a confidence value to the result based on the location of the logo in the key frame relative to the location of the logo in the reference frame. In general the confidence value may be higher if the logo detected in the key frame is in the same location as the logo from the reference frame. If more than one logo is detected in a key frame feature extractor can ignore those logos with a low confidence value.

Text detector may determine whether there is at least one text segment in the reference frame and if a text segment exists eliminate any key frames that do not include a text segment. In order to locate a segment of text in a natural image as opposed to a scan of a printed page an operator such as the Stroke Width Transform may be applied which may be capable of detecting text segments of various scales orientations and fonts. The Stroke Width Transform may compute the pixel width of likely text strokes i.e. contiguous parts of an image that form bands of nearly constant width .

Components may then be identified that contain a number of strokes. Each component may be considered a potential portion of text e.g. a letter number or other glyph . Components with particularly large variance in their stroke s widths may be rejected as can components with aspect ratios outside a defined range e.g. 0.1 10 . Remaining components may be considered text candidates which may be combined to detect words and other combinations of text.

As with logo detector text detector can assign a confidence value to the text segments in each key frame relative to the location of the text segment in the reference frame. If text is present in the reference frame but absent in the key frame the key frame may be eliminated.

Many different algorithms exist for locating a face in an image such as the Viola Jones face detection algorithm for example. Face detector may use any one of the known facial detection algorithms to determine whether the reference frame includes a face. If there is at least one face in the reference frame face detector may run the facial detection algorithm on the key frames. Any key frame without a face may be eliminated. A confidence value may be assigned to each key frame based on the location of the face in each key frame relative to the location of the face segment in the reference frame. In other embodiments if face detector detects more than one face in the reference frame key frames with less than the number of detected faces may be eliminated. For example if face detector detects five faces in the reference frame any key frame with less than five detected faces may be discarded.

Feature extractor may also use the existence of a dominant color in the reference frame to form the basis for eliminating a number of key frames using histogram module for example. A particular frame may be considered to have a dominant color if the largest peak in the frame s color histogram is separated from the next highest peak by a threshold value. It may be advantageous to generate the a histogram for the reference frame and each key frame with a relatively small number of bins e.g. 32 bins rather than 256 to minimize the importance of any shifts in color tones due to the angle at which the reference frame was captured and the lighting conditions under which the reference frame was captured.

Each key frame may then be compared against the reference frame to determine if the key frame shares the reference frame s dominant color. For example the bin corresponding to the reference frame s dominant color may be examined for each key frame. If that bin has a value e.g. a percentage of pixels above a defined threshold that frame may be kept. In some embodiments key frames may also be kept if one of the neighboring bins exceeds the threshold or if the combination of the corresponding bin and its neighboring bins exceeds the threshold. Key frames not meeting these criteria may be discarded.

Results from feature extractor may be passed on to channel eliminator which may eliminate a particular set of key frames that have very low likelihood of matching the captured visual content. For example a particular set of key frames may correspond to a TV channel extracted by A V extractor from third party content source or from a particular on demand content item. Channel eliminator may therefore determine whether any key frames remain in each key frame set passed to feature extractor . By thus filtering out sets of key frames irrelevant channels or on demand content items may be ignored during the following matching process.

The key frames from any channels or on demand content items remaining after processing by channel eliminator may be sent to matcher . The goal of matcher may be to determine which channel or on demand content item was scanned by client device . In order to assist matcher in this process prioritization engine and calibration engine may analyze data received from client device and or other data stored in database to make the matching process more efficient.

Prioritization engine may generate a list of channels or on demand content items that are most likely to match the visual content captured by client device . To generate this list prioritization engine may analyze a host of data passed from client device to content synchronization server and or other data stored in database .

According to some embodiments prioritization engine may receive data regarding the physical location of client device e.g. GPS data WiFi data or cellular data . Database may then be consulted to determine whether any other users scanned visual content at that location either concurrently or at some time in the past. For example if prioritization engine determines that client device captured visual content from a digital display at a particular sports bar in New York City database may be consulted to determine if any other users ever attempted to capture visual content from that location. If prioritization engine finds such a match the channel channels or on demand content detected from that location can be prioritized when attempting to find a match for the visual content detected by client device .

The degree to which a particular channel or on demand content item is prioritized may depend on a number of factors such as how close in time hits found in database are to the time that the visual content was captured by client device and the number of known digital displays at the location for example. Thus if the visual content captured by client device was captured only moments after another user captured visual content at the same location the channel or on demand content item detected for the previous user may be prioritized more highly than a channel or on demand content item captured at that location a week earlier. Further data regarding other channels or on demand content identified at a particular location may be given less priority if it is known that that location has ten digital displays that typically show different content.

In some embodiments prioritization engine may consult a list of the user s favorite channels or on demand content items and prioritize those channels for analysis by matcher . The list may be generated locally on client device and or based on the user s history interacting with content synchronization server . The list may be stored locally on client device and or in database .

In still other embodiments the user of client device may manually suggest a channel or on demand content item for prioritization. Thus if the user knows or believes to know the source of the content being displayed on a digital display in the user s vicinity she may provide that information to prioritization engine e.g. using a user interface may be provided on client device . Thus if the user knows or believes that she is watching a football game on ESPN she may provide that information to prioritization engine which can then suggest that matcher attempt to match that channel before any other channels. Similarly if the user believes that she is watching a particular movie from an on demand content source she may provide that information to prioritization engine in order to have matcher attempt to match that on demand content item before any others.

Prioritization engine may then provide a list of suggested channels or on demand content item to matcher such that matcher can attempt to match the captured visual content with the most likely potential matches available via third party content source . The list provided by prioritization engine may be provided in order of the engine s confidence in the suggested results.

Calibration engine may analyze the same data or similar data as prioritization engine and or the list of suggested channels or on demand content items generated by prioritization engine in order to reduce a confidence threshold required for matcher to declare a match between the captured visual content and a potential match. Accordingly based on the confidence level ascribed to a particular channel or on demand content item calibration engine can reduce the confidence level required before matcher declares a match. In this manner calibration engine can reduce the rate of misdetections by matcher .

Matcher can receive several inputs from other modules of content synchronization server in order to match captured visual content with content extracted from third party content source . Channel eliminator may provide matcher with the key frames from channels and on demand content items not eliminated during the channel elimination process. Digital display detector may provide matcher with the captured visual content frames and information regarding the location of the digital display in the frames. Still further prioritization engine may send matcher suggestions for which channels or on demand content items to attempt to match first and calibration engine can allow matcher a wider confidence range for declaring a match for certain channels or on demand content items.

Matcher may include several sub modules including prioritization module image processing module histogram equalization module interest point extractor affine transformation estimator comparator and matching decision maker . These modules may work with one another to edit and compare incoming captured visual content frames and key frames in order to determine whether the content source for a particular set of key frames matches the captured visual content. Upon determining that a match exists or that no match exists between the captured visual content frames and a set of key frames matching decision maker of matcher may generate a matching decision.

Attention is now turned to which shows a flowchart of an exemplary process for matching content in accordance with various embodiments. Process which may be implemented by a module and or submodules of a content synchronization server e.g. matcher and or submodules of content synchronization server may begin at step in which a series of captured visual input frames may be cropped and resized in an image processing module e.g. image processing module . The captured visual input frames may be frames captured using a client device e.g. client device as described above. The matcher may receive information in the form of coordinates for example regarding the location of a digital display in each frame e.g. as determined by digital display detector . The image processor may then crop the captured visual input frames in order to extract the portion of the frames containing the digital display and rotate and resize the extracted portions to match to the extent possible the size and proportions of the key frames received from a content I O module e.g. content I O module of the content synchronization server.

At step histogram equalization module can use histogram equalization to increase the contrast of the captured visual input frames. Histogram equalization may be understood as an image processing technique that can enhance contrast in an image by adjusting pixel intensities. Histogram equalization may facilitate extraction of interest points from the captured visual input frames and the key frames at step . In some embodiments histogram equalization may also be performed on the key frames.

At step an interest point extractor can extract interest points from the captured visual input frames and the key frames. Extracting interest points in step may be similar to the process of extracting key points described above. Thus interest point extractor can extract interest points using one or more known keypoint algorithms such as Kaze Features SURF or SIFT for example. In some embodiments an optical flow algorithm may be used to map the key frame to the captured input frame based on the interest points extracted in step .

At step affine transformation estimator can determine an optimal affine transformation to map the interest points in the key frame to the interest points in the captured visual content frame employing a matching algorithm such as RANSAC.

At step affine transformation estimator may warp the key frame using the affine transformation calculated in step and transform the warped key frame to grayscale. The captured input frame may also be converted to grayscale at step .

At step a mask may be applied to the key frame and captured visual content frame in order to ignore certain areas of the images. For example areas of the images that are smooth and lie between strong edges may be ignored as can pixels with zero value. Applying such a mask may improve the comparison between the captured visual content frame and the key frame by ignoring areas of the image that are especially susceptible to distortion.

At step frame differentiation module can compare the key frame the captured visual content frame. In some embodiments the absolute value of the difference between the two images may be taken. The square root of the L1 norm of the difference may then be calculated to determine a normalized difference between the two images.

At step the normalized difference may be compared to a threshold value using comparator and the results may be passed to matching decision maker . At step matching decision maker can take the output from comparator for all or a subset of captured visual content frame key frame comparisons to determine which channel or on demand content item was most likely the one scanned by the user. In order to make the determination matching decision maker may calculate a confidence level metric for each channel or on demand content item. The confidence level metric may be determined on the basis of how closely the set of key frames for the channel or on demand content item matched the set of captured visual input frames as measured by the normalized difference. The confidence level metric may then be compared to a minimum confidence level set by calibration engine . If the confidence level metric for a particular channel or on demand content item exceeds the minimum confidence level set by calibration engine matching decision maker can indicate that the channel or on demand content item is a match for the captured visual content.

Once matching decision maker indicates that a match has been found for the captured visual content that is available via third party content source content synchronization server may begin the process of providing synchronized content related to the captured visual content to client device .

In some embodiments the related content may be the audio stream associated with the captured visual content and the matched content available via third party content source . However it should be understood that any type of related content may be provided to client device including content related to the captured visual content such as bonus material e.g. behind the scenes content exclusive interviews quizzes images music videos etc. social media feeds discussing the captured visual content user interfaces for voting on some aspect of the captured visual content advertisements and so on. Because an audio stream corresponding to visual content may require the closest synchronization of all potential related content however the related content synchronization process will be disclosed assuming the related content is the audio stream corresponding to the captured visual content.

At step clock synchronization API can attach its own timestamp to the packet and return the packet to client device . Steps and may be repeated N times e.g. 20 50 100 or 1000 times . At step the clock difference between client device and content synchronization server may be determined as an average of the clock differences over the N times steps and were repeated.

Subsequently or simultaneously process may proceed to determine the lag between the captured visual content and the content received from third party content source . Thus at step channel synchronization server can store frames extracted from third party content source by A V extractor . The frames represent the matched content as determined by matching decision maker . Each of these frames may be associated with a timestamp consistent with the local clock of content synchronization server

At step channel synchronization server can store captured visual content frames received from client device . Each of these frames may be associated with a timestamp consistent with the local clock of client device .

At step channel synchronization server can obtain the closest frame to frame match between the captured visual content frames and the frames extracted by A V extractor and compare the timestamps thereof to calculate the lag between frames received from client device and the frames received from A V extractor .

In some embodiments channel synchronization server may combine the results of two capture frames to more accurately calculate the lag between the captured visual content frames and the frames extracted by A V extractor . In these embodiments channel synchronization server may take two captured visual content frames c0 and c1 with timestamps indicating that they are seconds apart. The best matches of the frames extracted by A V extractor are then found for the two captured visual content frames s c0 and s c1 respectively. Two combine match frames may then be determined from among the frames extracted by A V extractor as follows 0 0 0 1 0 2 2 1 1 1 0 1 2 3 Picking the best matched frame from among S c0 and S c1 the channel synchronization server can return either t0 or t1 for choosing the proper frame from which to determine the lag result.

At step channel synchronization server can calculate the playback delay as the difference between the calculated lag and the calculated clock difference. The playback delay may then be used to initiate the streaming an audio stream component of the content extracted by A V extractor to client device . Because it is possible that the local clocks of client device and content synchronization server can begin to drift apart over time from the originally calculated clock difference process may be repeated periodically to ensure that the relevant content received at client device remains synchronized. Once the relevant content is synchronized it may be streamed to client device using stream server . In some embodiments client device may be equipped with a user interface that permits manual adjustment of audio synchronization. The user interface may be in the form of one or more buttons or a slider for example.

Transmission of audio data over a wireless e.g. WiFi cellular or other non point to point data connection may be subject to sources of interference that can lead to data loss and reduction of audio quality. If the audio is played in real time i.e. as soon as the audio data arrives the interference can lead to audio going out of sync or audio artifacts such as clicks for example being audible. In highly synchronized applications with low tolerances for latency such as synchronizing audio to a video running in real time on a digital display for example interference may make it very difficult to keep the audio playing in time with the video. Two separate methods are disclosed to solve these problems.

The second method uses silence extension to compensate for lost audio. In this method the client device can analyze the audio data in buffer B to detect data fragments of silence. When such fragments are detected they may be extended to fill the buffer up to the expected size.

At step the client device may preprocess the captured visual content. Preprocessing the captured visual content may involve changing brightness levels or colors cropping or extracting key features for example. Pre processing the captured visual content may significantly reduce the computing resources required for a content synchronization server e.g. content synchronization server of to identify the visual content.

At step the client device may send the captured visual content to the content synchronization server. The content may be sent as a series of frames in the event that the captured visual content is a video. In some embodiments the client device may send additional information such as the client device s geolocation one or more relevant timestamps image properties camera state and or user identifying information for example.

At step the client device may receive synchronized relevant content from the content synchronization server. The synchronized relevant content received may depend for example on the type of visual content captured and or on the relevant content available on a third party content source communicatively coupled to other components of the content synchronization server. In some embodiments the synchronized relevant content may be an audio stream corresponding to video captured from a nearby digital display. In other embodiments the synchronized relevant content may include behind the scenes content exclusive interviews quizzes images music videos social media feeds discussing the captured visual content user interfaces for voting on some aspect of the captured visual content and or advertisements for example.

At step the synchronized relevant content may be presented to the user. How the synchronized relevant content is presented may depend on the type of content received at step . For instance if the synchronized relevant content is an audio stream corresponding to a video captured from a nearby digital display the audio stream may be output using an audio output device of the client device such as a speaker or an audio jack for example. If the synchronized relevant content is additional visual content however that content may be presented on a display coupled to the client device such as a connected monitor or a touchscreen display for example.

At step the content synchronization server can receive one or more sets of extracted content frames from a third party content source e.g. third party content source of . The third party content source may be a cable interface card capable of receiving a cable television signal or a database of on demand content for example. Frames of the content from the third party content source may be extracted by an A V extractor e.g. A V extractor of the content synchronization system.

At step the content synchronization server can match the captured visual content frames to frames extracted from the third party content source. The matching process may involve a number of steps including detecting key frames from the extracted frames from the third party content source detecting the location of a digital display in the captured visual content frames extracting features from all frames eliminating channels or on demand content items from consideration and matching the remaining key frames to the captured visual content frames. The matching process may also include receiving and considering input from a prioritization engine e.g. prioritization engine of which may prioritize certain channels or on demand content items for matching over others based on data received from the client device and or from a database of the content synchronization server e.g. database of . Still further the matching process may include receiving and considering input from a calibration engine e.g. calibration engine of which may adjust certain thresholds that can define a match between the captured visual content and content from the third party content source.

At step the content synchronization server can find content related to the captured visual content available on the third party content source. The related content may be an audio stream corresponding to the captured visual content which may be a separate component of the same content stream fed to the A V extractor. In other embodiments the related content may include behind the scenes content exclusive interviews quizzes images music videos social media feeds discussing the captured visual content user interfaces for voting on some aspect of the captured visual content and or advertisements for example. These types of related content may be associated with the channel or on demand content item identified in step such that upon receiving the indication the related content may be retrieved and sent to the client device. The related content may be stored in a database with fields that cross reference the channel or on demand content item as appropriate for example.

At step a channel synchronization server e.g. channel synchronization server of of the content synchronization server can synchronize the related content with the captured visual content. Synchronizing the related content may involve determining a difference in the local clocks between the client device and the content synchronization server determining a lag between the captured visual content and content received from the third party content source and subtracting those two values to determine how much to delay playback of the relevant content. In some embodiments the content synchronization server can periodically re synchronize the relevant content to ensure that the relevant content remains synchronized with the captured visual content.

At step a stream server e.g. stream sever of of the content synchronization server can send the synchronized relevant content to the client device. The stream server may stop sending the synchronized relevant content to the client device under any suitable circumstances such as when the user leaves the vicinity of the digital display e.g. as indicated by geolocation data received at the content synchronization server after a predetermined time period e.g. 30 min upon indication from the client device of a desire to stop receiving the synchronized relevant content and or at a natural stopping point for the synchronized relevant content e.g. the end of a television show .

Each functional component described above may be implemented as stand alone software components or as a single functional module. In some embodiments the components may set aside portions of a computer s random access memory to provide control logic that affects the interception scanning and presentation steps described above. In such an embodiment the program or programs may be written in any one of a number of high level languages such as FORTRAN PASCAL C C C Java Tel PERL or BASIC. Further the program can be written in a script macro or functionality embedded in commercially available software such as EXCEL or VISUAL BASIC.

Additionally the software may be implemented in an assembly language directed to a microprocessor resident on a computer. For example the software can be implemented in Intel 80x86 assembly language if it is configured to run on an IBM PC or PC clone. The software may be embedded on an article of manufacture including but not limited to computer readable program means such as a floppy disk a hard disk an optical disk a magnetic tape a PROM an EPROM or CD ROM.

The invention can be embodied in other specific forms without departing from the spirit or essential characteristics thereof. The foregoing embodiments are therefore to be considered in all respects illustrative rather than limiting on the invention described herein.

