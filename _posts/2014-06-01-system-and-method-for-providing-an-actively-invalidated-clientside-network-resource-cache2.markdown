---

title: System and method for providing an actively invalidated client-side network resource cache
abstract: A system and method for providing an actively invalidated client-side network resource cache are disclosed. A particular embodiment includes: a client configured to request, for a client application, data associated with an identifier from a server; the server configured to provide the data associated with the identifier and to establish a queue associated with the identifier at a scalable message queuing system, the client being configured to subscribe to the queue at the scalable message queuing system to receive invalidation information associated with the data; the server being further configured to signal the queue of an invalidation event associated with the data; the scalable message queuing system being configured to convey information indicative of the invalidation event to the client; and the client being further configured to re-request the data associated with the identifier from the server upon receipt of the information indicative of the invalidation event.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09578081&OS=09578081&RS=09578081
owner: IMVU, Inc.
number: 09578081
owner_city: Redwood City
owner_country: US
publication_date: 20140601
---
This is a continuation in part patent application of co pending U.S. patent application Ser. No. 13 019 505 filed Feb. 2 2011 by the same assignee as the present application. This present patent application draws priority from the referenced patent application. The entire disclosure of the referenced patent application is considered part of the disclosure of the present application and is hereby incorporated by reference herein in its entirety.

This application relates to a system and method for use with networked entities according to one embodiment and more specifically to a system and method for providing an actively invalidated client side network resource cache.

A portion of the disclosure of this patent document contains material that is subject to copyright protection. The copyright owner has no objection to the facsimile reproduction of the patent document or the patent disclosure as it appears in the Patent and Trademark Office patent files or records but otherwise reserves all copyright rights whatsoever. The following notice applies to the software and data as described below and in the drawings that form a part of this document Copyright 2009 2014 IMVU Corporation All Rights Reserved.

The use of chat or instant messaging communications in a networked computer environment is well known. The America Online Instant Messaging system AIM is one well known system for controlling instant messaging in a networked environment. In these prior art systems two computer users can communicate non persistent text messages in real time using an instant message IM client on their computers in concert with an IM server.

Most messaging services are subscription based or user identity based and may generate large numbers of content followers or users of particular message or content sources denoted herein as subscribers . These content followers or subscribers can form communities or social networks around a particular content source or content distribution system. Social networks have gained in popularity as people have used messaging as a basis for connecting with each other.

As the numbers and size of the user pool subscribers and social networks expand it becomes more difficult to track and manage the subscribers the listening users and the degree to which the users are involved with the message content. Similarly it becomes more difficult to identify and rank the most popular content items being consumed across a variety of content sources and social networks.

In current practice content is typically delivered from web servers to web clients using HTTP Hypertext Transfer protocol . The delivered content stored on a permanent medium of the web server can be transferred via a network to the web client through a sequence of caches. Data caching at multiple layers is necessary to reduce the network resources needed to transfer the content from the web server to the web client. The layered data caches work well for static content. However current multi layer data caching systems are not efficient when the data content is subject to frequent changes or updates.

In the following description for purposes of explanation numerous specific details are set forth in order to provide a thorough understanding of the various embodiments. It will be evident however to one of ordinary skill in the art that the various embodiments may be practiced without these specific details.

The Message Queue system of a particular embodiment is a stand alone software stack which implements a scalable efficient secure flexible and easily understood message and state update switching and routing fabric. The Message Queue system is a robust scalable message queue system to which clients connect that routes messages from the site to clients. The Message Queue system can also be considered a light weight message queue system and state management protocol optimized for real time interactive multi player systems. illustrates the components of the Message Queue system of an example embodiment. In an example embodiment the core Message Queue system comprises three main roles Client Gateway s Message Queue Node s and Queue Management . These are loosely referred to herein as Gateway Queue Node and Supervisor or Boss respectively. A node is generally any of the machines running as part of the Message Queue system. Note that a node as used herein is not the same as a queue node. A queue node may be one of many possible types of nodes.

Topologically the Client Gateways of various embodiments are the interface to the users at client systems . illustrates the components of the Gateways of an example embodiment. User requests come in to a server at a particular domain name. The received user requests are load balanced by load balancers across gateways . There is no requirement that successive sessions for each user go to the same gateway as there can only be one simultaneous session Transmission Control Protocol TCP connection per user at a time. Sessions are typically long lived. For example one session can last for the entire time a user is logged into the client. In a particular embodiment sessions can use a custom protocol based on Google Protocol Buffers over TCP.

Gateways can perform a muxing demuxing function to vector user requests onto a pool of message queue nodes where actual message queues are processed. Clients can get subscribed to inspect e.g. obtain access to the messages coming across specific named message queues. Additionally clients can get subscribed to send messages to specific named message queues. illustrate the components of the message queues of an example embodiment. In a particular embodiment each message queue node is responsible for a deterministic subset of message queues. The mapping of a queue name to a message queue node is performed by a queue map or node map which maps a queue name onto buckets using a hashing process e.g. hash queue name . illustrates the consistent hash processing of an example embodiment. Each bucket is served by a given queue node. In a particular embodiment a given queue node generally serves between eight and sixteen buckets with buckets being fairly distributed across nodes. It will be apparent to those of ordinary skill in the art that a particular embodiment can use a greater or lesser number of buckets per queue node.

A supervisor is the authoritative source of the node map as well as the central point where system wide statistics are aggregated for easy monitoring. However the supervisor is not involved in any real time message flow and thus the entire system can serve in a fully operational state even if the supervisor node is temporarily down. However the node map cannot be re configured without participation by the supervisor.

In a particular embodiment all machines run the same code source tree but the command line to start up each node determines what kind of node it is. Each node will register itself with the supervisor thus the supervisor is a convenient place to determine which nodes are effectively part of the Message Queue system . In a particular embodiment each node can be a typical 8 core 8 GB RAM compute only not database server. In a particular system one supervisor node boss is provided. If this supervisor node goes down instant replacement is not necessary but the ability to monitor the system will be degraded. In a particular embodiment between five and ten client gateway nodes can be provided. A load balancer can be provided to spread client requests across the available client gateway nodes. In a particular system between five and ten message queue nodes can be provided. These message queue nodes or queue nodes can mostly message between themselves. In a typical system internal traffic will be fairly low. In a typical embodiment the entire queue will likely use less than a gigabit of bandwidth in aggregate at full load until features and user counts swell beyond this point. A system level metric for network packets and bandwidth in out can also be provided.

The message queue nodes can also do some messaging into web machines mainly for authorization purposes. In a particular embodiment this traffic uses Hypertext Transport Protocol HTTP with JavaScript Object Notation JSON . The message queue nodes can either use an existing web pool or create a new pool specifically for the Message Queue system as described herein. In general a pool of servers is a set of servers that respond to a specific Domain Name Server DNS address using front end load balancing in the terminology used herein. The server can get to the set of gateways to send messages to users in a load balanced way. In a particular embodiment this traffic can use HTTP with JSON. In a particular embodiment the software can be written on top of Erlang OTP R13B04 which can be downloaded and built from source at http www.erlang.org 

Each node has a Uniform Resource Locator URL that can output statistics in Nagios compatible key value text format. Thus application level monitoring scripts can be written by simply hitting that URL on each node. Node software can be set up to start in etc init.d and can have the correct working directory and command line options specified.

The following sections describe a functionality level implementation of the Message Queue system processes of an example embodiment. The description includes an explanation of how the Message Queue system of an example embodiment is decomposed into software executables and processes. The Message Queue system of an example embodiment includes three kinds of processes 1 a plurality of Gateway processes 2 a plurality of Queue Node processes and 3 a singular Supervisor process. Additionally as shown in for a particular embodiment two more components can be provided a plurality of Translator processes and State Manager web services . These processes are described in more detail below. Initially however a summary of the terms used herein is provided.

The gateway processes maintain persistent connections with clients as well as with persistent server processes. Creating the connection establishes authentication which persists for the duration of the connection. Other than authentication and binary protocol decode the gateway process is relatively simple. The gateway process can map a queue name id to a target queue node and forward messages to the target queue node as well as return responses to the appropriate connected clients. illustrates the components of the gateways of an example embodiment. illustrates the message processing in an example embodiment. The gateway process can also create subscriptions between endpoints e.g. queues and clients . Gateway processes establish a routing fabric for the queue id namespace. For each subset of the namespace mapping to a queue node the gateway establishes one or more TCP connections over which the gateway routes all messages to queues on that node and receives messages from all queues on that node which have currently subscribed clients on the gateway. When a subscription to a queue is requested for a client the gateway adds the queue to a map of queues to which the gateway currently listens if the queue is not already in the map. Additionally the gateway adds a reference to the client from within that queue entry. That way the gateway can route messages received from queues on connected nodes to the appropriate set of subscribed clients. This mapping is undone when a client un subscribes from a queue or disconnects. When a client sends a message or state change request to a queue the gateway forwards the message to the appropriate queue. For any message that the gateway forwards the authenticated user that made the request is added onto the request. Thus a client would not must not include its own identity in any request instead the requester will be authoritatively identified in the message by the gateway process. That way the system is immune against impersonation attacks. The gateway also has a management interface that allows bulk subscription creation deletion and updates from trusted services on the web site. The gateway process may need a hook for sign off where the gateway process can call out to a web service to notify outside systems of the loss of a connected user. Alternatively the system could subscribe to a queue specifically for the given user and receive user left notification when the user disconnects. Gateways can register themselves as available when the process comes alive. Gateways can send statistics to the supervisor once per time period e.g. some number of minutes .

In a particular embodiment queue nodes operate message queues. illustrate the components of the message queues of an example embodiment. Each message queue has a list or set of currently subscribed listeners. These lists may be different for each message queue. Each message queue consists of a name and a set of subscribers. Within the queue message streams and state streams can be mounted. Additionally queues may have a configured state manager call out service as well as each mount within a queue. Queue nodes are largely passive where gateways create connections to the nodes and the nodes just react to incoming messages by routing and notifying interested gateways. When a queue node receives a user queue subscription request the requesting gateway is added to a list of outputs for the queue if not already present and the subscribing user is added as an entry in that gateway entry. This way a message only needs to be forwarded once to a gateway even if multiple users on that gateway are subscribing to the same queue. When a user un subscribes or a gateway disconnects e.g. because of failure this procedure is reversed. For queue migration described in more detail below the state of a queue must be fully serializable and transferable to another queue node. Additionally a queue node must be able to forward messages intended for a queue that has been transferred to the new queue node for the queue. Queue nodes can register themselves as available when the process comes alive. Queue nodes can send statistics to the supervisor once per time period e.g. some value of minute .

The supervisor process can manage all nodes in the Message Queue system. The supervisor process also manages the queue name to message queue node mapping table. In a particular embodiment this mapping is maintained as a traditional circle hash or alternatively as buckets mapped to nodes allowing a 1 to N re mapping of a new node added to a set of N existing nodes. Additionally the supervisor process collects statistics from the gateway nodes and queue nodes on performance and load of the system and provides an aggregate view of system performance through a management interface. The supervisor process is typically not visible from outside the cluster network but instead is fully accessed through web management services. The Supervisor can tell gateways about updates to the node mapping table.

In a particular embodiment there is only one supervisor process. However the supervisor process is not a single point of failure in that the configured network of gateways and queue nodes can continue operating even if the supervisor process is temporarily inactive. In this case the Message Queue system just won t be able to report statistics or re configure the node mapping table. The supervisor process can in a particular embodiment simply serve as collector of statistics. The main functionality lost from this simplification would be the ability to add capacity or redistribute load from downed nodes without interruption of existing client connections.

Older clients not updated to communicate with the Message Queue gateways directly or unable to do so because they are behind restrictive HTTP only proxy firewalls can continue to make XMLRPC calls to existing chat scripts. XMLRPC is a conventional remote procedure call RPC protocol which uses Extensible Mark up Language XML to encode its calls and HTTP as a transport mechanism. Those scripts may need to translate and forward messages to the Message Queue gateways. Because the Message Queue gateways use a persistent connection model and the XMLRPC web servers typically use a Representational State Transfer REST like polling model something needs to translate between the two models.

The translator process of a particular embodiment is a persistent stateful process. The translator process establishes connections to the message queue gateways one per client using the XMLRPC Application Programming Interface API . The translator process then receives JSON requests from XMLRPC and translates them to Message Queue system messages. Additionally the translator process buffers messages and state updates from Message Queue system queues and makes them available as JSON data to poll at the convenience of the XMLRPC system. If no messages have been polled for a pre determined time e.g. five minutes the client is assumed to have disconnected and the persistent connection is torn down.

A slight implementation variation would be to use XML instead of JSON for the translator inputs making the work for the existing scripts easier. A more substantial implementation variation would be to entirely emulate the behavior of chat scripts to significantly reduce or eliminate the need for chat web servers. Mapping from an XMLRPC chatweb server to a translator process can be done using a simple mod operation on the customer id cid . Alternatively the mapping can be done statically based on the chatweb instance or using any other consistent mapping function such as a circle hash or other consistent hashing mechanism.

A queue state stream that receives a request to introduce update or delete a specific state property value can call aside to a state manager service to enforce rules for state updates. The queue state stream can bundle up current state into a JSON formatted data block as well as the requested state change and post a request to a state manager web service. The state manager web service then returns one of three results a ok apply change as requested b denied make no change or c altered apply changes as returned by the state manager. State managers can run a stateless web service on a main web server. If load from state managers needs to be managed separately this can easily be shifted to a separate pool of web front ends as needed. The state manager in effect for a queue state stream is configured as a parameter when the queue is created. There can be any number of state managers in effect for a given queue. A given room object or service may use multiple queue state streams to implement different sets of state as needed. A state manager can listen to multiple mounted streams to make determinations based on multiple sets of state. For example a rule that requires information about per room avatar state who is king and information about per room state which node is the throne can be mounted to see both kinds of information to be able to enforce the rule that only kings can sit on thrones.

Gateway nodes can immediately start serving client requests when they have connected to the supervisor and received the queue namespace map. All gateway nodes need is to have the hardware load balancer start pointing clients at them. In a particular embodiment a design goal is for 20 000 users to connect to a single server 8 cores 24 GB s memory bus . For clients that drop and re establish TCP connections e.g. because of intermittent WiFi hotspots it is likely that a new gateway will be chosen for the new connection. To mitigate this subscription information for a client is kept in a state stream specific to the client and managed by the gateway. When the client re connects to the gateway the client will have re delivered all the subscriptions of which the client is currently partaking. This queue will be automatically removed after some amount of time out e.g. 60 seconds . If no client subscribes to the queue within the time out period the queue hard disconnects to not keep system resources allocated needlessly. To provide immediate sign off notification to buddies a controlled user initiated sign off will result in a message through the gateway and a state update in the buddy state channel before the gateway is disconnected.

Internally the Gateway of a particular embodiment is based on gen server specifically a TCP subclass for binary and a HTTP server subclass for HTTP interface . Each of the acceptors starts a new Erlang process for each incoming request. For long running connections binary protocol for clients this process builds up routing state specific to the user. Messages within the system are sent as Erlang tuples typically constructed as Erlang structs.

Additionally there is one node dispatcher process or mapped forwarder process per gateway node. This process translates a message queue name to a corresponding queue node and forwards outgoing packets to the queue node and receives packets from the queue node to dispatch to subscribing connected users. In a particular embodiment client connections come in typically through port . Each accepted connection spawns a client framing handler which decodes the protocol buffer protocol and turns requests into Erlang tuples structs . Most messages go on to the queue map which translates destination queue name to a given physical queue node and dispatches to the communication process for that queue node. There is at least one such process per node in each gateway node and used by gateway processes on that node. In a particular embodiment responses from queues to subscribers can bypass this node dispatcher process these responses can go straight to each subscribing gateway process. If the message node dispatcher process turns out to be a bottleneck process we can parallelize this process and round robin between N message queue node dispatcher processes when we configure connection handling processes. The point of making this a separate process is that it makes dynamic re mapping of the namespace much easier to implement.

The supervisor management interface is initially limited to pushing statistics to the supervisor. In a particular embodiment these statistics can include 

As part of the implementation of online queue node insertion movement removal as described in more detail below the management interface can also partake in the queue map update commit cycle. The HTTP interface accepts JSON requests and returns JSON data. The HTTP interface spawns one Erlang process per HTTP connection. In an alternative embodiment we can choose to move to a pool of request handlers. The interface translates JSON to Erlang structs and back again from response structs to JSON. We extend JSON to support xXX for hexadecimal and use that encoding for characters less than 32 or greater than 126. An alternative is to send the characters as is expected by Unicode style JSON or as uXXXX expected by ASCII JSON 

Message queue nodes cannot start serving queue requests until they have been assigned a namespace area from the queue node map which is managed by the supervisor. In a particular embodiment each queue is represented by a single Erlang process. This means that all mounted name spaces in the queue e.g. message streams and state streams denoted property bags are serialized within the queue. Call outs to web services are serialized as well for streams that require call out messages posted on message streams may not need call outs and thus will re order compared to state change requests needing external verification. Queues can also include in process filters or state managers loaded as plug ins into the Erlang process and run as part of the queue process itself again for serialization purposes .

In a particular embodiment the design goal is for up to 100 000 queues to live on a single server 8 GB 8 cores 24 GB s memory bus . This allows up to 80 kB of state per queue. If memory restrictions become a problem because of per queue overhead we can split onto more queue nodes rather than increase RAM in a single box so as to keep system wide latency low. In a particular embodiment the design goal is for a single message to flow through the system from incoming gateway via queue nodes out to listening clients within 100 milliseconds. Note that a server with 24 GB s memory bus and 8 GB of RAM would have to spend 350 milliseconds to work through an 8 GB working set.

Mapping of a named queue to a queue node is done through an MD5 hash of the queue name followed by a map of the top N bits e.g. 10 bits of the MD5 hash to a sequence of buckets allocated to node servers. MD5 hashing is well known to those of ordinary skill in the art. Initially many buckets will be allocated to each participating queue node. As more queue nodes come online buckets will be fairly removed from allocated nodes and added to the new node to retain stochastically balanced 1 N load mapping. The supervisor holds the master map of hash to node mappings. In a particular embodiment we can enforce a minimum number of buckets per node to even the load. For example if we allowed one node to be mapped by two buckets and one node to be mapped by three buckets the difference in load assuming otherwise homogenous load distribution would be 50 more on the second node than on the first node. In a particular embodiment a policy of a minimum number of buckets per node can be enforced e.g. eight buckets minimum per node . Once all nodes have eight buckets for example we can double the number of buckets per node without changing the mapped address space. In this manner each bucket turns into two new buckets for a total of 16 buckets per node. Then we can start fairly redistributing again without fear of load imbalance. The minimum number of buckets used per node determines the maximum load imbalance assuming otherwise homogenous loading no hot keys . For a minimum of eight buckets per node the worst case ratio is 8 9 or 12.5 more load on the second host. There is a cost in increasing the minimum number of buckets because the node map is hot data for the queue node mapper processes. Thus the node map should ideally fit well into an L1 cache.

Internally each message queue is an Erlang process. Additionally there is a receiving process for incoming messages from each gateway.

The message queue nodes receive requests from gateways. These requests go to the central input process for the physical node which in turn dispatches to the appropriate message queue process. Message queues are Erlang processes one per message queue. Within each message queue different named handlers are mounted. These implement specific behaviors such as message forwarding or state storage. Handlers take the form of Erlang modules referenced by name. The state storage handler additionally supports plug in state managers again written as Erlang modules. It will be apparent to those of ordinary skill in the art that modules of a type other than Erlang modules can be similarly used. One state manager handler plug in is the PHP call out handler. This means that plug ins must be allowed configuration data such as what URL to call out to in this case. Each queue contains the list of subscribed users sorted as a list of subscribed gateway nodes with a list of users per gateway entry allowing reference counting for the output nodes as well as generating presence information.

We design for hot code re load in most cases. This is illustrated as the loop function calling itself tail recursively explicitly through the module name. Certain data structure updates will require rolling restarts instead. Tests can be used to find most of these cases and we can detect them through exception counters similar to the web push monitor on staged deployment. The supervisor currently is the recipient of runtime metrics. In a particular embodiment these metrics can include 

The supervisor can also make the node receiver process partake in queue migration. Queue migration means that the node will direct message queues to serialize and move to a new host node after which messages intended for the moved message queues will be forwarded to the target node. This will continue until all gateways have committed the new message dispatch map at which time any knowledge about the migrated out queues can be purged. This process can be denoted a hot add node processing. illustrates the hot add node processing of an example embodiment. Queue migration can be requested one queue at a time which would allow the node to keep serving other requests in the meanwhile and queue only messages intended for the queue that is moving. When the moved queue reports success pended messages are forwarded and the next queue to move is started. If we have 100 000 queues and migrate one queue per millisecond and migrate 1 10th of all queues this process would take 10 seconds so being able to serve requests while migrating is important for system responsiveness.

The message queue is responsible for the re sending of missed messages if a client disconnects and re connects. To support this the message queue can number each outgoing message generated by mounts. When a client connects the client can provide a serial number for the last received message. If this serial number matches a message that s still remembered by the queue or the serial number prior to the oldest remembered message then messages after that will be re delivered and the client will be assumed to be up to date. If the serial number is 0 or if the serial number falls before the remembered range of messages then the connection will be treated as new and each mount will be called upon to deliver new state which for message streams does nothing and for state streams delivers a snapshot of all the state.

The supervisor will be addressed using a system wide registered name e.g. supervisor in the Erlang runtime. The supervisor is started as supervisor using special command line arguments. Message queue nodes and gateway nodes that come online will register themselves with the supervisor using the type described by command line parameters to the node s executing process. The supervisor will aggregate statistics from the different nodes and provide a comprehensive management overview of metrics within the system. When inserting a new message queue node into the system the supervisor will first tell all queue nodes about the new map and have them forward queue state as well as incoming traffic for the target queues to the new node. Then the supervisor will distribute the new map to all gateways so that gateways will know to send incoming traffic to the appropriate new node. Finally all nodes will be told that the new node has committed and the old nodes can remove any state related to the now moved message queues.

The client is updated to connect to the Message Queue system e.g. by a Domain Name Server DNS name for chat message based communications. XMLRPC calls such as checkForMessages are re vectored to be driven by traffic from the Message Queue gateway. There needs to be only a single connection between the gateway and the client. A user is identified to the gateway through a hash signed cookie containing an expiry time a user id and a hash signature. This cookie is issued by the web system when the client first logs in until and unless login happens entirely through the gateway . To avoid cookie theft there is a three way handshake where the gateway issues a cryptographically random challenge to the client and the client signs this with the user s password and returns to the gateway. The gateway then verifies that the signature correlates with the signature obtained through signing the challenge locally. This requires the user s password to be held server side. To counter this in one embodiment the user signs the challenge with a hash of the password and a password is stored server side which means that the hash of the password is the new password but avoids plaintext password leakage should we have a system intrusion event.

The system of various embodiments is designed to avoid user impersonation attacks. The system is also designed to mitigate identity theft attacks and to reduce the cost of authentication checking to the set up handshake phase. As long as services use the established identity e.g. customer id for any source dependent operations the system will be secure. Mal formed services that pass plaintext ids to services cannot be guarded against at this level but instead have to be mitigated by proper API design and separate service auditing.

All creation subscription and un subscription to queues happen on the server side as a side effect of some XMLRPC or other API call. For instance as part of user login processing a login process can create a user s system chat and buddy state queues and subscribe the user to these chat and state queues. When subscribing to a queue three flags can be specified 

Successful subscription generates a queue subscribed message on the network to the client which is how a client session learns of a queue subscription that happened out in an XMLRPC call. If no direct TCP connection has been established for that client the gateway remembers the queue subscription so that when the client does connect all subscription messages can be then sent down. This handles the case of queue subscriptions that occur during user login which happens before the client has connected to a gateway via TCP. Similarly un subscriptions will send a queue unsubscribed message to the client.

Initially a service is registered at the client to handle the client side processing associated with the Message Queue system described herein. In a particular embodiment a ServiceProvider module on the client can register a new service MQManager for handling the client side processing associated with the Message Queue system. After client login the MQManager service can have the necessary data to start an authentication and connection process on the client. See below for more detail on the login process in a particular embodiment.

Objects interested in queues can register as a listener for that queue by name with MQManager and provide a message handler callback. Only one object is responsible for authoritative handling and consumption of a given queue so only one object is allowed to listen. If MQManager already has a listener waiting for queue named X and another listener attempts to register to listen the MQManager will raise an exception. If more objects need to know of messages for that queue the listening object s message handler can communicate events to other listeners.

MQManager can store callbacks for each queue name and call those callbacks as the MQManager receives messages. The MQManager decodes the messages first so the listener s message handler receives a message object and not a bit string. The queue subscribed message itself will be sent along to the listener s message handler as the message may contain initial state or initial participant data. Also the listening object may be interested in the point of subscription. Similarly unsubscribe messages can be sent to the message handler. Note that when an unsubscribe message is received we do not immediately remove the listener from the list. Instead we want the listener to automatically detect if another subscribe request for the same queue happens without the listening object having to subscribe again. When MQManager receives a message and calls the listener s callback with the decoded message object MQManager can also pass the queue name for objects using one callback to listen to multiple queues. MQManager can filter out messages that are marked as originating from this user so chat sessions don t have to deal with echo messages.

If a subscription message received by MQManager doesn t have an object listening to that queue MQManager backlogs the subscription message by queue name. Subsequent messages that come in for that queue will be backlogged in the same spot. If an object attempts to listen to that queue name at some later point the object can immediately have this backlogged batch of messages sent to the object. This allows objects to listen for subscriptions if they don t immediately know of the queue name. For example a user creates a chat but doesn t know that the new chat s queue name is chat 49723942 messages until a call returns with the newly created chatId 49723942. When backlogging a message if the oldest message in the backlog for that queue is older than a time period e.g. 5 minutes we can log an error discard that queue s backlog and stop backlogging subsequent messages for that queue.

The message sending component of the MQManager e.g. sendMessage can take an optional expectAsyncResult flag. Messages sent with this flag can have an op id generated by MQManager to be appended to the message and returned by sendMessage. Messages sent with expectAsyncResult are state messages that expect a pass fail response a message sent with an op id can asynchronously receive a response from the network specifying to which op id the message relates and a pass fail result. What the calling object does with the op id created by sendMessage and the subsequent result message sent to its handler is entirely the responsibility of the calling object. See the chat disclosure below for an example of usage.

MQManager can also handle the sending of a keep alive ping over TCP every time period e.g. 20 seconds if there is no other outbound traffic. MQManager can also handle the receiving of a keep alive ping every time period e.g. 20 seconds from the gateway if there is no other inbound traffic. If the expected keep alive isn t present or the connection is otherwise unexpectedly lost MQManager can reconnect transparently queuing up messages to be sent during the non connected state including the message that may have been pending during a socket error. These queued up messages can be sent out once connection is reestablished. The gateway can maintain the subscriptions for the client automatically if the reconnect happens quickly enough. If a connection cannot be reestablished within a reasonable time frame as the gateway will consider this client s extended absence of connection a timeout and will kill existing subscriptions the client should behave like it normally does for an extended network outage requiring the user to sign in again. Connection lapse is the only thing that will cause an outgoing message to be queued up for later delivery any failure in sendMessage in the gateway or beyond e.g. sending to non existent queue sending to queue that one isn t subscribed to or can t write to will silently fail unless an op id was specified e.g. a expectAsyncResult was passed to MQManager.sendMessage . In this case an error result will come back.

Though the MQManager is created when it is registered with serviceProvider MQManager does not begin to establish a TCP connection until after user login is finished. After connection occurs the authentication process is initiated. In a particular embodiment authentication is a multi step process including the following steps 

If authentication fails the user is asked to log in again in a manner similar to a login failure. As part of the login process a login component creates and subscribes the user to a system chat queue and a buddy state queue. For both the system chat queue and the buddy state queue the subscriptions are marked as not interested in other subscribers and that the user is a keep alive participant. Additionally the login component loops over all of the user s friends fans recent chats anyone who would appear in the user s friends mode to subscribe the user to their queues and all of them to the user s queue. Because the subscriptions are marked as non creating nothing will happen for offline friends with no queues of their own or no way to subscribe to the user s queue. The subscriptions are also all marked as non keep alive.

In a particular embodiment there is a manager object which listens for systemchat subscription messages. Upon receiving systemchat messages the manager object echoes the systemchat message content on an event bus for all clients to consume. The manager object does not echo the subscription and un subscription messages. In one embodiment system chat events come from the server as a string token with a JSON blob of arbitrary payload.

Shortly after login on the client a BuddyState object is activated. The BuddyState object is the main buddy manager object that manages the user s buddies. The BuddyState object can immediately fetch a list of the user s friends buddies shortly after login. For each buddy in the list the BuddyState object can listen to the buddyState queue for that friend e.g. using one single callback for all of the buddies . A subscription message for a particular queue means that a user s friend is online and an un subscription message for that particular queue means that a user s friend is offline. Therefore at BuddyState object initialization time all friends are offline until proven otherwise. Other messages over those queues can notify the user of their actual buddy state e.g. Do Not Disturb DND adults only away etc. . Updates to online status and actual buddy state can be delivered to the various parts of the client using the BuddyState object s event system. The BuddyState object is also responsible for handling the user s buddyState queue and sending messages to the user s buddyState queue e.g. I went DND I went available I am signing off etc. . When a user signs off a signing off message is sent on their buddyState queue so the BuddyState object can interpret either a signoff message or an un subscription for some friend as the user going offline. When a user vanishes without the signing off message their gateway will eventually time them out and unsubscribe them from all of their queues. Because that user was the keep alive participant for their own buddy queue that buddy queue will be torn down and all other participants will be unsubscribed which is how other clients will learn of the user s timeout. Note that because listeners are kept around in MQManager even after an un subscription occurs the BuddyState object doesn t have to listen again when a friend goes offline and an un subscription is received from the network. A user learns of new friends coming online because part of that friend s login processing is to subscribe to all of their friends buddyState queue. For buddy list changes e.g. adding friends removing friends etc. the BuddyState object can listen to subscriptions for the new buddy and subscribe or un subscribe the users from each others buddyState queues. Similarly for new recent chats the BuddyState object can listen to the subscription for the new recent chat and can create the appropriate subscriptions between the recent chatting users.

A TCP connection that is cleanly disconnected will cause the gateway to unsubscribe the logged out user from all queues which depending on the user s keep alive status for those queues will tear down some queues entirely. In the event of an unclean shutdown the user s gateway will time out the user and perform the same steps. Additionally in case of a gateway crashing the existence of a queue stops if there have been no strong subscribed users for a period of time out.

When a user initiates a chat session at a client a client chat module or the MQManager can create a chat session identifier chatId . The MQManager can then create and subscribe the user to a message queue messageQueue and a state queue stateQueue for the chat. The message queue is marked as interested in participants and for both the message queue and the state queue the user is not marked as a keep alive user. Initial and ongoing participant information is communicated in either direction to the client s session objects through the messageQueue as well as actual messages and other chatstream messages such as animations. Initial and ongoing seat state is communicated in either direction to the chat session through the stateQueue. Seat assignment being a state queue message can expect an asynchronous response. Therefore the chat session when sending seat messages can set the expectAsyncResponse flag to true store the op id returned from sendMessage and expect a result message to come to its message listener function at some point in the future that has the result for that op id. For example we can move the seat locally send the message and record the seat move and op id in the chat session somewhere e.g. an object containing op id cid old seat new seat . When a result message is received from the network for that given op id we can either bump our entry out of the object containing op id if the seat assignment passed or undo the seat move locally if the seat assignment failed . Note that chatIds may only exist as a means to name and uniquely identify queues for a particular chat. The existence of a queue for some chat is what really represents the existence of that chat.

When a user joins a chat session at a client a client chat module or the MQManager can subscribe the user to the messageQueue and stateQueue for the chatId the user is attempting to join marking the subscriptions as non keep alive and non creating. In the event that the queues don t exist we can increment an error counter. The client chat module or the MQManager can be responsible for unsubscribing the user from the messageQueue and stateQueue for the chatId.

In a particular embodiment a chatgateway.attemptInvite XMLRPC call can be used to create an invite in the database and to send a systemChat notification to the invitee instructing their client to call chatGateway.checkForInvite retrieve the invite and either accept or give a decline reason. The accept or decline reason can be returned to the invitor as the synchronous return value to their attemptInvite call. In an alternative embodiment an invitor can send a systemchat invite to the invitee whose reply is delivered back asynchronously as another systemChat message to the invitor.

Referring to in an example embodiment a system and method for managing multiple queues of non persistent messages in a networked environment are disclosed. In various example embodiments an application or service typically operating on a host site e.g. a website is provided to simplify and facilitate non persistent message and state transfers between a plurality of users at user platforms from the host site . The host site can thereby be considered a message queue system site as described herein. Multiple user platforms provide a plurality of message streams of which a user may become a content consumer and or a content provider. The message queue system site and the user platforms may communicate and transfer messages related content and information via a wide area data network e.g. the Internet . Various components of the message queue system site can also communicate internally via a conventional intranet or local area network LAN .

Networks and are configured to couple one computing device with another computing device. Networks and may be enabled to employ any form of computer readable media for communicating information from one electronic device to another. Network can include the Internet in addition to LAN wide area networks WANs direct connections such as through a universal serial bus USB port other forms of computer readable media or any combination thereof. On an interconnected set of LANs including those based on differing architectures and protocols a router acts as a link between LANs enabling messages to be sent between computing devices. Also communication links within LANs typically include twisted wire pair or coaxial cable while communication links between networks may utilize analog telephone lines full or fractional dedicated digital lines including T1 T2 T3 and T4 Integrated Services Digital Networks ISDNs Digital User Lines DSLs wireless links including satellite links or other communication links known to those of ordinary skill in the art. Furthermore remote computers and other related electronic devices can be remotely connected to either LANs or WANs via a modem and temporary telephone link.

Networks and may further include any of a variety of wireless sub networks that may further overlay stand alone ad hoc networks and the like to provide an infrastructure oriented connection. Such sub networks may include mesh networks Wireless LAN WLAN networks cellular networks and the like. Networks and may also include an autonomous system of terminals gateways routers and the like connected by wireless radio links or wireless transceivers. These connectors may be configured to move freely and randomly and organize themselves arbitrarily such that the topology of networks and may change rapidly.

Networks and may further employ a plurality of access technologies including 2nd 2G 2.5 3rd 3G 4th 4G generation radio access for cellular systems WLAN Wireless Router WR mesh and the like. Access technologies such as 2G 3G 4G and future access networks may enable wide area coverage for mobile devices such as one or more of client devices with various degrees of mobility. For example networks and may enable a radio connection through a radio network access such as Global System for Mobile communication GSM General Packet Radio Services GPRS Enhanced Data GSM Environment EDGE Wideband Code Division Multiple Access WCDMA CDMA2000 and the like. Networks and may also be constructed for use with various other wired and wireless communication protocols including TCP IP UDP SIP SMS RTP WAP CDMA TDMA EDGE UMTS GPRS GSM LTE UWB WiMax IEEE 802.11x and the like. In essence networks and may include virtually any wired and or wireless communication mechanisms by which information may travel between one computing device and another computing device network and the like. In one embodiment network may represent a LAN that is configured behind a firewall not shown within a business data center for example.

The user platforms may include any of a variety of providers of network transportable digital content. Typically the file format that is employed is XML however the various embodiments are not so limited and other file formats may be used. For example feed formats other than HTML XML or formats other than open standard feed formats can be supported by various embodiments. Any electronic file format such as Portable Document Format PDF audio e.g. Motion Picture Experts Group Audio Layer 3 MP3 and the like video e.g. MP4 and the like and any proprietary interchange format defined by specific content sites can be supported by the various embodiments described herein. Syndicated content includes but is not limited to such content as news feeds events listings news stories blog content headlines project updates excerpts from discussion forums business or government information and the like. As used throughout this application including the claims the term feed sometimes called a channel refers to any mechanism that enables content access from a user platform .

In a particular embodiment a user platform with one or more client devices enables a user to access content from other user platforms via the message queue system site and network . Client devices may include virtually any computing device that is configured to send and receive information over a network such as network . Such client devices may include portable devices or such as cellular telephones smart phones display pagers radio frequency RF devices infrared IR devices global positioning devices GPS Personal Digital Assistants PDAs handheld computers wearable computers tablet computers integrated devices combining one or more of the preceding devices and the like. Client devices may also include other computing devices such as personal computers multiprocessor systems microprocessor based or programmable consumer electronics network PC s and the like. As such client devices may range widely in terms of capabilities and features. For example a client device configured as a cell phone may have a numeric keypad and a few lines of monochrome LCD display on which only text may be displayed. In another example a web enabled client device may have a touch sensitive screen a stylus and several lines of color LCD display in which both text and graphics may be displayed. Moreover the web enabled client device may include a browser application enabled to receive and to send wireless application protocol messages WAP and or wired application messages and the like. In one embodiment the browser application is enabled to employ HyperText Markup Language HTML Dynamic HTML Handheld Device Markup Language HDML Wireless Markup Language WML WMLScript JavaScript EXtensible HTML xHTML Compact HTML CHTML and the like to display and send a message.

Client devices may also include at least one client application that is configured to receive content or messages from another computing device via a network transmission. The client application may include a capability to provide and receive textual content graphical content video content audio content alerts messages notifications and the like. Moreover client devices may be further configured to communicate and or receive a message such as through a Short Message Service SMS direct messaging e.g. Twitter email Multimedia Message Service MMS instant messaging IM internet relay chat IRC mIRC Jabber Enhanced Messaging Service EMS text messaging Smart Messaging Over the Air OTA messaging or the like between another computing device and the like.

Client devices may also include a wireless application device on which a client application is configured to enable a user of the device to subscribe to at least one message source. Such subscription enables the user at user platform to receive through the client device at least a portion of the message content. Such content may include but is not limited to instant messages Twitter tweets posts stock feeds news articles personal advertisements shopping list prices images search results blogs sports weather reports or the like. Moreover the content may be provided to client devices using any of a variety of delivery mechanisms including IM SMS Twitter Facebook MMS IRC EMS audio messages HTML email or another messaging application. In a particular embodiment the application executable code used for content subscription as described herein can itself be downloaded to the wireless application device via network .

In some cases a user at user platform can subscribe to certain content and or content channels provided by all mechanisms available on the client device s . In various embodiments described herein the host site can employ processed information to deliver content channel information to the user using a variety of delivery mechanisms. For example content channel information can be delivered to a user via email Short Message Service SMS wireless applications and direct messaging e.g. Twitter to name a few. Additionally content channel information can be provided to a user in response to a request from the user.

Referring still to host site of an example embodiment is shown to include a message queue system intranet and message queue system database . Message queue system can also include gateways message queue nodes a supervisor translators state managers and load balancers . Each of these modules can be implemented as software components executing within an executable environment of message queue system operating on host site . Each of these modules of an example embodiment is described in more detail above in connection with the figures provided herein.

Referring now to another example embodiment of a networked system in which various embodiments may operate is illustrated. In the embodiment illustrated the host site is shown to include the message queue system . The message queue system is shown to include the functional components through . In a particular embodiment the host site may also include a web server having a web interface with which users may interact with the host site via a user interface or web interface. The host site may also include an application programming interface API with which the host site may interact with other network entities on a programmatic or automated data transfer level. The API and web interface may be configured to interact with the message queue system either directly or via an interface . The message queue system may be configured to access a data storage device either directly or via the interface .

In current practice content is delivered from web servers to web clients using the HTTP protocol. This protocol is an important part in REST ful application design e.g. see Fielding Roy Thomas Architectural Styles and the Design of Network based Software Architectures Doctoral dissertation University of California Irvine 2000 which can be found at the following web location http www.ics.uci.edu fielding pubs dissertation rest arch style.htm . To scale a data transfer system to Internet scale data needs to be cached at multiple layers. illustrates an example of network stack layers where caching can happen.

Referring to content stored on a permanent medium of a web server can be transferred to a web browser of a web client through a sequence of caches and . illustrates a conventional content delivery system. Many variations on this processing flow are common in practice. At each step of the way a participating node may cache the content to speed up later requests for the same content. Starting with the example of data that comes from a disk drive the disk drive often has a RAM Random Access Memory cache to buffer raw data. Many popular disk drives from many manufacturers include this feature. The operating system of the web server typically implements file caching for recently used files to avoid having to access the disk drive e.g. Microsoft Windows Linux and BSD UNIX operating systems all implement this feature . A caching reverse proxy is often used for load balancing in a web server infrastructure. The caching reverse proxy can also service requests for the same content where the request would otherwise go to the web server e.g. one particular implementation of this is the Varnish transparent HTTP cache application . A content delivery network cache may cache content closer to a client by keeping caching nodes in data centers close to consumers e.g. companies like Akamai and CloudFront can provide this service . Campus area caching may be employed to reduce utilization on internet connectivity in companies organizations and even locally to individual consumers e.g. a forwarding HTTP proxy like Squid is an example . Finally the web browser application can cache data to avoid having to access the network at all this cache is often persisted on local persistent storage. All popular web browsers e.g. Internet Explorer Mozilla Firefox Google Chrome and Apple Safari implement this feature.

To allow content to change over time data that is mutable can be given an expiry date. For example if the data consists of today s news the expiry date can be set to midnight after which new content may be available. All caching nodes on the network along this path using the HTTP protocol will be aware of this expiration header and will discard the cached data at the point at which the data becomes stale e.g. when the current time date passes the expiry time date corresponding to the data . For example see RFC 2616 the Expires header definition of the HTTP protocol http www.w3.org Protocols rfc2616 rfc2616 sec14.html.

When the expiration date of content is well known this method works great. When the expiration date of content is unpredictable some amount of caching will still help reduce network load and or request load on each of the pieces of data in the above described data pipeline. For example a cache time to live of fifteen minutes will help reduce perhaps thousands of requests per second to a small fraction yet allow new content to be delivered to consumers with at most a fifteen minute latency.

Caching is conceptually implemented as a key value look up store where the key is the Uniform Resource Identifier URI of the content also known as the endpoint address or endpoint for short and various metadata about the request known as the varying headers. For more information see RFC 2616 cited above.

When the latency involved in caching is not acceptable two additional methods exist to improve latency from the time of new content being available at a given endpoint to the time at which clients will see this new content.

The first such method is the ETags header which tells the client a version number or checksum of the content. When the client wants to receive content again the client requests the content again providing the Etag value the client previously received. The server can then compare this version number or other Etag value with the version number or other Etag value of the content currently on the server. If the Etag values match the server can return an empty response that says no change. The client then knows that the content in the cache is current and can be re used. This process saves network transfer time and resources over the network especially for large pieces of content such as images sounds and movies. However the process requires a network round trip to verify whether or not the content has changed. Depending on implementation details this process may also invalidate the use of caching proxies in the middle of the chain such as content delivery networks.

The method of providing cached data as described above is effective as long as the client doesn t need to be told about new data availability pre emptively. In other implementations the client needs to be aware of when to request new data should new data be available. As a result the method of providing cached data as described above is useful for non dynamic data such as common background assets used to build a web site e.g. images scripts etc. but the described caching method is not as useful or efficient for quickly changing dynamic main content.

A second method for improving latency typically used when content changes often is called long polling Long polling is sometimes referred to as comet style resource fetching . In long polling a client makes a request similar to the Etags request described above. But instead of immediately returning data the server stalls the request for some amount of time e.g. 15 30 seconds is typical . During this stall time no data flows back to the client. If the data at the endpoint changes within this time interval the new data is immediately returned to the client requesting this data. If the data at the endpoint does not change within this time interval the request is finally returned as a no change result. The client will then re request the same data and get stalled again in a process that repeats for as long as the client is interested in up to date data. An example of HTTP long polling is shown in .

Referring now to an example of HTTP long polling is illustrated. Long polling is effective because it is compatible with most web browser technology. However long polling uses significant resources on the web server side to keep these web requests alive but stalled. For example content edge caches cannot do this for the client the request has to go all the way to the server where server side resources such as open sockets and network control information are consumed leading to depletion of RAM and CPU central processing unit resources.

The various example embodiments described herein significantly improve on the responsiveness and machine resources needed to implement web applications that rely on frequently changing data. In the disclosed example embodiments the web server shown in and described above is augmented to include a non persistent message queuing system as disclosed herein. The new architecture is shown in .

Referring now to a data transfer architecture is shown wherein a web server is augmented to include or be in data communication with a non persistent message queue system such as the message queue system as described above. In the example embodiment shown in the message queue may be implemented as a network of collaborating server computers to allow for a suitably large scale. Similarly the web server may consist of multiple collaborating server computers. Additionally the data that is served by the illustrated data transfer architecture may come from persistent storage that is occasionally updated or the served data may be derived from some process inside the web server or another system upon which the web server relies. As an example consider a web service that serves the current temperature and wind speed. This server would rely on external sensors to actually read these values for processing and transport to the client.

As a result of the data transfer architecture as described herein and the inclusion of the message queue system as part of the architecture the standard HTTP protocol is now augmented with additional metadata. This additional metadata can indicate a message queue to which the client can connect and addressing data within that message queue on which the client can listen to hear about updates to particular resources endpoints. This additional metadata is transparently added to the HTTP headers of the response such that the content can still be served to clients who do not connect to the message queue. Additionally for resources on which data is expected to infrequently change the web server does not need to add the queue headers and the client will not attempt to subscribe to the queue.

When the server or some system upon which the server relies determines that the data for an endpoint has changed a message can be sent to a message queue in the message queue system . Clients connected to the message queue can then receive this message and become aware that it is time to re request the changed data from the given resource. Such a re request can use the Etags feature described above to bypass content caching. In this manner the data transfer architecture as described herein can provide a queue invalidation mechanism to alert a client of changed data.

As an alternative to adding queue connection and or subscription information to the HTTP headers the content can be modified to include this information. If the content is in a structured format that allows upwards downwards compatibility such as XML or JSON this modification can be done without affecting the ability of other clients to understand the response without knowing about the queue invalidation mechanism.

In an example embodiment content can be structured into a URI of four segments within the web server. These four URI segments are detailed below in the example embodiment 

In the example of the four URI segments as detailed above datatype identifies a particular kind of data such as user or picture or message. In the example embodiment the kind of data maps to a particular type and schema of the resulting endpoint which makes it possible for the client web browser to properly format and display the given data. Specifically each data type conforms to some JSON schema e.g. see http json schema.org .

At the endpoint datatype or datatype can be a collection list of URIs for all instances of that type that is visible to the current user. For very large collections this list may be paginated only partially shown with links available for getting more data or the list may be omitted entirely. Additionally query parameters may be used to modify the list such as finding all users of a particular age. The example URI for such a query would be user age 45 and the resulting entity would be the list possibly paginated of all user entities matching the query.

The URIs of each user entity will have the form user id where id is a unique identifier for each user. For example the unique user identifier could be a customer ID or an email address. Thus the URI of a particular user entity would be user 12345 . The data at this endpoint conforms to a defined JSON schema for a user entity and would be data specific to the user with customer ID 12345 within the namespace of this web application.

Part of the user entity is a set of relations. For example friends on the friends list of this user may be a relation named friends in the user entity. In JSON this would be expressed as a link to a friends relation such as the example below 

Rather than requiring the client application to explicitly know about friends relation URLs the relative URL can be explicitly specified in the user entity. For more information on this state of the art see Hypertext As The Engine Of Application State HATEOAS in the Fielding dissertation cited above.

The content of the user id friends endpoint is another list possibly paginated possibly queried filtered of URIs to entities that fulfill the relation in question. For example if user 12345 has made friends with users 654 and 98765432 the content might look like the following example 

Each data element is individually addressable. Thus a particular friend relation is addressable individually. This is important for allowing the application to add modify and delete specific friend relations. For example removing the friend relation to user 654 might be expressed in HTTP as a DELETE verb on the URI user 12345 friends 1 given the entity data above.

A particular example embodiment of cache invalidation implemented with the message queue as described herein can contain one queue per unique URI and can allow the client subscribe to each URI specific queue to learn about the availability of new data or modified data at this URI endpoint. As memory resources are required in the transient message queue whenever a subscription or queue is created the amount of RAM used can be fine tuned by merging multiple URI updates into a single queue. This implementation reduces the number of queues to which a client can subscribe but potentially increases the number of clients subscribed to each such queue. Two methods can be used by an example embodiment to manage the allocation of URIs to message queues. These methods are each described below 

If a URI is very long a shortened version of the URI can be used to avoid unnecessarily sending large amounts of data. Available methods of referring to long URIs using short methods are available in practice including 

The client additionally implements a local cache of URI endpoint data to re use data that has not been invalidated. This is necessary to avoid unnecessary re requests to the server when data is not stale and already received.

In practice the most popular data request and delivery protocol in use today is HTTP RFC 2616 and other protocols closely related or evolved from HTTP such as HTTPS secure layer and SPDY a Google specific extension precursor to version 2.0 of the HTTP protocol. 

In a particular embodiment a suitable protocol to use for connecting to the message queue is the WebSocket RFC 6455 protocol. WebSocket is a protocol providing full duplex communications channels over a single TCP connection. The Web Socket protocol was standardized by the Internet Engineering Task Force IETF as RFC 6455 in 2011. Additionally other protocols such as direct TCP connections or UDP connections can also be used assuming the semantics of the message queue can be retained through additional intermediate framing mechanisms.

In an alternative embodiment a system can be implemented where the updated data is sent through the invalidation mechanism queuing system to participating clients to avoid the second fetch of the resource over HTTP. However doing so complicates both the server and the client. The server is more complex because the server has to know how to format and transmit the data over two separate protocols HTTP and the queuing system. The client is more complex because the client needs to be able to receive and decode data over two separate protocols as well.

The example embodiment of cache invalidation implemented with the message queue as described herein lends itself very well to a Reactive Programming implementation approach in the client. In computing reactive programming is a programming paradigm oriented around data flows and the propagation of change. Reactive Programming is a best practice well known to those of ordinary skill in the art. One embodiment using reactive programming to implement an end user visible application on top of this near real time data delivery system is shown in .

Referring now to an example embodiment using reactive programming to implement an end user visible application on top of the disclosed near real time data delivery system is shown. The example embodiment illustrated in shows the data processing interactions between the components of the client application running on a client system the server system and the message queue. In the example embodiment the processing flow as shown in is described below.

In the various embodiments described above a main feature is that the program logic component as shown in is only involved in steps 1 through 3. The data transfers and data updates can be handled automatically through the pre configured user interface components which react to data updates from the local cache component. As a result the data at the user interface component is always current and the load on the program logic component is reduced. Additionally the client system as shown in can be used to communicate with servers that do not provide or don t support real time caching invalidation information. As such the disclosed client systems can be implemented with any type of server without any change in software implementation although the real time update feature will of course not be available with servers not supporting this feature. Such forwards and backwards compatibility is of significant value in building robust interactive systems.

In the example embodiment described above there can be a race condition in some circumstances. Specifically if the entity e.g. the data object changes after the data has been sent from the server to the client but before the client manages to connect to the queue and establish the connection the client may not get the invalidation event. There are two solutions to this problem 

Bootstrapping As an alternative to providing address information about the queuing system to use for invalidation messages in HTTP headers or other metadata attached to the response data one embodiment of the system disclosed herein uses a bootstrap URI that returns information about other endpoints for clients to use when interacting with the system. Those endpoints are in turn identified using known key names. An example response to the bootstrap URI might look like the following example 

This structure saves space in responses by providing the update address once rather than with each response. Additionally this structure lets the server re structure the URLs services used without breaking compatibility with existing client code by simply changing the URLs exposed for the well defined keys in the bootstrap response.

The example computer system includes a data processor e.g. a central processing unit CPU a graphics processing unit GPU or both a main memory and a static memory which communicate with each other via a bus . The computer system may further include a video display unit e.g. a liquid crystal display LCD or a cathode ray tube CRT . The computer system also includes an input device e.g. a keyboard a cursor control device e.g. a mouse a disk drive unit a signal generation device e.g. a speaker and a network interface device .

The disk drive unit includes a non transitory machine readable medium on which is stored one or more sets of instructions e.g. software embodying any one or more of the methodologies or functions described herein. The instructions may also reside completely or at least partially within the main memory the static memory and or within the processor during execution thereof by the computer system . The main memory and the processor also may constitute machine readable media. The instructions may further be transmitted or received over a network via the network interface device . While the machine readable medium is shown in an example embodiment to be a single medium the term machine readable medium should be taken to include a single non transitory medium or multiple media e.g. a centralized or distributed database and or associated caches and servers that store the one or more sets of instructions. The term machine readable medium can also be taken to include any non transitory medium or combination of transitory media collaborating to create a non transitory or semi non transitory medium that is capable of storing encoding or carrying a set of instructions for execution by the machine and that cause the machine to perform any one or more of the methodologies of the various embodiments or that is capable of storing encoding or carrying data structures utilized by or associated with such a set of instructions. The term machine readable medium can accordingly be taken to include but not be limited to solid state memories optical media and magnetic media.

The Abstract of the Disclosure is provided to comply with 37 C.F.R. 1.72 b requiring an abstract that will allow the reader to quickly ascertain the nature of the technical disclosure. It is submitted with the understanding that it will not be used to interpret or limit the scope or meaning of the claims. In addition in the foregoing Detailed Description it can be seen that various features are grouped together in a single embodiment for the purpose of streamlining the disclosure. This method of disclosure is not to be interpreted as reflecting an intention that the claimed embodiments require more features than are expressly recited in each claim. Rather as the following claims reflect inventive subject matter lies in less than all features of a single disclosed embodiment. Thus the following claims are hereby incorporated into the Detailed Description with each claim standing on its own as a separate embodiment.

