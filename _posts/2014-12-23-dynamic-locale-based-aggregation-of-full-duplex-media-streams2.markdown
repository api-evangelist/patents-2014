---

title: Dynamic locale based aggregation of full duplex media streams
abstract: A cloud-based video/audio conferencing system and method performs locale based aggregation of a full duplex media stream to organize multiple connections to a conference call that originate from the same physical location or a shared locale. The cloud-based video/audio conferencing system performs synchronization of the microphone and speaker audio signals of the same-locale connections. In this manner, a conference call may be held with multiple user devices making connections from the same physical location. User experience is enhanced by allowing each user in the same location to use his/her own individual devices to connect to the same conference call.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09641576&OS=09641576&RS=09641576
owner: Amazon Technologies, Inc.
number: 09641576
owner_city: Seattle
owner_country: US
publication_date: 20141223
---
This application claims priority to U.S. Provisional Patent Application No. 62 023 749 entitled DYNAMIC LOCALE BASED AGGREGATION OF FULL DUPLEX MEDIA STREAMS filed Jul. 11 2014 which is incorporated herein by reference for all purposes.

Video and audio conferences or conference calls may be held involving multiple participants located at the same physical location such as the same conference room with other participants located at a remote location. When a video audio conference includes multiple participants at the same physical locale the participants experience is often not optimal.

The same locale problem in video audio conferencing arises from the fact that generally only one actual device is able to be active for the conference call in any given locale. The device will inevitably be further from some participants in the room than others. This results in varied intelligibility of participants based solely on how near or far the participants are from the single active microphone. Further the experience of those participants in the shared locale itself is also less than ideal since those furthest from the device are often left struggling to hear the single in room device s speaker output.

Generally the reason that only one device per locale can be active at a time during a conference call has to do with the way that current Voice over Internet Protocol VoIP conferencing software is designed. When one participant connects to a cloud based or peer to peer conference call the participant is sent an audio mix of all other participants except himself. This makes sense as one has no need to hear oneself echoed back to him when speaking.

This audio mixing scheme is generally done per connection and is completely ignorant of each connection s locale or physical location. That is a cloud based or peer to peer conference call system or method is not aware of the physical location of each connection to the conference call. The audio mixing scheme works well for person to person calls and is the current approach employed by cloud based conferencing services such as Skype GoToMeeting Facetime WebEx and others. However on multi participant calls when some number of participants shares the same physical location the experience degrades and the problems associated with multiple participants at the same locale are compounded as more and more participants share the same locale. In other words the more participants per physical locale the more the overall experiential quality of the conference call degrades.

Using the conventional VoIP conferencing system the conference experience may be improved by having multiple participants in the same physical locale use multiple devices to join the conference. That is every participant has his her own microphone and own set of speakers. However using multiple active devices at the same physical location to hold a conventional VoIP conference call is actually not possible. The first issue is audio feedback. Since each same locale participant receives an audio mix with only his own microphone absent the participant will still be receiving the microphones of his colleagues who are connected in the same physical locale. The colleagues audio stream will come out of the participant s speaker and go back into the colleagues microphones where the audio signal originated and back out the participant s speaker and back into the colleagues microphones and so on. This results in the same classic audio feedback problem associated with a Public Address PA system when the speaker accidentally points his microphone at the PA speaker. A high pitched squeal ensues which is highly unpleasant.

In the conventional cloud based VoIP conferencing system the feedback problem can be eliminated by giving the audio server producing each connection s audio mix a token which identifies participants sharing the same physical locale as such. The conferencing system would use that token to produce locale based mixes where the mix sent to any connection in a particular physical locale would be absent of the microphone signals of the multiple devices sharing that locale. Thus if participant A and participant B are in the same room participant A s microphone signal would not emanate from participant B s speaker and vice versa. The feedback problem is thus solved.

However even with the feedback problem solved other problems remain. For instance each participant s audio client connection will have different latencies. Devices generally have different audio device hardware and driver buffer sizes which is one factor contributing to connection latency variations. An even bigger contribution to differing connection latencies is due to the nature of the Internet itself.

In a packet based VoIP system one technique employed to create the illusion of a continuous media stream is the use of adaptive jitter buffering. These buffers dynamically adjust to an ideal size to minimize packet loss with the lowest possible latency based on current network conditions. Since network conditions change often due to congestion topologies WiFi loss etc. each audio client will generally end up with differently sized jitter buffers relative to one another throughout the course of a conference call.

The end result of these differing connection latencies is a poor conference experience on both ends of the call. In the shared physical locale the participants will experience unsynchronized speaker signals among the in room devices which will render the conference call unusable. In the remote locale the participants will have a similar experience. The microphone signal the remote participants will be receiving from the shared locale will contain a mix of all of the in room device microphones which will be similarly desynchronized and jumbled because of the differing latencies.

A side effect of the desynchronization in the shared locale is that the in room device s Acoustic Echo Cancellation AEC software will fail to function for its intended purpose of echo cancellation. AECs are designed to cancel the remote farend signal being played out of a device s speakers from the device s microphone nearend signal being sent up to the audio server. The desynchronization of the microphone signals at the shared locale will cause the AEC operation to become ineffective so that remote participants end up hearing themselves echoed back to them anytime they speak. This severely degrades the participant experience. In some cases the remote participants may hear their own voices echoed multiple times depending on the number of devices in the shared locale. Therefore the conventional VoIP conference system does not permit multiple active devices to be used at the same physical location for holding a conference call.

The invention can be implemented in numerous ways including as a process an apparatus a system a composition of matter a computer program product embodied on a computer readable storage medium and or a processor such as a processor configured to execute instructions stored on and or provided by a memory coupled to the processor. In this specification these implementations or any other form that the invention may take may be referred to as techniques. In general the order of the steps of disclosed processes may be altered within the scope of the invention. Unless stated otherwise a component such as a processor or a memory described as being configured to perform a task may be implemented as a general component that is temporarily configured to perform the task at a given time or a specific component that is manufactured to perform the task. As used herein the term processor refers to one or more devices circuits and or processing cores configured to process data such as computer program instructions.

A detailed description of one or more embodiments of the invention is provided below along with accompanying figures that illustrate the principles of the invention. The invention is described in connection with such embodiments but the invention is not limited to any embodiment. The scope of the invention is limited only by the claims and the invention encompasses numerous alternatives modifications and equivalents. Numerous specific details are set forth in the following description in order to provide a thorough understanding of the invention. These details are provided for the purpose of example and the invention may be practiced according to the claims without some or all of these specific details. For the purpose of clarity technical material that is known in the technical fields related to the invention has not been described in detail so that the invention is not unnecessarily obscured.

According to embodiments of the present invention a cloud based video audio conferencing system and method performs locale based aggregation of a full duplex media stream to organize multiple connections to a conference call that originate from the same physical location or a shared locale. Accordingly multiple active devices can be used in the same locale to connect to a conference call. The cloud based video audio conferencing system further performs synchronization of the microphone and speaker audio signals of the same locale connections. In this manner a conference call may be held with multiple user devices making connections from the same physical location. User experience is enhanced by allowing each user in the same location to use his her own individual devices to connect to the same conference call.

In some embodiments the cloud based video audio conferencing system performs automatic detection of conference call connections that may originate from the same physical location. User experience is further enhanced by allowing users to automatically connect to a conference call using their own devices without additional setup steps regardless of how many other devices may be connected to the same conference call at the same physical location. The cloud based video audio conferencing system and method enables effective real time remote collaboration between users and improves participant experience in instant video and audio conferencing where some of the conference participants may be attending the conference call in the same physical location.

It is instructive to note that in a closed or dedicated or specially designed audio conferencing system alignment of speaker or microphone audio signals may be performed easily as the closed system has only a small number of variables and the variables can be ascertained beforehand and accounted for. In some cases such closed conference call systems are hard wired so that synchronization can be easily performed. However the cloud based video audio conferencing system of the present invention operates as an open system and often relies on the public Internet for transmission of audio data packets. Furthermore participants may connect to the conferencing system of the present invention using a variety of devices and a variety of connection media including wireless connections as well as wired connection and through public data networks such as the Internet or private data networks privately operated LAN . In such an open system the number of variables becomes large and the network condition is fluid and changing. Therefore synchronization of the audio signals becomes challenging and cannot be performed using techniques that are generally applied to closed dedicated conferencing systems. In embodiments of the present invention the cloud based video audio conferencing system implements audio synchronization using software based audio alignment method to enable synchronization of audio signals under challenging network environments.

A salient feature of the conferencing system of the present invention is that multiple participants at the same physical location may connect to the same conference call using individual devices. That is the conferencing system supports a conference call with multiple active device connections from the same physical location. For example at Locale D multiple participants may connect to the same conference call with each participant using his her own device to connect to the conference call.

In the present description a locale refers generally to a physical location at which one or more participants of a conference call is located. More specifically a locale refers to the physical space where audio feedback from multiple active devices used by multiple conference participants to connect to the same conference call would create feedback problems or would create unsynchronized speaker signals or would inhibit the proper functioning of acoustic echo cancellation used in conventional cloud based conferencing systems. A shared locale refers to a locale or physical location occupied by two or more participants of a conference call.

In the present description a user device or a device refers to any type of computing devices such as laptop computers desktop computers tablet computing devices or smart phones. The user device is capable of connecting to a data network through a wireless or wired connection and is also capable of providing audio and or video communication.

In the present description each user device makes a connection to the conference call using an audio client also referred to as a Voice over Internet Protocol VoIP audio client. In the present description an audio client executed on a participant s device is also referred to as an audio client node. The audio client may be a software process running on a processor of the participant s device. In a typical conference call each locale includes a single audio client and the audio client nodes at each locale communicate with the audio conferencing server to send and receive audio data packets during a conference call.

In embodiments of the present invention to facilitate multiple connections to a conference call from the same locale the cloud based video audio conferencing system of the present invention selects one audio client node at the shared locale as an audio client aggregation node . For example in Locale D includes four participants wishing to connect to the conference call using individual user devices with each user device running its own individual audio client . Thus Locale D includes four audio client nodes to that are to be connected to the conference call. Instead of allowing each audio client node to connect individually to the audio conferencing server one of the audio client node is selected as the audio client aggregation node . In the present example audio client node is selected to be the audio client aggregation node . The audio client aggregation node accepts connections from all audio client nodes in the same shared locale. In the present example the audio client aggregation node accepts connections from audio clients nodes . For example the local audio client nodes may communicate with the aggregation node through a local area network. Meanwhile the audio client aggregation node also process audio data packets from the audio client node to which it is associated.

The audio client aggregation node collects and organizes all local audio signals from audio client nodes and communicates with the audio conferencing server over the data network . In particular the audio client aggregation node organizes the multiple full duplex audio streams from the local audio client nodes and provides a single full duplex audio stream to the audio conferencing server . The audio client aggregation node receives a mixed audio stream from the audio conferencing server and distributes the mixed audio stream to the audio client nodes in the shared locale D. In particular the audio client aggregation node provides to each audio client the audio stream from all other conference participants except the audio client s own audio stream.

In the above described embodiments the cloud based video audio conferencing system of the present invention selects one audio client node at a shared locale with multiple audio client nodes to function as the audio client aggregation node . In other embodiments other methods to select or establish an audio client aggregation node at a shared locale can be used.

In some embodiments a dedicated audio client aggregation node may be provided at a shared locale as shown in . The dedicated audio client aggregation node may be provided on a computing device located at the shared locale such as a desktop computer a tablet computing device or a dedicated audio video conferencing device. When a dedicated audio client aggregation node is present in a shared locale all other audio clients nodes in the same locale connect to the dedicated audio client aggregation node to participate in a conference call. The dedicated audio client aggregation node collects and organizes all local audio signals from audio client nodes and communicates with the audio conferencing server over the data network . In some embodiments the audio client aggregation node organizes the multiple full duplex audio streams from the local audio client nodes and provides a single full duplex audio stream to the audio conferencing server . The audio client aggregation node receives a mixed audio stream from the audio conferencing server and distributes the mixed audio stream to the audio client nodes in the shared locale D. In particular the audio client aggregation node provides to each audio client the audio stream from all other conference participants except the audio client s own audio stream.

In some embodiments an audio client aggregation node may be provided on the audio conferencing server disposed on the data network as shown in . In that case the audio client aggregation node is a cloud based audio client aggregation node and the audio stream aggregation becomes centralized in the data network . The cloud based audio client aggregation node provided on the audio conferencing server may be accessible by locale based audio client nodes through the data network . With the cloud based audio client aggregation node thus configured audio client nodes in a shared locale connect to the cloud based audio client aggregation node over the data network to participate in a conference call. For example audio clients nodes in the shared locale D all connects to the cloud based audio client aggregation node over the data network to send and receive audio streams. The cloud based audio client aggregation node collects and organizes all audio signals from audio client nodes in the shared locale D and provides the audio signals to the audio conferencing server . In some embodiments the audio client aggregation node organizes the multiple full duplex audio streams from the audio client nodes in the shared locale D and provides a single full duplex audio stream to the audio conferencing server . The audio client aggregation node also receives a mixed audio stream from the audio conferencing server and transmits the mixed audio stream over the data network to the audio client nodes in the shared locale D. In particular the audio client aggregation node provides to each audio client the audio stream from all other conference participants except the audio client s own audio stream.

In some embodiments the cloud based video audio conferencing system of the present invention may be implemented using a hybrid aggregation node configuration. For example the cloud based video audio conferencing system may be configured using a selected audio client node as the aggregation node in one locale and using a dedicated audio client aggregation node for other locales. In another example the cloud based video audio conferencing system may be configured using the cloud based audio client aggregation node for some locales and dedicated or selected audio client aggregation node for other locales. In this manner the cloud based video audio conferencing system realizes flexibility by allowing different locales to implement suitable methods for assigning the audio client aggregation node.

With an audio client aggregation node thus selected the audio client aggregation node accepts connections from audio client nodes at the same locale. In most cases the audio client nodes and the aggregation node are on the same local area network and the audio client nodes connect to the aggregation node through the local area network either through a wired connection or a wireless connection. When a cloud based audio client aggregation node is used the audio client nodes may connect to the cloud based aggregation node through a data network which may be the publicly accessible Internet. The cloud based video audio conferencing system of the present invention is agnostic to the network environment in which it is implemented. In some embodiments the cloud based video audio conferencing system is preferably implemented in a data network having low latency or capable of providing stable network connections. That is the cloud based video audio conferencing system is preferably implemented in a data network where the maximum network latency and the minimum network latency is narrow.

Another salient feature of the conferencing system of the present invention is that the audio client aggregation node performs synchronization of the microphone and the speaker audio signals of all of the local audio client nodes. The microphone and speaker synchronization makes it possible for multiple active devices at the same locale to connect to the same conference call. The conferencing system can thus provide a high quality experience for a shared locale multi device conference call.

More specifically when the speakers of the participant devices in the shared locale are tightly synchronized those devices acoustic echo cancellation AEC software will function acceptably as the other speaker signals in the room would be seen as early room reflections of the farend signal that each device s AEC will cancel from its nearend signal. Early room reflections are signals that are within the tolerances of the AEC and conventional AEC operations are designed to suppress such reflection signals. The in room experience of the shared locale is improved when all participant devices speakers in the room are synchronized resulting in all participants being able to effectively hear the in room conference audio. More importantly aligning the speakers in the shared locale solves the echo feedback problems described above. The remote participant s conference experience is also improved by the better intelligibility provided by a synchronized mix of all the microphones present in the shared locale.

With the detection of two or more audio client connections at the same physical location the method then organizes the audio client nodes at the same physical location . The method selects an audio client aggregation node for the physical location to aggregate the locally generated audio signals. In the present embodiment one audio client node is selected as the audio client aggregation node. In some embodiments the selection of the audio client node to function as the aggregation node is dynamic in nature and one or more factors can be used to determine which audio clients should be selected to perform the role of the aggregation node. Once selected all other audio client nodes connect to the aggregation node and the aggregation node collects the audio streams from all other audio clients at the same locale.

In other embodiments instead of selecting one audio client node as the audio client aggregation node the method may select a dedicated device in the locale as the audio client aggregation node. In yet another embodiment the method may select a cloud based audio client aggregation node such as one associated with the conferencing server.

With the audio client aggregation node selected the method synchronizes the microphone audio signals of all of the audio clients at the same locale . In one embodiment the best microphone signal can be selected as the reference. The method further synchronizes the speaker audio signals of all of the audio clients at the same locale . In practice the synchronization of the microphone audio signals and the speaker audio signals can be performed at the same time or in any order. The specific order of speaker and microphone audio signal synchronization shown in is illustrative only and not intended to be limiting.

The method is capable of providing optimal user experience in a conference call by allowing users at the same locale to use individual devices to connect to the same conference call. The method organizes and synchronizes the incoming and outgoing full duplex audio streams so that undesired echo and feedback are prevented.

The operation of the audio client network processing loop will be described with audio signals being received at the microphone of the host device. The incoming audio signal is digitized by an analog to digital converter ADC on the host device. The digitized audio signal is then provided to the host operation system audio application programming interface Host OS Audio API and enters the audio API input device thread process. Meanwhile the audio client has registered a call back with the host operating system. For example the audio client is configured to request microphone samples from the audio API at a given sample rate.

When digitized audio signals or digitized audio samples are provided to the audio client the audio client performs digital signal processing input signal processing such as echo cancellation noise suppression and filtering. The processed audio samples are then put into an input ring buffer . The input ring buffer has a write head for receiving and storing incoming audio samples and a read head for reading out stored audio samples. At a predetermined rate the audio client pull audio samples out of the input ring buffer and sends out the audio samples in the form of an audio client send stream through a network socket to the data network. The audio samples from the microphone is sometimes referred to as microphone data packets and the audio client send stream is also referred to as a microphone packet stream . In the case the host device is not in a shared locale the audio client sends the microphone data packets to the audio conferencing server . In the case the host device is in a shared locale the audio client sends the microphone data packets to the audio client aggregation node .

Audio client also processes incoming audio data packets received either from the audio conferencing server or the audio client aggregation node that are to be played out on the speaker of the host device. At the host device incoming audio data packets are received by the network socket . The audio client receives incoming audio data packets from the network socket in the form of an audio client receive stream . The audio client receive stream is also referred to as a speaker packet stream. The audio client stores the incoming audio data packets also referred to as speaker data packets into a jitter buffer . Jitter buffer is used to compensate for network latency so as to create a seemingly continuous media stream for the speaker. The audio client retrieves audio samples from the jitter buffer at a periodic rate and places the audio samples into an output ring buffer . The Host OS Audio API pulls the audio samples out of the output ring buffer processes the signals output device signal processing and sends the processed audio signals to a digital to analog converter DAC to generate the analog audio signals that are to be played out on the speaker .

As thus implemented audio client processes outgoing microphone data packets to be sent out over the data network and processes incoming speaker data packets received from the data network. There are numerous delays or latency associated with the audio signal processing operation. For example the speaker and the microphone will each have its own hardware latency. The host OS audio API has an associated API driver latency. The input and output signal processing stages have associated signal processing latency. The input and output ring buffers have associated ring buffer latency. The jitter buffer introduces jitter buffer latency for the audio client receive stream. And finally the network socket has associated operating system network stack latency. The latency and the varying amount of latency between different host devices render it impossible for conference participants to use multiple active devices in the same locale to connect to the same conference call unless the microphone and speaker data is aligned.

The operation of the audio client aggregation node aggregation node will be described with one or more inbound microphone packet streams being received from the network sockets associated with one or more audio client nodes running on host devices at a shared locale where the audio client nodes are connected to the aggregation node .

In the present embodiment the aggregation node is configured to receive inbound microphone data packets from four shared locale audio clients. In the present example the aggregation node is configured on the host device of audio client node . Accordingly audio client nodes are connected to the aggregation node through a local area network to provide their microphone packet streams and audio client node provides its microphone packet stream directly to the aggregation node .

With inbound microphone packet streams being received from multiple audio client nodes in the shared locale the aggregation node stores the inbound microphone data packets into respective jitter buffers . Jitter buffers are used to compensate for the variable latency between the several microphone packet streams. The inbound microphone packet stream from the associated audio client node does not need a jitter buffer as the audio packets do not have to travel through a network to reach the aggregation node . Instead the audio packets from local audio client node can be injected into the aggregation node processing directly.

In embodiments of the present invention the aggregation node performs microphone alignment by introducing a delay to each microphone packet stream that is specific to the audio client node that generated the microphone packet stream. To that end the aggregation node includes a set of input delay lines one delay line for each audio client node connected to the aggregation node including the audio client associated with the aggregation node. Accordingly the aggregation node includes an input delay line for each microphone packet stream received by the aggregation node. In the present example four input delay lines are provided for audio client nodes . The input delay lines are controlled by an input signal aligner which is configured to generate delay lines adjustment values for each of the input delay lines . The amount of microphone delay applied to each delay line is calculated based on an audio timestamp for each audio client derived from a common timebase to align the inbound audio signals from multiple audio clients.

In operation the aggregation node pulls microphone data packets out of each jitter buffer and from the local audio client node audio client and processes the microphone data packets through the respective delay line . The delay lines introduce delays that are specific to each audio client node to each inbound microphone packet stream. The microphone data packets for each inbound microphone packet stream go through its own delay line . With each delay line applying the audio client specific delay the microphone signals from all the audio clients will be lined up after the delay lines. The aligned microphone signals are mixed together by mixer to form a mixed audio data packet containing the audio signals of all of the audio clients connected to the aggregation node . The mixed audio data packets form an outbound microphone packet stream . The outbound microphone packet stream from the aggregation node is then provided through the network socket to the audio conferencing server.

Meanwhile the network socket of the aggregation node receives inbound speaker packet stream from the audio conferencing server destined to the locale associated with the aggregation node. The inbound speaker packet stream contains inbound speaker audio signals being a mix of the audio signals of all of the conference participants. The inbound speaker audio signals are coupled to a splitter to produce copies of the inbound speaker audio signals for each audio client. The splitter produces individual speaker data packets destined for each audio client node for example audio client nodes . In particular each audio client node receives speaker audio signals from all other conference participants. That is each audio client receives the speaker audio signals from all conference participants except the audio client s own speaker audio signal. That is the audio signals received from the microphone associated with the audio client are not played out onto the speaker of the same audio client.

The separated speaker data packets for the audio client nodes are aligned when received from the audio conferencing server. However the latency through each audio client node is different. Therefore if the aligned speaker data packets are played out on the speakers of each audio client node the output sound will be mumbled as the audio signals played out by each speaker will not be aligned due to the differences in each host device s individual latency.

Accordingly the aggregation node dealigns the separated speaker data packets so that the speaker data packets will sound aligned when play out on the individual speakers of the host device of each audio client node. In embodiments of the present invention the aggregation node performs speaker alignment by introducing a delay to each speaker data packet that is specific to the audio client node to which the speaker data packet is destined. To that end the aggregation node includes a set of output delay lines one delay line for each audio client node connected to the aggregation node including the audio client associated with the aggregation node. Accordingly the aggregation node includes an output delay line for each outbound speaker packet stream to be generated by the aggregation node. In the present example four output delay lines are provided for audio client nodes . The output delay lines are controlled by an output signal aligner which is configured to generate delay lines adjustment values for each of the output delay lines . The amount of delay applied to each delay line is based on the playout delay associated with each audio client as calculated based on each audio client.

In operation the aggregation node receives the inbound speaker packet stream and generates separated speaker data packets destined for each audio client. The aggregation node processes the separated speaker data packets for each audio client through the respective output delay line . The delay lines introduce delays that are specific to each audio client node to each outbound speaker data packet. The speaker data packets for each audio client node go through its own delay line . With each delay line applying the audio client specific playout delay the speaker signals for all the audio clients will become dealigned. The network socket pulls the delay adjusted speaker data packets from the delay lines and sends the speaker data packets to the respective audio client as the audio client receive stream. The delay adjusted speaker data packets form the outbound speaker packet stream . The outbound speaker packet stream are then provided through the network socket to the respective audio client nodes.

In embodiments of the present invention the cloud based video audio conferencing system and method of the present invention is realized by synchronizing or aligning the microphone audio signals and speaker audio signals of conference participant devices at the same locale. Various methods for microphone signal alignment and speaker signal alignment are possible. Example microphone signal alignment and speaker signal alignment methods are described below. The description below is illustrative only and not intended to be limiting. Other methods for performing microphone alignment or synchronization and speaker alignment or synchronization are possible.

The microphone signal alignment and speaker signal alignment methods will be described with reference to . illustrate the operation of the input signal aligner and the output signal aligner respectively in embodiments of the present invention. illustrates an aggregation node inbound microphone data packet in embodiments of the present invention. illustrates an aggregation node outbound speaker audio packet in embodiments of the present invention.

In embodiments of the present invention the audio clients microphone audio packets are timestamped by the respective host OS audio API. The timing information is used by the input signal aligner at the aggregation node to align the audio clients microphone audio streams. In particular by estimating the latency contributed by the host device s audio hardware driver and the audio client signal processing the conferencing system and method of the present invention derives a relatively accurate estimation of when the current audio samples were produced in the real world and captured by the host device ADC for example ADC in . However the audio client s audio timestamps are expressed in each host device s local clock timebase audio client timebase . In order to align the audio streams relative to one another the audio timestamps based on the audio client timebase are first translated to a common timebase.

In embodiments of the present invention the audio client aggregation node provides the common timebase for aligning the microphone audio streams from all the audio client nodes. The audio client aggregation node accepts connections from other audio client devices in the room and uses the audio client timestamp information provided in the audio clients microphone data packets to provide the aligned mix of all the microphone audio signals in the shared locale to the remote audio conferencing server . A clock offset for each audio client relative to the aggregation node clock is calculated using clock sync meta information attached to each microphone data packet as shown in . The aggregation node applies the clock offset to each audio client audio timestamp. In this manner the clock of the aggregation node becomes the common timebase that is used to align all connected audio client microphone packet streams.

More specifically referring to the aggregation node sends its own local time the Server Ping Time in the aggregation node outbound speaker data packets going to each of the audio clients in the shared locale. The Server Ping Time TH is based on the aggregation node timebase. The Server Ping Time is echoed back to the aggregation node on the inbound microphone data packets from the audio clients of the shared locale. In particular referring to the inbound microphone data packet includes clock sync meta information which includes the Server Ping Time Echo TC which is the Server Ping time in a speaker data packet received from the aggregation node and to be echoed back to the aggregation node. The clock sync meta information further includes the audio client s local time at the time of packet departure Packet Departure Timestamp which is based on the audio client timebase. When the inbound microphone data packet arrives at the aggregation node the Packet Arrival Timestamp which is based on the aggregation node timebase is inserted into the clock sync meta information.

With the Server Ping Time Echo the Packet Departure Timestamp and the Packet Arrival Timestamp thus provided in the clock sync meta info the input signal aligner can then calculate the clock offset for each audio client node. In one embodiment a variant of Cristian s algorithm is used to calculate the clock offset. The accuracy of Cristian s algorithm is inversely correlated to the network Round Trip Time RTT between the computers whose clocks are being synchronized. The fact that the shared locale audio client nodes communicate with the aggregation node via the Local Area Network LAN enables the Cristian s algorithm to compute clock offsets which are accurate enough for the alignment purposes since these RTTs will generally be much lower and exhibit more stability than if the audio clients and the aggregation node were communicating over the Internet. The calculated clock offset for each audio client is then used to translate the audio timestamp for each audio client from the audio client timebase to the common timebase of the aggregation node.

Once a stable clock offset is calculated for a particular audio client the aggregation node is able to use the subsequent clock sync meta information transmitted in the microphone data packets to identify and correct for the inevitable clock drift that will happen between two different physical devices. In one embodiment a threshold based technique is used to resynchronize the audio client clocks which are drifting. In other embodiments the magnitude of the drift can be calculated and corrected by resampling the audio client microphone stream to match the aggregation node s clock.

In some embodiments the conferencing system and method also implements corrections for timestamp jitter. Audio devices often provide jittery timestamp information. Timestamp jitter is expected as the conferencing system is not running on a real time operating system and is subject to all sorts of scheduling jitter occurring within the host device s operating system. In some embodiments the conferencing system and method applies one or more techniques to smooth out the timestamp jitter to obtain timestamp values that are more meaningful and usable. In some embodiments techniques that can be applied to smooth out the timestamp jitter include a rolling linear regression Kalman filtering and a second order delay locked loop.

Referring still to the microphone data packet further includes alignment meta information which includes the client audio timestamp based on the client timebase and the audio client playout delay as calculated on the audio client. The client playout delay is used for speaker alignment and will be described in more detail below. The aggregation node translates the client audio timestamp to the common timebase by applying the respective clock offset for each audio client audio timestamp.

At the service loop iteration of the audio client aggregation node the input signal aligner compares the audio timestamps of all the audio client microphone packet streams as shown in . The audio timestamps have been translated to the common timebase so that they can be accurately compared. The audio client with the oldest timestamp is elected as the reference signal. In the present example the audio timestamp of audio client is selected as the reference signal. The input signal aligner generates input delay lines adjustment values for the delay lines based on the difference between the audio client timestamp and the reference timestamp that is each input delay line is adjusted by Client timestamp Reference timestamp . The delay lines adjustment values are provided to the respective delay lines . Once the current timestamp of the reference signal is determined the delays of the remaining audio client microphone packet streams are adjusted to bring them into alignment with each other and relative to the reference timestamp.

In the present description the aggregation node service loop refers to the thread running on the processor of the aggregation node to handle the operation of reading incoming audio packets from the network processing incoming audio packets producing and processing outgoing audio packets and sending outgoing audio packets to the network.

For example if an audio signal A from an audio client has a timestamp of 120 and the reference signal has a timestamp of 100 the audio signal A is assumed to be 20 units ahead of the reference signal and the delay line for audio signal A is adjusted to add an additional 20 units of delay thus bringing signal A into alignment with the reference signal. The same is done for the remaining audio clients.

In embodiments of the present invention the audio client aggregation node also synchronizes all the speaker packet streams for the shared locale audio clients connected to the aggregation node. The aggregation node receives speaker audio data packets from the remote audio conferencing server and constructs properly aligned or properly dealigned signals for each audio client connection to provide synchronized playback of the conference call within the shared locale.

In one embodiment as shown in each shared locale audio client reports its local playout delay back to the audio client aggregation node in its outgoing microphone data packets as the alignment meta info. In the present description the playout delay is defined as the total time elapsed from the time the local speaker audio data packet stream arrives at the network interface network socket of the audio client s host device until the speaker audio data packet leaves the Digital to Analog Converter DAC of the host device. In one embodiment referring back to the playout delay of an audio client is calculated by propagating the speaker data packet s arrival time at the Network Interface Card NIC network socket through the jitter buffer and into the output ring buffer serviced by the host device s audio API . The time elapsed from the speaker data packet arrival time until the speaker data packet is requested by the host audio API can be considered the playout delay. By taking into account the driver and hardware latencies of the output device an accurate assessment can be obtained of how long it takes for audio samples to hit the air from the time the audio samples arrive at the network interface card network socket .

In some embodiments the playout delay is computed as the time duration from the speaker data packet arriving at the audio client and the speaker data packet being played out on the speaker. Referring back to the arrival time Tof the speaker data packet at the network socket is known. The speaker data packet propagates through the jitter buffer and the ring buffer and at time T the host device s audio API requests the speaker data packet for playout. Then the speaker data packet is provided to the host device DAC to be played out on the speaker where the actual time for the speaker to play out the audio signal is time duration T. In the present embodiment the playout delay is given as T T T. The playout delay calculated by each audio client is inserted in the alignment meta information of the microphone data packet of the audio client to be sent up to the aggregation node. In this manner the aggregation node has the playout delay information for all of the audio clients.

During the service loop iteration of the audio client aggregation node the speaker playout delays reported by the connected audio clients are compared. In the present embodiment the output signal aligner compares the playout delay of all the audio clients as shown in . The audio client reporting the longest playout delay is elected as the reference signal for speaker alignment. In the present example the playout delay of audio client is selected as the reference signal. The output signal aligner generates output delay lines adjustment values for the delay lines based on the difference between the reference playout delay and the audio client playout delay that is each output delay line is adjusted by Reference playout delay Client playout delay . The delay lines adjustment values are provided to the respective delay lines .

When speaker data packets are received from the remote audio conferencing server prior to sending the speaker data packet to each of the connected audio clients the audio client aggregation node writes a copy of the speaker data packet to a delay line per audio client e.g. delay lines of . Alternately the aggregation node produces copies of the speaker data packet per audio client containing the audio signals for all other conference participant except the audio client s own audio signal. After that each audio client s delay line is adjusted to bring the speaker data packet into alignment with the reference signal.

For instance if client A reports a playout delay of 50 units and the reference signal has a playout delay of 100 units a delay of 50 units is added to client A s delay line. As a result of the delay line adjustment a set of seemingly dealigned speaker data packets is generated. The aggregation node produces the dealigned speaker data packets from each audio client s delay line and sends the dealigned speaker data packets via the LAN to each audio client of the shared locale. In this manner when the dealigned speaker data packets are played out by the speakers of the host devices in the shared locale the intentionally dealigned speaker signals will sound aligned on the play out due to the varying playout delay of each host device.

As described above in the speaker data packet the aggregation node also attaches its own local time Server Ping Time at the time of packet departure to enable the clock synchronization which takes place in the microphone stream processing. In this manner timestamp information for performing clock synchronization between the aggregation node and all the audio clients is piggybacked onto the audio streams that travels between the audio clients and the aggregation node. Conventional clock synchronization scheme often requires dummy data packets sent from the host to ping the client and to receive a response to calculate the time offset. In embodiments of the present invention the timestamp information for clock synchronization is appended to the audio data packets to simplify the clock synchronization operation.

In the above described embodiments synchronization of the microphone signals of multiple shared locale audio clients is achieved using a timestamp based method and synchronization of the speaker signals of multiple shared locale audio clients is achieved using the playout delay associated with the host device. As described above the playout delay is the delay time from the remote conferencing server to the speaker and the playout delay is measured and adjusted centrally.

In other embodiments other methods to perform audio synchronization of the microphone signals and the speaker signals at the shared locale can be used. In some embodiments the microphone signals can be aligned using a playup delay. The playup delay refers to the delay time from the microphone to the remote conferencing server. The playup delay is measured and reported to the aggregation node where the aggregation node aligns the microphone packet streams of the shared locale audio clients based on the reported playup delay information. In other embodiments the speaker signals can be aligned using a timestamp method. Each speaker packet is attached with a play at timestamp generated by the audio client aggregation node. Each speaker receiving a speaker packet with a play at field will adjust the playout delay of the speaker signals according to the play at timestamp so that all the speaker signals will be played out at the play at time specified by the aggregation node. Various methods for audio signal alignment can be used without departing from the spirit of the present invention. The same locale multiple device conferencing method of the present invention performs audio alignment or synchronization to realize an improved conference experience by enabling participants in a shared locale to use individual devices to connect to the conference call.

In embodiments of the present invention the same locale multiple device conferencing method implements various methods to detect for multiple connections at the same physical location. In particular a share locale detection and connection method is implemented at each audio client and determines when a particular audio client joining a conference call is in the same locale as other audio clients already connected to the conference call. The audio client may join a conference call and determine that it is in the same locale as an existing audio client aggregation node. The audio client may then be connected to the existing audio client aggregation node. Alternately the audio client may be selected to become an audio client aggregation node itself and begin accepting connections from other audio clients in the same locale. The share locale detection method is implemented to provide a seamless user experience to the conference participant joining the conference call. In some embodiments the share locale detection method operates automatically to connect participants in a shared locale to a conference call so that the participants are not aware of whether or not they are connected directly to the remote audio conferencing server or to an aggregation node brokering their interaction with the remote audio conferencing server on their behalf.

In some embodiments every audio client node that connects to the remote audio server can be assigned as an aggregation node. An audio client wishing to participate in a locale aggregated conference call will connect to the remote audio conferencing server requesting to join the conference call. In one embodiment the audio client is then provided with a list of all aggregation nodes currently on the conference call. As the list of aggregation nodes changes the audio clients may be notified of the changes. When an audio client wishes to opt in to the conference call as a locale aggregate the audio client loops through the list of audio client aggregation node candidates concurrently available and attempts to connect to one of the aggregation node. In this case multiple aggregation nodes may exist at the same location.

In some embodiments the remote conferencing server or an assigned aggregation node may broadcast a message informing all of the same locale audio clients of the identity of the aggregation node. The audio clients when attempting to connect to the conference call can then be directed to the identified aggregation node either manually or automatically. Alternately the remote conferencing server may redirect the audio client to the respective aggregation node.

In another embodiment the local audio client aggregation node produces a super audible sound signature unique to the locale and assigned to the aggregation node when the aggregation node connects to the remote audio conferencing server. When an audio client requests connection to a conference call the audio conferencing server can discern the locale by previewing the microphone stream of the connecting audio client and detecting for the super audible sound signature. The server compares the received microphone stream to the known sound signatures associated with one or more aggregation nodes and directs the audio client to connect to the aggregation node assigned with the same sound signature detected from the audio client s microphone. In this case the remote audio conferencing server is able to prevent multiple aggregation nodes from being selected at the same locale.

In yet another embodiment the remote audio conferencing server identifies a locale by sampling over a preview period the background noise signature received from each audio client requesting to join a conference call. In some embodiments the remote audio conferencing server may generate an Acoustic Background Spectrum of each audio client connecting to the conference call. The Acoustic Background Spectrum of each audio client can be thought of as a room fingerprint and would enable the remote audio conferencing server to identify if two or more audio clients may be in the same locale. In this manner the remote audio conferencing server may intelligently aggregate audio clients that have the same room fingerprint. The remote audio conferencing server may select an aggregation node as needed to build the ideal set of aggregated and unaggregated locales for the conference call. In other embodiments the Acoustic Background Spectrum can be used together with other heuristics to make the determination of a shared locale presence. For example the remote audio conferencing server can determine if one or more audio clients are on the same local area network. Alternately the remote audio conferencing server can determine the shared locale presence by using geodata from user devices to discern if two or more user devices are physically close to each other.

Finally in some embodiments the remote audio conferencing server can act as the centralized control to organize the audio clients at a shared locale. All audio client nodes connect to the remote audio conferencing server initially. The remote audio conferencing server determines that one or more audio clients are in fact in a shared locale and the remote audio conferencing server can then redirect the shared local audio clients to connect to the aggregation node assigned to the locale.

In embodiments of the present invention the selection of a user device as the aggregation node in a share locale conference call can be made using one or more factors. In one embodiment the remote conferencing server selects an audio client node with the best processor in terms of speed or capability. In another embodiment the remote conferencing server selects the audio client of the conference call organizer as the aggregation node. In other embodiments the remote conferencing server selects the audio client of a desktop computer over a mobile device to function as the aggregation node.

The cloud based video audio conferencing system of the present invention realizes many advantages over conventional cloud based conferencing systems. First the cloud based video audio conferencing system of the present invention enables multiple active devices to be used in a shared locale to connect to the same conference call. In the manner the user experience at the conference call is greatly improved. Second the cloud based video audio conferencing system of the present invention performs full duplex synchronization of the microphone and speaker signals. The full duplex synchronization enables the use of multiple user devices at the same locale for the same conference call. Lastly the cloud based video audio conferencing system of the present invention is capable of replicating the quality of a high send video conferencing system by using consumer devices.

Although the foregoing embodiments have been described in some detail for purposes of clarity of understanding the invention is not limited to the details provided. There are many alternative ways of implementing the invention. The disclosed embodiments are illustrative and not restrictive.

