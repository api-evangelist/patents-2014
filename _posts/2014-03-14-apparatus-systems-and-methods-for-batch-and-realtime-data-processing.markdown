---

title: Apparatus, systems, and methods for batch and realtime data processing
abstract: A traditional data processing system is configured to process input data either in batch or in real-time. On one hand, a batch data processing system is limiting because the batch data processing often cannot take into account any data received during the batch data processing. On the other hand, a real-time data processing system is limiting because the real-time system often cannot scale. The real-time data processing system is often limited to dealing with primitive data types and/or a small amount of data. Therefore, it is desirable to address the limitations of the batch data processing system and the real-time data processing system by combining the benefits of the batch data processing system and the real-time data processing system into a single data processing system.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09317541&OS=09317541&RS=09317541
owner: Factual Inc.
number: 09317541
owner_city: Los Angeles
owner_country: US
publication_date: 20140314
---
The entire content of each of the above referenced applications including both the provisional applications and the non provisional applications is herein incorporated by reference.

The present disclosure generally relates to data processing systems and specifically to data processing systems that can process data using batch processing and real time processing.

The system disclosed herein relates to receiving processing and storing data from many sources representing the most correct summary of facts and opinions from the data including being able to re compute this in real time and then using the results to respond to queries. As an example when a user inputs a query to a web based system mobile phone or vehicle navigation system searching for a child friendly Chinese restaurant in Greenwich Village that has valet parking the system can very quickly respond with a list of restaurants matching for example the attributes kid friendly true category Restaurant Chinese valet parking true neighborhood Greenwich Village . A mobile phone may then provide a button to call each restaurant. The information describing each restaurant may be spread across many websites sourced from many data stores and provided directly by users of the system.

A problem in the art is that all web pages references and data about all known businesses in the United States stored in any data store can be so large as to not be understandable and query able in real time. Updating and maintaining such a large amount of information can be difficult. For example information describing businesses in the United States has more than billions of rows of input data tens of billions of facts and tens of terabytes of web content.

At the same time new information is continuously becoming available and it is desirable to include such information in the production of query results. As an example the system may learn that a restaurant no longer offers valet parking that the restaurant disallows children or that the restaurant s phone number has been disconnected.

Accordingly it is desirable to be able to update a system that produces search results both on an ongoing basis e.g. to account for newly written reviews as well as on a whole sale basis e.g. to reevaluate the entire data and use information contained within that may have been previously unusable .

The disclosed system is configured to receive new data e.g. real time updates and old data e.g. data that has been gathered over a long period of time and is configured to periodically update the system based on a reevaluation of the new data and the old data.

Unlike systems that can only search web pages and return links to matching web pages the disclosed system can maintain attribute information about each entity and return the information directly. In conventional systems a web search for Chinese restaurant with valet parking for example can return links to web pages with the words Chinese restaurant valet and parking . This will generally include pages with statements like there was no valet parking to be found because the words valet and parking appeared in the text and were therefore indexed as keywords for the web page. By contrast the disclosed system has attributes such as the category of the restaurant and a value indicating whether the restaurant offers valet parking which advantageously allow the system to respond with more meaningful results. Additionally in the disclosed system a user can operate as a contributor to correct the data. Also the disclosed system can interpret facts across many web pages and arrive at a consensus answer which is then query able to further improve results.

In embodiments of the disclosed system a user can operate as a contributor that contributes data to the system. For example a user may provide direct feedback to the system to correct a fact. Multiple such submissions can be considered together by the disclosed system along with information on websites such as blogs about child friendly restaurants and summarized into a rapidly evolving data store that can quickly respond to queries. Therefore users of the disclosed system can access the newly corrected phone number and a more accurate assessment of its child friendliness.

In some embodiments the disclosed system can improve or expand the analytic methods for understanding information on web pages or feedback. The analytic methods can be improved or expanded on an ongoing basis. For example today s methods may be capable of extracting more information from a data source compared to the last month s methods. If one web page includes facts as simple text while another has opinions in complex prose the disclosed system using the last month s method may have been able to process simple text data such as Valet Parking yes but have been unable to process prose such as There was no place to park not even valet. However the disclosed system using the today s method may have expanded capability and be able to process the more nuanced prose data.

In general in an aspect embodiments of the disclosed subject matter can include a computing system for generating a summary data of a set of data. The computing system can include one or more processors configured to run one or more modules stored in non tangible computer readable medium. The one or more modules are operable to receive a first set of data and a second set of data wherein the first set of data comprises a larger number of data items compared to the second set of data process the first set of data to format the first set of data into a first structured set of data generate a first summary data using the first structured set of data by operating rules for summarizing the first structured set of data and store the first summary data in a data store process the second set of data to format the second set of data into a second structured set of data generate a second summary data based on the first structured set of data and the second structured set of data by operating rules for summarizing the first structured set of data and the second structured set of data determine a difference between the first summary data and the second summary data and update the data store based on the difference between the first summary data and the second summary data.

In general in an aspect embodiments of the disclosed subject matter can include a method for generating a summary data of a set of data. The method can include receiving at an input module operating on a processor of a computing system a first set of data and a second set of data wherein the first set of data comprises a larger number of data items compared to the second set of data processing at a first input processing module of the computing system the first set of data to format the first set of data into a first structured set of data generating at a first summary generation module of the computing system a first summary data using the first structured set of data by operating rules for summarizing the first structured set of data maintaining the first summary data in a data store in the computing system processing at a second input processing module of the computing system the second set of data to format the second set of data into a second structured set of data generating at a second summary generation module of the computing system a second summary data using the first structured set of data and the second structured set of data by operating rules for summarizing the first structured set of data and the second structured set of data determining at a difference generation module of the computing system a difference between the first summary data and the second summary data and updating by the computing system the data store based on the difference between the first summary data and the second summary data.

In general in an aspect embodiments of the disclosed subject matter can include a computer program product tangibly embodied in a non transitory computer readable storage medium. The computer program product includes instructions operable to cause a data processing system to receive a first set of data and a second set of data wherein the first set of data comprises a larger number of data items compared to the second set of data process the first set of data to format the first set of data into a first structured set of data generate a first summary data using the first structured set of data by operating rules for summarizing the first structured set of data and store the first summary data in a data store process the second set of data to format the second set of data into a second structured set of data generate a second summary data using the first structured set of data and the second structured set of data by operating rules for summarizing the first structured set of data and the second structured set of data determine a difference between the first summary data and the second summary data and update the data store based on the difference between the first summary data and the second summary data.

In any one of the embodiments disclosed herein the second set of data comprises real time data submissions and the one or more modules are operable to process the second set of data to format the second set of data into the second structured set of data in response to receiving the second set of data.

In any one of the embodiments disclosed herein the computing system the method or the computer program product can include modules steps or executable instructions for processing the first set of data to format the first set of data into the first structured set of data at a first time interval which is substantially longer than a second time interval at which the second set of data is formatted into the second structured set of data.

In any one of the embodiments disclosed herein each of the first summary data and the second summary data comprises an entity identifier and a value associated with the entity identifier and wherein the computing system the method or the computer program product can further include modules steps or executable instructions for determining the difference between the first summary data and the second summary data by determining that the first summary data and the second summary data include an identical entity identifier and comparing values associated with the identical entity identifiers in the first summary data and the second summary data.

In any one of the embodiments disclosed herein the computing system the method or the computer program product can include modules steps or executable instructions for providing the difference between the first summary data and the second summary data to other authorized computing systems.

In any one of the embodiments disclosed herein the computing system the method or the computer program product can include modules steps or executable instructions for providing the difference to other authorized computing systems via an application programming interface.

In any one of the embodiments disclosed herein the computing system the method or the computer program product can include modules steps or executable instructions for providing the difference to other authorized computing systems as a file.

In any one of the embodiments disclosed herein the computing system the method or the computer program product can include modules steps or executable instructions for combining at least the first set of data and the second set of data to generate a third set of data processing the third set of data to format the third set of data into a third structured set of data based on new rules for formatting a set of data and generating a third summary data using the third structured set of data.

In any one of the embodiments disclosed herein the first set of data and the third set of data each includes a first data element and wherein the first data element is associated with a first entity in the first summary data identified by the first entity identifier wherein the first data element is associated with a second entity in the third summary data and wherein the computing system the method or the computer program product can further include modules steps or executable instructions for associating the first entity identifier to the second entity in the third summary data so that the first data element maintains its association with the first entity identifier in the third summary data.

In any one of the embodiments disclosed herein the first structured set of data comprises a grouping of data items based on an entity identifier associated with the data items.

In any one of the embodiments disclosed herein the computing system comprises at least one server in a data center.

In any one of the embodiments disclosed herein the data store comprises a plurality of data store systems each of which is associated with a view and wherein the one or more modules are operable to select one of the plurality of data store systems in response to a query based on the view associated with the query.

In any one of the embodiments disclosed herein the computing system the method or the computer program product can include modules steps or executable instructions for identifying a third set of data received after the generation of the second summary data generating a third summary data based on the third set of data the first structured set of data and the second structured set of data by operating rules for summarizing the first structured set of data the second structured set of data and the third summary data determining a difference between the second summary data and the third summary data and updating the data store based on the difference between the second summary data and the third summary data.

A traditional data processing system is configured to process input data either in batch or in real time. On one hand a batch data processing system is limiting because the batch data processing cannot take into account any additional data received during the batch data processing. On the other hand a real time data processing system is limiting because the real time system cannot scale. The real time data processing system is often limited to dealing with primitive data types and or a small amount of data. Therefore it is desirable to address the limitations of the batch data processing system and the real time data processing system by combining the benefits of the batch data processing system and the real time data processing system into a single system.

It is hard for a system to accommodate both a real time processing and a batch processing because the data and or processes for a real time processing and a batch processing are quite different. For example in a batch processing system a program cannot access the data processing result until the entire data process is complete whereas in a real time processing system a program can access the processing result during the data processing.

The disclosed data processing apparatus systems and methods can address the challenges in integrating the batch data processing system and the real time processing system.

Some embodiments of the disclosed system can be configured to process unstructured data and convert the unstructured data into a summary data. The summary data can be stored in one or more data stores including for example one or more data storages and or one or more databases or one or more search servers and can be formatted and optionally indexed to be query able using one or more data stores or one or more search servers or using an application programming interface API by a third party user.

The summary data can include one or more unique entities and at least one attribute about those entities. One of the attributes about an entity can be an entity identifier that is unique amongst the entities. Additional attributes describe some properties of the entity such as a Boolean value e.g. whether restaurant A has valet parking is True or False an integer a string a set of characters binary data e.g. bytes representing an image or arrays or sets of these types or any other combinations thereof.

Some embodiments of the disclosed system can be configured to generate the summary data based on two types of data inputs a bulk data input and an intermittent data input. The bulk data input can refer to a large amount of data that has been gathered over time. In some cases the bulk data input can refer to all data that the disclosed system has received over a predetermined period of time which can be long. For example the bulk data input can include raw information received from multiple contributors or from a web crawler over a long period of time. In some embodiments the bulk data can be maintained in the disclosed system itself in other embodiments the bulk data can be received from another storage center via a communication interface. The intermittent data input can include a small amount of data that is provided to the disclosed system. The intermittent data input can include for example real time data submissions from contributors.

Some embodiments of the disclosed system can be configured to process both types of data inputs using a common processing framework. The common processing framework can include a real time system that can respond to the intermittent data input e.g. small incremental contributions from contributors and reflect changes based on those contributions in consideration along with data from the batch system in the summary data in substantially real time. The common professing framework can also include a batch system that can process the bulk data input. The batch system can be configured to format the bulk data to be amenable for further processing and use the formatted bulk data to generate summary data.

In some embodiments the batch processing system is configured to generate summary data by formatting the unstructured data in the bulk data inputs into a structured data. Then the batch system is configured to group the elements in the structured data and generate a representative identifier for each group also referred to as an entity. The batch system can then generate an identifier for each entity and calculate attribute values describing each entity.

For example when the large bulk data input includes 5 data elements associated with an existence of valet parking at a restaurant A then the batch system can determine that those 5 data elements belong to the same entity e.g. restaurant A and consolidate information associated with the 5 data elements. For instance if 3 data elements indicate that the restaurant A has valet parking and 2 data elements indicate that the restaurant A does not have valet parking then the batch system can consolidate the 5 elements and indicate that the attribute valet parking for entity restaurant A is True. This consolidation process is in some ways similar to the process disclosed in U.S. Patent Application Publication No. 2011 0066605 entitled PROCESSES AND SYSTEMS FOR COLLABORATIVE MANIPULATION OF DATA filed on Sep. 15 2009 which is herein incorporated by reference in its entirety.

In some embodiments an entity is a distinct object that the system elects to track. For example the system can consider each physical restaurant e.g. a chain with multiple locations would have an entity for each location as a separate entity i.e. summary record when the system receives reviews about each physical restaurant as data input. Similarly when toothpaste from a particular brand comes in 3 sizes and 4 flavors for each size the system can maintain 12 distinct entities for the toothpaste.

In some embodiments the real time system can be configured to update the summary data generated by the batch system as the real time system receives intermittent data inputs from contributors. For example if the real time system receives two additional data inputs from the contributors both indicating that the restaurant A does not have valet parking then the real time system can update the summary data to indicate that the attribute valet parking for entity restaurant A is False. 

In some embodiments the real time system can be configured to leverage the structured data generated by the batch system. For example when the real time system receives intermittent data input from a contributor the real time system can consolidate the intermittent data input with the structured bulk data generated by the batch system. This way the amount of computation required by the real time system can be reduced.

In some embodiments the batch system can be configured to run periodically with a predetermined period. The batch system can be operated less frequently compared to the real time system since the amount of computation needed by the batch system is considerably larger compared to the amount of computation needed by the real time system. For example the batch system can be operated so as to update the system on schedules like once an hour once a week or once a month. The real time system can be operated more frequently than the batch system. For example the real time system can be configured to operate whenever the real time system receives an intermittent data input or on inputs buffered over a short time frame such as 5 seconds or 5 minutes. The batch system can be updated with new intelligence and rules over time and can process new data provided at a scale that is beyond the capacity of the real time system.

One aspect of the disclosed data processing system is the data lifecycle. A data can be categorized as one of following types as the data progresses through the disclosed data processing system Raw Unprocessed Data see Unprocessed Raw Inputs see QuickProcessed Inputs see QuickProcessed Summaries see FullProcessed Inputs see and FullProcessed Summaries see . These data can be stored in a non transitory computer readable medium. The non transitory computer readable medium can include one or more of a hard disk a flash memory device a dynamic random access memory DRAM a static random access memory SRAM or any combinations thereof.

Raw Unprocessed Data is a data that is in a raw unprocessed form. For example a web page about a restaurant might say somewhere in the content of the webpage has valet parking In this case the raw data is a copy of the entire web page. In the disclosed system an input of valet parking true could for example originate from a webpage that said has valet parking As an additional example the system may contain a data store of restaurants for example a data storage having restaurant related data and or a database having restaurant related data. Examples of unprocessed data can include 

In the disclosed system Raw Unprocessed Data can be maintained for reprocessing e.g. the raw data may be stored at periodic intervals so that it can be used for new runs of the batch processing system . This is advantageous because new rules which may be developed at a later time may be able to extract additional inputs when the raw data is reprocessed. For example a website may have text saying the valet scratched my car while parking it. Even if an earlier run that evaluated content on the website failed to form any inputs about valet parking a subsequent run may extract an input of valet parking true. Because the disclosed system can store the raw data for reprocessing a batch process of the disclosed system can be rerun against raw data and a new rule that understands the more complex statement could for example extract an input of valet parking true on a subsequent run.

Unprocessed Inputs represent the original attribute values for an entity as they were received by the disclosed system from a contributor third party system web page and or any other suitable source of information. For example if a webpage stated somewhere in the content of the webpage has valet parking the statement has valet parking is an Unprocessed Input. Likewise a website about a clothing store raw data might contain the statement 50 percent off sale raw input . As another example a contribution from a contributor updating the address of a business may contain 1801 ave of stars los angeles . Initially the rules available when the data was first provided may have caused this input to be ignored because the address is insufficient. However a subsequent build with improved rules could refine it to be address 1801 Avenue of the Stars city Los Angeles state CA zipcode 90067 .

Unprocessed Inputs may for example be stored in one or more of the following a file system including a distributed file system a data store such as a relational or non relational i.e. nosq1 database.

FullProcessed Inputs are Inputs that have been Processed in the most recent Batch Data Build. For example if the raw input has valet parking were contained in an online restaurant review the Batch Data Build of the disclosed system could extract an processed input of valet parking true.

In some embodiments each Batch Data Build may entirely replace the previous set of Full Processed Inputs . For example suppose the disclosed system had entries for just one restaurant called Joes and five websites provided facts about the restaurant. Two websites might state that the type of food served is Chinese . One website might state that it s Cantonese . Another two websites might say that it is Italian . In this example FullProcessed Inputs could include id 1 name Joe s cuisine Chinese source website1 id 1 name Joe s cuisine Chinese source website2 id 1 name Joe s cuisine Cantonese source website3 id 1 name Joe s cuisine Italian source website4 id 1 name Joe s cuisine Italian source website5 . Based on the current rules the FullProcessed Summary may have id 1 name Joe s cuisine Italian because it trusted all contributions equally and Italian and Chinese were tied while Cantonese was treated as an independent cuisine. In this example a rule could be improved to determine that Cantonese is a type of Chinese cuisine and is also more specific resulting in a FullProcessed Summary of

At the start of a Batch Data Build QuickProcessed Inputs and QuickProcessed Summaries which represent the newly computed Inputs and Summaries since the start of the previous Batch Data Build may be set aside or discarded and an empty version of each of QuickProcessed Inputs and QuickProcessed Summaries may be allocated in the data store such as a database. For example a user on a mobile device might notice that Joe s restaurant is miscategorized as Italian . That user acting as a contributor could submit a correction through software on her mobile device that sends the data to the Public application programming interface API . That contributor s input could look like id 1 cuisine Chinese . Once that input is processed it could be saved to QuickProcessed Inputs and the entry for Joes could be re Summarized. In this example the new Summary for Joes would then be id 1 name Joe s cuisine Chinese and because it is different than the previous FullProcessed Summary the new Summary would be saved to QuickProcessed Summaries . In the disclosed system when determining the latest Summary for an entity the system can check for the latest Summary in QuickProcessed Summaries favoring that over the FullProcessed Summary which only changes in a Batch Data Build.

From time to time a Batch Data Build may be run to convert Unprocessed Data and Inputs into finished view Summary Data. The output of a Batch Data Build is FullProcessed Inputs and FullProcessed Summaries .

In some embodiments the Input Processing module can be configured to perform one or more of the extraction process the cleaning process the canonicalization process the filtering process and the validation process each of which is described below.

The Extraction step may for example include a selection of a fact for an attribute based on a matching rule from structured semi structured and unstructured data. For example the disclosed system could use the fact matching rule name NAME to extract a name. In this example the Extraction step includes selection of the name Mc Donalds in a record like name Mc Donalds using the fact matching rule name NAME . Additionally in the Extraction step the system could use a pattern matching rule like where the symbol represents a wildcard character to select the phone number 123 456 7890 from text such as Tel 123 456 7890. As an additional example the Extraction step could interpret raw text like This place has no high chairs for my children to create a fact in the form kid friendly false. The disclosed system can interpret raw text to create a rule by for example using advanced natural language processing and parsing.

The Cleaning step comprises cleaning extracted data. Cleaning extracted data may include a process to remove undesired or bad characters or entity attributes. For example extraction of a fact matching the rule Phone PHONE NUMBER might incorrectly extract incorrect information such as Phone call now or extract extra information like Phone 123 456 7890 click here . Cleaning can discard incorrect data or remove extra data that is not desired. For example if Phone call now were extracted the Cleaning step could discard the data because Phone call now is incorrect data for a phone number. Additionally if Phone 123 456 7890 click here were extracted the Cleaning step could discard click here because click here is extra data that is not part of the extracted phone number. In the disclosed system incorrect data or extra data can be discarded or removed by for example using two rules such as a fact matching rule and a pattern matching rule. For example using the fact matching rule Phone PHONE NUMBER the disclosed system could extract information like Phone 123 456 7890 click here and using the pattern matching rule the system could determine that click here is extra data and remove it during the Cleaning step.

Canonicalization refers to a rules driven step to convert data in various formats into their preferred or canonical representation. For example one contributor may describe a phone number as 123 456 7890 and a different contributor may submit 123 456 7890 . Converting data into a canonical representation makes it uniform and enables better entity resolution and summarization. The disclosed system can perform canonicalization by for example using multiple pattern matching rules and designating another pattern for the canonical representation. For example using the pattern matching rule and with the former designated the canonical representation the Canonicalization step could make the inputs 123 456 7890 and 123 456 7890 uniform by representing them both as 123 456 7890. 

Filtering refers to a rules driven step to reject data that is not necessarily incorrect but does not meet some desired criteria. This can include rejecting inputs that don t match a particular category or have insufficient confidence. For example a Science Fiction theme restaurant might advertise that it is located on the planet Earth in the Milky Way galaxy. While this statement is accurate an embodiment of the disclosed system might for example not have a category for the planet and galaxy where restaurants are located and as such the Filtering step in this example would reject the input located on the planet Earth in the Milky Way galaxy. Of course in alternate embodiments the disclosed system could have such categories. As an additional example in an embodiment the disclosed system could for example set a threshold of 100 visits for information from a website to be considered reliable. In this example if a website that had been visited only 15 times contained the statement it is the best store the system could reject the input because it does not meet the confidence rule. In other embodiments the disclosed system could use other rules for determining confidence.

Validation refers to a rules driven step to reject data based on non conformance with certain criteria. For example a phone number field where after canonicalization the phone number has fewer digits than are expected for a valid phone number e.g. Phone 123 it is possible to reject the attribute or the entire input based on failure to meet certain criteria.

Embodiments of the disclosed system may perform a Real time Summarization process. Referring to in embodiments of the disclosed system the Quick Summarization process module receives QuickProcessed inputs generated by the Input Processing module and FullProcessed inputs generated by the batch processing system.

The Quick Summarization process module can be configured to aggregate and filter the QuickProcessed inputs and FullProcessed inputs . For example the Quick Summarization process could receive QuickProcessed inputs and FullProcessed inputs regarding valet parking. In this example FullProcessed inputs might include inputs with the value valet parking false and QuickProcessed input might include inputs with the valet parking true. The Quick Summarization process module can be configured to aggregate and filter the QuickProcessed inputs and the FullProcessed inputs to create a QuickProcessed Summary . For example after filtering and processing the QuickProcessed Summary for an entity might be valet parking true.

In some embodiments the Quick Summarization process module can be configured to maintain and index data in the QuickProcessed Inputs and the FullProcessed Inputs in a sort order determined based at least in part on one or more of the entity identifier the identifier of the contributor or a user account that provided the data the technology used to extract the data the source or citation for the data and or a timestamp. To this end a connection or iterator is created to read data simultaneously starting from the first Input with the desired entity identifier from both the QuickProcessed Inputs and the FullProcessedInputs. In each case the iterator is advanced on either QuickProcessed Inputs or FullProcessed Inputs whichever has the earlier timestamp. Whenever any of the attributes enumerated above except for the timestamp change the previous input is added to the pool being considered while the others are ignored thus allowing the system to efficiently consider only the latest version of an input from a given user extraction technology and citation.

The Diff process module can be configured to compare QuickProcessed Summaries generated by the Quick Summarization process module and the FullProcessed Summaries . For example it might compare a QuickProcessed Summary with the value valet parking true and a FullProcessed Summary with a value valet parking false. Based on the comparison the Diff process module could then broadcast the result. For example it could broadcast that the FullProcessed Summaries from the previous batch build indicates that there is no valet parking whereas the QuickProcessed Summary from the newly computed Inputs and Summaries since the start of the previous Batch Data Build indicates that there is valet parking

Referring to as indicated by the arrow Real time the upper portion of the diagram generally depicts real time components of the system. The system receives External Contributions as inputs. External Contributions include bulk data contributions web documents and real time submissions from contributors. For example the system can receive bulk data contributions such as entire websites or data stores web documents such as individual web pages and real time submissions such as reviews on a website.

One source of External Contributions are User Writes . User Writes could include for example direct input from contributors on a web form or a mobile device.

In some embodiments the system can receive User Writes via Public API module . For example User Writes could be received through a publicly accessible endpoint such as a website that submits to the Public API module or through software on a website or mobile device that submits to the Public API module . User Writes may include identifiers for the contributor origin and developer added to them for consideration in summarization.

An input such as a User Write may have an entity identifier e.g. entity id already included with it. An entity identifier can be a string of letters and numbers. In some embodiments an entity identifier can signify that the input is an update to an existing entity. If the input does not have an identifier the system can determine and assign a temporary identifier referred to as a QuickProcessed Identifier using the Resolve process module . The Resolve process module can be configured to assign an entity identifier to an input or to match one representation of a record to another. This makes it possible to cluster similar inputs together and assign those that reference the same entity with a common entity identifier. In many cases inputs have different attributes but reference the same entity. The Resolve process can be used to compare inputs determine that the inputs reference the same entity and assign a common entity identifier to those inputs.

In some cases the Resolve process module can be configured to assign an identifier as a surrogate key generated by a random assignment b concatenating one or more input attributes e.g. name address c consistent hashing of one or more input attributes e.g. md5 name address or d taking the assigned id of an existing input if a sufficiently similar input exists e.g. name value phone of new input is similar enough to name value phone of existing input and generating a new surrogate key when it is not.

Once a QuickProcessed Identifier is determined for the input Internal API module can receive the input from Public API module . Before it is saved to storage a copy of the original input in its raw form can be made. The raw copy can be saved to storage for Unprocessed Inputs so that it can later be reprocessed in batch with updated software or subjected to more expensive computation including software that does entity identifier assignment.

Additionally Internal API module can be configured to interact with the Stitch process module for rules driven moderation. The Stitch process module for rules driven moderation can be configured to display or provide data submissions that match certain criteria to a human moderator or more expensive machine processes for further evaluation. For example a new restaurant owner might wish to drive business to his restaurant by diverting it from nearby restaurants. That restaurant owner might sign up for an account as a contributor of one of the disclosed system s customers and submit information that all of the other restaurants are closed. The system could then determine that a new contributor who has never interacted with the system has on one day reported that several local businesses have closed causing a rule in the system that looks for such patterns to flag those submissions and enqueue them for review by a human moderator. The human moderator could in turn determine that the businesses are indeed still open and reject the submissions and further blacklist the contributor such that additional submissions will be ignored.

At the same time the original raw input can be processed through software that performs Extraction Cleaning Canonicalization and Validation as described above producing a QuickProcessed Input . In some cases the QuickProcessed Input may have a QuickProcessed Identifier attached. If the QuickProcessed Input passes Validation it can be saved to storage for QuickProcessed Inputs and it can move forward in the process to QuickSummarization process module .

Real time QuickSummarization process module can be configured to analyze and combine the QuickProcessed Inputs and FullProcessed Inputs for an entity in substantially real time. QuickProcessed Inputs can represent new real time inputs since the last Batch Data Build process. FullProccessed Inputs are inputs generated from a previous Batch Process by the batch processing system. Together they comprise the full set of Inputs for each entity. For example the Real time QuickSummarization process module could receive QuickProcessed inputs and FullProcessed inputs regarding valet parking. In this example FullProcessed inputs might include inputs with the value valet parking false and QuickProcessed input might include inputs with the valet parking true. The QuickSummarization process module could then aggregate the QuickProcessed inputs and FullProcessed inputs and then filter them using High Confidence Filter and Low Confidence Filter to create a QuickProcessed Summary . For example after filtering and processing the QuickProcessed Summary for an entity might be valet parking true.

QuickProcessed Inputs and FullProcessed Inputs may be stored in a data store . The data store can be respectively clustered by an entity identifier e.g. a uuid such as 0e3a7515 44e0 42b6 b736 657b126313b5 . This can allow re Summarization to take place quickly as new Inputs are received such that Inputs only pertaining to an entity for which a new Input has been received are processed. QuickProcessed Inputs and FullProcessed Inputs may be stored in a sort order such that it facilitates processing the data in streams or skipping over Inputs that are determined to be superseded by Inputs that are for example newer submissions from the same submitter or citing an identical reference.

In some embodiments the QuickSummarization process module can read Inputs concurrently from the QuickProcessed Input data store and FullProcessed Input data store to facilitate choosing only the Inputs that need to be considered in Summarization. The summarization of the Inputs can be represented or displayed using a view e.g. a materialized view . A view is one possible summarization of the Inputs and representation of entities according to one or more rules. The one or more rules can determine which entities are included which attributes are included for each entity what indexing optimizations are performed and what additional attributes and attribute variations are computed for each entity. In some embodiments a view is uniquely identified by a view id. Data stores often track views in system tables and these system tables contain metadata about views. In our case a view is assigned an identifier and that identifier is used to lookup metadata about the view from the data stores such as the names of the attributes their datatype sort preferences indexing rules etc.

For each view associated with the dataset to which the Input was assigned a QuickSummarization Process can be performed. Views may have different rules about attributes to be computed the rules that apply to those attributes confidence thresholds for the Summary entity and other software rules and transformations. Each QuickSummarization process can produce a QuickProcessed Summary for each View. Each QuickProcessed Summary is compared to the most recent Summary retrieved from the QuickProcessed Summary data store or the FullProcessed Summary data store . If the QuickProcessed Summary is different than the previous version the new QuickProcessed Summary is saved to the QuickProcessed Summary data store and a Diff record is produced. A Diff record can include a row of data that includes for example 1 an entity identifier of the entity whose attributes have changed and 2 the changed attributes. The Diff record may include an entire copy of the new Summary or the attributes that are different from the previous Summary. The Diff record is saved to a Diff data store and published over the network to processes that listen for Diff records and update Materializations of Summary data.

The following is an example of a Diff record in one possible embodiment timestamp 1363321439041 payload region TX geocode level front door tel 281 431 7441 placerank 90 category labels Retail Nurseries and Garden Centers search tags Houston Grass South name Houston Grass longitude 95.464476 fax 281 431 8178 website http houstonturfgrass.com postcode 77583 country us category ids 164 category Shopping Nurseries Garden Centers address 213 McKeever Rd locality Rosharon latitude 29.507771 type update factual id 399895e6 0879 4ed8 ba25 98fc3e0c983f changed address tel . In this example the Diff record indicates that the address and telephone for Houston Grass have changed which can result in an update to each copy of the materialized data store or index rows for that entity.

Embodiments of the disclosed system may include materialized data stores or indexes . Materialized data stores or indexes are searchable relational or non relational data stores or search index servers. In some embodiments the materialized data stores or indexes can be associated with a particular application domain or a particular data service. The disclosed system may utilize data store systems like PostgreSQL relational and Apache Solr non relational search server interchangeably sometimes for the same data and can choose the one that best services the type of query requested. For example the disclosed system can receive a query for data associated with a particular view or type of data. In response the disclosed system can determine one or more of a type of the query a type of the entity an application or a device that sends the query an application domain associated with the entity or any relevant information associated with the query to determine one of the data store systems or a combination of such systems to use to respond to the query. Then the disclosed system can use the determined one of the data store systems or combinations of systems to respond to the query.

Referring to as indicated by the arrow Batch the lower portion of the diagram generally illustrates batch processing components of the system.

In addition the Batch Processing Workflow uses previously processed data such as previous FullProcessedInputs and previous QuickProcessedInputs for steps such as UUID Retention and Diff Generation. These steps are described below.

Prior to initiating the Batch Build process the real time processed data can be provided to a data store such as a Hadoop Distributed File System HDFS so they can be used as inputs for the Batch Build. When this step is initiated the time can be recorded that can be used during the Catchup Phase.

In some embodiments quick processed inputs from the previous version of the data are not used except for UUID Retention described below . In such embodiments this is accounted for by using the Unprocessed Inputs instead. This ensures that inputs are completely reprocessed.

The Batch Build is a process in which the data can be processed and made ready for loading into production.

Raw inputs and Unprocessed Inputs are fed into the Input Processing module from HDFS . The Extract step may not preserve any notion that the data was previously extracted. Extraction may be done on the raw inputs .

The Extract step may use a rule framework that canonicalizes cleans fills in values and filters inputs as described above and as illustrated in the examples below.

The Extract step can also sort out which inputs should be attached and which inputs should be batch resolved. The Extract step can optionally determine that some inputs should be reviewed by a human a computationally powerful process or a third party API. The Extract step can set a moderation action flag within the metadata of the input and insert it directly or via API into the Stitch data store which is used to coordinate relatively costly processes such as moderation.

Batch resolve which can be performed by the Resolve process module may take the extracted inputs and group them based on whether they represent the same entity or not and assigns a unique id to each set of inputs. For example batch resolve can assign a unique id generated by a random assignment b concatenating one or more input values from a set of inputs e.g. name address c hashing of one or more values form an asset of inputs e.g. md5 name address or d taking the assigned id of an existing set of inputs if a sufficiently similar input exists e.g. name value phone of new input is similar enough to name value phone of existing input .

After the batch resolve module completes its process the UUID Retention module can be initiated. An objective of the UUID Retention module can include modifying the identifier associated with entities e.g. entity id so that a single entity such as the Eiffel Tower can maintain the same entity identifier even when input data is re processed by the batch processes e.g. across multiple batch runs . This enables an entity to be associated with the same identifier even when data associated with the entity is re processed multiple times.

This is accomplished for example by reading in the previous FullProcessedInputs and generating a mapping file or table which includes a mapping between an input id an entity id. An input id is a unique identifier assigned to each set of attributes coming from a single input data contribution. For example all of the attributes pulled from the homepage of the French Laundry such as the name address phone number constitute one input data contribution. Lots of other websites and contributors can also provide input data contributions describing the French Laundry. Each of these input data contributions has its own input id that uniquely identifies it from other input data contributions. The input id can include a message digest 5 md5 hash of an input data contribution. In contrast an entity id is an identifier assigned to all of the input data contributions and the summary record of the French Laundry currently a QUID .

In some embodiments the input id to entity id mapping can be combined with the mappings for newly written summaries. For example each input in FullProcessed Inputs and QuickProcessed Inputs has an input id that uniquely identifies the original unprocessed input and an entity id representing the entity associated with the FullProcessed Inputs and QuickProcessed Inputs as determined by the Resolve process module . Using the input id to entity id mapping the input id can be used to assign the original entity id to all inputs data items that are within the same set.

In this example in the previous batch build input id 0 had the entity id original entity id . In the current Batch Build since the sample input set contains the input with input id input id 0 the disclosed system can map all inputs in the input set to original entity id. As such in this example the end result would be as follows

The output of the UUID Retention module can include a grouped set of inputs that have the same entity ids as they had in the previous batch run as well as preserving any entity ids that were generated in between batch runs. As described above the UUID Retention module can preserve the same UUID for the same entity across Batch Builds.

In some cases entities may be merged or split depending on the result of Batch Resolve. The UUID Retention module can in effect specify how to deal with split and merge cases. For example in the case of a merge it may be preferred to use the entity id with the greater number of inputs. In the case of a split the entity id may be assigned to the input set that has the greater number of inputs and generate a new id for the input cluster forming the new summary. This behavior can be customized depending on the dataset and desired outcome.

After UUID Retention the Data Attachment process may be performed by the Data Attachment module . A purpose of the Data Attachment process can be to attach inputs that are 1 unresolvable 2 derived from a summary 3 derived from an input or 4 for inputs that with a sufficient degree of confidence pertain to a specific entity id such as contributor edits to a specific entity id or an input that has geocode information pertaining to a specific input.

Data attachment can be based on an entity id or an input id. For example the Data Attachment module can be configured to attach or combine a source input to a set of inputs generated from UUID Retention when the source input has the same entity id as that of the set of inputs. As another example the Data Attachment module can be configured to attach or combine a source input when the source input is associated with the same parent input id as that of the set of inputs where the parent input id refers to a unique identifier of the input to which the source input should be attached. These examples are illustrated with the following embodiments.

In this example since the sample input set and the source data have the same entity id entity id 0 the attachment data is added to the sample input set.

In this example since the sample input set and contains an input that matches the parent input id of the source data the attachment data is added to the sample input set.

Extended Attribute Set Extraction is an additional extraction process performed by the extended attributes module . The extended attributes module can be configured to run extraction on certain inputs to extract an extended attribute set . An extended attribute set may not be a part of the core attribute set but may contain information that pertains to specific views. For example vegan is an attribute that would pertain to a restaurant view but not to a doctor s view.

In some embodiments rules may be written in a rules framework that determines whether a set of inputs is re extracted for extended attributes. For example if a set of inputs has a single input that has the category Restaurant all inputs in that input set can be re extracted for extended attributes pertaining to restaurants.

The output of the extended attributes module include final inputs . The final inputs can be stored in the FullProcessed Inputs storage which may relay the final inputs to the quick summarization module .

Summarization module is configured to perform a summarization process. The summarization process includes a process by which the final representation of a set of inputs representing the same entity can be generated. Summarization module can use a rules framework to generate a summary based on the final inputs . Each dataset may have multiple views including side effect views. Each set of inputs may generate multiple view summaries. Each of the summaries generated from the same set of inputs have the same entity id.

A side effect view includes a new view e.g. set of summary entities that does not have a one to one relationship with the entity id for the given inputs. A side effect view can be generated as a by product of other views and their inputs rather than directly producing summaries from the associated entity inputs. The side effect view allows the summarization module to provide an arbitrary number of summary records e.g. an arbitrary number of related entities from a single data input. One such example of this is Crosswalk which is a view that links entity ids to specific input sources. For instance the side effect view creation process can determine whether an input data matches a rule such as is a namespace we track in crowsswalk e.g. because it has a url like webname.com some place id and once there is a match the side effect view creation process can create a new entity for example with namespace webname id some place id factual id id of referenced entity . Therefore even if the input data is already associated with an entity the side effect view creation process can generate additional entities associated with the input data based on a rule maintained by the side effect view creation process.

Following Summarization the results may be filtered with High Confidence filter and Low Confidence Filter as described previously and stored as FullProcessed Summaries .

In this process the FullProcessedInputs and FullProcessedSummaries are built. FullProcessedInputs can include all inputs for a given dataset and can be organized in a way where entity id lookup and summarization is efficient. FullProcessedSummaries can contain all the summary records for all views in a given dataset organized in a way where entity id and view id lookup is efficient. These files can be bulk loaded into a data store during a MakeLive step. The output of these this step is represented by and in .

Diff Generation module can be configured to generate all the diff records that comprise the difference between the current batch run and the prior real time updated dataset and output them to Diff API to Download Partners which allows authorized partners to download the difference records from the system. Each such record can be referred to as a diff. Specific diff types are described above. Diffs can be generated by comparing each summary for a view against the prior version of the summary for that same view. Diffs can be generated for every view for each summary. The current summaries can be compared against the prior FullProcessedSummaries and prior QuickProcessedSummaries tables. The same diff generation mechanism can be used to generate the diffs for the indexes and the diff for third parties to be provided via the diff API .

The Diffs are also written to the Data store Format which allows for efficient lookup based on date and entity id.

Materialization Build module is configured to produce an output format that is ready for serving other computing systems such as data stores. For example the Materialization Build module can be configured to build an inverted index e.g. a data store that allows for searching of the inputs. In some embodiments the Materialization Build module can be configured to build a materialization on a per view basis. In other embodiments the Materialization Build module can be configured to build a materialization that includes multiple views.

In some embodiments a simplified example of an inverted index materialization can include the following 

Using the simplified index in the example the data can be easily searchable by keyword or other attributes. For example searching for Diego would yield summaries for doc id 0 and doc id 1 in this example.

If there are multiple views per materialization the view id could be used as an additional keyword filter for searches.

In some embodiments each materialized data store can be associated with a particular application domain a particular service or a particular view. Therefore when a system receives a query for data the system can determine based on the particular application domain the particular service and or the particular view associated with the query and or requested data one or more of the materialized data stores to serve the query.

MakeLive is a process by which a Batch Build can be put into production. The MakeLive process can be accomplished through Data store Loading Catchup and New Materialization Notification. After the MakeLive process is completed all API requests can use the newly batch built data.

Once a Batch Data Build passes all required regression and other Quality Assurance tests a new table in the data store can be created with a new version number for FullProcessedInputs FullProcessedSummaries QuickProcesesdInputs and QuickProcessedSummaries . The data store format files FullProcessedInputs FullProcessedSummaries can be loaded into their respective new tables. Diffs can be appended to an existing DiffTable.

In the Real Time Processing can refer to newly built FullProcessedInputs and FullProcessedSummaries through a data store api server once data store loading is complete. This can be accomplished by changing the pointer of the FullProcessedInputs and FullProcessedSummaries tables so that the newer tables are visible to Real Time Processing and the older references are no longer visible to Real Time Processing.

An example of the loading of FullProcessedInputs is illustrated in the transition from to in . An example of the loading of FullProcessedSummaries is illustrated in the transition from to in .

During the time between when the batch run was started and when the Catchup Phase is first initiated the data store may have taken additional real time writes that were not processed during our Batch Build step. The real time writes can refer to any writes that have been received in real time and have generated QuickProcessed inputs. Once the Batch Build step is completed thereby creating a newly batch built dataset the Catchup Phase may update the newly batch built dataset maintained in the indexed data stores or a Diff API to Download partners based on these new real time writes so that the newly batch built dataset becomes up to date with the additional real time writes.

The final step for making a batch built dataset into production ready dataset can include a process for enabling the FullProcessedInputs FullProcessedSummaries QuickProcessedInputs and QuickProcessedSummaries tables. A flag can be cleared and the Summary Materialization versions can be updated to point to the newly built ones. This process can change the pointer from previous versions of and with the newest versions of the materializations built by the latest Batch Build.

The Unprocessed Inputs provided by the Real Time Workflow at the Pre Batch Build step can be copied into the Unprocessed Inputs so that they can be processed by the next Batch Data Build. The Unprocessed Inputs can be deduplicated to prevent duplicate entries.

After these steps all updates to the data can be handled by the Real Time Data Processing workflow until the next Scheduled Batch Data Build.

Embodiments of the disclosed system can be used in a variety of applications. For example embodiments of the disclosed system can be used to gather and summarize data from various application domains such as social networking online advertisements search engines medical services media services consumer package goods video games support groups or any other application domains from which a large amount of data is generated and maintained.

Embodiments of the disclosed system may be built upon logic or modules comprising executable code. The executable code can be stored on one or more memory devices. Accordingly a logic does not have to be located on a particular device. In addition a logic or a module can be multiple executable codes located on one or more devices in the systems disclosed herein. For instance access logic responsive to an input for accessing and retrieving data stored in one or more cells in the data store can be one executable code on an application server. In alternative embodiments such access logic is found on one or more application servers. In still other embodiments such access logic is found on one or more application servers and other devices in the system including but not limited to gateway summary data servers and back end data servers. The other logics disclosed herein also can be one or more executable code located on one or more devices within a collaborative data system.

In certain embodiments the disclosed systems comprise one or more application servers as well as one or more summary data servers and one or more back end data servers. The servers comprise memory to store the logics disclosed herein. In particular embodiments the one or more application servers store the logics necessary to perform the tasks disclosed herein. In other embodiments the summary servers store the logics necessary to perform the tasks disclosed herein. In other embodiments the back end servers store the logics necessary to perform the tasks disclosed herein.

In certain embodiments the client web browser makes requests to the one or more application servers. Alternatively the disclosed systems comprise one or more summary or back end data servers to which the client web browser makes requests.

In an exemplary embodiment the one or more application servers receive requests from the client web browser for specific data or tables. Upon these requests the one or more application servers calls upon one or more data store servers to request summary or detail data from cells or tables. The one or more application servers also call upon the one or more data store servers when a request to submit new data inputs is made. The one or more application servers receive the data from the one or more summary servers and the one or more application servers generate HTML and JavaScript objects to pass back to the client web browser. Alternatively the one or more application servers generate XML or JSON to pass objects through an API.

In one embodiment the data store servers are based on an architecture involving a cluster of summary data servers and a cluster of back end data servers. Note however that a system could include a single summary server and back end data server. In this embodiment the array of summary data servers are utilized to request from back end data servers summary data and attributes of such summarized data points confidence counts etc. . The array of summary servers also caches such summary data and summary attributes so that faster access to such summary data can be access without the need for an additional request to the back end data server.

The present systems and processes rely on executable code i.e. logic stored on memory devices. Memory devices capable of storing logic are known in the art. Memory devices include storage media such as computer hard disks redundant array of inexpensive disks RAID random access memory RAM and optical disk drives. Examples of generic memory devices are well known in the art e.g. U.S. Pat. No. 7 552 368 describing conventional semiconductor memory devices and such disclosure being herein incorporated by reference .

The subject matter described herein can be implemented in digital electronic circuitry or in computer software firmware or hardware including the structural means disclosed in this specification and structural equivalents thereof or in combinations of them. The subject matter described herein can be implemented as one or more computer program products such as one or more computer programs tangibly embodied in an information carrier e.g. in a machine readable storage device or embodied in a propagated signal for execution by or to control the operation of data processing apparatus e.g. a programmable processor a computer or multiple computers . A computer program also known as a program software software application or code can be written in any form of programming language including compiled or interpreted languages and it can be deployed in any form including as a stand alone program or as a module component subroutine or other unit suitable for use in a computing environment. A computer program does not necessarily correspond to a file. A program can be stored in a portion of a file that holds other programs or data in a single file dedicated to the program in question or in multiple coordinated files e.g. files that store one or more modules sub programs or portions of code . A computer program can be deployed to be executed on one computer or on multiple computers at one site or distributed across multiple sites and interconnected by a communication network.

The processes and logic flows described in this specification including the method steps of the subject matter described herein can be performed by one or more programmable processors executing one or more computer programs to perform functions of the subject matter described herein by operating on input data and generating output. The processes and logic flows can also be performed by and apparatus of the subject matter described herein can be implemented as special purpose logic circuitry e.g. an FPGA field programmable gate array or an ASIC application specific integrated circuit .

Processors suitable for the execution of a computer program include by way of example both general and special purpose microprocessors and any one or more processor of any kind of digital computer. Generally a processor will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a processor for executing instructions and one or more memory devices for storing instructions and data. Generally a computer will also include or be operatively coupled to receive data from or transfer data to or both one or more mass storage devices for storing data e.g. magnetic magneto optical disks or optical disks. Information carriers suitable for embodying computer program instructions and data include all forms of non volatile memory including by way of example semiconductor memory devices e.g. EPROM EEPROM and flash memory devices magnetic disks e.g. internal hard disks or removable disks magneto optical disks and optical disks e.g. CD and DVD disks . The processor and the memory can be supplemented by or incorporated in special purpose logic circuitry.

To provide for interaction with a user the subject matter described herein can be implemented on a computer having a display device e.g. a CRT cathode ray tube or LCD liquid crystal display monitor for displaying information to the user and a keyboard and a pointing device e.g. a mouse or a trackball by which the user can provide input to the computer. Other kinds of devices can be used to provide for interaction with a user as well. For example feedback provided to the user can be any form of sensory feedback e.g. visual feedback auditory feedback or tactile feedback and input from the user can be received in any form including acoustic speech or tactile input.

The techniques described herein can be implemented using one or more modules. As used herein the term module refers to computing software firmware hardware and or various combinations thereof. At a minimum however modules are not to be interpreted as software that is not implemented on hardware firmware or recorded on a non transitory processor readable recordable storage medium. Indeed module is to be interpreted to include at least some physical non transitory hardware such as a part of a processor or computer. Two different modules can share the same physical hardware e.g. two different modules can use the same processor and network interface . The modules described herein can be combined integrated separated and or duplicated to support various applications. Also a function described herein as being performed at a particular module can be performed at one or more other modules and or by one or more other devices instead of or in addition to the function performed at the particular module. Further the modules can be implemented across multiple devices and or other components local or remote to one another. Additionally the modules can be moved from one device and added to another device and or can be included in both devices.

The subject matter described herein can be implemented in a computing system that includes a back end component e.g. a data server a middleware component e.g. an application server or a front end component e.g. a client computer having a graphical user interface or a web browser through which a user can interact with an implementation of the subject matter described herein or any combination of such back end middleware and front end components. The components of the system can be interconnected by any form or medium of digital data communication e.g. a communication network. Examples of communication networks include a local area network LAN and a wide area network WAN e.g. the Internet.

The terms a or an as used herein throughout the present application can be defined as one or more than one. Also the use of introductory phrases such as at least one and one or more should not be construed to imply that the introduction of another element by the indefinite articles a or an limits the corresponding element to only one such element. The same holds true for the use of definite articles.

It is to be understood that the disclosed subject matter is not limited in its application to the details of construction and to the arrangements of the components set forth in the following description or illustrated in the drawings. The disclosed subject matter is capable of other embodiments and of being practiced and carried out in various ways. Also it is to be understood that the phraseology and terminology employed herein are for the purpose of description and should not be regarded as limiting.

As such those skilled in the art will appreciate that the conception upon which this disclosure is based may readily be utilized as a basis for the designing of other structures methods and systems for carrying out the several purposes of the disclosed subject matter. It is important therefore that the claims be regarded as including such equivalent constructions insofar as they do not depart from the spirit and scope of the disclosed subject matter.

Although the disclosed subject matter has been described and illustrated in the foregoing exemplary embodiments it is understood that the present disclosure has been made only by way of example and that numerous changes in the details of implementation of the disclosed subject matter may be made without departing from the spirit and scope of the disclosed subject matter.

