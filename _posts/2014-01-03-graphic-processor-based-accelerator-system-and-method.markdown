---

title: Graphic processor based accelerator system and method
abstract: An accelerator system is implemented on an expansion card comprising a printed circuit board having (a) one or more graphics processing units (GPUs), (b) two or more associated memory banks (logically or physically partitioned), (c) a specialized controller, and (d) a local bus providing signal coupling compatible with the PCI industry standards. The controller handles most of the primitive operations to set up and control GPU computation. Thus, the computer's central processing unit (CPU) can be dedicated to other tasks. In this case a few controls (simulation start and stop signals from the CPU and the simulation completion signal back to CPU), GPU programs and input/output data are exchanged between CPU and the expansion card. Moreover, since on every time step of the simulation the results from the previous time step are used but not changed, the results are preferably transferred back to CPU in parallel with the computation.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09189828&OS=09189828&RS=09189828
owner: Neurala, Inc.
number: 09189828
owner_city: Boston
owner_country: US
publication_date: 20140103
---
The present application claims a priority benefit under 35 U.S.C. 120 as a continuation of U.S. application Ser. No. 11 860 254 now U.S. Pat. No. 8 648 867 B2 filed Sep. 24 2007 entitled Graphic Processor Based Accelerator System and Method which claims the priority benefit under 35 U.S.C. 119 e of U.S. Application No. 60 826 892 filed Sep. 25 2006. Each of the above identified applications is incorporated herein by reference in its entirety.

Graphics Processing Units GPUs are found in video adapters graphic cards of most personal computers PCs video game consoles workstations etc. and are considered highly parallel processors dedicated to fast computation of graphical content. With the advances of the computer and console gaming industries the need for efficient manipulation and display of 3D graphics has accelerated the development of GPUs.

In addition manufacturers of GPUs have included general purpose programmability into the GPU architecture leading to the increased popularity of using GPUs for highly parallelizable and computationally expensive algorithms outside of the computer graphics domain. When implemented on conventional video card architectures these general purpose GPU GPGPU applications are not able to achieve optimal performance however. There is overhead for graphics related features and algorithms that are not necessary for these non video applications.

Numerical simulations e.g. finite element analysis of large systems of similar elements e.g. neural networks genetic algorithms particle systems mechanical systems are one example of an application that can benefit from GPGPU computation. During numerical simulations disk and user input output can be performed independently of computation because these two processes require interactions with peripheral hardware disk screen keyboard mouse etc and put relatively low load on the central processing unit system CPU . Complete independence is not desirable however user input might affect how the computation is performed and even interrupt it if necessary. Furthermore the user output and the disk output are dependent on the results of the computation. A reasonable solution would be to separate input output into threads so that it is interacting with hardware occurs in parallel with the computation. In this case whatever CPU processing is required for input output should be designed so that it provides the synchronization with computation.

In the case of GPGPU the computation itself is performed outside of the CPU so the complete system comprises three peripheral components user interactive hardware disk hardware and computational hardware. The central processing unit CPU establishes communication and synchronization between peripherals. Each of the peripherals is preferably controlled by a dedicated thread that is executed in parallel with minimal interactions and dependencies on the other threads.

A GPU on a conventional video card is usually controlled through OpenGL DirectX or similar graphic application programming interfaces APIs . Such APIs establish the context of graphic operations within which all calls to the GPU are made. This context only works when initialized within the same thread of execution that uses it. As a result in a preferred embodiment the context is initialized within a computational thread. This creates complications however in the interaction between the user interface thread that changes parameters of simulations and the computational thread that uses these parameters.

A solution as proposed here is an implementation of the computational stream of execution in hardware so that thread and context initialization are replaced by hardware initialization. This hardware implementation includes an expansion card comprising a printed circuit board having a one or more graphics processing units b two or more associated memory banks that are logically or physically partitioned c a specialized controller and d a local bus providing signal coupling compatible with the PCI industry standards this includes but is not limited to PCI Express PCI X USB 2.0 or functionally similar technologies . The controller handles most of the primitive operations needed to set up and control GPU computation. As a result the CPU is freed from this function and is dedicated to other tasks. In this case a few controls simulation start and stop signals from the CPU and the simulation completion signal back to CPU GPU programs and input output data are the information exchanged between CPU and the expansion card. Moreover since on every time step of the simulation the results from the previous time step are used but not changed the results are preferably transferred back to CPU in parallel with the computation.

In general according to one aspect the invention features a computer system. This system comprises a central processing unit main memory accessed by the central processing unit and a video system for driving a video monitor in response to the central processing unit as is common. The computer system further comprises an accelerator that uses input data from and provides output data to the central processing unit. This accelerator comprises at least one graphics processing unit accelerator memory for the graphic processing unit and an accelerator controller that moves the input data into the at least one graphics processing unit and the accelerator memory to generate the output data.

In the preferred the central processing unit transfers the input data for a simulation to the accelerator after which the accelerator executes simulation computations to generate the output data which is transferred to the central processing unit. Preferably the accelerator controller dictates an order of execution of instructions to the at least one graphics processing unit. The use of the separate controller enables data transfer during execution such that the accelerator controller transfers output data from the accelerator memory to main memory of the central processing unit.

In the preferred embodiment the accelerator controller comprises an interface controller that enables the accelerator to communicate over a bus of the computer system with the central processing unit.

In general according to another aspect the invention also features an accelerator system for a computer system which comprises at least one graphics processing unit accelerator memory for the graphic processing unit and an accelerator controller for moving data between the at least one graphics processing unit and the accelerator memory.

In general according to another aspect the invention also features a method for performing numerical simulations in a computer system. This method comprises a central processing unit loading input data into an accelerator system from main memory of the central processing unit and an accelerator controller transferring the input data to a graphics processing unit with instructions to be performed on the input data. The accelerator controller then transfers output data generated by the graphic processing unit to the central processing unit as output data.

The above and other features of the invention including various novel details of construction and combinations of parts and other advantages will now be more particularly described with reference to the accompanying drawings and pointed out in the claims. It will be understood that the particular method and device embodying the invention are shown by way of illustration and not as a limitation of the invention. The principles and features of this invention may be employed in various and numerous embodiments without departing from the scope of the invention.

In more detail the computer system in one example is a standard personal computer PC . However this only serves as an example environment as computing environment does not necessarily depend on or require any combination of the components that are illustrated and described herein. In fact there are many other suitable computing environments for this invention including but not limited to workstations server computers supercomputers notebook computers hand held electronic devices such as cell phones mp3 players or personal digital assistants PDAs multiprocessor systems programmable consumer electronics networks of any of the above mentioned computing devices and distributed computing environments that including any of the above mentioned computing devices.

In one implementation the GPU accelerator is implemented as an expansion card includes connections with the motherboard on which the one or more CPU s are installed along with main or system memory and mass non volatile data storage such as hard drive or redundant array of independent drives RAID array for the computer system . In the current example the expansion card communicates to the motherboard via a local bus . This local bus could be PCI PCI Express PCI X or any other functionally similar technology depending upon the availability on the motherboard . An external version GPU accelerator is also a possible implementation. In this example the external GPU accelerator is connected to the motherboard through USB 2.0 IEEE 1394 Firewire or similar external peripheral device interface.

The CPU and the system memory on the motherboard and the mass data storage system are preferably independent of the expansion card and only communicate with each other and the expansion card through the system bus located in the motherboard . A system bus in current generations of computers have bandwidths from 3.2 GB s Pentium 4 with AGTL Athlon XP with EV6 to around 15 GB s Xeon Woodcrest with AGTL Athlon 64 Opteron with Hypertransport while the local bus has maximal peak data transfer rates of 4 GB s PCI Express 16 or 2 GB s PCI X 2.0 . Thus the local bus becomes a bottleneck in the information exchange between the system bus and the expansion card . The design of the expansion card and methods proposed herein minimizes the data transfer through the local bus to reduce the effect of this bottleneck.

The system memory is referred to as the main random access memory RAM in the description herein. However this is not intended to limit the system memory to only RAM technology. Other possible computer storage media include but are not limited to ROM EEPROM flash memory or any other memory technology.

In the illustrated example the GPU accelerator system is implemented on an expansion card on which the one or more GPU s are mounted. It should be noted that the GPU accelerator system GPU is separate from and independent of any GPU on the standard video card or other video driving hardware such as integrated graphics systems. Thus the computations performed on the expansion card do not interfere with graphics display including but not limited to manipulation and rendering of images .

Various brand of GPU are relevant. Under current technology GPU s based on the GeForce series from NVIDIA Corporation or the Catalyst series from ATI Advanced Micro Devices Inc.

The output to a video monitor is preferably through the video card and not the GPU accelerator system . The video card is dedicated to the transfer of graphical information and connects to the motherboard through a local bus that is sometimes physically separate from the local bus that connects the expansion card to the motherboard .

The GPU accelerator further preferably comprises one specifically designed accelerator controller . Depending upon the implementation the accelerator controller is field programmable gate array FPGA logic or custom built application specific ASIC chip mounted in the expansion card and in mechanical and signal coupling with the GPU and the associated memories and . During initial design a controller can be partially or even fully implemented in software in one example.

The controller commands the storage and retrieval of arrays of data on a conventional video card the arrays of data are represented as textures hence the term texture in this document refers to a data array unless specified otherwise and each element of the texture is a pixel of color information execution of GPU programs on a conventional video card these programs are called shaders hence the term shader in this document refers to a GPU program unless specified otherwise and data transfer between the system bus and the expansion card through the local bus which allows communication between the main CPU RAM and disk .

Two memory banks and are mounted on the expansion card . In some example these memory banks separated in the hardware as shown or alternatively implemented as a single logically partitioned memory component.

The reason to separate the memory into two partitions stems from the nature of the computations to which the GPU accelerator system is applied. The elements of computation computational elements are characterized by a single output variable. Such computational elements often include one or more equations. Computational elements are same or similar within a large population and are computed in parallel. An example of such a population is a layer of neurons in an artificial neural network ANN where all neurons are described by the same equation. As a result some data and most of the algorithms are common to all computational elements within population while most of the data and some algorithms are specific for each equation. Thus one memory the shader memory bank is used to store the shaders needed for the execution of the required computations and the parameters that are common for all computational elements and is coupled with the controller only. The second memory the texture memory bank is used to store all the necessary data that are specific for every computational element including but not limited to input data output data intermediate results and parameters and is coupled with both the controller and the GPU .

The texture memory bank is preferably further partitioned into four sections. The first partition a is designed to hold the external input data patterns. The second partition is designed to hold the data textures representing internal variables. The third partition is designed to hold the data textures used as input at a particular computation step on the GPU . The fourth partition holds the data textures used to accommodate the output of a particular computational step on the GPU . This partitioning scheme can be done logically does not require hardware implementation. Also the partitioning scheme is also altered based on new designs or needs of the algorithms being employed. The reason for this partitioning is further explained in the Data Organization section below.

A local bus interface on the controller serves as a driver that allows the controller to communicate through the local bus with the system bus and thus the CPU and RAM . This local bus interface is not intended to be limited to PCI related technology. Other drivers can be used to interface with comparable technology as a local bus .

Each computational element discussed above has output variables that affect the rest of the system. For example in the case of a neural network it is the output of a neuron. A computational element also usually has several internal variables that are used to compute output variables but are not exposed to the rest of the system not even to other elements of the same population typically. Each of these variables is represented as a texture. The important difference between output variables and internal variables is their access.

Output variables are usually accessed by any element in the system during every time step. The value of the output variable that is accessed by other elements of the system corresponds to the value computed on the previous not the current time step. This is realized by dedicating two textures to output variables one holds the value computed during the previous time step and is accessible to all computational elements during the current time step another is not accessible to other elements and is used to accumulate new values for the variable computed during the current time step. In between time steps these two textures are switched so that newly accumulated values serve as accessible input during the next time step while the old input is replaced with new values of the variable. This switch is implemented by swapping the address pointers to respective textures as described in the System and Framework section.

Internal variables are computed and used within the same computational element. There is no chance of a race condition in which the value is used before it is computed or after it has already changed on the next time step because within an element the processing is sequential. Therefore it is possible to render the new value of internal variable into the same texture where the old was read from in the texture memory bank. Rendering to more than one texture from a single shader is not implemented in current GPU architectures so computational elements that track internal variables would have to have one shader per variable. These shaders can be executed in order with internal variables computed first followed by output variables.

Further savings of texture memory is achieved through using multiple color components per pixel texture element to hold data. Textures can have up to four color components that are all processed in parallel on a GPU. Thus to maximize the use of GPU architecture it is desirable to pack the data in such a way that all four components are used by the algorithm. Even though each computational element can have multiple variables designating one texture pixel per element is ineffective because internal variables require one texture and output variables require two textures. Furthermore different element types have different numbers of variables and unless this number is precisely a multiple of four texture memory can be wasted.

A more reasonable packing scheme would be to pack four computational elements into a pixel and have separate textures for every variable associated with each computational element. In this case the packing scheme is identical for all textures and therefore can be accessed using the same algorithm. Several ways to approach this packing scheme are outlined here. An example population of nine computational elements arranged in a 3 3 square can be packed by element by row or by square .

Packing by element means that elements go into first pixel go into second pixel goes into third pixel. This is the most compact scheme but not convenient because the geometrical relationship is not preserved during packing and its extraction depends on the size of the population.

Packing by row column means that elements go into pixel go into pixel go into pixel . With this scheme the element s y coordinate in the population is the pixel s y coordinate while the element s x coordinate in the population is the pixel s x coordinate times four plus the index of color component. Five by five populations in this case will use 2 5 texture or 10 pixels. Five of these pixels will only use one out of four components so it wastes 37.5 of this texture. 25 1 population will use 6 1 texture six pixels and will waste 12.5 of it.

Packing by square means that elements go into pixel go into pixel go into pixel and goes into pixel . Both the row and the column of the element are determined from the row column of the pixel times two plus the second first bit of the color component index. Five by five populations in this case will use 3 3 texture or 9 pixels. Four of these pixels will only use two out of four components and one will only use one component so it wastes 34.4 of this texture. This is more advantageous than packing by row since the texture is smaller and the waste is also lower. 25 1 population on the other hand will use 13 1 texture thirteen pixels and waste 50 of it which is much worse than packing by row.

In order to eliminate waste altogether the population should have even dimensions in the square packing and it should have a number of columns divisible by four in row packing. Theoretically the chances are approximately equivalent for both of these cases to occur so the particular task and data sizes should determine which packing scheme is preferable in each individual case.

The method presented herein includes two execution streams that run on the CPU User Interaction Stream and Data Output Stream . These two streams preferably do not interact directly but depend on the same data accumulated during simulations. They can be implemented as separate threads with shared memory access and executed on different CPUs in the case of multi CPU computing environment. The third execution stream Computational Stream runs on the GPU accelerator of the expansion card and interacts with the User Interaction Stream through initialization routines and data exchange in between simulations. The Computational Stream interacts with the User Interaction Stream and the Data Output Stream through synchronization procedures during simulations.

The crucial feature of the interaction between the User Interaction Stream and the Computational Stream is the shift of priorities. Outside of the simulation the system is driven by the user input thus the User Interaction Stream has the priority and controls the data exchange between streams. After the user starts the simulation the Computational Stream takes the priority and controls the data exchange between streams until the simulation is finished or interrupted .

The user starts the framework through the means of an operating system and interacts with the software through the user interaction section of the graphic user interface executed on the CPU . The start of the implementation begins with a user action that causes a GUI initialization Disk input output initialization on the CPU and controller initialization of the GPU accelerator on the expansion card . GUI initialization includes opening of the main application window and setting the interface tools that allow the user to control the framework. Disk I O initialization can be performed at the start of the framework or at the start of each individual simulation.

The user interaction controls the setting and editing of the computational elements parameters and sources of external inputs. It specifies which equations should have their output saved to disk and or displayed on the screen. It allows the user to start and stop the simulation. And it performs standard interface functions such as file loading and saving interactive help general preferences and others.

The user interaction directs the CPU to acquire the new external input textures needed this includes but is not limited to loading from disk or receiving them in real time from a recording device parses them if necessary and initializes their transfer to the expansion card where they are stored in the texture memory bank by the controller . The user interaction also directs the CPU to parse populations of elements that will be used in the simulation convert them to GPU programs shaders compile them and initializes their transfer to the expansion card where they are stored in the shader memory bank by the controller . This operation is accompanied by the upload of the initial data into the input partition of the texture memory bank and stores the shader order of execution in the controller . The user can perform operations and as many times as necessary prior to starting the simulation or between simulations.

The editing of the system between simulations is difficult to accomplish without the hardware implementation of the computational thread suggested herein. The system of equations computational elements is represented by textures that track variables plus shaders that define processing algorithms. As mentioned above textures shaders and other graphics related constructs can only be initialized within the rendering context which is thread specific. Therefore textures and shaders can only be initialized in the computational thread.

Network editing is a user interactive process which according to the scheme suggested above happens in the User Interaction Stream . The simulation software thus has to take the new parameters from the User Interaction Stream communicate them to the Computational Stream and regenerate the necessary shaders and textures. This is hard to accomplish without a hardware implementation of the Computational Stream . The Computational Stream is forked from the User Interaction Stream and it can access the memory of the parent thread but the reverse communication is harder to achieve. The controller allows operations and to be performed as many times as necessary by providing the necessary communication to the User Interaction Stream .

After execution of the input parser texture generation and population parser shader generator and compiler are performed at least once the user has the option to initialize the simulation . During this initialization the main control of the framework is transferred to the GPU accelerator system s accelerator controller and computation is started see . The user retains the ability to interrupt the simulation change the input or to change the display properties of the framework but these interactions are queued to be performed at times determined by the controller driven data exchange and to avoid the corruption of the data.

The progress monitor is not necessary for performance but adds convenience. It displays the percentage of completed time steps of the simulation and allows the user to plan the schedule using the estimates of the simulation wall clock times. Controller driven data exchange updates the display of the results . Online screen output for the user selected population allows the user to monitor the activity and evaluate the qualitative behavior of the network. Simulations with unsatisfactory behavior can be terminated early to change parameters and restart. Controller driven data exchange also drives the output of the results to disk . Data output to disk for convenience can be done on an element per file basis. A suggested file format includes a leftmost column that displays a simulated time for each of the simulation steps and subsequent columns that display variable values during this time step in all elements with identical equations e.g. all neurons in a layer of a neural network .

Controller driven data exchange or input parser texture generator allows the user to change input that is generated on the fly during the simulation. This allows the framework monitoring of the input that is coming from a recording device video camera microphone cell recording electrode etc in real time. Similar to the initial input parser it preprocesses the input into a universal format of the data array suitable for texture generation and generates textures. Unlike the initial parser here the textures are transferred to hardware not whenever ready but upon the request of the controller .

The controller also drives the conditional testing and informs the CPU bound streams whether the simulation is finished. If so the control returns to the User Interaction Stream. The user then can change parameters or inputs and restart the simulation or quit the framework .

SANNDRA Synchronous Artificial Neuronal Network Distributed Runtime Algorithm http www.kinness.net Docs SANNDRA html was developed to accelerate and optimize processing of numerical integration of large non homogenous systems of differential equations. This library is fully reworked in its version 2.x.x to support multiple computational backends including those based on multicore CPUs GPUs and other processing systems. GPU based backend for SANNDRA 2.x.x can serve as an example practical software implementation of the method and architecture described above and pictorially represented in .

To use SANNDRA the application should create a TSimulator object either directly or through inheritance. This object will handle global simulation properties and control the User Interaction Stream Data Output Stream and Computational Stream. Through TSimulator timestep TSimulator outfileInterval and TSimulator outmode the application can set the time step of the simulation the time step of disk output and the mode of the disk output. The external input pattern should be packed into a TPattern object and bound to the simulation object through TSimulator resetInputs method. TSimulator simLength sets the length of the simulation.

The second step is to create at least one population of equations Tpopulation object . Population holds one equation object TEquation. This object contains only a formula and does not hold element specific data so all elements of the population can share single TEquation.

The TEquation object is converted to a GPU program before execution. GPU programs have to be executed within a graphical context which is stream specific. TSimulator creates this context within a Computational Stream therefore all programs and data arrays that are necessary for computation have to be initialized within Computational Stream. Constructor of TPopulation is called from User Interaction Stream so no GPU related objects can be initialized in this constructor.

TPopulation fillElements is a virtual method designed to overcome this difficulty. It is called from within the Computational Stream after TSimulator networkCreate is called in the User Interaction Stream. A user has to override TPopulation fillElements to create TEquation and other computation related objects both element independent and element specific. Element independent objects include subcomponents of TEquation and objects that describe how to handle interdependencies between variables implemented through derivatives of TGate class.

Element specific data is held in TElement objects. These objects hold references to TEquation and a set of TGate objects. There is one TElement per population but the size of data arrays within this object corresponds to population size. All TElement objects have to be added to the TSimulator list of elements by calling TSimulator addUnit method from TPopulation fillElements .

Finally TPopulation fillElements should contain a set of TElement add Dependency calls for each element. Each of these calls sets a corresponding dependency for every TGate object. Here TGate object holds element independent part of dependency and TElement add Dependency sets element specific details.

System provided TPopulation handles the output of computational elements both when they need to exchange the data and when they need to output it to disk. User implementation of TPopulation derivative can add screen output.

Listing 1 is an example code of the user program that uses a recurrent competitive field RCF equation 

With systems of equations that have complex interdependencies it is likely that the variable in some equation from a previous time step has to be used by some other equation after the new values of this variable are already computed for new time step. To avoid data confusion the new values of variables should be rendered in a separate texture. After the time step is completed for all equations these new values should be copied over old values so that they are used as input during the next time step. Copying textures is an expensive operation computationally but since the textures are referred to by texture IDs pointers swapping these pointers for input and output textures after each time step achieves the same result at a much lesser cost.

In the hardware solution suggested herein ID swapping is equivalent to swapping the base memory address for two partitions of the texture memory bank . They are swapped during synchronization and so that data transfer and the computation proceeds immediately and in parallel with data transfer as shown in . A hardware solution allows this parallelism through access of the controller to the onboard texture memory bank .

The main computation and data exchange are executed by the controller . It runs three parallel substreams of execution Computational Substream Data Output Substream and Data Input Substream . These streams are synchronized with each other during the swap of pointers to the input and output texture memory partitions of the texture memory bank and the check for the last iteration . Algorithmically these two operations are a single atomic operation but the block diagram shows them as two separate blocks for clarity.

The Computational Substream performs a computational cycle including a sequential execution of all shaders that were stored in the shader memory bank using the appropriate input and output textures. To begin the simulation the controller initializes three execution substreams and . On every simulation step the Computational Substream determines which textures the GPU will need to perform the computations and initiates the upload of them onto the GPU . The GPU can communicate directly with the texture memory bank to upload the appropriate texture to perform the computations. The controller also pulls the first shader known by the stored order from the shader memory bank and uploads it onto the GPU .

The GPU executes the following operations in this order performs the computation execution of the shader tells the controller that it is done with the computations for the current shader and after all shaders for this particular equation are executed sends the output textures to the output portion of the texture memory bank . This cycle continues through all of the equations based on the branching step .

An example shader that performs fourth order Runge Kutta numerical integration is shown in Listing 2 using GLSL notation 

The shader in Listing 2 can be executed on conventional video card. Using the controller this code can be further optimized however. Since the integration step does not change during the simulation the step itself as well as the halfstep and of the step can be computed once per simulation and updated in all shaders by a shader update procedures discussed above.

After all of the equations in the computational cycle are computed the main execution substream on the controller can switch the reference pointers of the input and output portions of the texture memory bank .

The two other substreams of execution on the controller are waiting blocks and respectively for this switch to begin their execution. The Data Input Substream is controlling the input of additional data from the CPU . This is necessary in cases where the simulation is monitoring the changing input for example input from a video camera or other recording device in the real time. This substream uploads new external input from the CPU to the texture memory bank so it can be used by the main computational substream on the next computational step and waits for the next iteration . The Data Output Substream controls the output of simulation results to the CPU if requested by the user. This substream uploads the results of the previous step to the main RAM so that the CPU can save them on disk or show them on the results display and waits for the next iteration .

Since the Computational Substream determines the timing of input and output data transfers these data transfers are driven by the controller . To further reduce the data transfer overhead and disk overhead also the controller initiates transfer only after selected computational steps. For example if the experimental data that is simulated was recorded every 10 milliseconds msec and the simulation for better precision was computed every 1 msec then only every tenth result has to be transferred to match the experimental frequency.

This solution stores two copies of output data one in the expansion card texture memory bank and another in the system RAM . The copy in the system RAM is accessed twice for disk I O and screen visualization . An alternative solution would be to provide CPU with a direct read access to the onboard texture memory bank by mapping the memory of the hardware onto a global memory space. The alternative solution will double the communication through the local bus . Since the goal discussed herein is reducing the information transfer through the local bus the former solution is favored.

The main substream determines if this is the last iteration . If it is the last iteration the controller waits for the all of the execution substreams to finish and then returns the control to the CPU otherwise it begins the next computational cycle.

1. Limited computations on the CPU . The CPU is only used for user input sending information to the controller receiving output after each computational cycle or less frequently as defined by the user writing this output to disk and displaying this output on the monitor . This frees the CPU to execute other applications and allows the expansion card to run at its full capacity without being slowed down by extensive interactions with the CPU .

2. Minimizing data transfer between the expansion card and the system bus . All of the information needed to perform the simulations will be stored on the expansion card and all simulations will take place on it. Furthermore whatever data transfer remains necessary will take place in parallel with the computation thus reducing the impact of this transfer on the performance.

3. New way to execute GPU programs shaders . Previously the CPU had full control over the order of shader s execution and was required to produce specific commands on every cycle to tell the GPU which shader to use. With the invention disclosed herein shaders will initially be stored on the shader memory bank on the expansion card and will be sent to the GPU for execution by the general purpose controller located on the expansion card.

4. Multiple parallelisms. The GPU is inherently parallel and is well suited to perform parallel computations. In parallel with the GPU performing the next calculation the controller is uploading the data from the previous calculation into main memory . Furthermore the CPU at the same time uses uploaded previous results to save them onto disk and to display them on the screen through the system bus .

5. Reuse of existing and affordable technology. All hardware used in the invention and mentioned here in are based on currently available and reliable components. Further advance of these components will provide straightforward improvements of the invention.

While this invention has been particularly shown and described with references to preferred embodiments thereof it will be understood by those skilled in the art that various changes in form and details may be made therein without departing from the scope of the invention encompassed by the appended claims.

