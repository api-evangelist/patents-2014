---

title: Stimulating reviews at a point of sale
abstract: A user is solicited to provide a review for a business, while the user is located on a premise of the business. Review data is received from the user. The review data is transmitted to a reputation platform configured to evaluate the received review data. If the review data indicates a positive review, a review request action is commenced. If the review data indicates a negative review, a remedial action is commenced.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09639869&OS=09639869&RS=09639869
owner: Reputation.com, Inc.
number: 09639869
owner_city: Redwood City
owner_country: US
publication_date: 20140123
---
This application is a continuation of co pending U.S. patent application Ser. No. 13 731 064 entitled STIMULATING REVIEWS AT A POINT OF SALE filed Dec. 30 2012 which is incorporated herein by reference for all purposes. U.S. patent application Ser. No. 13 731 064 claims priority to U.S. Provisional Patent Application No. 61 606 918 entitled BUSINESS REPUTATION SYSTEM filed Mar. 5 2012 and U.S. Provisional Patent Application No. 61 666 586 entitled BUSINESS REPUTATION SYSTEM filed Jun. 29 2012 both of which are incorporated herein by reference for all purposes.

Businesses are increasingly concerned with their online reputations. For example both positive and negative reviews posted to a review website can impact revenue. As more review websites are created and as more users post more content to those sites it is becoming increasingly difficult for businesses to monitor such sites. Further it can be difficult for businesses to determine whether they need to and how they can improve their online reputations.

The invention can be implemented in numerous ways including as a process an apparatus a system a composition of matter a computer program product embodied on a computer readable storage medium and or a processor such as a processor configured to execute instructions stored on and or provided by a memory coupled to the processor. In this specification these implementations or any other form that the invention may take may be referred to as techniques. In general the order of the steps of disclosed processes may be altered within the scope of the invention. Unless stated otherwise a component such as a processor or a memory described as being configured to perform a task may be implemented as a general component that is temporarily configured to perform the task at a given time or a specific component that is manufactured to perform the task. As used herein the term processor refers to one or more devices circuits and or processing cores configured to process data such as computer program instructions.

A detailed description of one or more embodiments of the invention is provided below along with accompanying figures that illustrate the principles of the invention. The invention is described in connection with such embodiments but the invention is not limited to any embodiment. The scope of the invention is limited only by the claims and the invention encompasses numerous alternatives modifications and equivalents. Numerous specific details are set forth in the following description in order to provide a thorough understanding of the invention. These details are provided for the purpose of example and the invention may be practiced according to the claims without some or all of these specific details. For the purpose of clarity technical material that is known in the technical fields related to the invention has not been described in detail so that the invention is not unnecessarily obscured.

Reputation platform is configured to collect reputation and other data from a variety of sources including review websites social networking websites and other websites . In some embodiments users of platform such as Alice and Bob can also provide offline survey data to platform . In the examples described herein review site is a general purpose review site that allows users to post reviews regarding all types of businesses. Examples of such review sites include Google Places Yahoo Local and Citysearch. Review site is a travel oriented review site that allows users to post reviews of hotels restaurants and attractions. One example of a travel oriented review site is TripAdvisor. Review site is specific to a particular type of business e.g. car dealers . Examples of social networking sites and include Twitter and Foursquare. Social networking sites allow users to take actions such as checking in to locations. Finally personal blog and online forum are examples of other types of websites on the open Web that can contain business reputation information.

Platform is illustrated as a single logical device in . In various embodiments platform is a scalable elastic architecture and may comprise several distributed components including components provided by one or more third parties. Further when platform is referred to as performing a task such as storing data or processing data it is to be understood that a sub component or multiple sub components of platform whether individually or in cooperation with third party components may cooperate to perform that task.

In order to access the services provided by reputation platform Bob first registers for an account with the platform. At the outset of the process he accesses interface e.g. a web based interface and provides information such as a desired username and password. He also provides payment information if applicable . If Bob has created accounts for his business on social networking sites such as sites and Bob can identify those accounts to platform as well.

Next Bob is prompted by platform to provide the name of his business e.g. Bob s Juice Company a physical address of the juice bar e.g. 123 N. Main St. Cupertino Calif. 95014 and the type of business that he owns e.g. restaurant or juice bar . The business information entered by Bob is provided to auto find engine which is configured to locate across sites the respective profiles on those sites pertaining to Bob s business e.g. www.examplereviewsite.com CA Cupertino BobsJuiceCo.html if present. Since Bob has indicated that his business is a juice bar reputation platform will not attempt to locate it on site a car dealer review site but will attempt to locate it within sites and .

In the example shown in sites and make available respective application programming interfaces APIs and that are usable by auto find engine to locate business profiles on their sites. Site does not have a profile finder API. In order to locate a business profile there auto find engine is configured to perform a site specific search using a script that accesses a search engine e.g. through search interface . As one example a query of site www.examplereviewsite.com Bob s Juice Company Cupertino could be submitted to the Google search engine using interface .

Results obtained by auto find engine are provided to verification engine which confirms that information such as the physical address and company name provided by Bob are present in the located profiles. Verification engine can be configured to verify all results including any obtained from site and and can also be configured to verify or otherwise process just those results obtained via interface . As one example for a given query the first ten results obtained from search interface can be examined. The result that has the best match score and also includes the expected business name and physical address is designated as the business s profile at the queried site.

In some embodiments verification engine presents results to Bob for verification that the located profiles correspond to his business. As one example Bob may be shown via interface a set of URLs corresponding to profiles on each of the sites where his business has been located and asked to verify that the profiles are indeed for his business. Once confirmed by Bob the URLs of the profiles also referred to herein as subscriptions and any other appropriate data are stored in database . Examples of such other data include overview information appearing on the business s profile page such as a description of the business and any social data e.g. obtained from sites .

In various embodiments users are given the option by platform to enter the specific URLs corresponding to their business profiles on review sites. For example if Bob knows the URL of the Google Places page corresponding to his business he can provide it to platform and use of auto find engine is omitted or reduced as applicable.

At results of the query or queries performed at are verified. As one example of the processing performed at verification engine performs checks such as confirming that the physical address received at is present in a given result. As another example a user can be asked to confirm that results are correct and if so that confirmation is received as a verification at . Finally at verified results are stored. As one example URLs for each of the verified profiles is stored in database . Although pictured as a single database in in various embodiments platform makes use of multiple storage modules such as multiple databases. Such storage modules may be of different types. For example user account and payment information may be stored in a MySQL database while extracted reputation information described in more detail below may be stored using MongoDB.

Where a business has multiple locations the business owner or a representative of the business such as Alice can be prompted to loop through process for each of the business locations. Physical addresses and or the URLs of the corresponding profiles on sites such as sites can also be provided to platform in a batch rather than by manually entering in information via interface . As one example suppose ACME Convenience Stores has 2 000 locations throughout the United States. Instead of manually entering in the physical location of each of the stores Alice may instead elect to upload to platform a spreadsheet or other file or set of files that includes the applicable information.

Tags associated with each location can also be provided to platform e.g. as name value pairs . For example Alice can tag each of the 2 000 locations with a respective store name Store 1234 manager name Tom Smith region designation West Coast brand ACME Quick vs. Super ACME etc. As needed tags can be edited and deleted and new tags can be added. For example Alice can manually edit a given location s tags e.g. via interface and can also upload a spreadsheet of current tags for all locations that supersede whatever tags are already present for her locations in platform . As will be described in more detail below the tags can be used to segment the business to create custom reports and for other purposes.

Once a business e.g. Bob s Juice Company has an account on reputation platform and once the various subscriptions i.e. the URLs of the business s profiles on the various review sites have been identified and stored in database collecting and processing of review and other data is performed. illustrates an example of components included in embodiments of a reputation platform. In particular illustrates components of platform that are used in conjunction with the ongoing collection and processing of data.

Reputation platform includes a scheduler that periodically instructs collection engine to obtain data from sources such as sites . In some embodiments data from sites and or is also collected by collection engine . Scheduler can be configured to initiate data collection based on a variety of rules. For example it can cause data collection to occur once a day for all businesses across all applicable sites. It can also cause collection to occur with greater frequency for certain businesses e.g. which pay for premium services than others e.g. which have free accounts . Further collection can be performed across all sites e.g. sites with the same frequency or can be performed at different intervals e.g. with collection performed on site once per day and collection performed on site once per week .

In addition to or instead of the scheduled collection of data data collection can also be initiated based on the occurrence of an arbitrary triggering event. For example collection can be triggered based on a login event by a user such as Bob e.g. based on a permanent cookie or password being supplied . Collection can also be triggered based on an on demand refresh request by the user e.g. where Bob clicks on a refresh my data button in interface . Other elements depicted in will be described in conjunction with process shown in .

At a determination is made as to which sites should be accessed. As one example in some embodiments collection engine reviews the set of subscriptions stored in database for Bob s Juice Company. The set of subscriptions associated with Bob s company are the ones that will be used by collection engine during the refresh operation. As previously mentioned a refresh can be performed on behalf of multiple or all businesses instead of an individual one such as Bob s Juice Company. In such a scenario portion of the process can be omitted as applicable.

At information is obtained from the sites determined at . As shown in collection engine makes use of several different types of helpers . Each helper e.g. helper is configured with instructions to fetch data from a particular type of source. As one example although site provides an API for locating business profiles it does not make review data available via an API. Such data is instead scraped by platform accordingly. In particular when a determination is made that reviews associated with Bob s Juice Company on site should be refreshed by platform an instance of helper is executed on platform . Instance is able to extract for a given entry on site various components such as the reviewer s name profile picture review title review text and rating. Helper is configured with instructions for scraping reviews from site . It is similarly able to extract the various components of an entry as posted to site . Site has made available an API for obtaining review information and helper is configured to use that API.

Other types of helpers can extract other types of data. As one example helper is configured to extract check in data from social site using an API provided by site . As yet another example when an instance of helper is executed on platform a search is performed across the World Wide Web for blog forum or other pages that discuss Bob s Juice Company. In some embodiments additional processing is performed on any results of such a search such as sentiment analysis.

In various embodiments information obtained on behalf of a given business is retrieved from different types of sites in accordance with different schedules. For example while review site data might be collected hourly or on demand social data collected from sites may be collected once a day. Data may be collected from sites on the open Web e.g. editorials blogs forums and or other sites not classified as review sites or social sites once a week.

At any new results i.e. those not already present in database are stored in database . As needed the results are processed e.g. by converting reviews into a single canonical format prior to being included in database . In various embodiments database supports heterogeneous records and such processing is omitted or modified as applicable. For example suppose reviews posted to site must include a score on a scale from one to ten while reviews posted to site must include a score on a scale from one to five. Database can be configured to store both types of reviews. In some embodiments the raw score of a review is stored in database as is a converted score e.g. in which all scores are converted to a scale of one to ten . As previously mentioned in some embodiments database is implemented using MongoDB which supports such heterogeneous record formats.

Prior to the first time process is executed with respect to Bob s Juice Company no review data is present in database . Portion of the process is performed for each of the data sources applicable to Bob s business via instances of the applicable helpers and the collected data is stored at . On subsequent refreshes of data pertinent to Bob s company only new changed information is added to database . In various embodiments alerter is configured to alert Bob e.g. via an email message whenever process or a particular portion thereof is performed with respect to his business. In some cases alerts are only sent when new information is observed and or when reputation scores associated with Bob s business described in more detail below change or change by more than a threshold amount.

Platform is configured to determine a variety of reputation scores on behalf of businesses such as Bob s Juice Company. In the case of multiple location businesses such as ACME individual reputation scores are determined for each of the locations and the scores of individual businesses can be aggregated in a variety of ways. As will be described in more detail below the scores provide users with perspective on how their businesses are perceived online. Also as will be described in more detail below users are able to explore the factors that contribute to their businesses reputation scores by manipulating various interface controls and they can also learn how to improve their scores. In the case of multi location businesses such as ACME users can segment the locations in a variety of ways to gain additional insight.

In region of interface a composite reputation score 728 points is depicted on a scale . Example ways of computing a composite score are described in conjunction with . The composite reputation score provides Bob with a quick perspective on how Bob s Juice Company is perceived online. A variety of factors can be considered in determining a composite score. Six example factors are shown in region each of which is discussed below. For each factor Bob can see tips on how to improve his score with respect to that factor by clicking on the appropriate box e.g. box for tips on improving score . In the example shown in a recommendation box is present for each score presented in region . In some embodiments such boxes are only displayed for scores that can should be improved. For example given that score is already very high in some embodiments box is omitted from the interface as displayed to Bob or an alternate message is displayed such as a general encouragement to keep up the good work. 

Overall Score This value reflects the average review score e.g. star rating across all reviews on all review sites. As shown Bob s business has an average rating of 0.50 across all sites. If Bob clicks on box he will be presented with a suggestion such as the following Overall score is the most influential metric. It can appear in both the review site search results and in your general search engine results. Generating a larger volume of positive reviews is the best way to improve the overall score. Typically volume is the best approach as your average happy customer will not write a review without being asked. Additional personalized advice may also be provided such as telling Bob he should click on tab and request five reviews.

Timeliness This score indicates how current a business s reviews are irrespective of whether they are positive or negative . In the example shown reviews older than two months have less of an impact than more recent reviews. Thus if one entity has 200 reviews with an average rating of four stars at least some of which were recently authored and a second entity has the same volume and star rating but none of the reviews were written in the last two months the first entity will have a higher timeliness score and thus a higher composite reputation score. If Bob clicks on box he will be presented with a suggestion such as the following Managing your online reviews is not a one time exercise but a continual investment into your business. Encourage a steady trickle of new reviews on a regular basis to ensure that your reviews don t become stale. Other measures of Timeliness can also be used such as a score that indicates the relative amount of new vs. old positive reviews and new vs. old negative reviews. I.e. to see whether positive or negative reviews dominate in time. 

Length This score indicates the average length of a business s reviews. Longer reviews add weight to the review s rating. If two reviews have the same star rating e.g. one out of five stars but the first review is ten words and the second review is 300 words the second review will be weighted more when computing the composite score. If Bob clicks on box he will be presented with a suggestion such as the following Encourage your positive reviewers to write in depth reviews. They should detail their experiences and highlight what they like about your business. This provides credibility and the guidance makes review writing easier for them. Other measures of Length can also be used such as a score that indicates the relative amount of long vs. short positive reviews and long vs. short negative reviews. I.e. to see whether positive or negative reviews dominate in length. 

Social Factors Reviews that have been marked with social indicators e.g. they have been marked by other members of the review community as being helpful or funny will have more bearing on the outcome of the composite score. By clicking on box Bob will be presented with an appropriate suggestion for improvement.

Reviewer Authority A review written by an established member of a community e.g. who has authored numerous reviews will have a greater impact on the outcome of the composite score than one written by a reviewer with little or no history on a particular review site. In some embodiments the audience of the reviewer is also taken into consideration. For example if the reviewer has a large Twitter following his or her review will have a greater bearing on the outcome of the score. If Bob clicks on box he will be presented with a suggestion such as the following Established reviewers can be a major boon to your review page. Their reviews are rarely questioned and their opinions carry significant weight. If you know that one of your customers is an active reviewer on a review site make a special effort to get him or her to review your business. 

Industry Review sites that are directly related to the vertical in which the entity being reviewed resides are given more weight. For example if the entity being reviewed is a car dealership and the review site caters specifically to reviews about car dealerships the reviews in that specific site will have a greater impact on the outcome of the composite score than those on vertically ambiguous websites. If Bob clicks on box he will be presented with a suggestion such as the following The most important review sites for your business should have your best reviews. Monitor your website analytics to find the sites having the biggest impact on your business and reinforce your presence on those sites. 

In various embodiments of interface additional controls for interactions are made available. For example a control can be provided that allows a user to see individual outlier reviews reviews that contributed the most to deviated the most from the overall score and or individual factors . As one example a one star review that is weighted heavily in the calculation of a score or scores can be surfaced to the user. The user could then attempt to resolve the negative feelings of the individual that wrote the one star review by contacting the individual. As another example a particularly important five star review e.g. due to being written by a person with a very high reviewer authority score can be surfaced to the user allowing the user to contact the reviewer and thank him or her. As yet another example if an otherwise influential review is stale and positive the review can be surfaced to the user so that the user can ask the author to provide an update or otherwise refresh the review.

A variety of weights can be assigned to the above factors when generating the composite score shown in region . Further the factors described above need not all be employed nor need they be employed in the manners described herein. Additional factors can also be used when generating a composite score. An example computation of a composite score is discussed in conjunction with .

In some embodiments whenever Bob accesses platform and or based on the elapsing of a certain amount of time the composite score shown at in is refreshed. In particular scoring engine retrieves from database review and other data pertaining to Bob s business and generates the various scores shown in . Example ways of computing a composite reputation score are as follows.

First scoring engine computes a base score B that is a weighted average of all of the star ratings of all of the individual reviews on all of the sites deemed relevant to Bob s business 

where N is the total number of reviews s is the number of stars for review i normalized to 10 w is the weight for review i is the Heaviside step function and N is the minimum number of reviews needed to score e.g. 4 . The factor 100 is used to expand the score to a value from 0 to 1000.

In the above D is the domain authority which reflects how important the domain is with respect to the business. As one example a doctor focused review site may be a better authority for reviews of doctors than a general purpose review site. One way to determine domain authority values is to use the domain s search engine results page placement using the business name as the keyword.

 R is the reviewer authority. One way to determine reviewer authority is to take the logarithm of 1 the number of reviews written by the reviewer. As explained above a review written by an individual who has authored many reviews is weighted more than one written by a less prolific user.

 S is the social feedback factor. One way to determine the factor is to use the logarithm of 1 the number of pieces of social feedback a review has received.

 L is the length factor. One way to specify this value is to use 1 for short reviews 2 for medium reviews and 4 for long reviews.

 T is the age factor. One way to specify this factor is through the following If the age is less than two months T 1 if the age a in months 2 months then the following value is used max 0.5 

 P is the position factor for review i. The position factor indicates where a given review is positioned among other reviews of the business e.g. it is at the top on the first page of results or it is on the tenth page . One way to compute the position factor is as follows 

In some cases a given site e.g. site may have an overall rating given for the business on the main profile page for that business on the site. In some embodiments the provided overall rating is treated as an additional review with age a aand position p pand given an additional weight factor of 2.

Once the base score has been computed it is normalized to generate B . In some embodiments this is performed by linearly stretching out the range of scores from 8 to 10 to 5 to 10 and linearly squeezing the range of scores from 0 to 8 to 0 to 5.

In some embodiments a correction factor C is used for the number of reviews in a given vertical and locale 

where N is the number of reviews for the business and the median number of reviews is taken for the business s vertical and locale. An example value for a is 0.3 and an example value for b is 0.7.

where N and N are the limits put on the comparator N in the denominator of the argument of the arctan in the correction factor. An example value for N is 4 and an example value for N is 20.

where C is a correction factor e.g. one of the two discussed above B is the normalized base score discussed above and uid is a unique identifier assigned to the business by platform and stored in database . The randomization correction can be used where only a small number of reviews are present for a given business.

As explained above a variety of techniques can be used by scoring engine in determining reputation scores. In some embodiments scores for all types of businesses are computed using the same sets of rules. In other embodiments reputation score computation varies based on industry e.g. reputation scores for car dealers using one approach and or one set of factors and reputation scores for doctors using a different approach and or different set of factors . Scoring engine can be configured to use a best in class entity when determining appropriate thresholds values for entities within a given industry. The following are yet more examples of factors that can be used in generating reputation scores.

Review volume The volume of reviews across all review sites can be used as a factor. For example if the average star rating and the number of reviews are high a conclusion can be reached that the average star rating is more accurate than where an entity has the same average star rating and a lower number of reviews. The star rating will carry more weight in the score if the volume is above a certain threshold. In some embodiments thresholds vary by industry. Further review volume can use more than just a threshold. For example an asymptotic function of number of reviews industry and geolocation of the business can be used as an additional scoring factor.

Multimedia Reviews that have multimedia associated with them e.g. a video review or a photograph can be weighted differently. In some embodiments instead of using a separate multimedia factor the length score of the review is increased e.g. to the maximum value when multimedia is present.

Review Distribution The population of reviews on different sites can be examined and where a review distribution strays from the mean distribution the score can be impacted. As one example if the review distribution is sufficiently outside the expected distribution for a given industry this may indicate that the business is engaged in gaming behavior. The score can be discounted e.g. by 25 accordingly. An example of advice for improving a score based on this factor would be to point out to the user that their distribution of reviews e.g. 200 on site and only 2 on site deviates from what is expected in the user s industry and suggest that the user encourage those who posted reviews to site do so on site as well.

Text Analysis Text analysis can be used to extract features used in the score. For example reviews containing certain key terms e.g. visited or purchased can be weighted differently than those that do not.

At a reputation score for an entity is generated. Various techniques for generating reputation scores are discussed above. Other approaches can also be used such as by determining an average score for each of the plurality of sites and combining those average scores e.g. by multiplying or adding them and normalizing the result . As mentioned above in some embodiments the entity for which the score is generated is a single business e.g. Bob s Juice Company . The score generated at can also be determined as an aggregate across multiple locations e.g. in the case of ACME Convenience Stores and can also be generated across multiple businesses e.g. reputation score for the airline industry and or across all reviews hosted by a site e.g. reputation score for all businesses with profiles on site . One way to generate a score for multiple locations and or multiple businesses is to apply scoring techniques described in conjunction with using as input the pool of reviews that correspond to the multiple locations businesses. Another way to generate a multi location and or multi business reputation score is to determine reputation scores for each of the individual locations and or businesses and then combine the individual scores e.g. through addition multiplication or other appropriate combination function .

Finally at the reputation score is provided as output. As one example a reputation score is provided as output in region of interface . As another example scoring engine can be configured to send reputation scores to users via email e.g. via alerter .

As explained above in addition to providing reputation information for single location businesses such as Bob s Juice Company platform can also provide reputation information for multi location businesses also referred to herein as enterprises . Examples of enterprises include franchises chain stores and any other type of multi location business. The following section describes various ways that enterprise reputation information is made available by platform to users such as Alice who represent such enterprises.

Presented in region is the average reputation score across all 2 000 ACME stores. Region indicates that ACME stores in Alaska have the highest average reputation score while region indicates that ACME stores in Nevada have the lowest average reputation score. A list of the six states in which ACME has the lowest average reputation scores is presented in region along with the respective reputation scores of ACME in those states. The reputation scores depicted in interface can be determined in a variety of ways including by using the techniques described above.

The data that powers the map can be filtered using the dropdown boxes shown in region . The view depicted in region will change based on the filters applied. And the scores and other information presented in regions will refresh to correspond to the filtered locations time ranges. As shown Alice is electing to view a summary of all review data authored in the last year across all ACME locations. Alice can refine the data presented by selecting one or more additional filters e.g. limiting the data shown to just those locations in California or to just those reviews obtained from site that pertain to Nevada locations . The filter options presented are driven by the data meaning that only valid values will be shown. For example if ACME does not have any stores in Wyoming Wyoming will not be shown in dropdown . As another example once Alice selects California from dropdown only Californian cities will be available in dropdown . To revert back to the default view Alice can click on Reset Filters .

Some of the filters available to Alice e.g. make use of the tags that she previously uploaded e.g. during account setup . Other filters e.g. are automatically provided by platform . In various embodiments which filters are shown in region are customizable. For example suppose ACME organizes its stores in accordance with Regions and Zones and that Alice labeled each ACME location with its appropriate Region Zone information during account setup. Through an administrative interface Alice can specify that dropdowns for selecting Region and Zone should be included in region . As another example Alice can opt to have store manager or other manager designations available as a dropdown filter. Optionally Alice could also choose to hide certain dropdowns using the administrative interface.

Suppose Alice would like to learn more about the reputation of ACME s California stores. She hovers or clicks her mouse on region of the map and interface updates into interface as illustrated in which includes a more detailed view for the state. In particular pop up is presented and indicates that across all of ACME s California stores the average reputation score is 3. Further out of the 24 California cities in which ACME has stores the stores in Toluca Lake Studio City and Alhambra have the highest average reputation scores while the stores in South Pasadena Redwood City and North Hollywood have the lowest average reputation scores. Alice can segment the data shown in interface by selecting California from dropdown and one or more individual cities from dropdown e.g. to show just the data associated with stores in Redwood City .

Alice can view more detailed information pertaining to reviews and ratings by clicking tab . Interface makes available in region the individual reviews collected by platform with respect to the filter selections made in region . Alice can further refine which reviews are shown in region by interacting with checkboxes . Summary score information is provided in region and the number of reviews implicated by the filter selections is presented in region . Alice can select one of three different graphs to be shown in region . As shown in the first graph shows how the average rating across the filtered set of reviews has changed over the selected time period. If Alice clicks on region she will be presented with the second graph. As shown in the second graph shows the review volume over the time period. Finally if Alice clicks on region she will be presented with the third graph. As shown in the third graph shows a breakdown of reviews by type e.g. portion of positive negative and neutral reviews .

If Alice clicks on tab she will be presented with interface of which allows her to view a variety of standard reports by selecting them from regions and . Alice can also create and save custom reports. One example report is shown in region . In particular the report indicates for a given date range the average rating on a normalized to 5 scale. A second example report is shown in . Report depicts the locations in the selected data range that are declining in reputation most rapidly. In particular what is depicted is the set of locations that have the largest negative delta in their respective normalized rating between two dates. A third example report is shown in . Report provides a summary of ACME locations in a list format. Column shows each location s average review score normalized to a 5 point scale. Column shows the location s composite reputation score e.g. computed using the techniques described in conjunction with . If desired Alice can instruct platform to email reports such as those listed in region . In particular if Alice clicks on tab she will be presented with an interface that allows her to select which reports to send to which email addresses and on what schedule. As one example Alice can set up a distribution list that includes the email addresses of all ACME board members and can further specify that the board members should receive a copy of the Location vs. Competitors report once per week.

If Alice clicks on tab she will be presented with interface depicted in . Interface shows data obtained from platform by social sites such as sites . As with the review data Alice can apply filters to the social data by interacting with the controls in region and can view various reports by interacting with region .

If Alice clicks on tab she will be presented with the interface shown in which allows her to send an email request for a review. Once an email has been sent the location is tracked and available in interface shown in . In the example shown in Alice is responsible for making decisions such as who to request reviews from and how frequently based on tips provided in region and or her own intuition . In various embodiments platform includes a review request engine that is configured to assist businesses in strategically obtaining additional reviews. In particular the engine can guide businesses through various aspects of review solicitation and can also automatically make decisions on the behalf of those businesses. Recommendations regarding review requests can be presented to users in a variety of ways. For example interface of can present a suggestion that additional reviews be requested if applicable. As another example periodic assessments can be made on behalf of a business and an administrator of the business alerted via email when additional reviews should be solicited.

As explained above e.g. in the section titled Additional Examples of Scoring Embodiments one factor that can be considered in determining a reputation score for a business is the review distribution of the business s reviews. As one example suppose a restaurant has a review distribution as follows Of the total number of reviews of the restaurant that are known to platform 10 of those reviews appear on travel oriented review site 50 of those reviews appear on general purpose review site and 40 of those reviews appear collectively elsewhere. In various embodiments review request engine is configured to compare the review distribution of the business to one or more target distributions and use the comparison to recommend the targeting of additional reviews.

A variety of techniques can be used to determine the target distributions used by review request engine . For example as will be described in more detail below in some embodiments reputation platform is configured to determine industry specific review benchmarks. The benchmarks can reflect industry averages or medians and can also reflect outliers e.g. focusing on data pertaining to the top 20 of businesses in a given industry . Further for a single industry benchmarks can be calculated for different regions e.g. one for Restaurants West Coast and one for Restaurants Mid West . The benchmark information determined by platform can be used to determine target distributions for a business. Benchmark information can also be provided to platform e.g. by a third party rather than or in addition to platform determining the benchmark information itself. In some embodiments a universal target distribution e.g. equal distribution across all review sites or specific predetermined distributions is used globally across all industries.

If a business has a review distribution that is significantly different from a target distribution e.g. the industry specific benchmark the review distribution component of the business s reputation score will be negatively impacted. In various embodiments review request engine uses a business s review distribution and one or more target distributions to determine on which site s additional reviews should be sought.

One example of process is as follows Once a week the review distribution for a single location dry cleaner Mary s Dry Cleaning is determined by platform . In particular it is determined that approximately 30 of Mary s reviews appear on site approximately 30 appear on site and 40 of Mary s reviews appear elsewhere . Suppose a target distribution for a dry cleaning business is 70 site 10 site and 20 remainder. Mary s review distribution is significantly different from the target and so at a determination is made that adjustments to the distribution should be sought. At review request engine provides as output an indication that Mary s could use significantly more reviews on site . The output can take a variety of forms. For example platform can send an email alert to the owner of Mary s Dry Cleaning informing her that she should visit platform to help correct the distribution imbalance. As another example the output can be used internally to platform such as by feeding it as input into a process such as process .

As will be described in more detail below in some embodiments the target distribution is multivariate and includes in addition to a proportion of reviews across various sites information such as target timeliness for the reviews a review volume and or a target average score whether on a per site basis or across all applicable sites . Multivariate target distributions can also be used in process . For example suppose that after a few weeks of requesting reviews e.g. using process the review distribution for Mary s Dry Cleaning is 68 site 12 site and 20 remainder . The site proportions in her current review distribution are quite close to the target. However other aspects of her review distribution may nonetheless deviate significantly from aspects of a multivariate target and need adjusting to bring up her reputation score. For example the industry target may be a total of 100 reviews i.e. total review volume and Mary s Dry Cleaning may only have 80 total reviews. Or the industry target average age of review may be six months while the average age for Mary s Dry Cleaning is nine months. Decisions made at to adjust the existing review distribution can take into account such non site specific aspects as well. In some embodiments these additional aspects of a target distribution are included in the distribution itself e.g. within a multivariate distribution . In other embodiments the additional information is stored separately e.g. in a flat file but is nonetheless used in conjunction with process when determining which sites to target for additional reviews. Additional information regarding multivariate distribution targets is provided below e.g. in the section titled Industry Review Benchmarking .

Another example of process is as follows Once a week the review distribution of each location of a ten location franchise is determined . Comparisons against targets can be done individually on behalf of each location e.g. with ten comparisons being performed against a single industry specific target. Comparisons can also be performed between the locations. For example of the ten locations the location having the review distribution that is closest to the industry specific target can itself be used to create a review target for the other stores. The review distributions of the other stores can be compared against the review distributions of the top store instead of or in addition to being compared against the industry target.

In some embodiments additional processing is performed in conjunction with process . For example as part of or prior to portion of the process a determination can be made as to whether or not the entity has a presence on e.g. has a registered account with each of the sites implicated in the target distribution. If an entity is expected to have a non zero number of reviews on a given site in accordance with the target distribution having a presence on that site is needed. As one example a car dealer business should have an account on review site a car dealer review site . A restaurant need not have an account on the site and indeed may not qualify for an account on the site. If the car dealer business does not have an account with site a variety of actions can be taken by platform . As one example an alert that the car dealer is not registered with a site can be emailed to an administrator of the car dealer s account on platform . As another example the output provided at can include e.g. in a prominent location a recommendation that the reader of the output register for an account with site . In some embodiments platform is configured to register for an account on or otherwise obtain a presence on the site on behalf of the car dealer.

As discussed above review request engine can use a variety of target distributions obtained in a variety of ways in performing process . Two examples of target distributions are depicted in respectively.

The target distributions shown in are stored as groups of lines in a single flat file where an empty line is used as a delimiter between industry records. The first line e.g. indicates the industry classification e.g. Auto Dealership . The second line e.g. indicates a target review volume across all websites e.g. 80 . The third line e.g. indicates the industry average review rating normalized to a 5 point scale e.g. 3.5 . The fourth line e.g. indicates for how long of a period of time a review will be considered fresh e.g. 1 year and thus count in the calculation of a business in that industry s reputation score. In some embodiments in addition to or instead of a specific freshness value a decay factor is included that is used to reduce the impact of a particular review in the calculation of a business s reputation score over time. The remaining lines of the group indicate what percentage of reviews should appear on which review sites. For example 40 of reviews should appear on general purpose review site 10 of reviews should appear on travel review site and 50 of reviews should appear on a review site focused on auto dealers.

As shown in different industries can have different values in their respective records. For example a target review volume for restaurants is 100 the industry average review rating is 4 and the freshness value is two years . The target review distribution is also different.

The target distributions depicted in can be used to model the impact that additional reviews would have for a business. For example for a given car dealer business simulations of additional reviews e.g. five additional positive reviews obtained on site vs. three additional positive reviews obtained on site can be run and a modeled reputation score e.g. using techniques described in Example Score Generation above determined. Whichever simulation results in the highest reputation score can be used to generate output at in process .

A small subset of data that can be included in a distribution also referred to herein as an industry table is depicted in . In various embodiments hundreds of rows i.e. industries sub industries and hundreds of columns i.e. review sites are included in the table. Further additional types of information can be included in table such as freshness values review volume over a period of time e.g. three reviews per week decay factors average scores etc.

As previously explained target distributions can be provided to platform in a variety of ways. As one example an administrator of platform can manually configure the values in the file depicted in . As another example the top business in each category i.e. the business having the highest reputation score can be used as a model and its values copied into the appropriate area of file depicted in whether manually or programmatically. As yet another example process can be used to generate target distribution .

The process begins at when review data is received. As one example at industry benchmarker queries database for information pertaining to all automotive sales reviews. For each automotive sales business e.g. a total of 16 000 dealers summary information such as each dealer s current reputation score current review distribution and current review volume is received at .

At the received data is analyzed to determine one or more benchmarks. As one example benchmarker can be configured to average the information received at into a set of industry average information i.e. the average reputation score for a business in the industry the averaged review distribution and the average review volume . Benchmarker can also be configured to consider only a portion of the information received at when determining a benchmark and or can request information for a subset of businesses at . As one example instead of determining an industry average at benchmarker can consider the information pertaining to only those businesses having reputation scores in the top 20 of the industry being benchmarked. In some embodiments multiple benchmarks are considered e.g. in process when making determinations. For example both an industry average benchmark and a top 20 benchmark can be considered e.g. by being averaged themselves when determining a target distribution for a business.

In some embodiments additional processing is performed at and or occurs after . For example a global importance of a review site e.g. its Page Rank or Alexa Rank is included as a factor in the target distribution or is used to weight a review site s values in table .

In various embodiments the industry benchmarked during process is segmented and multiple benchmarks are determined e.g. one benchmark for each segment along with an industry wide benchmark . As one example suppose the industry being benchmarked is Fast Food Restaurants. In some embodiments in addition to an industry wide benchmark benchmarks are determined for various geographic sub regions. One reason for performing regional benchmarking is that different populations of people may rely on different review websites for review information. For example individuals on the West Coast may rely heavily on site for reviews of restaurants while individuals in the Mid West may rely heavily on a different site. In order to improve its reputation score a restaurant located in Ohio will likely benefit from a review distribution that more closely resembles that of other Mid Western restaurants than a nationwide average distribution.

At a determination is made that at least one individual on the received list should be targeted with a review request. A variety of techniques can be used to make this determination. As one example all potential reviewers received at could be targeted e.g. because the list received at includes an instruction that all members be targeted . As another example suppose as a result of process a determination was made that a business would benefit from more reviews on Google Places. At any members of the list received at that have Google email addresses i.e. gmail.com addresses are selected at . One reason for such a selection is that the individuals with gmail.com addresses will be more likely to write reviews on Google Places because they already have accounts with Google . A similar determination can be made at with respect to other domains such as by selecting individuals with yahoo.com addresses when additional reviews on Yahoo Local are recommended.

Whether or not an individual has already registered with a review site can also be determined and therefore used at in other ways as well. For example some review sites may provide an API that allows platform to confirm whether an individual with a particular email address has an account with that review site. The API might return a yes or no response and may also return a user identifier if applicable e.g. responding with CoolGuy22 when presented with a particular individual s email address . As another example where the site does not provide such an API a third party service may supply mappings between email addresses and review site accounts to platform . As yet another example the automobile dealer could ask the purchaser for a list of review sites the user has accounts on and or can present the customer with a list of review sites and ask the customer to indicate which if any the customer is registered with.

In various embodiments any review site accounts identifiers determined to be associated with the customer are stored in database in a profile for the individual. Other information pertinent to the individual can also be included in the profile such as the number of reviews the user has written across various review sites the average rating per review and verticals e.g. health or restaurants associated with those reviews.

Additional alternate processing is performed at in various embodiments. As one example database can be queried for information pertaining to each of the potential reviewers received at and an analysis can be performed on the results. Individuals who have a history of writing positive reviews in general of writing positive reviews in the same vertical of writing positive reviews in a different vertical of frequently writing reviews of writing high quality reviews e.g. having a certain minimum length or including multimedia irrespective of whether the review itself is positive can be selected. Individuals with no histories and or with any negative aspects to their review histories can be removed from consideration as applicable. In some embodiments an examination of the potential reviewer e.g. an analysis of his or her existing reviews is performed on demand in conjunction with the processing of . In other embodiments reviewer evaluations are performed asynchronously and previously performed assessments e.g. stored in database are used in evaluating potential reviewers at .

In various embodiments review request engine is configured to predict a likelihood that a potential reviewer will author a review and to determine a number of reviews to request to arrive at a target number of reviews. For example suppose a company would benefit from an additional five reviews on site and that there is a 25 chance that any reviewer requested will follow through with a review. In some embodiments engine determines that twenty requests should be sent i.e. to twenty individuals selected from the list received at . Further various thresholding rules can be employed by platform when performing the determination at . For example a determination may have been made e.g. as an outcome of process that a business would benefit from fifty additional reviews being posted to site . However it may also be the case that site employs anti gaming features to identify and neutralize excessive suspicious reviews. In some embodiments platform determines limits on the number of requests to be made and or throttles the rate at which they should be made at .

At transmission of a review request to a potential reviewer is facilitated. The processing of can be performed in a variety of ways. As one example all potential reviewers determined at can be emailed identical review request messages by platform in accordance with a template stored on platform . Information such as the name of the business to be reviewed and the identity of each potential reviewer is obtained from database and used to fill in appropriate fields of the template. In various embodiments different potential reviewers of a given business receive different messages from platform . For example the message can include a specific reference to one or more particular review site s e.g. where the particular reviewer has an account. Thus one potential reviewer might receive a message including the phrase please review us on Site while another might receive a message including the phrase please review us on Site . In various embodiments multiple review sites are mentioned in the request and the position of the respective site varies across the different requests sent to different potential reviewers. For example the request can include a region such as region as depicted in . The ordering of the sites can be based on factors such as the concentration of new reviews needed to maximize a business s score increase and or factors such as where the potential reviewer already has an account and or is otherwise most likely to complete a review.

Where statistical information is known about the potential reviewer e.g. stored in database is information that the reviewer typically writes reviews in the evening or in the morning that information can be used in conjunction with facilitating the transmission of the review request e.g. such that the review is sent at the time of day most likely to result in the recipient writing a review . Where statistical information is not known about the specific potential reviewer statistical information known about other individuals can be used for decision making Different potential reviewers can also be provided messages in different formats. For example some reviewers can be provided with review request messages via email while other reviewers can be provided with review requests via social networking websites via postal mail or other appropriate contact methods.

In various embodiments AB testing is employed by platform in message transmission. For example a small number of requests can be sent some at one time of day and the others at a different time of day or sent on different days of week or with different messaging . Follow up engine can be configured to determine after a period of time e.g. 24 hours how many of the targeted reviewers authored reviews and to use that information as feedback in generating messages for additional potential reviewers. Other information pertaining to the message transmission and its reception can also be tracked. For example message opens and message click throughs and their timing can be tracked and stored in database .

At a determination is made that the potential reviewer to whom the review request was transmitted at has not responded to the request by creating a review. In some embodiments portion of process is performed by follow up engine . As one example when an initial review request is sent e.g. at information associated with that request is stored in database . Follow up engine periodically monitors appropriate review sites to determine whether the potential reviewer has created a review. If engine determines that a review was authored in some embodiments no additional processing is performed by follow up engine e.g. beyond noting that a review has been created and collecting statistical information about the review such as the location of the review and whether the review is positive or negative . In other embodiments platform takes additional actions such as by sending the reviewer a thank you email. In the event it is determined that no review has been created follow up engine determines a follow up action to take regarding the review request.

A variety of follow up actions can be taken and cam be based on a variety of factors. As one example follow up engine can determine from information or any other appropriate source whether the potential reviewer opened the review request email. The follow up engine can also determine whether the potential reviewer clicked on any links included in the email. Follow up engine can select different follow up actions based on these determinations. For example if the potential reviewer did not open the email one appropriate follow up action is to send a second request with a different subject line i.e. in the hopes the potential reviewer will now open the message . If the potential reviewer opened the email but didn t click on any links an alternate message can be included in a follow up request. If the potential reviewer opened the email and clicked on a link but did not author a review another appropriate action can be selected by follow up engine as applicable such as by featuring a different review site or altering the message included in the request. Another example of a follow up action includes contacting the potential reviewer using a different contact method than the originally employed one. For example where a request was originally sent to a given potential reviewer via email follow up engine can determine that a follow up request be sent to the potential reviewer via a social network or via a physical postcard. Another example of a follow up action includes contacting the potential reviewer at a different time of day than was employed in the original request e.g. if the request was originally sent in the morning send a follow up request in the evening .

In various embodiments follow up engine is configured to determine a follow up schedule. For example based on historical information whether about the potential reviewer or based on information pertaining to other reviewers follow up engine may determine that a reminder request asking that the potential reviewer write a review should be sent on a particular date and or at a particular time to increase the likelihood of a review being authored by the potential reviewer. Follow up engine can also determine other scheduling optimizations such as how many total times requests should be made before being abandoned and or what the conditions are for ceasing to ask the potential reviewer for a review. In various embodiments AB testing is employed e.g. with respect to a few potential reviewers that did not write reviews by follow up engine to optimize follow up actions.

One problem for some businesses such as fast food restaurants is that visiting such restaurants and receiving the expected quality of service food is sufficiently routine mundane that most people will not bother to write a positive review of their experience on a site such as site . Only where people experience a significant problem will they be sufficiently motivated to author a review leading to the overall review that is likely unfairly negative.

Illustrated in is an interface to such devices. In region the visitor is asked to provide a rating. In region the visitor is asked to provide additional feedback. And in region the visitor is asked to provide an email address and identify other information such as the purpose of the visitor s visit. In region the visitor is offered an incentive for completing the review but is not required to provide a specific type of review e.g. positive review . When the visitor has completed filling out the information asked in interface the user is asked to click button to submit the review. When the visitor clicks button the device receives the review data at of process . Finally at the device transmits the visitor s review data to platform .

In various embodiments platform is configured to evaluate the review data. If the review data indicates that the visitor is unhappy e.g. a score of one or two a remedial action can be taken potentially while the visitor is still in the store. For example a manager can be alerted that the visitor is unhappy and can attempt to make amends in person. As another example the manager can write to the visitor as soon as possible potentially helping resolve diffuse the visitor s negativity prior to the visitor reaching a computer e.g. at home or at work and submitting a negative review to site . In various embodiments platform is configured to accept business specific rules regarding process . For example a representative of a business can specify that for that business negative is a score of one through three i.e. including neutral reviews or that a positive is a score of 4.5 or better. The business can also specify which actions should be taken e.g. by having a manager alerted to positive reviews not just negative reviews .

If the review data indicates that the visitor is happy e.g. a score of four or five a different action can be taken. As one example platform can automatically contact the visitor via the visitor s self supplied email address provide a copy of the visitor s review information supplied via interface and ask that the visitor post the review to a site such as site or site . As another example if the visitor is still interacting with the device at the time platform can instruct the device to ask the visitor for permission to post the review on the visitor s behalf. As needed the device and or platform can facilitate the posting e.g. by obtaining the user s credentials for a period of time .

Although the foregoing embodiments have been described in some detail for purposes of clarity of understanding the invention is not limited to the details provided. There are many alternative ways of implementing the invention. The disclosed embodiments are illustrative and not restrictive.

