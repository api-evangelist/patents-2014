---

title: Systems and methods for automated interest region detection in retinal images
abstract: Embodiments disclose systems and methods that aid in screening, diagnosis and/or monitoring of medical conditions. The systems and methods may allow, for example, for automated identification and localization of lesions and other anatomical structures from medical data obtained from medical imaging devices, computation of image-based biomarkers including quantification of dynamics of lesions, and/or integration with telemedicine services, programs, or software.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08879813&OS=08879813&RS=08879813
owner: Eyenuk, Inc.
number: 08879813
owner_city: Woodland Hills
owner_country: US
publication_date: 20140430
---
This application claims the benefit of priority under 35 U.S.C. 119 e of U.S. Provisional Application No. 61 893 885 filed Oct. 22 2013 entitled SYSTEMS AND METHODS FOR COMPUTERIZED RETINAL IMAGE ANALYSIS AND COMPUTATION OF IMAGE BASED BIOMARKERS the entire content of which is hereby incorporated by reference herein in its entirety and should be considered a part of this specification. This application was filed on the same day as the following application Ser. No. 14 266 688 titled System and Methods for Automated Enhancement of Retinal Images Ser. No. 14 266 746 titled Systems and Methods for Automatically Generating Descriptions of Retinal Images and Ser. No. 14 266 753 titled Systems and Methods for Processing Retinal Images for Screening of Diseases or Abnormalities all of which are hereby incorporated by reference in their entirety herein.

The inventions disclosed herein were made with government support under Grants EB013585 and TR000377 awarded by the National Institutes of Health. The government has certain rights in the invention.

Imaging of human organs plays a critical role in diagnosis of multiple diseases. This is especially true for the human retina where the presence of a large network of blood vessels and nerves make it a near ideal window for exploring the effects of diseases that harm vision such as diabetic retinopathy seen in diabetic patients cytomegalovirus retinitis seen in HIV AIDS patients glaucoma and so forth or other systemic diseases such as hypertension stroke and so forth . Advances in computer aided image processing and analysis technologies are essential to make imaging based disease diagnosis scalable cost effective and reproducible. Such advances would directly result in effective triage of patients leading to timely treatment and better quality of life.

In one embodiment a computing system for enhancing a retinal image is disclosed. The computing system may include one or more hardware computer processors and one or more storage devices configured to store software instructions configured for execution by the one or more hardware computer processors in order to cause the computing system to access a medial retinal image for enhancement the medical retinal image related to a subject compute a median filtered image Iwith a median computed over a geometric shape at single or multiple scales determine whether intensity at a first pixel location in the medical retinal image I x y is lower than intensity at a same position in the median filtered image I x y if the intensities at the first pixel location is lower then set a value at the first pixel location in an enhanced image to half of a maximum possible intensity value for the medical retinal image Cscaled by a ratio of intensity at medical retinal image to intensity in the median filtered image as expressed by

In an additional embodiment a computer implemented method for enhancing a retinal image is disclosed. The method may include as implemented by one or more computing devices configured with specific executable instructions accessing a medial retinal image for enhancement the medical retinal image related to a subject computing a median filtered image Iwith a median computed over a geometric shape at single or multiple scales determining whether intensity at a first pixel location in the medical retinal image I x y is lower than intensity at a same position in the median filtered image I x y if the intensities at the first pixel location is lower then setting a value at the first pixel location in an enhanced image to half of a maximum possible intensity value for the medical retinal image Cscaled by a ratio of intensity at medical retinal image to intensity in the median filtered image as expressed by

In a further embodiment non transitory computer storage that stores executable program instructions is disclosed. The non transitory computer storage may include instructions that when executed by one or more computing devices configure the one or more computing devices to perform operations including accessing a medial retinal image for enhancement the medical retinal image related to a subject computing a median filtered image Iwith a median computed over a geometric shape at single or multiple scales determining whether intensity at a first pixel location in the medical retinal image I x y is lower than intensity at a same position in the median filtered image I x y if the intensities at the first pixel location is lower then setting a value at the first pixel location in an enhanced image to half of a maximum possible intensity value for the medical retinal image Cscaled by a ratio of intensity at medical retinal image to intensity in the median filtered image as expressed by

In an additional embodiment a computing system for automated detection of active pixels in retinal images is disclosed. The computing system may include one or more hardware computer processors and one or more storage devices configured to store software instructions configured for execution by the one or more hardware computer processors in order to cause the computing system to access a retinal image generate a first median normalized image using the retinal image with a median computed over a first geometric shape of a first size generate a second median normalized image using the retinal image with a median computed over the first geometric shape of a second size the second size different from the first size automatically generate a difference image by computing a difference between the first median normalized image and the second median normalized image generate a binary image by computing a hysteresis threshold of the difference image using at least two thresholds to detect dark and bright structures in the difference image apply a connected component analysis to the binary image to group neighboring pixels of the binary image into a plurality of local regions compute the area of each local region in the plurality of local regions and store the plurality of local regions in a memory of the computing system.

In a further embodiment a computer implemented method for automated detection of active pixels in retinal images is disclosed. The method may include as implemented by one or more computing devices configured with specific executable instructions accessing a retinal image generating a first median normalized image using the retinal image with a median computed over a first geometric shape of a first size generating a second median normalized image using the retinal image with a median computed over the first geometric shape of a second size the second size different from the first size automatically generating a difference image by computing a difference between the first median normalized image and the second median normalized image generating a binary image by computing a hysteresis threshold of the difference image using at least two thresholds to detect dark and bright structures in the difference image applying a connected component analysis to the binary image to group neighboring pixels of the binary image into a plurality of local regions computing the area of each local region in the plurality of local regions and storing the plurality of local regions in a memory.

In another embodiment non transitory computer storage that stores executable program instructions is disclosed. The non transitory computer storage may include instructions that when executed by one or more computing devices configure the one or more computing devices to perform operations including accessing a retinal image generating a first median normalized image using the retinal image with a median computed over a first geometric shape of a first size generating a second median normalized image using the retinal image with a median computed over the first geometric shape of a second size the second size different from the first size automatically generating a difference image by computing a difference between the first median normalized image and the second median normalized image generating a binary image by computing a hysteresis threshold of the difference image using at least two thresholds to detect dark and bright structures in the difference image applying a connected component analysis to the binary image to group neighboring pixels of the binary image into a plurality of local regions computing the area of each local region in the plurality of local regions and storing the plurality of local regions in a memory.

In an additional embodiment a computing system for automated generation of descriptors of local regions within a retinal image is disclosed the computing system may include one or more hardware computer processors and one or more storage devices configured to store software instructions configured for execution by the one or more hardware computer processors in order to cause the computing system to access a retinal image generate a first morphological filtered image using the retinal image with a the said morphological filter computed over a first geometric shape generate a second morphological filtered image using the retinal image with a morphological filter computed over a second geometric shape the second geometric shape having one or more of a different shape or different size from the first geometric shape generate a difference image by computing a difference between the first morphological filtered image and the second morphological filtered image and assign the difference of image pixel values as a descriptor value each descriptor value corresponding to given pixel location of the said retinal image.

In a further embodiment a computer implemented method for automated generation of descriptors of local regions within a retinal image is disclosed. The method may include as implemented by one or more computing devices configured with specific executable instructions accessing a retinal image generating a first morphological filtered image using the retinal image with a the said morphological filter computed over a first geometric shape generating a second morphological filtered image using the retinal image with a morphological filter computed over a second geometric shape the second geometric shape having one or more of a different shape or different size from the first geometric shape generating a difference image by computing a difference between the first morphological filtered image and the second morphological filtered image and assigning the difference of image pixel values as a descriptor value each descriptor value corresponding to given pixel location of the said retinal image.

In another embodiment non transitory computer storage that stores executable program instructions is disclosed. The non transitory computer storage may include instructions that when executed by one or more computing devices configure the one or more computing devices to perform operations including accessing a retinal image generating a first morphological filtered image using the retinal image with a the said morphological filter computed over a first geometric shape generating a second morphological filtered image using the retinal image with a morphological filter computed over a second geometric shape the second geometric shape having one or more of a different shape or different size from the first geometric shape generating a difference image by computing a difference between the first morphological filtered image and the second morphological filtered image and assigning the difference of image pixel values as a descriptor value each descriptor value corresponding to given pixel location of the said retinal image.

In an additional embodiment a computing system for automated processing of retinal images for screening of diseases or abnormalities is disclosed. The computing system may include one or more hardware computer processors and one or more storage devices configured to store software instructions configured for execution by the one or more hardware computer processors in order to cause the computing system to access retinal images related to a patient each of the retinal images comprising a plurality of pixels for each of the retinal images designate a first set of the plurality of pixels as active pixels indicating that they include interesting regions of the retinal image the designating using one or more of conditional number theory single or multi scale interest region detection vasculature analysis or structured ness analysis for each of the retinal images compute descriptors from the retinal image the descriptors including one or more of morphological filterbank descriptors median filterbank descriptors oriented median filterbank descriptors Hessian based descriptors Gaussian derivatives descriptors blob statistics descriptors color descriptors matched filter descriptors path opening and closing based morphological descriptors local binary pattern descriptors local shape descriptors local texture descriptors local Fourier spectral descriptors localized Gabor jets descriptors edge flow descriptors and edge descriptors such as difference of Gaussians focus measure descriptors such as sum modified Laplacian saturation measure descriptors contrast descriptors or noise metric descriptors and classify one or more of a pixel in the plurality of pixels an interesting region within the image the entire retinal image or a collection of retinal images as normal or abnormal using supervised learning utilizing the computed descriptors using one or more of a support vector machine support vector regression k nearest neighbor naive Bayes Fisher linear discriminant neural network deep learning or convolution networks.

In a further embodiment a computer implemented method for automated processing of retinal images for screening of diseases or abnormalities is disclosed. The method may include accessing retinal images related to a patient each of the retinal images comprising a plurality of pixels for each of the retinal images designating a first set of the plurality of pixels as active pixels indicating that they include interesting regions of the retinal image the designating using one or more of conditional number theory single or multi scale interest region detection vasculature analysis or structured ness analysis for each of the retinal images computing descriptors from the retinal image the descriptors including one or more of morphological filterbank descriptors median filterbank descriptors oriented median filterbank descriptors Hessian based descriptors Gaussian derivatives descriptors blob statistics descriptors color descriptors matched filter descriptors path opening and closing based morphological descriptors local binary pattern descriptors local shape descriptors local texture descriptors local Fourier spectral descriptors localized Gabor jets descriptors edge flow descriptors and edge descriptors such as difference of Gaussians focus measure descriptors such as sum modified Laplacian saturation measure descriptors contrast descriptors or noise metric descriptors and classifying one or more of a pixel in the plurality of pixels an interesting region within the image the entire retinal image or a collection of retinal images as normal or abnormal using supervised learning utilizing the computed descriptors using one or more of a support vector machine support vector regression k nearest neighbor naive Bayes Fisher linear discriminant neural network deep learning or convolution networks.

In another embodiment non transitory computer storage that stores executable program instructions is disclosed. The non transitory computer storage may include instructions that when executed by one or more computing devices configure the one or more computing devices to perform operations including accessing retinal images related to a patient each of the retinal images comprising a plurality of pixels for each of the retinal images designating a first set of the plurality of pixels as active pixels indicating that they include interesting regions of the retinal image the designating using one or more of conditional number theory single or multi scale interest region detection vasculature analysis or structured ness analysis for each of the retinal images computing descriptors from the retinal image the descriptors including one or more of morphological filterbank descriptors median filterbank descriptors oriented median filterbank descriptors Hessian based descriptors Gaussian derivatives descriptors blob statistics descriptors color descriptors matched filter descriptors path opening and closing based morphological descriptors local binary pattern descriptors local shape descriptors local texture descriptors local Fourier spectral descriptors localized Gabor jets descriptors edge flow descriptors and edge descriptors such as difference of Gaussians focus measure descriptors such as sum modified Laplacian saturation measure descriptors contrast descriptors or noise metric descriptors and classifying one or more of a pixel in the plurality of pixels an interesting region within the image the entire retinal image or a collection of retinal images as normal or abnormal using supervised learning utilizing the computed descriptors using one or more of a support vector machine support vector regression k nearest neighbor naive Bayes Fisher linear discriminant neural network deep learning or convolution networks.

In an additional embodiment a computing system for automated computation of image based lesion biomarkers for disease analysis is disclosed. The computing system may include one or more hardware computer processors and one or more storage devices configured to store software instructions configured for execution by the one or more hardware computer processors in order to cause the computing system to access a first set of retinal images related to one or more visits from a patient each of the retinal images in the first set comprising a plurality of pixels access a second set of retinal images related to a current visit from the patient each of the retinal images in the second set comprising a plurality of pixels perform lesion analysis comprising detecting interesting pixels computing descriptors from the images and classifying active regions using machine learning techniques conduct image to image registration of a second image from the second set and a first image from the first set using retinal image registration the registration comprising identifying pixels in the first image as landmarks identifying pixels in the second image as landmarks computing descriptors at landmark pixels matching descriptors across the first image and the second image and estimating a transformation model to align the first image and the second image compute changes in lesions and anatomical structures in registered images and quantify the changes in terms of statistics wherein the computed statistics represent the image based biomarker that can be used for one or more of monitoring progression early detection or monitoring effectiveness of treatment or therapy.

In a further embodiment a computer implemented method for automated computation of image based lesion biomarkers for disease analysis is disclosed. The method may include accessing a first set of retinal images related to one or more visits from a patient each of the retinal images in the first set comprising a plurality of pixels accessing a second set of retinal images related to a current visit from the patient each of the retinal images in the second set comprising a plurality of pixels performing lesion analysis comprising detecting interesting pixels computing descriptors from the images and classifying active regions using machine learning techniques conducting image to image registration of a second image from the second set and a first image from the first set using retinal image registration the registration comprising identifying pixels in the first image as landmarks identifying pixels in the second image as landmarks computing descriptors at landmark pixels matching descriptors across the first image and the second image and estimating a transformation model to align the first image and the second image computing changes in lesions and anatomical structures in registered images and quantifying the changes in terms of statistics wherein the computed statistics represent the image based biomarker that can be used for one or more of monitoring progression early detection or monitoring effectiveness of treatment or therapy.

In another embodiment non transitory computer storage that stores executable program instructions is disclosed. The non transitory computer storage may include instructions that when executed by one or more computing devices configure the one or more computing devices to perform operations including accessing a first set of retinal images related to one or more visits from a patient each of the retinal images in the first set comprising a plurality of pixels accessing a second set of retinal images related to a current visit from the patient each of the retinal images in the second set comprising a plurality of pixels performing lesion analysis comprising detecting interesting pixels computing descriptors from the images and classifying active regions using machine learning techniques conducting image to image registration of a second image from the second set and a first image from the first set using retinal image registration the registration comprising identifying pixels in the first image as landmarks identifying pixels in the second image as landmarks computing descriptors at landmark pixels matching descriptors across the first image and the second image and estimating a transformation model to align the first image and the second image computing changes in lesions and anatomical structures in registered images and quantifying the changes in terms of statistics wherein the computed statistics represent the image based biomarker that can be used for one or more of monitoring progression early detection or monitoring effectiveness of treatment or therapy.

In an additional embodiment a computing system for identifying the quality of an image to infer its appropriateness for manual or automatic grading id disclosed. The computing system may include one or more hardware computer processors and one or more storage devices configured to store software instructions configured for execution by the one or more hardware computer processors in order to cause the computing system to access a retinal image related to a subject automatically compute descriptors from the retinal image the descriptors comprising a vector of a plurality of values for capturing a particular quality of an image and including one or more of focus measure descriptors saturation measure descriptors contrast descriptors color descriptors texture descriptors or noise metric descriptors and use the descriptors to classify image suitability for grading comprising one or more of support vector machine support vector regression k nearest neighbor naive Bayes Fisher linear discriminant neural network deep learning or convolution networks.

In a further embodiment a computer implemented method for identifying the quality of an image to infer its appropriateness for manual or automatic grading. The method may include accessing a retinal image related to a subject automatically computing descriptors from the retinal image the descriptors comprising a vector of a plurality of values for capturing a particular quality of an image and including one or more of focus measure descriptors saturation measure descriptors contrast descriptors color descriptors texture descriptors or noise metric descriptors and using the descriptors to classify image suitability for grading comprising one or more of support vector machine support vector regression k nearest neighbor naive Bayes Fisher linear discriminant neural network deep learning or convolution networks.

In another embodiment non transitory computer storage that stores executable program instructions is disclosed. The non transitory computer storage may include instructions that when executed by one or more computing devices configure the one or more computing devices to perform operations including accessing a retinal image related to a subject automatically computing descriptors from the retinal image the descriptors comprising a vector of a plurality of values for capturing a particular quality of an image and including one or more of focus measure descriptors saturation measure descriptors contrast descriptors color descriptors texture descriptors or noise metric descriptors and using the descriptors to classify image suitability for grading comprising one or more of support vector machine support vector regression k nearest neighbor naive Bayes Fisher linear discriminant neural network deep learning or convolution networks.

In one embodiment of the system a retinal fundus image is acquired from a patient then active or interesting regions comprising active pixels from the image are determined using multi scale background estimation. The inherent scale and orientation at which these active pixels are described is determined automatically. A local description of the pixels may be formed using one or more of median filterbank descriptors shape descriptors edge flow descriptors spectral descriptors mutual information or local texture descriptors. One embodiment of the system provides a framework that allows computation of these descriptors at multiple scales. In addition supervised learning and classification can be used to obtain a prediction for each pixel for each class of lesion or retinal anatomical structure such as optic nerve head veins arteries and or fovea. A joint segmentation recognition method can be used to recognize and localize the lesions and retinal structures. In one embodiment of the system this lesion information is further processed to generate a prediction score indicating the severity of retinopathy in the patient which provides context determining potential further operations such as clinical referral or recommendations for the next screening date. In another embodiment of the system the automated detection of retinal image lesions is performed using images obtained from prior and current visits of the same patient. These images may be registered using the disclosed system. This registration allows for the alignment of images such that the anatomical structures overlap and for the automated quantification of changes to the lesions. In addition system may compute quantities including but not limited to appearance and disappearance rates of lesions such as microaneurysms and quantification of changes in number area perimeter location distance from fovea or distance from optic nerve head. These quantities can be used as image based biomarker for monitoring progression early detection or evaluating efficacy of treatment among many other uses.

Retinal diseases in humans can be manifestations of different physiological or pathological conditions such as diabetes that causes diabetic retinopathy cytomegalovirus that causes retinitis in immune system compromised patients with HIV AIDS intraocular pressure buildup that results in optic neuropathy leading to glaucoma age related degeneration of macula seen in seniors and so forth. Of late improved longevity and stationary stress filled lifestyles have resulted in a rapid increase in the number of patients suffering from these vision threatening conditions. There is an urgent need for a large scale improvement in the way in which these diseases are screened diagnosed and treated.

Diabetes mellitus DM in particular is a chronic disease which impairs the body s ability to metabolize glucose. Diabetic retinopathy DR is a common microvascular complication of diabetes in which damaged retinal blood vessels become leaky or occluded leading to vision loss. Clinical trials have demonstrated that early detection and treatment of DR can reduce vision loss by 90 . Despite its preventable nature DR is the leading cause of blindness in the adult working age population. Technologies that allow early screening of diabetic patients who are likely to progress rapidly would greatly help reduce the toll taken by this blinding eye disease. This is especially important because DR progresses without much pain or discomfort until the patient suffers actual vision loss at which point it is often too late for effective treatment. Worldwide 371 million people suffer from diabetes and this number is expected to grow to half a billion by 2030. The current clinical guideline is to recommend annual DR screening for everyone diagnosed with diabetes. However the majority of diabetics do not get their annual screening for many reasons including lack of access to ophthalmology clinicians lack of insurance or lack of education. Even if the patients have knowledge and experience the number of clinicians screening for DR is an order of magnitude less than that required to screen the current diabetic population. This is as true for first world countries including America and Europe as it is for the developing world. The exponentially growing need for DR screening can be met effectively by a computer aided DR screening system provided it is robust scalable and fast.

For effective DR screening of diabetics telescreening programs are being implemented worldwide. These programs use fundus photography using a fundus camera typically deployed at a primary care facility where the diabetic patients normally go for monitoring and treatment. Such telemedicine programs significantly help in expanding the DR screening but are still limited by the need for human grading of the fundus photographs which is typically performed at a reading center.

Methods and systems are disclosed that provide automated image analysis allowing detection screening and or monitoring of retinal abnormalities including diabetic retinopathy macular degeneration glaucoma retinopathy of prematurity cytomegalovirus retinitis and hypertensive retinopathy.

In some embodiments the methods and systems can be used to conduct automated screening of patients with one or more retinal diseases. In one embodiment this is accomplished by first identifying interesting regions in an image of a patient s eye for further analysis followed by computation of a plurality of descriptors of interesting pixels identified within the image. In this embodiment these descriptors are used for training a machine learning algorithm such as support vector machine deep learning neural network naive Bayes and or k nearest neighbor. In one embodiment these classification methods are used to generate decision statistics for each pixel and histograms for these pixel level decision statistics are used to train another classifier such as one of those mentioned above to allow screening of one or more images of the patient s eye. In one embodiment a dictionary of descriptor sets are formed using a clustering method such as k means and this dictionary is used to form a histogram of codewords for an image. In one embodiment the histogram descriptors are combined with the decision statistics histogram descriptors before training image level eye level and or encounter level classifiers. In one embodiment multiple classifiers are each trained for specific lesion types and or for different diseases. A score for a particular element can be generated by computing the distance of the given element from the classification boundary. In one embodiment the screening system is further included in a telemedicine system and the screening score is presented to a user of the telemedicine system.

The methods and systems can also be used to conduct automated identification and localization of lesions related to retinal diseases including but not limited to diabetic retinopathy macular degeneration retinopathy of prematurity or cytomegalovirus retinitis.

The methods and systems can also be used to compute biomarkers for retinal diseases based on images taken at different time intervals for example approximately once every year or about six months. In one embodiment the images of a patient s eye from different visits are co registered. The use of a lesion localization module allows for the detection of lesions as well as a quantification of changes in the patient s lesions over time which is used as an image based biomarker.

The methods and systems can also be used to conduct co registration of retinal images. In one embodiment these images could be of different fields of the eye and in another embodiment these images could have been taken at different times.

The methods and systems can also be used to enhance images to make it easier to visualize the lesions by a human observer or for analysis by an automated image analysis system.

The systems and methods disclosed herein include an automated screening system that processes automated image analysis algorithms that can automatically evaluate fundus photographs to triage patients with signs of diabetic retinopathy DR and other eye diseases. An automated telescreening system can assist an at risk population by helping reduce the backlog in one or more of the following ways.

For example to screen an estimated 371 million diabetics worldwide and to scale the screening operation as the diabetic population grows to over half a billion by 2030 one embodiment of the automated screening system can be deployed at massive scales. At these numbers it is recognized that automation is not simply a cost cutting measure to save the time spent by the ophthalmologists but rather it is the only realistic way to screen such large growing patient population.

The critical need for computerized retinal image screening has resulted in numerous academic and a few commercial efforts at addressing the problem of identifying and triaging patients with retinal diseases using automatic analysis of fundus photographs. For successful deployment automated screening systems may include one or more of the following features 

For automated telescreening to gain acceptance among clinicians and administrators the accuracy sensitivity and specificity should be high enough to match trained human graders though not necessarily retina experts. Studies suggest that sensitivity of 85 with high enough specificity is a good target but other sensitivity levels may be acceptable.

Many prior approaches work by using algorithms that learn directly or indirectly from a set of examples of already graded fundus images. This training data could have a key influence on the sensitivity and specificity of the algorithm. An algorithm whose behavior varies significantly between datasets is not preferred in some embodiments. Instead in some embodiments the computerized screening algorithm performs well on cross dataset testing that is the algorithm generalizes well when trained on one dataset and tested on another. Hence what is sometimes desired is a system that can generalize in a robust fashion performing well in a cross dataset testing scenario.

In a deployed setup an algorithm does not have control over the make or model of the camera the illumination the skill level of the technician or the size of the patient s pupil. Hence in some embodiments a computerized retinal disease screening system is configured to work in varying imaging conditions.

In some embodiments a screening system processes and grades large growing databases of patient images. The speed at which the algorithm performs grading can be important. In addition testing time for a new image to be screened remains constant even as the database grows such that it does not take longer to screen a new test image as the database size increases as more patients are screened. What is sometimes desired is a method that takes a constant time to evaluate a new set of patient images even as the database size grows.

In some embodiments the system does not disrupt the existing workflow that users are currently used to. This means that the system inter operates with a variety of existing software. What is sometimes desired is a system that can be flexibly incorporated into existing software and devices.

Customized methods for low level description of medical image characteristics that can lead to accuracy improvement is another potential feature. Furthermore approaches that leverage information such as local scale and orientation within local image regions in medical images leading to greater accuracy in lesion detection could also provide many benefits.

In addition the availability of an effective biomarker a measurable quantity that correlates with the clinical progression of the disease and greatly enhances the clinical care available to the patients. It could also positively impact drug research facilitating early and reliable determination of biological efficacy of potential new therapies. It will be a greatly added benefit if the biomarker is based only on images which would lead to non invasive and inexpensive techniques. Because retinal vascular changes often reflect or mimic changes in other end organs such as the kidney or the heart the biomarker may also prove to be a valuable assay of the overall systemic vascular state of a patient with diabetes.

Lesion dynamics such as microaneurysm MA turnover have received less attention from academia or industry. Thus a system that improves the lesion detection and localization accuracy could be beneficial. Furthermore a system and method for computation of changes in retinal image lesions over successive visits would also be of value by leading to a variety of image based biomarkers that could help monitor the progression of diseases.

Certain aspects advantages and novel features of the systems and methods have been and are described herein. It is to be understood that not necessarily all such advantages or features may be achieved in accordance with any particular embodiment. Thus for example those skilled in the art will recognize that the systems and methods may be embodied or carried out in a manner that achieves one advantage feature or group of advantages features as taught herein without necessarily achieving other advantages features as may be taught or suggested herein.

In some embodiments the systems and methods provide for various features of automated low level image processing which may include image enhancement or image level processing blocks.

In some embodiments the system may also make it easier for a human or an automated system to evaluate a retinal image and to visualize and quantify retinal abnormalities. Retinal fundus images can be acquired from a wide variety of cameras under varying amounts of illumination by different technicians and on different people. From an image processing point of view these images have different colors levels different dynamic ranges and different sensor noise levels. This makes it difficult for a system to operate on these images using the same parameters. Human image graders or experts may also find it a hindrance that the images often look very different overall. Therefore in some embodiments the image enhancement process applies filters on the images to enhance them in such a way that their appearance is neutralized. After this image enhancement processing the enhanced images can be processed by the same algorithms using identical or substantially similar parameters.

In one embodiment the images are first subjected to an edge preserving bilateral filter such as the filter disclosed in Carlo Tomasi and Roberto Manduchi Bilateral Filtering for Gray and Color Images in 19981998 839 846 and Ben Weiss Fast Median and Bilateral Filtering in vol. 25 2006 519 526. The filter removes noise without affecting important landmarks such as lesions and vessels.

In one embodiment the system then uses a median filter based normalization technique referred to as median normalization to locally enhance the image at each pixel using local background estimation. In some embodiments the median normalized image intensity Iat pixel location x y is computed as 

Typically retinal fundus photographs have a central circular region of the eye visible with a dark border surrounding it. Sometimes information pertaining to the patient or the field number may also be embedded in the corners of the photograph. For retinal image analysis these border regions of the photograph do not provide any useful information and therefore it is desirable to ignore them. In one embodiment border regions of the retinal photographs are automatically identified using morphological filtering operations as described below.

In one embodiment the input image is first blurred using a median filter. A binary mask is then generated by thresholding this image so that locations with pixel intensity values above a certain threshold are set to 1 in the mask while other areas are set to 0. The threshold is empirically chosen so as to nullify the pixel intensity variations in the border regions so that they go to 0 during thresholding. In one embodiment this threshold is automatically estimated. The binary mask is then subjected to region dilation and erosion morphological operations to obtain the final mask. In one embodiment the median filter uses a radius of 5 pixels and the threshold for binary mask generation is 15 for an 8 bit image with pixel values ranging from 0 255 though other radii and thresholds can be used. The dilation and erosion operations can be performed using rectangular structuring elements such as for example size 10 and 20 pixels respectively. and show two different retinal image types and and show embodiments of fundus masks for these two images generated using the above described embodiment.

In some embodiments it may be beneficial to detect the optic nerve heard ONH within a retinal image. A ONH can be robustly detected using an approach that mirrors the one for lesions as described in section below entitled Lesion Localization . In another embodiment multi resolution decomposition and template matching is employed for ONH localization.

In one embodiment the ONH localization is performed on a full resolution retinal fundus image or a resized version of the image or the image full or resized processed using one or more morphological filters that can be chosen from minimum filter or maximum filter dilation filter morphological wavelet filter or the like. An approximate location of the ONH is first estimated in the horizontal direction by filtering horizontal strips of the image whose height is equal to the typical ONH diameter and width is equal to the image width with a filter kernel of size approximately equal to the typical ONH size. The filter kernel can be a circle of specific radius square of specific side and orientation Gaussian of specific sigmas that is standard deviations ellipse of specific orientation and axes rectangle of specific orientation and sides or a regular polygon of specific side. The filtered image strips are converted to a one dimensional signal by collating the data along the vertical dimension by averaging or taking the maximum or minimum or the like. The largest N local maxima of the one dimensional signal whose spatial locations are considerably apart are considered as likely horizontal locations of the ONH since the ONH is expected to be a bright region. In a similar fashion the vertical position of the ONH is approximated by examining vertical image strips centered about the N approximate horizontal positions. This ONH position approximation technique produces M approximate locations for the ONH.

In one embodiment the approximate sizes or radii of the possible ONHs can be estimated by using a segmentation algorithm such as the marker controlled watershed algorithm. In one embodiment the markers are placed based on the knowledge of the fundus mask and approximate ONH location. In another embodiment typical ONH sizes or radii can also be used as approximate ONH sizes or radii.

In one embodiment these approximate locations and sizes for the ONH can be refined by performing template matching in a neighborhood about these approximate ONH locations and choosing the one location and size that gives the maximum confidence or probability of ONH presence.

In another embodiment the ONH position can be estimated as the vertex of the parabola approximation to the major vascular arch.

Different retinal fundus cameras capture images at varying resolutions and field of view. In order to process these different resolution images using the other blocks in one embodiment the images are standardized by scaling them to have identical or near identical pixel pitch. The pixel pitch is computed using the resolution of the image and field of view information from the metadata. In one embodiment if a field of view information is absent then the pixel pitch is estimated by measuring the optic nerve head ONH size in the image as described in the section above entitled Optic Nerve Head Detection. In one embodiment an average ONH size of 2 mm can be used. The image at the end of size standardization is referred to as I. The fundus mask is generated for Iand can be used for further processing. In another embodiment the diameter of the fundus mask is used as a standard quantity for the pitch. The diameter may be calculated as described in the section above entitled Image Level Fundus Mask Generation or in the section below entitled Encounter Level Fundus Mask Generation. 

Fundus images usually have visible sensor noise that can potentially hamper lesion localization or detection. In order to reduce the effect of noise while preserving lesion and vessel structures in one embodiment a bilateral filter may be used such as for example the filter disclosed in Tomasi and Manduchi Bilateral Filtering for Gray and Color Images and Weiss Fast Median and Bilateral Filtering. Bilateral filtering is a normalized convolution operation in which the weighting for each pixel p is determined by the spatial distance from the center pixel s as well as its relative difference in intensity. In one embodiment for input image I output image J and window the bilateral filtering operation is defined as follows 

While capturing images using commercial cameras retinal cameras or medical imaging equipment several images could be captured in a short duration of time without changing the imaging hardware. These images will have certain similar characteristics that can be utilized for various tasks such as image segmentation detection or analysis. However the images possibly may have different fields of view or illumination conditions.

In particular medical or retinal images captured during a patient visit are often captured using the same imaging set up. The set of these images is termed an encounter of that patient on that date. For the specific case of retinal images data from multiple images in an encounter can be used to produce fundus segmentation masks and detect image artifacts due to dust or blemishes as described in the sections that follow.

Many medical images such as those acquired using ultrasound equipment and those of the retina have useful information only in a portion of the rectangular image. In particular most retinal fundus photographs have a central circle like region of the eye visible with the remainder of the photograph being dark. Information pertaining to the patient or the field number may be embedded in the regions of the photograph that do not contain useful image information. Therefore before analysis of such photographs it is desirable to identify regions of the photographs with useful image information using computer aided processes and algorithms. One benefit of such identification is that it reduces the chances of false positives in the border regions. Additionally this identification can reduce the analysis complexity and time for these images since a subset of pixels in the photographs is to be processed and analyzed.

In particular for retinal images prior techniques to determine fundus masks include processing one retinal image at a time which are based on thresholding the pixel intensities in the retinal image. Although these image level fundus mask generation algorithms may be accurate for some retinal fundus photographs they could fail for photographs that have dark fundus regions such as those shown in . The failure of the image level fundus mask generation algorithm as in and is primarily due to the pixel intensity thresholding operation that discards dark regions that have low pixel intensities in the images shown in and .

The drawbacks of image level fundus mask generation can be overcome by computing a fundus mask using multiple images in an encounter that is a given visit of a given patient. For example three or more images in an encounter may be used if the images in the encounter have been captured using the same imaging hardware and settings and hence have the same fundus mask. Therefore the encounter level fundus mask computed using data from multiple images in an encounter will be more robust for low pixel intensities in the regions with useful image information.

Embodiments of encounter level fundus masks generated using multiple images within an encounter are shown in J K and L. It can be noted that in pixels with low intensity values that are within the fundus regions are correctly identified by the encounter level fundus mask shown in unlike in the image level fundus masks shown in .

In one embodiment the fundus mask generation algorithm validates that the images in an encounter share the same fundus mask by computing the image level fundus masks and ensuring that the two masks obtained differ in less than for example 10 of the total number of pixels in each image by logically AND ing and OR ing the individual image level fundus masks. If the assumption is not validated the image level fundus masks are used and the encounter level fundus masks are not calculated. Median values of absolute differences that are close to zero can be identified by hysteresis thresholding for example by using techniques disclosed in John Canny A Computational Approach to Edge Detection no. 6 1986 679 698. In one embodiment the upper threshold is set to 2 and the lower threshold is set to 3 such that medians of the pixel values are determined to be small if they are less than 15 the same value used for thresholding pixel values during image level fundus mask generation.

Dust and blemishes in the lens or sensor of an imaging device manifest as artifacts in the images captured using that device. In medical images these dust and blemish artifacts can be mistaken to be pathological manifestations. In particular in retinal images the dust and blemish artifacts can be mistaken for lesions by both human readers and image analysis algorithms. However detecting these artifacts using individual images is difficult since the artifacts might be indistinguishable from other structures in the image. Moreover since images in an encounter are often captured using the same imaging device and settings the blemish artifacts in these images will be co located and similar looking. Therefore it can be beneficial to detect the dust and blemish artifacts using multiple images within an encounter. Image artifacts due to dust and blemishes on the lens or in the sensor are termed as lens dust artifacts for simplicity and brevity since they can be detected using similar techniques within the framework described below.

Additional information about embodiments of each of these blocks of the lens dust detection algorithm is discussed below. In one embodiment lens dust detection is disabled if there are fewer than three images in the encounter since in such a case the lens dust artifacts detected may not be reliable. Moreover the lens dust detection uses the red and blue channels of the photographs since vessels and other retinal structures are most visible in the green channel and can accidentally align in small regions and be misconstrued as lens dust artifacts. The lens dust artifacts are detected using multiple images in the encounter as described below and indicated by a binary lens dust mask which has true values at pixels most likely due to lens dust artifacts.

In one embodiment noise may be removed from the images in the encounter using the algorithm described in the section above entitled Noise Removal . These denoised images are denoted as I I . . . Iwhere N is the total number of images in the encounter and the individual channels of the denoised images are denoted as Iwhere c r and b indicates which of the red or blue channels is being considered. If N 3 and the image level fundus masks are consistent for example as determined by performing encounter level fundus mask generation the input images comprising the red and blue channels are individually normalized and or enhanced using the processes described in the section above entitled Image Enhancement. As shown in for each channel of each input image I two enhanced images are generated using different radii for the median filter Iwith radius h and Iwith radius l . The difference between the two enhanced images I I I is calculated and hysteresis thresholded using different thresholds and to detect dark and bright structures from which large and elongated structures are removed and . The masks indicating the bright and dark structures are logically OR ed to obtain the mask M.

As shown in the mask Mfor the red channel and the mask Mfor the blue channel are further logically OR ed to get a single mask M showing locations of bright and dark structures that are likely to be lens dust artifacts in the image I. If a spatial location is indicated as being part of a bright or dark structure in more than 50 of the images in the encounter it is likely that a lens dust artifact is present at that pixel location. This is indicated in a binary mask M.

The normalized images I i 1 2 . . . N c r b are processed using a Gaussian blurring filter to obtain smoothed versions I as shown in . Then as shown in pair wise absolute differences of these smoothed normalized images are generated. In one embodiment the difference between the 80th percentile a high percentile for example and 20th percentile a lower percentile for example of these absolute differences is computed as Iand hysteresis thresholded to obtain a mask M c r b that indicates the spatial image locations where the images are similar within each of the red and blue channels.

Finally as illustrated in the lens dust mask for the images in the encounter is obtained by logically AND ing the mask M indicating co located structures and the logically OR ed per channel similarity masks M c r b .

In one embodiment median filter radii of h 100 pixels and l 5 pixels are used to normalize the images. The hysteresis thresholding of the median normalized difference Ito obtain the bright mask is performed using an upper threshold that is the maximum of 50 and the 99th percentile of the difference values and a lower threshold that is the maximum of 40 and the 97th percentile of the difference values. The dark mask is obtained by hysteresis thresholding I the negative of the median normalized difference with an upper threshold for example the minimum of 60 and the 99th percentile of Iand a lower threshold that is the minimum of 50 and the 97th percentile of I. In one embodiment groups of pixels with eccentricity less than 0.97 and with more than 6400 pixels are discarded. The smoothed normalized image Iis obtained using a Gaussian smoothing filter with 2. To obtain the similarity mask as shown in I the negative difference of the 80th and 20th percentile of the pair wise absolute differences of I is hysteresis thresholded with an upper threshold that is the maximum of 5 and 95th percentile of Iand a lower threshold that is the minimum of 12 and 90th percentile of I. However it is recognized that other values may be used to implement the processor.

Typically a large percentage of a retinal image comprises of background retina pixels which do not contain any interesting pathological or anatomic structures. Identifying interesting pixels for future processing can provide significant improvement in processing time and in reducing false positives. To extract interesting pixels for a given query multi scale morphological filterbank analysis is used. This analysis allows the systems and methods to be used to construct interest region detectors specific to lesions of interest. Accordingly a query or request can be submitted which has parameters specific to a particular concern. As one example the query may request the system to return bright blobs larger than 64 pixels in area but smaller than 400 pixels or red elongated structures that are larger than 900 pixels . A blob includes a group of pixels with common local image properties.

Retinal fundus image Iis scaled down by factor f n times and scaled images I I. . . Iare obtained. In one embodiment the ratio between different scales is set to 0.8 and 15 scales are used. At each scale s the median normalized images Iand Iare computed with radius rand r r ras defined by Equation 1 where S defined as a circle of radius r. In one embodiment values of r 7 and r 3 can be used. Then the difference image I I Iis convolved with a Gaussian kernel and gradients L x y L x y L x y and L x y are computed on this image. The Hessian H is computed at each pixel location x y of the difference as 

In another embodiment Fis obtained after hysteresis thresholding Iin 3 above with the high threshold tand low threshold t. This approach may lead to a larger number of interesting points being picked.

In another embodiment the maximum number of interesting areas or blobs that are detected for each scale can be restricted. This approach may lead to better screening performance. Blobs can be ranked based on the determinant of Hessian score. Only the top M blobs per scale based on this determinant of Hessian based ranking are preserved in the interest region mask. Alternatively a blob contrast number can be used to rank the blobs where the contrast number is generated by computing mean maximum or median of intensity of each pixel within the blob or by using a contrast measure including but not limited to Michelson contrast. The top M blobs per scale based on this contrast ranking are preserved in the interest region mask. Alternatively at each scale the union of the top M blobs based on contrast ranking and the top N blobs based on determinant of Hessian based ranking can be used to generate the interest region mask. Blobs that were elongated potentially belong to vessels and can be explicitly excluded from this mask. Blobs might be approximately circular or elongated. Approximately circular blobs may often represent lesions. Elongated blobs represent vasculature. The top blobs are retained at each scale and this is used to generate the Pmask. The resultant Pis then used to pick the detected pixels. Another variation used for Pmask generation was logical OR of the mask obtained with top ranked blobs based on the doh score and the contrast score. Blot hemorrhages can be included by applying a minimum filter at each scale to obtain Grather than using the median normalized difference image.

The pixels in the pruned mask Zat each of the scale sare rescaled to scale sand the result is a set of pixels marked for further lesion analysis. This leads to natural sampling of large lesion blobs choosing a subset of pixels in large blobs rather than using all the pixels. In one embodiment on average retinal fundus images with over 5 million pixels can be reduced to about 25 000 interesting pixels leading to elimination of 99.5 of the total pixels. shows the detected interest regions for an example retinal image of .

As part of the automated detection in one embodiment the system may be configured to process the retinal image and during such processing progressively scale up or down the retinal image using a fixed scaling factor designate groups of neighboring pixels within a retinal image as active areas and include the active areas from each scale as interest regions across multiple scales.

The pixels or the local image regions flagged as interesting by the method described above in the section entitled Interest Region Detection can be described using a number or a vector of numbers that form the local region descriptor . In one embodiment these descriptors are generated by computing two morphologically filtered images with the morphological filter computed over geometric shaped local regions such as a structuring element as typically used in morphological analysis of two different shapes or sizes and taking the difference between these two morphological filtered images. This embodiment produces one number scalar describing the information in each pixel. By computing such scalar descriptors using morphological filter structural elements at different orientations and or image scales and stacking them into a vector oriented morphological descriptors and or multi scale morphological descriptors can be obtained. In one embodiment a median filter is used as the morphological filter to obtain oriented median descriptors and multi scale median descriptors. In another embodiment multiple additional types of local descriptors can be computed alongside the median and or oriented median descriptors.

As part of the automated generation of descriptors in one embodiment the first geometric shape is either a circle or a regular polygon and the second geometric shape is an elongated structure with a specified aspect ratio and orientation and the system is configured to generate a vector of numbers the generation comprising varying an orientation angle of the elongated structure and obtaining a number each for each orientation angle and stacking the obtained numbers into a vector of numbers.

In another embodiment the number or the vectors of numbers can be computed on a multitude of images obtained by progressively scaling up and or down the original input image with a fixed scaling factor referred to as multi scale analysis and stacking the obtained vector of numbers into a single larger vector of numbers referred to as multi scale descriptors.

These local region descriptors can be tailored to suit specific image processing and analysis applications such as for example 

This section describes embodiments directed to image to image registration. Image to image registration includes automated alignment of various structures of an image with another image of the same object possibly taken at a different time or different angle different zoom or a different field of imaging where different regions are imaged with a small overlap. When applied to retinal images registration can include identification of different structures in the retinal images that can be used as landmarks. It is desirable that these structures are consistently identified in the longitudinal images for the registration to be reliable. The input retinal images Source image I Destination image I can be split into two parts 

Landmarks are detected at the constant regions and are matched using different features. These matches are then used to evaluate the registration model. shows an overview of the operations involved in registering two images in one embodiment. The keypoint descriptor computation block computes the descriptors used for matching image locations from different images. One embodiment of the keypoint descriptor computation block is presented in . The blocks shown in here can be implemented on the cloud a computer or computing device a mobile device or the like as shown in . The matching block matches image locations from different images. The RANdom Sample And Consensus RANSAC based model fitting block estimates image transformations based on the matches computed by the matching block . The warping block warps the image based on the estimated image transformation model evaluated by RANSAC based model fitting block . Source image is the image to be transformed. Destination image is the reference image to whose coordinates the source image is to be warped using the warping block . Source image registered to destination image is the source image warped into the destination image coordinates using the warping block .

Branching of vessels can be used as reliable landmark points or keypoints for registration. By examining for blobs across multiple scales at locations with high vesselness locations that are promising keypoints for registration can be extracted. In one embodiment vesselness map is hysteresis thresholded with the high and low thresholds set at 90 and 85 percentiles respectively for the given image. These thresholds may be chosen based on percentage of pixels that are found to be vessel pixels on an average. The resulting binary map after removing objects with areas smaller than a predefined threshold chosen for example based on the smallest section of vessels that are to be preserved V is used as a mask for potential keypoint locations. For example 1000 pixels are used as the threshold in one embodiment a value chosen based on the smallest section of vessels to be preserved.

In one embodiment the fundus image can be smoothed with Gaussian filters of varying sigma or standard deviation. In one implementation the range of sigmas or standard deviations can be chosen based on vessel widths. For example sigmas of 10 13 20 and 35 pixels can be used to locate vessel branches at different scales. Scale normalized determinant of Hessian can be computed at pixel locations labeled by Vat each of these scales. In one embodiment local peaks in the determinant of Hessian map evaluated with the minimum distance between the peaks for example D 1 0.8 0.3 are chosen as keypoints for matching.

The local image features used as descriptors in some embodiments are listed below. Some descriptors are from a patch of N N points centered at the keypoint location. In one embodiment N is 41 and the points are sampled with a spacing of 10. Local image features used as descriptors for matching in one embodiment can include one or more of the following 

In one embodiment the keypoints in the source and destination images are matched using the above defined descriptors. shows matched keypoints from the source and destination images. In one embodiment Euclidean distance is used to measure similarity of keypoints. In one embodiment brute force matching is used get the best or nearly best matches. In one embodiment matches that are significantly better than the second best or nearly best match are preserved. The ratio of the distance between the best possible match and the second best or nearly best possible match is set to greater than 0.9 for these preserved matches. In one embodiment the matches are then sorted based on the computed distance. The top M matches can be used for model parameter search using for example the RANSAC algorithm. In one embodiment M can be 120 matches.

Some embodiments pertain to the estimation of the model for image to image registration. The RANSAC method can be used to estimate a model in the presence of outliers. This method is helpful even in situations where many data points are outliers which might be the case for some keypoint matching methods used for registration. Some embodiments disclose a framework for model estimation for medical imaging. However the disclosed embodiments are not limited thereto and can be used in other imaging applications.

The RANSAC method can include the following actions performed iteratively hypothesize and test framework .

These two actions can be performed iteratively until the probability of finding a better CS drops below a threshold. The model that gives the largest cardinality for the CS can be taken to be the solution. The model can be re estimated using the points of the CS. The RANSAC method used can perform one or more of the following optimizations to help improve the accuracy of estimation and efficiency of computation in terms of number of the iterations.

The random selection of points for building the MSS could result in degenerate cases from which the model cannot be reliably estimated. For example homography computation might use four Cartesian points k 4 but if three of the four points are collinear then the model may not be reliably estimated. These degenerate samples can be discarded. Checks performed during image registration to validate the MSS can prevent or minimize the occurrence of three or more of collinear chosen points as well as allowing the three points to be at a certain distance from each other to obtain good spatial distribution over the image.

Other processes for obtaining retinal image registration can be used. Customizations usable with the RANSAC method in order to compute the models are also provided.

A point on an image can be denoted as a 2D vector of pixel coordinates x y R. It can also be represented using homogeneous coordinates as a 3D vector wx wy w in projective space where all vectors that differ only by a scale are considered equivalent. Hence the projective space can be represented as P R 0 0 0 . The augmented vector x y 1 can be derived by dividing the vector components of the homogeneous vector by the last element w. The registration models can be discussed using this coordinate notation with x y 1 the point in the original image and x y 1 the point in the registered image.

The rotation scaling translation RST model can handle scaling by a factor s rotation by an angle and translation by tt . In one embodiment the transformation process can be expressed as 

This model denoted by T can be referred to as a similarity transformation since it can preserve the shape or form of the object in the image. The parameter vector sco ssin tt can have 4 degrees of freedom one for rotation one for scaling and two for translation. The parameters can be estimated in a least squares sense after reordering Equation 3 as 

The above matrix equation has the standard least squares form of A b with being the parameter vector to be estimated. Each keypoint correspondence contributes two equations and since total number of parameters is four at least two such point correspondences can be used to estimate . In this example the cardinality of MSS is k 2. The equations for the two point correspondences are stacked over each other in the above form A b with A being a matrix of size 4 4 and b being vector of size 4 1. In this example at each hypothesize operation of RANSAC two point correspondences are randomly chosen and the parameters are estimated. The error between the ith pair of point correspondences xand x for the computed model Tcan be defined as 

The first term in the above equation can be called the reprojection error and eas a whole can be referred to as the symmetric reprojection error SRE . In one embodiment point correspondences whose SRE are below a certain threshold can be retained as inliers in the test operation of RANSAC. The average SRE over the points in the CS can be used as the residue to compare two CS of the same size.

In one embodiment the parameter vector for affine model can be of size 6 and can be implemented with three point correspondences k 3 . In this example the above equation can be re written into the standard least squares form A b with A being a matrix of size 6 6 and b being vector of size 6 1 for the three point correspondences. As before can then be estimated using least squares. The selection of points for MSS can be done to avoid the degenerate cases by checking for collinearity of points. The SRE can then be computed with T being the affine model and used to validate inliers for CS and compute the residue for comparison of two CS of the same size.

The homography model can handle changes in view point perspective in addition to rotation scaling translation and shear and represented as 

In this example even though the homography matrix H is a 3 3 matrix it has only 8 degrees of freedom due to the w scaling factor in the left hand side of the above equation. In order to fix the 9th parameter an additional constraint of 1 can be imposed where . . . . Estimation of this parameter vector can be performed with four point correspondences and done using the normalized direct linear transform DLT method algorithm which can produce numerically stable results. For the MSS selection one or more of the following actions can be taken to avoid degenerate cases 

The quadratic model can be used to handle higher order transformations such as x dependent y shear and y dependent x shear. Since the retina is sometimes modeled as being almost spherical a quadratic model is more suited for retinal image registration. In one embodiment the model can be represented as 

As with homography MSS selection may be done to avoid degenerate cases. Since the transform may not be invertible the reprojection error that is the first term on the right hand side of Equation 4 is computed and used to form and validate the CS.

The models discussed above present a set of models that can be used in one or more embodiments of the image registration module. This does not preclude the use of other models or other parameter values in the same methods and systems disclosed herein.

In one embodiment an initial estimate of homography is computed as described in the section above entitled Model Estimation Using RANSAC . Using the initial homography estimate the keypoint locations in the source image Iare transformed to the destination image Icoordinates. In one embodiment the keypoint matching operation can be repeated with an additional constraint that the Euclidean distance between the matched keypoints in the destination image coordinates be lesser than the maximum allowable registration error R. In one embodiment Rcan be fixed at 50 pixels. This process constrains the picked matches and results and can improve registration between the source and destination images.

Using the refined matches various registration models can be fitted including Rotation Scale Translation RST Homography and Quadratic. In one embodiment for each model the minimum number of matches may be subtracted from the size of the obtained consensus set. In one embodiment the model with the maximum resulting quantity can be chosen as the best model. If two models end up with identical values then the simpler model of the two can be chosen as the best model.

An aspect of the image registration module may involve warping of the image to the coordinate system of the base image. shows examples of source and destination images that are registered warped and overlaid on each other. In one embodiment the computed registration models can be used to transform the pixel locations from the original image to the registered image. When transformation is applied directly the integer pixel locations in the input image can map to non integer pixel locations in the registered image resulting in holes in the registered image for example when the registered image dimensions are larger than that of the input image. The holes can be filled by interpolating the transformed pixels in the registered image. Alternatively inverse transform can be used to map registered pixel locations to the input image. For pixels that land at integer locations after inverse mapping the intensity values can be copied from the input image while the intensity values at non integer pixels in input image can be obtained by interpolation.

The above approach can be applied to the invertible registration models such as RST affine or homography. If the non invertible quadratic model is used a forward transform Tcan be used to build a mapping of the integer pixel locations in the input image to the registered image. To find the pixel intensity at an integer location in the registered image the forward mapping can be checked for any input location maps to the registered location under consideration. If such a mapping exists the intensity value is copied. In the absence of such a value the n connected pixel locations in an m m neighborhood around the registered pixel can be checked. In one embodiment n is 8 and m is 3. In one embodiment the closest n pixels in the input image are found and the pixel intensity at their centroid location is interpolated to obtain the intensity value at the required pixel location. This analysis may be helpful when pixels in a neighborhood in the input image stay in almost the same relative positions even in the registered image for retinal image registration. In another embodiment the estimated quadratic model can be used to compute the forward mapping swapping the input and registered pixel locations and estimating the inverse mapping circumflex over T using least squares can be used to compute the forward mapping. A mapping can be applied to the integer locations in the registered image to generate the corresponding mapping from the input image.

In some embodiments automated image assessment can be implemented using one or more features of the automated low level image processing and or image registration techniques described above however using these techniques is not mandatory nor necessary in every embodiment of automated image assessment.

Typically multiple images of the fundus from various fields and both eyes are collected from a patient during a visit. In addition to the color fundus images photographs of the patient s eye s lens may also be added to the patient encounter images as illustrated in . In one embodiment an automated DR screening system automatically and reliably separates these lens shot images from the actual color fundus images.

In one embodiment lens shot image classification is achieved by primarily using structural and color descriptors. A given image is resized to a predetermined size. The histogram of orientations HoG feature is computed on the green channel to capture the structure of the image. The vesselness maps for images are computed using for example the processes disclosed in the section below entitled Vessel Extraction . The vesselness maps are hysteresis thresholded with the lower and higher thresholds set for example to 90 and 95 percentiles respectively to obtain a mask. The color histograms of the pixels within the mask are computed. The final descriptor is obtained by appending the color histogram descriptors to the HoG descriptors.

The order in which the images were obtained is also sometimes an indicator of an image being a lens shot image. This was encoded as a binary vector indicating absolute value of the difference between the image index and half the number of images in an encounter.

On a dataset of 10 104 images with over 2000 lens shot images on 50 50 train test splits area under receiver operating characteristics ROC curve AUROC of over 0.998 were obtained.

In one embodiment the system may include computer aided assessment of the quality or gradability of an image. Assessment of image gradability or image quality can be important to an automated screening system. The factors that reduce quality of an image may include for example poor focus blurred image due to eye or patient movement large saturated and or under exposed regions or high noise. In addition the quality of image can be highly subjective. In the context of retinal image analysis image characteristics that allow for effective screening of retinopathy by a human grader or software are preferred whereas images with hazy media are flagged as being of insufficient quality for effective grading. Quality assessment can allow the clinician to determine whether he needs to immediately reimage the eye or refer the patient to a clinician depending on the screening setup employed.

In one embodiment the images are first processed using a Hessian based interest region and vesselness map detection technique as shown in . The obtained image is then converted to a binary mask by employing hysteresis thresholding followed by morphological dilation operation. The application of this binary mask to the original image greatly reduces the number of pixels to be processed by the subsequent blocks of the quality assessment pipeline without sacrificing the accuracy of assessment.

Next image quality descriptors are extracted using the masked pixels in the image. Table 1 is one embodiment of example descriptors that may be used for retinal image quality assessment.

In one embodiment using 3 channel RGB color retinal fundus images the green channel is preferred over red or blue channels for retinal analysis. This is because the red channel predominantly captures the vasculature in the choroidal regions while the blue channel does not capture much information about any of the retinal layers. This is illustrated for an example color fundus image shown in as grayscale with the red channel shown in as grayscale the green channel shown in as grayscale and the blue channel shown in as grayscale. Hence in one embodiment the green channel of the fundus image is used for processing. In other embodiments all the 3 channels or a subset of them are used for processing.

In one embodiment the system classifies images based on one or more of the descriptors discussed below 

In one embodiment for measuring the degree of focus or blur in the image the sum modified Laplacian is used. This has shown to be an extremely effective local measure of the quality of focus in natural images as discussed in S. K. Nayar and Y. Nakagawa Shape from Focus 16 no. 8 1994 824 831. For the input image I the sum modified Laplacian Iat a pixel location x y can be computed as 2 1 1 2 1 1 .

A normalized histogram can be computed over the sum modified Laplacian values in the image to be used as focus measure descriptor. In practice Ivalues that are too low or too high may be unstable for reliably measuring focus in retinal images and can be discarded before the histogram computation. In one embodiment the low and high thresholds are set to 2.5 and 20.5 respectively which was empirically found to give good results. The computed descriptor has a length of 20. In practice computing the focus descriptors on the image obtained after enhancement and additional bilateral filtering provides better gradability assessment results.

In one embodiment the local saturation measure captures the pixels that have been correctly exposed in a neighborhood by ignoring pixels that have been under exposed or over exposed. The correctly exposed pixels are determined by generating a binary mask M using two empirically estimated thresholds Sfor determining under exposed pixels and Sfor determining over exposed pixels. At a pixel location x y the binary mask is determined as 

In one embodiment contrast is the difference in luminance and or color that makes an object or its representation in an image distinguishable. The contrast measure may include Michelson contrast also called visibility as disclosed in Albert A. Michelson Studies in Optics Dover Publications. com 1995 . The local Michelson contrast at a pixel location x y is represented as 

In one embodiment normalized RGB color histograms are computed over the whole image and used as descriptors of color. In one embodiment the computed descriptor has a length of 20 for each of the R G and B channels.

In one embodiment descriptors based on local entropy for example using techniques disclosed in Rafael C. Gonzalez and Woods E. Richard Digital Image Processing 0 201 18075 8 2002 are incorporated to characterize the texture of the input image. For an image of bit depth B the normalized histogram pat pixel location x y is first computed considering the pixels that lie in a neighborhood D around location x y . In one embodiment D is a circular patch of radius r pixels. Denoting the local normalized histogram as p x y i 0 1 . . . 2 1 the local entropy is obtained as 

A normalized histogram of the local entropy image Iis then used as a local image texture descriptor. In one embodiment the computed descriptor would have a length of 20.

In addition to entropy in another embodiment local binary patterns LBP based descriptors are also computed to capture the texture in the image. The LBP can be computed locally for every pixel and in one embodiment the normalized histogram of the LBP image can be used as a descriptor of texture. The computed descriptor would still have a length of 20.

In one embodiment since noise also affects the quality of an image a noise metric descriptor for retinal images is also incorporated using for example techniques disclosed in Noriaki Hashimoto et al. Referenceless Image Quality Evaluation for Whole Slide Imaging 3 2012 9. For noise evaluation an unsharp masking technique may be used. The Gaussian filtered blurred retinal image G is subtracted from the original retinal image I to produce a difference image D with large intensity values for edge or noise pixels. In one embodiment to highlight the noise pixels the center pixel in a 3 3 neighborhood B is replaced with the minimum difference between it and the 8 surrounding pixels as 

In one embodiment the system includes a classification action for image quality assessment. In another embodiment regression analysis is conducted to obtain a number or value representing image quality. One or more quality descriptors discussed above are extracted and concatenated to get a single N dimensional descriptor vector for the image. It is then subjected to dimensionality reduction new dimension M using principal component analysis PCA to consolidate the redundancy among the feature vector components thereby making quality assessment more robust. The PCA may include techniques disclosed in Herv Abdi and Lynne J. Williams Principal Component Analysis 2 no. 4 2010 433 459. In one embodiment the PCA reduced descriptor then train a support vector regression SVR engine to generate a continuous score to be used for grading the images for example as being of poor fair or adequate quality. The SVR may include techniques disclosed in Harris Drucker et al. Support Vector Regression Machines 1997 155 161. In one embodiment the parameters of the SVR were estimated using a 5 fold cross validation on a dataset of 125 images 73 adequate 31 fair and 21 poor labeled for retinopathy gradability by experts. shows example images of varying quality that have been scored by the system. In another embodiment a support vector classifier SVC is trained to classify poor quality images from fair or adequate quality images. On the 125 image dataset the adequate and fair quality images were classified from the poor quality images with accuracy of 87.5 with an area under receiver operating characteristics AUROC of 0.90. Further improvements are expected with the incorporation of texture descriptors. In one embodiment the descriptor vector has a length of N 140 which gets reduced to

In one embodiment the system is configured to identify retinal vasculature for example the major arteries and veins in the retina in retinal images by extracting locations of vasculature in images. Vasculature often remains fairly constant between patient visits and can therefore be used to identify reliable landmark points for image registration. Additionally vessels in good focus are indicative of good quality images and hence these extracted locations may be useful during image quality assessment.

One embodiment for vesselness computation is provided in . refers to the standard deviation of the Gaussian used for smoothing. Gaussian smoothing convolves the image with a Gaussian filter of standard deviation . This operation is repeated at different values of . Hessian computation computes the Hessian matrix for example using Equation 2 at each pixel. Structureness block computes the Frobenius norm of the Hessian matrix at each pixel. Eigen values of the Hessian matrix are computed at each pixel. Vesselness in Equation 5 is computed at a given pixel after smoothing the image with Gaussian smoothing block of standard deviation . The maximum at each pixel over multiple values of vesselness is computed at different smoothing. Vesselness indicates the vesselness of the input image .

In one embodiment the vessels in the green channel of the color fundus image can be enhanced after pre processing using a modified form of Frangi s vesselness using for example techniques disclosed in Alejandro F. Frangi et al. Multiscale Vessel Enhancement Filtering in 98 Springer 1998 130 137 Frangi et al. 1988 . The input image is convolved with Gaussian kernels at a range of scales. Gradients L x y L x y L x y and L x y are then computed on these images and Hessian His computed at multiple scales using for example Equation 2.

In one embodiment is fixed at 0.5 as per Frangi et al. 1998 and c is fixed as the 95 percentile of the structureness S. The vesselness measure across multiple scales is integrated by evaluating the maximum across all the scales. Vesselness over multiple standardized datasets were evaluated using for example DRIVE as disclosed in Joes Staal et al. Ridge Based Vessel Segmentation in Color Images of the Retina 23 no. 4 April 2004 501 509 and STARE as disclosed in A. Hoover V. Kouznetsova and M. Goldbaum Locating Blood Vessels in Retinal Images by Piecewise Threshold Probing of a Matched Filter Response 19 no. 3 2000 203 210. The combination of the custom image enhancement and modified Frangi vesselness computation can result in performance that is close to the state of the art. In one embodiment the unsupervised non optimized implementation takes less than 10 s on a 605 700 pixel image. Some example vessel segmentations are shown in . The receiver operating characteristics ROC curve of one embodiment on the STARE dataset is shown in . Table 2 compares the AUROC and accuracy of one embodiment of the system on the DRIVE and STARE datasets with human segmentation. This embodiment has better accuracy with respect to gold standard when compared to secondary human segmentation.

In one embodiment the vesselness map is then processed by a filterbank of oriented median filters. In one embodiment the dimensions of the median filters are fixed based on the characteristics of the vessels to be preserved for example Height 3 pixels Length 30 pixels or 8 orientations. At each pixel the difference between the maximum and median filter response across orientations was evaluated. This provides a vasculature estimate that is robust to identify the presence of blob lesions or occlusions. shows an example vessel extraction using the custom morphological filterbank analysis on a poor quality image.

In one embodiment level set methods such as fast marching are employed for segmenting the vessels and for tracing them. For example fast marching can be used with techniques disclosed in James A. Sethian A Fast Marching Level Set Method for Monotonically Advancing Fronts 93 no. 4 1996 1591 1595. The vessel tracing block may focus on utilizing customized velocity functions based on median filterbank analysis for the level sets framework. At each pixel location the velocity function is defined by the maximum median filter response. This embodiment leads to an efficient mathematically sound vessel tracing approach. In one embodiment automatic initialization of start and end points for tracing the vessels in the image is performed using automated optic nerve head ONH identification within a framework that provides a lesion localization system.

In one embodiment the system is configured to localize lesions in retinal images. The lesions may represent abnormalities that are manifestations of diseases including diabetic retinopathy macular degeneration hypertensive retinopathy and so forth. depicts one embodiment of a lesion localization process. The illustrated blocks may be implemented on the cloud a computer or computing system a mobile device or the like as shown in . The image refers in general to the retinal data single or multidimensional that has been captured using a retinal imaging device such as cameras for color image capture fluorescein angiography FA adaptive optics optical coherence tomography OCT hyperspectral imaging scanning laser ophthalmoscope SLO wide field imaging or ultra wide field imaging. Fundus mask generation block estimates the mask to extract relevant image sections for further analysis. Image gradability computation module computes a score that automatically quantifies the gradability or quality of the image in terms of analysis and interpretation by a human or a computer. Image enhancement module enhances the image to normalize the effects of lighting different cameras retinal pigmentation or the like. Interest region identification block generates an indicator image with a true or false value for each pixel in the original image that indicates or determines whether the pixel is interesting or represents active regions that may be considered for further processing. Descriptor set computation block computes a single or multi dimensional float or integer valued vector that provides a description of an image region. Examples include shape texture spectral or other descriptors. Lesion classification block classifies each pixel marked by interest region identification block using descriptors computed using descriptor set computation block into different lesions. Joint segment recognition block analyzes the information and provides an indicator of any recognized lesions.

In some embodiments interest region detection techniques described in the section above entitled Interest Region Detection can be used to locate lesions.

In one embodiment a set of descriptors that provide complementary evidence about presence or absence of a lesion at a particular location can be used. Embodiments of the disclosed framework developed can effectively describe lesions whose sizes vary significantly for example hemorrhages and exudates due to local description of interest regions at multiple scales.

Table 3 lists one embodiment of pixel level descriptors used for lesion localization and how the descriptors may contribute to lesion classification.

Many of the descriptor sets are developed specifically for retinal images with a focus on low level image processing. Measures of local image properties alongside with some retinal fundus image specific measures at multiple scales can be used. Each of the descriptors listed below can be computed on scaled images I I. . . I. In one embodiment the ratio between different scales is set to 0.8 and 10 scales are used. Examples of multi scale descriptors that can be used for lesion localization and or screening at each interest pixel x y are listed above in Table 3. The following provides information about one or more descriptors that may be used.

At each scale sa morphological filter can be applied to the image with the morphological filter computed over circles squares or regular polygons or different sizes. For example circles of different radii can be used. In one embodiment the median filtering is used as the said morphological filter. In this embodiment at each scale sthe median normalized RGB images Aare computed for example using Equation 1 with medians computed within circles of different values of radius r such that r r. In one embodiment median filterbank descriptor is A x y for all values of j. In one embodiment r 7 15 31 j 1 2 3 and r 3.

In one embodiment the morphological filterbank descriptors are computed employing the following generating a first morphological filtered image using the retinal image with a the said morphological filter computed over a first geometric shape generating a second morphological filtered image using the retinal image with a morphological filter computed over a second geometric shape the second geometric shape having one or more of a different shape or different size from the first geometric shape generating a difference image by computing a difference between the first morphological filtered image and the second median filtered image and assigning the difference image pixel values as a descriptor value each corresponding to given pixel location of the said retinal image. In one embodiment the morphological filter employed is a median filter. In one embodiment these descriptors are evaluated on a set of images obtained by progressively resizing the original image up and or down by a set of scale factors so as to obtain a number or a vector of numbers for each scale multi scale analysis which are then concatenated to make a composite vector of numbers multi scale descriptors .

At each scale sthe oriented morphological filtered images are computed using structuring elements geometric shapes that resemble elongated structures such as rectangles ellipse or the like. These filters are applied at different orientations representing angular steps of . Two different parameters of the structuring element for example length and width in case of a rectangular structuring element are used to compute two morphological filtered images at each orientation. Taking the difference of these two images gives us the quantity of interest at each pixel which then forms part of the said oriented morphological filterbank descriptors. In one embodiment median filters are used as the said morphological filter to obtain. In this embodiment at each scale sthe oriented median normalized images Iare computed for example using Equation 1 with medians computed within rectangular area R of length l and width w at angular steps of . In one embodiment length l 30 and width w 2 and angular steps of 15 degrees are used. At each scale sthe median normalized images Iare computed for example using Equation 1 with medians computed within circle C of radius r. In one embodiment a radius of r 3 is used.

Oriented median filterbank descriptor is I x y at the different orientations. These descriptors can distinguish elongated structures from blob like structures. The maximum or minimum value of the filterbank vector is identified and the vector elements are rearranged by shifting each element by P positions until the said maximum or minimum value is in the first position while the elements going out of the vector boundary are pulled back into the first position sometimes referred to as circular shifting.

In one embodiment the oriented morphological filterbank descriptors are computed employing the following 

a. Computing morphological filtered image with the morphological filter computed over a circle or regular polygon structuring element of the median filter 

b. Computing another morphological filtered image with the morphological filter computed over a geometric shape elongated structure such as a rectangle of specified aspect ratio width height and orientation angle or an ellipse of specified foci and orientation angle of its principal axis

c. Computing the difference image between the morphological filtered images computed in a and in b and assign the difference image value at a given pixel as its descriptor.

d. Computing a vector of numbers oriented median descriptors by a varying the orientation angle of the elongated structure and obtaining one number each for each orientation angle and b stacking thus computed numbers into a vector of numbers.

In one embodiment the maximum or minimum value of the oriented morphological filterbank descriptor vector is identified and the vector elements are rearranged by shifting each element by P positions until the said maximum or minimum value is in the first position while the elements going out of the vector boundary are pulled back into the first position circular shifting .

In one embodiment these descriptors are evaluated on a set of images obtained by progressively resizing the original image up and or down by a set of scale factors so as to obtain a number or a vector of numbers for each scale multi scale analysis which are then concatenated to make a composite vector of numbers multi scale descriptors .

Median normalized difference image is computed with radii rand r such that r rat each scale s. This difference image Iis then filtered using Gaussian filters G. The image after filtering with second derivative of the Gaussian is also computed. The Gaussian derivative descriptors are then F x y and F x y . These descriptors are useful in capturing circular and ring shaped lesions for example microaneurysms .

Median normalized difference image with bright vessels is computed with radii rand r such that r rat each scale s. Then Hessian H is computed at each pixel of the difference image I. Determinant of Hessian map Lof the difference image Iis the map of the determinant of Hessian H at each pixel. The sum modified Laplacian is computed to describe the local image focus. Vesselness and structureness may be computed for example as shown in . The Eigen values and of H such that and their ratio are evaluated. The Hessian based descriptor vector is collated from these values at the interest pixel locations x y . These describe local image characteristics of blobs and tubes such as local sharpness.

Using the interest regions mask Zcomputed at scale s the region properties listed in are measured at each blob. The interest pixels within a particular blob region are assigned with the same blob statistics descriptor.

Average color is measured in a square block of length l centered at the pixel of interest. The color in RGB space is used as the color descriptor for the pixel. In one embodiment smoothing square of length l 5 is used.

The natural logarithm of the Fourier transform magnitude and first derivative of Fourier transform phase of a patch of image B centered at the pixel of interest at various frequencies are computed. These descriptors are invariant to rotation and scaling and can survive print and scanning. The natural logarithm of Fourier transform magnitude of the image patch can be computed as follows ln where F and F are the fourier spectral descriptors F is the fourier transform operation at frequency and denotes phase.

Gabor jets are multi resolution Gabor features constructed from responses of multiple Gabor filters at several frequencies and orientations. Gabor jet descriptors are computed as follows 

2D Gaussian filter is used as a kernel for multi resolution match filtering. Gaussian filters of a range of sigmas are used as the filterbank as follows 

Path opening and closing based morphological descriptors use flexible line segments as structuring elements during morphological operations. Since these structuring elements are adaptable to local image structures these descriptors may be suitable to describe structures such as vessels.

Local binary patterns LBP capture texture information in images. In one embodiment a histogram with 20 bins to describe the LBP images is used.

In one embodiment a support vector machine SVM is used for lesion classification. In other embodiments classifiers such as k nearest neighbor naive Bayes Fisher linear discriminant deep learning or neural networks can be used. In another embodiment multiple classifiers can be used together to create an ensemble of classifiers. In one embodiment four classifiers one classifier for each of cottonwoolspots exudates hemorrhages and microaneurysms are trained and tested. In one embodiment ground truth data with lesion annotations on 100 images is used for all lesions plus more than 200 images for microaneurysms. The annotated dataset is split in half into training and testing datasets and interest region detector is applied on the training dataset images. The detected pixels are sampled such that the ratio of the number of pixels of a particular category of lesion in the training dataset to those labeled otherwise remains a constant referred to as the balance factor B. In one embodiment B 5 for cottonwoolspots exudates and hemorrhages classifiers and B 10 for microaneurysms.

In one embodiment interest region detector is applied on the testing dataset images. The detected pixels are classified using the 4 different lesion classifiers noted above. Each pixel then has 4 decision statistics associated with it. A decision statistic for a particular pixel is generated by computing the distance of the given element from the given lesion classification hyper plane defined by the support vectors in the embodiment using SVM for lesion classification or in the embodiment using Fisher linear discriminant or the like. In case of the embodiment using a naive Bayes classifier or the embodiment using the k nearest neighbor the class probability for lesion class and non lesion class are computed and are used as the decision statistic.

In one embodiment a biologically inspired framework is employed for joint segmentation and recognition in order to localize lesions. Segmentation of interest region detector outputs the candidate lesion or non lesion blobs. The decision statistic output from pixel level classifiers can provide evidence to enable recognition of these lesions. These decision statistics from different pixels and different lesion types are pooled within each blob to arrive at a blob level recognition. The pooling process may include computing the maximum minimum or the average of decision statistics for a given lesion type for all the pixels in a given blob. This process can be repeated iteratively although in some embodiments a single iteration can be sufficient. shows an example embodiment of microaneurysm localization. shows an example embodiment of hemorrhages localization. shows an example of exudates localization. illustrates one embodiment of a graph that quantifies the performance of lesion detection.

In another embodiment the pixel level decision statistics over each blob and building secondary descriptors can be combined. Secondary descriptors can be one or more of the following 

These aggregated descriptors can then be used to train blob level lesion classifiers and can be used to recognize and or segment lesions. These descriptors can also be used for screening.

Some embodiments pertain to computation of lesion dynamics which quantifies changes in the lesions over time.

In one embodiment longitudinal retinal fundus images are registered to the baseline image as described in the section above entitled Image Registration . On each of the images including the baseline image lesions are localized as described in the section above entitled Lesion Localization . In some embodiments characterizing dynamics of lesions such as exudates EX and microaneurysms MA may be of interest. In one embodiment the appearance and disappearance of MA also referred to as MA turnover is considered. The first image in the longitudinal series is referred to as the baseline image Iand any other registered longitudinal image is denoted as h.

In one embodiment binary images Band Bwith lesions of interest marked out are created for the baseline and longitudinal images. Lesion locations are labeled in Band compared to the corresponding regions in Bwith a tolerance that can for example be specified by maximum pixel displacement due to registration errors. The labeled lesion is marked as persistent if the corresponding region contains a MA else it is marked as a disappearing MA. Labeling individual lesions in Band comparing them to corresponding regions in Bgives a list of newly appeared lesions. B C and D depict embodiments and examples of longitudinal images for comparison to identify persistent appeared and disappeared lesions. The images are zoomed to view the lesions. shows the baseline image. shows the registered longitudinal image. shows labeled MAs in the baseline image with persistent MAs indicated by ellipses and non persistent MAs by triangles. shows labeled MAs in the longitudinal image with persistent MAs indicated by ellipses. Counting the newly appeared lesions and disappeared lesions over the period of time between the imaging sessions allows computation of lesion turnover rates or MA turnover if the lesion under consideration is MA.

In another embodiment the baseline image Iand registered longitudinal image Iare used rather than the registered binary lesion maps. Potential lesion locations are identified using the interest region detector as for example described in the section above entitled Interest Point Detection . In one embodiment these pixels are then classified using lesion classifier for example as described in the lesion localization section using for example descriptors listed in Table 3. The regions with high certainty of including lesions in I as expressed by the decision statistics computed over the pixels are labeled. In one embodiment these regions are then matched with corresponding regions in Iwith a tolerance for example as specified by maximum pixel displacement which may be due to registration errors using decision statistics. In one embodiment regions with matches to the labeled lesions with high confidence are then considered to be persistent lesions and labeled regions with no matches are considered to be disappeared lesions. Newly appearing lesions can be found by labeling image Iand comparing those regions to corresponding regions in Ito identify newly appearing lesions.

Some factors can confound lesion turnover computation such as MA turnover computation variation in input images errors in image alignment or errors in MA detection and localization. Some errors can cascade and cause the MA turnover computed to be drastically different from the actual value which could be a failure for the tool. In some embodiments a system that gracefully degrades when faced with the above confounding factors is desirable. At each stage rather than making a binary decision the probability that a blob is classified as an MA or the probability that two blobs are marked as matched MAs and hence persistent is estimated. As noted above a blob includes a group of pixels with common local image properties and chosen by the interest region detector. shows a patch of retina with microaneurysms. shows the ground truth labelling for microaneurysms in the image patch shown in . shows the detected MAs marked by disks with the corresponding confidence levels indicated by the brightness of the disk. An estimated range for MA turnover is computed rather than a single number. A larger range may represent some uncertainty in the turnover computation nevertheless it can provide the clinician with useful diagnostic information. In one embodiment one or more of the following is performed when confounding factors are present.

As shown in embodiments of the range for turnover numbers is then assessed from the blob level probabilities and persistent MA pair probabilities using thresholds identified from the ground truth.

Medical and retinal images captured during a given visit of a given patient are typically captured using the same imaging set up. The set of these images is termed an encounter of that patient on that date . The analysis of the images in a given encounter can be performed jointly using data from all the images. For example the presence or absence of lesions in one eye of a given patient can be determined after examining all the images captured of that eye.

In one embodiment a method for detection of regions with abnormality in medical particularly retinal images using one or at least two or more images obtained from the same patient in the same visit encounter can include one or more of the following 

a. Identifying a subset of images for further analysis based on image quality image content such as the image being a lens shot or a non retinal image or of poor quality or fidelity 

b. For each image identified in a designating some pixels in the image as active pixels meaning they contain the interesting regions of the image using of one or more techniques from i conditional number theory ii multi scale interest region detection iii vasculature analysis and iv structured ness analysis 

c. For each image identified in a computing a vector of numbers primary descriptors at each of the pixels identified in b using one or at least two or more types from i median filterbank descriptors ii oriented median filterbank descriptors iii Hessian based descriptors iv Gaussian derivatives descriptors vi blob statistics descriptors vii color descriptors viii matched filter descriptors ix path opening and closing based morphological descriptors x local binary pattern descriptors xi local shape descriptors xii local texture descriptors xiii local Fourier spectral descriptors xiv localized Gabor jets descriptors xv edge flow descriptors xvi edge descriptors such as difference of Gaussians xvii focus measure descriptors such as sum modified Laplacian xix saturation measure descriptors xx contrast descriptors or xxi noise metric descriptors 

d. For each image for each pixels identified in b computing pixel level classifier decision statistic a number quantifying the distance from the classification boundary using supervised learning utilizing the primary descriptors computed in c using one or more of i support vector machine ii support vector regression iii k nearest neighbor iv naive Bayes v Fisher linear discriminant vi neural network vii deep learning viii convolution networks or ix an ensemble of one or more classifiers including from i viii with or without bootstrap aggregation 

e. For each image identified in a computing a vector of numbers image level descriptors by using one or least two or more types from 

f. Combining the image level descriptors computed in e with or without further processing for the subset of images identified in a to obtain encounter level descriptors 

g. Classifying encounters using encounter level descriptors computed in f as normal or abnormal one classifier each for each abnormality lesion or disease using one or more of supervised learning techniques including but not limited to i support vector machine ii support vector regression iii k nearest neighbor iv naive Bayes v Fisher linear discriminant vi neural network vii deep learning viii convolution networks or ix an ensemble of one or more classifiers including from i viii with or without bootstrap aggregation.

In another embodiment the combining image level descriptors into encounter level descriptors for the images of the patient visit encounter identified in a is achieved using operations that include but are not limited to averaging maximum minimum or the like across each index of the descriptor vector so that the said encounter level descriptors are of the same length as the image level descriptors.

In another embodiment the combining image level descriptors for the images of the patient visit encounter identified in a to obtain encounter level descriptors is achieved using a method including i combining image level descriptors to form either the image field of view identified from meta data or by using position of optic nerve head and macula specific or eye identified from meta data or by using position of optic nerve head and macula specific descriptors or ii concatenating the field specific or eye specific descriptors into the encounter level descriptors.

Images in an encounter can be identified to be lens shot images using for example the method described in the section above entitled Lens Shot Image Classification. These lens shot images can be ignored and excluded from further processing and analysis since they may not provide significant retinal information. The images that are not retinal fundus images are ignored in this part of the processing.

Images in an encounter can be identified as having poor quality using for example the method described in the section above entitled Image Quality Assessment. These poor quality images can be excluded from further processing and analysis since the results obtained from such images with poor quality are not reliable. If a given encounter does not have the required number of adequate good quality images then the patient is flagged to be re imaged.

Encounter level descriptors can be obtained by combining image level primary descriptors many of which are described in the sections above entitled Processing That Can Be Used To Locate The Lesions. and Features that can be used for this type of automatic detection . In one embodiment the image level descriptors include one or more types from 

In one embodiment the encounter level descriptors can be evaluated as the maximum value across all the image level descriptors for the images that belong to an encounter or created by concatenating eye level descriptors. In one embodiment the computation of encounter level descriptors for the images of the patient visit encounter is achieved using a method comprising i combining image level descriptors to form either the image field of view specific descriptors identified from metadata or by using position of ONH as described in the section above entitled Optic Nerve Head Detection or by using the position of the ONH and macula or eye specific descriptors identified from metadata or position of ONH and macula or the vector from the focus to the vertex of the parabola that approximates the major vascular arch using operations such as maximum average minimum or the like and ii concatenating the field specific or eye specific descriptors into the encounter level descriptors. These encounter level descriptors can then be classified for example using classifiers described in the section below entitled Diabetic Retinopathy Screening to obtain the encounter level decisions. Combination of image level descriptors to form encounter level descriptors is discussed in further detail in section Multi Level Descriptors For Screening .

Encounter level decisions can also be made by combining image level decision statistics histograms using average maximum and minimum operations or the like.

Methods systems and techniques described can also be used to automate screening for various medical conditions or diseases which can help reduce the backlog of medical images that need to be screened. One or more of the techniques described earlier or in the following sections may be used to implement automated screening however using these techniques is not required by for every embodiment of automated screening.

The embodiments described above are adaptable to different embodiments for screening of different retinal diseases. Additional embodiments are described in the sections below related to image screening for screening for diabetic retinopathy and image screening for screening for cytomegalovirus retinitis.

Ground truth labels for retinopathy and maculopathy can indicate various levels of severity for example R0 R1 M0 and so on. This information can be used to build different classifiers for separating the various DR levels. In one embodiment improved performance can be obtained for classification of R0M0 no retinopathy no maculopathy cases from other disease cases on Messidor dataset by simply averaging the decision statistics of the no retinopathy and no maculopathy R0M0 versus the rest classifier and no or mild retinopathy and no maculopathy R0R1M0 versus the rest classifier. A publically available dataset is kindly provided by the Messidor program partners at http messidor.crihan.fr . One or more of the following operations may be applied with the weights won each training element initialized to the same value on each of the classifier hobtained. In some embodiments the operations are performed sequentially.

1. With the training dataset weighted the best remaining classifier his applied to evaluate AUROC A. The output weight for this classifier is computed as below 

The output weights are used to weight the output of each of the classifiers to obtain a final classification decision statistic.

In one embodiment ensemble classifiers are employed which are a set of classifiers whose individual predictions are combined in a way that provides more accurate classification than the individual classifiers that make them up. In one embodiment a technique called stacking is used where an ensemble of classifiers at base level are generated by applying different learning algorithms to a single dataset and then stacked by learning a combining method. Their good performance is proved by the two top performers at the Netflix competition using for example techniques disclosed in Joseph Sill et al. arXiv e print Nov. 3 2009. The individual weak classifiers at the base level may be learned by using algorithms such as decision tree learning na ve Bayes SVM or multi response linear regression. Then at the meta level effective multiple response model trees are used for stacking these classifier responses.

In another embodiment the system employs biologically plausible deep artificial neural network architectures which have matched human performance on challenging problems such as recognition of handwritten digits including for example techniques disclosed in Dan Cirean Ueli Meier and Juergen Schmidhuber arXiv e print Feb. 13 2012. In other embodiments traffic signs or speech recognition are employed using for example techniques disclosed in M. D. Zeiler et al. On Rectified Linear Units for Speech Processing 2013. Unlike shallow architectures for example SVM deep learning is not affected by the curse of dimensionality and can effectively handle large descriptors. In one embodiment the system uses convolution networks sometimes referred to as cony nets based classifiers which are deep architectures that have been shown to generalize well for visual inputs.

In one embodiment the system allows screening of patients to identify signs of diabetic retinopathy DR . A similar system can be applied for screening of other retinal diseases such as macular degeneration hypertensive retinopathy retinopathy or prematurity glaucoma as well as many others.

When detecting DR two DR detection scenarios are often of interest i detecting any signs of DR even for example a single microaneurysm MA since the lesions are often the first signs of retinopathy or ii detecting DR onset as defined by the Diabetes Control and Complications Trial Control and Group that is the presence of at least three MAs or the presence of any other DR lesions. The publicly available Messidor dataset which contains 1200 retinal images that have been manually graded for DR and clinically significant macular edema CSME can be used for testing the system. In one embodiment the screening system when testing for this Messidor dataset uses 5 MAs or 0 Hemorrhages HMs as criteria for detecting DR onset. For both of the detection scenarios the goal is to quantify working on cross dataset testing training on a completely different data or on a 50 50 test train split of the dataset.

In one embodiment two approaches can be used in the system one for the 50 50 train test split and the other for the cross dataset testing with training on one dataset and testing on another. One embodiment uses the Messidor dataset and the DEI dataset kindly provided by Doheny Eye Institute which comprises 100 field 2 images with four lesions diligently annotated pixel wise MA HM EX and CW and 125 field 2 images with MAs marked. When using the system on these datasets the annotations performed precisely often verifying the annotations using the corresponding fluorescein angiography FA images. This precise annotation sets high standards for the automated lesion localization algorithms especially at lesion level.

In this embodiment a dictionary of low level features is computed by unsupervised learning of interesting datasets referred to as codewords. The dictionary may be computed by technology disclosed in J. Sivic and A. Zisserman Video Google A Text Retrieval Approach to Object Matching in Videos in 92003 1470 1477. Then an image is represented using a bag of words description for example a histogram of codewords found in the image. This may be performed by finding the codeword that is closest to the descriptor under consideration. The descriptors for an image are processed in this manner and contribute to the histogram.

A 50 50 split implies that training is done with half the dataset and testing is done on the other half. The computation of the dictionary can be an offline process that happens once before the system or method is deployed. In one embodiment the unsupervised learning dataset is augmented with descriptors from lesions. In an example implementation the descriptors from lesions locations annotated on the DEI dataset are used. For this example implementation the total number of descriptors computed is Nand N for DEI and Messidor datasets respectively. Then N mN where m 1.0 can be any real number with each Messidor training image contributing equally to the Ndescriptor count. In one embodiment m is set to 1 and in another embodiment it is set to 5. The random sampling of interesting locations allows signatures from non lesion areas to be captured. The computed N Ndescriptors are pooled together and clustered into K partitions using K means clustering the centroids of which give K codewords representing the dictionary. The K means clustering may be performed using techniques disclosed in James MacQueen Some Methods for Classification and Analysis of Multivariate Observations in vol. 1 1967 14.

After the dictionary computation the bag of words based BOW secondary descriptors are computed. In one embodiment for each image the lesion descriptors are computed. Using vector quantization each descriptor is assigned a corresponding codeword from the previously computed dictionary. The vector quantization may be performed using techniques disclosed in Allen Gersho and Robert M. Gray Kluwer Academic Publishers 1992 . This assignment can be based on which centroid or codeword is closest in terms of Euclidean distance to the descriptor. A normalized K bin histogram is then computed representing the frequency of codeword occurrences in the image. The histogram computation does not need to retain any information regarding the location of the original descriptor and therefore the process is referred to as bagging of codewords. These descriptors are referred to as bag of words BOW descriptors.

Table 5 is comparison of embodiments of the screening methods. The results for one embodiment is provided for reference alone noting that the other results are not cross dataset. NA in the table indicates the non availability of data. The column labelled Quellec provides results when applying the method described in Gw nol Quellec et al. A Multiple Instance Learning Framework for Diabetic Retinopathy Screening 16 no. 6 August 2012 1228 1240 the column labelled Sanchez shows results when applying the method described in C. I. Sanchez et al. Evaluation of a Computer Aided Diagnosis System for Diabetic Retinopathy Screening on Public Data 52 no. 7 Apr. 28 2011 4866 4871 and the column labelled Barriga shows results when applying the method of E. S. Barriga et al. Automatic System for Diabetic Retinopathy Screening Based on AM FM Partial Least Squares and Support Vector Machines in 2010 2010 1349 1352.

In one embodiment after the BOW descriptors have been computed for the images they are subjected to term frequency inverse document frequency tf idf weighting using for example techniques disclosed in Christopher D. Manning Prabhakar Raghavan and Hinrich Sch tze vol. 1 Cambridge University Press Cambridge 2008 . This is done to scale down the impact of codewords that occur very frequently in a given dataset and that are empirically less informative than codewords that occur in a small fraction of the training dataset which might be the case with lesion codewords. In some embodiments the inverse document frequency idf computation is done using the BOW descriptors of the training dataset images. In addition during computation of document frequency a document may be considered if the raw codeword frequency in it is above a certain threshold T. The tf idf weighting factors computed on training dataset are stored and reused on the BOW descriptors computed on the images in the test split of Messidor dataset during testing.

In one embodiment the system adds a histogram of the decision statistics for example the distance from classifier boundaries for pixel level MA and HM classifiers. This combined representation may be used to train a support vector machine SVM classifier using the 50 50 test train split. In one embodiment the number of descriptors computed is N N 150 000 and these 300K descriptors are clustered to get K 300 codewords. In addition the document frequency computation may use T 0 but for other embodiments may use T 3. These parameter choices of these embodiments result in an impressive ROC curve with AUROC of 0.940 for DR onset and 0.914 for DR detection as shown in Table 5 and . These are the best results among those reported in literature for the Messidor dataset.

In addition in one embodiment a histogram of blob level decision statistics that is computed using one or more of the following operations is added i computation of the blobs in the image at various scales using the detected pixels ii computation of the average of the decision statistics to obtain one number per blob iii training of one or more another classifiers for lesions using the blob level decision statistics as the feature vector and use the new decision statistic or iv computation of one or more histograms of these decision statistics to form a blob level histogram s descriptor. In one embodiment these histogram descriptors are normalized to sum to 1 so as to mathematically look like a probability distribution.

As discussed above different descriptor types may be combined in various embodiments this does not preclude the use of any individual descriptor type or an arbitrary combination of a subset of descriptor types.

In another embodiment the method or system could be applied to a cross dataset scenario. This implies that the testing is done on a completely new unseen dataset. In an example implementation cross dataset testing is applied on all 1200 Messidor images without any training on this dataset. Instead the system uses the decision statistics computed for the various lesions. These statistics are the distances from classifier boundaries with the classifier being trained on the expert annotated images. In this example implementation 225 images from the DEI dataset are employed. The ROC curves for this example implementation shown in demonstrate an impressive cross dataset testing performance especially for detecting DR onset AUROC of 0.91 . For detecting any signs of DR the AUROC of 0.86 convincingly beats the best reported in literature including cross dataset AUROC of 0.76 disclosed in Quellec et al. A Multiple Instance Learning Framework for Diabetic Retinopathy Screening. . Table 5 presents a comparison of screening performance of some embodiments with various competing approaches on the Messidor dataset clearly showing superior diagnostic efficacy of the disclosed embodiments. Table 6 compares the results from the two approaches. Table 6 provides screening results AUROC for the two embodiments of screening system on Messidor dataset.

Cytomegalovirus retinitis CMVR is a treatable infection of the retina affecting HIV and AIDS patients and is a leading cause of blindness in many developing countries. In one embodiment methods and systems for screening of Cytomegalovirus retinitis using retinal fundus photographs is described. Visual inspection of the images from CMVR patients reveals that images with CMVR typically have large sub foveal irregular patches of retinal necrosis appearing as a white fluffy lesion with overlying retinal hemorrhages as seen in . These lesions have severely degraded image quality for example focus contrast normal color when compared with images of normal retina as shown in . A system which can effectively capture and flag the degradation in image quality can be used to screen for CMVR. Accordingly in one embodiment the image quality descriptors are adapted to the problem of CMVR screening providing a new use of the image quality descriptors described herein.

In one embodiment the image analysis engine automatically processes the images and extracts novel quality descriptors using for example the process described in the section above entitled Lens Shot Image Classification . These descriptors are then subjected to principal component analysis PCA for dimensionality reduction. They can then be used to train a support vector machine SVM classifier in a 5 fold cross validation framework using images that have been pre graded for Cytomegalovirus retinitis by experts for example into two categories normal retina and retina with CMVR. In one embodiment images graded by experts at UCSF and Chiang Mai University Medical Centre Thailand are employed. The system produces a result of refer for a patient image from category retina with CMVR and no refer for a patient image from category normal retina.

One embodiment was tested using 211 images graded for CMVR by randomly splitting them into 40 different training testing datasets. In each split 75 of the images were used for training and the other 25 were reserved for testing. As expected the lesion degraded poor quality images were flagged to be positive for CMVR by the system with an average accuracy of 85 where average area under ROC curve AUROC was 0.93. For many of the images the presence of large out of focus blurry or over under exposed regions such as shown in F for example resulted in the degradation of image quality causing the experts to be unsure about the presence or absence of CMVR during screening. These images marked with category cannot determine were excluded from the above experiments. By choosing an SVM classifier that produces an ROC curve with an AUROC close to the average of 0.93 obtained during the 40 experiments an additional 29 images from the cannot determine category were tested. None of these images were included during training phase. The system recommended that 27 of 29 images patients be referred which is acceptable given that experts too did not have consensus on CMVR status of those two images.

In one embodiment the quality of the image is first analyzed using a gradability assessment module. This module will flag blurry saturated or under exposed images to be of poor quality and unsuitable for reliable screening. The actual CMVR screening would then be performed on images that have passed this quality module. Both system could use the same descriptors but one can use a support vector regressor engine trained to assess quality and the other a support vector classifier trained to screen for CMVR. In another embodiment additional descriptors are included such as texture color layout and or other descriptors to the CMVR screening setup to help distinguish the lesions better.

Patients with early forms of Alzheimer s disease AD display narrower retinal veins compared to their peers without AD as discussed in Fatmire Berisha et al. Retinal Abnormalities in Early Alzheimer s Disease Investigative Ophthalmology Visual Science 48 no. 5 May 1 2007 2285 2289. Hence AD can be screened by customized vasculatoic analysis.

The retinal arterioles may narrow as a result of chronic hypertension and this may predict stroke and other cardiovascular diseases independent of blood pressure level as discussed in Tien Yin Wong Ronald Klein A. Richey Sharrett David J. Couper Barbara E. K. Klein Duan Ping Liao Larry D. Hubbard Thomas H. Mosley Cerebral white matter lesion retinopathy and risk of clinical stroke The Atherosclerosis Risk in the Communities Study . JAMA 2002 288 67 74. Thus the system may also be used to screen for strokes.

The retinal arterioles may narrow as a result of chronic hypertension and this may predict stroke and other cardiovascular diseases independent of blood pressure level as discussed in Tien Y. Wong Wayne Rosamond Patricia P. Chang David J. Couper A. Richey Sharrett Larry D. Hubbard Aaron R. Folsom Ronald Klein Retinopathy and risk of congestive heart failure . JAMA 2005 293 63 69. Thus the system may be used to screen for cardiovascular diseases.

Neovascularization vessel tortuosity and increased vessel thickness indicate retinopathy of prematurity as discussed in Flynn Jt et al. Retinopathy of Prematurity. Diagnosis Severity and Natural History. Ophthalmology 94 no. 6 June 1987 620 629. Thus retinopathy of prematurity can be analyzed by automated retinal image analysis tools for screening.

Lesions may also indicate macular degeneration as discussed in A. C. Bird et al. An International Classification and Grading System for Age Related Maculopathy and Age Related Macular Degeneration Survey of Ophthalmology 39 no. 5 March 1995 367 374. Thus lesions such as drusen bodies can be detected and localized using the lesion localization system described in the section above entitled Lesion Localization and this disease can be screened for using a similar setup as described in the section Diabetic retinopathy screening .

It is recognized that the systems and methods may be implemented in a variety of architectures including telemedicine screening cloud processing or using other modalities.

In one embodiment the system includes a flexible application programming interface API for integration with existing or new telemedicine systems programs or software. The Picture Archival and Communication System PACS is used as an example telemedicine service to enable such an integration. Block diagram of one embodiment of such a system is shown in . The system includes an API allowing coding of one or more of patients metadata de identifying images to anonymize patients for analysis and protect privacy analyzing image quality initiating reimaging as needed updating patient metadata storing images and analysis results in database inputting and or outputting transmission interfaces. The Image Analysis System IAS comprises one or more of the following input and output transmission interfaces for communication with the PACS system a database updater a quality assessment block to assess image gradability an analysis engine that can include a combination of one or more of the following tools disease screening lesion dynamics analysis or vessel dynamics analysis. In one embodiment the PACS and or the IAS system could be hosted on remote or local server or other computing system and in another embodiment they could be hosted or cloud infrastructure.

In one embodiment the API is designed to enable seamless inter operation of the IAS with a telemedicine service such as PACS though any telemedicine system software program or service could be used. An interface for one embodiment is presented in .

In one embodiment IAS initiates the transfer of results to PACS. In this mode of operation PACS would not have a control over when it would receive the results. The transfer may include one or more of the following 

In another embodiment PACS initiates the transfer of results to its system. In this mode of operation PACS can choose when to retrieve the analysis results from IAS server. This circumvents the possibility of data leaks since the screening results are sent from IAS upon request. The transfer may include one or more of the following 

Table 7 presents one embodiment of technical details of an interface with telemedicine and error codes for a Telemedicine API. The design includes considerations directed to security privacy data handling error conditions and or independent server operation. In one embodiment the PACS API key to obtain write permission to IAS server would be decided during initial integration along with the IAS API key to obtain write permission to PACS. The API URL such as https upload.eyepacs.com eyeart analysis upload for IAS to transfer results to PACS could either be set during initial registration or communicated each time during the POST request to https api.eyenuk.com eyeart upload.

Table 8 shows one embodiment of details of an IAS and PACS API. One embodiment of error codes are described in Table 7. The URLs uses in the table are for illustrative purposes only.

Image processing and analysis can be performed on the cloud including by using systems or computing devices in the cloud. Large scale retinal image processing and analysis may not be feasible on normal desktop computers or mobile devices. Producing results in near constant time irrespective of the size of the input dataset is possible if the retinal image analysis solutions are to be scaled. This section describes the retinal image acquisition and analysis systems and methods according to some embodiments as well as the cloud infrastructure used to implement those systems and methods.

In an embodiment of cloud based operation the patient image refers to the retinal data single or multidimensional captured from the patient using a retinal imaging device such as cameras for color image capture fluorescein angiography FA adaptive optics optical coherence tomography OCT hyperspectral imaging scanning laser ophthalmoscope SLO wide field imaging or ultra wide field imaging. The acquired images are stored on the local computer or computing device or mobile device and then transmitted to a central data center . Operators at the data center can then use traditional server based or computing device based desktop based or mobile based clients to push these images to the cloud for further analysis and processing. The cloud infrastructure generates patient level diagnostic reports which can trickle back to the patients for example through the same pipeline in reverse.

In another embodiment of cloud based operation the imaging setup can communicate with the cloud as indicated by dotted lines in . The images can be pushed to the cloud following acquisition. The diagnostic results are then obtained from the cloud typically within minutes enabling the clinicians or ophthalmologists to discuss the results with the patients during their imaging visit. It also enables seamless re imaging in cases where conclusive results could not be obtained using the initial images.

In another embodiment of cloud based operation data centers store images from thousands of patients . The data for example may have been collected as part of a clinical study for either disease research or discovery of drugs or treatments. The patient images may have been acquired in preparation for the study and then pushed to the cloud for batch processing. The images could also be part of routine clinical workflow where the analysis is carried out in batch mode for several patients. The cloud infrastructure can be scaled to accommodate the large number of patient encounters and perform retinal analysis on the encounters. The results can be presented to the researchers in a collated fashion enabling effective statistical analysis for the study.

In one embodiment the cloud operation described above has been implemented using Amazon Web Services infrastructure and the cloud storage is implemented using Simple Storage Service S3 . The input and output message queues may be implemented with Simple Queue Service SQS . The web server is hosted on a t1 micro Elastic Cloud Compute EC2 instance. The database is implemented with the Relational Database Service RDS running a MySQL database instance. Each worker machine is a c3.8xlarge EC2 instance with 32 processors and 60 GB of RAM. The cloud metrics are obtained using Cloud Watch. The scaling of EC2 capacity automatic creation and termination of worker machines is done using Amazon Auto Scaling. The software that runs on each of the worker machines is stored as an Amazon Machine Image AMI .

Widefield and ultra widefield retinal images capture fields of view of the retina in a single image that are larger than 45 50 degrees typically captured in retinal fundus images. These images are obtained either by using special camera hardware or by creating a montage using retinal images of different fields. The systems and methods described herein can apply to widefield and ultra widefield images.

Fluorescein angiography involves injection of a fluorescent tracer dye followed by an angiogram that measures the fluorescence emitted by illuminating the retina with light of wavelength 490 nanometers. Since the dye is present in the blood fluorescein angiography images highlight the vascular structures and lesions in the retina. The systems and methods described herein can apply to fluorescein angiography images.

Scanning laser retinal imaging uses horizontal and vertical mirrors to scan a region of the retina that is illuminated by laser while adaptive optics scanning laser imaging uses adaptive optics to mitigate optical aberrations in scanning laser images. The systems and methods described herein can apply to scanning laser and adaptive optics images.

In some embodiments the process of imaging is performed by a computing system such as that disclosed in .

In some embodiments the computing system includes one or more computing devices for example a personal computer that is IBM Macintosh Microsoft Windows or Linux Unix compatible or a server or workstation. In one embodiment the computing device comprises a server a laptop computer a smart phone a personal digital assistant a kiosk or a media player for example. In one embodiment the computing device includes one or more CPUS which may each include a conventional or proprietary microprocessor. The computing device further includes one or more memory such as random access memory RAM for temporary storage of information one or more read only memory ROM for permanent storage of information and one or more mass storage device such as a hard drive diskette solid state drive or optical media storage device. Typically the modules of the computing device are connected to the computer using a standard based bus system. In different embodiments the standard based bus system could be implemented in Peripheral Component Interconnect PCI Microchannel Small Computer System Interface SCSI Industrial Standard Architecture ISA and Extended ISA EISA architectures for example. In addition the functionality provided for in the components and modules of computing device may be combined into fewer components and modules or further separated into additional components and modules.

The computing device is generally controlled and coordinated by operating system software such as Windows XP Windows Vista Windows 7 Windows 8 Windows Server Embedded Windows Unix Linux Ubuntu Linux SunOS Solaris iOS Blackberry OS Android or other compatible operating systems. In Macintosh systems the operating system may be any available operating system such as MAC OS X. In other embodiments the computing device may be controlled by a proprietary operating system. Conventional operating systems control and schedule computer processes for execution perform memory management provide file system networking I O services and provide a user interface such as a graphical user interface GUI among other things.

The exemplary computing device may include one or more commonly available I O interfaces and devices such as a keyboard mouse touchpad touchscreen and printer. In one embodiment the I O interfaces and devices include one or more display devices such as a monitor or a touchscreen monitor that allows the visual presentation of data to a user. More particularly a display device provides for the presentation of GUIs application software data and multimedia presentations for example. The computing device may also include one or more multimedia devices such as cameras speakers video cards graphics accelerators and microphones for example.

In the embodiment of the imaging system tool of the I O interfaces and devices provide a communication interface to various external devices. In the embodiment of the computing device is electronically coupled to a network which comprises one or more of a LAN WAN and or the Internet for example via a wired wireless or combination of wired and wireless communication link . The network communicates with various computing devices and or other electronic devices via wired or wireless communication links.

According to in some embodiments images to be processed according to methods and systems described herein may be provided to the computing system over the network from one or more data sources . The data sources may include one or more internal and or external databases data sources and physical data stores. The data sources may include databases storing data to be processed with the imaging system according to the systems and methods described above or the data sources may include databases for storing data that has been processed with the imaging system according to the systems and methods described above. In some embodiments one or more of the databases or data sources may be implemented using a relational database such as Sybase Oracle CodeBase MySQL SQLite and Microsoft SQL Server as well as other types of databases such as for example a flat file database an entity relationship database and object oriented database NoSQL database and or a record based database.

In the embodiment of the computing system includes an imaging system module that may be stored in the mass storage device as executable software codes that are executed by the CPU . These modules may include by way of example components such as software components object oriented software components class components and task components processes functions attributes procedures subroutines segments of program code drivers firmware microcode circuitry data databases data structures tables arrays and variables. In the embodiment shown in the computing system is configured to execute the imaging system module in order to perform for example automated low level image processing automated image registration automated image assessment automated screening and or to implement new architectures described above.

In general the word module as used herein refers to logic embodied in hardware or firmware or to a collection of software instructions possibly having entry and exit points written in a programming language such as for example Python Java Lua C and or C . A software module may be compiled and linked into an executable program installed in a dynamic link library or may be written in an interpreted programming language such as for example BASIC Perl or Python. It will be appreciated that software modules may be callable from other modules or from themselves and or may be invoked in response to detected events or interrupts. Software modules configured for execution on computing devices may be provided on a computer readable medium such as a compact disc digital video disc flash drive or any other tangible medium. Such software code may be stored partially or fully on a memory device of the executing computing device such as the computing system for execution by the computing device. Software instructions may be embedded in firmware such as an EPROM. It will be further appreciated that hardware modules may be comprised of connected logic units such as gates and flip flops and or may be comprised of programmable units such as programmable gate arrays or processors. The block diagrams disclosed herein may be implemented as modules. The modules described herein are preferably implemented as software modules but may be represented in hardware or firmware. Generally the modules described herein refer to logical modules that may be combined with other modules or divided into sub modules despite their physical organization or storage.

Each of the processes methods and algorithms described in the preceding sections may be embodied in and fully or partially automated by code modules executed by one or more computer systems or computer processors comprising computer hardware. The code modules may be stored on any type of non transitory computer readable medium or computer storage device such as hard drives solid state memory optical disc and or the like. The systems and modules may also be transmitted as generated data signals for example as part of a carrier wave or other analog or digital propagated signal on a variety of computer readable transmission mediums including wireless based and wired cable based mediums and may take a variety of forms for example as part of a single or multiplexed analog signal or as multiple discrete digital packets or frames . The processes and algorithms may be implemented partially or wholly in application specific circuitry. The results of the disclosed processes and process steps may be stored persistently or otherwise in any type of non transitory computer storage such as for example volatile or non volatile storage.

The various features and processes described above may be used independently of one another or may be combined in various ways. All possible combinations and subcombinations are intended to fall within the scope of this disclosure. In addition certain method or process blocks may be omitted in some implementations. The methods and processes described herein are also not limited to any particular sequence and the blocks or states relating thereto can be performed in other sequences that are appropriate. For example described blocks or states may be performed in an order other than that specifically disclosed or multiple blocks or states may be combined in a single block or state. The example blocks or states may be performed in serial in parallel or in some other manner. Blocks or states may be added to or removed from the disclosed example embodiments. The example systems and components described herein may be configured differently than described. For example elements may be added to removed from or rearranged compared to the disclosed example embodiments.

Conditional language such as among others can could might or may unless specifically stated otherwise or otherwise understood within the context as used is generally intended to convey that certain embodiments include while other embodiments do not include certain features elements and or steps. Thus such conditional language is not generally intended to imply that features elements and or steps are in any way required for one or more embodiments or that one or more embodiments necessarily include logic for deciding with or without user input or prompting whether these features elements and or steps are included or are to be performed in any particular embodiment. The term including means included but not limited to. The term or means and or .

Any process descriptions elements or blocks in the flow or block diagrams described herein and or depicted in the attached figures should be understood as potentially representing modules segments or portions of code which include one or more executable instructions for implementing specific logical functions or steps in the process. Alternate implementations are included within the scope of the embodiments described herein in which elements or functions may be deleted executed out of order from that shown or discussed including substantially concurrently or in reverse order depending on the functionality involved as would be understood by those skilled in the art.

All of the methods and processes described above may be embodied in and partially or fully automated via software code modules executed by one or more general purpose computers. For example the methods described herein may be performed by the computing system and or any other suitable computing device. The methods may be executed on the computing devices in response to execution of software instructions or other executable code read from a tangible computer readable medium. A tangible computer readable medium is a data storage device that can store data that is readable by a computer system. Examples of computer readable mediums include read only memory random access memory other volatile or non volatile memory devices CD ROMs magnetic tape flash drives and optical data storage devices.

It should be emphasized that many variations and modifications may be made to the above described embodiments the elements of which are to be understood as being among other acceptable examples. All such modifications and variations are intended to be included herein within the scope of this disclosure. The foregoing description details certain embodiments. It will be appreciated however that no matter how detailed the foregoing appears in text the systems and methods can be practiced in many ways. For example a feature of one embodiment may be used with a feature in a different embodiment. As is also stated above it should be noted that the use of particular terminology when describing certain features or aspects of the systems and methods should not be taken to imply that the terminology is being re defined herein to be restricted to including any specific characteristics of the features or aspects of the systems and methods with which that terminology is associated.

