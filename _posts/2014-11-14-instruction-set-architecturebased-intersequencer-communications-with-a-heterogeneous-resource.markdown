---

title: Instruction set architecture-based inter-sequencer communications with a heterogeneous resource
abstract: In one embodiment, the present invention includes a method for directly communicating between an accelerator and an instruction sequencer coupled thereto, where the accelerator is a heterogeneous resource with respect to the instruction sequencer. An interface may be used to provide the communication between these resources. Via such a communication mechanism a user-level application may directly communicate with the accelerator without operating system support. Further, the instruction sequencer and the accelerator may perform operations in parallel. Other embodiments are described and claimed.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09459874&OS=09459874&RS=09459874
owner: Intel Corporation
number: 09459874
owner_city: Santa Clara
owner_country: US
publication_date: 20141114
---
This application is a continuation of U.S. patent application Ser. No. 13 791 298 filed Mar. 8 2013 which is a continuation of U.S. patent application Ser. No. 11 321 779 filed Dec. 29 2005 the content of which is hereby incorporated by reference.

Embodiments of the present invention relate to improving communications in a processor based system and more particularly to a system including multiple sequencers.

Computer systems include various components to process and communicate data. Typical systems include one or multiple processors each of which may include multiple cores along with associated memories input output I O devices and other such components. To improve computation efficiencies computation accelerators special purpose I O devices and other such specialized units may be provided via one or more specialized components referred to generically herein as helper units. However inefficiencies may occur in using such helper units as in a typical computing environment that implements a general purpose processor and an industry standard operating system OS environment a software stack can impede efficient usage. That is in a typical OS environment system software is isolated from application software via different privilege levels and operations in each of these different privilege levels are subject to OS context save and restore operations among other limitations.

Thus whenever a helper unit such as a special purpose accelerator is incorporated it is usually exposed as a device and a user level application can only indirectly use the helper unit via the OS s device driver software stack which has direct access to the raw physical helper unit resource. Consequently the helper unit resource via the associated device driver is a system wide resource and not an application level resource such as general purpose registers virtual memory or sequencers which are virtualized across context switches.

The problem with having to use a device driver to access a helper unit is the inefficiency in terms of path length from application to driver to the helper unit and inflexibility due to OS imposed restrictions related to standardized driver interfaces.

Classic examples of a computation accelerator are coprocessors such as math coprocessors like so called x87 floating point coprocessors for early Intel Architecture IA 32 processors . Typically such coprocessors are coupled to a main processor e.g. a central processing unit CPU via a coprocessor interface which is of a common instruction set architecture ISA as the main processor. Furthermore the interaction between these resources is via a traditional escape wait signal protocol in which the main processor is placed in a wait state while the coprocessor performs its requested function at the conclusion of which control returns back to the main processor. However during coprocessor operations the main processor cannot perform useful work while waiting for a result from the coprocessor. That is the coprocessor is integrated such that it architecturally operates sequentially with the program order of the control flow of a main processor. This leads to inefficiencies in processor utilization especially when the coprocessors are capable of operations that are concurrent with computation on the main processor. A need thus exists for an improved manner of communicating with and using such helper units.

In various embodiments mechanisms are provided to enable instruction set architecture ISA based inter sequencer communications. As used herein a sequencer is a distinct thread execution resource and may be any physical or logical unit capable of executing a thread. A sequencer may be a logical thread unit or a physical thread unit and may include next instruction pointer logic to determine the next instruction to be executed for the given thread.

More particularly ISA based inter sequencer communications may be implemented between a first sequencer of a first ISA and a second resource which may be a sequencer or non sequencer of a heterogeneous nature. That is the second resource may be a sequencer of a different ISA or may be a non sequencer resource such as a fixed function unit FFU an application specific integrated circuit ASIC or other pre programmed logic. In various embodiments an intermediary or interface referred to herein as an exo skeleton may provide for communication between such heterogeneous resources. In different embodiments an exo skeleton may take various forms including software hardware and or firmware. In some embodiments the exo skeleton may be implemented in a finite state machine FSM tightly coupled to the heterogeneous resource. Of course other implementations are possible.

Referring now to shown is a block diagram of a processor in accordance with one embodiment of the present invention. As shown in processor includes a variety of different resources. In different implementations processor may be a single core processor or a multi core processor. Such a processor may be implemented in different types of systems including a chip multiprocessor CMP system or a simultaneous multithreading SMT system or a switch on event multithreading SoeMT system among other such systems.

As shown in processor includes a plurality of sequencers and i.e. sequencers 1 4 and generically sequencers . While shown with four such sequencers in the embodiment of it is to be understood that the scope of the present invention is not so limited. As shown in sequencers in processor implement an ISA which in one embodiment may be an Intel Architecture IA 32 instruction set architecture and or its 64 bit extension also called Intel extended memory 64 bit technology EM64T . Processor further includes other resources including a first resource i.e. resource 1 a second resource i.e. resource 2 and a third resource i.e. resource 3 and generically resources . These resources may be heterogeneous resources that do not implement the ISA of processor . While shown in the embodiment of as including three such resources more or fewer are possible in different embodiments.

Each resource includes a sequencer which may implement a different ISA from ISA non sequencer processing engine or other specialized functional logic referred to generically herein as an accelerator. In different embodiments different types of resources may be implemented as accelerators including a graphics processing unit GPU typically a sequencer a cryptographic unit typically a non sequencer a physics processing unit PPU typically a non sequencer a fixed function unit FFU typically a non sequencer and the like. As shown in each resource may include an accelerator generically and more specifically accelerators and each associated with one of resources . Accelerators are also referred to herein as helper units. Because resources may be of another ISA or may even be a non sequencer and as such can be heterogeneous with respect to sequencers an interface may be used to provide the ability to communicate with such resources. Specifically as shown in exo skeletons and generically exo skeleton may be associated with each of resources . Each resource may thus be referred to as an exo sequencer indicating the tight coupling between exo skeleton and its associated accelerator . In this manner these heterogeneous resources may be integrated with homogeneous sequencer resources in a unified ISA framework that supports inter sequencer communication and or shared memory based addressing if applicable . Furthermore the various resources may execute in a parallel fashion for example in a multiple instruction multiple data MIMD fashion so that each resource may be used concurrently to improve performance.

However in other embodiments resources may be homogeneous sequencer resources with respect to sequencers and can be symmetric cores such that they include the same or similar architecture as sequencers . In such manner concurrent fibers may be implemented and legacy OS scalability can be enhanced. Still further in other implementations resources may be asymmetric cores. In other words these resources may be of the same ISA as sequencers but of a different microarchitecture. Such embodiments may help manage the asymmetry and provide compatibility with a legacy OS.

For embodiments that implement heterogeneous resources an exo skeleton in accordance with an embodiment may provide the illusion that these heterogeneous resources are of a common ISA to achieve minimal compliance for inter sequencer communications. Thus in various embodiments a heterogeneous resource can function as a user level functional unit resource rather than a system level device . In other words various user level applications may directly communicate with and access the accelerator such that it becomes a user level functional unit. In this way various accelerator resources can become ISA managed heterogeneous components.

For example each exo sequencer or resource may expose a special purpose computation accelerator with a sequencer like wrapper exo skeleton so that these accelerators can be used directly by application programs as user level heterogeneous computation resources such as MIMD resources.

Accordingly an application programmer can directly use a user level ISA to program these helper units even though the helper unit itself physically is not necessarily of the ISA . Further the programmer can use a uniform ISA mechanism to program a diverse set of heterogeneous helper units each having distinct characteristics in terms of ISA or device attributes . In effect an exo sequencer in accordance with an embodiment of the present invention allows the programmer to short circuit a legacy device driver software stack. To this end an exo sequencer may thus equip a helper unit with a veneer exo skeleton and make the helper unit appear to be a minimal sequencer that can participate in sequencer aware ISA based inter sequencer operations of a processor or other device. From the perspective of a software stack with an exo sequencer an application program or user runtime can provide an application level software layer to manage the accelerator as an application resource and as a tightly coupled part of the application binary without the need for using a loosely coupled OS based device driver to manage the accelerator as a system level resource.

Furthermore in some implementations one or more exo sequencers may include an explicit MIMD multi sequencer ISA interface where each participant helper unit is architecturally used as a minimal sequencer resource interacting with the main processor i.e. a first sequencer via sequencer aware synchronous operations or asynchronous inter sequencer interactions via fly weight user level event yield mechanisms. Even though the helper units and the first sequencer are tightly coupled to the same OS thread architecturally the main processor and exo skeleton equipped helper unit interact as two distinct MIMD sequencers. In particular the data and control exchange between the first sequencer and the helper unit via the exo skeleton interface is architecturally equivalent to an inter sequencer exchange.

While shown with the particular resources in the embodiment of it is to be understood that processor may be a single physical processor that can support multiple hardware thread contexts without loss of clarity also called thread context note this is not the same as software thread context each including a set of the architectural state. In some embodiments certain resources may be visible to these thread contexts while other resources are invisible. Thus as shown in each of sequencers may correspond to a thread context. When at least some of these thread contexts e.g. m out of n m n are made visible to the operating system these thread contexts are sometimes referred to as logical processors or OS managed sequencers. Each thread context maintains a set of the architecture state AS AS respectively. The architecture state includes for example data registers segment registers control registers debug registers and most of the model specific registers. The thread contexts may share most microarchitectural resources of the physical processor such as caches execution units branch predictors control logic and buses. Although such features may be shared each thread context of processor can independently generate a next instruction address and perform for instance a fetch from an instruction cache an execution instruction cache or trace cache . Each of sequencers corresponding to a thread context is associated with a corresponding architecture state generically . More specifically architecture state AS may be associated with sequencer ASmay be associated with sequencer ASmay be associated with sequencer and ASmay be associated with sequencer for example.

Using processor or a similar such processor ISA based inter sequencer communications may occur without involving an OS. For example in a shared memory multiprocessing paradigm an application programmer may split a software program i.e. an application or process into multiple tasks to be run concurrently in order to express parallelism. All threads of the same software program process share a common logical view of memory address space. However an OS thread may be associated with multiple user level threads that may not be created scheduled or otherwise managed by the operating system. Such user level threads may be referred to as shreds in order to distinguish them from OS threads. These shreds may not be visible to the OS scheduler and therefore the OS does not manage when or how the associated OS thread schedules a shred to run on an assigned logical sequencer address. The OS thread is itself usually responsible to schedule when and how to run one of its shreds.

Architectural support for ISA based inter sequencer communications may include extensions to an ISA such that one or more instructions are provided to allow a user to directly manipulate control and state transfers between sequencers which may be so called sequencer aware or sequencer arithmetic instructions. Such instructions may include instructions that either provide for a first sequencer to signal another i.e. a second sequencer one instruction is referred to herein as a shred transfer or SXFR instruction which may send egress control information called an egress scenario and may also carry data payload or provide for setting up a second sequencer to monitor for such a signal referred to herein as a shred monitor or SEMONITOR instruction and perform control transfer to a handler upon receiving the signal called an ingress scenario asynchronously.

Sequencer aware instructions may also include other instructions such as sequencer aware state save and restore instructions. Upon execution of such a state save instruction a first sequencer can create a snapshot copy of the architectural state s of a second sequencer. The sequencer aware restore instruction may designate that the saved architectural states be loaded to a specified sequencer.

For at least one embodiment one or more sequencer aware instructions may be coded by a programmer into the shreds belonging to an OS thread. Such instructions when executed during operation of the OS thread may cause creation control transfer context save context restore or other operations for shreds without intervention of OS scheduling logic.

In such manner ISA based inter sequencer communication in accordance with an embodiment reduces overhead improving performance. In addition to communication between sequencers of the same ISA in various embodiments ISA based inter sequencer communications may occur between heterogeneous sequencers or between a sequencer and a non sequencer e.g. via an exo skeleton in accordance with an embodiment of the present invention.

Referring now to shown is a block diagram of a portion of a system in accordance with one embodiment of the present invention. As shown in system includes a sequencer and an accelerator . Accelerator may take many different forms in different embodiments but for purposes of discussion herein it may be assumed that accelerator is of a heterogeneous nature to sequencer . In other words accelerator may be of a different ISA or may be a non sequencer. In various embodiments however sequencer and accelerator may be implemented on a single substrate e.g. as part of a multi core processor . Alternately sequencer and accelerator may be implemented in different pieces of silicon within an integrated circuit IC or in different ICs such as located on a package or a motherboard or in another manner.

To enable ISA based inter sequencer communications an exo skeleton may be coupled to accelerator . Exo skeleton and accelerator together are also referred to herein as exo sequencer . In embodiments in which accelerator is of a heterogeneous ISA or is a non sequencer exo skeleton which may be a finite state machine FSM or virtualization layer may be implemented in hardware firmware or even in software depending on specific embodiments so that accelerator can participate in inter sequencer communications. Such ISA based inter sequencer communications provide a signaling protocol in an ingress direction into accelerator so that it may monitor and respond to ingress scenarios sent by SXFR from another sequencer or exo sequencer including GET and or SET instructions for the exo sequencer s architectural state. Furthermore the signaling protocols include egress communications from accelerator to signal sequencer with an egress scenario including an indication for exception handling such as a proxy execution request for such events as page faults. Furthermore in some embodiments accelerator via exo skeleton may participate in capability reporting oriented communication activities such that upon a query from sequencer or another resource exo skeleton can communicate information regarding the capabilities of accelerator to permit their more effective usage.

As shown in exo skeleton may be tightly coupled with accelerator . As further shown in system includes a user level runtime software layer denoted as a shred library to manage and schedule shreds i.e. user level threads. In different implementations shred library may implement operations to support and manage shreds on sequencers such as sequencer . For example shred library may manage shred scheduling shred context save switch and restore and the like. In this way lower overhead is consumed as an OS is not involved in these operations.

A user application may be executing within system and may request execution of a specialized or compute intensive function. To enable improved performance particularly in a MIMD or parallel environment user application may request that sequencer communicate with accelerator such that accelerator is to perform a function in parallel with sequencer performing other useful work of the application or other shreds to be executed by sequencer . In this manner improved execution is effected as both sequencer and accelerator may execute in parallel on different portions of an application thread as shreds. Accordingly using embodiments of the present invention control flow of sequencer can run in parallel and asynchronously with special purpose computation performed on an exo sequencer which effectively operates as a separate shred.

To reduce overhead ISA based inter sequencer communication between sequencer and exo sequencer via sequencer aware instructions may not require involvement of OS . In this way a device driver stack of OS can be avoided and instead direct communications between sequencer and exo sequencer can be effected. Thus as shown in in one embodiment ISA based inter sequencer communications may proceed directly between sequencer to exo skeleton through sequencer aware instructions. Application code which may be a user application can use such ISA based inter sequencer communication through shred runtime library . A minimal OS layer supports an OS thread s context save and context restore operations. When the OS thread performs context save or context restore instructions on an OS managed sequencer the contexts for all application managed sequencers and exo sequencers that are associated with the OS managed sequencer will be saved and restored accordingly. An OS may provide sufficient save area per OS thread for saving these states across OS thread context switch. There the ISA based inter sequencer communication may be translated in such a manner that it can be provided to and acted upon by exo sequencer . Similar communication flow may occur in the reverse direction. In at least one embodiment the context save and restore at each sequencer or exo sequencer upon an OS thread context switch can be performed by each sequencer or exo sequencer in parallel with other sequencers exo sequencers in the same OS thread. Such parallel implementation may improve performance of the overall save and restore operations.

While shown in the embodiment of as including a single sequencer it is to be understood that the scope of the present invention is not so limited and in other embodiments a multi sequencer system may be provided. In such a system exo skeleton may further implement virtualization functions so that a single physical accelerator may be associated by the multiple sequencers. Accordingly exo skeleton may implement storages for multiple contexts to store multiple copies of the architectural state of accelerator for the different sequencers each coupled to a logical exo sequencer. Exo skeleton may further include logic to enable the saving and restoring of the context state as well as to provide for the sequencing or multiplexing of the different sequencers to use accelerator . In this way accelerator may service multiple sequencers of a system. In at least one embodiment the exo skeleton can implement multiple logical exo sequencers via switch on event multithreading. In such an embodiment the logical exo sequencer contexts can be implemented as distinct on chip register files or dedicated scratch memory and the exo sequencer context switching condition can be implemented in logic or in firmware such as microcode. In at least another embodiment a distinct physical exo sequencer can be implemented for each sequencer. In this embodiment multiple physical exo sequencers can be implemented as simultaneous multithreading SMT or CMP systems.

Embodiments such as that shown in may be used when an accelerator is not of the same ISA as sequencer and or the remaining system or where the accelerator is a non sequencer. In such embodiments the data flow shown in provides for efficient ISA based inter sequencer communications between sequencer and accelerator without involvement of OS .

However in other implementations an accelerator may be of a common ISA as a sequencer or other portion of the system. Still further in some implementations an accelerator may be closely configured with a remainder of a system. For example in some implementations an accelerator may be a system component that is configured to execute specialized operations. However in a given system configuration the component is disabled or minimally used in favor of other components such as peripheral or add in devices. For example an integrated graphics processing unit GPU may be implemented as part of a chipset such as a graphics memory controller hub GMCH on a system motherboard. However in certain system configurations an add in discrete graphics card is also configured as plug in e.g. as on a Peripheral Component Interconnect PCI slot to the motherboard. In such instances the integrated GPU may be disabled and otherwise unused. Similarly a GPU or other such component may include multiple different processing engines some of which may not be fully utilized in certain system configurations.

In such implementations these otherwise disabled or minimally used processing resources may be configured as an accelerator in accordance with an embodiment of the present invention. Referring now to shown is a block diagram of portion of a system which includes a sequencer and an accelerator . In the embodiment of accelerator may include a first portion and a second portion . First portion may be configured to act as the exo sequencer while second portion may be configured to perform various functions such as specialized graphics or media functions under normal OS control. Accordingly as shown in second portion is coupled to a device driver stack of an OS such that it may communicate with an application of an OS based execution model through OS application programming interfaces APIs to device driver stack . In this manner second portion can perform processing functions under request of application through a conventional OS communication route.

In contrast first portion is configured to directly communicate with sequencer via an exo skeleton . Exo skeleton may be hardware software firmware or a combination thereof to enable ISA based communication between first portion and sequencer . Accordingly a user level application can use ISA based inter sequencer instructions for communications between sequencer and first portion through exo skeleton . Typically user level shred library can be used and a minimal layer of OS support is used to provide support to OS thread context save and restore operations as discussed above.

Thus in the embodiment of two software stacks may co exist namely the OS driver stack and a user level runtime stack in accordance with an embodiment of the present invention. By providing for resource sharing of accelerator improved performance may be effected as both legacy based applications e.g. using an OS device driver model and user level applications that provide for minimal overhead via ISA based inter sequencer communications can utilize the resources of accelerator . In some implementations exo skeleton may perform virtualization tasks with respect to accelerator such that application and application both believe that they have ownership of the full resources as visible to the application of accelerator . Thus in different embodiments exo skeleton may perform virtualization tasks including providing multiple contexts of architectural state for accelerator and providing supporting to context switching under conditions akin to switch on event multithreading.

As such a subset of the functionality in accelerator may be associated with legacy applications via OS while a different subset of the functionality may be associated with user level applications via an ISA based inter sequencer communication protocol in accordance with an embodiment of the present invention. Thus the physical resources of accelerator may support co existence of these two disparate paradigms.

Referring now to shown is a block diagram of a system in accordance with one embodiment of the present invention. As shown in system includes a processor e.g. sequencer coupled to a graphics memory controller hub GMCH which in turn is coupled to a memory that may be for example a dynamic random access memory DRAM . Furthermore GMCH is coupled to a display such as a flat panel display . GMCH may include an integrated graphics accelerator. GMCH is further coupled to an input output I O controller hub ICH which may be used to couple various peripheral devices to system . Shown for example in the embodiment of is an external graphics device which may be a discrete graphics device coupled to ICH along with another peripheral device .

Because system is configured with a separate external discrete graphics device the integrated graphics within GMCH may be disabled. For example the system basic input output system BIOS may program a disable bit or another mechanism may disable graphics functionality in GMCH . The otherwise idle processing resources used for graphics processing in GMCH may instead be converted into an accelerator in accordance with an embodiment of the present invention. Accordingly the resource may be used to implement various functionality e.g. via a user level application and without the need for OS involvement. In this way improved performance may be effected particularly where such processing by the graphics resources of GMCH are performed in parallel e.g. in a MIMD fashion with tasks in sequencers in processor .

In some embodiments the exo sequencers for graphics functionality in the integrated graphics of GMCH may include various graphics processing units to perform orthogonal functionalities. One or more of these processing resources may be configured as a ISA based media accelerator exo sequencer to implement media operations within system without involvement of one or more media device drivers associated with an OS of system . In this way media operations such as the encoding and decoding of digital media can be performed in system with minimal involvement of processor and furthermore without suffering the overhead of a device driver stack of the OS.

In various embodiments like a sequencer an exo sequencer is a form of application level architectural resource and thus may have a unique and virtualizable name space for use in sequencer aware user level operations. Thus inter sequencer communication may be performed entirely at user level without OS intervention. With sequencer virtualization the logical exo sequencer architecture resource can be managed in ways similar to register renaming and memory virtualization where the number of physical resources do not need to be the same as the number of logical resources. Further there can be a variety of differences between the physical resources in terms of a spectrum of metrics of merit including architectural microarchitectural thermal power consumption characteristics and the like. These differences effectively manifest themselves as asymmetry and heterogeneity amongst the exo sequencer resources. In addition as virtually addressable resources exo sequencer resources can also be subject to capability based management where bits in a virtual sequencer address can be used by certain hardware firmware and software layers of abstraction to represent physical sequencer specific capabilities to manage synchronous or asynchronous access control of the exo sequencer resources.

In at least one embodiment an exo sequencer may support minimal inter sequencer exchange protocols by providing a response to the following two canonical ingress scenarios from a first sequencer a GET ingress scenario in which a first sequencer can use SXFR to send the exo sequencer the GET scenario to read attain a state local to the exo sequencer and a SET ingress scenario in which the first sequencer can use SXFR to send the exo sequencer the SET scenario to write update a state local to the exo sequencer. Still further the exo sequencer can use a NOTIFY egress scenario and SXFR to send the first sequencer a NOTIFY scenario to perform asynchronous event notification of completion progress or an exception condition for example. A PROXY scenario which may cause an accelerator to operate in a proxy execution mode on behalf of the first sequencer is a special form of a NOTIFY egress scenario.

With this minimalist state report and update capability the first sequencer can compose high level control messages to manipulate data and control states individually or in the aggregate . In particular various composite state accesses can be built. For example using a context save restore either the first sequencer can repeatedly use SXFR to read or update a set of states to be saved restored across a context switch or in an alternative embodiment the first sequencer can use one SXFR to read and update a subset of states that the exo sequencer locally iterates over multiple states. Furthermore the first sequencer can use SXFR to query configuration i.e. capability enumeration including states associated attributes write read etc. format and the like. In such manner a special class of integrated devices which traditionally are either not sequencers or are not capable of working with a general purpose processor without an OS based device driver may be virtualized as desired by a particular application as if they are functional units of the processor and their states are subject to OS thread context save and restore across OS thread context switches.

In one embodiment a helper unit of a sequencer type and of a different ISA from the first sequencer can use its private ISA to implement the exo skeleton finite state machine FSM to communicate via incoming and outgoing scenarios that are architecturally defined for the first sequencer. In this manner the helper unit can choose to use its private ISA to implement various algorithms in response to an operation requested by the first sequencer even though the helper unit hardware itself does not directly support the operation. This sequencer FSM implementation can be done on any physical interconnect or interface. The two devices on the interconnect may signal each other and the signal may be received and translated locally as a user level event logically and architecturally on the first sequencer and as trigger input to the FSM. In one embodiment the exo sequencer s aggregate state subject to query related context save restore ingress egress scenarios may be a subset or full set of the helper unit s original states. In this case the exo skeleton code in the helper unit s private ISA behaves like a microcode routine implemented in firmware to support the first sequencer s sequencer aware operation.

In another embodiment a sequestered sequencer of the same ISA as the first sequencer can be used as the exo skeleton to implement a minimal FSM on behalf of the helper unit. While the private protocol between the exo sequencer and the helper unit is not necessarily native ISA compliant the first sequencer can interact with the exo skeleton so that the exo skeleton presents to the first sequencer the aggregate states of itself and the helper unit i.e. of a different ISA . The helper unit s states can be mapped 1 1 or M 1 where M 1 by the exo skeleton sequencer to the exo sequencer s aggregate state. Again the code sequence implementing the FSM to support the inter sequencer operation of the first sequencer is like microcode based control of the helper unit hardware.

In another example an exo sequencer may include a hardwired fixed function helper device that has no built in control sequencing of any ISA e.g. a special ASIC block with I O logic. For such a helper unit in at least one embodiment the exo skeleton can be built via a proxy FSM attached to the fixed function unit. The minimal sequencer state enumeration and query interface can be implemented in the exo skeleton FSM which has physical access to the ASIC. The exposed ASIC states can be mapped 1 1 or M 1 to the exo skeleton sequencer states and may be implemented in the exo skeleton FSM. The FSM logic may be physically implemented as an interface to the ASIC block. In other embodiments the exo skeleton FSM can be a programmable microcontroller sequencer or a hard wired ASIC block but capable of intercepting ingress scenarios and emitting egress scenarios.

In one embodiment a sequestered sequencer of the same ISA as the first sequencer can act to implement the exo skeleton FSM and the exo skeleton sequencer may have physical access to the ASIC states and logically architecturally aggregate the ASIC states plus its own states as the exo sequencer state. In another embodiment a sequestered sequencer of different ISA from the first sequencer can act to implement the exo skeleton FSM.

In either case the sequestration of the exo skeleton sequencer can be done either physically by reserving a physical sequencer statically dedicated to a given helper unit or dynamically multiplexed as multiple logical sequestered sequencers each logically associated with a unique helper unit. In other words the exo skeleton sequencer can be a virtualized sequencer via any desired scheme for sequencer address virtualization.

In one embodiment an accelerator can have non coherent scratch pad memory with respect to the first sequencer. To make such a helper unit an exo sequencer the non coherent scratch pad states can be mapped as part of the published i.e. enumerable aggregate state of the exo sequencer. If the scratch memory size is sufficiently large the exo skeleton can subject only a limited subset of the scratch state including zero amount of state as part of the corresponding exo sequencer state. For example even though an accelerator can process a frame of data of 1 kilobyte Kb length the exo sequencer may only expose the accelerator as including 256 bytes of buffer state to be save restored.

For a helper unit device that uses a direct memory access DMA protocol to transfer data to from a first sequencer in one embodiment a private protocol between the helper unit and the exo skeleton can remain raw DMA based while the exo sequencer can present to the first sequencer the aggregate state of the exo skeleton and the DMA configuration states on the helper unit device. Then the DMA configuration and exo sequencer emulation implementation FSM states on the exo skeleton can be subject to context save restore and thus be virtualizable across an OS context switch . However the first sequencer does not need to be architecturally aware of any of the DMA configuration information which is implementation specific.

In another embodiment an exo sequencer e.g. as a user level architectural resource and a system level e.g. a privileged level device resource can actually share a common physical raw building block resource. The published logical states for the exo sequencer can be renamed to the same physical states in the building block that are accessible via read write commands by the system level logical device. Inside the physical helper unit the division partition of resources can be statically or dynamically managed and transparent to the ISAs for both the user level exo sequencer and the system level device.

Certain server platforms are equipped with encryption decryption accelerators for speeding up network stack computations. These crypto engines are usually physically coupled to a network processor that includes programmable micro engines to control the crypto accelerators. As an example a cryptographic engine may include a random number generator or a pseudo random number generator such as for use in hash table computations. In such an implementation a processor micro engine may be reprogrammed as an exo skeleton FSM coupled with a crypto engine to be an exo sequencer.

In one embodiment a sequestered accelerator can be emulated via a virtual machine extension VMX based emulator virtual machine monitor VMM . The ingress egress scenario for state access and updates including context save restore may be implemented on top of the sequencer emulation with additional architectural states as the exo sequencer s aggregate state. Asynchronous NOTIFY signaling from the exo sequencer to the first sequencer may be implemented as a PROXY scenario.

Via an exo skeleton additional ingress scenarios that can represent various inter sequencer computation primitives can be emulated or passed through via the exo sequencer FSM emulator and delivered to the sequestered raw physical resource for computation acceleration. Effectively a subset of the raw computation resources of the helper unit may be used by application programs as if they are user level MIMD sequencer architecture resources.

Referring now to shown is a flow diagram of a method in accordance with one embodiment of the present invention. As shown in method may be implemented in a sequencer for example a processor core or the like. As shown in method may begin by performing a start monitor e.g. SEMONITOR instruction to start monitoring a signal from an accelerator and associate the signal with an event handler in the sequencer block . Specifically such signal may be an identification or notification signal to indicate a message from the accelerator. The accelerator may be a heterogeneous resource with respect to the instruction sequencer for example of a different ISA or a non sequencer. Accordingly communications between the instruction sequencer and the accelerator may be via an exo skeleton. Accordingly the signal received from the accelerator may be received by the sequencer via the exo skeleton.

To configure and enable the accelerator to perform operations on behalf of a user level application an architectural state for use with the application may be transferred to the accelerator block e.g. via SXFR. For example the instruction sequencer may transfer via the exo skeleton various information for a shred architectural state corresponding to register values configuration information and the like.

Next the instruction sequencer may prepare command information for the accelerator so that the accelerator can perform one or more operations on behalf of the user level application block . For example the instruction sequencer may prepare command information which may further include data to be manipulated or processed by the accelerator and or particular acceleration functions to be applied on the data. Then this command information may be communicated via an inter sequencer protocol block e.g. including use of the SXFR instruction. More specifically this protocol may be of the instruction sequencer s ISA. This information may be communicated directly to the exo skeleton using the protocol which may translate the protocol so that the underlying data can be passed to the accelerator.

Different manners of implementing such inter sequencer protocol communications can occur in various embodiments. For example in some embodiments a communication may include one or more instructions for each work element stored in a buffer which may be a shared memory buffer between the instruction sequencer and the accelerator. In other embodiments minimal command information may be sent from the instruction sequencer for example an instruction pointer IP to a code segment desired to be executed on the accelerator. Then assuming that the accelerator is itself a sequencer the accelerator may implement operations to fetch the code from the indicated location. The granularity with which command information is sent can also vary. For example command information may be sent on a per command basis or on a broader granularity in different embodiments. In yet another embodiment the first sequencer can send native commands to the accelerator via the exo skeleton and without going through memory and the accelerator can then directly execute the command.

Still referring to next the instruction sequencer may perform orthogonal e.g. independent operations in parallel with the accelerator s operations under the instruction sequencer s command block . That is in some embodiments parallel operations for example MIMD operations may be performed so that both the instruction sequencer and the accelerator can perform useful work in parallel. In this way the instruction sequencer need not wait for results from the accelerator and can instead perform other useful work. Of course the instruction sequencer may perform related operations in parallel.

Next it may be determined whether the instruction sequencer has received an event from the accelerator diamond . Such event may be in the form of a notification or other message indicating a status or completion of a task in the accelerator. Note that this determination need not be via synchronous polling and instead the notification may be asynchronous and event driven and thus non polling based. If no such event is received at diamond control may pass back to block discussed above. Instead when an event is received the instruction sequencer may initiate an event handler e.g. originally registered via SMONITOR in block to receive and process the data from the accelerator block . In various implementations this event handler may be a light weight user level handler such that the overhead of an OS call or context switch is avoided. In this way improved operation may occur and result data may be obtained from the accelerator and used as desired by the user level application. The control transfer to the handler may occur after the next instruction pointer is recorded e.g. pushed to the stack such that the suspended execution can later resume after the event handler completes.

Referring now to shown is a flow diagram of a method corresponding to operations performed in an accelerator with an exo skeleton in accordance with an embodiment of the present invention. As described above this accelerator may be of a heterogeneous nature with respect to the sequencer. Accordingly the exo skeleton can provide inter sequencer protocol communications. In response to block of discussed above the accelerator may receive shred architecture state information from the sequencer. Accordingly the accelerator may be configured based on this information block . Because of the heterogeneous nature of the accelerator the exo skeleton may be used to receive this inter sequencer communication and translate it so that it may be handled by the accelerator.

Next in response to block of command information may be received from the sequencer via the exo skeleton block . Such command information may include control and or configuration information along with data to be processed. Accordingly the accelerator may perform operations pursuant to the command information block . These operations may be performed in parallel with the sequencer performing operations on its own. That is the sequencer need not wait for the accelerator to complete its operations before performing other useful work. As described above the operations in the sequencer and the accelerator may be performed in MIMD fashion and in some implementations may correspond to orthogonal operations.

When the accelerator completes its operations the exo skeleton may inform the sequencer accordingly block . Then under control of user level code for example a lightweight user level yield mechanism that initiates a user level event handler the data corresponding to various results obtained in the accelerator may be transferred to the sequencer via the exo skeleton block . While described with this particular flow in the embodiments of and it is to be understood that the scope of the present invention is not so limited.

Using an exo sequencer in accordance with an embodiment of the present invention different levels of abstraction may be implemented. For example in some implementations virtual or complex instruction set computing CISC instructions may be sent to an exo skeleton which then performs an expansion to map such instructions to a native physical instruction or command sequence for an accelerator which may be a sequencer of different ISA from the first sequencer or not a sequencer at all. Accordingly if a basic command set of the accelerator is modified or improved over time legacy support can still be provided via such levels of abstraction. In this way improved performance on the accelerator may be effected even for legacy user level applications. In other implementations direct or reduced instruction set computing RISC instructions may be sent from a sequencer to an exo sequencer that can directly execute the instruction on the accelerator hardware.

Embodiments may be implemented in many different system types. Referring now to shown is a block diagram of a system in accordance with an embodiment of the present invention. As shown in multiprocessor system is a point to point interconnect system and includes a first processor and a second processor coupled via a point to point interconnect . As shown in each of processors and may be multicore processors including first and second processor cores i.e. processor cores and and processor cores and . Each of processors and may further include an exo sequencer i.e. a first exo sequencer and a second exo sequencer . As discussed above exo sequencers and may be heterogeneous resources with respect to the remaining resources of processor cores and . While shown with only a single exo sequencer per processor it is to be understood that the scope of the present invention is not so limited. In other embodiments multiple exo sequencers may be present in a given processor. Furthermore one or more exo sequencers may be associated with each individual core of a processor.

First processor further includes a memory controller hub MCH and point to point P P interfaces and . Similarly second processor includes a MCH and P P interfaces and . As shown in MCH s and couple the processors to respective memories namely a memory and a memory which may be portions of main memory locally attached to the respective processors.

First processor and second processor may be coupled to a chipset via P P interconnects and respectively. As shown in chipset includes P P interfaces and . Furthermore chipset includes an interface to couple chipset with a high performance graphics engine . In one embodiment an Advanced Graphics Port AGP bus may be used to couple graphics engine to chipset . AGP bus may conform to the 2.0 published May 4 1998 by Intel Corporation Santa Clara Calif. Alternately a point to point interconnect may couple these components.

In turn chipset may be coupled to a first bus via an interface . In one embodiment first bus may be a Peripheral Component Interconnect PCI bus as defined by the 2.1 dated June 1995 or a bus such as a PCI Express bus or another third generation I O interconnect bus although the scope of the present invention is not so limited.

As shown in various I O devices may be coupled to first bus along with a bus bridge which couples first bus to a second bus . In one embodiment second bus may be a low pin count LPC bus. Various devices may be coupled to second bus including for example a keyboard mouse communication devices and a data storage unit such as a disk drive or other mass storage device which may include code in one embodiment. Further an audio I O may be coupled to second bus . Note that other architectures are possible. For example instead of the point to point architecture of a system may implement a multi drop bus or another such architecture.

Embodiments may be implemented in code and may be stored on a storage medium having stored thereon instructions which can be used to program a system to perform the instructions. The storage medium may include but is not limited to any type of disk including floppy disks optical disks compact disk read only memories CD ROMs compact disk rewritables CD RWs and magneto optical disks semiconductor devices such as read only memories ROMs random access memories RAMs such as dynamic random access memories DRAMs static random access memories SRAMs erasable programmable read only memories EPROMs flash memories electrically erasable programmable read only memories EEPROMs magnetic or optical cards or any other type of media suitable for storing electronic instructions.

While the present invention has been described with respect to a limited number of embodiments those skilled in the art will appreciate numerous modifications and variations therefrom. It is intended that the appended claims cover all such modifications and variations as fall within the true spirit and scope of this present invention.

