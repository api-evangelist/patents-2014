---

title: Knowledge registry systems and methods
abstract: Systems and methods are disclosed that provide high-level, ontology-based analysis of low-level data stored within an unstructured key/value store. The systems and methods allow an analyst to make sense of massive amounts of data from diverse sources without having any knowledge of the underlying physical data storage. Additional features include feasibility queries to determine if requested data exists in the key/value store before performing an expensive query; automatic query optimization using secondary indexes; and a usage history service to identify performance bottlenecks and fine tune the storage schema.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09367610&OS=09367610&RS=09367610
owner: MASSACHUSETTS INSTITUTE OF TECHNOLOGY
number: 09367610
owner_city: Cambridge
owner_country: US
publication_date: 20140116
---
This invention was made with Government support under Grant No. FA8721 05 C 0002 awarded by the U.S. Air Force. The Government has certain rights in the invention.

As is known in the art many organizations including private and public businesses as well as government agencies have a need to conduct real time ontology based analysis of massive amounts of data collected from diverse sources. For example a cyber security expert may be tasked with making sense of billions of network events generated by millions of unique users. Such data may be logged by many different network proxies web servers Dynamic Host Configuration Protocol DHCP and user authentication systems each having a different log format.

As is also known modern unstructured key value stores i.e. so called Big Data databases are well suited to storing massive amounts from diverse data sources. Key value stores are generally more flexible compared to traditional databases e.g. SQL databases because they generally do not impose a schema or other constraints on the data stored therein. A single table within a key value can store data from multiple data sources that use disparate naming conventions and data formats. Further key value stores generally provide better write read performance and scalability compared with traditional databases.

It has been appreciated herein that although unstructured key value stores are well suited for storing massive amounts of data from various data sources it is difficult to perform high level analysis on data stored therein.

In accordance with the concepts sought to be protected herein a method includes storing one or more ontology entities associated with an ontology and one or more table definitions in a memory. Each table definition comprises a mapping between one or more of the ontology entities and one or more database column identifiers. The method also includes storing one or more data collection records each data collection record associated with one of the stored table definitions each data collection record comprising one or more database row identifiers each row having one or more columns corresponding each row column corresponding to one of the table definition columns.

In accordance with one aspect the method further includes receiving a request the request identifying one or more of the stored ontology entities identifying at least one table definition from the memory associated with the identified ontology entities selecting one or more data collection records from the memory associated with the identified table definition and returning a response the response including the identified table definition mapping and the database row identifiers from the selected data collection records.

With this particular arrangement a method for providing high level ontology based query capabilities for use with an unstructured key value store is provided.

In one aspect the request further comprises at least one operator name and the method further comprises storing in the memory one or more operator records each operator record having an name and being associated with one or more of the stored ontology entities and determining if a matching operator record exists in the memory the matching operator record having a name matching the operator name and being associated with the identified ontology entities wherein the response indicates the existence of the matching operator record.

In some aspects at least one of the database row identifiers comprises a first timestamp and a second timestamp and wherein the request further comprises a time range the method further comprising filtering the selected data collection records based upon the time range and the data collection row identifier timestamps. In one aspect at least one of the table definitions includes one or more secondary indexes wherein the response further includes any secondary indexes included within the identified table definition.

In accordance with one aspect the method further comprise one or more of the following storing in the memory a usage history record associated with the matching operator record the usage history record comprising a timestamp generally indicating the time required to perform processing associated with the identified operator identifying from among the stored operator records at least two operator records associated with the identified ontology entities and selecting one of the two identified operator records based upon associated usage history records.

In some aspects the method further comprises retrieving one or more data records from a database wherein the location of the data records in the database is based upon the table definition mapping and the database row identifiers included within the response. In one aspect the database comprises a key value store. In some aspects the database comprises event data and the request further comprises an event type.

Also in accordance with the concepts sought to be protected herein a system includes a memory a content model update service and a data store state update service to perform the method.

Before describing exemplary embodiments of the systems and methods used to teach the broad concepts sought to be protected herein some introductory concepts and terminology used in conjunction with the exemplary embodiments are explained. As used herein the terms data record and record are used to describe a set of attributes each attribute having a value and a corresponding identifier sometimes referred to as the attribute name . The terms data record collection and data collection are used to describe a group of one or more related data records. As used herein the term soft deleted refers to a data record stored within a system that is hidden from the system users but is not physically deleted from the system.

The term analyst is used herein to refer to any person or system capable of using the analytics systems and methods described herein to obtain high level analytics information including but not limited to humans and intelligent machines e.g. machines having neural network capability . The term engineer is used herein to refer to any person or system capable of configuring maintaining or operating the systems described herein.

The term dimension is used to describe a normalized opaque data type for use within the present systems and methods. The term dimension set is used herein to describe a group of related dimensions. In one respect dimensions and dimension sets are entities included within an ontology i.e. ontology entities . For example in the cyber security domain an ontology may include the dimensions IPAddress Client IPAddress DomainName Server DomainName and Time each of which is included within the dimension set WebRequest .

Reference will sometimes be made herein to the Knowledge Query Language KQL and KQL queries. KQL is an ontology based domain specific structured query language designed for use in the present systems and methods.

In general a KQL query includes a dimension set DIMENSION SET and one or more operations OPERATIONS each operation including a query operator OPERATOR an input section INPUT and an output section OUTPUT . The query operators are identifiers e.g. strings which correspond to opaque operations implemented by the systems described herein. Although the present systems are not limited to any specific KQL query operators four operators are discussed herein for explanatory purposes including SELECT DISTINCT COUNT and DIFF each of which is described further below in conjunction with TABLE 2.

The input and output sections can include a dimension identifier DIMENSION and a corresponding constraint value VALUE . The constraint value may include but is not limited to a scalar e.g. google.com a range e.g. 201208110300 201208120300 and or commonly used relational operators e.g. . For an input section the dimension identifier specifies the type of data which the corresponding operator expects to receive as input. For an output section the dimension identifier specifies the type of data that should be output by the corresponding operation. As a special case the dimension identifier ALL DIMENSIONS may be used within the output section to indicate all available dimensions should be included within the corresponding output result data. In one embodiment the specified input and output dimension identifiers must be included within the specified identified dimension set.

An exemplary KQL query for use in cyber security applications is shown in TABLE 1 and will now be discussed. This query which is shown encoded as JSON may be issued by an analyst to obtain a distinct collection of client IP addresses that have made web requests to a web server having the domain google.com . It should be appreciated that KQL queries can be encoding using other suitable encoding techniques including XML.

The query in TABLE 1 includes two operators having respective operator names DISTINCT and SELECT . The operators are to be executed sequentially in reverse order. The first operator SELECT selects all available web request data in the given time period where the corresponding web requested either originated from or was sent to a web server with a domain matching google.com . The second operator DISTICT computes the set of distinct IP addresses among the data returned by the first operator.

Various exemplary embodiments are discussed hereinbelow making use of KQL. It is envisioned however that the broad concepts described herein are equally applicable to other query languages and that the concepts described herein are not limited to any particular query language.

Each of the system components may include hardware and or software components used to implement the respective functionality described hereinbelow. The components may be coupled together as shown in . Each connection may be provided as a hardware based connection a software based connection or a connection provided from a combination of both hardware and software. Thus it should be appreciated by those skilled in the art that the system could be implemented entirely within a single computing device or distributed among a plurality of networked computing devices the computing devices being either virtual machines or hardware based devices. It should further be appreciated that the components illustrated in may also be coupled in configurations other than shown in . One of ordinary skill in the art after the reading the disclosure provided herein will appreciate that a wide variety of different configurations may be used.

The data ingest platform also referred to herein as the ingest platform may be coupled to the data sources the key value store the knowledge registry and the query executor as shown in exemplary embodiment of . In other embodiments the query executor and or query analyzer may be included within the ingest platform . A data ingest engineer can manually operate the ingest platform and or configure the platform for generally autonomous operation.

In operation the ingest platform receives data from the plurality of data sources groups the data into a collection of data records stores the data records within the key value store and provides information about the collection to the knowledge registry . The key value store can be any unstructured storage facility capable of efficiently storing and retrieving massive amounts of data. Suitable off the shelf key value stores include but are not limited to Apache Accumulo Apache HBase Apache Cassandra other high performance data storage systems such as Google Inc. s BigTable database.

The ingest platform includes a hardware or software component referred to herein as a database driver configured to read and write to from the key value store . In one exemplary embodiment the database driver is encapsulated in ingest platform using a generic database interface and or plugin system thereby making it easy to change the key value store implementation and allow multiple key value stores to be used simultaneously within the ingest platform.

As is known in the art several unstructured key value stores e.g. Apache Cassandra utilize a so called triple store architecture wherein data is organized by tables rows and columns . A table includes an arbitrary number of rows indexed by a row key . Row keys are arbitrary fixed length values chosen by a user. Several triple store databases including Apache Accumulo as one example store rows in lexicographical order by key and therefore allow range queries to efficiently retrieve multiple rows. A row includes an arbitrary number of columns indexed by a column name . Typically each column stores a single data value. Thus each data value is located by a 3 tuple a table a row key and a column name. It will be appreciated that a triple store database is particularly well suited for storing and retrieving collections data records.

Thus in some embodiments the key value store utilizes a triple store architecture with range query capabilities and the data ingest platform stores each ingested data record in a separate row. Further the ingest platform generates row keys such that all rows within a given data collection can be retrieved using a single range query. For time oriented data e.g. event data the data ingest platform may group data records by time and include corresponding lexicographically encoded timestamps.

In some embodiments the ingest platform includes one or more syntactic analysis processors or modules which execute one or more parsing techniques parsers to parse one or more different input data formats such as comma separated CSV or tab delimited formats widely used for log data. To facilitate the use of many diverse data sources the ingest platform may include a plug in system wherein several different parsers can be supported simultaneously and new parsers can easily be added to the platform. The data ingest engineer can configure an appropriate parser to be used for each of the data sources .

As discuss above the ingest platform may group the parsed data records into collections. In some embodiments each collection generally has the same number of records. In one exemplary embodiment this fixed size may be configured by the data ingest engineer. In other embodiments wherein the received data includes log data the number of records in each collection corresponds to the number of lines in a log file and thus collection sizes vary. In yet other embodiments the ingest platform groups time oriented data records based on a specified time period such as every minute every 10 minutes or every hour. The data ingest platform may allow these time periods referred to as a buffer period hereinbelow to be configured for each data source and the ingest platform can use the buffer period configurations to perform automatic period data ingestion. In one exemplary embodiment the data ingest engineer may configure the time periods via the data ingest platform .

Those skilled in the art will appreciate that the size of a data record collection presents certain tradeoffs to the system performance. For example smaller collection sizes can be processed more quickly thus providing more real time insight to the analyst . In embodiments the ingest platform includes a streaming mode wherein data is ingested into the key value store as soon as it becomes available and thus collections may contain as few as one data record. On the other hand larger collections processed less frequently allow for certain processing and space wise efficiencies in the system .

Various filtering processing capabilities may be added to the data ingest platform . For example to reduce the volume of data stored in the key value store the ingest platform may filter or aggregate duplicate or similar data records. As another example the ingest platform may normalize data before storing in the key value store such as converting IP address from a non standard format to the standard dotted quad form.

After storing a collection of data records into the key value store the ingest platform provides information about the newly ingested data collection to the knowledge registry . Thereby the knowledge registry is notified that new data is available and in turn the new data is accessible the analyst . In one exemplary embodiment the information is provided as metadata the metadata may include substantially the same attributes as a data collection record used within the knowledge registry and discussed below in conjunction with .

The knowledge registry may be coupled to the ingest platform query executor and query analyzer as shown. Further the knowledge registry may receive input from and provide output to a knowledge engineer . To reduce data transfer times the knowledge registry may be implemented as part of the ingest platform . The structure and operation of the knowledge registry is discussed in detail below in conjunction with .

The analytics platform may be coupled to the query executor and the query analyzer . The analytics platform may include a plurality of applications e.g. information visualization applications some of which include a user interface UI for use by the analyst . The query analyzer may be coupled to the knowledge registry the query executor and the analytics platform as shown. In embodiments the query analyzer may be part of the analytics platform .

In operation the query analyzer generally receives KQL queries from the analytics platform utilizes the knowledge registry s data store state access service to translate query ontology entities into key value store identifiers e.g. row keys column names and secondary indexes and issues appropriate communications calls to the query executor .

Another function of the query analyzer is to improve and ideally optimize query execution times and required processing power compared to execution times and required processing power without such improvements optimizations. In one embodiment the knowledge registry tracks which columns have secondary indexes and the query analyzer automatically applies these secondary indexes when available. In another embodiment the query analyzer may consult the knowledge registry s usage history service to determine which queries have historically resulted in relatively slow execution and thus should be avoided. As another optimization the query analyzer heuristically reduces and ideally minimizes query execution time by selecting a query with a relatively few and ideally the fewest number of operators. As yet another optimization the query analyzer can determine if any data is available for a given time range e.g. the value specified with a Time dimension if no data is available the query analyzer can return an empty null response to the user and not waste system resources e.g. processing power invoking the query executor . Such feasibility or executability queries may be performed implicitly as a form of optimization by the query analyzer or issued explicitly by an analyst .

In the exemplary embodiment of the query executor is coupled to the data ingest platform knowledge registry query analyzer and analytics platform . In some embodiments the query executor may be part of the data ingest platform . In alternate embodiments the query executor is directly coupled to the key value store and therefore may include one or more components e.g. hardware software or a combination of hardware and software needed to communicate with the key value store . For example the query executor may include one or more of the database drivers discussed above in conjunction with the ingest platform .

The query executor performs two primary functions. First the query executor is the only system component which is directly coupled to the key value store to execute database operation thereon although in some embodiments the data ingest platform may write data collections into the data store . Thus it is possible to add remove and change the key value store implementation without requiring any change to the knowledge registry the query analyzer or the analytics platform . Second the query executor provides a query operator application programming interface API for use by the query analyzer . In one embodiment the operator based API includes a separate call for each query operator such as the operators shown below in TABLE 2. This separation of concerns enables the query analyzer to focus on analyzing and optimizing user queries while the query executor can focus on providing improved and ideally optimized implementations of the various query operators based upon the underlying database storage structure.

If a particular operator is implemented within the key value store the query executor may delegate some all of the work thereto. The other operators can be implemented directly within the query executor i.e. the query executor can post process data retrieved from the key value store . For example if the key value store includes a native count function the query executor may implement the COUNT operator API call merely by delegating to the key value store. Of course the SELECT operator API call will be delegated to an appropriate key value store query function. However if the key value store does not include a native unique distinct function the query executor must include a suitable processor based implementation of that function. In some embodiments one or more of the operators is implemented within the data ingest platform and the query executor delegates corresponding API calls thereto.

After executing the requested operation the query executor returns a resulting data collection the results to the query analyzer or directly to the analytics platform . Before doing so the query executor may perform a reverse mapping whereby the results are converted from native key value store column names and data types to the corresponding query dimension names and data types. As discussed below in conjunction with the knowledge registry may associate a data type with each ontology dimension and or field and therefore the query executor can retrieve this information via the data store state access service to convert from native data types to normalized ontology based data types.

In a particular embodiment executing a query may require retrieving data from multiple key value stores. Here the CIM may include information regarding how to access one or more key value stores referred to hereinbelow as data store access information such as an IP address a network port and a database name for each key value store. Further the CIM may associate each data collection ingested by the data ingest platform with one more key value store. During query processing the query executor can use the data store access information to retrieve data from the respective stores and combine join the results data as needed using any suitable techniques known in the art including any join techniques common used in relational databases.

It should be appreciated that various analytics system components of the can be combined and or further partitioned and therefore the system shown in is merely one exemplary embodiment.

Referring now to an exemplary knowledge registry may be the same as or similar to the knowledge registry in . The knowledge registry includes a Content Information Model CIM update service a data store state update service a data store state access service a usage history service a CIM and a registry data store . A plurality of users and or applications may access the various services via a network which may be a local area network LAN wide area network WAN such as the Internet or any other suitable type of computer network. The applications may include a data ingest platform a query executor a query analyzer and or an analytics platform . The users may include an analyst a knowledge engineer and or a data ingest engineer any of whom may interact with the knowledge registry directly via the network or indirectly via one of the applications .

Those skilled in the art will appreciate that the knowledge registry can be implemented and deployed using a variety of software hardware and network architectures. In one embodiment the knowledge registry is a monolithic software application that implements the several services the CIM and the registry data store . In another embodiment the registry data store is a standalone database management system. In yet another embodiment each of the services is a separate software application coupled to the CIM and the registry data store . Further multiple instances of the knowledge registry may execute concurrently on one or more physical virtual computing environments. In one embodiment the services include Web Service APIs responsive to one or more request response content types such as JSON and XML. The services may include access controls user authentication and or a data encryption.

Although the operation of the knowledge registry services will be discussed further below in conjunction with a brief overview is now given. The content model update service is generally used by the knowledge engineer to update the ontology information stored within the registry data store . The data store state update service is used by the data ingest platform to update data collection metadata stored within the registry data store . The data store state access service is used by the query analyzer to determine the location and availability of data requested by the analyst . The data store state access service may also be used by the query executor to perform a reverse mapping as discussed further below. The usage history service is used by the query analyzer to retrieve historical query execution timing information which is also stored within the registry data store . The usage history is also used by the query analyzer and or query executor to store new query execution timing information.

The CIM is a data model which describes a mapping between one or more ontologies and data stored in key value store . The CIM comprises executable code configuration data and or user data which may be included within the various services and or stored within the registry data store . For example the CIM includes a schema such as shown in used within the registry data store and software modules which encapsulates the various schema entities to provide a record based API to the knowledge registry services . As another example the ontology portion of the CIM may be described using an ontology language such as the Web Ontology Language OWL stored within the registry data store . A detailed description of an exemplary CIM is presented below in conjunction with .

The registry data store stores various information used by the services . The store may include or be coupled to a non volatile memory such as a solid state disk SSD or a magnetic hard disk HD . In one embodiment the registry data store includes a relational database management system RDBMS such as MySQL. In another embodiment the registry data store is an unstructured data store and therefore may be included with the key value store . The registry data store can be widely distributed or can be at a single location in a single database.

The ontology portion describes one or more ontologies used within the knowledge registry . Thus the ontology portion determines how knowledge is represented within the knowledge registry . The ontology portion can be domain specific that is the data model entities therein may vary based upon the type of data that is stored in the key value store and the corresponding ontologies. In particular entities that describe domain specific knowledge concepts may be added to the CIM and therefore it should be appreciated that the exemplary ontology portion shown in is merely a generalized baseline data model which can be readily extended.

The exemplary ontology portion includes one or more dimensions one or more dimension sets and one or more operators . A dimension includes a name and a data type . The name is an arbitrary ontological identifier provided by the knowledge engineer such as IPAddress or Time . The data type indicates a normalized data type and format in which corresponding result data is encoded. The data type may be a C style format string an enumerated value or any other suitable identifier. As discussed further below the dimension data types and field data type may be collectively used by the query executor to map native data types formats to normalized ontology data types formats.

In some embodiments a dimension may be comprised of one or more other dimensions i.e. dimensions may bay be associated with other dimensions . For example in the cyber security domain the knowledge engineer may generate a URL dimension referring to Uniform Resource Locators that is comprised of an IPAddress dimension and a Port dimension. Such decomposition capability allows the knowledge engineer to map a complex ontology entity to multiple low level columns in the key value store.

A dimension set represents a grouping of related ontology entities and thus includes one or more dimensions . Dimensions are generally unordered within a dimension set in contrast fields are generally ordered within a table definition as discussed below. Dimension sets include a name e.g. WebRequest which may be provided by the knowledge engineer . Dimension names and or dimension set names may be unique within the knowledge registry allowing them to be used as primary identifiers. In some embodiments a dimension set is associated with one or more operators such that the knowledge registry services can determine which operators are available for a given dimension set. The specific dimensions and dimension sets available within the knowledge registry are configured by the knowledge engineer via the content model update service .

It should be known that the meaning of the various dimension sets relates to the specific ontology being modeled within the CIM . For example if event data is being modeled i.e. the ontology is an event based ontology each configured dimension set may represent a different event type. Thus in such a domain specific embodiment a dimension set may be referred to as an event type or the like.

An operator includes a name an input signature and an output signature the combination of which may be unique within the knowledge registry . Example operator names are shown above in TABLE 2. An operator represents either an opaque operation to retrieve a data collection e.g. SELECT or an opaque transformation on a data collection. Accordingly the input signature and the output signature specify the ontology entities expected to appear in the input collections and output collections respectively for retrieval operations the input collection corresponds to the data retrieved from the key value store . It should be appreciated that the signatures can be readily constructed based on the INPUT and OUTPUT sections of a KQL query. In some embodiments the ontology portion of the CIM may be provided by the knowledge engineer via the content model update service using OWL.

The table definitions portion represents a mapping between an ontology used within knowledge registry and one or more table structures within the key value store . The exemplary table definitions portion shown in includes one or more table definitions one or more fields and one or more data sources . A data source represents one or more of the data sources from which the key value store is populated. A data source includes a name a create timestamp that indicates the date time when the data source was added to the knowledge registry and a delete timestamp that indicates the date time the data source was soft deleted from the knowledge registry. The data source names may be unique with the knowledge registry . A data source may include additional attributes used by the data ingest platform to perform automatic period data ingestion such as a buffer period and an expected collection delay . A table definition includes a unique name a create timestamp indicating when the definition was added to the knowledge registry and a delete timestamp indicating when the definition was soft deleted i.e. removed from the knowledge registry. Data sources may be generated updated and soft deleted by the data ingest engineer via the data ingest platform which uses the knowledge registry s data store state update service . The data ingest engineer provides a unique name and other required attributes.

In some embodiments a data source further includes data store access information . In one embodiment the data store access information comprises an IP address a network port and a database name and is used to configure a database driver within the query executor and or data ingest platform .

A table definition includes one or more fields each of which includes a column name that corresponds to a column name within the key value store . A table definition may be associated with one or more dimension sets such that the knowledge registry services can determine which table definitions implement a given dimension set. In addition one or more of the fields may be associated with an ontology entity i.e. a dimension or a dimension set such that given a list of ontology entities the services can determine the names of columns within the key value store that contain relevant data. As discussed above a dimension may comprise other dimensions and thus may be associated with a plurality of fields in other words a discrete ontology entity may span multiple key value store columns.

In some embodiments a field further includes a native data type which indicates the type and or format of data stored within the corresponding key value store columns. The native data type can be used by the query executor to reverse map a data collection retrieved from the key value store from a native type format to a normalized ontological data type format associated with the ontology.

A field may further include an order value which is used by the data ingest platform to interpret ordered data from a given data source. In some embodiments a data source may also be associated with a table definition and therefore using the field ordering may periodically automatically receive data from the data source and populate the key value store therewith.

In a particular embodiment a field further includes secondary index information . In one embodiment the secondary index information is a simple flag i.e. boolean value that indicates whether the key value store includes a secondary index on the corresponding column. In other embodiments the secondary index information may be a string which indicates the name of the index and the information may be used by the query executor to construct an appropriate key value store query. In most embodiments the query analyzer and or query executor uses the secondary index information to generate queries which take less time and or power to execute.

It should now be appreciated that in one aspect the table definitions portion of the CIM in association with the ontology portion of the CIM defines a mapping between a knowledge based ontology and an unstructured data store. Moreover a table definition and associated fields define how data is stored within the key value store thus imparting a meta structure onto unstructured data stores.

Table definitions fields and their associations with the ontology portion may be assigned by a knowledge engineer via the data ingest platform which uses one or more of the knowledge registry service and stored in the registry data store .

The data store state portion of the CIM represents the contents of the key value store that is it tracks which data presently exists in the key value store and can be used to answer queries from an analyst. The data store state portion may include one or more data collection records each of which represents a collection of data records ingested from a data source into the key value store . As discussed above in some embodiments an ingested data collection is stored as a plurality of rows within the key value store . A data collection record may include a serial number which uniquely identifies the collection with the knowledge registry an ingestion timestamp that indicates the time the data was ingested into the key value store the number of records in the collection and the size of each record . A data collection also includes one or more attributes to locate the corresponding data records i.e. rows within the key value store for example a begin timestamp and an end timestamp which can be used by the data ingest platform to generate the start end keys for a range of rows. A data collection record is associated with a table definition thereby allowing the knowledge registry services to locate rows within the key value store that contain data corresponding to a given ontology entities. For reference purposes a data collection record may also be associated with a data source .

The data store state portion may also include one or more usage history records each of which corresponds to a query executed by an analyst . In one embodiment a usage history record tracks operations performed by the query executor and thus may be associated with an operator as shown. A usage history record may include a query identifier a start timestamp indicating the time the query execution started an end timestamp indicating the time the query execution completed. The query executor may generate usage history records via the usage history service when a operation is completed. As discussed above a KQL query may result in multiple operations and thus to track the overall execution time of a KQL query a common query identifier can be used across several usage history records .

It should now be appreciated that the knowledge registry in particular the services and the CIM are entirely isolated from the key value store and therefore the database structure used within the key value store can be changed independently of the data models used within the knowledge registry and vice versa. More specifically dimensions dimension sets and operators are implementation independent such that the data ingest platform has the freedom to store data in the key value store using any structure it chooses so long as the mappings are stored in the knowledge registry .

Referring now to an exemplary method for use in a knowledge registry such as knowledge registry is shown. The method comprises three sub methods updating the content model updating data store state and processing a query .

It should be appreciated that show a flowchart corresponding to the below contemplated technique which may be implemented in a computer system . Rectangular elements typified by element herein denoted processing blocks represent computer software instructions or groups of instructions. Rectangular elements having double vertical lines typified by element herein denoted sub methods represent a logical and or physical grouping of processing blocks. Diamond shaped elements typified by element herein denoted decision blocks represent computer software instructions or groups of instructions which affect the execution of the computer software instructions represented by the processing blocks. Alternatively the processing blocks sub methods and decision blocks represent steps performed by functionally equivalent circuits such as a digital signal processor circuit or an application specific integrated circuit ASIC . The flowchart does not depict the syntax of any particular programming language but rather illustrates the functional information one of ordinary skill in the art requires to fabricate circuits or to generate computer software to perform the processing required of the particular apparatus. It should be noted that many routine program elements such as initialization of loops and variables and the use of temporary variables are not shown.

It will be appreciated by those of ordinary skill in the art that unless otherwise indicated herein the particular sequence of blocks described is illustrative only and can be varied without departing from the spirit of the systems and methods sought to be protected herein. Thus unless otherwise stated the blocks described below are unordered meaning that when possible the steps can be performed in any convenient or desirable order. In particular the sub methods can be executed in any order and one or more sub method may be executed in parallel an ordered serial method is shown in merely for convenience of explanation.

In general the exemplary sub method generates and or updates certain portions of the CIM within the knowledge registry . More specifically the sub method generates dimension dimension set and or operator records within the registry data store and or updates existing such records. The sub method may be implemented within the content model update service used by a knowledge engineer .

The sub method begins at block where one or more ontology entities i.e. dimensions or dimension sets are generated updated. Next at block one or more operators are generated updated. Finally at block the generated updated ontology entities are associated with one or more operators and similarly the generated updated operators are associated with one or more ontology entities the nature of these associations is discussed further above in conjunction with .

The exemplary sub method generates updates table definition field data source and data collection records within CIM . The sub method may be implemented within the data store state update service used by a data ingest engineer .

The sub method begins at block where one or more table definitions records are generated updated. If a column is added to the key value store block includes generating one or more associated fields . If a column is removed from the key value store block includes deleting disassociating one or more fields .

Next at block one or more table definitions typically the table definitions generated updated in processing block are mapped to ontology entities as follows. First each table definition is associated to a dimension set indicating that the associated data collections and corresponding rows comprise data related to the dimension set ontology. Second one or more of the fields within the table definition is associated to a dimension indicating that the corresponding column name stores data having that dimension.

At processing block one or more data collection record is generated within the registry data store indicating that new data has been ingested into the key value store . In the final block of exemplary sub method each of the newly generated data collection records is associated with a table definition .

It should now be appreciated that processing blocks and generate a mapping between a table definition and an ontology and the processing blocks and associate the table definition to one or more identified rows within the key value store . Typically the blocks and will be repeated more frequently compared to the blocks and .

The exemplary sub method processes an ontology based query such as a KQL query. The sub method may be implemented within the data store state access service used by an analyst via an analytics platform and or a query analyzer . The sub method begins at block where a query is received the query having an operator name and identifying one or more ontology entities. In an embodiment the query ontology entities includes an operator name a dimension set identifier one or more input dimension identifiers and one or more output dimension identifiers. Here the query may correspond to a single operator from a KQL query. Using the exemplary KQL query from TABLE 1 the data store state access service may receive an ontology based query having the dimension set identifier WebRequest the operator name SELECT input dimensions Server DomainName and Time and output dimension ALL DIMENSIONS . The query analyzer may receive a full KQL query from an analyst and iterate over the operations therein invoking the sub method once for each such operation.

Next at block at least one table definition is identified based upon the received query. In one embodiment where the query includes a dimension set identifier the data store state access service first retrieves a dimension set based upon the query dimension set identifier and then finds a table definition associated with the identified dimensions set . As discussed above the table definition and associated fields defines a mapping between column names used in the key value store and one or more ontology entities.

Next at block one or more data collection records are selected. In one embodiment all data collection records associated with the identified table definition are selected.

Next at block the selected data collection records may be filtered. In some embodiments the key value store includes event data and one or more of the data collection records includes a range of event times. Herein the selected data collection records may be filtered based on a time range included with the query e.g. the Time value constraint shown in TABLE 1 data collection records that have a begin timestamp or an end timestamp outside the time range are excluded. For example referring back to the query in TABLE 1 only events which occurred on or after 2012 08 11 03 00 00 UTC and on or before 2012 08 12 03 00 00 UTC are selected in TABLE 1 the time zone UTC is implied .

Next decision block may be performed. If all of the data collection records are excluded by the filtering a response is sent at block indicating that no data is available to satisfy the query. Such a feasibility check is provided for efficiency allowing the system to avoid unnecessary expensive database queries. If any data collection records remain the sub method continues as follows.

In embodiments where the received query includes an operator name decision block may be performed next. Herein it is determined whether an operator exists having a name matching the query operator name. If no such operator exists a response is sent at block indicating that the requested operation is not available.

Otherwise at block a response is sent which includes the identified table definition column mapping and row identifies which are based upon the selected data collection records. In one embodiment the row identifiers comprise one or more time ranges i.e. a begin timestamp and an end timestamp corresponding to the time ranges in the selected data collection records overlapping and contiguous time ranges may be combined to reduce the size of the response.

Finally at block a usage history record may be stored and associated with the operator matched in block .

Processing may be implemented in hardware software or a combination of the two. Processing may be implemented in computer programs executed on programmable computers machines that each includes a processor a storage medium or other article of manufacture that is readable by the processor including volatile and non volatile memory and or storage elements at least one input device and one or more output devices. Program code may be applied to data entered using an input device to perform processing and to generate output information.

The system can perform processing at least in part via a computer program product e.g. in a machine readable storage device for execution by or to control the operation of data processing apparatus e.g. a programmable processor a computer or multiple computers . Each such program may be implemented in a high level procedural or object oriented programming language to communicate with a computer system. However the programs may be implemented in assembly or machine language. The language may be a compiled or an interpreted language and it may be deployed in any form including as a stand alone program or as a module component subroutine or other unit suitable for use in a computing environment. A computer program may be deployed to be executed on one computer or on multiple computers at one site or distributed across multiple sites and interconnected by a communication network. A computer program may be stored on a storage medium or device e.g. CD ROM hard disk or magnetic diskette that is readable by a general or special purpose programmable computer for configuring and operating the computer when the storage medium or device is read by the computer. Processing may also be implemented as a machine readable storage medium configured with a computer program where upon execution instructions in the computer program cause the computer to operate.

Processing may be performed by one or more programmable processors executing one or more computer programs to perform the functions of the system. All or part of the system may be implemented as special purpose logic circuitry e.g. an FPGA field programmable gate array and or an ASIC application specific integrated circuit .

Having described exemplary embodiments which serve to illustrate various concepts structures and techniques which are the subject of this patent it will now become apparent to those of ordinary skill in the art that other embodiments incorporating these concepts structures and techniques may be used. Accordingly it is submitted that that scope of the patent should not be limited to the described embodiments but rather should be limited only by the spirit and scope of the following claims.

