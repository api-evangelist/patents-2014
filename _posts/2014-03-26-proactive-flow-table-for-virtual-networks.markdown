---

title: Proactive flow table for virtual networks
abstract: In general, techniques are described for enhancing operations of virtual networks. In some examples, a network system includes a server that executes a virtual router configured to receive, from a switch fabric, a tunnel packet for a virtual network of the virtual networks, wherein the tunnel packet comprises an outer header and an inner packet that defines a packet flow. The virtual router is also configured to determine, based at least on the outer header, that the packet is associated with a virtual network of the one or more virtual networks, determine a packet flow defined by the inner packet does not match any flow table entry of a flow table that identifies active flows only for virtual network and, in response, add a flow table entry for a reverse packet flow of the packet flow to the flow table.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09473394&OS=09473394&RS=09473394
owner: Juniper Networks, Inc.
number: 09473394
owner_city: Sunnyvale
owner_country: US
publication_date: 20140326
---
This application claims the benefit of U.S. Provisional Application No. 61 926 079 filed Jan. 10 2014 the entire content of which is incorporated herein by reference.

Techniques of this disclosure relate generally to computer networks and more particularly to virtual networks.

In a typical cloud data center environment there is a large collection of interconnected servers that provide computing and or storage capacity to run various applications. For example a data center may comprise a facility that hosts applications and services for subscribers i.e. customers of data center. The data center may for example host all of the infrastructure equipment such as networking and storage systems redundant power supplies and environmental controls. In a typical data center clusters of storage systems and application servers are interconnected via high speed switch fabric provided by one or more tiers of physical network switches and routers. More sophisticated data centers provide infrastructure spread throughout the world with subscriber support equipment located in various physical hosting facilities.

In general techniques are described for enhancing operations of virtual networks. For example a virtual network controller is described that configures and manages an overlay network within a physical network formed by plurality of switches. A plurality of servers is interconnected by the switch fabric and each of the servers provides an operating environment executing one or more virtual machines in communication via the overlay networks. A set of virtual routers operating within the servers and or other devices of the physical network extends the overlay network as a virtual network to the operating environment of the virtual machines. The controller may instruct the servers and the virtual routers to perform various operations such as forwarding traffic through the overlay networks re routing traffic in the virtual networks due to network events replicating traffic for multicasting networking services including security NAT mirroring and load balancing providing multi tenant services to support multiple virtual networks monitoring and logging traffic characteristics within the virtual networks and other operations.

The techniques described herein may be utilized to enhance for example operation of the virtual routers or other devices that provide virtual networks. In general a virtual router for a virtual network executes multiple routing instances for corresponding virtual networks. Each virtual network interconnects multiple virtual routers collectively implementing the virtual network. Packets received by the virtual router from the underlying physical network fabric may include an outer header to allow the physical network fabric to tunnel the payload or inner packet to a physical network address for a network interface of the server that executes the virtual router. The outer header may include not only the physical network address of the network interface of the server but also a virtual network identifier such as a VxLAN tag or Multiprotocol Label Switching MPLS label that identifies one of the virtual networks as well as the corresponding routing instance executed by the virtual router. An inner packet includes an inner header having a destination network address that conform to the virtual network addressing space for the virtual network identified by the virtual network identifier.

In one example of enhancing the operation of the virtual routers a virtual router may as described herein buffer and aggregate multiple tunneled packets received from the underlying physical network fabric prior to delivery to the appropriate routing instance for the packets. In some examples the virtual router aggregates multiple packets according to matching criteria that includes the virtual network identifier of the outer header as well as one or more fields of the inner header. The virtual router may in some cases extend a kernel based offload engine that seamlessly and automatically aggregates multiple incoming packets from a single packet flow. For example the virtual router may extend a Generic Receive Offload GRO or Large Receive Offload LRO routines available by the server kernel and that is specialized for processing layer two L2 packets but the virtual router may leverage the GRO routine in a way so as to utilize the routine to aggregate and manipulate multiple tunneled packets as if they were L2 packets. In some examples the virtual router provides multiple tunneled packets to GRO for aggregation by in part setting the respective virtual network identifiers and invoking the GRO routine as if the virtual network identifiers are a L2 destination address for the inner packets of the tunneled packets. In this way the GRO routine considers each packet received from the virtual router for aggregation purposes as a non tunneled layer 2 packet that includes at least a L2 destination address e.g. a destination MAC address set to the virtual network identifier for a received tunneled packet and a layer 3 network packet that corresponds to the inner packet for the received tunneled packet. By matching according to at least L2 data link destination address and one or more header fields of the layer 3 packet the GRO routine may aggregate multiple by merging such packets into a single aggregate packet for delivery to the appropriate routing instance. In this way the aggregation techniques may increase the virtual router bandwidth by reducing the number of packet headers for processing and concomitantly reducing the amount of network stack traversal needed to process multiple received packets.

In another example of enhancing the operation of the virtual routers techniques are described for steering received packets among multiple processor cores to facilitate packet processing load balancing among the cores. For instance a particular network interface card of a server that executes a virtual router may be associated with a designated processor core to which the network interface card directs all received packets. The designated processor core rather than processing each of the received packets offloads flows to one or more other processor cores for processing to take advantage of available work cycles of the other processor cores. In some cases the designated processor core applies a hash function to an inner header of each received packet to determine a corresponding hash value that maps to one of the processor cores of the server and directs the received packet to the mapped processor core for processing. In some cases the processor cores of the server progressively and separately apply a hash function to both the outer and inner headers of received packets. For instance for a received packet the designated processor core may apply the hash function to the outer header of the received packet to identify a processor core of the server with which to apply a hash function to the inner header of the received packet. The identified processor core may then partially process the received packet by first applying a hash function to the inner header of the received packet to identify a processor core with which to process the received packet. The identified processor core with which to process the received packet for the server may then process the received packet. In this way various packet flows received by the server may distribute incoming packet flows among multiple processing cores of the server to use more than the processing core designated for the network interface card. Receive packet steering may be enabled in this way on a per interface basis.

In another example of enhancing the operation of the virtual routers techniques are described for proactively adding by the virtual router flow table entries to identify reverse flows of flows processed by a routing instance of the virtual router. Each flow traversing a routing instance of the virtual router in either the inbound received from the underlying physical network or outbound direction for transmission to the underlying physical network may be identified according to an n tuple of the flow such as a combination of source and destination network address or the conventional 5 tuple including the source and destination network address source and destination port and protocol.

The virtual router upon receiving a packet for a packet flow that does not include a flow table entry in a flow table that would otherwise enable the virtual router to apply fast path processing to the packet instead applies slow path processing to determine a forwarding policy for the packet flow and add an entry to the forwarding table to associate the flow with the forwarding policy for subsequent fast path operations for subsequent packets for the flow and received by the virtual router. In addition the virtual router proactively adds an entry to the forwarding table to associate with reverse packet flow for the packet flow with a forwarding policy for the reverse packet flow despite not yet receiving a packet for the reverse packet flow for the packet flow. A reverse packet flow for a packet flow may be identified using the same header fields as that used to identify the packet flow. However the reverse packet flow includes mirrored values for symmetric fields of the packet header. For example a packet flow identified by the combination of source network address A and destination network address A has a corresponding reverse packet flow identified by the combination of source network address A and destination network address A where the values of A and A are mirrored for the symmetric source and destination network address fields. In some cases the virtual router first determines a forwarding policy for the reverse packet flow according to slow path processing and associates the reverse packet flow with the forwarding policy for the reverse packet flow. The proactive flow table techniques described above may permit the virtual router to avoid initial slow path processing for an initial packet of a flow that matches the proactively added flow table entry for a reverse flow thereby reducing latency for the initial packet and potentially improving the overall bandwidth of the server

In one example a method includes receiving by a virtual router of a computing device for one or more virtual networks a tunnel packet comprising an outer header and an inner packet that defines a packet flow. The method also includes determining based at least on the outer header that the packet is associated with a virtual network of the one or more virtual networks. The method also includes determining by the virtual router a packet flow defined by the inner packet does not match any flow table entry of a flow table that identifies active flows only for the virtual network. The method also includes in response to determining the inner packet does not match any flow table entry of the flow table for the virtual network adding a first flow table entry for the packet flow to the flow table and adding a second flow table entry for a reverse packet flow of the packet flow to the flow table.

In another example a network system includes a switch fabric comprising a plurality of switches interconnected to form a physical network. The network system also includes a virtual network controller configured to configure and manage virtual networks within the physical network. The network system also includes a plurality of servers interconnected by the switch fabric wherein each of the servers comprises an operating environment configured to execute one or more virtual machines in communication via the virtual networks and wherein the servers comprise a set of virtual routers configured to extend the virtual networks to the virtual machines. A virtual router of the set of virtual routers is configured to receive from the switch fabric a tunnel packet for a virtual network of the virtual networks wherein the tunnel packet comprises an outer header and an inner packet that defines a packet flow determine based at least on the outer header that the packet is associated with the virtual network determine a packet flow defined by the inner packet does not match any flow table entry of a flow table that identifies active flows only for that virtual network and in response to determining the inner packet does not match any flow table entry of the flow table add a first flow table entry for the packet flow to the flow table and add a second flow table entry for a reverse packet flow of the packet flow to the flow table.

In another example a non transitory computer readable medium comprises instructions for causing one or more programmable processors to receive by a virtual router of a computing device for one or more virtual networks a tunnel packet comprising an outer header and an inner packet that defines a packet flow determine based at least on the outer header that the packet is associated with a virtual network of the one or more virtual networks determine by the virtual router a packet flow defined by the inner packet does not match any flow table entry of a flow table that identifies active flows only for virtual network and in response to determining the inner packet does not match any flow table entry of the flow table add a first flow table entry for the packet flow to the flow table and add a second flow table entry for a reverse packet flow of the packet flow to the flow table.

The details of one or more embodiments of the invention are set forth in the accompanying drawings and the description below. Other features objects and advantages of the invention will be apparent from the description and drawings and from the claims.

In some examples data center may represent one of many geographically distributed network data centers. As illustrated in the example of data center may be a facility that provides network services for customers . Customers may be collective entities such as enterprises and governments or individuals. For example a network data center may host web services for several enterprises and end users. Other exemplary services may include data storage virtual private networks traffic engineering file service data mining scientific or super computing and so on. In some embodiments data center may be individual network servers network peers or otherwise.

In this example data center includes a set of storage systems and application servers A X herein servers interconnected via high speed switch fabric provided by one or more tiers of physical network switches and routers. Switch fabric is provided by a set of interconnected top of rack TOR switches A BN collectively TOR switches coupled to a distribution layer of chassis switches A M collectively chassis switches . Although not shown data center may also include for example one or more non edge switches routers hubs gateways security devices such as firewalls intrusion detection and or intrusion prevention devices servers computer terminals laptops printers databases wireless mobile devices such as cellular phones or personal digital assistants wireless access points bridges cable modems application accelerators or other network devices.

In this example TOR switches and chassis switches provide servers with redundant multi homed connectivity to IP fabric and service provider network . Chassis switches aggregate traffic flows and provides high speed connectivity between TOR switches . TOR switches may be network devices that provide layer two e.g. MAC and or layer 3 e.g. IP routing and or switching functionality. TOR switches and chassis switches may each include one or more processors and a memory and that are capable of executing one or more software processes. Chassis switches are coupled to IP fabric which performs layer 3 routing to route network traffic between data center and customers by service provider network .

Virtual network controller VNC provides a logically and in some cases physically centralized controller for facilitating operation of one or more virtual networks within data center in accordance with one or more embodiments of this disclosure. In some examples virtual network controller may operate in response to configuration input received from network administrator . Additional information regarding virtual network controller operating in conjunction with other devices of data center or other software defined network is found in International Application Number PCT US2013 044378 filed Jun. 5 2013 and entitled PHYSICAL PATH DETERMINATION FOR VIRTUAL NETWORK PACKET FLOWS which is incorporated by reference as if fully set forth herein.

Typically the traffic between any two network devices such as between network devices within IP fabric not shown or between servers and customers or between servers for example can traverse the physical network using many different paths. For example there may be several different paths of equal cost between two network devices. In some cases packets belonging to network traffic from one network device to the other may be distributed among the various possible paths using a routing strategy called multi path routing at each network switch node. For example the Internet Engineering Task Force IETF RFC 2992 Analysis of an Equal Cost Multi Path Algorithm describes a routing technique for routing packets along multiple paths of equal cost. The techniques of RFC 2992 analyzes one particular multipath routing strategy involving the assignment of flows to bins by hashing packet header fields that sends all packets from a particular network flow over a single deterministic path.

For example a flow can be defined by the five values used in a header of a packet or five tuple i.e. the protocol Source IP address Destination IP address Source port and Destination port that are used to route packets through the physical network. For example the protocol specifies the communications protocol such as TCP or UDP and Source port and Destination port refer to source and destination ports of the connection. A set of one or more packet data units PDUs that match a particular flow entry represent a flow. Flows may be broadly classified using any parameter of a PDU such as source and destination data link e.g. MAC and network e.g. IP addresses a Virtual Local Area Network VLAN tag transport layer information a Multiprotocol Label Switching MPLS or Generalized MPLS GMPLS label and an ingress port of a network device receiving the flow. For example a flow may be all PDUs transmitted in a Transmission Control Protocol TCP connection all PDUs sourced by a particular MAC address or IP address all PDUs having the same VLAN tag or all PDUs received at the same switch port.

In accordance with various aspects of the techniques described in this disclosure one or more of servers may include a virtual router that executes multiple routing instances for corresponding virtual networks within data center . Packets received by the virtual router of server A for instance from the underlying physical network fabric may include an outer header to allow the physical network fabric to tunnel the payload or inner packet to a physical network address for a network interface of server A that executes the virtual router. The outer header may include not only the physical network address of the network interface of the server but also a virtual network identifier such as a VxLAN tag or Multiprotocol Label Switching MPLS label that identifies one of the virtual networks as well as the corresponding routing instance executed by the virtual router. An inner packet includes an inner header having a destination network address that conform to the virtual network addressing space for the virtual network identified by the virtual network identifier.

In some aspects the virtual router buffers and aggregates multiple tunneled packets received from the underlying physical network fabric prior to delivery to the appropriate routing instance for the packets. In some examples the virtual router aggregates multiple packets according to matching criteria that includes the virtual network identifier of the outer header as well as one or more fields of the inner header. That is a virtual router executing on one of servers may receive inbound tunnel packets of a packet flow from switches and prior to routing the tunnel packets to a locally executing virtual machine process the tunnel packets to construct a single aggregate tunnel packet for forwarding to the virtual machine. That is the virtual router may buffer multiple inbound tunnel packets and construct the single tunnel packet in which the payloads of the multiple tunnel packets are combined into a single payload and the outer overlay headers on the tunnel packets are removed and replaced with a single header virtual network identifier. In this way the aggregate tunnel packet can be forwarded by the virtual router to the virtual machine as if a single inbound tunnel packet was received from the virtual network. Moreover to perform the aggregation operation the virtual router may leverage a kernel based offload engine that seamlessly and automatically directs the aggregation of tunnel packets.

As one example the virtual router may extend a Generic Receive Offload GRO routine available by the server kernel and that is specialized for processing layer two L2 packets but the virtual router may leverage the GRO routine in a way so as to utilize the routine to manipulate multiple tunneled packets as if they were L2 packets thereby efficiently constructing the aggregate tunnel packet. In some examples the virtual router provides multiple tunneled packets to GRO for aggregation by at least in part setting the respective virtual network identifiers and invoking the GRO routine as if the virtual network identifiers are a L2 header for the inner packets of the tunneled packets. In this way the GRO routine considers each packet received from the virtual router for aggregation purposes as a non tunneled L2 packet that includes at least a portion of an L2 header e.g. a destination MAC address set to the virtual network identifier for a received tunneled packet and a layer 3 network packet that corresponds to the inner packet for the received tunnel packet. By matching according to the L2 data link header and one or more header fields of the layer 3 packet the GRO routine may aggregate multiple such packets into an aggregated packet for delivery to the appropriate routing instance. In this way the aggregation techniques may increase the virtual router bandwidth by reducing the number of packet headers for processing.

In some example implementations the virtual routers executing on servers may steer received inbound tunnel packets among multiple processor cores to facilitate packet processing load balancing among the cores when processing the packets for routing to one or more virtual and or physical machines. As one example server A may include multiple network interface cards and multiple processor cores to execute the virtual router and may steer received packets among multiple processor cores to facilitate packet processing load balancing among the cores. For instance a particular network interface card of server A may be associated with a designated processor core to which the network interface card directs all received packets. The various processor cores rather than processing each of the received packets offloads flows to one or more other processor cores in accordance with a hash function applied to at least one of the inner and outer packet headers for processing to take advantage of available work cycles of the other processor cores.

In other example implementations the virtual routers executing on servers may proactively add by the virtual router flow table entries to identify reverse flows of flows processed by a routing instance of the virtual router. In an example implementation the virtual router of server A may proactively add flow table entries to identify reverse flows of flows processed by a routing instance of the virtual router. For example a virtual machine executing on server A and a member of a virtual network implemented by data center may receive an initial inbound tunnel packet for a packet flow originated by virtual machine executing on server X and also a member of the virtual network. Upon receiving the initial inbound tunnel packet in addition to adding a flow table entry specifically for the inbound packet flow the virtual router of server A may also proactively add a flow table entry specifically for the reverse packet flow i.e. an outbound packet flow that corresponds to the received inbound packet flow. In this way server A may predict the need to process outbound tunnel packets having reverse flow criteria and as a result more efficiently look up and use the flow table entry for the reverse packet flow to process subsequent packets that belong to the reverse packet flow.

Each virtual router may execute within a hypervisor a host operating system or other component of each of servers . Each of servers may represent an x86 or other general purpose or special purpose server capable of executing virtual machines . In the example of virtual router A executes within hypervisor also often referred to as a virtual machine manager VMM which provides a virtualization platform that allows multiple operating systems to concurrently run on one of servers . In the example of virtual router A manages virtual networks each of which provides a network environment for execution of one or more virtual machines VMs on top of the virtualization platform provided by hypervisor . Each VM is associated with one of the virtual networks VN VN and may represent tenant VMs running customer applications such as Web servers database servers enterprise applications or hosting virtualized services used to create service chains. In some cases any one or more of servers or another computing device may host customer applications directly i.e. not as virtual machines. Virtual machines as referenced herein e.g. VMs and servers or a separate computing device that hosts a customer application may alternatively referred to as hosts. 

In general each VM may be any type of software application and may be assigned a virtual address for use within a corresponding virtual network where each of the virtual networks may be a different virtual subnet provided by virtual router A. A VM may be assigned its own virtual layer three L3 IP address for example for sending and receiving communications but may be unaware of an IP address of the physical server A on which the virtual machine is executing. In this way a virtual address is an address for an application that differs from the logical address for the underlying physical computer system e.g. server A in the example of .

In one implementation each of servers includes a corresponding one of virtual network VN agents A X collectively VN agents that controls the overlay of virtual networks and that coordinates the routing of data packets within server . In general each VN agent communicates with virtual network controller which generates commands to control routing of packets through data center . VN agents may operate as a proxy for control plane messages between virtual machines and virtual network controller . For example a VM may request to send a message using its virtual address via the VN agent A and VN agent A may in turn send the message and request that a response to the message be received for the virtual address of the VM that originated the first message. In some cases a VM may invoke a procedure or function call presented by an application programming interface of VN agent A and the VN agent A may handle encapsulation of the message as well including addressing.

In one example network packets e.g. layer three L3 IP packets or layer two L2 Ethernet packets generated or consumed by the instances of applications executed by virtual machines within the virtual network domain may be encapsulated in another packet e.g. another IP or Ethernet packet that is transported by the physical network. The packet transported in a virtual network may be referred to herein as an inner packet while the physical network packet may be referred to herein as an outer packet or a tunnel packet. Encapsulation and or de capsulation of virtual network packets within physical network packets may be performed within virtual routers e.g. within the hypervisor or the host operating system running on each of servers . As another example encapsulation and de capsulation functions may be performed at the edge of switch fabric at a first hop TOR switch that is one hop removed from the application instance that originated the packet. This functionality is referred to herein as tunneling and may be used within data center to create one or more overlay networks. Besides IPinIP other example tunneling protocols that may be used include IP over GRE VxLAN MPLS over GRE MPLS over UDP etc.

As noted above virtual network controller provides a logically centralized controller for facilitating operation of one or more virtual networks within data center . Virtual network controller may for example maintain a routing information base e.g. one or more routing tables that store routing information for the physical network as well as one or more overlay networks of data center . Similarly switches and virtual routers maintain routing information such as one or more routing and or forwarding tables. In one example implementation virtual router A of hypervisor implements a network forwarding table NFT for each virtual network . In general each NFT stores forwarding information for the corresponding virtual network and identifies where data packets are to be forwarded and whether the packets are to be encapsulated in a tunneling protocol such as with a tunnel header that may include one or more headers for different layers of the virtual network protocol stack.

For example virtual machine VM sends a packet an inner packet virtual router A by an internal link. Virtual router A uses NFTto look up a virtual network destination network address for packet . NFTspecifies an outbound interface for virtual router A and encapsulation for packet . Virtual router A applies the encapsulation to add a tunnel header to generate outer packet and outputs outer packet on the outbound interface in this case toward TOR switch A.

The routing information may for example map packet key information e.g. destination IP information and other select information from packet headers to one or more specific next hops within the networks provided by virtual routers and switch fabric . In some case the next hops may be chained next hop that specify a set of operations to be performed on each packet when forwarding the packet such as may be used for flooding next hops and multicast replication. In some cases virtual network controller maintains the routing information in the form of a radix tree having leaf nodes that represent destinations within the network. U.S. Pat. No. 7 184 437 provides details on an exemplary embodiment of a router that utilizes a radix tree for route resolution the contents of U.S. Pat. No. 7 184 437 being incorporated herein by reference in its entirety.

As shown in each virtual network provides a communication framework for encapsulated packet communications for the overlay network established through switch fabric . In this way network packets associated with any of virtual machines may be transported as encapsulated packet communications via the overlay network. In addition in the example of each virtual router includes a default network forwarding table NFTand provides a default route that allows a packet to be forwarded to virtual subnet VN without encapsulation i.e. non encapsulated packet communications per the routing rules of the physical network of data center . In this way subnet VN and virtual default network forwarding table NFTprovide a mechanism for bypassing the overlay network and sending non encapsulated packet communications to switch fabric .

Moreover virtual network controller and virtual routers may communicate using virtual subnet VN in accordance with default network forwarding table NFT during discovery and initialization of the overlay network and during conditions where a failed link has temporarily halted communication via the overlay network. Once connectivity with the virtual network controller is established the virtual network controller updates its local routing table to take into account new information about any failed links and directs virtual routers to update their local network forwarding tables . For example virtual network controller may output commands to virtual network agents to update one or more NFTs to direct virtual routers to change the tunneling encapsulation so as to re route communications within the overlay network for example to avoid a failed link.

When link failure is detected a virtual network agent local to the failed link e.g. VN Agent A may immediately change the encapsulation of network packet to redirect traffic within the overlay network and notifies virtual network controller of the routing change. In turn virtual network controller updates its routing information any may issues messages to other virtual network agents to update local routing information stored by the virtual network agents within network forwarding tables .

Computing device includes in this example a system bus coupling hardware components of a computing device hardware environment. System bus couples memory network interface cards NICs A B collectively NICs storage disk and multi core computing environment having a plurality of processing cores A J collectively processing cores . Network interface cards include interfaces configured to exchange packets using links of an underlying physical network. Multi core computing environment may include any number of processors and any number of hardware cores from for example four to thousands. Each of processing cores each includes an independent execution unit to perform instructions that conform to an instruction set architecture for the core. Processing cores may each be implemented as separate integrated circuits ICs or may be combined within one or more multi core processors or many core processors that are each implemented using a single IC i.e. a chip multiprocessor .

Disk represents computer readable storage media that includes volatile and or non volatile removable and or non removable media implemented in any method or technology for storage of information such as processor readable instructions data structures program modules or other data. Computer readable storage media includes but is not limited to random access memory RAM read only memory ROM EEPROM flash memory CD ROM digital versatile discs DVD or other optical storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium that can be used to store the desired information and that can be accessed by cores .

Main memory includes one or more computer readable storage media which may include random access memory RAM such as various forms of dynamic RAM DRAM e.g. DDR2 DDR3 SDRAM or static RAM SRAM flash memory or any other form of fixed or removable storage medium that can be used to carry or store desired program code and program data in the form of instructions or data structures and that can be accessed by a computer. Main memory provides a physical address space composed of addressable memory locations.

Memory may in some examples present a non uniform memory access NUMA architecture to multi core computing environment . That is cores may not have equal memory access time to the various storage media that constitute memory . Cores may be configured in some instances to use the portions of memory that offer the lowest memory latency for the cores to reduce overall memory latency.

In some instances a physical address space for a computer readable storage medium may be shared among one or more cores i.e. a shared memory . For example cores A B may be connected via a memory bus not shown to one or more DRAM packages modules and or chips also not shown that present a physical address space accessible by cores A B. While this physical address space may offer the lowest memory access time to cores A B of any of portions of memory at least some of the remaining portions of memory may be directly accessible to cores A B. One or more of cores may also include an L1 L2 L3 cache or a combination thereof. The respective caches for cores offer the lowest latency memory access of any of storage media for the cores .

Memory network interface cards NICs A B collectively NICs storage disk and multi core computing environment provide an operating environment for a software stack that executes a virtual router and one or more virtual machines A K collectively virtual machines . Virtual machines may represent example instances of any of virtual machines of . The computing device partitions the virtual and or physical address space provided by main memory and in the case of virtual memory by disk into user space allocated for running user processes and kernel space which is protected and generally inaccessible by user processes. An operating system kernel not shown in may execute in kernel space and may include for example a Linux Berkeley Software Distribution BSD another Unix variant kernel or a Windows server operating system kernel available from Microsoft Corp. Computing device may in some instances execute a hypervisor to manage virtual machines also not shown in . An example hypervisor is illustrated in . Example hypervisors include Kernel based Virtual Machine KVM for the Linux kernel Xen ESXi available from VMware Windows Hyper V available from Microsoft and other open source and proprietary hypervisors. In some examples specialized hardware programmed with routing information such as FIBs may execute the virtual router .

Eth A and Eth B represent devices according to a software device model and provide device driver software routines for handling packets for receipt transmission by corresponding NICs . Packets received by NICs from the underlying physical network fabric for the virtual networks may include an outer header to allow the physical network fabric to tunnel the payload or inner packet to a physical network address for one of NICs . The outer header may include not only the physical network address but also a virtual network identifier such as a VxLAN tag or Multiprotocol Label Switching MPLS label that identifies one of the virtual networks as well as the corresponding routing instance . An inner packet includes an inner header having a destination network address that conform to the virtual network addressing space for the virtual network identified by the virtual network identifier. For example virtual router forwarding plane may receive by Eth from NIC a packet having an outer header than includes a VxLAN associated in virtual router forwarding plane with routing instance A. The packet may have an inner header having a destination network address that is a destination address of VM A that taps via tap interface A into routing instance A.

Virtual router in this example includes a kernel space module virtual router forwarding plane as well as a user space module virtual router agent . Virtual router forwarding plane executes the forwarding plane or packet forwarding functionality of the virtual router and virtual router agent executes the control plane functionality of the virtual router . Virtual router agent may represent an example instance of any of VN agents of .

Virtual router forwarding plane includes multiple routing instances A C collectively routing instances for corresponding virtual networks. Each of routing instances includes a corresponding one of forwarding information bases FIBs A C collectively FIBs and flow tables A C collectively flow tables . Although illustrated as separate data structures flow tables may in some instances be logical tables implemented as a single table or other associative data structure in which entries for respective flow tables are identifiable by the virtual network identifier e.g. a VRF identifier such as VxLAN tag or MPLS label . FIBs include lookup tables that map destination addresses to destination next hops. The destination addresses may include layer 3 network prefixes or layer 2 MAC addresses. Flow tables enable application of forwarding policies to flows. Each of flow tables includes flow table entries that each match one or more flows that may traverse virtual router forwarding plane and include a forwarding policy for application to matching flows. For example virtual router forwarding plane attempts to match packets processed by routing instance A to one of the flow table entries of flow table A. If a matching flow table entry exists for a given packet virtual router forwarding plane applies the flow actions specified in a policy to the packet. This may be referred to as fast path packet processing. If a matching flow table entry does not exist for the packet the packet may represent an initial packet for a new packet flow and virtual router forwarding plane may request virtual router agent to install a flow table entry in the flow table for the new packet flow. This may be referred to as slow path packet processing for initial packets of packet flows and is represented in by slow path .

In this example virtual router agent may be a user space process executed by computing device . Virtual router agent includes configuration data virtual routing and forwarding instances configurations VRFs and policy table policies . Virtual router agent exchanges control information with one or more virtual network controllers e.g. VNC of . Control information may include virtual network routes low level configuration state such as routing instances and forwarding policy for installation to configuration data VRFs and policies . Virtual router agent may also report analytics state install forwarding state to FIBs of virtual router forwarding plane discover VMs and attributes thereof. As noted above virtual router agent further applies slow path packet processing for the first initial packet of each new flow traversing virtual router forwarding plane and installs corresponding flow entries to flow tables for the new flows for fast path processing by virtual router forwarding plane for subsequent packets of the flows.

In some example implementations virtual router includes a kernel based offload engine that seamlessly and automatically aggregates multiple incoming packets from a single packet flow. In the example of computing device includes Generic Receive Offload GRO configured to aggregate multiple packets received by NICs from the underlying physical network and to merge the multiple packets to a single packet prior to delivery to virtual router forwarding plane . In this illustrated example GRO is included in kernel space and may be for example a Linux kernel routine. GRO may however be executed in user space in some examples or within one or more of NICs . In addition GRO may be executed during any step of the packet processing process including prior to or after delivery to virtual router forwarding plane . That is virtual router forwarding plane may in some examples apply GRO to received packets.

GRO aggregates multiple packets according to matching criteria selected from fields of the inner header and virtual network identifier of the packets. In accordance with techniques described herein GRO may aggregate multiple received packets according to a combination of virtual network identifier and one or more fields of the inner header e.g. source and destination network address. To aggregate the multiple received packet having matching criteria GRO may combine e.g. concatenate the respective payloads of the received packets while disregarding i.e. removing the virtual network identifiers and inner headers of the packets i.e. concatenating only the payloads of the inner packets and not in some instances the entire inner packets themselves and add a single instance of at least the virtual network identifier and the inner header to the consolidated payloads to form a complete packet. In some instances GRO adds only a single instance of the inner header common to the aggregated packets so as to form the complete packet as if the complete packet were received directly by one of network interface cards .

In some examples the interface for GRO is configured to receive layer 2 L2 packets and GRO aggregates multiple L2 packets that have matching destination L2 addresses e.g. MAC addresses and at least in some cases also matching one or more L3 packet fields and transport layer layer 4 or L4 packet fields. To leverage GRO to aggregate multiple received tunnel packets Eth B or another other component of computing device may append the virtual network identifiers to the received tunnel packets modify the received tunnel packets using the virtual network identifiers or otherwise provide the received tunnel packets to GRO as if the virtual network identifiers were instead at least a part of an L2 header for the received packets. Consequently GRO may view the multiple tunnel packets as L2 packets and GRO can be leveraged to aggregate received packets having a common virtual network identifier and other common L3 L4 fields of the inner packet and return an aggregated packet having the common virtual network identifier as part of an L2 header for the aggregated packet. The virtual network identifiers may include for example MPLS labels each associated with a different overlay network.

As a result of the above techniques virtual router forwarding plane may receive a single aggregated packet to be processed and forwarded by routing instances rather than a series of multiple packets each having separate headers that must be individually processed and forwarded by routing instances . This may improve the overall bandwidth of computing device by reducing cores cycles taken for destination lookup the number of packets passed by the hypervisor e.g. hypervisor of to the virtual router and potentially other packet header processing tasks.

In some examples the GRO interface may conform at least in part to the following example of a GRO routine implemented in the Linux kernel int napi gro receive struct napi struct napi struct sk buff skb 

In the above function prototype that defines an example of the GRO interface skb includes a buffer that stores a packet received by computing device . Virtual router invokes the napi gro receive function to provide received packets for aggregation into aggregate packets prior to application of the virtual router forwarding plane . GRO may store a list of one or more received packets provided to the GRO via the napi gro receive function.

In addition to the buffer included in skb the skb includes pointers to the L2 header and L3 header portions of the packet stored in the buffer. The virtual router may receive via the Eth interfaces via the NICs an inbound packet that includes an L2 e.g. MAC header outer L3 e.g. IP header tunnel header that includes a virtual network identifier an inner L3 header these are described more fully below with respect to and payload. The virtual router may remove or strip the L2 header outer IP header and tunnel header of the inbound packet and invoke the GRO with a modified packet that includes the virtual network identifier concatenated with only the inner IP header and payload. In such examples the skb pointer to the L2 header may point to the virtual network identifier and the skb pointer to the L3 header may point to the inner IP header. The virtual router may define a new napi struct to define the length of the L2 header for the packet provided to the GRO in order to define the interface for GRO in accordance with techniques described herein. In instances in which the virtual network identifier is an MPLS label having a one to one mapping to a virtual overlay network the L2 header of the packet provided to the GRO is the MPLS label. A MPLS label may be a 4 byte value that includes the 20 bit label identifying the virtual overlay network. Accordingly the virtual router may define the length of the L2 header as 4 bytes thereby alleviating any need to pad the L2 header of the skb with leading trailing zeroes.

Outer header also includes tunnel encapsulation which in this example includes GRE protocol field to specify the GRE protocol here MPLS and MPLS label field to specify the MPLS label value here . The MPLS label field is an example of a virtual network identifier and may be associated in a virtual router e.g. virtual router of computing device of with a routing instance for a virtual network.

Inner packet includes inner header and payload . Inner header may include protocol or type of service TOS field as well as private i.e. for a particular virtual routing and forwarding instance IP address information in the form of source IP address field and destination IP address field along with transport layer information in the form of source port field and destination port field . Payload may include application layer layer 7 L7 and in some cases other L4 L7 information produced by or for consumption by a virtual machine for the virtual network. Payload may include and thus alternatively be referred to as an L4 packet UDP packet or TCP packet. 

In accordance with techniques described in this disclosure a computing device may perform GRO to aggregate multiple instances of tunnel packet having multiple different payloads to form an aggregate tunnel packet that includes all of the different payloads from the various packets yet has a single instance of inner header . In some cases the aggregate tunnel packet may also include at least the virtual network identifier in this example MPLS label field of tunnel encapsulation . To identify packets to be aggregated to form an aggregate tunnel packet the computing device may read certain match fields of the packets that define matching criteria. The match fields may include at least the virtual network identifier. In the illustrated example the match fields include MPLS label field a virtual network identifier protocol field private source IP address field private destination IP address field source port and destination port . In other words the inner header of inner packet along with MPLS field . The computing device may aggregate instances of tunnel packet that match on all of the match fields to generate an aggregate tunnel packet.

In some instances the computing device may generate or otherwise provide to the GRO routine L2 headers for inner packet using the virtual network identifier for tunnel packet e.g. MPLS label field . In this way the GRO routine applied by the computing device may match virtual network identifiers re characterized as e.g. destination MAC addresses or other elements of an L2 header and thus without requiring modification of the GRO routine and interface to separately match packets according to a specific virtual network identifier parameter.

If the multiple tunnel packets do not have the same match fields which include in this example a field that specifies respective virtual network identifiers for the tunnel packets NO branch of the computing device separately processes and forwards each of the multiple tunnel packets according to one or more routing instances of the computing device . If however the multiple tunnel packets have the same match fields including the same virtual network identifiers YES branch of computing device aggregates the tunnel packets to form an aggregate tunnel packet . For example as described herein computing device may modify each inbound tunnel packet such that the virtual network identifiers conform to or otherwise appear as L2 headers or computing device may provide each inbound tunnel packet to a kernel based offload engine e.g. GRO such that the virtual network identifiers appear as L2 headers to the kernel based offload engine. Computing device may then invoke the kernel based offload engine to merge the multiple inbound tunnel packets into a single aggregate tunnel packet as if the inbound packets were L2 packets. In some cases the kernel based offload engine removes the outer header from the aggregate tunnel packet while leaving the virtual network identifier as part of tunnel header. Computing device may then process and forward the aggregate tunnel packet according to a routing instance for a virtual network without separately processing and forwarding the multiple tunnel packets .

In accordance with techniques described in this disclosure cores are configured to apply receive packet steering to distribute packet processing load of inbound packet flows among multiple cores . In other words rather than processing all inbound packets of packet flows with core A cores steer packets for at least some of the packet to cores B J for processing.

To apply receive packet steering cores are configured to apply a hash function to received packets to compute a hash value within a hash function value space defined as the continuous range of possible values that result from applying the hash function to inputs. Hashing function values may alternatively be referred to as hash indexes or hash buckets. Example hash functions include e.g. SHA 1 MD5 or a cyclic redundancy check such as CRC32 or CRC64.

In some examples cores apply receive packet steering to received packets according to headers of the packets. Because in the context of virtual networks packets of packet flows may include both an outer header and an inner header cores may steer a received packet by applying the hash function to header fields of at least one of the inner header and the outer header of the received packet to compute a hash value that is associated with one of cores . For example the hash function applied for receive packet steering may be configured with four buckets e.g. hash values 0 3 that identify respective processing cores A B C and J. In some cases hash buckets may be allocated among processing cores according to available resources of cores . For instance a more powerful processing core A may be allocated more hash buckets for the hash function than a comparatively less powerful processing core B. In some cases hash buckets may be allocated only for cores that are members of the came processing unit e.g. CPU as core A that is the designated core for NIC B. Other processing units having other cores may be designated cores for other NICs of computing device .

The one of cores that is associated with a hash value computed for a received packet then processes the packet by executing virtual router to apply a forwarding policy to the packet. In the example illustrated in core A receives packet flows A B from NIC B. Core A applies receive packet steering to both packet flows A B. That is core A applies a hash function to at least one of the inner and outer headers of packets of both packet flows A B to determine respective cores with which to process the packets. For example with respect to packets of packet flow A specifically core A applies the hash function to one or more fields of the outer headers of the packets to determine core A with which to apply the virtual router to the packets of packet flow A. With respect to packets of packet flow B specifically core A applies the hash function to one or more fields of the outer headers of the packets to determine core B with which to apply the virtual router to the packets of packet flow B.

In the example of computing device of cores are configured to apply an extended form of receive packet steering described with respect to computing device of . Techniques described above with respect to computing device are similarly applicable to computing device . With extended receive packet steering different cores of cores apply first and second steering operations to the outer and inner headers of packets respectively. For example core A may be configured to apply a first steering operation with respect to the outer headers of the packets and any of cores B J may be configured to apply an extended steering operation with respect to the inner headers of the packets. In this way even the operation of steering packets associated with an overlay network may be efficiently distributed across the cores without the steering operations becoming a bottleneck for processing of inbound tunnel packets associated with the overlay network. As illustrated NIC B receives packet flows A C collectively packet flows and directs packet flows to core for initial processing. Computing device may represent an example of any of servers TOR switches or computing device .

In this example core A applies a first hash function to one or more fields of the outer headers of packets of each of packet flows to determine for each of the packets one of cores to apply a second hash to the inner header of the packet. In the case of packet flow A core A determines core A. In the case of packet flows B and C core A determines cores B and C respectively and directs the packets accordingly for application of the second hash.

Cores then apply a second hash operation to one or more fields of the inner headers of packets directed from core A to determine for each packet one of cores with which to apply the virtual router . In the case of packet flow A core A applies the second hash operation to determine core A to apply virtual router . Core B having received packet flow B C as part of the initial receive packet steering operation determines cores B and C to apply virtual router to packet flows B and C respectively. Accordingly core B directs packet flow C to core C for application of virtual router . As a result packet flows may in some cases traverse three separate cores to distribute the load of packet processing among multiple cores of the computing device . In addition applying the hash functions to packet flow C for instance sequentially by cores A B may facilitate processing the packets of packet flow C in order.

Example computing device of illustrates another computing hardware architecture that may be configured to apply packet steering techniques described herein. illustrates computing device in simplified form for ease of illustration and does not include e.g. a disk such as disk . Computing device may represent an example of any of servers TOR switches or computing device for instance.

In this example computing device includes sixteen cores cores A D cores A D cores A D and cores A D. Each of cores and may be similar to cores of computing device and represents an independent execution unit to perform instructions that conform to an instruction set architecture for the core. Any of the cores as herein may each be implemented as separate integrated circuits ICs or may be combined within one or more multi core processors or many core processors that are each implemented using a single IC i.e. a chip multiprocessor .

Various subsets of cores and may be combined in a multi core processor to share processor components while each core of the subset maintains at least an independent execution unit to perform instructions substantially independently of the other cores of the subset. For example cores A D may share a level 3 L3 cache and a memory management unit MMU for a multi core processor A that includes the cores. However each of the cores A D in this example each include a separate execution unit and separate level 1 L1 level 2 L2 caches. Alternatively the cores A D may share L2 L3 caches and an MMU of the multi core processor A. Each of multi core processors A D may include more or fewer cores.

In the illustrated example multi core processors A includes cores multi core processor B includes cores multi core processor C includes cores and multi core processor D includes cores . In some examples of computing device however the various cores may be allocated among any one or more multi core processors or may each be an independent processing unit. Multi core processors A D may interconnect by inter multi core communication bus which may for example represent a Quick Path Interconnect QPI or other bus by which multi core processors exchange data and control signals. Multi core processors A D are coupled by respective memory busses A D to respective memories A D which constitute working memories for the multi core processors. Memories A D may each be similar to memory of computing device .

Computing device also includes multiple network interface cards A D that may each be similar to any of NICs of computing device . NICs A D communicatively couple to cores and via respective I O busses A D. For example NIC C communicatively couples to cores via I O bus C. I O busses may represent PCI PCIe PCI E PCI X HyperTransport Infiniband I2C or other types of I O busses operative to communicatively couple a NIC to one or more processing cores and or memories. Each of busses A D may represent a channel for one or more shared physical busses for the busses A D. Other example instances of computing device may include more fewer cores NICs memories etc. Memories network interface cards NICs A B collectively NICs and cores provide an operating environment for a software stack that executes a virtual router and one or more virtual machines A K collectively virtual machines .

NICs A D receive respective inbound packet flows . That is NIC A receives one or more inbound packet flows NIC B receives one or more inbound packet flows and so forth. In accordance with techniques described herein each of NICs A D of is allocated to one of cores cores cores or cores for steering to and processing the sets of inbound packet flows received by the NICs. For example NIC A is allocated to and steers inbound packet flows to cores A D which process the inbound packet flows according to the receive packet steering RPS and extended RPS techniques described herein.

Each of NICs A D is also configured with a designated core of one of its allocated cores of to initially process packets of the inbound packet flows. For instance NIC A may be associated with designated core A of cores allocated for processing inbound packet flows received by NIC A i.e. inbound packet flows . Likewise NIC C may be associated with designated core A of cores allocated for processing inbound packet flows received by NIC C i.e. inbound packet flows . Each of the sets of cores may then process the respective sets of inbound packets flows similarly to cores as described above with respect to .

For example NIC A may direct one or more inbound packet flows to designated core A for processing. Core A may apply receive packet steering to packet flows in a manner similar to the application by core A to packet flows as described in . That is designated core A may perform a first steering association with respect to each of packet flows A C and in some examples the cores to which core A steers packet flows A C may apply virtual router to process forward the packets of the packet flows RPS or perform a secondary steering operation to further distribute the application of virtual router extended RPS . In this manner computing device may process significant numbers of packet flows received at multiple NICs A D using multiple distinct sets of processing cores.

Core A of multi core computing environment receives from an underlying physical network for a virtual network corresponding to one of routing instances a tunnel packet having an inner header for an inner packet and an outer header for the tunnel packet for physical network switching . Based at least on the outer header for the tunnel packet core A identifies core B with which to process the tunnel packet . Core A may apply a hash function to the outer header to identify core B. Based at least on the inner header for the tunnel packet core B identifies core C with which to process the tunnel packet . Core B may apply a hash function to the inner header to identify core C. The packet having been distributed to core C using receive packet steering core C applies virtual router to the packet to process the packet . In some examples packets need not traverse the multiple cores and instead pointers or other references to the packets may be communicated between the cores .

Flow table A of routing instance A identifies packet flows and specifies forwarding or other policies to apply to flows that match any of the flow table entries A K collectively flow table entries . Flow table entries in this example include matching fields for the 5 tuple with which to map flow i.e. source IP address SRC IP A destination IP address DST IP B source port SRC PORT C destination port DST PORT D and type of service TOS E. In addition each of flow table entries specifies a policy for application to packet flows that match the corresponding matching fields of the flow table entry .

Virtual router forwarding plane executed by computing device receives packet flows from NICs for processing and forwarding. Packet flows include packets tunneled for one or more virtual networks. Virtual router forwarding plane processes each tunnel packet to determine a virtual network and select the corresponding routing instance with which to process the tunneled packet according to policies configuration data virtual routing and forwarding instances configurations VRFs and policy table policies of virtual router agent . Policy table represents a table database or other data structure that includes one or more policies that define operations to be applied by virtual router to packet flows that traverse virtual router .

Upon receiving a tunneled packet i.e. an inner packet of a tunnel packet that none of flow table entries of flow table A match e.g. an initial packet of a new flow routing instance A processes the tunneled packet according to the virtual router slow path A which may be an example of slow path . Slow path A includes virtual router agent which determines for the tunneled packet one of policies to apply to the tunneled packet and therefore to any additional packets for the flow for the tunneled packet received by virtual router forwarding plane while the flow is active. Virtual router agent upon determining a policy for a flow installs a flow table entry for the flow to flow table A for application by virtual router forwarding plane to subsequent packets that match the flow matching fields of the flow table entry according to virtual router fast path B. The appropriate policy to apply to a packet being specified by one of flow table entries processing a packet according to fast path B may be performed by virtual router forwarding plane without recourse to virtual router agent .

In the illustrated example routing instance A receives an initial packet for packet flow A and determines whether the initial packet matches any of flow table entries . Packet flow A is a tunneled flow for a virtual network corresponding to routing instance A. Accordingly the initial packet is an inner packet of a tunnel packet transported by an underlying physical network connected to an interface of NIC B. As the initial packet for packet flow A the initial packet does not match any of flow table entries and virtual router processes the packet using virtual router agent according to slow path A. Virtual router agent queries at least one of VRFs and policies to determine forwarding policy P for the packet flow A. Virtual router agent also installs new flow table entry C having matching fields A E that match packet flow A and policy field F that specifies the policy P for packet flow A. Virtual router forwarding plane matches subsequent packets of packet flow A processed by routing instance A to flow table entry C and applies the flow actions specified policy P to the subsequent packets according to fast path B.

In accordance with techniques described herein and in response to receiving the initial packet of packet flow A virtual router agent additionally proactively installs new flow table entry D having matching fields A E that match a reverse flow for packet flow A despite not receiving a tunneled packet for the reverse flow at least since removing any matching flow table entries for the flow . In this case flow table entries have symmetric field pairs source IP address A and destination IP address B as well as source port C and destination port D. Accordingly e.g. destination IP address B of flow table entry D for the reverse flow is the source IP address A of flow table entry C and source IP address A of flow table entry D for the reverse flow is the destination IP address A of flow table entry C. Virtual router agent may determine a separate policy P for the reverse flow and specify the policy in policy field E for flow table entry D matching the reverse flow.

Subsequently virtual router forwarding plane receives for processing packets for a packet flow B that is a reverse flow of packet flow A. Packet flows A B may be for instance a bidirectional communication session between applications such as an HTTP session FTP session content or media delivery session and so forth. Virtual router forwarding plane is able to match the initial and any subsequent packets for packet flow B according to fast path B without virtual router having to perform processing according to slow path A by matching the packets for packet flow B to flow table entry D proactively added by virtual router agent on receiving packet flow A that is a reverse flow for packet flow B.

A virtual router executed by computing device receives an initial tunnel packet for a packet flow from an underlying physical network . The initial tunnel packet is associated with a virtual network. The virtual router processes the initial tunnel packet according to slow path A using virtual router agent which determines one of policies for application to the packet flow . The virtual router additionally processes the initial tunnel packet to identify the associated virtual network and to determine the routing instance A of routing instances that corresponds to the identified virtual network. The virtual router may determine the corresponding routing instance using a virtual network identifier of the initial tunnel packet as described in detail above. For purposes of description only the corresponding routing instance in this example is routing instance A. Virtual router agent adds a flow table entry matching the packet flow to flow table A of the determined routing instance A and specifying the policy determined for the packet flow .

In addition in response to receiving the initial tunnel packet virtual router determines a policy for a reverse packet flow of the packet flow . Also in response to receiving the initial tunnel packet virtual router adds to flow table A of the determined routing instance A a flow table entry for the reverse packet flow that specifies the policy for the reverse packet flow of the packet flow . Accordingly virtual router forwarding plane may process any subsequently received packets for the reverse packet flow using the flow table entry by fast path B without the reverse packet flow having to undergo processing according to slow path A. In this way the techniques may reduce latency that would otherwise accrue from slow path A processing and may improve overall bandwidth of the computing device .

The techniques described herein including in the preceding any of sections may be implemented in hardware software firmware or any combination thereof. Various features described as modules units or components may be implemented together in an integrated logic device or separately as discrete but interoperable logic devices or other hardware devices. In some cases various features of electronic circuitry may be implemented as one or more integrated circuit devices such as an integrated circuit chip or chipset.

If implemented in hardware this disclosure may be directed to an apparatus such a processor or an integrated circuit device such as an integrated circuit chip or chipset. Alternatively or additionally if implemented in software or firmware the techniques may be realized at least in part by a computer readable data storage medium comprising instructions that when executed cause a processor to perform one or more of the methods described above. For example the computer readable data storage medium may store such instructions for execution by a processor.

A computer readable medium may form part of a computer program product which may include packaging materials. A computer readable medium may comprise a computer data storage medium such as random access memory RAM read only memory ROM non volatile random access memory NVRAM electrically erasable programmable read only memory EEPROM Flash memory magnetic or optical data storage media and the like. In some examples an article of manufacture may comprise one or more computer readable storage media.

In some examples the computer readable storage media may comprise non transitory media. The term non transitory may indicate that the storage medium is not embodied in a carrier wave or a propagated signal. In certain examples a non transitory storage medium may store data that can over time change e.g. in RAM or cache .

The code or instructions may be software and or firmware executed by processing circuitry including one or more processors such as one or more digital signal processors DSPs general purpose microprocessors application specific integrated circuits ASICs field programmable gate arrays FPGAs or other equivalent integrated or discrete logic circuitry. Accordingly the term processor as used herein may refer to any of the foregoing structure or any other structure suitable for implementation of the techniques described herein. In addition in some aspects functionality described in this disclosure may be provided within software modules or hardware modules.

Various embodiments have been described. These and other embodiments are within the scope of the following examples.

