---

title: Database replication
abstract: A database server receives a request from a client application for performing a data transaction on persistent data storage. The request is sent to a set of replication servers. An acknowledgement for the request is received from each replication server, including a start sequence number and an end sequence number for data that is stored in local cache of the replication server, and a latest committed sequence number for data that was written to the persistent data storage by the replication server. A maximum value of latest committed sequence numbers received from the set of replication servers is determined. For each replication server, it is examined whether there is a gap between the start sequence number for data stored in local cache and the maximum value of the latest committed sequence numbers. Based on the examining, it is determined whether there is an occurrence of loss of data.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09361348&OS=09361348&RS=09361348
owner: Google Inc.
number: 09361348
owner_city: Mountain View
owner_country: US
publication_date: 20140728
---
This application claims the benefit of U.S. application Ser. No. 13 646 015 filed Oct. 5 2012 which claims the benefit of U.S. Provisional Application Ser. No. 61 543 430 filed on Oct. 5 2011 the entirety of which are hereby incorporated by reference as if fully set forth therein.

A service provider may provide database services for remote client applications. The database services may be hosted using database servers spread across multiple data centers.

In one aspect a write request for writing data to persistent data storage is received at a database server from a client application. In response to receiving the write request the database server selects a set of multiple replication servers. The data is sent from the database server to the selected set of multiple replication servers for writing to the persistent data storage. Confirmation is received at the database server from replication servers in the selected set of multiple replication servers. In response to receiving confirmation from the replication servers in the selected set of multiple replication servers the database server sends to the client application information indicating success of the write request.

Implementations may include one or more of the following. For example selecting the set of multiple replication servers in response to receiving the write request may comprise determining by the database server that a mastership is acquired by the database server for servicing the write request and selecting by the database server the set of multiple replication servers in response to determining that a mastership is acquired. The mastership may indicate a lease on persistent data storage by an instance of the database server that is configured for servicing the write request from the client application.

Selecting the set of multiple replication servers may comprise selecting the set of multiple replication servers from an available pool of replication servers based on a random selection mechanism. Selecting the set of multiple replication servers may comprise determining a level of quality guarantee that is provided to a user associated with the client application and selecting the set of multiple replication servers in geographically distributed locations based on the determined level of quality guarantee. The level of quality guarantee may be based on different price structures the quality guarantee including at least one of a lower bound on data durability and an upper bound on latency experienced in using the database server.

Selecting the set of multiple replication servers may comprise determining an attribute chosen by a user associated with the client application the attribute selected from the group consisting of a number of replication servers location of replication servers size of cache in each replication server used for the client application location of the persistent data storage and size of the persistent data storage and selecting the set of multiple replication servers based on the determined attribute. Selecting the set of multiple replication servers may comprise performing the selection of the set of multiple replication servers independently for each write request.

The database server may receive a second write request from the client application for writing second data to persistent data storage. In response to receiving the second write request the database server may select a second set of multiple replication servers. The second data may be sent from the database server to the second set of multiple replication servers for writing to the persistent data storage. The database server may receive confirmation from less than all of the replication servers in the second set of multiple replication servers. In response to receiving confirmation from less than all of the replication servers in the second set of multiple replication servers the database server may select one or more additional replication servers as replacement for the replication servers from which confirmation is not received and send the second data from the database server to the additional replication servers.

The database server may receive confirmation from the one or more additional replication servers. In response to receiving confirmation from the one or more additional replication servers information may be sent from the database server to the client application indicating success of the second write request. The replication servers in the selected set of multiple replication servers may be geographically distributed.

In another aspect a first write request for writing first data to persistent data storage is received at a replication server from a database server. The replication server determines that the first write request is designated as asynchronous. In response to determining that the first write request is asynchronous the first data is saved to local cache at the replication server and an acknowledgement is sent to the database server. The local cache stores second data to be written to persistent data storage when the first data is saved to the local cache. The first data and the second data are written from the local cache to the persistent data storage in a batch operation.

Implementations may include one or more of the following. The write request may include information on a lease on the persistent data storage by the database server for servicing the first write request. The replication server may determine that the information on the lease matches lease information locally stored at the replication server. The first data may be saved to the local cache in response to determining that the information on the lease matches lease information locally stored at the replication server. Writing the first data and the second data from the local cache to the persistent data storage in a batch operation may comprise renewing the lease on the persistent data storage for the database server.

The replication server may receive a second write request from the database server for writing third data to persistent data storage. The replication server may determine that the second write request is designated as synchronous. In response to determining that the second write request is synchronous the third data may be written to persistent data storage and an acknowledgement may be sent to the database server after writing the third data to persistent storage.

Determining that the first write request is designated as asynchronous may comprise determining that the first write request is asynchronous based on information received with the first write request. The information may be provided by one of the database server or a client application that originated the first write request.

Writing the first data and the second data from the local cache to the persistent data storage in a batch operation may comprise determining whether a threshold is reached for writing the locally cached data to the persistent data storage. Based on determining that the threshold is reached the first data and the second data may be written from the local cache to the persistent data storage in the batch operation. The threshold may be based on at least one of an amount of data in the local cache or a time period that is user configurable.

A replication server may be configured to perform read and write operations directly on the persistent data storage. Writing the first data and the second data to the persistent data storage in a batch operation may comprise determining that sequence numbers associated with the first data and the second data are contiguous. The first data and the second data may be written to the persistent data storage in the batch operation in response to determining that the sequence numbers are contiguous.

Writing the first data and the second data from the local cache to the persistent data storage in a batch operation may comprise determining that the first data and the second data have not been written to persistent data storage by another replication server. The first data and the second data may be written from the local cache to the persistent data storage in the batch operation in response to determining that the first data and the second data have not been written to persistent data storage by another replication server.

Writing the first data and the second data from the local cache to the persistent data storage in the batch operation may be performed independently of a state of operation of the database server. The replication server may be included in a group of replication servers that are geographically distributed.

In another aspect a first write request for writing first data to persistent data storage and an indication that the first write request is designated as asynchronous are received at a database server from a client application. In response to receiving the first write request the database server selects a first set of replication servers. The database server determines that the first write request is designated as asynchronous. The first data is sent from the database server to the first set of replication servers for writing to the persistent data storage along with an indication that the first write request is asynchronous wherein replication servers included in the first set are configured to save the data to respective local caches in response to receiving the indication that the first write request is asynchronous send a confirmation to the database server when the data is saved to the respective local caches and write the first data to persistent storage in a batch operation with other data stored in the respective local caches. Confirmation is received at the database server from the replication servers in the first set. In response to receiving confirmation from the replication servers in the first set information indicating success of the first write request is sent from the database server to the client application.

The database server receives from the client application a second write request for writing second data to persistent data storage and an indication that the second write request is designated as synchronous. In response to receiving the second write request the database server selects a second set of replication servers. The database server determines that the second write request is designated as synchronous. The second data is sent from the database server to the second set of replication servers for writing to the persistent data storage along with an indication that the second write request is synchronous wherein replication servers included in the second set are configured to write the second data to persistent data storage in response to receiving the indication that the second write request is synchronous and send confirmation to the database server after writing the second data to persistent storage. Confirmation is received at the database server from the replication servers in the second set. In response to receiving confirmation from the replication servers in the second set information is sent from the database server to the client application indicating success of the second write request.

Implementations may include one or more of the following. The replication servers included in at least one of the first set or the second set may be geographically distributed. A replication server may be configured for performing read and write operations directly on the persistent data storage.

Receiving the first write request from the client application for writing the first data to persistent data storage may include determining by the database server whether a mastership is acquired by the database server for servicing the first write request and selecting by the database server the first set of replication servers in response determining that mastership is acquired for servicing the first write request. The mastership may indicate a lease on persistent data storage by an instance of the database server that is configured for servicing the first write request from the client application. Determining whether a mastership is acquired for servicing the first write request may include generating by the database server a mastership identifier associated with servicing the first write request based on determining that mastership is not acquired for servicing the first write request and sending an instruction from the database server to a replication server for writing a lease to an underlying replication mechanism that is configured for writing data to the persistent data storage the instruction including the mastership identifier.

Sending the first data from the database server to the first set of replication servers for writing to the persistent data storage may include sending information on a lease on the persistent data storage by the database server for servicing the first write request. A replication server included in the first set may determine whether the information on the lease matches lease information locally stored at the replication server. The first data may be saved to the local cache at the replication server based on determining that the information on the lease matches lease information locally stored at the replication server. The replication server may configured for renewing the lease on the persistent data storage for the database server upon writing the locally cached data to the persistent data storage.

In another aspect a read request is received at a database server from a client application for reading data from persistent data storage. The database server determines whether one or more replication servers associated with the database server have a complete cache window corresponding to data of the client application. A complete cache window indicates an entire amount of uncommitted data for the client application is stored in a local cache of a replication server and the uncommitted data is contiguous with committed data associated with the client application. Committed data is data that has been written to persistent data storage and uncommitted data is data that has not been written to persistent data storage but is in local cache of the replication servers. Based on determining that one or more replication servers associated with the database server have a complete cache window a replication server is selected from the one or more replication servers and the read request is sent to the selected replication server.

Implementations may include one or more of the following. The one or more replication servers may be geographically distributed. A replication server may be configured to perform read and write operations directly on the persistent data storage.

Sequence numbers may be associated with the data. A replication server may have a cache start sequence number and a cache end sequence number for uncommitted data for the client application present in the local cache of the replication server the cache start sequence number associated with least recent uncommitted data for the client application present in the local cache and the cache end sequence number associated with most recent uncommitted data for the client application present in the local cache.

A difference between the cache start sequence number and the cache end sequence number for the uncommitted data in the local cache of the replication server may indicate a cache window for the client application at the replication server. The cache window may include the complete cache window for the client application if the local cache includes data with a sequence number that is greater by one than a highest sequence number of committed data for the client application and the cache end sequence number corresponds to a highest sequence number of the uncommitted data for the client application received from the database server by the one or more replication servers.

Determining whether one or more replication servers associated with the database server have a complete cache window corresponding to data of the client application may include receiving at the database server and from each replication server a cache start sequence number and a cache end sequence number associated with the respective replication server along with information indicating a highest sequence number of the data committed by the replication server for the client application. An overall highest sequence number of the committed data for the client application may be determined based on the information received from the replication servers. A subset of the replication servers that each have data with a sequence number that is greater than the overall highest sequence number by one may be selected. A replication server in the selected subset of the replication servers with a highest value of the cache end sequence number may be determined. The determined replication server may be selected as having a complete cache window.

Selecting a replication server from the one or more replication servers may include selecting the replication server from the one or more replication servers based on a random selection mechanism. Selecting a replication server from the one or more replication servers may include determining a level of processing activity at each of the one or more replication servers. A replication server with a level of processing activity that is lower than the level of processing activity associated with all other replication servers in the one or more replication servers may be selected.

In another aspect a user associated with a client application is provided with a set of different levels of quality guarantee associated with a database server which is configured for performing read and write requests on persistent data storage for the client application. Selection of a particular level of quality guarantee from the set of different levels of quality guarantee is received from the user. Responsive to receiving the selection of the particular level of quality guarantee values of one or more attributes associated with the database server are determined. The one or more attributes include different values for different levels of quality guarantee. Based on determining the values of the one or more attributes a set of replications servers are configured for performing read and write operations on the persistent data storage based on the read and write requests from the client application.

Implementations may include one or more of the following. A replication server may be configured for performing read and write operations directly on the persistent data storage. The different levels of quality guarantee may be based on different price structures.

The one or more attributes may include data durability provided to data associated with the client application latency experienced by the client application in using the database server for performing read and write requests on the persistent data storage and peak throughput of the replication servers in writing data to the persistent data storage.

Configuring a set of replications servers may include selecting a number and geographic distribution of replication servers for performing read and write requests on persistent data storage for the client application. The number and the geographic distribution may be based on the value of data durability to be provided to data associated with the client application. The number of the replication servers selected may be higher for higher values of data durability than lower values of data durability. The geographic distribution of the replication servers selected may be greater for higher values of data durability than lower values of data durability. The geographic distribution of the replication servers selected may be lower for higher values of the peak throughput.

Configuring a set of replications servers may include configuring a size of local cache in each replication server. The local cache in a replication server may be configured for temporary storage of data for the client application before being written to the persistent data storage. The size of the local cache in a replication server may be configured to be smaller for higher values of data durability than lower values of data durability. The size of the local cache in a replication may be configured to be larger for lower values of latency experienced by the client application than higher values of latency. The size of the local cache in a replication server may be configured to be larger for higher values of the peak throughput.

Configuring a set of replications servers may include selecting a number and geographic distribution of persistent data storage for performing read and write requests for the client application. The number and the geographic distribution may be based on at least one of the value of data durability to be provided to data associated with the client application and the latency experienced by the client application. The number of the persistent data storage selected may be higher for higher values of data durability than lower values of data durability. The number of the persistent data storage selected may be lower for lower values of latency experienced by the client application than higher values of latency. The geographic distribution of the persistent data storage selected may be greater for higher values of data durability than lower values of data durability. The geographic distribution of the persistent data storage selected may be lower for higher values of the peak throughput.

The database server may receive from the user of the client application selection of a new level of quality guarantee from the set of different levels of quality guarantee at a time when the database server is processing read and write requests for the client application based on the particular level of quality guarantee selected previously. The new level of quality guarantee may be different from the particular level of quality guarantee selected previously. Responsive to receiving the selection of the new level of quality guarantee new values of the one or more attributes associated with the database server may be determined. Based on the new values of the one or more attributes a new set of replications servers may be configured for performing read and write operations on the persistent data storage based on the read and write requests from the client application.

In another aspect a request is received at a database server from a client application for performing a data transaction on persistent data storage. The request is sent to a set of replication servers. An acknowledgement for the request is received from each replication server including a start sequence number and an end sequence number for data associated with the client application that is stored in local cache of the replication server and a latest committed sequence number for data associated with the client application that was written to the persistent data storage by the replication server. A maximum value of latest committed sequence numbers received from the set of replication servers is determined. For each replication server it is examined whether there is a gap between the start sequence number for data stored in local cache of the replication server and the maximum value of the latest committed sequence numbers. Based on the examining it is determined whether there is an occurrence of loss of data associated with the client application.

Implementations may include one or more of the following. Examining whether there is a gap between the start sequence number for data stored in local cache of the replication server and the maximum value of the latest committed sequence numbers may include identifying that there is a gap between the start sequence number for data stored in local cache of each replication server and the determined maximum value of the latest committed sequence numbers. Determining whether there is an occurrence of loss of data associated with the client application based on the examining may include determining that there is an occurrence of loss of data associated with the client application based on identifying that there is a gap for each replication server.

Examining whether there is a gap between the start sequence number for data stored in local cache of the replication server and the maximum value of the latest committed sequence numbers may include identifying at least one replication server for which there is no gap between the start sequence number for data stored in the respective local cache and the determined maximum value of the latest committed sequence numbers. Determining whether there is an occurrence of loss of data associated with the client application based on the examining may include determining that there is no occurrence of loss of data associated with the client application based on identifying the at least one replication server.

A replication server may be configured for performing read and write operations directly on the persistent data storage. Sending the request to a set of replication servers may include determining a set of replication servers that are configured for performing data transactions associated with the client application and sending the request to the determined set of replication servers.

Implementations of the above techniques include a method computer program product and a system. The computer program product is suitably embodied in a non transitory machine readable medium and includes instructions executable by one or more processors. The instructions are configured to cause the one or more processors to perform the above described actions.

The system includes one or more processors and instructions embedded in a non transitory machine readable medium that are executable by the one or more processors. The instructions when executed are configured to cause the one or more processors to perform the above described actions.

The details of one or more implementations are set forth in the accompanying drawings and the description below. Other features aspects and advantages will become apparent from the description the drawings and the claims.

The client device may execute a database application and access the associated database using the resources provided by the hosted database service. The hosted database service may enable the application to store the data remotely using a geographically distributed storage system provided by the hosted database service and retrieve data from the remote storage as needed. The hosted database service may enable remote data storage and replication for redundancy for example to prevent data loss in case a data store gets corrupted or for any other suitable reason. In some implementations the client device may represent a web application running in one of the data centers for example a hosted application that is running in either data center A or data center B .

The client device may be for example a desktop computer a tablet computer a notebook computer a laptop computer a smart phone an e book reader a music player or any other appropriate portable or stationary computing device. The client device may include one or more processors configured to execute instructions stored by a computer readable medium for performing various client operations such as input output communication data processing and the like. In general the client device may be implemented using a computing device such as the computing device or the mobile device that are described in the following sections.

The database services that the client device accesses in the hosted database service are exchanged with the hosted database service over the network . The network may include a circuit switched data network a packet switched data network or any other network able to carry data for example Internet Protocol IP based or asynchronous transfer mode ATM based networks including wired or wireless networks. The network may be configured to handle web traffic such as hypertext transfer protocol HTTP traffic and hypertext markup language HTML traffic. The network may include the Internet Wide Area Networks WANs Local Area Networks LANs analog or digital wired and wireless networks for example IEEE 802.11 networks Public Switched Telephone Network PSTN Integrated Services Digital Network ISDN and Digital Subscriber Line xDSL Third Generation 3G or Fourth Generation 4G mobile telecommunications networks a wired Ethernet network a private network such as an intranet radio television cable satellite and or any other delivery or tunneling mechanism for carrying data or any appropriate combination of such networks.

The hosted database service may be implemented as a network connected fault tolerant and redundant distributed data service that is provided by a database service provider to users such as a user of the client device . The fault tolerance and redundancy may be provided by data replication in which the database resources are replicated across multiple data centers. The hosted database service provides data processing and access to stored data to database applications running on client devices that may be remotely located.

The database service provider may own and or manage a number of facilities that host the data centers for example data center A or data center B . A data center may include an enclosed space and can occupy a building such as a large and open warehouse or shed space or be one or more rooms within a building. The enclosed space may be sufficiently large for installation of a large number for example dozens or hundreds or thousands of racks of computer equipment and may house a large number for example tens hundreds thousands or tens of thousands of computers.

A number of such data centers may be dispersed in different geographic regions for example across one or more states countries and or continents. The different regions may correspond to any appropriate geographical subdivisions as determined by the database service provider. For example region may be in the United States while region may be in Canada. In some implementations combinations of geographic subdivisions may be used to define one or more regions.

The facilities or data centers may be communicably linked through a data network such as the Internet or via a private network such as a fiber network owned by or that is otherwise accessible to the database service provider. Each data center may have a number of different features. For example data center A includes one or more of each of the database server the replication server and the data storage . In addition data center A may include other components and modules that are not shown for example a dedicated gateway that interfaces to the network or a resource table indexing all the resources present in the data center A .

Different data centers may have different hardware and software operating costs or usage profiles. For example the components and modules included in data center B may be similar to the components and modules included in data center A or they may be dissimilar. For example data center B may include a different number of the database server the replication server and or the data storage .

Each database server or may generally be a server computer hosting one or more software processes that are configured to receive and process read write data transactions also referred to as requests from users of the system. For example when a database application running on client device sends a write transaction to data center A the request may be received by the database server which may then process the request and attempt to persist the transaction in storage. In some implementations the database server may include one or more software applications such as a hosted database for example MySQL and a virtual file system VFS that interfaces the database with other devices in the data center such as the replication server or and the data storage or . Each database server or may run multiple clones or instances of the software applications to perform parallel processing that is handle multiple requests from different client devices separately at the same time. In some implementations multiple clone processes may be configured to handle multiple requests from the same client device for example from different applications running on a client device.

Each replication server or may generally be a server computer hosting one or more software processes and configured to act as a proxy between the database server or and the data storage or . For example replication server or may be configured to accept read write transactions from the database server or and apply them to the data storage or . Each of replication servers or may accept a write transaction from the database server or and write the associated data to a local cache in the respective replication server.

In some implementations replication server or may be a JAVA server. The local cache may be an in memory cache in the random access memory RAM of the replication server or implemented in a section of a hard disk solid state drive or phase change memory coupled to the replication server. By storing write transactions in local caches the replication servers may enable the database servers or to continue processing transactions while the replication servers may asynchronously perform the relatively slower copy operations of the cache data to the data storage. At certain rigger points the replication server or may batch together all the data in its local cache and perform a write of the batched data in the cache to data storage or .

In some implementations trigger points may not be used. Instead each replication server may be configured to dynamically determine at run time an appropriate time to batch the data in local cache and write the batched data to persistent data storage. Each replication server may be configured to write the data to more than one data storage. For example replication server may write the data to data storage that is locally present in the data center A . In addition replication server may replicate the data by writing to additional remote data storage present in other data centers such as data storage in geographically distributed data center B . In order to write to remote data storage a replication server may use a dedicated connection to the remote storage such as an intranet accessible by the database service provider or it may share connections established over public networks such as the Internet.

The data storage or may generally be a storage device that is configured to write data to storage included within. Examples of data storage or include a hard disk or an array of hard disks magnetic magneto optical disks optical disks or any other suitable form of magnetic optical or solid state storage device. A data center may include multiple such data storage for example an array of interconnected storage devices. Examples of data storage or include in addition hosted data storage services that may be provided by storage service providers.

The data stored in the various data storage for example data storage or may be replicated amongst multiple different data centers in the various regions of the geographically distributed hosted database service. For example as indicated by the dashed line labeled R the data in either data storage or may be replicated between data centers A and B in region and region respectively.

In some implementations the data may be replicated using strongly consistent replication such that the data stored in each data storage is consistent at any given time with the data stored in each of the other data storage across the entire system providing the database service. In other words by using strongly consistent replication of the data all of the data centers throughout the database service system may have a consistent view of where a particular data is located in the system regardless of the particular database server receiving the read transaction.

The system includes a client device that communicates via a network with the data center . The data center may be a part of a geographically distributed hosted database service that has multiple data centers across different regions including the one or more remote data centers . For example the data center may be data center A located in region while a remote data center may be data center B in region .

The data center includes a service interface that connects the data center to one or more client devices via the network . Data center also includes at least one database server multiple replication servers and and data storage . The database server has one or more modules such as communications module database storage handling module and a monitoring module . The data storage includes multiple data stores and . Although the system shows multiple replication servers and in some implementations a single replication server may be used. Similarly in some implementations a single data store may be used instead of the multiple data stores and that are shown in system .

Each remote data center includes data storage that comprises multiple data stores and . However in some implementations a single data store may be used instead of multiple data stores. In addition each remote data center may include one or more components not shown that are similar to data center . Each remote data center also may include one or more components not shown that are different from the components in data center .

The hosted database service in system may provide scalable stores for storing data resources associated with the database applications utilizing the hosted database service. Database applications running on the client device may upload data to the hosted database service including the data centers and and control access to the uploaded data. Access control may include a range of sharing levels for example private shared with one or more individuals shared with one or more groups public etc. . The database applications running on client device may utilize the hosted database service for any number of a variety of reasons. For example the database service may be used for business reasons for example submission of work product ordered by the owner and or manager of the hosted database service or for use in data processing by other services for example using uploaded images to automatically and dynamically create a photo gallery web page .

The client device may be similar to the client device . The client device may communicate across the network using communication protocols such as for example one or more of Transmission Control Protocol Internet Protocol TCP IP Hypertext Transfer Protocol HTTP or other appropriate protocols. While only a single client device is shown there may be multiple client devices communicating across the network with the hosted database service represented by the data center and or other services and devices. In addition there may be one or more client devices running in data center .

The network may be similar to the network . In some implementations different client devices may communicate with the database service over different networks while in some other implementations different client devices communicate with the database service over a single network .

The data center may be implemented such that database applications executing on client device such as a client application may read or write to the database service hosted by the data center . In general a request such as a read or write transaction that is sent by the client application to the data center is received at the data center by the service interface . For example in some implementations the database service may be implemented as a World Wide Web WWW service with a corresponding set of web service application programming interfaces APIs . The web service APIs may be implemented for example as a Representational State Transfer REST based HTTP interface or a Simple Object Access Protocol SOAP based interface. Service interface may receive requests from the client application and parse the requests into a format usable by the database server in the backend such as a remote procedure call RPC that is understandable by the database server . The service interface may format responses generated by the database server for transmission to the client application . In some implementations multiple service interfaces may be implemented for example to support multiple access protocols.

The service interface may include a graphical user interface GUI front end for example to display on a web browser for data access. The GUI may provide the ability to submit database queries such as SQL queries to perform reads and writes on the database. The service interface may monitor load information and update logs for example to track and protect against denial of service DOS attacks.

The database server may be similar to either database server or . The database server may handle request authentication and authorization manage data and metadata and track activity such as for billing. The database server may also provide additional or alternative functionality. For example the database server may provide functionality for load balancing for resource utilization and responsiveness under heavy loads.

The database server includes one or more modules that perform various functionalities for the hosted database service for example functionalities for processing database requests and for data storage and replication. The modules may be implemented in software or they may be implemented in hardware or a suitable combination of software and hardware. For example the modules may include one or more software processes interacting with one or more processors coupled to the database server to perform various functions.

One of the modules in the database server is the communications module . The communications module communicates with the service interface to receive client requests and to return data or acknowledgments due to processing based on the client requests.

A client application request for example a read or write transaction that is received at the database server by the communications module is sent to the database for further processing. The database may manage metadata associated with data resources. In some implementations the database may be a MYSQL database. The metadata may include access control lists ACLs which may generally define who is authorized to perform actions on corresponding resources and the nature of the permitted actions.

When the database receives a request from the client application running on the client device via the communications module the database processes the request transaction and attempts to read or write to the local disk s of database server as appropriate. For example if the request is a write transaction the database attempts to write the resulting data to the local disk and if the request is a read transaction the database attempts to read the appropriate data from the local disk. These access attempts are intercepted by the storage handling module . For example to intercept the access attempts the storage handling module may replace at link time the native calling routines of the database with routines that are implemented by the storage handling module . In a particular example libc may be replaced at link time with a custom library that intercepts and forwards access attempts to the storage handling module . In other cases the storage handling module may be enabled to intercept the access attempts through the use of a Filesystem in Userspace FUSE module implemented by the database server.

In some implementations the storage handling module implements a virtual file system VFS . The VFS receives the read or write requests from the database and forwards individual requests to one or more replication servers and using for example an RPC. The replication servers and in turn perform bulk operations on the replicated data storage system and based on multiple requests.

The database server also includes a monitoring module that monitors the various processes executing in the database server to insure that the data replication and storage functions are operating correctly. In some implementations the database server may be configured to simultaneously serve multiple instances of the database . Each instance of the database may be serving a different client application. The database server spawns a clone process for each instance of the database that it wants to serve and runs the instance of the database associated with the clone process. The clone process runs the storage handling module for intercepting the reads or writes from the instance of the database and forwarding RPCs as appropriate to one or more replication servers via the database server .

The data center includes a pool of replication servers that are configured to interface between the database server and the data storage and . Each replication server may be similar to either replication server or . When the database spawns a clone process to handle an instance of the database the clone process selects a number N of replication servers from the pool of replication servers for interfacing with the data storage for example replication servers and . In one implementation the number Nis three. The storage handling module serving the instance of the database in a clone process sends the read transactions to one of the N selected replication servers and the write transactions to all N selected replication servers.

The replication servers provide an API that allows the storage handling module to communicate with the replication servers. The API may provide multiple methods such as read and write transactions. In addition the API may provide methods to acquire and manage leases by the clone process on the underlying data storage for example using a mastership mechanism that is described in the following sections.

In one implementation the N selected replication servers may be in different racks in the data center in order to reduce the possibility that all selected replication servers may fail at the same time for example when compared to a case in which all of the replication servers are in the same rack in which case a rack failure would result in all replication servers failing . In another implementation the N selected replication servers may be in different data centers spread across multiple regions in order to reduce the possibility of failure due to geographically correlated disasters for example an earthquake that disrupts the operations of a data center . By selecting replication servers that are geographically distributed the durability guarantees on the stored data may be improved but the latency for accessing the data may increase while the peak throughput of the replication servers in writing data to the persistent data storage which is also referred to as peak write throughput or simply throughput may decrease.

Each of the replication servers may be implemented using hardware and software applications written in an appropriate software programming language for example JAVA. A replication server serving an instance of the database accepts a write transaction from the storage handling module associated with the instance of the database and the data associated with the write transaction and saves the write transaction to a local cache associated with the replication server. In one implementation the local cache may be implemented in a random access memory RAM associated with the replication server while in another implementation the local cache may be implemented in a hard disk associated with the replication server. In either implementation for every write transaction the replication server sends an acknowledgement to the storage handling module after saving the data to local cache. At certain trigger points the replication server collects the data saved in local cache and writes the data to the data storage. The trigger point may be for example a threshold percentage of the cache getting full or it may be based on a periodic time interval or some other suitable trigger. In some implementations instead of trigger points each replication server may be configured to dynamically determine at run time an appropriate time to batch the data in local cache and write the batched data to persistent data storage. For example a replication server may execute a write process in a continuous loop that writes data to persistent storage as long as the local cache is not empty. If the replication server detects that some other replication server is succeeding in writing the data to persistent data storage before it then it backs off and retries after some time.

The replication servers may write the data in batches to multiple data storage for replication for example to local data storage and one or more of data storage . The replication servers may utilize a distributed mechanism for performing the write across multiple data storage. For example the replication servers may use a storage system that is based on the PAXOS distributed consensus mechanism to achieve replication in data storage across multiple data centers. A storage system using the PAXOS algorithm is described for example in the paper Baker et al. presented at the 5Biennial Conference on Innovative Data Systems Research CIDR 11 Jan. 9 12 2011 Asilomar Calif. USA and available from http static.googleusercontent.com external content untrusted dlcp research.google.com en us p ubs archive 36971.pdf which is incorporated herein by reference.

The data storage may include multiple data stores to . Although three data stores are shown more or fewer are possible. Each of the data stores may store data in a particular format. For example data store may store data as Binary Large Object BLOB data store may store data in a distributed file system for example Network File System and data store may store data resource in a MYSQL database.

Similarly the data storage in each remote data center may include multiple data stores and . Each of the data stores and may store data in a format that is different from the format for the other data stores. However in an alternative implementation all the data stores and may store data in a similar format or a combination of similar and dissimilar formats.

During operation a read or write transaction request may be received at the communications module from the service interface . In some cases when the transaction request is received the database server may have already spawned a clone process for an instance of the database to handle requests from the particular client application . In this case the communications module forwards the request to the instance of the database that is handling the client application . However in other cases the database server may not have an existing process to handle requests from the particular client . In this case when the communications module receives the request from the service interface the database server spawns a clone process and runs an instance of the database in the clone process to handle the client . The database then processes the request and attempts a read or write to the local filesystem as appropriate. The storage handling module in the clone process intercepts and handles these read or write requests from the instance of the database . In addition the database server selects from the pool of available replication servers in the data center a number N of replication servers to interface with the data storage for the clone process. In one implementation the number N is three. In some applications the database server may select the N replication servers randomly from the pool of available replication servers while in other implementations the database server may select the N replication servers based on suitable selection criteria. In case of a write request the storage handling module forwards the write request in the form of an RPC to all N selected replication servers for example replication servers and . Along with the data the storage handling module may send a unique sequence number associated with the data. In case of a read request the storage handling module forwards the read request in the form of an RPC to one of the N replication servers that is determined to have a complete cache window as described in the following sections.

Upon receiving the data from the storage handling module each selected replication server may perform a series of checks to determine that the storage handling module sending the data is authorized to do so for the particular instance of the database that is serving the client and that the data is consistent. The checks may help to ensure that there is only one clone process for a given instance of the database at any one time. If the checks indicate success each replication server writes the data to its local cache and returns success to the storage handling module along with one or more sequence numbers indicating data that is present in local cache and data that have been written to data storage for the client . Upon receiving an indication of success from all N selected replication servers the storage handling module determines that a successful commit of the write request has been performed. Therefore from the perspective of the storage handling module and the instance of the database every write request may be considered to be written to persistent data storage immediately.

From the perspective of the replication servers the write transactions are asynchronous. Each replication server attempts to write to persistent data storage the data stored in its local cache as described earlier. In one implementation all the selected replication servers may attempt to write to data storage at the same time. The replication servers may compete to acquire a lock in order to proceed with the write. One replication server may successfully acquire the lock and commit the data that is perform the actual write to storage in which case the other replication servers back off. The replication server that performs the actual write determines success upon receiving an acknowledgement from the distributed mechanism that it utilizes to perform the commit for example a storage system that employs the PAXOS algorithm. Upon a determination of success the replication server flushes or deletes the committed data from its cache. Each replication server that backed off queries the distributed mechanism for the sequence number of the latest committed data and compares the sequence numbers of the data in local cache with the sequence numbers of the data that is committed to the data storage. If the comparison indicates that any data in the local cache of the replication server have been committed to data storage by another replication server then the replication server deletes from its cache the corresponding data that have been committed.

In some situations the N selected replication servers may perform synchronous replication that is every write request from the storage handling module is immediately committed to data storage. For example in some implementations a user utilizing the database services for example a user of the client device may be enabled to configure the service to use synchronous or asynchronous replication based on his or her requirements.

Generally speaking when using synchronous replication data durability may be high relative to asynchronous replication since every transaction is performed immediately. For example for a write transaction the data is written to persistent storage immediately upon receiving the write transaction from the client application without any wait period. However when using asynchronous replication there may be a wait period between receiving a transaction for example a write transaction from the client application and performing the transaction on the persistent storage. During the wait period the data associated with the transaction may be cached locally in volatile memory in the database server or any of the replication servers and . If the database server or the replication servers crash while having in local cache data that has not been written to persistent storage the data may be lost. That is if all of the servers caching the data crash before writing to persistent storage the data may be lost. Therefore using asynchronous replication the durability provided is usually lower compared to synchronous replication.

On the other hand synchronous replication may have higher latency and lower throughput compared to asynchronous replication. There is a communications and processing overhead typically expressed in terms of latency or delay in time units associated with every read write transaction that is performed on the persistent data storage. Since in synchronous replication every transaction is performed as an individual operation multiple transactions that are performed at or near the same time compete for resources. In addition for each synchronous transaction the database has to wait a complete cycle to write to persistent storage and receive an acknowledgement of the write before the database can process the next transaction. The above competition for resources is manifested as higher latency and lower throughput per transaction.

However in asynchronous replication the database just has to wait for the transaction to be written to the cache of the replication servers which batch together multiple transactions and perform the batched transactions on the persistent data storage in one operation. Therefore the competition for resources is lower resulting in lower average latency and higher throughput compared to synchronous replication. For example the latency for synchronous replication across distributed persistent data storage may be of the order of 50 100 milliseconds for one operation. However the latency for asynchronous replication using three replication servers that are distributed across data centers in a metropolitan geographic area may be of the order of 20 milliseconds.

Given the tradeoffs as described above synchronous replication may be used when the data is critical such that even a small possibility of data loss due to failure of temporary storage for example in replication servers cannot be tolerated. Synchronous replication also may be used when the amount of data is relatively small such that the overall higher latency and or lower throughput are within acceptable margins. However if the applications can withstand small probabilities of data loss and or large amounts of data are being committed such that the latency throughput are important considerations then asynchronous replication may be preferred.

In addition to allowing a user to select between asynchronous and synchronous replication the database service may provide a user with the ability to control either directly or indirectly other parameters that further effectuate a trade off between durability latency and throughput performance. For example the database service may provide its clients with a user interface or an API through which the user may select one or more attributes of the system that effect a trade off between data durability and latency and or throughput performance. The one or more attributes may include for example the number N of replication servers the location of the replication servers and the amount of cache in each replication server that is used for the particular database application. In addition the one or more attributes may include the geographic distribution and size of the persistent storage hardware.

Increasing the number N of the replication servers may improve the durability since the probability that at least one replication server will be functioning when the other replication servers crash is higher with a higher number of the replication servers. Similarly the durability may be improved by selecting the location of the replication servers such that they are geographically distributed as described earlier. A reason for this is that the risk of correlated failures that are based on physical proximity decrease as the data centers are geographically distributed. Examples of correlated failures based on physical proximity include a power outage in a data center that takes down the replication servers located in the data center or an earthquake that affects the data centers in a metropolitan area. However increasing the number N of replication servers or distributing them geographically may have an adverse impact on the average throughput and or latency. Selecting a higher number of replication servers for serving an instance of the database may result in a greater consumption of the network bandwidth as the replication servers attempt to access the persistent data storage. In addition the wait for the storage handling module to receive acknowledgement of a commit may be longer since the storage handling module will have to wait to hear from a greater number of servers.

On the other hand increasing the amount of cache in a replication server may improve the throughput and or latency at the expense of data durability. A larger amount of local cache would allow the replication servers to temporarily store a larger amount of data and return acknowledgements to the storage handling module more frequently. The replication servers may also write to the persistent data storage less often. Therefore the average throughput may improve and the latency may go down. However having a larger amount of data in local cache means that in the event of simultaneous failure of all N replication servers the data loss may be greater thereby resulting in lower durability. Furthermore having a larger amount of data in the local cache may increase the chances of data loss due to failure of all N replication servers because of the amount of time that will be needed to write the data to persistent storage. For example if the cache is sized to hold a week s worth of data that is it takes a week to write that data to storage once one replication server is lost the window for the other servers to write that data is still one week. If the other servers fail during that week then data may be lost. Relatively speaking a larger cache is more likely to have data loss than a smaller cache because the larger cache corresponds to a greater opportunity for all N replication servers to fail before the data in the cache is written to persistent storage.

Increasing the number of hardware units of persistent storage may improve the data durability for example by allowing data to be written to more locations thereby increasing redundancy. Conversely decreasing the quantity of persistent storage hardware may decrease the data durability. In terms of geographic distribution the more spread out the persistent storage hardware are the better it is for fault tolerance and therefore the data durability may be improved. On the other hand persistent storage units that are in physical proximity may increase the likelihood of correlated failures that affect a certain geographic area thereby decreasing the data durability.

The size and spread of persistent storage units may have the opposite effect on latency and throughput. A lower number of hardware units of persistent storage indicates that on average the data may be made redundant using lesser number of storage units thereby reducing the latency. In addition the closer the persistent storage hardware are located the less variability in the latency and throughput may be experienced therefore the latency and throughput may improve if the persistent storage hardware are less geographically distributed.

The database service may provide the client with different combinations of attributes depending on the price that the client may be paying for using the services. The database service also may provide the client with different levels of quality guarantee for example an upper or lower bound on the data durability or an upper bound on latency or a minimum throughput experienced in using the system with different price structures.

Alternatively or additionally based on multiple factors including the level of guarantee or the attributes selected by the user the replication servers may perform dynamic switching between synchronous and asynchronous replication or may dynamically select other attributes such as the number N of selected replication servers the location of the replication servers the size of local cache in each replication server the location and or size of the persistent data storage when the clone process for the database is instantiated.

Process begins when a write transaction from a client is received . For example the communications module receives a write request from the service interface which in turn received the write request with associated data from the client application via the network . The communications module forwards the write transaction to the clone process in the database server that is handling transactions for the client application . The instance of the database in the clone process attempts to access the local disk to perform the write transaction. The write request from the database is intercepted by the storage handling module in the clone process.

The write transaction is sent to replication servers . For example the storage handling module sends RPCs associated with the write transaction to one or more of the N replication servers that are serving the instance of the database . In case the write transaction is to be performed synchronously the storage handling module may send the RPCs to one of the N replication servers. However in case the write transaction is to be performed asynchronously the storage handling module sends the RPCs to all N replication servers.

The storage handling module sends additional control information with the write transaction that provides a context for the write transaction. The additional control information may include for example a sequence number associated with the particular write transaction and a unique mastership session identifier mastership ID associated with the instance of the database in the clone process serving the client application . The storage handling module increments the sequence number for every write transaction. The sequence numbers help to maintain an order for the write transactions. The sequence numbers also aid the storage handling module and the replication servers to maintain information on the current state of the replication servers and the data storage serving the client application for example information on data for the client application that has been committed to persistent data storage and the data for the client application that may be currently available in the local cache of each of the N replication servers. This is useful in maintaining information on data loss if any and on sending read transactions to one of the N replication servers as described in the following sections.

The mastership ID indicates a lease on the data storage for the clone process handling the particular instance of the database that is serving the client application . A clone process with a current mastership ID may perform read or write requests for the client application on the persistent data storage using the N replication servers. Requests from clone processes that do not have a current mastership ID are not successfully processed by the replication servers. The mastership ID may be generated by the storage handling module using a hash on a concatenation of the Internet Protocol IP address of the database server a port number of the network interface handling the network connection to a replication server and a random number.

In some implementations the storage handling module checks various attributes of the system that are selected by the user of the client device and configures one or more parameters of the system while sending the write transaction to the N replication servers. For example the storage handling module may set a flag such as a value of a control bit which is included in the RPC for the write transaction to indicate whether the particular write transaction should be a synchronous write or an asynchronous write operation.

The write transaction sent from the storage handling module is received at one or more of the N replication servers that is serving the instance of the database . In case the write transaction is to be performed synchronously the transaction may be received at one replication server that is selected by the storage handling module for performing the write transaction. However in case the write transaction is to be performed asynchronously the transaction is received at all N replication servers.

In some implementations a replication server receiving the write transaction checks the value of the control bit that is sent with the write transaction to determine whether the write transaction will be performed in synchronous mode . If it is determined that the write transaction is to be performed in synchronous mode then the replication server receiving the write transaction directly commits the write transaction to the persistent data storage such as data storage and . In such instances the replication servers act merely as proxies for writing to the data storage and they do not store the data in local cache.

In some other implementations the local cache may be used for synchronous writes as well as asynchronous writes. In this case for example a replication server receiving the write transaction may store the data in the local cache and a process on the replication server may read the data from the local cache and write to persistent storage as soon as possible. The replication server then sends an acknowledgement after the data is written to persistent storage in contrast to the an asynchronous write in which case the replication server would send the acknowledgment in response to receiving the write request regardless of whether the data has been written to persistent storage.

The synchronous mode offers the highest level of durability guarantees and enables implementation of a database that is synchronously replicated across the geographically distributed database service while potentially incurring a high level of latency for every write transaction for example in the range of 50 to 80 milliseconds and also being subject to throughput limitations of the underlying replication mechanism that is used by the replication servers.

The commit is considered successful on receiving an acknowledgement from the underlying replication mechanism that the data was written to persistent storage. Upon determining that the commit was successful the replication server returns success to the database server . The replication server may also send to the database server information on the latest sequence number that was committed to data storage.

In some implementations the control bit that is sent with the write transaction may indicate that the write transaction is to be performed in asynchronous mode. In some other implementations each transaction may be performed in asynchronous mode by default when a control bit or flag is not included with the request. In either of these scenarios if the write transaction is to be performed in asynchronous mode then the replication servers save the write transaction to their respective local caches for performing batch writes operations with other data in their local caches.

Each replication server then returns success to the database server such as storage handling module of the database server. In addition the replication servers send one or more sequence numbers indicating the latest data that has been committed to the data storage and the start and end sequence numbers of the data that are currently stored in their respective local caches. As described previously the sequence numbers may help the database server and in particular the storage handling module to maintain state for each of the replication servers that is useful when sending read transactions. In addition the sequence numbers may help to identify if there is data loss for example data stored in the local caches of the replication servers not being committed due to failure of all N replication servers at the same time.

Each replication server storing data in local cache batch commits the write transactions to persistent data storage at certain trigger points. For example a replication server may attempt to batch the data in local cache and write the batched data when its local cache is full or at regular time intervals when the most recent period for locally caching data expires.

When a trigger point is reached each replication server attempts to commit the data that are locally cached in batches. In some implementations all N replication servers may attempt to commit the data in batches at the same time while in other implementations different replication servers may attempt to commit the batched data at different times. Different replication servers may have different ranges of data in their local caches with the difference being in the earliest sequence number of the uncommitted data in their local caches. All N replication servers that are active and have some data stored in their local caches have the same sequence number of the most recent data that is written to local cache.

The replication servers may perform a series of checks for consistency before performing the commit which are described in the following sections. The replication servers may utilize a distributed mechanism for performing the commit for replication across distributed data storage thereby writing the data to persistent data storage in the local data center for example data storage and one or more data storage in remote data centers for example data storage in remote data center .

In some implementations the N replication servers compete to acquire a transaction lock to perform the operation and only one replication server that successfully acquires the transaction lock commits the data to the data storage. The commit is considered successful on receiving an acknowledgement from the underlying replication mechanism that the data was written to persistent storage. The replication servers that did not acquire the transaction lock compare the sequence number of the data that were committed to the sequence number of data in their respective local caches. If it is determined that data in their respective local caches have been committed then each replication server determines that the commit was successful. Upon determining that the commit was successful each replication server returns success to the database server . Each replication server may also send to the database server information on the latest sequence number that was committed to data storage.

However in some other implementations each replication server may attempt to commit the data without acquiring a transaction lock. Therefore all N replication servers may attempt to commit the data but only one copy of the data is written to persistent data storage by the underlying replication mechanism. One of the N replication servers determines that the commit is successful on receiving an acknowledgement from the underlying replication mechanism that the data it was attempting to commit was written to persistent data storage. The other replication servers that do not receive such acknowledgement compare the sequence numbers of the data that were committed to the sequence numbers of data in their respective local caches. If it is determined that data in their respective local caches have been committed then each replication server determines that the commit was successful. Upon determining that the commit was successful each replication server returns success to the database server as described previously.

As described previously the asynchronous mode may provide higher throughput compared to the synchronous mode and reduce latency but with a small risk of data loss. The level of throughput and latency may depend on the number and location of the replication servers. For example using three replication servers located in the same data center may have latency in the order of 1 millisecond while using the same number of replication servers distributed in different data centers in a metropolitan area may have latency in the order of 20 milliseconds. As described previously in some implementations the number of replication servers and the location of the replication servers may be user configurable and may depend on the level of service selected by the user with a specified price structure.

The risk of data loss depends on all N replication servers failing within a time window t with uncommitted data being present in their local caches. The risk of data loss may be mitigated by selecting a higher number of replication servers and by distributing the replication servers across multiple regions. For example for an implementation where N is originally three a fourth replication server may be used that is N is set to four and a forced commit of the uncommitted data may be performed when the system detects that the local cache of only a single replication server has uncommitted data. The forced commit may provide better durability while increasing the average latency for commits.

The process starts by checking and obtaining a mastership ID . For example the database server checks whether it has a current mastership ID for serving an instance of the database and obtains a mastership ID if needed upon receiving a write transaction for the instance of the database . The write transaction may have been generated by the client application and received at the database server by the communications module from the service interface . The communications module forwards the write transaction to the instance of the database that is serving the application .

For example upon receiving a read or write transaction the communications module checks whether an instance of the database exists for serving requests from the client application . If an instance of the database does not exist then the communications module creates an instance of the database and spawns a clone process of the storage handling module to serve that instance. The database server may serve multiple instances of the database but any one instance of the database may be served by only one clone process at a time.

Upon creation of the clone process or if the clone process is already existing the database server acquires mastership of the instance before it can service the request from the client application . This may be applicable to both read and write transactions. If the clone process does not have a current mastership either because it is newly created or for some other suitable reason the storage handling module also generates a unique session identifier for the mastership ID to identify the session for which the instance of the database has the mastership. In order to acquire mastership the storage handling module may send a RPC for modifying the mastership of the instance to a randomly chosen replication server that is selected to serve as a proxy for writing a lease to the underlying replication mechanism. The RPC includes the mastership ID generated by the storage handling module . All subsequent read and write transactions for the storage handling module serving the particular instance of the database are identified by the mastership ID.

The storage handling module may periodically renew the mastership ID by sending an RPC to one replication server for renewing the lease on the data in persistent storage and for returning the renewed lease. The replication server may be one of the N replication servers or it may be a different replication server. The storage handling module also may release the lease when it is done serving the instance of the database . In order to release the lease the storage handling module may issue an empty RPC to the selected N replication servers in synchronous mode such that all N replication servers commit the data in their local caches to persistent storage and then empty the local caches.

After obtaining or checking the mastership or lease on the persistent storage for performing transactions for the instance of the database the database server sends RPCs for the write transaction along with the mastership ID and a sequence number to selected replication servers . After sending the write transaction to one or more of the N replication servers that are serving the instance of the database the storage handling module waits to receive confirmation from the replication servers that the write transaction was successfully committed to persistent storage.

The N replication servers are selected when the clone process is spawned for serving the instance of the database from a pool of replication servers that are available to serve different instances of the database . The replication servers in the pool are distinct from the database server . The replication servers in the pool and also the N selected replication servers may be present in different data centers that are geographically distributed. In some implementations the selection of the N replication servers may be done randomly while in other implementations the selection may be based on attributes that are chosen by the user of the client device and or on the level of quality guarantee that is provided to the user of the client device as described previously.

The storage handling module may wait for a predetermined timeout interval to receive confirmation from the replications servers. For example the timeout interval may be 10 milliseconds such that the storage handling module waits for 10 milliseconds after sending the RPCs to receive confirmation from all N replication servers.

Upon receiving a confirmation from any replication server the storage handling module checks whether it has received confirmation of the write transaction from all N replication servers . If confirmation is received from all N replications servers within the timeout interval then the write transaction is determined to have been successfully committed and the database server returns success to the client application . If the cache for the instance of the database on a replication server is full a write transaction to the cache will block until the replication server is able to free up some space in its cache for example after the next successful commit to data storage. Blocked transactions may result in higher latency they may be avoided by smart cache management.

On the other hand if the timeout interval expires without receiving confirmation from at least one replication server then the particular replication server is considered to have failed. In such an event the storage handling module replaces the unavailable replication server with a new replication server which may be selected from the pool of available replication servers. The replacement of the unavailable replication server may be achieved quickly by maintaining the pool of available replication servers. In addition since a replication server is replaced if a single write transaction fails the local caches on the replication servers always have a contiguous range of write transactions and there are no gaps in the data in the local caches. This is because for asynchronous writes each of the N active replication servers receives every write transaction which are at contiguous sequence numbers. Therefore the local cache of each active replication server contains a contiguous sequence of writes. A new replication server that replaces a failed replication server will see all future writes. So the new replication server will have a contiguous sequence of writes in its local cache.

The new replication server will have no data for the particular instance of the database in its cache and in one implementation the data that currently may exist in the cache of the other N 1 replication servers is not copied to the cache of the new replication server. In this case the storage handling module sends the write transaction to the new replication server and repeats the process of waiting for a period of time equal to the timeout interval to receive confirmation from the new replication server . Even if a replacement replication server starts with an empty cache for the particular instance of the database the data in the cache of the replacement replication server matches up with the data in the caches of the other N 1 replication servers within a finite amount of time based on the other replication servers emptying their caches after successful commits and also based on all the N replication servers receiving the new write transactions. This is because all N replication servers eventually reach a point where the oldest data in their local caches is at a sequence number that is greater than or equal to the sequence number of the first write transaction in the local cache of the new replication server. At this point the new replication server has caught up with the other N 1 replication servers.

If confirmation is received from the new replication server within the timeout interval thereby completing the quorum of N replication servers from which confirmations have been received then the write transaction is determined to have been successfully committed. The database server returns success to the client application . However if the timeout interval expires without receiving confirmation from the new replication server then the process may repeat for selecting another replication server and writing data to the newly selected replication server. However in some implementations this behavior may not repeat indefinitely. The process may limit the total time of the write operation or the number of replication servers it will try to use before reporting to the client application that the write transaction has failed.

The process starts when a replication server receives a write transaction from a database server . For example the replication server may be one of N replication servers selected for serving an instance of the database that performs read write transactions for the client application .

The write transaction that is received may include a mastership ID that identifies the session for which the instance of the database has the mastership. The replication server checks whether the mastership ID sent with the write request matches locally stored mastership ID for the particular session . The replication server compares the mastership IDs to confirm that the clone process that sent the write transaction currently has the lease on the data storage for writing data for the instance of the database .

If the mastership IDs do not match then the replication server generates an error . For example in some implementations the replication server may send an error message to the storage handling module serving the particular instance of the database . The error message may indicate that the mastership IDs did not match. In other implementations the replication server may discard the write transaction without sending any message to the storage handling module .

On the other hand if the mastership IDs match the replication server determines that the instance of the database that sent the write transaction currently holds the lease for writing data for the client application . Based on the determination the replication server saves the write transaction in its local cache . Upon successfully saving the data in local cache the replication server returns a success indication to the database server along with sequence numbers . As described previously the replication server sends the start sequence number of the data that is currently in its local cache. The replication server also sends the most recent sequence number of the data that has been committed for the particular instance of the database . In addition in some implementations the replication server may send the cache end sequence numbers. However in other implementations the replication server may not send the cache end sequence numbers. In such implementations the storage handling module may determine the cache end sequence number based on its knowledge of the sequence number of the latest write transaction that has been sent to the replication servers. The sequence number of the latest write transaction sent to the replication servers represents the latest sequence number that is expected to have been written to the local cache of the replication servers and is expected to be the same as the cache end sequence number for each replication server that has data written to its local cache.

The replication server checks whether a threshold for committing the data is reached . In some implementations the replication server checks whether the trigger point is reached for writing the data in its local cache for the particular instance of the database . The threshold or trigger point may be a certain percentage of the local cache getting filled with uncommitted data. Alternatively the trigger point may be the expiration of a time period for caching the data.

If the threshold is not reached the replication server may continue to receive write transactions from the storage handling module for the particular instance of the database . In some implementations each replication server may perform in parallel the operations of writing new data to its local cache and committing previously written data from its local cache to persistent data storage. For example a replication server may execute a separate process that is active as long as there is data in the cache and the process commits data and updates the local cache. This process may run in parallel with another process that serves requests from the database server and writes data to the local cache of the replication server.

On the other hand if the threshold for committing the data is reached the replication server prepares for committing to storage the data in its local cache . The replication server may obtain a token from the underlying replication mechanism in order to determine whether another commit happened between the start of the commit process by the replication server and the actual commit. In addition the replication server determines whether the mastership is still held by comparing whether the session identifier of the latest lease in persistent data storage matches the mastership ID for the clone process. The replication server may also check whether the lease is free. The check whether the mastership is held guarantees that only the instance that currently holds the lease can commit the data.

If the comparison of the mastership IDs returns a mismatch the replication server may return an error . The error may be due to the clone process that sent the write transaction being a zombie instance that is a database process that is no longer serving the instance of the database but that has persisted in the database server beyond its useful lifetime. Zombie instances may be created for example if the clock in the database server malfunctions. A zombie instance may serve read requests directly from its cache and also may succeed in writing to the local caches of the replication servers.

In some implementations the replication server may send an error message to the storage handling module serving the particular instance of the database . The error message may indicate that the mastership IDs did not match and or may indicate that the clone process may be a zombie instance. In some other implementations the replication server may discard the write transaction without sending any message to the storage handling module .

On the other hand if the comparison of the mastership IDs returns a match the replication server determines that storage handling module that sent the write transaction is the instance that currently holds the lease. The replication server then compares sequence numbers. The replication server checks whether the sequence numbers are contiguous that is there is no gap between the latest committed sequence number in data storage and the earliest sequence number it is trying to commit. If the replication server determines that the sequence numbers are not contiguous then the replication server backs off instead of attempting to commit the data and waits for a predetermined back off period. This guards against data corruption from out of order commits.

On the other hand if the replication server determines that the sequence numbers are contiguous then the replication server proceeds with the commit of the write transactions to the data storage and renews the mastership . As described previously the replication server attempts to commit the write transactions to data storage based either on acquiring a transaction lock or by simply attempting to commit without a lock. As part of the commit the replication server writes the data in its local cache for the particular instance of the database to the data storage using the underlying replication mechanism. The replication server updates the latest committed sequence number to the highest sequence number in its commit. In addition the replication server renews the mastership that is it renews the lease that is held on the data storage for the clone process serving the particular instance of the database .

The replication server then determines whether the commit succeeded . For example the replication server may receive an acknowledgement from the underlying replication mechanism that the data was written to the data storage. If the replication server does not receive any such acknowledgement within a predetermined timeout interval it determines that the commit was not successful. Based on such a determination the replication server backs off and waits for a predetermined back off period.

However if the replication server receives an acknowledgement within the predetermined timeout interval it determines that the commit was successful. Based on such a determination the replication server clears all data from its local cache .

If the replication server had backed off then at the expiry of back off period the replication server checks if the committed data includes data that is present in its local cache . The replication server obtains from the underlying replication mechanism the latest committed sequence number in data storage and compares the latest committed sequence number to the sequence number of data in the local caches. If the replication server determines that all or some of the data in the local cache has been committed for example by one of the N 1 other replication servers then the replication server clears the corresponding data from its local cache . In some implementations the replication server empties the local cache if it determines that all the data in its local cache have been committed by another replication server.

If the local cache was full upon clearing some or all of the data from its local cache the replication server is again ready to accept write transactions from the storage handling module for the particular instance of the database . However if the local cache was partly full then the replication server may perform in parallel writing new transactions to its local cache and committing previously stored transactions to persistent data storage.

In some implementations each replication server commits the data in its local cache independent of the other replication servers. In addition a replication server may continue to commit data and renew the lease as long as it has data in its local cache. The replication server may continue to perform the commit even if the storage handling module serving the instance of the database crashes. Moreover in some implementations until the replication server has completed the commit that is until all the data in its local cache has been written to persistent data storage no other clone process may acquire a lease on the data and serve the instance of the database . This may help to ensure that an instance of the database is not served by a new clone process until its pending data have been committed.

The process begins when a read request is received from a client for example from the client application at the communications module of the database server . The communications module receives the read request from the service interface which in turn received the read request with associated data from the client application via the network . The communications module forwards the read request to the clone process in the database server that is serving the instance of the database handling transactions for the client application . The instance of the database in the clone process may attempt to access the local disk to perform the read transaction. The read request from the database is intercepted by the storage handling module serving the instance of the database in the clone process.

The storage handling module sends the read request to a replication server that has the complete cache window . For example the storage handling module generates one or more RPCs associated with the read request and sends the RPCs to a replication server that has the complete cache window. Generally speaking the cache window of a replication server indicates the amount of data that is present in the local cache of the replication server that is the total number of cached transactions that are not yet committed. The cache window may be computed as the value of the difference between the cache start sequence number and the cache end sequence number for the transactions in the local cache. A cache window is considered to be a complete cache window if the cache start sequence number is one higher than the latest committed sequence number and the cache end sequence number corresponds to the highest sequence number of the uncommitted data that has been received from the storage handling module . In some implementations the local cache may have a cache start sequence number that is lower than the latest committed sequence number. However the local cache may still have data with a sequence number that is one higher than the latest committed sequence number. In such implementations the cache window is considered a complete cache window if the cache end sequence number corresponds to the highest sequence number of the uncommitted data that has been received from the storage handling module .

Therefore a replication server that has the complete cache window is a replication server that has all the uncommitted data for the instance of the database in its local cache and the data in its local cache is contiguous with the data that have been committed. In order to send the read request to the particular replication server the clone process keeps track of the cache windows of all N replication servers as described in the following sections.

The read request sent from the storage handling module includes with the request the mastership ID for the instance of the database . The replication server that receives the read request checks whether the mastership ID matches . The replication server performs the check by determining whether the mastership ID sent with the read request matches the mastership ID stored locally at the replication server for the particular session. The replication server compares the mastership IDs to confirm that the clone process that sent the read transaction is the instance that currently has the lease on the persistent data storage for reading data for the instance of the database serving the client application .

If the mastership IDs do not match then the replication server generates an error . In some implementations the replication server may send an error message to the storage handling module serving the particular instance of the database . The error message may indicate that the mastership IDs did not match. In other implementations the replication server may discard the read transaction without sending any message to the storage handling module .

On the other hand if the mastership IDs match the replication server checks whether it has access to all the writes that is data up to and including the latest written sequence number for the particular instance of the database . Some of the data may be in its local cache and not yet written to the persistent data storage. If the replication server determines that it does not have access to all the data then it generates an error . In some implementations the replication server may send an error message to the storage handling module serving the particular instance of the database . The error message may indicate that the replication server does not have access to all the data. In other implementations the replication server may discard the read transaction without sending any error message to the storage handling module .

If the replication server determines that it has access to all the data then it retrieves the data and sends the requested data to the database server . The requested data may include data retrieved from persistent data storage or uncommitted data that is present in the local cache of the replication server or both. If there is uncommitted data in the local cache then the read transaction is performed on the local cache and on the persistent data storage if needed. However if there is no data in the local cache then the read request may be performed directly on the persistent data storage. Along with the requested data the replication server may send the latest committed sequence number and the start and end sequence numbers of the data in its cache.

Upon receiving the requested data from the replication server the particular instance of the database sends the requested data to the client application . In addition the storage handling module serving the particular instance of the database saves the sequence numbers that are received with the data. The sequence numbers help to determine replication servers with the complete cache window and determine whether there has been any data loss as described in the following sections.

The multiple checks performed by the replication server to determine whether the mastership ID matches and whether it has access to all the writes guarantees that if the read request is successful it will be strongly consistent that is the data that is returned is the most recent data for the client application .

The process starts when the storage handling module receives from the replication servers the cache start sequence numbers L L . . . LN the cache end sequence numbers and the latest committed sequence number . As described previously the cache start sequence cache end sequence and the latest committed sequence number correspond to a clone process serving a particular instance of the database that may be serving the client application. For example the storage handling module receives from each of the N replication servers the start sequence number for the uncommitted data in its cache and also the latest committed sequence number that is known to the particular replication server. The start sequence number provides a lower bound on the oldest sequence number that is present in the local cache of the particular replication server.

In some implementations the storage handling module also receives the cache end sequence number from each replication server. The cache end sequence number provides an upper bound on the latest sequence number stored in the local cache of the particular replication server. Alternatively the storage handling module may determine the cache end sequence number based on its knowledge of the sequence number of the latest write transaction that has been sent to the replication servers. The sequence number of the latest write transaction sent to the replication servers represents the latest sequence number that has been written to the local cache of the replication servers and is the same as the cache end sequence number for each replication server.

The storage handling module compares the latest committed sequence numbers to determine maximum value C of the latest committed sequence number that has been committed to the data storage . For example the storage handling module determines C as the maximum of the latest committed sequence numbers received from the N replication servers.

On the other hand if C is not less than min L L . . . LN 1 then at least one of the N replication servers has contiguous sequence numbers in its local cache and data has not been lost. The storage handling module determines which replication servers have the complete cache window . For example the storage handling module computes the cache window for each of the N replication servers and determines which replication servers have the complete cache window as described previously. The storage handling module receives the sequence numbers from the replication servers with acknowledgement for each read or write transaction. Based on the sequence numbers the storage handling module determines for every transaction one or more replication servers with the complete cache window and whether there has been data loss. The storage handling module selects a replication server with the complete cache window and sends read transactions to the selected replication server.

The computing device includes a processor a memory a storage device a high speed interface connecting to the memory and multiple high speed expansion ports and a low speed interface connecting to a low speed expansion port and the storage device . Each of the processor the memory the storage device the high speed interface the high speed expansion ports and the low speed interface are interconnected using various busses and may be mounted on a common motherboard or in other manners as appropriate. The processor can process instructions for execution within the computing device including instructions stored in the memory or on the storage device to display graphical information for a GUI on an external input output device such as a display coupled to the high speed interface . In other implementations multiple processors and or multiple buses may be used as appropriate along with multiple memories and types of memory. In addition multiple computing devices may be connected with each device providing portions of the necessary operations for example as a server bank a group of blade servers or a multi processor system .

The memory stores information within the computing device . In some implementations the memory is a volatile memory unit or units. In some implementations the memory is a non volatile memory unit or units. The memory may also be another form of computer readable medium such as a magnetic or optical disk.

The storage device is capable of providing mass storage for the computing device . In some implementations the storage device may be or include a computer readable medium such as a floppy disk device a hard disk device an optical disk device or a tape device a flash memory or other similar solid state memory device or an array of devices including devices in a storage area network or other configurations. Instructions can be stored in an information carrier. The instructions when executed by one or more processing devices for example processor perform one or more methods such as those described above. The instructions can also be stored by one or more storage devices such as computer or machine readable mediums for example the memory the storage device or memory on the processor .

The high speed interface manages bandwidth intensive operations for the computing device while the low speed interface manages lower bandwidth intensive operations. Such allocation of functions is an example only. In some implementations the high speed interface is coupled to the memory the display for example through a graphics processor or accelerator and to the high speed expansion ports which may accept various expansion cards not shown . In the implementation the low speed interface is coupled to the storage device and the low speed expansion port . The low speed expansion port which may include various communication ports for example USB Bluetooth Ethernet wireless Ethernet may be coupled to one or more input output devices such as a keyboard a pointing device a scanner or a networking device such as a switch or router for example through a network adapter.

The computing device may be implemented in a number of different forms as shown in the figure. For example it may be implemented as a standard server or multiple times in a group of such servers. In addition it may be implemented in a personal computer such as a laptop computer . It may also be implemented as part of a rack server system . Alternatively components from the computing device may be combined with other components in a mobile device not shown such as a mobile computing device . Each of such devices may include one or more of the computing device and the mobile computing device and an entire system may be made up of multiple computing devices communicating with each other.

The mobile computing device includes a processor a memory an input output device such as a display a communication interface and a transceiver among other components. The mobile computing device may also be provided with a storage device such as a micro drive or other device to provide additional storage. Each of the processor the memory the display the communication interface and the transceiver are interconnected using various buses and several of the components may be mounted on a common motherboard or in other manners as appropriate.

The processor can execute instructions within the mobile computing device including instructions stored in the memory . The processor may be implemented as a chipset of chips that include separate and multiple analog and digital processors. The processor may provide for example for coordination of the other components of the mobile computing device such as control of user interfaces applications run by the mobile computing device and wireless communication by the mobile computing device .

The processor may communicate with a user through a control interface and a display interface coupled to the display . The display may be for example a TFT Thin Film Transistor Liquid Crystal Display display or an OLED Organic Light Emitting Diode display or other appropriate display technology. The display interface may comprise appropriate circuitry for driving the display to present graphical and other information to a user. The control interface may receive commands from a user and convert them for submission to the processor . In addition an external interface may provide communication with the processor so as to enable near area communication of the mobile computing device with other devices. The external interface may provide for example for wired communication in some implementations or for wireless communication in other implementations and multiple interfaces may also be used.

The memory stores information within the mobile computing device . The memory can be implemented as one or more of a computer readable medium or media a volatile memory unit or units or a non volatile memory unit or units. An expansion memory may also be provided and connected to the mobile computing device through an expansion interface which may include for example a SIMM Single Inline Memory Module card interface. The expansion memory may provide extra storage space for the mobile computing device or may also store applications or other information for the mobile computing device . Specifically the expansion memory may include instructions to carry out or supplement the processes described above and may include secure information also. Thus for example the expansion memory may be provide as a security module for the mobile computing device and may be programmed with instructions that permit secure use of the mobile computing device . In addition secure applications may be provided via the SIMM cards along with additional information such as placing identifying information on the SIMM card in a non hackable manner.

The memory may include for example flash memory and or NVRAM memory non volatile random access memory as discussed below. In some implementations instructions are stored in an information carrier such that the instructions when executed by one or more processing devices for example processor perform one or more methods such as those described above. The instructions can also be stored by one or more storage devices such as one or more computer or machine readable mediums for example the memory the expansion memory or memory on the processor . In some implementations the instructions can be received in a propagated signal for example over the transceiver or the external interface .

The mobile computing device may communicate wirelessly through the communication interface which may include digital signal processing circuitry where necessary. The communication interface may provide for communications under various modes or protocols such as GSM voice calls Global System for Mobile communications SMS Short Message Service EMS Enhanced Messaging Service or MMS messaging Multimedia Messaging Service CDMA code division multiple access TDMA time division multiple access PDC Personal Digital Cellular WCDMA Wideband Code Division Multiple Access CDMA2000 or GPRS General Packet Radio Service among others. Such communication may occur for example through the transceiver using a radio frequency. In addition short range communication may occur such as using a Bluetooth Wi Fi or other such transceiver not shown . In addition a GPS Global Positioning System receiver module may provide additional navigation and location related wireless data to the mobile computing device which may be used as appropriate by applications running on the mobile computing device .

The mobile computing device may also communicate audibly using an audio codec which may receive spoken information from a user and convert it to usable digital information. The audio codec may likewise generate audible sound for a user such as through a speaker for example in a handset of the mobile computing device . Such sound may include sound from voice telephone calls may include recorded sound for example voice messages music files etc. and may also include sound generated by applications operating on the mobile computing device .

The mobile computing device may be implemented in a number of different forms as shown in the figure. For example it may be implemented as a cellular telephone . It may also be implemented as part of a smart phone personal digital assistant or other similar mobile device.

Various implementations of the systems and techniques described here can be realized in digital electronic circuitry integrated circuitry specially designed ASICs application specific integrated circuits computer hardware firmware software and or combinations thereof. These various implementations can include implementation in one or more computer programs that are executable and or interpretable on a programmable system including at least one programmable processor which may be special or general purpose coupled to receive data and instructions from and to transmit data and instructions to a storage system at least one input device and at least one output device.

These computer programs also known as programs software software applications or code include machine instructions for a programmable processor and can be implemented in a high level procedural and or object oriented programming language and or in assembly machine language. As used herein the terms machine readable medium and computer readable medium refer to any computer program product apparatus and or device for example magnetic discs optical disks memory Programmable Logic Devices PLDs used to provide machine instructions and or data to a programmable processor including a machine readable medium that receives machine instructions as a machine readable signal. The term machine readable signal refers to any signal used to provide machine instructions and or data to a programmable processor.

To provide for interaction with a user the systems and techniques described here can be implemented on a computer having a display device for example a CRT cathode ray tube or LCD liquid crystal display monitor for displaying information to the user and a keyboard and a pointing device for example a mouse or a trackball by which the user can provide input to the computer. Other kinds of devices can be used to provide for interaction with a user as well for example feedback provided to the user can be any form of sensory feedback for example visual feedback auditory feedback or tactile feedback and input from the user can be received in any form including acoustic speech or tactile input.

The systems and techniques described here can be implemented in a computing system that includes a back end component for example as a data server or that includes a middleware component for example an application server or that includes a front end component for example a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the systems and techniques described here or any combination of such back end middleware or front end components. The components of the system can be interconnected by any form or medium of digital data communication for example a communication network . Examples of communication networks include a local area network LAN a wide area network WAN and the Internet.

The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client server relationship to each other.

Although a few implementations have been described in detail above other modifications are possible. For example the logic flows depicted in the figures may not require the particular order shown or sequential order to achieve desirable results. In addition other actions may be provided or actions may be eliminated from the described flows and other components may be added to or removed from the described systems. Accordingly other implementations are within the scope of the following claims.

