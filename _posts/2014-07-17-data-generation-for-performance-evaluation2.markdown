---

title: Data generation for performance evaluation
abstract: The present disclosure describes methods, systems, and computer program products for generating data for performance evaluation. One computer-implemented method includes identifying a source dataset from a source database, extracting a schema defining the source database, analyzing data within the source dataset to generate a value model, the value model describing features of data in each column of each table of the source dataset, analyzing data within the source database to determine data dependency, and generating a data specification file combining the extracted schema, the value model, and the data dependencies.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09613074&OS=09613074&RS=09613074
owner: SAP SE
number: 09613074
owner_city: Walldorf
owner_country: DE
publication_date: 20140717
---
Typically before deployment of an application test data is required to test and evaluate the performance of the application in a test environment and then the application is deployed in a target environment. Since real data is typically not available during the testing and development stages generation of close to real synthetic data is desirable. Existing approaches of data generation mostly focus on generating data for testing the correctness of the application but not very useful for performance evaluation of the application.

The present disclosure relates to computer implemented methods computer readable media and computer systems for generating data for performance evaluation for example for data centric systems. One computer implemented method includes identifying a source dataset from a source database for performing a performance evaluation extracting a schema defining the source database analyzing data within the source dataset to generate a value model the value model describing features of data in each column of each table of the source dataset analyzing data within the source database to determine data dependency and generating a data specification file combining the extracted schema the value model and the data dependencies.

Other implementations of this aspect include corresponding computer systems apparatuses and computer programs recorded on one or more computer storage devices each configured to perform the actions of the methods. A system of one or more computers can be configured to perform particular operations or actions by virtue of having software firmware hardware or a combination of software firmware or hardware installed on the system that in operation causes or causes the system to perform the actions. One or more computer programs can be configured to perform particular operations or actions by virtue of including instructions that when executed by data processing apparatus cause the apparatus to perform the actions.

The foregoing and other implementations can each optionally include one or more of the following features alone or in combination 

A first aspect combinable with the general implementation wherein determining data dependency comprises determining one or more of inter table dependency intra table dependency or inter column dependency of data within the source database.

A second aspect combinable with any of the previous aspects comprising storing the data specification file in a database or writing the data specification file in a file accessible by a user.

A third aspect combinable with any of the previous aspects comprising generating synthetic data based on the data specification file wherein the generated synthetic data corresponds to the extracted schema of the source database wherein the generated synthetic data corresponds to the generated value model of the source database wherein the generated synthetic data corresponds to the determined data dependency and wherein the generated synthetic data includes different data than the analyzed data within the source database.

A fourth aspect combinable with any of the previous aspects wherein the generated synthetic data has a smaller size a same size or a larger size than the analyzed data.

A fifth aspect combinable with any of the previous aspects wherein generating synthetic data comprises identifying a data source identifying a value model for the synthetic data based on the generated value model of the source database generating a composite value model based on the determined data dependency and generating row values of the synthetic data based on the data source the value model and the composite value model.

A sixth aspect combinable with any of the previous aspects wherein generating synthetic data comprises generating a foreign key graph and generating a data tables in an order according to the foreign key graph.

The subject matter described in this specification can be implemented in particular implementations so as to realize one or more of the following advantages. First the example techniques and tools for generating synthetic data can support complex database schema and can generate close to real synthetic data focusing on the effects of data on the performance behavior of its target application. Second the example techniques and tools can automatically generate large volumes of synthetic data based on a manually created e.g. by a performance analyst or application developer or automatically extracted data specification that captures the essence of the data to be generated. Third the combination of the data extraction and generation process together with the ability to capture complex data dependencies make the example data extraction and generation DEG tool a desirable solution to a variety of use cases in the development of data centric applications. For example in cases where access to real customer data is limited for performance testing DEG can be used to extract the performance relevant characteristics of the data and generate large volumes of synthetic data that mimic the performance effect of real customer data on the target application. The example techniques and tools described herein can be applied and reusable in broad contexts for various applications not limited to specialized functionality or context. Other advantages will be apparent to those skilled in the art.

The details of one or more implementations of the subject matter of this specification are set forth in the accompanying drawings and the description below. Other features aspects and advantages of the subject matter will become apparent from the description the drawings and the claims.

Like reference numbers and designations in the various drawings indicate like elements according to an implementation.

The following detailed description is presented to enable any person skilled in the art to make use and or practice the disclosed subject matter and is provided in the context of one or more particular implementations. Various modifications to the disclosed implementations will be readily apparent to those skilled in the art and the general principles defined herein may be applied to other implementations and applications without departing from scope of the disclosure. Thus the present disclosure is not intended to be limited to the described and or illustrated implementations but is to be accorded the widest scope consistent with the principles and features disclosed herein.

The huge amount of data used by today s business applications induce new challenges for development and testing of data centric systems. For example for realistic performance evaluation of the applications a large amount of close to real synthetic data that account for the inherent complexity of the data structures in today s data centric systems are desirable. Typically the test data available during the development phase is much smaller than the actual data in the target environment. The example techniques described in this disclosure can automatically analyze existing test data of an application and based on this analysis generate the same size or larger volume of synthetic data that closely simulate real data in terms of performance characteristics.

The example techniques can take into account both the existing data e.g. test data and or actual customer data in the source database and the similarity of generated data to source data with respect to their influence on performance behavior of data centric applications. The example techniques can be implemented as computer implemented methods computer program products and systems for generating synthetic data for performance evaluation of an application. The example techniques can be applied among others in the following scenarios 

 A There is no test data but the data semantics and the requirements are known. In this case domain experts can create a data specification describing the nature of the data together with its internal and external dependencies based on which large volumes of test data can be generated.

 B There is only a relatively small test data. In this case synthetic data in large volumes can be generated based on extracted characteristics of the test data which can support realistic performance testing and performance prediction of the application.

 C The customer real data is confidential and cannot be used for testing of the application. In this case synthetic data can be generated with the same performance relevant characteristics of the real data which can then be used for testing.

The example techniques can analyze existing data of a data centric application to extract its performance relevant characteristics and then generate synthetic data that is close to the real data in terms of performance behavior of the target application. Furthermore a toolset referred to as Data Extractor and Generator DEG is developed for extracting data characteristics from a source database and generating data into a destination database. Systematic measurement experiments have been performed to evaluate the generated data in terms of its effect on the performance of the target application. The approach has already been successfully evaluated using a number of case studies.

Data characteristics relevant to performance can be divided in three groups Schema Constraints Value Models and Dependencies. In the following these categories of data characteristics will be explained and the data characteristics will be introduced.

Schema constraints capture the specification of the data schema that has to be created on the target database. These constraints can be divided into two groups obligatory and non obligatory constraints.

Obligatory constraints are constraints or features and or attributes without which the data generation is not possible in other words the minimum attributes that shape the destination schema. For instance a generated table needs to have a table name and the generated table should have the same number of columns as of its source table. For columns the most important constraints include for example column name data type and primary key flag.

The non obligator constraints are constraints that are not necessary for creating a correct schema but they potentially can play an important role in the performance of the target application. The non obligatory constraints can be replaced by default values of a database management system DBMS if they are not set at the time of data generation. Example non obligatory constraints include Table Type Auto Delta Merge Table Partitioning and Indexes.

Value models capture the nature of the data in each table column. Each value model can include a name and a set of parameters which shape a formal description of values in a column of a data set. Example value models include for example 

Random Distribution Value Models value models that generate random data including Uniform and Gaussian models.

Sequential Value Model a value model offering sequential numeric date values or character string values.

Sample Based Probabilistic Distribution Value Model a random data value model with a distribution histogram that can be extracted through sampling of the source data.

Pool Value Model a value model that retrieves the values from a SQL query executed on the destination database.

Expression Value Model a value model based on a mathematical or a programming language expression that specifies the values of the column in each row as a function of the values of other columns of the same table or of other tables.

Most of the data correlations and dependencies in databases are not explicitly defined but usually affect the performance behavior of applications running on the data. These correlations can exist for example among data in a table or between two columns of two different tables. The data correlations and dependencies can also impact the value models explained above. For example if a column is recognized as numeric data type in a range between a minimum and a maximum the result data could be generated randomly within this range. But assuming there is a strong correlation say a foreign key relation between a referencing or child data set and a referenced or parent data set the parent data needs to be considered as the only source to generate data for the child otherwise the generated data may violate the foreign key rules.

In some implementations the dependencies can be categorized in three groups Inter Column Dependency Intra Table Dependency and Inter Table Dependency. The example techniques described herein consider these dependencies extracts them and applies them to generate data with better quality with respect to performance behavior.

Inter Column Dependencies refer to any data dependency between the columns of the same row in the same table. For instance in Germany the land transfer tax Grunderwerbsteuer varies by state thus the land transfer tax depends on the location of the property. For this reason if the location of property and the land transfer tax are stored in one table this correlation needs to be realized.

Intra Table Dependencies refer to the dependencies between values of different rows within the same data set. Such dependencies happen for example in the N to 1 relationships of normalized schemas and their de normalized counterparts.

Inter Table Dependencies refer to the dependencies of two tables that have a parent child foreign key relationship. In such a case for each value in the foreign key column of the child table a primary key value in the parent table can exist. The classical example of inter table dependency is referential integrity RI which is usually called Foreign Key FK relations.

Before data generation a Foreign Key Graph can be built for example by the DEG. A foreign key graph is a directed graph in which the tables are nodes and foreign key relations are edges. In some instances the foreign key graph needs to be a Directed Acyclic Graph DAG otherwise it may lead to a deadlock. Building such a graph can for example facilitate finding probable contradictions in foreign key relations. In addition the data generation of tables needs to be done in the right order. In other words the parent tables should be generated before child tables. The reason is the child tables need to read the foreign key values from parent tables thus the order of creating tables and data generation is important.

The extracted data characteristics can be stored in a data model and then persisted in file system or database. As such the extracted data can be reusable with minimum effort if the machine database or any other underlying infrastructure is changed. In some instances it would be more simple and easier for the performance analyst to read the extracted information in a file or database than objects in memory. In some implementations the extracted information can be modified customized or otherwise managed by the users.

In some implementations the example techniques described herein include extracting the characteristics of a source dataset from a database that are relevant for the performance of one or more applications that will use the dataset. Example characteristics include but not limited to the database schema defining the source database value models capturing the features of data in each column of each table in the source database data dependencies including but not limited to inter table intra table and inter column dependencies. A data specification file with zero or more sample data files can be generated based on the extracted characteristics. The data specification file can combine the extracted schema the value models and the data dependencies. An artificial or synthetic dataset with a smaller the same or larger size than the source dataset can be generated using the generated data specification file. The synthetic dataset can mimic the same performance relevant characteristics of the source dataset for the purpose of performance evaluation of applications.

As illustrated the example system includes a data model extractor and a data generator . The example system can include or be communicably coupled with additional modules or components of the example system . The data model extractor and data generator can be implemented for example as software module hardware module enterprise application application programming interface API or a combination of these and other types of function blocks. Although illustrated as two separate modules in the data model extractor and a data generator may be integrated together according to particular needs desires or particular implementations of the example system .

The data model extractor can be responsible for schema and data model extraction. For example the data model extractor can extract the schema from a source database DB . All the obligatory and non obligatory schema constraints can be extracted. In addition the data model extractor can extract value models and data correlation and dependencies such that the existing data e.g. from the source database can be abstracted in a way that can be later used to generate data. As an example the data model extractor can extract table structures relations between tables and dependencies among data in a table analyze the source data extract these performance relevant characteristics and summarize them into value models explained below .

In some implementations the data model extractor can be divided into or implemented as two sub modules data extractor DE and data analyzer DA not shown . The DE component can be responsible for extracting the schema with all defined constraints as well as required data. The DA component can analyze the source data in the source database to derive the performance relevant characteristics. For example the DA can analyze each column to find a suitable value model for the column and can find different dependencies between tables and columns.

In some implementations the extracted schema value models and data correlations and dependencies can be stored in an extended schema as an output of the data model extractor accessible for users and other components the example system . For instance the data model extractor can generate a data specification file as an example extended schema that combines the extracted schema and value model. The data specification can be written in JAVA C or other suitable language providing data in extensible markup language XML format or other suitable format. In some implementations a user or a performance analyst can access the data specification file and modify the extracted attributes using for example a normal XML editor. As such maintainability of the extracted information is enabled.

As illustrated the extended schema model includes extracted information of one or more tables included the source data. For example the extended schema model includes a Database class that present the entire schema and can include one or more Tables . The class Table can represent a table relation in relational database. Each Table can include various table attributes such as table name table size field and constraint . Detailed features of the table attributes can be further specified. As an example field can represent field or columns of a table. Detailed field attributes of the field can include for example field name data type IsPrimaryKey and values . The boolean attribute IsPrimaryKey is a primary key flag determines whether a column is primary key or not. The Values is an attribute that contains the value model information and can be referred to as a class. Each field can have zero or more value models. The value model can have one or more Params . Param represents parameters used in each value model. In some instances some value models may not have any parameters. Constraints can represent schema constraints like indexes or can be explicitly defined. Constraints can include detailed attributes such as constraint name type parameters can be specified. In some implementations detailed parameters attributes such as parameter names and respective values of the parameters can be specified for example for the Params of the Values and the Params of the Constraint .

In some implementations the extended schema model can include additional or different information indicating the schema value models data correlations and dependencies extracted in the source data. The extended schema model outlines the data structure and organization of the source data. Synthetic data can be generated based on the detailed information and structure included in the extended schema model .

Referring back to the data specification file or extended schema can be provided for example to the data generator . The data generator is responsible for generating data based on the information extracted by the data model extractor . The generated data can be written directly in a generated or target DB or into a file. The generated data should have similar effect on performance of an application as the source data. The term similar effect refers to the similar influence of data on performance behavior of the application. 

For instance after extracting the data characteristics e.g. by the data model extractor and arbitrarily adjusting them e.g. by the user the data generator can generate synthetic data based on the XML specification and output the generated data for example by inserting into the database or storing in a comma separated values CSV format file.

The database includes a collection of tables . Two or more tables can be related by an inter table relationship or dependency . For example two tables can have a parent child foreign key relationship such that a foreign key e.g. a field or a collection of fields in one table uniquely identifies a row of another table. The inter table relationship can also include a foreign key e.g. a column or a combination of columns that is used to establish and enforce a link between two tables and the tables can be cross referenced by the inter table relationship . In some implementations a Foreign Key Graph can be built to reflect the inter table relationships of the multiple tables in the database .

The multiple tables can be governed by data constraints or schema constraints that define the organization of data in the database . For example the data constraints can include obligatory and non obligatory constraints and specify tables columns data types or other features of the database .

The multiple tables can each include a number of fields or columns . Two or more columns can be related by inter column relationship or dependency . The inter column relationship can be any data dependency among data entries of the columns. As an example the data of two columns can be proportional or otherwise correlated. The proportionality or other correlations between the data of the two columns and be extracted and included in the inter column relationship .

The characteristics of data in the fields can be described by one or more value models . In some implementations the inter table relationship the inter column relationship can be incorporated into the value models to capture the distribution value range or other characterizes of the data in the fields . The value models can be used for example by the data generator of to generate values of synthetic data.

The example data extraction subsystem includes a data extractor DE and a data analyzer DA communicably coupled with data model tools DMTs value model tools VMTs database tools DBTs and configuration tools . The DE component is responsible for extracting the schema with all defined constraints as well as required data. For example schema constraints can be derived by the DE from meta data of the source database e.g. an in memory database a traditional database etc. . The DE can use DBT in order to read required information such as schema constraints from source database . DBT is responsible for database DB operations reading from source database and read write from in destination database e.g. destination database in as well. In some implementations the DE can let the DA to analyze each column to derive a suitable value model. The DA can analyze and identify data relationships and dependencies.

The DMT can provide an interface for accessing the data structure creating and reading the data models e.g. extended schema . In some implementations the DMT uses the VMTs to create the required value models. During the extraction phase the required parameters for the VMTs can be gathered. The VMTs can produce the value models in an appropriate format. The VMTs can also be used for example by the DE to read already produced value models.

The example data generator subsystem includes a data generator DG communicably coupled with data model tools DMTs value model tools VMTs database tools DBTs and configuration tools . The DMTs VMTs DBTs and configuration tools can be the same DMTs VMTs DBTs and configuration tools of the data extraction subsystem in e.g. the data extraction subsystem and the data generation subsystem share the same tools and modules or one or more of the tools can be different e.g. implemented as separated or independent tools or modules .

The DG can use DMTs DBTs and configuration tools to generate synthetic data. The DG can use the VMTs to generate values based on the parameters of the value models and take into account the dependencies during the generation phase.

The configuration tools and can be used by other components and tools of the example data extraction subsystem and the example data generation subsystem for example to read the required parameters from configuration files and for extracting and generating the data respectively. The configuration files and can store for example connection information of source DB and destination DB different thresholds variables set by user and required tables or other information. Such information is not likely to be changed during the runtime therefore can be stored in the configuration files and and loaded once for the sake of efficiency.

At the DE reads user configuration and or environment specification for example from a configuration file e.g. the configuration file in . Then the DE passes the required connection information to the DBTs .

At the DBTs establishes the connection to source database e.g. the source database in and reads the tables from database at . In some implementations if the required tables are defined in configuration file then only those tables will be read otherwise the entire tables in given schema can be read.

At whether a next table exists can be determined for example by the DE . If no next table exists e.g. all required tables have been read the example method stops. Otherwise the example method proceeds to .

At table fields columns can be read for example by the DBTs if any table is read from the database. The columns with all constraints can be passed to the DA .

At the DA analyzes each table column and identifies proper value models for each column and data dependencies. Example techniques for identifying value models and dependencies will be described in greater detail as below.

At the DE puts all the extracted data together and at sends the aggregated or complete set of characteristics e.g. schema constraints value models data dependencies to the DMTs .

At the DMTs creates the data model which matches the extended schema. This process will be continued until all the tables are read. The extracted information can be then written into an extended schema instance.

The Data Analyzer e.g. the DA or component can perform two main tasks finding a suitable value model for every column and finding dependencies in the data at column and table levels. is a sample code illustrating an example method for analyzing existing data in the source database to identify data characteristics relevant for performance evaluation according to an implementation. The example method includes 

There are two example approaches for extracting inter table dependencies ITD or foreign key mappings query driven and data driven. In query driven techniques query workloads can be used to discover the inter table relationships by investigating the existing join operators in the queries. For example a join operation of the form A.x B.y indicates a possible relationship between column x of table A and column y of table B. Here one challenge is to have the right set of queries. In certain databases such as an in memory database one can look into the cache of all the join operators that are used in deployed database views. These operators also indicate potential relationships between tables and their columns On the other hand the data driven techniques can use some heuristics to compare the values of different columns in different tables in order to find the inclusion dependencies. They use either samples or the whole data and perform tests to check if two columns have inclusion Foreign Key dependency or not.

However in some cases looking at existing join operations to find ITDs is not sufficient. Therefore the candidates can be pruned by means of data driven methods. Based on pruning methods while assuming A.x and B.y are sets of unique values of columns A.x and B.y the following Pruning Rules PR can be defined i.e. an ITD between A.x and B.y will not be considered if

1. there is at least one value from column A.x which does not exist in B.y the only exception is null value because if the A.x is nullable then some rows may not have values. Formally if A.x B.y then the candidate will be pruned.

2. all the values of B.y exist in A.x and the size of B is greater than the size of A. Formally if A.x B.y and B A.

3. all values of B.y do not exist in A.x size of A is greater than the size of B and the ratio of unique values of A.x in compare to unique values of B.y is less than a threshold. Formally if A.x B.y A B and

4. all the values of B.y do not exist in A.x size of A is less than or equal to the size of B and the ratio of unique values of A.x in compare to unique values of B.y is less than a threshold. Formally if A.x B.y A B and

At foreign key FK candidates of a table can be identified. For example an FK candidate Column X of Table T can be identified and denoted as T X. In some implementations the candidates can be identified for example by extracting them from join views stored in meta data of the database e.g. an in memory database .

At whether another FK candidate exists between the two tables can be determined. If all FK candidates have been identified the currently identified FKs can be output at and the example method stops afterwards 7. Otherwise the example method proceeds to .

At the set of distinct values of Column Y of table FT is compared to the set of distinct values of Column X of Table T. If the former set is a superset of the latter the method continues to . Otherwise the candidate is discarded pruned and the method continues with checking other candidates at .

At it is determined if the set of all distinct values of Column Y of Table FT our foreign key candidate is a subset of all distinct values of Column X of Table T. If not then it adds Column Y of Table FT as a foreign key for Column X of Table T in and the method moves on to the next candidate. Otherwise the method still needs to do one more check at .

At the sizes row counts of Tables T and FT are determined. If the size of Table T is larger than Table FT then Table FT is considered a foreign key table for Table T at . Otherwise Table T is considered a foreign key table for Table FT.

The extracted information can be stored as a value model in the extended schema specification because ITD columns do not need anything other than the foreign key information. Thus a value model called Foreign Key Value Model FKVM can be built.

In some implementations for some databases foreign keys may not be explicitly defined in schema definitions the example techniques can first find candidates by looking into all JOIN operators in existing database Views and considers join columns as indications for foreign key relationships.

To extract Intra Table Dependencies IATD the below condition can be checked to examine whether two columns. c and care candidates for IATD getDistrictCount concat .getTableSize If this condition is true it means that columns cand cbuild a unique key for the entire table or formally c c tableSize and it can be considered as a case of One to N Dependency OTND . Accordingly the ICDRs for both columns ICDRCC are less than one. In this case the column with more unique values has larger ICDR which determines the values of the other column. Thus it can be enough just to compare the number of unique values of two columns with

It is possible to generalize the OTND to check more columns at once e.g. three columns or more . Alternatively the extraction methods can be improved to find dependencies between pair of columns. Nevertheless there may be some tables that contain primary keys PKs based on more than two columns. In order to resolve the primary key violation problem we can either rely on column pair dependencies like what we explained above or generate values in a way that the concatenations of values of all columns remain unique. The latter solution can be referred to as Multiple Dependency MD .

At the DG reads user configuration and or environment specification for example from a configuration file e.g. the configuration file in .

At the DG pass the user configuration and or environment specification to the DBT and set up the DBT to establish the connection to the destination database e.g. the destination database in .

At the DMT reads the extended schema e.g. the extended schema generated by the data extraction subsystem and builds the data model e.g. a data structure complied with the specifications of the extended schema . The data model is then validated at for internal and external consistency e.g. whether the data model matches the existing tables in the destination database . When the data model is determined as validated at the DMT can create a value model instance for each column.

At after the validation and creation of data model the foreign key FK graph can be built by the DMT . FK graph can be a directed graph in which the tables are nodes and foreign key relation are edges. Tables can be generated in an order based on the FK graph. For example the parent tables can be generated before the children tables because the children tables need to read the foreign key values from parent tables.

At whether a next table exists can be determined. If there exists another table to be generated a table can be picked to generate data. In some implementations the table can be picked in an order which they do not have any dependency on other tables.

At the DG checks whether the selected table exists in the destination DB. If the table does not exist then it can be created with the given specification from the extended schema at otherwise the given specification and the one of the existing table can be compared at . If the two specifications are not the same the example method will end at and the DG will not continue data generation.

If the two specifications are identical at the DG or the DMT can start generating data that comply with the schema constraints value models and data dependencies specified in the extended schema until the required size for each target table is reached determined at .

At a decision can be made to determine whether to write the generated data into the destination DB or into a file that is set by the user. Accordingly the generated can be written into the destination database at or written into a file at .

In some implementations to improve the efficiency of data generation the parallel threads can be used. For example the DG generates a certain number of records or batch and then passes them to the parallel handler to manage it. The parallel handler can write the batch into a file or a database depending on the settings. The batch size could optionally be set by the user in order to reach the maximum efficiency.

In some implementations for every column of each table a Data Source can be created or identified that models the nature of the data in that column e.g. character string values with a given pattern . On top of a Data Source a Value Model can be determined to encapsulate how values are selected from the given data source e.g. randomly or sequential and how and when the data source to be reset. To model data dependencies between columns a Composite Value Model may be created around a value model to control the values generated by the value model so that they comply with the relationship requirements. For instance the Composite Value Model can drive the value model to generate valued according to identified data dependencies of the source data. Row Constraints can capture higher level constraints on multiple rows such as primary key constraint and describe the constraints over the values in a row. A Row Generator can orchestrate all these components to iteratively generate row values for every table.

Row values of the synthetic data can be generated based on the data source the value model the composite value model and a dependency graph a foreign key graph . For example for every table to be generated the appropriate data sources value models and if needed composite value model elements for each column of the table can be created for example by the data generation module or subsystem or . A dependency graph can be then generated to capture the intra table and inter column dependencies. The graph is traversed from zero in degree nodes to generate values for columns. For every row value row constraints can be checked and if validated the row is passed to be inserted into the table. If a row constraint fails depending on the constraint definition and the characteristics of the data in the affected columns new values for the affected columns can be generated.

The generated data for the tables can be either stored in CSV files to be manually imported to the database or inserted directly to the database for example using a Java Database Connectivity JDBC driver. The generated data can be used for example in a test environment for evaluating performances of an application.

Implementations of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry in tangibly embodied computer software or firmware in computer hardware including the structures disclosed in this specification and their structural equivalents or in combinations of one or more of them. Implementations of the subject matter described in this specification can be implemented as one or more computer programs i.e. one or more modules of computer program instructions encoded on a tangible non transitory computer storage medium for execution by or to control the operation of data processing apparatus. Alternatively or in addition the program instructions can be encoded on an artificially generated propagated signal e.g. a machine generated electrical optical or electromagnetic signal that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus. The computer storage medium can be a machine readable storage device a machine readable storage substrate a random or serial access memory device or a combination of one or more of them.

The term data processing apparatus refers to data processing hardware and encompasses all kinds of apparatus devices and machines for processing data including by way of example a programmable processor a computer or multiple processors or computers. The apparatus can also be or further include special purpose logic circuitry e.g. a central processing unit CPU a FPGA field programmable gate array or an ASIC application specific integrated circuit . In some implementations the data processing apparatus and or special purpose logic circuitry may be hardware based and or software based. The apparatus can optionally include code that creates an execution environment for computer programs e.g. code that constitutes processor firmware a protocol stack a database management system an operating system or a combination of one or more of them. The present disclosure contemplates the use of data processing apparatuses with or without conventional operating systems for example LINUX UNIX WINDOWS MAC OS ANDROID IOS or any other suitable conventional operating system.

A computer program which may also be referred to or described as a program software a software application a module a software module a script or code can be written in any form of programming language including compiled or interpreted languages or declarative or procedural languages and it can be deployed in any form including as a stand alone program or as a module component subroutine or other unit suitable for use in a computing environment. A computer program may but need not correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data e.g. one or more scripts stored in a markup language document in a single file dedicated to the program in question or in multiple coordinated files e.g. files that store one or more modules sub programs or portions of code. A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network. While portions of the programs illustrated in the various figures are shown as individual modules that implement the various features and functionality through various objects methods or other processes the programs may instead include a number of sub modules third party services components libraries and such as appropriate. Conversely the features and functionality of various components can be combined into single components as appropriate.

The processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by and apparatus can also be implemented as special purpose logic circuitry e.g. a CPU a FPGA or an ASIC.

Computers suitable for the execution of a computer program can be based on general or special purpose microprocessors both or any other kind of CPU. Generally a CPU will receive instructions and data from a read only memory ROM or a random access memory RAM or both. The essential elements of a computer are a CPU for performing or executing instructions and one or more memory devices for storing instructions and data. Generally a computer will also include or be operatively coupled to receive data from or transfer data to or both one or more mass storage devices for storing data e.g. magnetic magneto optical disks or optical disks. However a computer need not have such devices. Moreover a computer can be embedded in another device e.g. a mobile telephone a personal digital assistant PDA a mobile audio or video player a game console a global positioning system GPS receiver or a portable storage device e.g. a universal serial bus USB flash drive to name just a few.

Computer readable media transitory or non transitory as appropriate suitable for storing computer program instructions and data include all forms of non volatile memory media and memory devices including by way of example semiconductor memory devices e.g. erasable programmable read only memory EPROM electrically erasable programmable read only memory EEPROM and flash memory devices magnetic disks e.g. internal hard disks or removable disks magneto optical disks and CD ROM DVD R DVD RAM and DVD ROM disks. The memory may store various objects or data including caches classes frameworks applications backup data jobs web pages web page templates database tables repositories storing business and or dynamic information and any other appropriate information including any parameters variables algorithms instructions rules constraints or references thereto. Additionally the memory may include any other appropriate data such as logs policies security or access data reporting files as well as others. The processor and the memory can be supplemented by or incorporated in special purpose logic circuitry.

To provide for interaction with a user implementations of the subject matter described in this specification can be implemented on a computer having a display device e.g. a CRT cathode ray tube LCD liquid crystal display LED Light Emitting Diode or plasma monitor for displaying information to the user and a keyboard and a pointing device e.g. a mouse trackball or trackpad by which the user can provide input to the computer. Input may also be provided to the computer using a touchscreen such as a tablet computer surface with pressure sensitivity a multi touch screen using capacitive or electric sensing or other type of touchscreen. Other kinds of devices can be used to provide for interaction with a user as well for example feedback provided to the user can be any form of sensory feedback e.g. visual feedback auditory feedback or tactile feedback and input from the user can be received in any form including acoustic speech or tactile input. In addition a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user for example by sending web pages to a web browser on a user s client device in response to requests received from the web browser.

The term graphical user interface or GUI may be used in the singular or the plural to describe one or more graphical user interfaces and each of the displays of a particular graphical user interface. Therefore a GUI may represent any graphical user interface including but not limited to a web browser a touch screen or a command line interface CLI that processes information and efficiently presents the information results to the user. In general a GUI may include a plurality of user interface UI elements some or all associated with a web browser such as interactive fields pull down lists and buttons operable by the business suite user. These and other UI elements may be related to or represent the functions of the web browser.

Implementations of the subject matter described in this specification can be implemented in a computing system that includes a back end component e.g. as a data server or that includes a middleware component e.g. an application server or that includes a front end component e.g. a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification or any combination of one or more such back end middleware or front end components. The components of the system can be interconnected by any form or medium of wireline and or wireless digital data communication e.g. a communication network. Examples of communication networks include a local area network LAN a radio access network RAN a metropolitan area network MAN a wide area network WAN Worldwide Interoperability for Microwave Access WIMAX a wireless local area network WLAN using for example 802.11 a b g n and or 802.20 all or a portion of the Internet and or any other communication system or systems at one or more locations. The network may communicate with for example Internet Protocol IP packets Frame Relay frames Asynchronous Transfer Mode ATM cells voice video data and or other suitable information between network addresses.

The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client server relationship to each other.

In some implementations any or all of the components of the computing system both hardware and or software may interface with each other and or the interface using an application programming interface API and or a service layer. The API may include specifications for routines data structures and object classes. The API may be either computer language independent or dependent and refer to a complete interface a single function or even a set of APIs. The service layer provides software services to the computing system. The functionality of the various components of the computing system may be accessible for all service consumers via this service layer. Software services provide reusable defined business functionalities through a defined interface. For example the interface may be software written in JAVA C or other suitable language providing data in extensible markup language XML format or other suitable format. The API and or service layer may be an integral and or a stand alone component in relation to other components of the computing system. Moreover any or all parts of the service layer may be implemented as child or sub modules of another software module enterprise application or hardware module without departing from the scope of this disclosure.

While this specification contains many specific implementation details these should not be construed as limitations on the scope of any invention or on the scope of what may be claimed but rather as descriptions of features that may be specific to particular implementations of particular inventions. Certain features that are described in this specification in the context of separate implementations can also be implemented in combination in a single implementation. Conversely various features that are described in the context of a single implementation can also be implemented in multiple implementations separately or in any suitable sub combination. Moreover although features may be described above as acting in certain combinations and even initially claimed as such one or more features from a claimed combination can in some cases be excised from the combination and the claimed combination may be directed to a sub combination or variation of a sub combination.

Similarly while operations are depicted in the drawings in a particular order this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order or that all illustrated operations be performed to achieve desirable results. In certain circumstances multitasking and parallel processing may be advantageous. Moreover the separation and or integration of various system modules and components in the implementations described above should not be understood as requiring such separation and or integration in all implementations and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.

Particular implementations of the subject matter have been described. Other implementations alterations and permutations of the described implementations are within the scope of the following claims as will be apparent to those skilled in the art. For example the actions recited in the claims can be performed in a different order and still achieve desirable results.

Accordingly the above description of example implementations does not define or constrain this disclosure. Other changes substitutions and alterations are also possible without departing from the spirit and scope of this disclosure.

