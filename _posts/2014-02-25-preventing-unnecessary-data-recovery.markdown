---

title: Preventing unnecessary data recovery
abstract: A method that prevents unnecessary data recovery includes receiving, at a data processing device, a status of a resource of a distributed system. When the status of the resource indicates a resource failure, the method includes executing instructions on the data processing device to determine whether the resource failure is correlated to any other resource failures within the distributed system. When the resource failure is correlated to other resource failures within the distributed system, the method includes delaying execution on the data processing device of a remedial action associated with the resource. However, when the resource failure is uncorrelated to other resource failures within the distributed system, the method includes initiating execution on the data processing device of the remedial action associated with the resource.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09223644&OS=09223644&RS=09223644
owner: Google Inc.
number: 09223644
owner_city: Mountain View
owner_country: US
publication_date: 20140225
---
A distributed system generally includes many loosely coupled computers each of which typically includes a computing resource e.g. one or more processors and or storage resources e.g. memory flash memory and or disks . A distributed storage system overlays a storage abstraction e.g. key value store or file system on the storage resources of a distributed system. In the distributed storage system a server process running on one computer can export that computer s storage resources to client processes running on other computers. Remote procedure calls RPC may transfer data from server processes to client processes. Alternatively Remote Direct Memory Access RDMA primitives may be used to transfer data from server hardware to client processes.

One aspect of the disclosure provides a method that includes receiving at a data processing device a status of a resource of a distributed system. When the status of the resource indicates a resource failure the method includes executing instructions on the data processing device to determine whether the resource failure is correlated to any other resource failures within the distributed system. When the resource failure is correlated to other resource failures within the distributed system the method includes delaying execution on the data processing device of a remedial action associated with the resource. However when the resource failure is uncorrelated to other resource failures within the distributed system the method includes initiating execution on the data processing device of the remedial action associated with the resource.

Implementations of the disclosure may include one or more of the following features. In some implementations when the resource failure is correlated to other resource failures within the distributed system the method includes executing the remedial action on the data processing device after a first threshold period of time. In addition when the resource failure is uncorrelated to other resource failures within the distributed system the method includes executing the remedial action on the data processing device after a second threshold period of time. The first threshold period of time is greater than the second threshold period of time. The second threshold period of time may be between about 15 minutes and about 30 minutes. Other threshold periods are possible as well.

The resource may include non transitory memory or computer processors. When the resource includes non transitory memory the method may include initiating data reconstruction as the remedial action for any data stored on the non transitory memory. The data may include chunks of a file where the file is divided into stripes having data chunks and non data chunks. Moreover when the resource includes a computer processor the method includes migrating or restarting a job previously executing on a failed computer processor to an operational computer processor.

In some implementations the method includes determining whether the resource failure is correlated to any other resource failures within the distributed system based on a system hierarchy of the distributed system. The system hierarchy includes system domains where each system domain has an active state or an inactive state. The resource e.g. non transitory memory or computer processor belongs to at least one system domain. The method may further include determining the resource failure as correlated to other resource failures when a statistically significant number of the resources having failures reside in the same system domain or when the resource resides in an inactive system domain.

Another aspect of the disclosure provides a recovery system for a distributed system. The recovery system includes a data processing device in communication with resources of the distributed system. The data processing device receives a status of a resource of the distributed system. When the status of the resource indicates a resource failure the data processing device executes instructions to determine whether the resource failure is correlated to any other resource failures within the distributed system. When the resource failure is correlated to other resource failures within the distributed system the data processing device delays execution of a remedial action associated with the resource. However when the resource failure is uncorrelated to other resource failures within the distributed system the data processing device initiates execution of the remedial action associated with the resource.

In some implementations when the resource failure is correlated to other resource failures within the distributed system the data processing device delays execution of the remedial action associated with the resource for a first threshold period of time. In addition when the resource failure is uncorrelated to other resource failures within the distributed system the data processing device initiates execution of the remedial action associated with the resource after a second threshold period of time. The first threshold period of time is greater than the second threshold period of time. The second threshold period of time may be between about 15 minutes and about 30 minutes.

The resources may include non transitory memory or a computer processor. When the resource includes non transitory memory the data processing device initiates data reconstruction as the remedial action for any data stored on the non transitory memory. The data includes chunks of a file where the file is divided into stripes having data chunks and non data chunks. When the resource includes a computer processor the data processing device migrates or restarts a job previously executing on a failed computer processor to an operational computer processor.

In some implementations the data processing device determines whether the resource failure is correlated to any other resource failures within the distributed system based on a system hierarchy of the distributed system. The system hierarchy includes system domains. Each system domain has an active state or an inactive state. The resources belong to at least one system domain. The data processing device determines the resource failure as correlated to other resource failures when a statistically significant number of the resources having failures reside in the same system domain or when the resource resides in an inactive system domain.

Yet another aspect of the disclosure provides a method for receiving at a data processing device a status of a resource of a distributed system. When the status of the resource indicates a resource failure the method includes executing instructions on the data processing device to determine a correlation between the resource failure and any other resource failures within the distributed system and a time duration of the resource failure. When the resource failure is correlated to other resource failures within the distributed system and the time duration is greater than a first threshold period of time the method includes executing on the data processing device a remedial action associated with the resource. However when the resource failure is uncorrelated to other resource failures within the distributed system and the time duration is greater than a second threshold period of time the method includes executing on the data processing device the remedial action associated with the resource. The first threshold period of time is greater than the second threshold period of time.

In some implementations when the resource includes non transitory memory the method includes initiating data reconstruction as the remedial action for any data stored on the non transitory memory. However when the resource includes a computer processor the method includes migrating or restarting a job previously executing on a failed computer processor to an operational computer processor.

The method may further include determining whether the resource failure is correlated to any other resource failures within the distributed system based on a system hierarchy of the distributed system. The system hierarchy includes system domains where each system domain has an active state or an inactive state and the resource belongs to at least one system domain. In some examples the method may include determining the resource failure as correlated to other resource failures when a statistically significant number of the resources having failures reside in the same system domain or when the resource resides in an inactive system domain.

The system hierarchy may include system levels e.g. first through fourth levels . The first system level corresponds to host machines of data processing devices non transitory memory devices or network interface controllers. Each host machine has a system domain. The second system level corresponds to power deliverers communication deliverers or cooling deliverers of racks housing the host machines. Each power deliverer communication deliverer or cooling deliverer of the rack has a system domain. The third system level corresponds to power deliverers communication deliverers or cooling deliverers of cells having associated racks. Each power deliverer communication deliverer or cooling deliverer of the cell has a system domain. The fourth system level corresponds to a distribution center module of the cells. Each distribution center module has a system domain.

The details of one or more implementations of the disclosure are set forth in the accompanying drawings and the description below. Other aspects features and advantages will be apparent from the description and drawings and from the claims.

Referring to in some implementations a distributed system includes loosely coupled resource hosts e.g. computers or servers each having a computing resource e.g. one or more processors or central processing units CPUs in communication with storage resources e.g. memory flash memory dynamic random access memory DRAM phase change memory PCM and or disks that may be used for caching data. A storage abstraction e.g. key value store or file system overlain on the storage resources allows scalable use of the storage resources by one or more clients . The clients may communicate with the resource hosts through a network e.g. via RPC .

The distributed system may include multiple layers of redundancy where data is replicated and or encoded and stored in multiple data centers. Data centers not shown house computer systems and their associated components such as telecommunications and storage systems . Data centers usually include backup power supplies redundant communications connections environmental controls to maintain a constant temperature and security devices. Data centers can be large industrial scale operations that use a great amount of electricity e.g. as much as a small town . Data may be located in different geographical locations e.g. different cities different countries and different continents . In some examples the data centers or a portion thereof requires maintenance e.g. due to a power outage or disconnecting a portion of the storage system for replacing parts or a system failure or a combination thereof . The data stored in these data centers and in particular the distributed system may be unavailable to users clients during the maintenance period resulting in the impairment or halt of a user s operations. During maintenance or unplanned failures of the distributed system some resource hosts become inactive and unavailable preventing their access by a user client . It is desirable to determine if the unavailable resource host is associated with other unavailable resource hosts to determine whether to recover reconstruct data of the unavailable resource host or wait until the unavailable resource hosts becomes active again. If the unavailability of one resource host is correlated to the unavailability of other resource hosts the unavailable resource host may likely become active again soon so reconstruction of any data associated with the unavailable resource host may not be necessary.

In some implementations the distributed system is single sided eliminating the need for any server jobs for responding to remote procedure calls RPC from clients to store or retrieve data on their corresponding resource hosts and may rely on specialized hardware to process remote requests instead. Single sided refers to the method by which most of the request processing on the resource hosts may be done in hardware rather than by software executed on CPUs of the resource hosts . Rather than having a processor of a resource host e.g. a server execute a server process that exports access of the corresponding storage resource e.g. non transitory memory to client processes executing on the clients the clients may directly access the storage resource through a network interface controller NIC of the resource host . In other words a client process executing on a client may directly interface with one or more storage resources without requiring execution of a routine of any server processes executing on the computing resources . This single sided distributed architecture offers relatively high throughput and low latency since clients can access the storage resources without interfacing with the computing resources of the resource hosts . This has the effect of decoupling the requirements for storage and CPU cycles that typical two sided distributed storage systems carry. The single sided distributed system can utilize remote storage resources regardless of whether there are spare CPU cycles on that resource host furthermore since single sided operations do not contend for server CPU resources a single sided system can serve cache requests with very predictable low latency even when resource hosts are running at high CPU utilization. Thus the single sided distributed system allows higher utilization of both cluster storage and CPU resources than traditional two sided systems while delivering predictable low latency.

In some implementations the distributed system includes a storage logic portion a data control portion and a data storage portion . The storage logic portion may include a transaction application programming interface API e.g. a single sided transactional system client library that is responsible for accessing the underlying data for example via RPC or single sided operations. The data control portion may manage allocation and access to storage resources with tasks such as allocating storage resources registering storage resources with the corresponding network interface controller setting up connections between the client s and the resource hosts handling errors in case of machine failures etc. The data storage portion may include the loosely coupled resource hosts 

The distributed system may store data in dynamic random access memory DRAM and serve the data from the remote hosts via remote direct memory access RDMA capable network interface controllers . A network interface controller also known as a network interface card network adapter or LAN adapter may be a computer hardware component that connects a computing resource to the network . Both the resource hosts and the client may each have a network interface controller for network communications. A host process executing on the computing processor of the resource host registers a set of remote direct memory accessible regions of the memory with the network interface controller . The host process may register the remote direct memory accessible regions of the memory with a permission of read only or read write. The network interface controller of the resource host creates a client key for each registered memory region 

The single sided operations performed by the network interface controllers may be limited to simple reads writes and compare and swap operations none of which may be sophisticated enough to act as a drop in replacement for the software logic implemented by a traditional cache server job to carry out cache requests and manage cache policies. The transaction API translates commands such as look up or insert data commands into sequences of primitive network interface controller operations. The transaction API interfaces with the data control and data storage portions of the distributed system .

The distributed system may include a co located software process to register memory for remote access with the network interface controllers and set up connections with client processes . Once the connections are set up client processes can access the registered memory via engines in the hardware of the network interface controllers without any involvement from software on the local CPUs of the corresponding resource hosts .

Referring to in some implementations the distributed system includes multiple cells each cell including resource hosts a curator in communication with the resource hosts and a data processing device in communication with the resource hosts and the curator . The curator e.g. process may execute on a computing processor e.g. server having a non transitory memory connected to the network and manage the data storage e.g. manage a file system stored on the resource hosts control data placements and or initiate data recovery. Moreover the curator may track an existence and storage location of data on the resource hosts . Redundant curators are possible. In some implementations the curator s track the striping of data across multiple resource hosts and the existence and or location of multiple copies of a given stripe for redundancy and or performance. In computer data storage data striping is the technique of segmenting logically sequential data such as a file in a way that accesses of sequential segments are made to different physical storage devices e.g. cells and or resource hosts . Striping is useful when a processing device requests access to data more quickly than a storage device can provide access. By performing segment accesses on multiple devices multiple segments can be accessed concurrently. This provides more data access throughput which avoids causing the processor to idly wait for data accesses.

In some implementations the transaction API interfaces between a client e.g. with the client process and the curator . In some examples the client communicates with the curator through one or more remote procedure calls RPC . In response to a client request the transaction API may find the storage location of certain data on resource host s and obtain a key that allows access to the data . The transaction API communicates directly with the appropriate resource hosts via the network interface controllers to read or write the data e.g. using remote direct memory access . In the case that a resource host is non operational or the data was moved to a different resource host the client request fails prompting the client to re query the curator .

Referring to in some implementations the curator stores and manages file system metadata . The metadata may include a file map that maps files to file descriptors . The curator may examine and modify the representation of its persistent metadata . The curator may use three different access patterns for the metadata read only file transactions and stripe transactions.

Referring to data may be one or more files where each file has a specified replication level and or error correcting code . The curator may divide each file into a collection of stripes with each stripe being encoded independently from the remaining stripes . For a replicated file each stripe is a single logical chunk that the curator replicates as stripe replicas and stores on multiple storage resources . In that scenario a stripe replica is also referred to as a chunk . For an erasure encoded file each stripe consists of multiple data chunks and non data chunks e.g. code chunks that the curator places on multiple storage resources where the collection of data chunks and non data chunks forms a single code word. In general the curator may place each stripe on storage resources independently of how the other stripes in the file are placed on the storage resources . The error correcting code adds redundant data or parity data to a file so that the file can later be recovered by a receiver even when a number of errors up to the capability of the code being used were introduced. The error correcting code is used to maintain data integrity in storage devices to reconstruct data for performance latency or to more quickly drain machines.

As shown in each stripe is divided into data chunks and non data chunks based on an encoding level e.g. Reed Solomon Codes nested codes layered codes or other erasure coding. The non data chunks may be code chunks e.g. for Reed Solomon codes . In other examples the non data chunks may be code check chunks CC word check chunks WC and code check word check chunks CCWC for layered or nested coding .

A data chunk is a specified amount of data . In some implementations a data chunk is a contiguous portion of data from a file . In other implementations a data chunk is one or more non contiguous portions of data from a file . For example a data chunk can be 256 bytes or other units of data .

A damaged chunk e.g. data chunk or non data chunk is a chunk containing one or more errors. Typically a damaged chunk is identified using an error detecting code . For example a damaged chunk can be completely erased e.g. if the chunk was stored in a hard drive destroyed in a hurricane or a damaged chunk can have a single bit flipped. A healthy chunk is a chunk that is not damaged. A damaged chunk can be damaged intentionally for example where a particular resource host is shut down for maintenance. In that case damaged chunks can be identified by identifying chunks that are stored at resource hosts that are being shut down.

The non data chunks of a file include an error correcting code chunk . The error correcting code chunks include a chunk of data based on one or more data chunks . In some implementations each code chunk is the same specified size e.g. 256 bytes as the data chunks . The code chunks are generated using an error correcting code e.g. a Maximal Distance Separable MDS code. Examples of MDS codes include Reed Solomon codes. Various techniques can be used to generate the code chunks . In general any error correcting code can be used that can reconstruct d data chunks from any set of d unique healthy chunks either data chunks or code chunks .

A codeword is a set of data chunks and code chunks based on those data chunks . If an MDS code is used to generate a codeword containing d data chunks and n code chunks then all of the chunks data or code can be reconstructed as long as any d healthy chunks data or code are available from the codeword.

Referring to the data processing device may determine a system hierarchy of the distributed system to identify the levels e.g. levels 1 4 at which maintenance or failure may occur without affecting a user s access to stored data . Maintenance may include power maintenance cooling system maintenance networking maintenance updating or replacing parts or other maintenance or power outage affecting the distributed system . The system hierarchy may include maintenance units system domains for the various components and resources of the distributed system . The system domains may be overlapping or non overlapping depending on the nature of the components. For example a power domain may not align with a networking domain.

The data processing device may determine or receive a system hierarchy of the distributed system to identify the levels e.g. levels 1 4 at which maintenance may occur without affecting a user s access to stored data . Maintenance or failures strict hierarchy non strict hierarchy may include power maintenance failure cooling system maintenance failure networking maintenance failure updating or replacing parts or other maintenance or power outage affecting the distributed system . Maintenance may be scheduled and in some examples an unscheduled system failure may occur.

The system hierarchy includes system levels e.g. levels 1 4 with maintenance units system domains spanning one or more system levels 1 4. Each system domain has an active state or an inactive state. A distribution center module includes one or more cells and each cell includes one or more racks of resource hosts . Each cell also includes cell cooling cell power e.g. bus ducts and cell level networking e.g. network switch es . Similarly each rack includes rack cooling rack power e.g. bus ducts and rack level networking e.g. network switch es .

The system levels may include first second third and fourth system levels 1 4. The first system level 1 corresponds to resource hosts or host machines of data processing devices non transitory memory devices or network devices e.g. NICs . Each host machine resource host has a system domain . The second system level 2 corresponds to racks and cooling deliverers power deliverers e.g. bus ducts or communication deliverers e.g. network switches and cables of the host machines at the rack level. Each rack or rack level cooling deliverer power deliverer or communication deliverer has a system domain . The third system level 3 corresponds to any cells of the distribution center module and the cell cooling cell power or cell level networking supplied to the associated racks . Each cell or cell cooling cell power or cell level networking has a system domain . The fourth system level 4 corresponds to the distribution center module . Each distribution center module has a system domain .

The data processing device determines based on the mappings of the hierarchy components which resource hosts are inactive when a hierarchy component undergoes maintenance. Once the data processing device maps the system domains to the resource hosts and therefore to their corresponding processor resources and storage resources the data processing device determines a highest level e.g. levels 1 4 at which maintenance can be performed while maintaining processor or data availability.

A system domain includes a hierarchy component undergoing maintenance and any hierarchy components depending therefrom. Therefore when one hierarchy component undergoes maintenance that hierarchy component is inactive and any other hierarchy components in the system domain of the hierarchy component are also inactive. For example when a resource host is undergoes maintenance a level 1 system domain which includes the storage device the data processor and the NIC is in the inactive state. When a rack undergoes maintenance a level 2 system domain which includes the rack and any resource hosts depending from the rack is in the inactive state. When a cell for example to any one of the cell cooling component the bus duct and or the network switch of the cell component undergoes maintenance a level 3 system domain which includes the cell and any hierarchy components in levels 1 and 2 that depend from the cell component is in the inactive state. Finally when a distribution center module undergoes maintenance a level 4 system domain which includes the distribution center module and any hierarchy components in levels 1 to 3 depending from the distribution center module is in the inactive state.

In some examples as shown in a non strict hierarchy component may have dual feeds i.e. the hierarchy component depends on two or more other hierarchy components . For example a cell may have a feed from two distribution center modules and or a rack may have a dual feed from two cells . As shown a level 3 system domain may include two racks where the second rack includes two feeds from two cells . Therefore the second rack is part of two system domains and . Therefore the lower levels of the system hierarchy are maintained without causing the loss of the higher levels of the system hierarchy . This causes a redundancy in the system which allows for data accessibility. In particular the distribution center module may be maintained without losing any of the cells depending from it. In some examples the racks include a dual powered rack that allows the maintenance of the bus duct without losing power to the dual powered racks depending from it. In some examples system domains that may be maintained without causing outages are ignored when distributing chunks to allow for maintenance however the ignored system domains may be included when distributing the chunks since an unplanned outage may still cause the loss of chunks .

In some examples a cooling device such as the cell cooling and the rack cooling are used to cool the cell components and the racks respectively. The cell cooling component may cool one or multiple cell components . Similarly a rack cooling component may cool one or more racks . The data processing device stores the association of the resource hosts with the cooling devices i.e. the cell cooling and the rack cooling . In some implementations the data processing device considers all possible combinations of maintenance that might occur within the storage system to determine a system hierarchy or a combination of maintenance hierarchies . For example a system hierarchy where one or more cooling devices fail or a system hierarchy where the network devices fail or a system hierarchy where the power distribution fails.

Therefore when a hierarchy component in the storage system undergoes maintenance that hierarchy component and any hierarchy components that are mapped to or depending from that hierarchy component are in an inactive state. A hierarchy component in an inactive state is inaccessible by a user while a hierarchy component in an active state is accessible by a user allowing the user to process access data stored supported maintained by that hierarchy component . As previously mentioned during the inactive state a user is incapable of accessing the resource host associated with the system domains undergoing maintenance and therefore the client is incapable of accessing the files i.e. chunks which include stripe replicas data chunks and non data chunks .

In some implementations the data processing device restricts a number of chunks distributed to storage devices and or processing jobs distributed to data processors of any one system domain e.g. based on the mapping of the hierarchy components . Therefore if a level 1 system domain is inactive the curator maintains accessibility to the file or stripe although some chunks may be inaccessible. In some examples for each file or stripe the data processing device determines a maximum number of chunks that may be placed within any storage device within a single system domain so that if a system domain associated with the storage device storing chunks for a file is undergoing maintenance a client may still retrieve the file . The maximum number of chunks ensures that the data processing device is capable of reconstructing the file although some chunks may be unavailable. In some examples the maximum number of chunks is set to a lower threshold to accommodate for any system failures while still being capable of reconstructing the file from the chunks . When the data processing device places chunks on the storage devices the data processing device ensures that within a stripe no more than the maximum number of chunks are inactive when a single system domain undergoes maintenance. Moreover the data processing device may also restrict the number of processing jobs on a data processor of a resource host within a system domain e.g. based on the mapping of the hierarchy components . Therefore if a level 1 system domain is inactive the data processing device maintains accessibility to the processing jobs e.g. by migrating or restarting the jobs on other data processing devices that are available although some of the processors of the resource hosts are inactive.

Referring to in some implementations and as previously discussed the system may undergo maintenance or unplanned failures which cause one or more system domains to be in an inactive state . The inactive state may include two phases a down phase and a dead phase . The down phase is a transition phase between the active state and the dead phase of the inactive state . During the down phase the system waits for the hierarchy component to go back to the active state however during the dead phase the system considers the data stored on the storage devices and the processing jobs being processed on the data processors as lost data and lost processes and begins to reconstruct the data or re initiate the processes of the jobs .

Referring to in some implementations the data processing device monitors the system domains including the hierarchy components of the system domains and receives a status of the hierarchy components e.g. active state or inactive state . In some examples the data processing device monitors the hierarchy components periodically. In other examples the hierarchy components send the data processing device a status update when a change in a status of one of the hierarchy components occurs. When the system is not undergoing maintenance or any system failures the hierarchy components of the system domains are in an active state .

Therefore at decision block the data processing device determines if one of the hierarchy components remains in the active state or is experiencing a failure i.e. the hierarchy component is no longer in the active state . If the data processing device determines that the hierarchy component remains in the active state i.e. no failure occurred the data processing device maintains the status of the hierarchy component as active .

If the data processing device determines that the hierarchy component is not in an active state i.e. a failure occurred then the data processing device updates the status of the hierarchy component to a component down state the component is to be a down component in the down state . As previously mentioned the down state is a transition between the active state and the dead phase . Therefore at decision block the data processing device determines if the failure of the down component is correlated to one or more other down components having failures within the distributed system . If so then the data processing device maintains the down state status of the down component .

In some implementations at decision block if the data processing device determines that the failure of the down component is correlated to one or more other down components within the distributed storage system then the data processing device determines at block if the down component is inactive for a period of time Tthat is greater than a threshold period of time T. If the inactive period of time Tis greater than the threshold period of time T T T then the data processing device updates the status of the down component to an inactive dead state and the data processing device initiates execution of a remedial action associated with the resource lost at block . The threshold period of time Tat block delays the transition of the down component from the down state to the dead state therefore delaying execution of the remedial action when the failure of the down component is associated with failures of other down components of the distributed system . This provides a delay in resource utilization e.g. storage device and processor during the remedial action. In some examples the system avoids reconstructing many bytes terabytes of data unnecessarily and is able to run the data centers at a higher user load thereby extending the data center s effective capacity. The data processing device determines a new correlation decision each time at block . As such a correlated failure in one decision cycle at block may become an uncorrelated failure at a subsequent decision cycle at block e.g. a few minutes later .

As previously explained a resource host includes storage devices for storing chunks of a file and processors for executing jobs . Therefore a remedial action may be a storage remedial action or a processor remedial action. During a storage remedial action for the storage devices the data processing device reconstructs a file or more specifically reconstructs the stripes of a file where each stripe includes chunks . Therefore the data processing device reconstructs the missing chunks of a stripe using healthy chunks of the stripe . A processor remedial action for a data processor of a resource host is different than the storage remedial action for the storage devices . During a processor remedial action the data processing device migrates or restarts a job that was previously executing on a down data processor e.g. failed data processor in an inactive state to an operational data processor in an active state . In some examples processor remedial actions are not delayed because the resource cost of a processor remedial action is low compared to a storage remedial action.

Referring back to decision block when the data processing device determines that the failure of the down component is not correlated to another one or more down components then the data processing device at decision block determines if the down component has been inactive for a period of time Tgreater than a dead phase threshold time T T T then the data processing device updates the status of the down component to a dead state and the data processing device executes a remedial action associated with the resource to recover the data or processing job that was lost on the down component at block that transitioned to the dead state

In some implementations the system e.g. the data processing device determines if the down component is correlated to any other down components within the distributed system . A correlated failure may be a failure of a down component within the distributed system more specifically the system hierarchy that is similar to other failures experienced by other down components . The failures may include failures of components within the same level of the system hierarchy failures of components within the same vicinity failures of components associated with the same system domain same type of component or any other similarity of the component .

Referring to in some implementations the system determines if a down component is part of a larger correlated failure by determining if the down component is in a known inactive system domain at block . If the down component is in a known inactive system domain down state then the data processing device determines that the down component is part of a larger correlated failure and therefore the data processing device delays transitioning the down component from the down state to the dead state because the down component is associated with failures of other down components of the distributed system . However if at block the down component is not associated with a known inactive system domain the system considers the next block block . For example referring back to if the down component is a resource host e.g. the NIC the storage device or the data processor of a system hierarchy and the resource host becomes inactive or down then the data processing device determines if the system domains that includes the resource host of system domain is inactive. Therefore the data processing device determines if the level 2 system domain that includes the resource host or the cell component that includes the resource host or the distribution center module are in the inactive state. If one of those system domains includes the resource host that is experiencing a failure then that resource host is correlated to failures of other down components because a failure in any one of those components results in a failure of all the resource hosts that depend on that down component .

Referring back to at block the data processing device determines if the down component is associated with other down components in the same system domain . If so then the data processing device determines that the down component is part of a larger correlated failure and therefore the data processing device delays transitioning the down component from the down state to the dead state because the down component is associated with failures of other down components of the distributed system . If not then the data processing device moves to the next block . For example referring back to if a resource host in level 1 is in the down state the data processing device considers other resource hosts that are in level 1 and depend from the same system hierarchy as the down resource host . In some implementations the data processing device determines if a statistically significant number of down components having failures reside in the same system domain as the down component . A statistically significant number is the probability that an effect is not likely due to just change alone. The statistically significant number is considered important because it has been predicted as unlikely to have occurred by chance alone.

Referring back to at block the data processing device determines if the down component is in the same vicinity e.g. physical location as other down components of the distributed system . For example the data processing device may consider the proximity of storage devices within a rack . If so then the system determines that the down component is part of a larger correlated failure delaying the transition from the down state to the dead state . However if the down component is not part of a larger correlated failure then the system moves to block .

At block the data processing device determines if the down component is the same type as other down components . If so then the data processing device may determine that the down component is part of a larger correlated failure and therefore the data processing device delays transitioning the down component from the down state to the dead state . However if not then the data processing device moves to block . The data processing device may use this test in combination with other tests to determine the correlation of failures. For example referring back to if a storage host is in the down state the data processing device determines if other components are also in the down state and if those components are resource hosts as well the down resource host may be part of a larger correlated failure. However if a resource host is in the down state but another resource host sharing the same system hierarchy is not in the down state then the failure is not part of a larger correlated failure and might be due to a failure in the resource host itself.

Referring to in some implementations a method includes receiving at a data processing device a status of a resource e.g. resource hosts including data processors and storage devices of a distributed system . When the status of the resource indicates a resource failure due to maintenance or system failure the method includes executing instructions on the data processing device to determine whether the resource failure is correlated to any other resource failures within the distributed system . When the resource failure is correlated to other resource failures within the distributed system the method includes delaying execution on the data processing device of a remedial action associated with the resource . However when the resource failure is uncorrelated to other resource failures within the distributed system the method includes initiating execution on the data processing device of the remedial action associated with the resource . As previously discussed a remedial action may be a storage remedial action or a processor remedial action. During a storage remedial action for the storage devices the data processing device reconstructs a file or more specifically reconstructs the stripes of a file where each stripe includes chunks . Therefore the data processing device reconstructs the missing chunks of a stripe using healthy chunks of the stripe . A processor remedial action for a data processor of a resource host is different than the storage remedial action for the storage devices . During a processor remedial action the data processing device migrates or restarts a job that was previously executing on a down data processor e.g. failed data processor in an inactive state to an operational data processor in an active state .

In some implementations when the resource failure is correlated to other resource failures within the distributed system the method includes executing the remedial action on the data processing device after a first threshold period of time T. In addition when the resource failure is uncorrelated to other resource failures within the distributed system the method includes executing the remedial action on the data processing device after a second threshold period of time T. The first threshold period of time Tis greater than the second threshold period of time T which causes a delay in the execution of the remedial action and provides a delay in the resource utilization e.g. storage device or processor during the remedial action. The second threshold period of time Tmay be between about 15 minutes and about 30 minutes.

The resource may include non transitory memory or computer processors . When the resource is non transitory memory the method may include initiating data reconstruction as the remedial action for any data stored on the non transitory memory . The data may include chunks of a file where the file is divided into stripes having data chunks and non data chunks as discussed with reference to . Moreover when the resource includes a data processor the method includes migrating or restarting a job previously executing on a failed computer processor to an operational computer processor .

In some implementations the method includes determining whether the resource failure is correlated to any other resource failures within the distributed system based on a system hierarchy of the distributed system e.g. as described with reference to . The system hierarchy includes system domains where each system domain has an active state or an inactive state . The resource e.g. non transitory memory or computer processor belongs to at least one system domain . The method may further include determining the resource failure as correlated to other resource failures when a statistically significant number of the resources having failures reside in the same system domain or when the resource resides in an inactive system domain .

Referring to in some implementations a method includes receiving at a data processing device a status of a resource e.g. resource hosts includes data processors and storage devices of a distributed system . When the status of the resource indicates a resource failure due to maintenance or system failure the method includes executing instructions on the data processing device to determine a correlation between the resource failure and any other resource failures within the distributed system and a time duration of the resource failure T. When the resource failure is correlated to other resource failures within the distributed system and the time duration Tis greater than a first threshold period of time T the method includes executing on the data processing device a remedial action associated with the resource . However when the resource failure is uncorrelated to other resource failures within the distributed system and the time duration Tis greater than a second threshold period of time T the method includes executing on the data processing device the remedial action associated with the resource . The first threshold period of time Tis greater than the second threshold period of time T.

In some implementations when the resource includes non transitory memory the method includes initiating data reconstruction as the remedial action for any data stored on the non transitory memory . However when the resource includes a computer processor the method includes migrating or restarting a job previously executing on a failed computer processor to an operational computer processor .

The method may further include determining whether the resource failure is correlated to any other resource failures within the distributed system based on a system hierarchy of the distributed system as discussed with reference to FIGS. A and B . The system hierarchy includes system domains where each system domain has an active state or an inactive state and the resource belongs to at least one system domain . In some examples the method may include determining the resource failure as correlated to other resource failures when a statistically significant number of the resources having failures reside in the same system domain or when the resource resides in an inactive system domain .

Various implementations of the systems and techniques described here can be realized in digital electronic circuitry integrated circuitry specially designed ASICs application specific integrated circuits computer hardware firmware software and or combinations thereof. These various implementations can include implementation in one or more computer programs that are executable and or interpretable on a programmable system including at least one programmable processor which may be special or general purpose coupled to receive data and instructions from and to transmit data and instructions to a storage system at least one input device and at least one output device.

These computer programs also known as programs software software applications or code include machine instructions for a programmable processor and can be implemented in a high level procedural and or object oriented programming language and or in assembly machine language. As used herein the terms machine readable medium and computer readable medium refer to any computer program product apparatus and or device e.g. magnetic discs optical disks memory Programmable Logic Devices PLDs used to provide machine instructions and or data to a programmable processor including a machine readable medium that receives machine instructions as a machine readable signal. The term machine readable signal refers to any signal used to provide machine instructions and or data to a programmable processor.

Implementations of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry or in computer software firmware or hardware including the structures disclosed in this specification and their structural equivalents or in combinations of one or more of them. Moreover subject matter described in this specification can be implemented as one or more computer program products i.e. one or more modules of computer program instructions encoded on a computer readable medium for execution by or to control the operation of data processing apparatus. The computer readable medium can be a machine readable storage device a machine readable storage substrate a memory device a composition of matter affecting a machine readable propagated signal or a combination of one or more of them. The terms data processing apparatus computing device and computing processor encompass all apparatus devices and machines for processing data including by way of example a programmable processor a computer or multiple processors or computers. The apparatus can include in addition to hardware code that creates an execution environment for the computer program in question e.g. code that constitutes processor firmware a protocol stack a database management system an operating system or a combination of one or more of them. A propagated signal is an artificially generated signal e.g. a machine generated electrical optical or electromagnetic signal that is generated to encode information for transmission to suitable receiver apparatus.

A computer program also known as an application program software software application script or code can be written in any form of programming language including compiled or interpreted languages and it can be deployed in any form including as a stand alone program or as a module component subroutine or other unit suitable for use in a computing environment. A computer program does not necessarily correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data e.g. one or more scripts stored in a markup language document in a single file dedicated to the program in question or in multiple coordinated files e.g. files that store one or more modules sub programs or portions of code . A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.

The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by and apparatus can also be implemented as special purpose logic circuitry e.g. an FPGA field programmable gate array or an ASIC application specific integrated circuit .

Processors suitable for the execution of a computer program include by way of example both general and special purpose microprocessors and any one or more processors of any kind of digital computer. Generally a processor will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data. Generally a computer will also include or be operatively coupled to receive data from or transfer data to or both one or more mass storage devices for storing data e.g. magnetic magneto optical disks or optical disks. However a computer need not have such devices. Moreover a computer can be embedded in another device e.g. a mobile telephone a personal digital assistant PDA a mobile audio player a Global Positioning System GPS receiver to name just a few. Computer readable media suitable for storing computer program instructions and data include all forms of non volatile memory media and memory devices including by way of example semiconductor memory devices e.g. EPROM EEPROM and flash memory devices magnetic disks e.g. internal hard disks or removable disks magneto optical disks and CD ROM and DVD ROM disks. The processor and the memory can be supplemented by or incorporated in special purpose logic circuitry.

To provide for interaction with a user one or more aspects of the disclosure can be implemented on a computer having a display device e.g. a CRT cathode ray tube LCD liquid crystal display monitor or touch screen for displaying information to the user and optionally a keyboard and a pointing device e.g. a mouse or a trackball by which the user can provide input to the computer. Other kinds of devices can be used to provide interaction with a user as well for example feedback provided to the user can be any form of sensory feedback e.g. visual feedback auditory feedback or tactile feedback and input from the user can be received in any form including acoustic speech or tactile input. In addition a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user for example by sending web pages to a web browser on a user s client device in response to requests received from the web browser.

One or more aspects of the disclosure can be implemented in a computing system that includes a backend component e.g. as a data server or that includes a middleware component e.g. an application server or that includes a frontend component e.g. a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification or any combination of one or more such backend middleware or frontend components. The components of the system can be interconnected by any form or medium of digital data communication e.g. a communication network. Examples of communication networks include a local area network LAN and a wide area network WAN an inter network e.g. the Internet and peer to peer networks e.g. ad hoc peer to peer networks .

The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client server relationship to each other. In some implementations a server transmits data e.g. an HTML page to a client device e.g. for purposes of displaying data to and receiving user input from a user interacting with the client device . Data generated at the client device e.g. a result of the user interaction can be received from the client device at the server.

While this specification contains many specifics these should not be construed as limitations on the scope of the disclosure or of what may be claimed but rather as descriptions of features specific to particular implementations of the disclosure. Certain features that are described in this specification in the context of separate implementations can also be implemented in combination in a single implementation. Conversely various features that are described in the context of a single implementation can also be implemented in multiple implementations separately or in any suitable sub combination. Moreover although features may be described above as acting in certain combinations and even initially claimed as such one or more features from a claimed combination can in some cases be excised from the combination and the claimed combination may be directed to a sub combination or variation of a sub combination.

Similarly while operations are depicted in the drawings in a particular order this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order or that all illustrated operations be performed to achieve desirable results. In certain circumstances multi tasking and parallel processing may be advantageous. Moreover the separation of various system components in the embodiments described above should not be understood as requiring such separation in all embodiments and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.

A number of implementations have been described. Nevertheless it will be understood that various modifications may be made without departing from the spirit and scope of the disclosure. Accordingly other implementations are within the scope of the following claims. For example the actions recited in the claims can be performed in a different order and still achieve desirable results.

