---

title: Migration of complex applications within a hybrid cloud environment
abstract: A system and methods for the migration of complex computer applications and the workloads comprising them between physical, virtual, and cloud servers that span a hybrid cloud environment comprising private local and remote customer data centers and public cloud data centers, without modification to the applications, their operational environments, or user access procedures. A virtual network manager securely extends the subnets and VLANS within the customer's various data center across the distributed, hybrid environment using overlay networks implemented with virtual network appliances at nodes of the overlay network. A server migrater migrates individual workloads of servers used by the complex application from one pool of server resources to another. A migration manager application provides a control interface, and also maps and manages the resources of the complex application, the hybrid environment, and the virtual network spanning the hybrid cloud environment.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09461969&OS=09461969&RS=09461969
owner: RACEMI, INC.
number: 09461969
owner_city: Atlanta
owner_country: US
publication_date: 20141001
---
This application claims the benefit under 35 U.S.C. 119 e of U.S. Provisional Patent Application No. 61 885 324 filed Oct. 1 2013 entitled Migration of Complex Applications Within a Hybrid Cloud Environment incorporated herein by reference in its entirety.

The present disclosure relates to complex computer applications that consist of a plurality of communicating workloads and the methods and apparatuses for migrating them between physical virtual private cloud and public cloud servers.

A server image is the logical embodiment of a server computer a k a a server that contains all of the programs and data needed to provide one or more services by the server computer to its client computers typically including but not limited to a kernel and operating system device drivers that are normally associated with the hardware related components of the server running the image application software and data and configuration settings including those associated with the network and storage environments surrounding the server. A workload is the embodiment of a single server running an image. The image can run on a dedicated physical server in which case the workload will have direct access to all of the server s available resources. Or the image can run on a virtual server provided by a hypervisor host e.g. VMware ESX or Citrix XenServer or public cloud infrastructure e.g. Amazon EC2 Rackspace in which case the hypervisor or cloud infrastructure regulates access to a pool of resources that are shared between all its virtual servers.

Many computer servers are now being provided in hybrid cloud environments. A hybrid cloud environment is a combination of computer servers provided in private data centers and in public clouds such as provided by Amazon EC2 Rackspace and other entities that provide computing services and communications for a wide variety of customers and applications. A hybrid cloud environment offers varied opportunities to implement workloads on servers through selection of computers computing services data storage application hosting localized and remote data centers and other computing functions.

There are many factors to consider when choosing a server to run a workload including performance cost security robustness of the infrastructure geographic location etc. Over time the optimal server for a workload might change due to changes in the workload s life cycle development test production etc. the number of clients accessing the workload the availability of more efficient physical resources or the need to change its geographical location. Over its life time the total cost of ownership for a workload would be minimized if there were a way to migrate it such that it was always running on the most cost effective resource that met its current needs.

Migrating a workload from one server to another can be a difficult task and is more difficult in a hybrid cloud environment. While running on the original source server the image is configured for the hardware real or virtual provided by that server and the network and storage environments surrounding that server. After migration the new server might differ in regard to hardware or environmental configuration such that simply copying the image from the source to the target does not result in a functional workload. United States Patent Application Publication No. US 2013 0290542 Server Image Migrations into Public and Private Cloud Infrastructures Charles T. Watt et. al. discusses the issues associated with server image migration and discloses methods and apparatuses for migrating a single server image between physical virtual and cloud servers. These methods provide great benefit for applications that can be implemented within a single workload but they can fail when migrating complex computer applications.

 Complex computer applications as the term is used herein are applications that require multiple computer programs often on different computers physical or virtual that communicate with each other in a networked arrangement such as a local area network or within a data center to exchange data and provide services and sometimes with other external computers in a broader networked arrangement such as the Internet . Complex applications are often implemented using multiple workloads each providing a service that is accessible to the other workloads via a network connection. shows an example of a typical Internet complex computer application. In the example application an end user accesses the application using a web browser which connects to a web server workload WSW via a firewall . The WSW connects to an application server workload ASW to access any application logic. The ASW connects to a database workload DBW to store or retrieve any application data. To provide secure isolation of the workloads the communications between the workloads occur on separate local area networks LANs either physical LANs or virtual LANs VLAN . The connection between the end user and the firewall occurs over the Internet . The connection between the firewall and the WSW occurs over LAN 1 . The connection between the WSW and the ASW occurs over LAN 2 . And the connection between the ASW and the DBW occurs over LAN 3 . Either or any of LAN 1 LAN 2 or LAN 3 could be a VLAN. To facilitate routing in this environment each LAN uses a separate subnet. Thus the WSW and ASW are each configured with two network addresses one for each subnet to which the workload connects. Each workload is configured to communicate with peer services using the visible network address of the peer. In the example the WSW accesses the ASW by connecting to IP address 10.1.1.20.

When migrating one or more workloads of a complex application onto new resources it is often not possible to replicate the original VLANs subnets and addresses in the new environment. For example the cloud servers provided by many public cloud vendors come with just a single network interface and a single network address that is arbitrarily specified by the vendor based upon the specific internal details of the cloud s infrastructure. After migrating one or more servers into such an environment the complex application will no longer function because the individual workloads will still be configured for the original network addresses. This problem is illustrated in where the WSW has been migrated into a new environment. Its network interfaces are now on LANs and . If the WSW keeps its original network addresses there will be no way to route connections back to its original LAN 1 and LAN 2 . To restore the application the WSW will need to be reconfigured to use new IP addresses that work on its new subnets and it will need its routing set to access LANs 1 and 2. Even though they haven t moved the firewall and AWS will also need to be reconfigured in order to access the WSW using its new addresses and might need to add route table entries pointing to LANs and .

In practice the number of workloads comprising a complex application can be quite large. The task of reconfiguring all the workloads after migration can be quite time consuming and expensive and sometimes even impossible if the relevant knowledge is no longer available on how to reconfigure the application. Worse yet the full set of workloads comprising the application might not be known. Many common services such as authentication directory and file storage are often not associated with an application because they are shared with other applications and considered common infrastructure. But if the complex application is migrated into another environment that does not have access to these ancillary services it will not function correctly.

An overlay network uses software that sits on top of a shared network infrastructure to transparently link portions of that shared infrastructure into an isolated secure functional LAN. An overlay network is a computer network that is a separate network entity either physical or virtual built on top of another network such as the Internet. For example a company s virtual private network VPN which has nodes within multiple physical facilities connected by a public network such as the Internet is a form of overlay network since users on the VPN are transparently connected to each other. Overlay networks are often used to provide multi tenant use of a shared network infrastructure. They are also used to extend a LAN across multiple network infrastructures.

Further details about aspects of the exemplary overlay network and operations thereof can be found below in connection with the discussion of .

For the foregoing and other reasons it is difficult and time consuming to migrate such complex applications within data centers and public cloud data centers even for skilled computer and information technology IT workers. What is needed is a solution that creates a hybrid cloud environment out of the physical virtual and cloud servers and associated network resources in a plurality of private data centers and a plurality of public cloud data centers and allows efficient migration of complex applications to different computing resources within the hybrid cloud environment as new and better computing resources are brought to market.

According to one aspect there is disclosed an improved methods and system for migrating complex applications within a hybrid cloud environment comprising 1 a network virtualization manager also called a virtual network manager to create secure overlay networks that transparently extend across the hybrid environment such that any workloads configured to communicate with each other over a LAN can do so regardless of where they reside in the hybrid environment 2 a workload migrater that can migrate workloads between physical virtual or cloud servers throughout the hybrid environment 3 an image library that can be used to store server images such that they can be deployed at some later time and 4 a migration manager application that provides a control interface that maps and manages the resources of the hybrid environment and the virtual network and that coordinates the process of migrating a complex application. Such a solution can fully automate the migration of one or more of the workloads comprising a complex application to any server throughout the hybrid environment such that all workloads retain their original network configuration and network relationships with peer workloads ensuring that the application its operational environment and its user access procedures are all substantially unmodified. Using the image library to store server images the solution can also create multiple copies of a complex application scale portions of the application when necessary by increasing the number of workloads recover failed workloads and provide disaster recovery.

According to another aspect a system and methods as disclosed herein uses overlay networks to link the server and network resources of a plurality of private data centers and a plurality of public clouds to create a hybrid cloud environment. Within this environment one or more of the workloads comprising a complex application can be migrated from their original source servers to any other available servers while maintaining the network relationships between them. This automates the migration of complex applications throughout the hybrid environment without modifying the application its operational environment or its user access procedures.

In one embodiment the network virtualization manager uses Layer 2 bridging and encrypted tunnels to securely and transparently create overlay networks that span the plurality of data centers in the hybrid cloud environment. Servers that are directly connected to a trusted network infrastructure such as that in a private data center can be connected to an overlay network using VLANs. Servers that are directly connected to an untrusted network infrastructure such as that in a public cloud are connected to an overlay network using a special tunnel driver that creates an encrypted tunnel between the server and a trusted virtual network appliance that is installed within that data center environment. Thus when a workload is migrated from one server to another its network communications with peer workloads are undisturbed. Neither it nor its peers can tell that it has moved even if it has changed data centers.

In one embodiment the workload migrater manages the migration of individual workloads from their source server to any available physical virtual or cloud server provided that the source and target server are object code compatible. When deployed onto a new server the workload s configuration is automatically updated to account for changes in server hardware and to install the tunnel driver when necessary. The workload migrater can also save a copy of the workload s image in an image library so that it can be deployed at a later time.

In one embodiment a migration manager interacts with administrative users and applications maintains a map of the resources throughout the hybrid environment and manages the deployment scaling and migration of complex applications. It makes use of the network virtualization manager to create and manage the overlay networks that link the hybrid cloud environment. It makes use of the workload migrater to migrate the individual workloads that comprise the complex application.

For the purpose of promoting an understanding of the principles of the present disclosure reference will now be made to the embodiments illustrated in the drawings and specific language will be used to describe the same. It will nevertheless be understood that no limitation of the scope of the disclosure is thereby intended any alterations and further modifications of the described or illustrated embodiments and any further applications of the principles of the disclosure as illustrated therein are contemplated as would normally occur to one skilled in the art to which the disclosure relates.

As shown in an exemplary embodiment of a system for automating the migration of complex applications within a hybrid cloud environment comprises a migration manager that is responsible for interacting with the end user and coordinating the migration of complex applications a workload migrater that is responsible for migrating individual server images from one physical virtual or cloud server to another an image library that is used to store individual server images such that they can be deployed at a later time and a virtual network manager that creates overlay networks that span a hybrid cloud environment comprising one or more data centers and and one or more public cloud providers .

Briefly summarized a system constructed as described herein carries out a process or processes for migrating a complex computer application from an initial application configuration to a new migrated application configuration. As used herein an initial application configuration is one where the application may have been initially installed and is operative on various initial computer servers has various initial application specific network connections to other servers or network devices and has externally appearing network connections to other computer systems and or network devices. As used herein a migrated application configuration is a transformed configuration where the application is installed and is operative on other perhaps improved or upgraded computer servers selected from available resources maintains certain application specific network connections to other servers or network devices but also transparently maintains any externally appearing network connections to other computer systems and or network devices.

As will be understood after the following detailed description and in one aspect the system facilitates the identification of available computing and or network resources from one or more resource pools provided within one or more data centers public or private establishment of certain virtual network appliances VNAs from available computing and or networking resources and automated deployment of the necessary connections of virtual network appliances VNAs to create overlay networks and other elements so that the complex application can run in the new migrated application configuration. This allows convenient and efficient benefits such as application scaling load balancing application and network performance optimization system failure restoration redundancy infrastructure system upgrades cost control and other similar benefits.

Details of an exemplary migration manager can be found in and the corresponding text. The migration manager controls the overall process of migrating a complex application to the new migrated application configuration. In one embodiment a migration manager is a computer program or program module that executes on a computer that may be considered as a migration manager computer.

Details of an exemplary workload migrater can be found in and the corresponding text. The workload migrater is responsible for migrating individual workloads as a part of an application migration operation. In one embodiment a workload migrater is a computer program or program module that executes on a computer that may be considered as a workload migrater computer.

Details of an exemplary virtual network manager can be found in and the corresponding text. The virtual network manager is responsible for building and managing the overlay networks required by the complex application so as to maintain the transparency of the complex computer application as to its connections and operations by end users of the application. In one embodiment a virtual network manager is a computer program or program module that executes on a computer that may be considered as a virtual network manager computer.

Details of an exemplary image library can be found in and the corresponding text. The image library stores the server images captured by the workload migrater so that they can be reused at a later date.

In one embodiment the virtual network manager VNM creates one or more overlay networks that run on top of the available network infrastructures within each data center environment e.g. by deploying and then managing at least one virtual network appliance VNA into each data center. As shown in three VNAs are shown one provided for each of the data centers respectively forming one or more overlay networks e.g. OVNET 10 OVNET 11 having elements within each of the data centers. If a data center contains more than one isolated network environment the VNM deploys at least one VNA to each network environment. More than one VNA can be used within a network environment to provide redundancy and scalability i.e. greater network throughput between environments . According to one aspect each VNA can support a plurality of overlay networks and joins one or more network segments within its local network environment to each of the overlay networks for which it is configured. A VNA s local network environment includes all of the LANs or VLANs that it can directly access via at least one of its network interfaces.

In one exemplary embodiment an overlay network is extended between VNAs using a tunneled connection . When the connection between VNAs must traverse an untrusted network infrastructure such as the Internet the tunneled connection can be encrypted and authenticated to ensure that the overlay network remains secure and isolated. In the example embodiment in overlay network OVNET 10 extends across all three data center environments and . Within the primary private data center OVNET 10 traffic is carried on VLAN X1 and and on the virtual network X2 provided by the hypervisor private cloud infrastructure . In the remote private data center OVNET 10 traffic is carried on VLAN X2 . The public cloud provider does not expose VLAN support to its users so OVNET 10 traffic is carried over encrypted tunnels to each server within that environment that is authorized for access to the overlay network.

Although they are in different data centers on network infrastructure managed by different organizations the servers and are all connected to the overlay network OVNET 10 share a subnet and communicate as if they were connected to the same local LAN.

Refer now to for further details and aspects of overlay networks in particular with the exemplary overlay network shown in . shows additional details on the operation of the overlay network in . In this example the application server workload ASW and web server workload WSW are in different physical locations and are not connected to the same local area network LAN . Prior to any network communications between the ASW and WSW VNA1 is deployed into the physical location containing the ASW and VNA2 is deployed into the physical location containing the WSW. VNA1 is configured such that its network interface 0 NIC 0 is connected to the Internet and its network interface 1 NIC 1 is connected to a subnet within its physical location that corresponds to LAN 1 . VNA2 is also configured such that its network interface 0 NIC 0 is connected to the Internet and its network interface 1 NIC 1 is connected to the subnet within its physical location that corresponds to LAN 1 . VNA2 is configured to run a tunnel server software process that connects to a bridge software process that routes traffic for LAN 1 . VNA1 is configured to run a tunnel client software process that connects to a bridge software process that routes traffic for LAN 1 . VNA1 is configured to create an encrypted tunnel between itself and VNA2 . The tunnel client software process on VNA 1 is configured to create an encrypted tunnel connection to the tunnel server software process on VNA2 . In this particular embodiment the tunnel client relies upon the Internet transmission control protocol TCP to create a reliable network connection with the tunnel server over the Internet or other intervening network technology.

Prior to the AWS sending data to the WSW it must first identify where to send the data. It will use the domain name service DNS to convert the name of the destination WSW to its network address 10.1.1.10. It will then use the address resolution protocol ARP to ask the surrounding network where to send data for address 10.1.1.10. This ARP request is received by the LAN 1 bridge on VNA1 which forwards it through the tunnel to the LAN 1 bridge on VNA2 . The bridge on VNA2 forwards the ARP request onto its local LAN 1 where it is received by the WSW . The WSW returns an ARP response indicating that all traffic for address 10.1.1.10 should be sent to its layer 2 typically ethernet address. The response travels back over the same path. It is received by the LAN1 bridge on VNA2 . The bridge records the layer 2 address of the WSW and then substitutes its own layer 2 address into the ARP response before sending it on to the LAN1 bridge on VNA1 . This bridge records the layer 2 address of the previous bridge and substitutes its own address into the ARP response before returning it to the ASW.

When the ASW sends data to the WSW it will package it into a data packet using a data transmission protocol such as TCP as shown in . The TCP header contains the address of the destination WSW which is 10.1.1.10. The header is followed by the actual data . Remembering the results of the ARP process described above the ASW sends all network packets addressed to 10.1.1.10 to NIC 1 of VNA1 where it is received by VNA1 s LAN1 bridge . The bridge forwards the packet to the LAN1 bridge on VNA2 by forwarding it through the encrypted tunnel. Before sending the packet over the Internet the tunnel client on VNA1 encrypts the entire packet and and stores it within a new TCP data packet . The tunnel client sets the address in the new TCP header to the network address of VNA2 . When this encapsulated packet is received by the tunnel server on VNA2 it pulls the original packet out of the TCP data and decrypts it recovering the original TCP packet . This is forwarded to the LAN1 bridge on VNA2 which forwards it again to the WSW. The entire multi hop forward and encapsulation process is completely transparent to ASW and WSW i.e. the process looks the same to the ASW and WSW as if they were directly connected to one another on a local area network.

From the foregoing those skilled in the art will understand and appreciate that an overlay network such as the example provides a mechanism for maintaining the addressing relationships between workloads of a complex application in a manner transparent to the servers that execute the workloads. According to an aspect of the disclosure as discussed in connection with a VNA such as that shown at in is deployed at endpoints within a primary private data center any remote private data centers and or any public cloud providers that provide a pool or pools of server resources that can be assigned for use as target servers in a migration operation with encrypted tunneling if desired for secure communications thereby providing for transparent continued operation of the complex computer application upon migration of the application into new server resources.

In the example of the VNA is implemented as a workload that runs on a physical virtual or cloud server such that the VNM can deploy it automatically using the workload migrater . This allows a system constructed as described herein to automatically deploy the virtual network infrastructure into new regions of the hybrid cloud. Those skilled in the art will recognize that other implementations of a VNA can also be used provided that they are able to join local LANs and VLANs to those of a remote peer using an encrypted network tunnel. Examples of other implementations of a VNA that would work with the disclosed exemplary system include open source software such as OpenVPN or Vyatta and most commercial virtual private network VPN implementations. It will also be appreciated that the VNA can be implemented in other forms such as software that runs on a physical network switch or as a virtual switch that runs within a hypervisor host.

Once a VNA e.g. as shown at in has been deployed and is operational within its environment a local network mapper associated with the VNA discovers the LANs subnets and servers within its environment using common discovery techniques such as ICMP and SNMP although those skilled in the art will recognize that other discovery techniques can be used. The VNM queries the VNA s application programming interface API to read the results of the discovery process and adds them to its network map. The VNM will also use the API to extend an overlay network into the new environment. Using the API the VNM can create a software switch on the VNA for the overlay network connect it to one or more local network segments configure a tunnel server for receiving tunnel connections from workloads or other VNAs and configure a tunnel client to connect to another VNA. As part of this configuration process the VNM supplies the VNA with the cryptographic key information and needed to authenticate and encrypt tunnel connections for the overlay network.

After an overlay network has been configured on a VNA it collects packet flow statistics from the network and all of its joined segments . The VNM can retrieve these statistics using the API and analyze them to determine the health of the VNA and its overlay networks. The statistics can also be used as data for the routing algorithms used by the VNM to optimize the connections between VNAs that comprise the overlay network topology.

Network segment joins a network segment for which the VNA is not able to use tagged VLANS to the overly network. This might be the case if a network interface NIC is running a protocol that does not support VLAN tagging or if the network switch is configured for untagged VLAN support. For example the endpoint for the network interface NIC is connected directly to the overlay switch and cannot be used for any other overlay network. Ensuring that traffic on the overlay network is properly isolated and secure requires the assistance of a network switching infrastructure that is trusted to isolate LAN traffic throughout the local network environment. It can do this using logical separation throughout the local network or by providing a physically isolated environment.

Network segments and show three types of tunneled network segments. Tunneled segments are used for two purposes 1 to join an individual workload rather than a network segment to an overlay network and 2 to extend an overlay network between VNAs.

A tunneled network segment wraps a layer 2 packet e.g. a packet constructed using the Ethernet protocol within a higher layer typically layer 3 IP or layer 4 UDP or TCP datagram or connection and sends it to a peer. The peer unwraps the layer 2 packet examines the address headers and forwards it on a local network segment to the intended recipient. Wrapping the layer 2 packet with an upper layer protocol allows the VNA to do two things 1 use the capabilities available at the upper layers to reliably route and transfer the packet across networks that could not be traversed at layer 2 and 2 optionally wrap the layer 2 packet using encryption and authentication to provide data integrity and secrecy over untrusted network infrastructures. The example shown in uses the OpenVPN protocol for tunneling. Those skilled in the art will understand that other tunneling protocols such as the Point to Point Tunnel Protocol PPTP Layer 2 Tunneling Protocol L2TP Virtual eXtensible LAN VXLAN Network Virtualization with GRE NVGRE Stateless Transport Tunneling STT or Network Virtualization Overlays NVO3 could also be used.

Still referring to exemplary network segment illustrates a connection oriented tunnel that runs over TCP with the VNA receiving incoming connections. Those skilled in the art will understand that connection oriented protocols other than TCP could be used for a tunnel connection. A tunnel server listens for incoming connection requests. Upon receiving a connection from a remote peer it joins the connection to the overlay switch . Remote peers might include individual workloads or remote appliances. The tunnel server and switch can support any number of concurrent connections. If the network infrastructure between the VNA and the peer is trusted then it is not necessary to use encryption to protect the tunnel connection. If the intervening network is not trusted then the tunnel connection needs to be authenticated and the tunnel protocol needs to cryptographically ensure the integrity and secrecy of any data transferred. Cryptographic keys and are used to provide security services if required. These can be shared secret keys or public private key pairs. Keys are distributed to the VNA by the VNM when it adds the overlay network configuration to the VNA. Keys are distributed to an individual workload by the workload migrater as part of the migration process. Those skilled in the art will understand that any security protocol and cryptographic algorithm that provide the necessary authentication integrity and secrecy can be used to protect tunnel traffic.

Exemplary network segment illustrates a connection oriented tunnel that runs over TCP with the VNA initiating the connection from its tunnel client to the tunnel server of a peer VNA in order to extend an overlay network between them. Those skilled in the art will understand that connection oriented protocols other than TCP could be used for a tunnel connection. Cryptographic keys and are used to provide security services if required. They are distributed to the VNAs by the virtual network manager when it adds the overlay network configuration to the VNA.

Exemplary network segment illustrates a connectionless datagram tunnel. The datagram tunnel process exchanges layer 2 packets wrapped within UDP datagrams with one or more peer processes on individual workloads or peer VNAs. Those skilled in the art will understand that connectionless protocols other than UDP can be used for a datagram tunnel. Cryptographic keys and are used to provide security services if required. They are distributed in the same manner as the keys for the connection oriented tunnels. Connectionless datagram tunnels are more efficient than connection oriented tunnels in network environments with low latency and low packet loss. Thus connectionless tunnels are typically used when peers are within the same data center environment. Connection oriented tunnels are more efficient with higher latencies and packet loss and recover better from network disruptions. Thus connection oriented tunnels are typically used to link VNAs over the Internet or other wide area networks WANs .

Server 1 is a trusted server and is connected to OVNET 10 by configuring it with a network interface for tagged VLAN 32 which the VNA maps to OVNET 10. A server can be trusted when the operating system OS and administrators of that server are trusted not to change the tagged VLAN configuration and if the server is properly secured such that an attacker cannot change the configuration for if it were changed the server might then be connected to an unapproved overlay network. When using a tagged VLAN connection to an overlay network the workload migrater will configure the server s network setup to include the tagged VLAN interface. This approach works with network infrastructure that cannot be directly managed by the VNM i.e. the VNM cannot change its configuration when deploying a server but is trusted to properly isolate VLAN traffic. Due to the risk associated with trusting the server its OS and administrators this approach is not often used and is usually supplanted with a tunneled connection as described below for Server 4 .

Server 2 is an untrusted server that is connected to OVNET 10 by connecting its network interface to VLAN 32. As the server is not trusted it is placed onto the necessary VLAN ID 32 by using an untagged switch port on the managed switch . As part of the server deploy process the VNM configures the switch using one of the switch s APIs . This approach can only be used when an infrastructure manager see on the VNM has a driver or plug in that can be used to configure the specific model of switch connected to the server. As there are many models of network switches this is not always the case. If an appropriate switch driver is not available a tunneled connection can be used instead.

Still referring to virtual server 3 is an untrusted server running on the trusted virtualization host . When the workload migrater creates the virtual machine as part of the deploy process it will use the virtualization host s API to create the virtual machine such that it has a virtual network interface connected to a virtual switch that is connected to VLAN 32 . This approach requires a virtualization host and an unmanaged network infrastructure that are trusted to enforce VLAN isolation. It also requires that the infrastructure manager see on the VNM have a driver or plug in that can be used to configure the specific model of virtualization host. As just a few hypervisors VMware Xen KVM HyperV provide the vast majority of virtual machines only a few plug ins are required.

Server 4 is an untrusted host that is connected to an untrusted network infrastructure . When the workload migrater deploys the server it connects it to OVNET 10 by installing and configuring a tunnel client driver and then installing the key material and configuration data necessary to connect to the VNA s tunnel server . Because the tunnel traffic is encrypted and authenticated its integrity and secrecy is maintained even when traveling over untrusted networks. The most likely attack that could be mounted by a subverted server or network would be a denial of service by refusing to deliver the encrypted packets. This approach works for all workloads whether they run on trusted or untrusted servers and networks. But due to the overhead associated with encrypting decrypting the network traffic a VLAN approach is preferred where available when the server or network infrastructure is trusted.

Virtual Server 5 is an untrusted server that runs on an untrusted virtualization host and communicates over an untrusted network infrastructure and . This is a typical example for public cloud servers where the cloud vendor and its employees cannot be trusted by the private data center. The approach uses encrypted tunnels just as was described for Server 4 above. Because the tunnel traffic is encrypted and authenticated its integrity and secrecy is maintained even when traveling over untrusted networks hypervisors and cloud infrastructures. This approach is listed as a separate case only to emphasize the fact that encrypted tunnels also protect the overlay network from compromised cloud infrastructure. Virtual Server 6 runs as an untrusted VM on the trusted Virtualization Host 3 on which the standard virtual network switch that comes by default with the virtualization host such as and has been replaced with a VNA plug in . Operation of the plug in is nearly identical to that of the standard virtual switch. Like the standard virtual switch the plug in is configured with a separate virtual switch for each VLAN overlay network to be enforced. Each network interface on a VM is connected to one of the configured switches within the plug in. But rather than forward external traffic i.e. communications that travel outside of the virtualization host over a tagged VLAN and enforcing VLAN access controls as does the standard virtual switch the plug in forwards external traffic to peer VNAs using encrypted tunnels and enforces overlay network access controls. This approach supports untrusted VMs and untrusted network infrastructure without the use of client tunnel drivers within the workload. But as the tunnel client has been moved from the workload into the plug in this approach requires that the virtualization host and plug in be trusted.

The example embodiment extends all configured overlay networks by connecting all secondary VNAs to the primary VNA to create a star network topology as shown at in . When two servers are in the same data center environment such as servers 2 and 3 in there is at most a single VNA hop between them VNA 2 in this example . If this VNA is using VLANs to implement the overlay network within its network environment and the two servers are connected to this VLAN there wouldn t be any hop through the VNA as the two servers can communicate directly over the trusted network infrastructure using tagged VLANs. When one server is in the primary data center e.g. 140 and another is in a secondary data center e.g. 120 such as servers 1 and 2 in there are two VNA hops VNA 1 and VNA 2 and one tunnel between them. When two servers are in different secondary data centers such as servers 3 and 4 there are three VNA hops VNA2 VNA1 VNA3 and two tunnels between them. The star topology guarantees that the worst delay between any two communicating servers is three hops and two tunnels.

Packet loops i.e. network topologies that contain a loop such that packets can be forwarded continuously around the loop are dangerous as they can consume all network bandwidth within the network infrastructure preventing useful communication. The use of layer 2 protocols such as the Spanning Tree Protocol STP and Shortest Path Bridging SPB eliminate loops by disabling redundant connections. However the packets used by these protocols to communicate between switches are sometimes suppressed within public cloud environments for security reasons. This makes it dangerous to rely upon these protocols for protection against loops when creating an overlay network that includes a public cloud network. Thus the example embodiment uses the star topology to eliminate any possibility of a loop while guaranteeing a maximum of three hops for all network communications.

Each hop through a VNA and each data transfer over a tunnel between VNAs adds delay to network communications. In a star topology such as in if the servers and communications between them are randomly distributed the majority of communications will incur the maximum three hop delay. Thus an important performance consideration when migrating one or more workloads of an application is to keep portions of an application that communicate together within the same VNA environment.

As will be discussed in later paragraphs the VNM maintains a network map such as shown in of all network environments and the overlay networks within. The migration manager maintains an application map that lists the servers in each application which environment they are in and the overlay networks that they use.

Turning now to a VNM can use this combined information to perform topology optimization for each overlay network as shown at in . The legend in shows connections between a number of components servers and VNAs forming overlay networks OVNET1 OVNET4 .

For OVNET 1 servers 1 and 8 communicate with server 2 but not with each other. Therefore making VNA 2 the hub for OVNET 2 eliminates all three hop connections. As there is no reason why all networks need to use the same hub the choice of hub for each overlay network can be made to optimize its traffic.

OVNET 2 is only used by the servers in network environments 2 and 3 so there is no need for a hub and the network consists of a single connection between VNA 2 and VNA 3.

For OVNET 3 servers 3 and 5 communicate with server 1 but not with each other so making VNA 1 the hub optimizes the traffic between them. Server 6 is also on OVNET 3 but only communicates with server 5 so VNA 4 can be connected directly to VNA 3 to optimize the traffic between them.

When a server connects to an overlay network using a tunnel client but does not access any servers within its own environment on that network such as server 7 on OVNET 4 it can be configured to connect directly to a remote VNA e.g. VNA 1 in rather than its local VNA eliminating one or two hops and tunnels.

Together this set of changes eliminates all three hop connections from the example network of converting some three hop connections to a single hop. Note that when making these optimizations the VNM must ensure that it does not create any loops among the connections for an overlay network.

In a further optimization the VNM can collect packet flow statistics for all overlay networks throughout the hybrid cloud environment and dynamically reconfigure their connection topology to optimize the number of hops taken by network traffic. Various optimization algorithms could be used to enforce different policies such as minimizing the aggregate number of hops for all traffic or optimizing the number of hops for specific high priority traffic flows or servers.

An appliance manager is used to extend the hybrid cloud environment by deploying monitoring and managing VNAs. The appliance manager makes use of the workload migrater to perform the actual deployment of the VNA into a network environment within a data center. Once a VNA has been deployed into a network environment the appliance manager can extend an overlay network into that environment by using the VNA API to configure the VNA with a software switch specific to the overlay network connecting local network segments to the switch and installing overlay network key material.

A tunnel manager configures monitors and manages the tunnel connections throughout the hybrid cloud environment. It establishes the topology of an overlay network by configuring the tunnels between VNAs using the VNA API and can dynamically reconfigure these tunnels to optimize traffic flow. The tunnel manager also creates the software installation packages that are installed by the workload migrater on top of a migrated server image in order to link a migrated workload into an overlay network.

A VLAN manager configures monitors and manages the VLAN segments that are linked into an overlay network. Using the VNA API it configures a VNA to add a VLAN to an overlay network by adding a tagged VLAN interface to the VNA or linking a specific network interface to the software switch. It connects individual servers to an overlay network by configuring their network switch port or hypervisor interface for the proper VLAN ID using the API of the corresponding network switch or hypervisor host.

An infrastructure manager provides an abstraction API that is used by the other VNM components for managing VNAs public clouds hypervisor hosts and network switches. This allows invention system constructed as described herein to support multiple technologies for each type of device. When the infrastructure manager needs to manage an infrastructure component such as a network switch that it cannot access directly because it is on a remote network segment it can proxy its commands to the device through the VNA that is local to the device.

Most of the operations performed by the VNM involve many steps and many components. A task sequencer is provided in the VNM and responsible for performing the sequence of steps in a robust manner waiting for steps to finish retrying them to recover from transient errors and backing out changes when encountering an unrecoverable error.

If an organization has already linked its data centers with a virtual private network VPN or other secure link it can be treated in either of two ways 1 the linked data centers can be considered a single network domain or 2 each data center can be considered a separate network domain. Note that in the first approach network traffic might travel over the VPN in order to reach the VNA so that it can be tunneled to another network domain. Thus the decision between the two approaches will typically be made based upon the expected network traffic and the bandwidth provided by the existing secure link implementation. Public cloud providers that have more than one regional data center are typically modeled using a separate network domain for each region. Even if the cloud vendor provides secure internal networking between its geographically dispersed regions the efficiency of the overlay networks is typically improved by using a separate VNA in each region.

Each overlay network in the list in is a virtual network that operates across the physical network infrastructures provided by the various network domains. The OverlayNetwork record within the network map is used to provide a global identifier for an overlay network that traverses more than one network domain. The LocalOverlayNetwork records within each NetworkDomain define how the overlay network is implemented within the infrastructure of that network domain.

Each OverlayNetwork record contains an ID OVNET ID and name OVNETName that are unique throughout the hybrid cloud environment. Other attributes stored for an overlay network can include a description an optimization policy to specify the topology used to link VNAs and access control attributes controlling which end users can manage the network access the network or deploy a server onto the network.

Each NetworkDomain record contains an ID NetworkDomainID and name Network DomainName that are unique throughout the hybrid cloud environment a description and access control attributes via a data item AccessControls controlling which end users can manage the domain access the domain or deploy a server into the domain. It also contains a plurality of VNA records that record information about the VNAs that are deployed within the domain. Each VNA record specifies the type of VNA appliance the current state of the appliance the external network address of the appliance that is used by tunnel clients when connecting to the appliance s tunnel server the URI for accessing the appliance s management API and the credentials for administering the appliance . Access control information is not needed for the VNA as its access is controlled using the access control data item AccessControls of the network domain record .

Each NetworkDomain record also includes a plurality of ResourceDomain records specifying server resources that can be dynamically provisioned by the migration manager within that network domain. Each ResourceDomain record represents a pool of server resources in other words a resource pool that are managed through a common management API such as a physical servers that are configured for deployment using PXE Preboot eXecution Environment boot or ISO boot i.e. boot from an ISO 9660 file system such as a CD ROM b one or more hypervisor hosts such as a group of VMware ESX hosts managed by vCenter Server or Citrix XenServer hosts c a private cloud infrastructures such as a VMware vCloud or OpenStack infrastructure or d a public cloud provider account such as Amazon EC2 or Rackspace. Those skilled in the art will recognize that additional types of physical servers hypervisor hosts private cloud infrastructures and public cloud accounts can also be supported. Those skilled in the art will also understand and appreciate that each ResourceDomain record contains information that allows access to a resource pool as discussed elsewhere herein for the purpose of identifying selecting and configuring server resources within a data center corresponding to that ResourceDomain. These server resources can be assigned for use as a target servers virtual network appliances or other computing and or networking functions as may be desired in connection with a migration operation.

According to one aspect a server resource in a resource pool ResourceDomain associated with a network domain NetworkDomain may be selected and dedicated for use in a migration operation to serve as a computing or networking resource or workload such as a VNA a web server an application server a database server or other similar device. The resource pool can be accessed using its API to determine the characteristics of any available resources to select one or more resource and to make those resources available for migration.

Typically a resource pool is a collection of server resources that are available within a particular physical data center environment although it will be appreciated that a resource pool could include server resources that are available in different and physically separated data center environments but connected for data communications via high speed connections e.g. via VPN thereby forming a logical or virtual data center comprising facilities that are physically separated.

A ResourceDomain record includes an ID ResourceDomainID that is unique throughout the hybrid cloud a name ResourceDomainName that is unique within the network domain a vendor name identifying the vendor and type of infrastructure a URI for accessing the infrastructure s administrative API the credentials to be used for administering the infrastructure a set of vendor specific attributes to be used when creating a virtual server within the infrastructure and a set of access control attributes for controlling which end users can manage the resources access the resources and deploy a server to the resources.

Each NetworkDomain record also includes a plurality of LocalOverlayNetwork records each of which identifies an OverlayNetwork that is defined within the network domain and specifies the isolation method used for that overlay network within the network domain. Each LocalOverlayNetwork record contains an ID OVNET ID that matches one of the globally configured OverlayNetworks in the network map. It also contains a list of zero or more local VLAN IDs LVID that are linked into the overlay network and a configuration flag to specify whether or not to provide a tunnel server for linking tunnel clients into the overlay network. The LocalOverlayNetwork record also includes a URI pointing to the peer MasterServer VNA if any to which the domain s VNA should connect in order to link the network domain into the overlay network.

According to an aspect a system constructed in accordance with this disclosure further includes UserAccount data for storing information associated with authorized users of the system. The UserAccount record shown in is not part of the network map per se and is stored instead in the User Account database . It is shown here as it is essential in evaluating the access controls used to protect the resources represented by the network map. The Migration Manager maintains a database of its authorized users with a UserAccount record for each user. This contains an ID and name as well as the login credentials used to authenticate the user. The record also contains a set of group memberships to which the user belongs and a set of authorizations granting the user permission to access specific features of the system. The UserAccount record also contains user default settings such as the user s default image library .

A computer implemented process by which a VNM deploys and manages a virtual network via overlay networks implemented with virtual network appliances VNAs is shown in . The VNM begins by reading the network map from the network mapper and the available application maps from the migration manager at step . For each globally defined overlay network it then computes the optimal topology connections between the defined VNAs at step . As previously discussed there are many potential topologies and optimizations. The VNM might optionally read network traffic statistics from the VNAs in order to compute the optimal topology. The optimal topology is added to the network map by specifying the inter VNA connections as peer URIs that are added to the domains overlay network configuration data .

With the topology determined the VNM then checks each network domain as identified by NetworkDomain records in in turn at step to see whether the VNAs specified for that domain have been deployed and are operational at inquiry step . If a VNA is not operational the VNM calls the appliance manager to deploy a new VNA workload to an available server physical virtual or cloud within the domain at step . As the VNA is considered nothing more than another workload it is deployed by the workload migrater using the standard workload deploy process shown in . The target server to be used for the VNA does not require any special network configuration. It must have access to the other VNAs via a suitable network such as the Internet and it must have access to any local VLANs or subnets within its environment that need to be connected to overlay networks. The process loops at step to check for additional network domains that may require a VNA deployment. During the network domain checks at step the VNM also decommissions and removes any network domains and deployed VNAs that are no longer defined in the network map at step .

After all of the defined VNAs are determined to be operational at step the VNM iterates the list of network domains a second time as shown at . For each network domain the VNM then checks the list of overlay networks at step . The overlay networks within a NetworkDomain are identified by one or more LocalOverlayNetwork records . If the overlay network is not currently configured for the network domain by a check at step but is found on the VNA the VNM removes the overlay switch from the VNA at step . If the overlay network is configured for the domain at step the VNM checks the network status by calling the VNA s API . If the network is not yet configured on the VNA the VNM adds a software switch to the VNA at step . Using its VLAN manager the VNM links into the overlay switch any local i.e. local to the remote VNA s network environment VLANs that are listed in the network map as part of the overlay network via steps . The VNM removes from the switch any local VLANs that are currently linked to the switch but are no longer defined in the network map at step .

If the network domain s local overlay network record specifies that the VNA is to provide a tunnel server for the overlay network at step the VNM checks the server status using the VNA API. If a tunnel server is not running for the overlay network the VNM uses its tunnel manager to configure and start a tunnel server on the remote VNA and then links it to the software switch at step . If there are currently any tunnel servers running on the VNA that are no longer defined in the network map the VNM removes them from the VNA at step .

Still referring to if the network domain s local overlay network record specifies one or more peer URIs as the result of the previous computation of network topology from step the VNM uses its tunnel manager to configure a tunnel client on the VNA at step connect it to the software switch at step and then connect it to the specified peer tunnel server at step . Note that if due to the order in which the network domains are processed the tunnel client attempts to connect to a peer tunnel server before that tunnel server has been created the tunnel client will retry its connection until the tunnel server is available and it has a successful connection. The VNM then removes any tunnel clients that it finds configured on the VNA that are no longer defined in the network map at step .

After the overlay networks have been configured for all network domains the VNM checks the network status at step by reading full status information from each VNA. It also reads the VNA traffic statistics for each overlay network at step and recomputes the topology connections based upon actual packet flow statistics at step . It makes any necessary changes to the topology connections at step and then updates the network map with the current state of the network at step . The virtual overlay network is now fully deployed. If the VNM is configured to continuously monitor the virtual network it pauses for a configured interval and then loops over the networks status check and topology update as shown in steps .

Referring now to a workload migrater is responsible for deploying a VNA into a network domain. It is also responsible for migrating an individual workload that is part of a complex application from a source server to any other physical virtual or cloud server within the hybrid cloud environment. The example embodiment uses the workload migrater described in United States Patent Application Publication No. US 2013 0290542 Server Image Migrations into Public and Private Cloud Infrastructures Charles T. Watt et al. . The internal details of this workload migrater and other aspects of server image migration are described in more detail therein and incorporated by reference herein. Those skilled in the art will recognize that other approaches to workload migration can also be used as long as they can reliably migrate a server image from any physical virtual or cloud server to any other physical virtual or cloud server.

As shown in the workload migrater used by the example embodiment works in conjunction with a capture agent that is installed on the source server and a deploy agent that is installed on the target server . As described in the referenced Watt et al. patent application and as will be appreciated by those skilled in the art server image migration can also be performed without the use of a capture and or deploy agent if the workload migrator has access to the server s image such as when the image is stored on a network storage device like SAN storage area network LUN logical unit or NAS network attached storage or when it is stored on a hypervisor host.

The capture agent associated with the workload migrator gathers source image information about the source server and its image reporting the information back to the workload migrater . The capture agent can also capture the server image to an image library or stream the image directly to a target server . After streaming or capturing its image the capture agent can synchronize all changes that have been made to the source server s image since the last capture or synchronization directly to the target server or to the image library where they are stored as an incremental capture .

The deploy agent associated with the workload migrater gathers target server information about the target server and reports it back to the workload migrater . Upon receiving instructions from the workload migrater the deploy agent streams the captured image from the source server or image library and deploys it to the target server along with any additional software packages and configuration changes specified by the workload manager.

The source image information contains system and application configuration data collected from the source image being migrated. This data includes the operating system vendor and version the size and layout of the file systems and the number of network interfaces and their configuration. During an image capture the source image configuration data is also stored in the image library along with the captured image.

A capture process of the workload migrater manages the capture agent through the steps of migrating or capturing the source server s image. The image will either be streamed directly to the deploy agent on the target server or stored in the image library .

A deploy process of the workload migrater manages the deploy agent through the steps of deploying a captured image to the target server. It gathers source image information about the server from the agent compares it with the configuration of the original server and its workload considers any requirements specified by the end user or migration manager as specified by a deployment profile and determines how to map the image onto the resources available on the target server. For example the deploy process might consolidate multiple file systems that had originally been on separate disk drives onto the single virtual drive available on the target.

A software installer of the workload migrater installs any additional software packages on top of the original image as part of the deployment process. This is used to install drivers into the image to handle the changes in hardware between the source and target server platforms. It is also used to install the tunnel client drivers that are necessary for connecting a server that is running on untrusted infrastructure to an overlay network for example as shown at in . Software packages can also be used to add any special software that is required by the resource domain e.g. cloud infrastructure hypervisor host etc. for the workload to function properly within the domain.

An image configurer of the workload migrater is responsible for modifying the operating system and application configuration settings on the target server after the original source image has been deployed. This is used to configure tagged VLAN interfaces on the target workload in order to connect a trusted server to an overlay network for example as shown at in . The image configurer makes the configuration changes that are specified in the deployment profile which is passed to the workload migrater by the migration manager. In the example embodiment the deployment profile is an XML document describing the required configuration of the workload s network interfaces storage volumes file systems etc. Those skilled in the art will recognize that the deployment profile can take many other forms such as command line parameters to the workload migrater a database record or a data file.

Still referring to a driver library of the workload migrater provides the drivers necessary for installing an image on the target server hardware. This may include drivers for storage devices network interface devices graphical display drivers etc. In the example embodiment the driver library is stored on the workload migrater as an hierarchical directory of files. The driver files are collected from hypervisor hosts cloud infrastructures CD ROM and ISO images of driver distributions software installation packages and directly from running servers. Those skilled in the art will recognize that driver files can be collected from many other sources and can be stored in many other formats such as a database.

An infrastructure manager of the workload migrater is used to create virtual machines in cloud and hypervisor infrastructures. It is also used to configure the network interfaces on a target server so that they will access the correct overlay network. On clouds and hypervisors this is done by attaching the virtual network interface to the local VLAN within the target network domain that is mapped to the overlay network using the cloud or hypervisor plug in to manage the cloud or hypervisor infrastructure. When using untagged VLANs to attach a server to an overlay network the infrastructure manager must manage the network switch to which to the server is attached using the appropriate switch plug in . Those skilled in the art will recognize that some workload migration technologies do not support the ability to manage infrastructure devices such as network switches and cloud or hypervisor infrastructures and that the same results can be achieved by implementing the infrastructure management within the migration manager rather than the workload manager . As with the previously described virtual network manager the migration manager will proxy commands to infrastructure devices that it cannot directly access through the VNA local to the device.

Still referring to a synchronization manager of the workload migrater synchronizes source and target images after a migration if discrepancies between such images occur due to the time it takes to complete a migration. It will be appreciated that the time taken to migrate a workload from one server to another will vary substantially depending upon whether the source server is online or offline at the time of migration the size of the server image the network bandwidth between the source and target environments and many other factors. Migration times can vary from minutes to days depending upon these factors. To ensure that a consistent image is captured from a running source server it is necessary to use point in time snapshots of the source server image using technologies such as volume shadow copy VSS on a Windows server. Depending upon the length of the migration process the deployed image may be out of date and unusable by the time the initial migration has completed. The synchronization manager is responsible for synchronizing the source and target images after migration quickly copying only the changes that have been made to the source image since the point in time capture from the source to the target. The example embodiment uses the Open Source rsync http rsync.samba.org application for performing synchronization between source and target. Those skilled in the art will recognize that there are many other ways to synchronize the servers.

The synchronization manager is also responsible for cutover i.e. activating the new target server and deactivating the original source. The example embodiment implements cutover by fencing the target server i.e. placing it on an isolated overlay network so that it will not interfere with the running source server until the time of cutover. During cutover the source server is fenced and the target is moved onto the production overlay networks. Those skilled in the art will recognize that there are other methods for achieving cutover to the target server such as by configuring it with a temporary network address for deployment and synchronization and then changing its network address to that of the source server after the source has been deactivated.

Turn next to for a description of a migration process . In the example embodiment as described herein the workload migrater as shown in performs the actual migration of each individual workload. But it is unaware of the complex application the relationships between workloads and the virtual network environment. Accordingly the migration manager MM performs all of the additional steps required such that the newly migrated server fits seamlessly into the complex application. is a flow chart showing the entire migration or workload deployment process for a single workload that is part of a complex application. A deployment process may be viewed as having a capture half and a deploy deployment half Note that the capture half of the migration is not shown in the figure as no changes to the capture are necessary for migrating the complex application. Details of the capture for the example embodiment can be found in the Watt et al. patent application referenced above.

Note that certain data items used in the workload deployment process are shown in as stored in connection with an application map.

The deploy half of a migration begins with the MM determining which overlay networks to connect the target server to and the method see to use within the target environment for connecting the server to those overlay networks. Note that the target server might be deployed using a different set of overlay networks than were used for the original source server. If the target server is being deployed to replace the original source server then it is typically deployed to the same overlay networks as the source server. But if the target server is being deployed to a duplicate copy of the application it will typically be deployed to a different set of overlay networks so that the source and target servers will not conflict.

The application map see contains a Node record describing every server that is part of the application and an ApplicationNetwork record describing every overlay network used by those servers. Each network interface on a server is defined by a NetworkInterface record within the Node . The NetworkInterface includes an ApplicationNetworkID that points to the ApplicationNetwork to be associated with the interface. The OVNET ID field in the ApplicationNetwork in points to the actual OverlayNetwork that should be connected to the network interface.

Thus to determine the proper configuration for the target server s network connections the MM first step reads the source image configuration from the captured image data stored within the image library . It then step reads the network map for the hybrid cloud environment from the VNM s network mapper and the application map see from the application mapper . The MM identifies the overlay networks of the original source server step by reading the source server s MAC address from the source image configuration using it to identify the Node record corresponding to the source server and gathering the set of ApplicationNetworkIDs within the Node . The MM maps these to the desired set of overlay networks within the new target server environment step by collecting the OVNET ID fields from the ApplicationNetwork records pointed to by the ApplicationNetworkIDs. If the target server will be deployed while the source server is still active the migration manager may prepare two sets of overlay networks for the target server a temporary set to be used for fencing the target server during deployment given by FenceOVNET ID and the final production set to be used when the target is moved into actual production OVNET ID .

From the network map the MM reads the NetworkDomain record for the target server environment. For each overlay network ID identified in step there will be a LocalOverlayNetwork record within the NetworkDomain that describes how the overlay network is implemented within the target environment.

If the network domain for the target server uses local VLANs to isolate an overlay network the migration manager prepares a deployment profile with the appropriate configuration settings for the target server at step . If the target network domain uses tunnels to isolate an overlay network the migration manager creates a software installation package containing the tunnel client driver configuration information and tunnel key material to be used when deploying the target . These are generated for the migration manager by the VNM s tunnel manager .

If the application map indicates that the workload is connected to a database either external to the workload or within the image itself the migration manager checks the migration mode data MigrateMode for the database at step . If it is an external database and in leave behind mode as determined at step there is nothing else to be done. Database access is being made using standard network protocols such as iSCSI or FCIP. The target server will be deployed with the same network address as the original source server and the overlay networks will ensure that the target has access to the database.

If at step the database storage is external to the workload and in external mirror mode as indicated by the data item MigrateMode then the database is being migrated to a new physical location by some process external to the invention such as LUN mirroring by a SAN device. The new copy of the database will have a new address in the target environment. This new address is provided by the TgtLocation field in the Database record . The database address can take different forms depending upon the technology being used for storing the data it could be a device name if it shows up as a local storage volume it could be a network address IP fibre channel etc. of an external storage volume. The migration manager adds the database address to the workload s deployment profile so that the address can be updated in the workload during the deploy process.

If the database MigrateMode is sync mode as determined at step then the database appears as a local storage volume to the source and target servers and will be migrated by the workload migrater . The migration manager configures the workload s deployment profile step to ensure that actual data within the database is synchronized between the source and target servers after the migration has completed. This ensures that the data on the target server is up to date before moving the target server into production.

With the target server s deployment profile now configured to account for any overlay networks and databases the migration manager calls the workload migrater at step passing it the deployment profile and any software installation packages. The workload migrater checks the specification for the target server . If it is a virtual machine on a cloud or hypervisor infrastructure as determined at step it uses its infrastructure manager to create the target VM at step and to configure its virtual network interfaces such that they are placed on the proper local VLAN at step . If the deployment profile specifies the use of an untagged VLAN to place the server on an overlay network at step the workload migrater uses its infrastructure manager to configure the switch port connected to the server s interface .

With the hardware and virtual hardware configuration set up correctly for any overlay networks the workload migrater proceeds with a standard workload migration as described in Watt et al. as shown at step . After this completes the workload migrater examines the deployment profile that it received from the migration manager . If the deployment profile specifies the use of a tagged VLAN to place the server on an overlay network as determined at step the workload migrater configures the server with a tagged network interface deploying the image . If the workload migrater was called with a software installation package as determined at it installs this onto the server after deploying the image at step . This may contain a tunnel client driver configuration data and cryptographic key material so that the server will be connected to the network domain s VNA tunnel server for the overlay network. The target server is now fully configured and is rebooted at step .

Optionally after the target server reboots onto the migrated image the migration manager checks whether synchronization is required. If so it triggers a synchronization operation from source to target at step . The individual workload is now fully migrated. If this was part of a complex application the migration manager will coordinate multiple workload migrations and perform cutover when all have successfully completed.

While the real time migration of a complex application from one set of servers to another is a common use case the invention supports other use cases where the actual deployment of the application or one of its workloads occurs at some time after the images have been captured for example the recovery of a failed workload disaster recovery scaling one component of the complex application by adding additional copies of the workload or cloning a copy of the application for testing or development purposes. As described in the incorporated Watt et al. application the image library is used to store server images until they are needed. An image stored in the library can be deployed multiple times as needed.

The synchronization process used to synchronize the target server to the source after deployment can also be used with an image in the library producing an incremental capture containing the changes to the source workload since the last full or incremental capture. When deploying a target server the server can be restored to any point in time for which there is an incremental capture by first deploying the original captured image and then deploying each incremental capture in turn until reaching the desired recovery point. Additional operations that can be supported on incremental captures include consolidating multiple incremental captures into a single unit that contains all of the changes of the original captures deleting capture points etc. Incremental captures can also be taken that are based on the original full capture rather than the most recent incremental capture.

The image library can be geographically distributed with storage located in or near a plurality of network domains throughout the hybrid cloud environment. Storing an image closest to where it will be deployed minimizes network lag and speeds up the deployment process. Storing multiple copies of an image in different network domains provides for redundancy.

The internal structure of the image library can be segmented by end user identity in order to provide secure multi tenancy. To further improve security in a multi tenant environment images can be encrypted with a user specific key while stored in the library.

The migration manager is the central point of control for all operations concerning application migration. High level details are shown in . The example embodiment provides a user interface that includes a graphical user interface GUI a command line interface CLI and an application programming interface API so that the features of application migration can be used by end users administrators and computer automation systems. Access to all of the interfaces is authenticated using a username and password. A user account database stores user authentication and authorization data which is used to control access to the various network domains overlay networks and cloud and hypervisor infrastructures. In a multi tenant installation the user account data in the database also includes a user specific network map user specific application maps and a URI and authentication information for accessing a user specific image library. Those skilled in the art will understand that authentication mechanisms other than passwords can be used to authenticate users and that user account information can be stored using other mechanisms than a database such as a file or directory service. And while this description of the example embodiment does not explicitly mention access control checks those skilled in the art will understand that all access to user resources or the administrative features of the invention must be approved by an appropriate access control check that compares the user s identity associated groups and authorizations against the ownership and security settings of the resource being accessed.

The user interface provides access to administrative functions which include but are not limited to the installation and configuration of the system the configuration of the virtual network managing users managing application maps specifying access control information to limit access to system resources such as virtual networks applications image libraries and server resource pools. The user interface also provides access for appropriately authorized non administrative users to migration related services that include but are not limited to defining an application examining application resources monitoring capturing migrating and deploying an application cloning or scaling an application recovering a failed workload for an application and disaster recovery.

Still referring to an application mapper monitors the servers and networks within the hybrid cloud environment and correlates their activity to provide application dependency mapping ADM . By identifying which servers are communicating and which port numbers they are using the ADM process identifies the complex applications that are running within the hybrid cloud resources. It identifies the workloads comprising each application and the network relationships between them. It stores this information in an application map database using a separate XML file for each complex application. Those skilled in the art will understand that other mechanisms and formats can be used to store the application map information.

The application mapper is primarily used for three purposes 1 during the initial installation and configuration of the migration manager the application manager is used to discover any pre existing complex applications within the hybrid environment 2 when adding a new network domain to the hybrid environment the application manager is used to discover any new complex applications that have been added with the new domain 3 the application manager is run periodically to detect any changes to the hybrid environment and the known applications. In the example embodiment the application mapper is provided by a 3party ADM product. One skilled in the art will understand that any ADM tool that provides the necessary mapping information can be used.

The application map database created by the application mapper stores data in files not separately shown describing complex applications that are currently running within the hybrid environment that were discovered by the application mapping process. The application map database files are also used to describe complex applications in many other states such as when captured for disaster recovery converted to a template for deploying a new copy of the application or as a running application that has been created using a template. Application map database files in these other states are created by other parts of the system as part of the application migration process. Alternatively an end user can create modify or delete an application map using the user interface .

Still referring to an application migrater is responsible for capturing a complex application that has been defined by an application map. It uses an infrastructure manager to access the configured migration manager in order to capture an image from each Node or NodePool defined by the application map. The application migrater is also responsible for deploying a new copy of an application from the template of a previously captured application and for migrating an application directly from a running application to a new set of resources. When deploying or migrating an application the application migrater will use the infrastructure manager to access the virtual network manager plugin in order to configure the virtual network for the new resources to which the application will be deployed and the migration manager to deploy individual server images to those resources. As the capture deploy and migration of complex applications involves the coordinated steps from many components of the invention a task sequencer is used to sequence the operation through its many steps to restart the operation in the event of a recoverable failure and to recover from a failed operation. In addition to the capture deploy migration of entire complex applications the migration manager can perform operations on individual workloads comprising the application such as adding additional workloads to a node farm recovering a failed workload to a new resource or moving a single workload to a new resource or different resource domain.

As shown in an application map is used to describe a complex application. An application map comprises a number of different data elements for example application data table Application an image library ImageLibrary image data Image node data NodePool node data Node server data Server and application network data ApplicationNetwork . In the example embodiment each complex application is described by an application map that is stored as a separate XML file. One skilled in the art will recognize that the application map can be stored in many other formats such as a relational database JSON or other data communication and or storage protocol.

Within the application map the application record provides a unique description of the complex application. Each application record includes a unique ID and name a description a parent ID used to identify the application if any from which this application is derived and access control attributes controlling which end users can create modify manage scale or delete the application create a template of the application or deploy additional copies of the application.

The application s default placement instructions are used when migrating the application to new resources scaling the resources of the application or creating a copy of the application. The default placement includes a list of resource domains in which to deploy any new workloads.

The application s state information includes both a state value and a status value. The state value indicates the application s current operational state as shown in which will be one of discovering the application mapper is monitoring the hybrid cloud environment and creating the initial application map template the map contains a template to be used in creating a copy of the application and does not refer to an actual production copy of the application deploying the system is in the process of deploying a copy of the application from the template off all resources for the application have been provisioned but are currently not running booting the system is in the process of bringing the application on line fenced all resources are powered on and are communicating on the alternate overlay networks running all resources are powered on and communicating on the primary overlay networks scaling the system is in the process scaling its capacity up or down by modifying the number of copies of a server image running within one or more of the node Pools of the application where each node Pool consists of a set up servers deployed from the same source image capturing the system is in the process of capturing server images from one or more nodes of the application shutting down the system is in the process of powering off the application resources and deleting the system is in the process of terminating all resources used by the application . The allowed state transitions are also shown in .

The application status value in the State data indicates the application s current health or status and will be one of ok warning or error. When indicating a warning or error condition additional information may be included to provide details about the specific warning or error.

The application s CaptureMode works in conjunction with the application node s CapturePriority to determine the order in which the servers used in a complex application will be captured. Servers are captured in order of increasing CapturePriority. If more than one server has the same priority the CaptureMode determines whether the migration manager will capture an image from them in parallel for maximum speed or sequentially to reduce the impact upon the network resources both within the application and between the application and the image library.

A record in the ImageLibrary data specifies the location of the storage used to hold captured server images the URI and credentials used to access the storage and the access controls used to control access to the library.

A record in the Image data record records information about an image that has been captured from a server. It includes the image ID and name access controls for controlling access to the image the ID of the image library in which the capture image data resides the ID of the server from which it came the capture date and schedule for any recurring captures and the last synchronization date and synchronization schedule for any incremental updates. The synchronization options specify which portions of the image can be covered by the incremental update. Whether or not an image captured for a migration will be retained after the migration process completes is determined by the KeepPostMigrate flag in the node record . If this is set true the migration manager will retain an image from the source server when performing a direct server to server migration.

A record in the Server data stores information about an actual deployed server that is part of the application. It contains the server ID and name and the access controls controlling which users can access capture or migrate the server. The ResDomainID points to the resource domain record within the network map associated with the resource on which the server is running. The ServerSize includes the number of CPUs number of cores amount of memory number and size of disk drives etc. used for sizing the server capacity. The SourceImageID points to the image record from which the server was generated and the date time at which it was deployed or last updated. If it was not created by the migration manager these fields will be empty. The PrimaryMAC data item is used to uniquely identify the server when accessing other management systems such as SNMP based server monitoring tools.

Each record in the Node data record identifies one of the workloads comprising the complex application. When the data in an Application record refers to an actual deployed application the node s ServerID field points to the server record associated with the server running the workload. When the data in an Application record refers to a template to be used in deploying an application the node s SourceImageID field points to the image record from which to construct the server when deploying the application and the ImageSyncDate field specifies which date to use when there is more than one image or incremental update available from the source server. The Placement field contains resource domain and server size information necessary for selecting an appropriate server resource for deploying the server. The DeployPriority field specifies the order in which the workloads should be deployed for the complex application. For example it is usually necessary to deploy a database server before deploying an application server that needs access to the database. The DeployProfile field is used to specify modifications to the server configuration when deploying the workload. For example if a database has been migrated using LUN replication this might specify the new location of the database. A node record may also include a plurality of NetworkInterface subrecords that specify how to configure the network interfaces on the deployed server. The boot protocol IP address network mask gateway etc. will be taken directly from the source server or source image when the workload is being deployed as a migration to replace the original source application. When deploying a second copy of an application these values may be changed to avoid network conflicts or the address can be kept the same and the network interface placed on a different overlay network.

Still referring to a NetworkInterface subrecord in a network interface s ApplicationNetworkID field controls the overlay network to which the interface is connected. This points to an ApplicationNetwork record that in turn points to two OverlayNetwork data items e.g. and . The OVNET ID field points to the production overlay network to which the interface should be connected when the application is fully deployed and placed into production. The FenceOVNET ID field is optionally supplied to specify an alternate overlay network to which the interface should be connected prior to going live. This effectively fences the interface from the original source server so they will not conflict if using the same IP address.

When a complex application requires more than one copy of a workload for example it uses a pool of web servers to better handle high volume traffic a NodePool record is used to identify the pool of related servers and provide the template from which to build additional copies. A NodePool record works as a template its SourceImageID ImageSyncDate Placement DeployPriority and DeployProfile fields are the same as a Node record. The PrimeNodeID field specifies the node from which to copy the server image during a capture of migrate operation. The MinNodes field and MaxNodes field specify the minimum and maximum number of nodes that can be deployed and running for the application.

The NetworkInterface data comprises subrecords for the NodePool data are different than those for the Node in that they do not specify IP address information. Rather if BootProto specifies static addressing the IPAddressRange specifies a range of addresses from which it should allocate a new address when deploying a server. All Nodes that are associated with a NodePool set their NodePoolID to point to the controlling NodePool record.

The records within an application map will be linked in different ways depending upon the state of the application. shows a set of application maps in block diagram form as they might be seen in several varied stages or operations for example as discovered by an application mapper of the migration manager using a node pool and after application capture. It will be understood that the data representing the application map will take the form as discussed above in connection with after the operations as will be described.

In some embodiments the application mapper may not detect that the two web servers which are independent nodes on the network are clones. Thus FIG. B shows an application map created by the user editing the original application map e.g. using the Migration Manager user interface . A NodePool record is created and the two nodes are linked to it. The NodePool s PrimeNodeID is set to WebServer1. It will be appreciated that the addition and use of the NodePool record allows the system to automatically scale the application s web server capacity.

As discussed above illustrates an application state diagram which was discussed above and need not be described further here.

The installation process starts at step with the installation and configuration of the migration manager workload migrater virtual network manager and configuration of the image library . The image library is configured at step . This includes allocating storage setting up access controls and otherwise configuring the network storage device so that it is accessible by the other components of the system. Prior to deploying the primary VNA any firewall or security device protecting the primary network domain must be configured to allow incoming tunnel connections to the primary VNA as shown at step . An administrative user now logs into the migration manager UI and specifies the configuration of the primary network domain which includes identifying the resource domains that can be used for deploying servers. Once the resource domains have been defined the migration manager can deploy the primary VNA at step . When first run the VNA will run its discovery process at step on the primary network domain discovering its VLANs subnets and servers.

After the primary VNA has been deployed at step the installation process continues with the configuration of the virtual network by a network configuration process . The administrator specifies the necessary network domains at step that define the hybrid cloud environment deploying the necessary remote VNAs at step configuring the overlay networks at step and then deploying the overlay networks at step . The last step is shown in full detail in .

The application configuration process begins by running the application mapper to discover the applications currently running within the hybrid environment and create an initial set of application maps similar to . The resulting application map should be examined by the end user. Depending upon the accuracy of the mapping tool it might be necessary to add or remove nodes from the map or to consolidate multiple nodes into a node pool as shown at step .

Also at step the user should then examine the application networks data ApplicationNetwork defined in the application map. Rather than directly show each node s connections to the globally defined overlay networks the mapping tools creates a set of application network records . The node interfaces are then mapped to the application networks using their ApplicationNetworkID data which are then mapped to the currently used overlay network data . This allows the user to easily specify the use of alternate networks when deploying the application whether to safely deploy an additional copy of the application without interfering with the original copy or to specify a set of networks to use for temporarily fencing the application after initial deployment.

The final step of the application configuration process prior to starting the application migration process is to specify the placement data for the deployed application at step using the application s placement record . This specifies a list of one or more resource domains in which to deploy the application. Without further instruction the migration manager will attempt to deploy all of the application workloads into a single resource domain and will search the supplied list of domains for one with sufficient resources. The servers chosen or created for deployment will be sized to match the original source servers. Each node and node pool have an additional placement fields and respectively that override the application placement instructions. These placement records include both a list of target resource domains as well as required server size data. This allows an individual node to be deployed to a separate set of resources or to be scaled up down in capacity.

With the placement data specified the application migration process can begin. The first step is to install the capture agent onto the source servers at step . The procedure for doing this depends upon the selection of workload migrater. In the example embodiment agent installation can be automated if the node s administrative credentials are made available to the migration manager. Otherwise the agents can be installed by some other means such as manual installation or the use of a software patch manager.

With the agents installed the source servers are ready for capture. The capture process can be sequenced using the node s CapturePriority if it is necessary to ensure that images are captured in a specific order. Images can be captured in parallel at step if there is sufficient bandwidth between the servers and the image library. This provides the fastest migration. Otherwise the images can be capture sequentially at step to minimize the load placed on the network and storage infrastructures.

After all images have been captured they are deployed out to the target resources specified by the placement data. The deployments are performed in a prioritized manner at step to ensure that the workloads become available in the correct order to meet any application dependencies. The detailed deployment process for an individual workload is shown in .

After all workloads have been deployed the migration manager waits until all the new servers are running at step . The migration manager can tell when a deployed server is up and running because the capture agent on the original source server is migrated to the target and the new server thus appears in the set of servers visible to the migration manager. Using the application s optional verification script the migration manager waits for the application to become available. The verification script is custom to the complex application. It exercises enough of the application s functionality to ensure that the application and all of its associated workloads are functioning correctly. Verifying the operation of a complex operation can be a difficult chore often more than can be handled by a single script. Thus the migration manager can optionally check with the user for final verification.

After the application has been verified if automated cutover has been specified at step the migration manager contacts the capture agents on all source servers to switch the servers onto alternate overlay networks making them inaccessible for production use. It then immediately contacts the capture agents on the newly deployed servers to switch the servers from the fenced to the production overlay networks at step making them accessible for production use. The original source servers are left running on the fenced networks in case there is a need to revert back to the original servers.

If only a portion of the workloads comprising the application were marked for migration then only those servers will be swapped into production. The resulting application will then run with some workloads on their original servers and some on newly deployed servers. The application will continue to function as it did with the original servers because the newly deployed servers wherever they reside are using the same network addresses as the original servers that they replaced and are connected to the remaining original servers via the overlay networks

Many corporate and public cloud data center environments enhance security by isolating some network segments so that a server connected to the isolated network cannot be accessed from an external network and cannot itself access an external network. In order for invention system constructed as described herein to function on such isolated networks they must be tied into a virtual overlay network using a VNA that has access to both the isolated network and an external network on which it can establish tunnel connections to other VNAs. One approach for handling isolated environments is to create a special provisioning overlay network. The capture and deploy agents can connect to the provisioning network for the duration of migration tasks and the workload can remain isolated during normal operation.

From the foregoing it will be understood that various aspects of the processes described herein are software processes that execute on computer systems that form parts of the system . Accordingly it will be understood that various embodiments of the system described herein are generally implemented as specially configured computers including various computer hardware as discussed in greater detail below. Embodiments within the scope of the present disclosure also include computer readable media for carrying or having computer executable instructions or data structures stored thereon. Such computer readable media can be any available media which can be accessed by a computer or downloadable through communication networks. By way of example and not limitation such computer readable media can comprise various forms of data storage devices or media such as RAM ROM flash memory EEPROM CD ROM DVD or other optical disk storage magnetic disk storage solid state drives SSDs or other data storage devices any type of removable non volatile memories such as secure digital SD flash memory memory stick etc. or any other medium which can be used to carry or store computer program code in the form of computer executable instructions or data structures and which can be accessed by a general purpose or special purpose computer or a mobile device.

When information is transferred or provided over a network or another communications connection either hardwired wireless or a combination of hardwired or wireless to a computer the computer properly views the connection as a computer readable medium. Thus any such a connection is properly termed and considered a computer readable medium. Combinations of the above should also be included within the scope of computer readable media. Computer executable instructions comprise for example instructions and data which cause a general purpose computer special purpose computer or special purpose processing device such as a mobile device processor to perform one specific function or a group of functions.

Those skilled in the art will understand the features and aspects of a suitable computing environment in which aspects of the disclosure may be implemented. Although not required the embodiments of the claimed inventions are described in the context of computer executable instructions such as program modules or engines as described earlier being executed by computers in networked environments. Such program modules are often reflected and illustrated by flow charts sequence diagrams exemplary screen displays and other techniques used by those skilled in the art to communicate how to make and use such computer program modules. Generally program modules include routines programs functions objects components data structures application programming interface API calls to other computers whether local or remote etc. that perform particular tasks or implement particular defined data types within the computer. Computer executable instructions associated data structures and or schemas and program modules represent examples of the program code for executing steps of the methods disclosed herein. The particular sequence of such executable instructions or associated data structures represent examples of corresponding acts for implementing the functions described in such steps.

Those skilled in the art will also appreciate that the claimed systems and methods may be practiced in network computing environments with many types of computer system configurations including personal computers smartphones tablets hand held devices multi processor systems microprocessor based or programmable consumer electronics networked PCs minicomputers mainframe computers and the like. Embodiments of the claimed invention are practiced in distributed computing environments where tasks are performed by local and remote processing devices that are linked either by hardwired links wireless links or by a combination of hardwired or wireless links through a communications network. In a distributed computing environment program modules may be located in both local and remote memory storage devices.

An exemplary system for implementing various aspects of the described operations includes a computing device including a processing unit a system memory and a system bus that couples various system components including the system memory to the processing unit. The computer will typically include one or more data storage devices for reading data from and writing data to. The data storage devices provide nonvolatile storage of computer executable instructions data structures program modules and other data for the computer.

Computer program code that implements the functionality described herein typically comprises one or more program modules may be stored on a data storage device. This program code as is known to those skilled in the art usually includes an operating system one or more application programs other program modules and program data. A user may enter commands and information into the computer through keyboard touch screen pointing device a script containing computer program code written in a scripting language or other input devices not shown such as a microphone etc. These and other input devices are often connected to the processing unit through known electrical optical or wireless connections.

The computer that effects many aspects of the described processes will typically operate in a networked environment using logical connections to one or more remote computers or data sources which are described further below. Remote computers may be another personal computer a server a router a network PC a peer device or other common network node and typically include many or all of the elements described above relative to the main computer system in which the inventions are embodied. The logical connections between computers include a local area network LAN a wide area network WAN virtual networks WAN or LAN and wireless LANs WLAN that are presented here by way of example and not limitation. Such networking environments are commonplace in office wide or enterprise wide computer networks intranets and the Internet.

When used in a LAN or WLAN networking environment a computer system implementing aspects of the invention is connected to the local network through a network interface or adapter. When used in a WAN or WLAN networking environment the computer may include a modem a wireless link or other mechanisms for establishing communications over the wide area network such as the Internet. In a networked environment program modules depicted relative to the computer or portions thereof may be stored in a remote data storage device. It will be appreciated that the network connections described or shown are exemplary and other mechanisms of establishing communications over wide area networks or the Internet may be used.

While various aspects have been described in the context of a preferred embodiment additional aspects features and methodologies of the claimed inventions will be readily discernible from the description herein by those of ordinary skill in the art. Many embodiments and adaptations of the disclosure and claimed inventions other than those herein described as well as many variations modifications and equivalent arrangements and methodologies will be apparent from or reasonably suggested by the disclosure and the foregoing description thereof without departing from the substance or scope of the claims. Furthermore any sequence s and or temporal order of steps of various processes described and claimed herein are those considered to be the best mode contemplated for carrying out the claimed inventions. It should also be understood that although steps of various processes may be shown and described as being in a preferred sequence or temporal order the steps of any such processes are not limited to being carried out in any particular sequence or order absent a specific indication of such to achieve a particular intended result. In most cases the steps of such processes may be carried out in a variety of different sequences and orders while still falling within the scope of the claimed inventions. In addition some steps may be carried out simultaneously contemporaneously or in synchronization with other steps.

The embodiments were chosen and described in order to explain the principles of the claimed inventions and their practical application so as to enable others skilled in the art to utilize the inventions and various embodiments and with various modifications as are suited to the particular use contemplated. Alternative embodiments will become apparent to those skilled in the art to which the claimed inventions pertain without departing from their spirit and scope. Accordingly the scope of the claimed inventions is defined by the appended claims rather than the foregoing description and the exemplary embodiments described therein.

