---

title: Techniques for video data encoding
abstract: The techniques for encoding video content are disclosed. In an online game environment, the techniques include obtaining information for a first and second successive frames of video content and information for a position of a virtual camera associated with each frame, determining virtual camera translation information based on the positions of the virtual camera, determining a projected movement between the frames of an object included in each frame, determining the portion of the first frame to be excluded from the second frame and a new portion of the second frame, and providing the determined encoded information for a reconstruction of the second frame based on the provided information.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09497487&OS=09497487&RS=09497487
owner: Amazon Technologies, Inc.
number: 09497487
owner_city: Seattle
owner_country: US
publication_date: 20141205
---
This application is a continuation of application Ser. No. 13 076 363 filed Mar. 30 2011 entitled TECHNIQUES FOR VIDEO DATA ENCODING and issued to U.S. Pat. No. 8 908 775 on Dec. 9 2014 the disclosure of which is incorporated herein by reference in its entirety for all purposes.

Video encoding or video compression is technique for reducing the quantity of data used to represent digital video images usually comprising a sequence of video frames and is a combination of spatial image compression and temporal motion compensation. Ordinarily a video encoding scheme is applied to encode video data at a content provider e.g. a video data server and the encoded data is delivered to a client e.g. a client computing device where the delivered data is decoded and a corresponding image is displayed to a user. Typically video encoding is based on a frame by frame comparison where two adjacent video frames e.g. a current frame and a next frame are compared at a server and the determined video data differences if any are transmitted to a client configured to reconstruct the next frame. When substantial differences between the frames are detected a video compression scheme must send more data to keep up with the larger number of pixels data units comprising a video image that are changing from the current frame to the next.

Further in order to maintain an acceptable level of quality conventional frame by frame analysis may involve more than just two adjacent frames for example analyzing sixteen frames at a time which may require substantial processor and memory resources. Also conventional frame to frame analysis may cause lags in image delivery particularly in real time video or computer game environment. If video content includes an explosion flames a flock of birds or any other image with a great deal of high frequency detail image quality from the current frame to the next frame may decrease and ultimately may be compromised.

Techniques are described in accordance with various embodiments for the encoding and or decoding of video data. In various embodiments video data encoding utilizes information relating to camera movement during a time period between at least a current frame and a next frame. Various techniques further utilize information defining a video content object location in the next frame relative to the object location in the current frame. Such techniques avoid the need for a frame by frame comparison as in conventional video encoding techniques where two adjacent video frames are compared and the determined video data differences if any are transmitted to a decoder configured to reconstruct the next frame based on the current frame and the determined differences between the current and the next frames.

Camera movement information be it a video camera or virtual camera may be utilized to encode differences between two subsequent frames in order to reconstruct a frame based on a previous frame and a camera movement that occurred between frames. In an embodiment a camera position may shift between a current frame and a next frame. Based on a new camera position it is possible to determine a portion of the current frame that is included in the next frame and therefore may be used for a reconstruction of the next frame. Knowing the camera movement parameters e.g. direction speed and or angle it is possible to determine a location of the portion of the current frame in the new frame. Essentially a portion of the current frame translates into the new frame according to the camera translation movement that occurred between the frames. Thus it is possible to encode and utilize the translation information corresponding to the portion of the current frame for subsequent reconstruction of the next frame based at least on the portion of the current frame and the translation parameters corresponding to the known camera movement between frames.

Furthermore objects included in a frame may also move between frames. Information indicating object movement translation between the frames may also be captured encoded and used for the reconstruction of the next frame by a decoder for rendering the frame on a client user device. Additional differences between the current and next frame e.g. a portion of the next frame that was not included in the current frame may be determined and corresponding information may be encoded and subsequently used in the reconstruction of the next frame.

As shown in the environment includes a content server that may be operated by a content provider . In the example illustrated in the content provider via the content server offers an online computer game to be rendered on a client device . While the example environment shows the online computer game for the purpose of illustration it should be understood that any type of video content may be included in the environment or variations thereof. Content may be provided in various ways such as through an application interface or through other mechanisms. The environment further includes a camera in an embodiment a virtual camera associated with the content server . In an embodiment the virtual camera is configured to provide a viewpoint information for each video frame rendered to the client device .

In another embodiment the camera is a live camera used for transmitting a live broadcast to users and the camera location relative to each video frame may also be established as described above. In the above examples client devices may not be limited to computing devices. Client devices may encompass TV sets smart phones and other devices suitable for decoding and rendering video images to the users . The environment further includes an encoder associated with the content server . The encoder is configured to encode video content in accordance with the embodiments further described in reference to .

Conventional video encoding techniques involve a frame by frame analysis. Typically video encoding or video compression operates on square shaped groups of neighboring pixels often called macro blocks. These pixel groups or blocks of pixels are compared from one frame to the next and the video compression codec encode decode scheme sends only the differences within those blocks. The conventional encoding technique works well if the video content delivered in video frames has no or little motion. A still frame of text for example can be repeated with very little transmitted data. Typically however video content contains moving objects changing scenery and the like that change from video frame to video frame.

Differences between frames may occur for a variety of reasons. For example an object may move e.g. change its position in a new frame relative to the object s position in a previous frame. Furthermore a camera that records a video may change its own position e.g. shift up and down move from left to right and or change its angle.

In an embodiment as illustrated by the users may interact with content in order for example to play a computer game offered by the content provider . Typically video games include video content e.g. video images of a variety of objects generated by video game software and rendered on a user video device. For example a video game may include a landscape or a cityscape as a background and an automobile controlled by a user via a user interface as illustrated in and described below. The computer game shown in includes at least one virtual representation of an object i.e. avatar .

Typically generating and rendering video images e.g. images associated with a computer game involve 3D computer graphics that use a three dimensional representation of geometric data stored at the computer for the purposes of performing calculations and rendering 2D images. Rendering a video image is the process of generating an image from a scene file by means of computer programs. A scene file may contain objects in a strictly defined language or data structure it may also contain geometry viewpoint texture lighting and shading information as a description of the virtual scene. The data contained in the scene file is then passed to a rendering program renderer to be processed and output to a digital image or raster graphics image file. Though the technical details of rendering methods vary the general challenges to overcome in producing a 2D image from a 3D representation stored in a scene file are outlined as the graphics pipeline along a rendering device such as a Graphics Processing Unit GPU .

Rendering 3D images using 2D data in a video game may be compared to rendering a video broadcast. For example a computer game related image frame of a 3D landscape that a user views on his her video device may be considered as taken through an imaginary camera point so that the user sees the landscape as if it was shot by a virtual camera from the camera point chosen by a game developer. For example a first person shooter video game centers the gameplay around projectile weapon based combat through the first person s perspective i.e. the player experiences the action through the eyes of a protagonist similar to an action scene viewed through a virtual camera. In an embodiment a virtual camera viewpoint may be described as the camera coordinates and or direction in a 3D space that is defined by a video frame that may be viewed from the camera viewpoint. 

By way of example in a video game environment illustrated in if a virtual camera shifts a few pixels or pixel columns comprising a frame to the right between the current frame and the next frame a corresponding pixel velocity may be determined and assigned to pixels comprising a portion of the current frame that translates into the next frame. Further the pixels comprising a part of the left side of the current frame that are no longer included in the next frame because of the camera shift to the right may be determined and dropped from the next frame when it is reconstructed on the client side. Also the pixels comprising a part of the right side that need to be added to the right side of the next frame due to a camera shifting to the right may be determined from the boundaries of the next frame relative to the boundaries of the current frame on the server side and supplied to the decoder on the client side to be used for the reconstruction of the next frame.

Accordingly as shown in the frame still includes the tree but no longer includes the sun as the sun is in the portion of the frame that is not included in the new frame due to the camera viewpoint change. The portion of the frame is the new portion of the frame that was not included in the current frame .

Element shows in detail the next frame taken by camera after it was shifted to the new position . As illustrated in frame the tree is now showing on the left side of the frame and the frame includes the added portion that was not included in the frame and was added to the frame because of the camera shift to the right. The dotted line element that overlaps with the frame is a boundary of the current frame shown in relation to the new frame . The portion to reconstruct of the frame shown in the frame as a common portion belongs to both frames. The virtual camera translation information comprising the data related to the camera movement e.g. coordinates of the camera positions and the camera direction or angle may be provided to an encoder . The new frame may be encoded by using the virtual camera translation information for the common portion and information defining the dropped portion corresponding to the portion to drop and the added portion corresponding to the portion to add .

In one embodiment it may be beneficial to store e.g. cache the dropped portion so as to be accessible by the encoder if needed at least for a determined period of time or determined number of frames. For example the camera after shifting to the right may subsequently one or more frames later shift back to the left thus again covering at least a part of the dropped portion. In this case the data defining the dropped portion may be utilized for encoding a part of the dropped portion that again appeared in a new frame.

As discussed above both a camera and an object in the frame may move within the time period between frames. In an example illustrated in the camera may shift to the right from a current frame to next frame while the object may move to the left at the same time. In that case the camera movement information and the object movement information may be obtained and relative velocity may be derived from the camera movement information and the object movement information. For example if a camera moves 3 pixel columns to the right relative to the current frame and the object in the frame moves 5 pixel columns to the left relative to the current frame a total scene movement is 8 pixel columns with respect to the car but only 3 pixel columns with respect to the background of the frame e.g. landscape or cityscape .

Accordingly as shown in in addition to the elements of landscape that should be included in the next frame taken from the viewpoint by the camera the car has moved in the time period between the frame and the next frame and assumed a new position as shown by the contour . The contour of the car illustrates a new position of the car in the middle of the frame i.e. assuming that the camera remains still .

The next frame is illustrated in greater detail as an element which includes the common portion with the frame . The portion of the frame is not included in the next frame . The common portion includes the car in its new position corresponding to the contour in the frame . The common portion further includes a contour of the car corresponding to the initial position of the car in the frame . The common portion may be reconstructed as a part of new frame based on the known camera translation information sent to encoder and the known or predicted movement of the car from one frame to the next. The position vacated by the car and illustrated by the contour may be filled with the video data defining the part of the landscape occupied by the car in the frame and vacated in the frame . As illustrated in the diagram the next frame may be reconstructed using the camera translation information to reconstruct the common portion a portion of the landscape that is vacated by the car and the information regarding the added portion and the dropped portion .

In one embodiment the information assembled as described above may be provided to a decoder associated with a client e.g. user video device in order to efficiently and correctly reconstruct the next frame from the current frame . The decoder may be configured to hold e.g. store information associated with the current frame. The decoder may receive the provided information decode the camera translation information and object translation information reconstruct the common for current and next frames portion and append the next frame with the portion of the landscape that is vacated by the car add portion and drop portion . The next frame thus reconstructed may then be rendered on the user video device. The process of encoding the information needed for the reconstruction of the next frame will be discussed in greater detail in reference to .

In real time broadcast environment for example a camera used for recording video content may contain and if necessary provide its movement information also referred hereinafter as translation information . For example a camera may have a mechanism to measure its angle relative to a horizon e.g. a gyroscope and provide the taken measurement if needed. The camera movement may be tracked using a camera tracking technique known in the film industry. For example camera tracking technique may involve placing point markers in the scene that help determine the camera position in 3D space at all times. In general a camera position in 3D space may be determined and corresponding data injected into and delivered along with the recorded video data.

In video game environment as in a live video broadcast environment a virtual camera movement may be tracked. For example Microsoft DirectX a collection of application programming interfaces APIs for handling tasks related to game programming and video has a capability to provide data associated with a change of a virtual camera viewpoint in a game and therefore effectively provide information related to a virtual camera movement camera translation from one frame to another. Accordingly in an embodiment known camera movement information be it a video camera or virtual camera may be utilized to determine differences between two subsequent frames in order to reconstruct a frame based on a previous frame and a camera movement that occurred between frames.

Referring back to at block the camera movement translation information is determined based on the new camera position relative to the camera s previous position as described above. By knowing the virtual camera movement parameters e.g. direction speed and or angle it is possible to determine how the portion of the current frame that is present in the new frame translates into the new frame. The translation information may include pixel velocities assigned to each pixel comprising the common portion of the frame.

At block a projected object position in the new frame is determined. For example in an embodiment video game data may contain information regarding the object movement and inform the encoder accordingly. Video game software may be configured such that the object movement information is scripted and therefore may be accessible. As another example a user may control object movements and the movement information may be derived from the user s movement of an object. Furthermore the object s size shape color and other characteristics may change between frames and may also be approximated based on one or more historic frames preceding the next frame. In summary there are numerous ways to educate the encoder with the object s expected position in the next frame vis vis the object s position in the current frame. Having an approximation of what an object should look like in the next frame relative to the current frame it is possible to transmit to the decoder the movement translation information of an object along with the differences between the object s shape and size in the current and next frames in order.

Thus information related to an object movement from the current frame to the next frame may be obtained in a number of different ways. In one embodiment this information may be derived from the known movement of the object between a frame previous to the current frame and a current frame. For example an object may change its position and or direction size or color between a previous frame and a current frame. Based on that information it is possible to approximate the object position and other characteristics in the next frame relative to the current frame. Based on the above determination at block the object translation information is determined. As described above this information may include pixel velocities size and color characteristics that are assigned to the data set comprising an object in the current frame so that the object translates correctly into the new frame. At block additional data necessary to complete the reconstruction of the new frame is determined as described above in reference to . Finally at block the determined and encoded translation information and additional data may be provided for subsequent frame reconstruction. The process then ends.

As discussed above a camera position may shift between a current frame and a next frame. Based on a new position of a camera it is possible to determine a boundary of the next frame relative to that of the current frame and accordingly determine a portion of the current frame that is included in the next frame. Specifically knowing the virtual camera movement parameters e.g. direction speed and or angle it is possible to determine a location of the portion of the current frame in the new frame. Essentially the portion of the current frame translates into the new frame according to the camera translation movement that occurred between the frames. Thus the translation information corresponding to the portion of the current frame may be supplied to a decoder for reconstruction of the next frame based at least on the portion of the current frame and the translation parameters corresponding to the known camera movement between frames.

Referring back to if at decision block it is determined that no information for the camera movement between frames is received at block the new frame information is determined using other techniques e.g. a conventional frame to frame analysis as described above. Then at block the next frame data obtained as a result of the frame to frame analysis is transmitted to a client device. If the information for the camera movement between frames is received at block pixel velocity values are based on the camera movement information and assigned to a portion of the current frame that translates into the next frame as described above. At block additional data necessary to complete next frame as described above in relation to is determined. At block the pixel velocity values and additional data are provided for reconstruction of the next frame. The process then ends.

The process begins at block where the current frame data is received. At block the next frame data is received that includes the indication that the camera is still or static. At determination block it is determined whether the object moved between frames. If the object moved at block the object movement translation information is determined as discussed above in reference to . At determination block it is determined whether the object changed its characteristics e.g. size shape angle or color between frames. If any of these characteristics changed at block additional data is determined that reflects the object differences between the frames.

As discussed above objects in a frame may move turn change shapes and the like. For example a car e.g. car included in a video image or in a live video footage may move a certain distance in the frame during the time period between the current frame and next frame. Information indicating object movement translation between the frames may also be captured and used for the reconstruction of the next frame on the client side. In an embodiment related to a video game environment a renderer may inform the encoder with the information related to a projected movement of an object from a current frame to the next frame.

Referring back to at block the object translation information and additional data determined at block and are provided for next frame reconstruction. If the object did not move between frames at block it is determined that the next frame is practically the same as the current frame because the camera did not move either. The process then ends.

At determination block it is determined whether the frame degradation is above threshold. If it is determined that degradation is below threshold at determination block it is determined whether the object change between frames is above threshold. If the degradation due to the camera movement is determined to be above threshold or the object has been determined to change above predetermined threshold the process moves to block where the key frame is transmitted to client as the next frame. If the frame degradation due to the camera movement is determined to be below threshold and the object change is determined to be below threshold at block the data for new frame encoding is generated using the camera movement information and projected object movement information as described above in reference to . Finally at block the generated data may be assembled and provided for frame reconstruction. The process then ends.

The process begins at block where the decoder may receive the provided new frame information including virtual camera position information and additional data needed for reconstruction of new frame. At block the camera translation information is extracted from the received data. At block a projected object position in a new frame is extracted from the received data. At block additional data needed to complete new frame for example information defining a current frame portion that should not be included in the next frame information defining a frame portion to be added to the new frame and information defining a portion to fill the space vacated by the moved object is extracted from the received data. Finally at block the extracted data is used to reconstruct the next frame based on the extracted information and the stored current frame and to render the reconstructed new frame to the user on the user video device. At least this portion of the process then ends.

The illustrative environment includes at least one application server and a data store . It should be understood that there can be several application servers layers or other elements processes or components which may be chained or otherwise configured which can interact to perform tasks such as obtaining data from an appropriate data store. As used herein the term data store refers to any device or combination of devices capable of storing accessing and retrieving data which may include any combination and number of data servers databases data storage devices and data storage media in any standard distributed or clustered environment. The application server can include any appropriate hardware and software for integrating with the data store as needed to execute aspects of one or more applications for the client device e.g. applications for handling tasks related to game programming and video handling a majority of the data access and business logic for an application. The application server provides access control services in cooperation with the data store and is able to generate content such as text graphics audio and or video to be transferred to a viewer which may be served to the viewer by the Web server in the form of HTML XML or another appropriate structured language in this example. The handling of all requests and responses as well as the delivery of content between the client device and the application server can be handled by the Web server. It should be understood that the Web and application servers are not required and are merely example components as structured code discussed herein can be executed on any appropriate device or host machine as discussed elsewhere herein.

The data store can include several separate data tables databases or other data storage mechanisms and media for storing data relating to a particular aspect. For example the data store illustrated includes mechanisms for storing production data and user information which can be used to serve content for the production side. The data store also is shown to include a mechanism for storing log data which can be used for reporting generating statistics and other such purposes. It should be understood that there can be many other aspects that may need to be stored in the data store such as page image information and access right information which can be stored in any of the above listed mechanisms as appropriate or in additional mechanisms in the data store . The data store is operable through logic associated therewith to receive instructions from the application server and obtain update or otherwise process data in response thereto. In one example a viewer might submit a search request for a certain type of item. In this case the data store might access the user information to verify the identity of the viewer and can access the catalog detail information to obtain information about items of that type. The information then can be returned to the viewer such as in a results listing on a Web page that the viewer is able to view via a browser on the user device . Information for a particular item of interest can be viewed in a dedicated page or window of the browser.

Each server typically will include an operating system that provides executable program instructions for the general administration and operation of that server and typically will include a computer readable medium storing instructions that when executed by a processor of the server allow the server to perform its intended functions. Suitable implementations for the operating system and general functionality of the servers are known or commercially available and are readily implemented by persons having ordinary skill in the art particularly in light of the disclosure herein.

The environment in one embodiment is a distributed computing environment utilizing several computer systems and components that are interconnected via communication links using one or more computer networks or direct connections. However it will be appreciated by those of ordinary skill in the art that such a system could operate equally well in a system having fewer or a greater number of components than are illustrated in . Thus the depiction of the system in should be taken as being illustrative in nature and not limited to the scope of the disclosure.

As discussed above the various embodiments can be implemented in a wide variety of operating environments which in some cases can include one or more client computers computing devices or processing devices which can be used to operate any of a number of applications. Client devices can include any of a number of general purpose personal computers such as desktop or laptop computers running a standard operating system as well as cellular wireless and handheld devices running mobile software and capable of supporting a number of networking and messaging protocols. Such a system also can include a number of workstations running any of a variety of commercially available operating systems and other known applications for purposes such as development and database management. These devices also can include other electronic devices such as dummy terminals thin clients gaming systems and other devices capable of communicating via a network.

Various aspects also can be implemented as part of at least one service or Web service such as may be part of a service oriented architecture. Services such as Web services can communicate using any appropriate type of messaging such as by using messages in extensible markup language XML format and exchanged using an appropriate protocol such as SOAP derived from the Simple Object Access Protocol . Processes provided or executed by such services can be written in any appropriate language such as the Web Services Description Language WSDL . Using a language such as WSDL allows for functionality such as the automated generation of client side code in various SOAP frameworks.

Most embodiments utilize at least one network that would be familiar to those skilled in the art for supporting communications using any of a variety of commercially available protocols such as TCP IP OSI FTP UPnP NFS CIFS and AppleTalk. The network can be for example a local area network a wide area network a virtual private network the Internet an intranet an extranet a public switched telephone network an infrared network a wireless network and any combination thereof.

In embodiments utilizing a Web server the Web server can run any of a variety of server or mid tier applications including HTTP servers FTP servers CGI servers data servers Java servers and business application servers. The server s also may be capable of executing programs or scripts in response to requests from client devices such as by executing one or more Web applications that may be implemented as one or more scripts or programs written in any programming language such as Java C C or C or any scripting language such as Perl Python or TCL as well as combinations thereof. The server s may also include database servers including without limitation those commercially available from Oracle Microsoft Sybase and IBM .

The environment can include a variety of data stores and other memory and storage media as discussed above. These can reside in a variety of locations such as on a storage medium local to and or resident in one or more of the computers or remote from any or all of the computers across the network. In a particular set of embodiments the information may reside in a storage area network SAN familiar to those skilled in the art. Similarly any necessary files for performing the functions attributed to the computers servers or other network devices may be stored locally and or remotely as appropriate. Where a system includes computerized devices each such device can include hardware elements that may be electrically coupled via a bus the elements including for example at least one central processing unit CPU at least one input device e.g. a mouse keyboard controller touch screen or keypad and at least one output device e.g. a display device printer or speaker . Such a system may also include one or more storage devices such as disk drives optical storage devices and solid state storage devices such as random access memory RAM or read only memory ROM as well as removable media devices memory cards flash cards and the like.

Such devices also can include a computer readable storage media reader a communications device e.g. a modem a network card wireless or wired an infrared communication device and working memory as described above. The computer readable storage media reader can be connected with or configured to receive a computer readable storage medium representing remote local fixed and or removable storage devices as well as storage media for temporarily and or more permanently containing storing transmitting and retrieving computer readable information. The system and various devices also typically will include a number of software applications modules services or other elements located within at least one working memory device including an operating system and application programs such as a client application or Web browser. It should be appreciated that alternate embodiments may have numerous variations from that described above. For example customized hardware might also be used and or particular elements might be implemented in hardware software including portable software such as applets or both. Further connection to other computing devices such as network input output devices may be employed.

Storage media and computer readable media for containing code or portions of code can include any appropriate media known or used in the art including storage media and communication media such as but not limited to volatile and non volatile removable and non removable media implemented in any method or technology for storage and or transmission of information such as computer readable instructions data structures program modules or other data including RAM ROM EEPROM flash memory or other memory technology CD ROM digital versatile disk DVD or other optical storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium which can be used to store the desired information and which can be accessed by the system device. Based on the disclosure and teachings provided herein a person of ordinary skill in the art will appreciate other ways and or methods to implement the various embodiments.

The specification and drawings are accordingly to be regarded in an illustrative rather than a restrictive sense. It will however be evident that various modifications and changes may be made thereunto without departing from the broader spirit and scope of the present disclosure as set forth in the claims.

